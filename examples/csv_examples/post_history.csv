id,post_id,user_id,post_history_type_id,user_display_name,content_license,revision_guid,text,comment,creation_date
48,28,4.0,1,,CC BY-SA 2.5,18914b59-1d73-4c14-a8b6-25d429a1888e,The Two Cultures: statistics vs. machine learning?,,2010-07-19 19:14:44.080
47,28,4.0,3,,CC BY-SA 2.5,18914b59-1d73-4c14-a8b6-25d429a1888e,<statistics><machine-learning>,,2010-07-19 19:14:44.080
46,28,4.0,2,,CC BY-SA 2.5,18914b59-1d73-4c14-a8b6-25d429a1888e,"Last year, I read a blog post from [Bendan O'Connor][1] entitled [""Statistics vs. Machine Learning, fight!""][2] that discussed some of the differences between the two fields.  [Andrew Gelman responded to favorably to this][3]:

Simon Blomberg: 
> From R's fortunes
> package: To paraphrase provocatively,
> 'machine learning is statistics minus
> any checking of models and
> assumptions'.
> -- Brian D. Ripley (about the difference between machine learning
> and statistics) useR! 2004, Vienna
> (May 2004) :-) Season's Greetings!

Andrew Gelman:

> In that case, maybe we should get rid
> of checking of models and assumptions
> more often. Then maybe we'd be able to
> solve some of the problems that the
> machine learning people can solve but
> we can't!

There was also the [**""Statistical Modeling: The Two Cultures""** paper][4] by Leo Breiman in 2001 which argued that Statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the *predictive accuracy* of models.

Has the Statistics field changed over the last decade in response to these critiques?  Do the *two cultures* still exist or has Statistics grown to embrace machine learning techniques such as neural networks and support vector machines?


  [1]: http://anyall.org/
  [2]: http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/
  [3]: http://www.stat.columbia.edu/~cook/movabletype/archives/2008/12/machine-learnin.html
  [4]: http://www.stat.osu.edu/~bli/dmsl/papers/Breiman.pdf",,2010-07-19 19:14:44.080
289,143,114.0,3,,CC BY-SA 2.5,657b3316-c103-42cc-bdd9-a52a5fedc530,<algorithms><running><median>,,2010-07-19 21:32:38.523
288,143,114.0,1,,CC BY-SA 2.5,657b3316-c103-42cc-bdd9-a52a5fedc530,Algorithms to compute the running median?,,2010-07-19 21:32:38.523
287,143,114.0,2,,CC BY-SA 2.5,657b3316-c103-42cc-bdd9-a52a5fedc530,"On smaller window sizes, `n log n` sorting might work. Are there any better algorithms to achieve this?",,2010-07-19 21:32:38.523
323,143,,6,user88,CC BY-SA 2.5,a6573b2a-0409-4d07-98ba-52887586892e,<algorithms><possibly-off-topic><running-median>,edited tags,2010-07-19 22:05:23.010
670,143,,6,,CC BY-SA 2.5,eabdccd9-fcd9-47a7-a1aa-76e66eef254d,<algorithms><running-median>,edited tags,2010-07-20 15:47:15.207
672,143,,6,,CC BY-SA 2.5,c541c63b-6843-45e8-aeb3-ff70f45b4d85,<algorithms><possibly-off-topic><running-median>,edited tags,2010-07-20 15:56:17.447
708,143,,6,,CC BY-SA 2.5,91e1ac65-2a05-465e-8a14-176318e40a18,<algorithms><running-median>,edited tags,2010-07-20 18:12:30.547
717,143,0.0,10,,,27606156-98ee-43d6-992a-478139afd75a,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":13,""DisplayName"":""Sharpie""},{""Id"":103,""DisplayName"":""rcs""},{""Id"":28,""DisplayName"":""Srikant Vadali""}]}",2,2010-07-20 18:54:42.177
750,356,166.0,1,,CC BY-SA 2.5,47492cb7-f60a-4302-819f-86df9d13f334,What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?,,2010-07-21 00:24:35.500
751,356,166.0,3,,CC BY-SA 2.5,47492cb7-f60a-4302-819f-86df9d13f334,<distributions><statistical-significance><normality-assumption>,,2010-07-21 00:24:35.500
752,356,166.0,2,,CC BY-SA 2.5,47492cb7-f60a-4302-819f-86df9d13f334,What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?  When will results from these two methods differ?,,2010-07-21 00:24:35.500
864,412,186.0,2,,CC BY-SA 2.5,cafcf180-f874-4b98-a9a4-8167bc6c8e35,"What book would you recommend for scientists who are not statisticians?

Clear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.",,2010-07-21 15:01:21.127
865,412,186.0,1,,CC BY-SA 2.5,cafcf180-f874-4b98-a9a4-8167bc6c8e35,What book would you recommend for non-statistician?,,2010-07-21 15:01:21.127
866,412,186.0,16,,,cafcf180-f874-4b98-a9a4-8167bc6c8e35,,,2010-07-21 15:01:21.127
863,412,186.0,3,,CC BY-SA 2.5,cafcf180-f874-4b98-a9a4-8167bc6c8e35,<references><science>,,2010-07-21 15:01:21.127
872,414,4.0,16,,,f43ac356-f089-4201-b246-91cced19a08b,,,2010-07-21 15:13:21.493
871,414,4.0,1,,CC BY-SA 2.5,f43ac356-f089-4201-b246-91cced19a08b,"What is your favorite ""data analysis"" cartoon?",,2010-07-21 15:13:21.493
869,414,4.0,2,,CC BY-SA 2.5,f43ac356-f089-4201-b246-91cced19a08b,"This is one of my favorites:

![alt text][1]

One entry per answer.  This is in the vein of [this StackOverflow question][2].

P.S. Do not hotlink the cartoon without the site's permission please.


  [1]: http://imgs.xkcd.com/comics/correlation.png
  [2]: http://stackoverflow.com/questions/84556/whats-your-favorite-programmer-cartoon",,2010-07-21 15:13:21.493
870,414,4.0,3,,CC BY-SA 2.5,f43ac356-f089-4201-b246-91cced19a08b,<humor>,,2010-07-21 15:13:21.493
1014,412,190.0,6,,CC BY-SA 2.5,673b9773-839a-4584-a7a9-2c64642d1fd8,<textbook><references><science>,edited tags,2010-07-22 12:56:00.153
1106,541,,2,user28,CC BY-SA 2.5,9ba6544f-eddd-49b9-834a-4a38974300ff,"ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression. 

In light of their equivalence, is there any reason why ANOVA is used instead of linear regression? 

Note: I am particularly interested in hearing about **technical** reasons for the use of ANOVA instead of linear regression. ",,2010-07-23 15:17:56.770
1104,541,,3,user28,CC BY-SA 2.5,9ba6544f-eddd-49b9-834a-4a38974300ff,<regression><anova>,,2010-07-23 15:17:56.770
1105,541,,1,user28,CC BY-SA 2.5,9ba6544f-eddd-49b9-834a-4a38974300ff,Why is ANOVA taught / used as if it is a different research methodology compared to linear regression? ,,2010-07-23 15:17:56.770
7706,3188,450.0,2,,CC BY-SA 2.5,1c727d83-861b-4e39-be04-68dda5440df8,"It's bad form to sort an array to compute a median. Medians (and other quantiles) are typically computed using the [quickselect][1] algorithm, with $O(n)$ complexity.


You may also want to look at my answer to a recent related question [here][2].


  [1]: http://www.ics.uci.edu/~eppstein/161/960125.html
  [2]: http://stats.stackexchange.com/questions/3372/is-it-possible-to-accumulate-a-set-of-statistics-that-describes-a-large-number-of/3376#3376",,2010-10-09 19:02:09.717
8970,3646,211.0,1,,CC BY-SA 2.5,362780e6-f8e1-4074-a58c-1bc9aa366330,Kendall Tau or Spearman's rho ?,,2010-10-24 13:15:49.687
8969,3646,211.0,2,,CC BY-SA 2.5,362780e6-f8e1-4074-a58c-1bc9aa366330,"In which cases should one prefer the one over the other?

I found someone who claims an advantage for Kendall, [for pedagogical reasons][1], are there other reasons?


  [1]: http://www.rsscse.org.uk/ts/bts/noether/text.html",,2010-10-24 13:15:49.687
8971,3646,211.0,3,,CC BY-SA 2.5,362780e6-f8e1-4074-a58c-1bc9aa366330,<correlation><nonparametric>,,2010-10-24 13:15:49.687
1111,543,182.0,2,,CC BY-SA 2.5,604765be-29fa-42bb-8e40-6909a0f62e8f,"As an economist, the analysis of variance (ANOVA) is taught and usually understood in relation to linear regression (e.g. in Arthur Goldberger's *A Course in Econometrics*). Economists/Econometricians typically view ANOVA as uninteresting and prefer to move straight to regression models. From the perspective of linear (or even generalised linear) models, ANOVA assigns coefficients into batches, with each batch corresponding to a ""source of variation"" in ANOVA terminology.

Generally you can replicate the inferences you would obtain from ANOVA using regression but not always OLS regression. Multilevel models are needed for analysing hierarchical data structures such as ""split-plot designs,"" where between-group effects are compared to group-level errors, and within-group effects are compared to data-level errors. Gelman's paper [1] goes into great detail about this problem and effectively argues that ANOVA is an important statistical tool that should still be taught for it's own sake.

In particular Gelman argues that ANOVA is a way of understanding and structuring multilevel models. Therefore ANOVA is not an alternative to regression but as a tool for summarizing complex high-dimensional inferences and for exploratory data analysis. 

Gelman is a well-respected statistician and some credence should be given to his view. However, almost all of the empirical work that I do would be equally well served by linear regression and so I firmly fall into the camp of viewing it as a little bit pointless. Some disciplines with complex study designs (e.g. psychology) may find ANOVA useful.

[1] Gelman, A. (2005). Analysis of variance: why it is more important than ever (with discussion). *Annals of Statistics* 33, 1–53.",,2010-07-23 15:35:55.653
1112,541,,5,user28,CC BY-SA 2.5,9e914d5f-91a3-4993-afe7-e09b9455d94a,"ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression. 

In light of their equivalence, is there any reason why ANOVA is used instead of linear regression? 

Note: I am particularly interested in hearing about **technical** reasons for the use of ANOVA instead of linear regression. 

**Edit**

Here is one example using one-way ANOVA. Suppose, you want to know if the average height of male and females is the same. To test for your hypothesis you would collect data from a random   sample of male and females (say 30 each) and perform the ANOVA analysis (i.e., sum of squares for gender and error) to decide whether an effect exists.

You could also use linear regression to test for this as follows:

Define:

Gender = 1 if respondent is a male and 0 otherwise.

Height = Intercept + beta * Gender + error

where

error ~ N(0,sigma^2)

Then a test of whether beta = 0 is a an equivalent test for your hypothesis.",added 650 characters in body,2010-07-23 15:44:44.420
1184,412,,6,,CC BY-SA 2.5,07dd87c5-5a69-4a97-b688-f7c2f19ef30a,<textbook><references><subjective><science>,Added the 'subjective' tag to the question,2010-07-24 09:53:34.437
1185,412,,9,,CC BY-SA 2.5,cdbfc8ae-9930-4e8d-85fb-2e9dd9e5ad49,<textbook><references><science>,Rollback to [673b9773-839a-4584-a7a9-2c64642d1fd8],2010-07-24 09:58:03.123
1253,412,,6,,CC BY-SA 2.5,51620419-30bc-4850-8129-9357dfc28496,<textbook><science>,edited tags,2010-07-26 09:48:59.033
1255,412,186.0,9,,CC BY-SA 2.5,6b52e104-aad0-4e04-bad5-8a5c25a64489,<references><science>,Rollback to [cafcf180-f874-4b98-a9a4-8167bc6c8e35],2010-07-26 10:35:37.920
2349,143,,6,,CC BY-SA 2.5,1d9dd915-3289-489f-8816-9699a9ffd407,<algorithms><median>,edited tags,2010-08-03 12:14:50.543
2694,1248,399.0,1,,CC BY-SA 2.5,e161200b-7be7-4d74-86d2-a71767e9dcfa,Statistics Jokes,,2010-08-06 01:53:47.023
2696,1248,399.0,3,,CC BY-SA 2.5,e161200b-7be7-4d74-86d2-a71767e9dcfa,<humor>,,2010-08-06 01:53:47.023
2695,1248,399.0,2,,CC BY-SA 2.5,e161200b-7be7-4d74-86d2-a71767e9dcfa,"Well we've got favourite statistics quotes. What about statistics jokes?

So, what's your favourite statistics joke?",,2010-08-06 01:53:47.023
2699,1248,132.0,16,,,00000000-0000-0000-0000-000000000000,,,2010-08-06 02:30:56.857
3008,28,4.0,16,,,c45d4a16-e06d-4fd0-9c2c-540cd4aafb53,,,2010-08-09 13:05:50.603
3007,28,4.0,6,,CC BY-SA 2.5,c45d4a16-e06d-4fd0-9c2c-540cd4aafb53,<machine-learning><statistics>,edited tags,2010-08-09 13:05:50.603
3201,412,,5,,CC BY-SA 2.5,d733de28-55d8-4b51-8726-b570a14f5734,"What book would you recommend for scientists who are not statisticians?

Clear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.
",added 2 characters in body; edited title,2010-08-11 08:26:44.373
3202,412,,4,,CC BY-SA 2.5,d733de28-55d8-4b51-8726-b570a14f5734,What book would you recommend for non-statisticians?,added 2 characters in body; edited title,2010-08-11 08:26:44.373
3207,414,,5,,CC BY-SA 2.5,2e347005-52d7-4f9b-abd1-cb638923071e,"This is one of my favorites:

![alt text][1]

One entry per answer. This is in the vein of the Stack Overflow question *[What’s your favorite “programmer” cartoon?][2]*.

P.S. Do not hotlink the cartoon without the site's permission please.

  [1]: http://imgs.xkcd.com/comics/correlation.png
  [2]: http://stackoverflow.com/questions/84556/whats-your-favorite-programmer-cartoon",Named the link.,2010-08-11 08:49:02.137
3437,143,0.0,11,,,24eae004-c4b8-4ea9-b154-03167c95c3d1,"{""Voters"":[{""Id"":251,""DisplayName"":""ars""},{""Id"":159,""DisplayName"":""Rob Hyndman""}]}",,2010-08-13 00:30:00.863
4009,1760,723.0,2,,CC BY-SA 2.5,2f78dcca-ecb1-4452-ba76-9a3f08a4d4a8,"A question which bothered me for some time, which I don't know how to address:

Every day, my weatherman gives a percentage chance of rain (let's assume its calculated to 9000 digits and he has never repeated a number). Every subsequent day, it either rains or does not rain.

I have years of data - pct chance vs rain or not. *Given this weatherman's history*, if he says tonight that tomorrow's chance of rain is X, then what's my best guess as to what the chance of rain really is?",,2010-08-19 05:56:06.483
4010,1760,723.0,3,,CC BY-SA 2.5,2f78dcca-ecb1-4452-ba76-9a3f08a4d4a8,<statistical-analysis>,,2010-08-19 05:56:06.483
4008,1760,723.0,1,,CC BY-SA 2.5,2f78dcca-ecb1-4452-ba76-9a3f08a4d4a8,Is my weatherman accurate?,,2010-08-19 05:56:06.483
23159,8529,,25,,,b92a50c2-35a0-47ed-aecc-68cbee4e0a5b,,http://twitter.com/#!/StackStats/status/56463592052109312,2011-04-08 21:09:04.103
23167,8529,,4,user88,CC BY-SA 3.0,11972e04-1927-4d3e-8f39-b951d3b85b96,What are some interesting and well-written applied statistics papers?,edited title,2011-04-08 23:28:40.090
23237,8529,674.0,6,,CC BY-SA 3.0,582ad9c9-14e6-459d-9848-43f193916195,<big-list><application>,add tag + link to CVJC paper,2011-04-09 18:54:46.130
68726,22797,674.0,4,,CC BY-SA 3.0,808bec7e-d707-4701-afc0-0ab8c121377c,How does regression with and without intercept followed by test of stationarity affect cointegration test?,edited title,2012-04-02 10:34:42.647
176732,54724,16174.0,6,,CC BY-SA 3.0,53b4720d-700c-43c1-84e2-57f73d508e02,<machine-learning><model>,"Sentence case style in title, tag, LaTeX",2013-09-04 20:39:44.190
176731,54724,,24,,CC BY-SA 3.0,53b4720d-700c-43c1-84e2-57f73d508e02,,"Proposed by 22468 approved by 7290, 14156 edit id of 5245",2013-09-04 20:39:44.190
4068,1787,668.0,2,,CC BY-SA 2.5,77e02a1c-de83-4092-9220-e10415c1128e,"In effect you are thinking of a model in which the *true* chance of rain, *p*, is a function of the *predicted* chance *q*: *p* = *p(q*).  Each time a prediction is made, you observe one realization of a Bernoulli variate having probability *p(q)* of success.  This is a classic logistic regression setup if you are willing to model the true chance as a linear combination of basis functions *f1*, *f2*, ..., *fk*; that is, the model says

>Logit(*p*) = *b0* + *b1 f1(q)* + *b2 f2(q)* + ... + *bk fk(q)* + *e*

with iid errors *e*.  If you're agnostic about the form of the relationship (although if the weatherman is any good *p(q) - q* should be reasonably small), consider using a set of splines for the basis.  The output, as usual, consists of estimates of the coefficients and an estimate of the variance of *e*.  Given any future prediction *q*, just plug the value into the model with the estimated coefficients to obtain an answer to your question (and use the variance of *e* to construct a prediction interval around that answer if you like).

This framework is flexible enough to include other factors, such as the possibility of changes in the quality of predictions over time.  It also lets you test hypotheses, such as whether *p* = *q* (which is what the weatherman implicitly claims).",,2010-08-19 13:21:56.153
4922,2156,114.0,2,,CC BY-SA 2.5,de2d608e-1eae-4ffb-b7aa-21aab24c7e98,"I'm working on a small (200M) corpus of text, which I want to explore with some cluster analysis. What books or articles on that subject would you recommend?  

",,2010-09-01 23:57:06.760
4923,2156,114.0,1,,CC BY-SA 2.5,de2d608e-1eae-4ffb-b7aa-21aab24c7e98,Recommended books or articles as introduction to Cluster Analysis?,,2010-09-01 23:57:06.760
4924,2156,114.0,3,,CC BY-SA 2.5,de2d608e-1eae-4ffb-b7aa-21aab24c7e98,<machine-learning><statistics><clustering><recommendations>,,2010-09-01 23:57:06.760
4955,2156,,16,user88,,00000000-0000-0000-0000-000000000000,,,2010-09-02 09:35:37.073
4959,2169,674.0,2,,CC BY-SA 2.5,fd9acf1f-7cf0-4596-82c6-e8a9ce70feb0,"It may be worth looking at M.W. Berry's books: 

1. *Survey of Text Mining I: Clustering, Classification, and Retrieval* (2003)
2. *Survey of Text Mining II: Clustering, Classification, and Retrieval* (2008)

They consist of series of applied and review papers. The latest seems to be available as PDF at the following address: http://bit.ly/deNeiy.

Here are few links related to CA as applied to text mining: 

* [Document Topic Generation in Text Mining by Using Cluster Analysis with EROCK][1]
* [An Approach to Text Mining using Information Extraction][2]

You can also look at *Latent Semantic Analysis*, but see my response there: [Working through a clustering problem][3]. 


  [1]: http://www.cscjournals.org/csc/manuscript/Journals/IJCSS/volume4/Issue2/IJCSS-271.pdf
  [2]: http://www.google.fr/url?sa=t&source=web&cd=8&ved=0CFoQFjAH&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.21.8862%26rep%3Drep1%26type%3Dpdf&ei=P3h_TJXfAdT34AbIuN3TCw&usg=AFQjCNHaVKyo45HtjNKiEPvFnPuvD2MvmQ&sig2=vPQ1BmIz8vb16q1Oysi8_g
  [3]: http://stats.stackexchange.com/questions/369/working-through-a-clustering-problem/2196#2196",,2010-09-02 10:25:32.673
4958,2169,0.0,16,,,fd9acf1f-7cf0-4596-82c6-e8a9ce70feb0,,,2010-09-02 10:25:32.673
5781,2509,628.0,2,,CC BY-SA 2.5,27c1371d-3a9a-4aba-ad50-45fbe0e7477c,"In today's pattern recognition class my professor talked about Principal Component Analysis , Eigen Vectors & Eigen Values. 

I got the mathematics of it. If I'm asked to find eigen values etc. I'll do it correctly like a machine. But I didn't **Understand** it. I didn't get the purpose of it. I didn't get the feel of it.  I strongly believe in 

>you do not really understand something unless you can explain it to your grandmother -- Albert Einstien

Well, I can't explain these concepts to a layman or grandma.

1. Why Principal Component Analysis , Eigen Vectors & Eigen Values? What was the *need* for these concepts?
2. How would you explain these to a layman?",,2010-09-15 20:05:55.993
5782,2509,628.0,1,,CC BY-SA 2.5,27c1371d-3a9a-4aba-ad50-45fbe0e7477c,"Making sense of Principal Component Analysis , Eigen Vectors & Eigen Values",,2010-09-15 20:05:55.993
5783,2509,628.0,3,,CC BY-SA 2.5,27c1371d-3a9a-4aba-ad50-45fbe0e7477c,<statistical-analysis><classification><pca>,,2010-09-15 20:05:55.993
5791,2509,668.0,5,,CC BY-SA 2.5,f7802b4e-ec49-4fa0-818b-48ab1b0e74d6,"In today's pattern recognition class my professor talked about Principal Component Analysis , Eigenvectors & Eigenvalues. 

I got the mathematics of it. If I'm asked to find eigenvalues etc. I'll do it correctly like a machine. But I didn't **Understand** it. I didn't get the purpose of it. I didn't get the feel of it.  I strongly believe in 

>you do not really understand something unless you can explain it to your grandmother -- Albert Einstein

Well, I can't explain these concepts to a layman or grandma.

1. Why Principal Component Analysis , Eigenvectors & Eigenvalues? What was the *need* for these concepts?
2. How would you explain these to a layman?","Fixed spelling of ""eigenvectors"" and ""eigenvalues"".; deleted 1 characters in body",2010-09-15 21:36:58.120
5792,2509,668.0,4,,CC BY-SA 2.5,f7802b4e-ec49-4fa0-818b-48ab1b0e74d6,"Making sense of Principal Component Analysis , Eigenvectors & Eigenvalues","Fixed spelling of ""eigenvectors"" and ""eigenvalues"".; deleted 1 characters in body",2010-09-15 21:36:58.120
5797,2509,,4,user88,CC BY-SA 2.5,c6793033-d493-4d22-92c7-dd55595f21ce,"Making sense of principal component analysis, eigenvectors & eigenvalues","Fixed spelling and format, removed unrelated tags.",2010-09-15 22:14:55.840
5798,2509,,6,user88,CC BY-SA 2.5,c6793033-d493-4d22-92c7-dd55595f21ce,<pca>,"Fixed spelling and format, removed unrelated tags.",2010-09-15 22:14:55.840
5799,2509,,5,user88,CC BY-SA 2.5,c6793033-d493-4d22-92c7-dd55595f21ce,"In today's pattern recognition class my professor talked about PCA, eigenvectors & eigenvalues. 

I got the mathematics of it. If I'm asked to find eigenvalues etc. I'll do it correctly like a machine. But I didn't **understand** it. I didn't get the purpose of it. I didn't get the feel of it.  I strongly believe in 

>you do not really understand something unless you can explain it to your grandmother -- Albert Einstein

Well, I can't explain these concepts to a layman or grandma.

1. Why PCA, eigenvectors & eigenvalues? What was the *need* for these concepts?
2. How would you explain these to a layman?","Fixed spelling and format, removed unrelated tags.",2010-09-15 22:14:55.840
5811,2509,628.0,6,,CC BY-SA 2.5,855da758-58dc-4e69-956d-1c22aa1d4ecd,<pca><beginner>,edited tags,2010-09-16 04:37:42.657
5851,1760,,6,user88,CC BY-SA 2.5,567aeb04-9f9f-47bb-946f-a25f5cdc1f0f,<hypothesis-testing>,edited tags,2010-09-16 06:59:48.217
5868,2509,,6,user88,CC BY-SA 2.5,ace200aa-2690-4cf4-a2ba-8df9cbd31c4a,<pca>,edited tags,2010-09-16 07:13:27.197
6097,2156,,6,user88,CC BY-SA 2.5,8f6ca963-1ec8-4380-b083-f985a474999b,<machine-learning><references><clustering>,edited tags,2010-09-17 20:23:04.700
6591,1760,190.0,6,,CC BY-SA 2.5,a6f6e9f6-f447-412f-8923-11b1ba896e95,<hypothesis-testing><forecasting>,edited tags,2010-09-23 16:05:25.783
30738,10911,22.0,1,,CC BY-SA 3.0,c0dec5b6-cdb4-4ac3-99c2-397b1aef27d2,How to calculate the confidence interval of the mean of means?,,2011-06-16 16:58:13.537
179773,48658,,25,,,c9633158-4ccd-4218-8441-96ee96c3e23e,,http://twitter.com/#!/StackStats/status/380301995867127808,2013-09-18 12:07:14.613
8978,3649,674.0,2,,CC BY-SA 2.5,ecf8eef0-7f18-40b3-b477-449356e0ce6d,"I found that Spearman correlation is mostly used in place of usual linear correlation when working with integer valued scores on a measurement scale, when it has a moderate number of possible scores or when we don't want to make rely on assumptions about the bivariate relationships. As compared to Pearson coefficient, the interpretation of Kendall's tau seems to me less direct than that of Spearman's rho, in the sense that it quantifies the difference between the % of concordant and discordant pairs among all possible pairwise events. In my understanding, Kendall's tau more closely resemble [Goodman-Kruskal Gamma][1].

I just browsed an article from Larry Winner in the J. Statistics Educ. (2006) which discusses the use of both measures, [NASCAR Winston Cup Race Results for 1975-2003][2].

I also found [@onestop][3] answer about [Pearson's or Spearman's correlation with non-normal data][4] interesting in this respect.


  [1]: http://en.wikipedia.org/wiki/Gamma_test_(statistics)
  [2]: http://www.amstat.org/publications/jse/v14n3/datasets.winner.html
  [3]: http://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data/3744#3744
  [4]: http://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data",,2010-10-24 14:26:35.747
8986,3646,,4,user88,CC BY-SA 2.5,e2a8e6de-b9e0-43a1-99b4-65d18f8c9c50,Kendall Tau or Spearman's rho?,edited title,2010-10-24 15:46:10.027
9216,28,,6,user88,CC BY-SA 2.5,59d2fd0b-c0aa-4798-b72a-bdbfae0d45e4,<machine-learning>,edited tags,2010-10-28 10:39:42.043
9334,3649,674.0,5,,CC BY-SA 2.5,32685c81-b163-4348-aa75-1b11ad3f4128,"I found that Spearman correlation is mostly used in place of usual linear correlation when working with integer valued scores on a measurement scale, when it has a moderate number of possible scores or when we don't want to make rely on assumptions about the bivariate relationships. As compared to Pearson coefficient, the interpretation of Kendall's tau seems to me less direct than that of Spearman's rho, in the sense that it quantifies the difference between the % of concordant and discordant pairs among all possible pairwise events. In my understanding, Kendall's tau more closely resembles [Goodman-Kruskal Gamma][1].

I just browsed an article from Larry Winner in the J. Statistics Educ. (2006) which discusses the use of both measures, [NASCAR Winston Cup Race Results for 1975-2003][2].

I also found [@onestop][3] answer about [Pearson's or Spearman's correlation with non-normal data][4] interesting in this respect.

Of note, Kendall's tau (the *a* version) has connection to Somers' D (and Harrell's C) used for predictive modelling (see e.g., [Interpretation of Somers’ D under four simple models][5] by RB Newson and reference [6] therein, and articles by Newson published in the Stata Journal 2006). An overview of rank-sum tests is provided in [Efficient Calculation of Jackknife Confidence Intervals for Rank Statistics][6], that was published in the JSS (2006).


  [1]: http://en.wikipedia.org/wiki/Gamma_test_(statistics)
  [2]: http://www.amstat.org/publications/jse/v14n3/datasets.winner.html
  [3]: http://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data/3744#3744
  [4]: http://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data
  [5]: http://www.imperial.ac.uk/nhli/r.newson/miscdocs/intsomd1.pdf
  [6]: http://www.jstatsoft.org/v15/i01/paper",add a note on kendall's tau and predictive modelling,2010-10-31 10:29:50.693
10524,4187,287.0,1,,CC BY-SA 2.5,a78d2b81-a02b-4460-b6f1-70eb56ff41b6,Statistical Sins,,2010-11-15 18:46:37.113
10523,4187,287.0,2,,CC BY-SA 2.5,a78d2b81-a02b-4460-b6f1-70eb56ff41b6,"I'm a grad student in psychology, and as I pursue more and more independent studies in statistics, I am increasingly amazed by the inadequacy of my formal training. Both personal and second hand experience suggests that the paucity of statistical rigor in undergraduate and graduate training is rather ubiquitous within psychology. As such, I thought it would be useful for independent learners like myself to create a list of ""Statistical Sins"", tabulating statistical practices taught to grad students as standard practice that are in fact either superseded by superior (more powerful, or flexible, or robust, etc) modern methods or shown to be frankly invalid. Anticipating that other fields might also experience a similar state of affairs, I propose a community wiki where we can collect a list of Statistical Sins across disciplines. Please submit one ""sin"" per answer.",,2010-11-15 18:46:37.113
10525,4187,287.0,3,,CC BY-SA 2.5,a78d2b81-a02b-4460-b6f1-70eb56ff41b6,<teaching><best-practices>,,2010-11-15 18:46:37.113
10540,4187,,16,user88,,00000000-0000-0000-0000-000000000000,,,2010-11-15 20:28:53.713
10547,4187,,4,user88,CC BY-SA 2.5,9443f777-cbfa-4c91-97ad-14a55f37830e,What are common statistical sins?,edited title,2010-11-15 20:29:19.393
10548,4187,,6,user88,CC BY-SA 2.5,ca45a05e-b875-4679-8b09-79abc81142e9,<best-practices>,edited tags,2010-11-15 20:29:26.957
11966,4705,1209.0,3,,CC BY-SA 2.5,0998ef5d-278a-43dd-bada-15885d12473c,<methodology><analysis>,,2010-12-04 00:08:23.027
11965,4705,1209.0,1,,CC BY-SA 2.5,0998ef5d-278a-43dd-bada-15885d12473c,Most famous statisticians,,2010-12-04 00:08:23.027
11967,4705,1209.0,2,,CC BY-SA 2.5,0998ef5d-278a-43dd-bada-15885d12473c,What are the most important statisticians and what is it that made them famous? (Reply just one scientist per answer please),,2010-12-04 00:08:23.027
11969,4705,,16,user88,,00000000-0000-0000-0000-000000000000,,,2010-12-04 00:12:46.677
11971,4705,,6,user88,CC BY-SA 2.5,f4b20939-9bad-402d-8814-94b8a172c23f,<history>,edited tags; added 3 characters in body,2010-12-04 00:13:35.163
11970,4705,,5,user88,CC BY-SA 2.5,f4b20939-9bad-402d-8814-94b8a172c23f,"What are the most important statisticians and what is it that made them famous?  
(Reply just one scientist per answer please)",edited tags; added 3 characters in body,2010-12-04 00:13:35.163
11977,4705,1209.0,9,,CC BY-SA 2.5,f07672a7-e6c0-40c4-abf9-d21990165d5c,<methodology><analysis>,Rollback to [0998ef5d-278a-43dd-bada-15885d12473c],2010-12-04 00:50:06.453
11976,4705,1209.0,8,,CC BY-SA 2.5,f07672a7-e6c0-40c4-abf9-d21990165d5c,What are the most important statisticians and what is it that made them famous? (Reply just one scientist per answer please),Rollback to [0998ef5d-278a-43dd-bada-15885d12473c],2010-12-04 00:50:06.453
11978,4705,1209.0,6,,CC BY-SA 2.5,a1721099-9036-4699-842d-268642f2cfb9,<methodology><history><analysis>,edited tags,2010-12-04 00:58:41.857
11984,4705,,5,user1108,CC BY-SA 2.5,c6e5a4e5-17d0-4936-9782-879a8565bce9,Who are the most important statisticians and what is it that made them famous? (Reply just one scientist per answer please),"changed ""what"" to ""who""",2010-12-04 02:46:25.963
11995,4714,60.0,2,,CC BY-SA 2.5,3352fdd3-59c5-4e30-a8f7-eb83f1b625b7,[Thomas Bayes](http://en.wikipedia.org/wiki/Thomas_Bayes) for inventing Bayes' theorem,,2010-12-04 03:46:20.583
11996,4714,,16,,,a55c79d7-bc33-46d8-ab20-7fd2745692be,,,2010-12-04 03:46:20.583
12727,4705,,6,user88,CC BY-SA 2.5,584325bc-be4a-41e9-9610-e18fab7f9329,<history>,edited tags,2010-12-13 18:16:21.700
12749,4705,,6,,CC BY-SA 2.5,85d40d6e-6022-4e93-a149-983b0a77aae8,<big-list><history>,edited tags,2010-12-13 23:12:56.030
12757,5015,1542.0,1,,CC BY-SA 2.5,92add026-16b7-4c9c-8f33-8c20789f9710,regression: the interaction wipes out my direct effects...,,2010-12-13 23:43:17.117
12758,5015,1542.0,3,,CC BY-SA 2.5,92add026-16b7-4c9c-8f33-8c20789f9710,<regression><interaction>,,2010-12-13 23:43:17.117
12756,5015,1542.0,2,,CC BY-SA 2.5,92add026-16b7-4c9c-8f33-8c20789f9710,"In a regression, the interaction term wipes out both related direct effects? Do I drop the interaction or report the outcome? The interaction was not part of the original hypothesis. ",,2010-12-13 23:43:17.117
12768,5020,1411.0,2,,CC BY-SA 2.5,25f11557-8533-4ac3-9e06-02f74a63b3d4,"I think this one is tricky; as you hint, there's 'moral hazard' here: if you hadn't looked at the interaction at all, you'd be free and clear, but now that you have there is a suspicion of data-dredging if you drop it.

The key is *probably* a change in the meaning of your effects when you go from the main-effects-only to the interaction model. What you get for the 'main effects' depends very much on how your treatments and contrasts are coded. In R, the default is treatment contrasts with the first factor levels (the ones with the first names in alphabetical order unless you have gone out of your way to code them differently) as the baseline levels.

Say (for simplicity) that you have two levels, 'control' and 'trt', for each factor. Without the interaction, the meaning of the 'v1.trt' parameter (assuming treatment contrasts as is the default in R) is ""average difference between 'v1.control' and 'v1.trt' group""; the meaning of the 'v2.trt' parameter is ""average difference between 'v2.control' and 'v2.trt'"".

With the interaction, 'v1.trt' is the average difference between 'v1.control' and 'v1.trt' **in the 'v2.control' group**, and similarly 'v2.trt' is the average difference between v2 groups in the 'v1.control' group. Thus, if you have fairly small treatment effects in each of the control groups, but a large effect in the treatment groups, you could easily see what you're seeing.

The only way I can see this happening *without* a significant interaction term, however, is if all the effects are fairly weak (so that what you really mean by ""the effect disappeared"" is that you went from p=0.06 to p=0.04, across the magic significance line).

Another possibility is that you are 'using up too many degrees of freedom' -- that is, the parameter estimates don't actually change that much, but the residual error term is sufficiently inflated by having to estimate another 4 [ = (2-1)*(5-1)] parameters that your significant terms become non-significant. Again, I would only expect this with a small data set/relatively weak effects.

One possible solution is to move to sum contrasts, although this is also delicate -- you have to be convinced that 'average effect' is meaningful in your case. The very best thing is to plot your data and to look at the coefficients and understand what's happening in terms of the estimated parameters.

Hope that helps.",,2010-12-14 02:25:20.237
12797,5015,674.0,5,,CC BY-SA 2.5,7da4b295-54b3-4e2a-9d0b-24a0eed66e21,"In a regression, the interaction term wipes out both related direct effects. Do I drop the interaction or report the outcome? The interaction was not part of the original hypothesis. ",edited body; edited title,2010-12-14 08:35:11.153
12796,5015,674.0,4,,CC BY-SA 2.5,7da4b295-54b3-4e2a-9d0b-24a0eed66e21,What if interaction wipes out my direct effects in regression?,edited body; edited title,2010-12-14 08:35:11.153
13201,1760,190.0,6,,CC BY-SA 2.5,cf0f7035-f62d-44a0-ad2d-65acc7414a4d,<hypothesis-testing><forecasting>,edited tags,2010-12-20 07:28:59.943
16377,4187,,38,user88,,6887d756-76f7-4279-bd02-adccbc90ac17,"[{""Id"":88,""DisplayName"":""mbq""}]",from http://stats.stackexchange.com/questions/6880/what-are-the-most-dangerous-concepts-in-the-practice-of-data-analysis,2011-02-06 11:01:16.173
17674,6788,1790.0,2,,CC BY-SA 2.5,4a47f257-7908-4b94-be00-8058328c39fb,"I recently stumbled upon the concept of **sample complexity**, and was wondering if there are any texts, papers or tutorials that provide:

 1. A good introduction to the concept
 2. An analysis of the sample complexity of different classification methods or kernel machines.
 3. Advice or information on how to measure it in practice.

Any help with the topic would be greatly appreciated.


",,2011-02-19 22:41:23.000
17676,6788,1790.0,3,,CC BY-SA 2.5,4a47f257-7908-4b94-be00-8058328c39fb,<machine-learning>,,2011-02-19 22:41:23.000
17675,6788,1790.0,1,,CC BY-SA 2.5,4a47f257-7908-4b94-be00-8058328c39fb,Measuring and analyzing sample complexity,,2011-02-19 22:41:23.000
17697,6788,,25,,,5754225a-24bc-43dc-94fb-11f46b56aa28,,http://twitter.com/#!/StackStats/status/39238529569652736,2011-02-20 08:22:49.150
17724,6788,1790.0,5,,CC BY-SA 2.5,a2ac4c34-4688-43ed-b4e6-224c50c69736,"I recently stumbled upon the concept of [**sample complexity**][1], and was wondering if there are any texts, papers or tutorials that provide:

 1. A good introduction to the concept
 2. An analysis of the sample complexity of different classification methods or kernel machines.
 3. Advice or information on how to measure it in practice.

Any help with the topic would be greatly appreciated.


  [1]: http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=%22sample%20complexity%22",added 93 characters in body,2011-02-20 17:52:31.897
17729,6788,1790.0,5,,CC BY-SA 2.5,4ef8d6b5-e493-4e90-8c13-694a1956fdc2,"I recently stumbled upon the concept of [**sample complexity**][1], and was wondering if there are any texts, papers or tutorials that provide:

 1. A good introduction to the concept
 2. An analysis of the sample complexity of different classification methods or kernel methods.
 3. Advice or information on how to measure it in practice.

Any help with the topic would be greatly appreciated.


  [1]: http://www.google.com/search?q=%22sample%20complexity%22",deleted 25 characters in body; deleted 1 characters in body,2011-02-20 19:06:47.563
21473,7965,1691.0,3,,CC BY-SA 2.5,e9e8566c-becc-467f-ad03-079813adc239,<r><clustering>,,2011-03-24 14:51:27.800
21472,7965,1691.0,1,,CC BY-SA 2.5,e9e8566c-becc-467f-ad03-079813adc239,Colinearity and scaling when using kmeans,,2011-03-24 14:51:27.800
21471,7965,1691.0,2,,CC BY-SA 2.5,e9e8566c-becc-467f-ad03-079813adc239,"I'm trying to gain a better understanding of kmeans clustering and am still unclear about colinearity and scaling of data. To explore colinearity, I made a plot of all five variables that I am considering shown in the figure below, along with a correlation calculation.
![colinearity][1]

I started off with a larger number of parameters, and excluded any that had a correlation higher than 0.6 (an assumption I made). The five I choose to include are shown in this diagram.

Then, I scaled the date using the `R` function `scale(x)` before applying the `kmeans()` function. However, I'm not sure whether `center = TRUE` and `scale = TRUE` should also be included as I don't understand the differences that these arguments make. (The `scale()` description is given as `scale(x, center = TRUE, scale = TRUE)`).

Is the process that I describe an appropriate way of identifying clusters?



 
  [1]: https://i.stack.imgur.com/W5MZJ.jpg",,2011-03-24 14:51:27.800
21482,7965,,4,user88,CC BY-SA 2.5,7c37b9c4-a16d-4065-82f3-a3ddc8437d2e,Colinearity and scaling when using k-means,edited title,2011-03-24 16:12:33.673
21484,7965,,25,,,f582b0ff-7e07-4a71-baa2-988b09484c82,,http://twitter.com/#!/StackStats/status/50971048736329728,2011-03-24 17:23:39.727
23138,8529,793.0,2,,CC BY-SA 3.0,d059ff9f-0d0d-425d-a518-c3b37da95746,"What are some good papers describing *applications* of statistics that would be fun and informative to read? Just to be clear, I'm not really looking for papers describing new statistical methods (e.g., a paper on least angle regression), but rather papers describing how to solve real-world problems.

For example, one paper that would fit what I'm looking is the climate paper from the second Cross-Validated Journal Club. I'm kind of looking for more statistics-ish papers, rather than machine learning papers, but I guess it's kind of a fuzzy distinction (I'd classify the Netflix Prize papers as a bit borderline, and a paper on sentiment analysis as something I'm *not* looking for).

I'm asking because most of the applications of statistics I've seen are either the little snippets you seen in textbooks, or things related to my own work, so I'd like to branch out a bit.",,2011-04-08 19:01:11.850
23137,8529,793.0,1,,CC BY-SA 3.0,d059ff9f-0d0d-425d-a518-c3b37da95746,What are some interesting and well-written *applied* statistics papers to read?,,2011-04-08 19:01:11.850
23139,8529,793.0,3,,CC BY-SA 3.0,d059ff9f-0d0d-425d-a518-c3b37da95746,<application>,,2011-04-08 19:01:11.850
23140,8529,668.0,16,,,ac32a436-a0b4-4ad8-bc04-d15a5937bcb1,,,2011-04-08 19:04:21.230
23141,8529,793.0,4,,CC BY-SA 3.0,78024e68-fd28-435a-8c4e-d5171457e657,What are some interesting and well-written *applied* statistics papers?,edited title,2011-04-08 19:10:58.470
23236,8529,674.0,5,,CC BY-SA 3.0,582ad9c9-14e6-459d-9848-43f193916195,"What are some good papers describing *applications* of statistics that would be fun and informative to read? Just to be clear, I'm not really looking for papers describing new statistical methods (e.g., a paper on least angle regression), but rather papers describing how to solve real-world problems.

For example, one paper that would fit what I'm looking is the climate paper from the [second Cross-Validated Journal Club][1]. I'm kind of looking for more statistics-ish papers, rather than machine learning papers, but I guess it's kind of a fuzzy distinction (I'd classify the Netflix Prize papers as a bit borderline, and a paper on sentiment analysis as something I'm *not* looking for).

I'm asking because most of the applications of statistics I've seen are either the little snippets you seen in textbooks, or things related to my own work, so I'd like to branch out a bit.


  [1]: http://meta.stats.stackexchange.com/questions/685/second-cross-validated-journal-club",add tag + link to CVJC paper,2011-04-09 18:54:46.130
23707,8681,1040.0,3,,CC BY-SA 3.0,38e5df04-e0a7-429c-a6fc-5bbd8c1e1979,<dataset>,,2011-04-14 01:33:55.987
23706,8681,1040.0,1,,CC BY-SA 3.0,38e5df04-e0a7-429c-a6fc-5bbd8c1e1979,Where can I find good publicly available data that I could use to teach z-scores to my college students?,,2011-04-14 01:33:55.987
23705,8681,1040.0,2,,CC BY-SA 3.0,38e5df04-e0a7-429c-a6fc-5bbd8c1e1979,I am sick of using the examples in the book. Is there an easy place to find data for which z-score/percentile/normal distribution stuff would be easy to see?,,2011-04-14 01:33:55.987
23711,8681,,25,,,89f82674-de81-4977-91aa-a6b75f07e493,,http://twitter.com/#!/StackStats/status/58366156850999296,2011-04-14 03:09:10.617
23758,8699,155.0,2,,CC BY-SA 3.0,a969388e-10a0-4c40-b68e-4762e25de01f,"You may wish to read answers to this existing question on [freely available datasets](http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples).

In general, I imagine that you'd want a dataset with some interesting metric variables.
In psychology research methods classes that I've taught, we've often looked at datasets with intelligence or personality test scores.

If you want a personality example, I have some [personality data and metadata on github](https://github.com/jeromyanglim/Sweave_Personality_Reports) based on the [IPIP](http://ipip.ori.org/), an public domain measure of the Big 5 factors of personality.

* [github repository home](https://github.com/jeromyanglim/Sweave_Personality_Reports)
* [data](https://github.com/jeromyanglim/Sweave_Personality_Reports/blob/master/data/ipip.tsv)
* [metadata](https://github.com/jeromyanglim/Sweave_Personality_Reports/blob/master/meta/ipipmeta.tsv)
* [David Smith's summary](http://blog.revolutionanalytics.com/2010/12/how-to-create-pdf-reports-with-r.html)",,2011-04-14 10:55:13.367
26020,412,1693.0,4,,CC BY-SA 3.0,cde75dd3-4f25-4c36-b8b6-b15e4b15091a,What book would you recommend for non-statistician scientists?,That 1 word seemed necessary.,2011-05-04 18:19:29.047
26420,9524,2872.0,1,,CC BY-SA 3.0,b70c32e3-f269-4e7e-8fdb-8fb77aa0fa6d,"Any good movies with maths, probabilities etc?",,2011-05-07 11:13:51.243
26419,9524,2872.0,2,,CC BY-SA 3.0,b70c32e3-f269-4e7e-8fdb-8fb77aa0fa6d,"Can you suggest some good movies which involve math, probabilities etc? One example is [21][1]. I would also be interested in movies that involve algorithms (e.g. text decryption). In general ""geeky"" movies with famous scientific theories but no science fiction or documentaries. Thanks in advance!


  [1]: http://en.wikipedia.org/wiki/21_%282008_film%29",,2011-05-07 11:13:51.243
26421,9524,2872.0,3,,CC BY-SA 3.0,b70c32e3-f269-4e7e-8fdb-8fb77aa0fa6d,<probability>,,2011-05-07 11:13:51.243
26426,9524,,25,,,78ac57da-6346-4ccb-9f31-ed7f30c5100a,,http://twitter.com/#!/StackStats/status/66837557815676928,2011-05-07 12:11:30.140
26427,9524,674.0,16,,,101c07d2-4565-4357-abbe-0b1d27f2aced,,,2011-05-07 12:32:39.210
26429,9524,674.0,4,,CC BY-SA 3.0,db37784d-623d-4729-9123-68d777ee8d93,Are there any good movies involving mathematics or probability?,edited tags; edited title,2011-05-07 12:34:34.433
26428,9524,674.0,6,,CC BY-SA 3.0,db37784d-623d-4729-9123-68d777ee8d93,<probability><references>,edited tags; edited title,2011-05-07 12:34:34.433
26440,9529,192.0,16,,,0a4c4ecc-5f60-49ec-b06b-60958c546b3b,,,2011-05-07 14:19:42.887
26439,9529,192.0,2,,CC BY-SA 3.0,1b2c43cf-4004-4f05-892c-6916f9394665,"[Pi][1]


  [1]: http://www.imdb.com/title/tt0138704/",,2011-05-07 14:19:42.887
27801,10008,1506.0,3,,CC BY-SA 3.0,a636a9f6-56e9-4a4a-b514-bbbdc8628926,<modeling><interaction>,,2011-05-20 01:19:45.107
27799,10008,1506.0,2,,CC BY-SA 3.0,a636a9f6-56e9-4a4a-b514-bbbdc8628926,"Is it ever valid to include a two-way interaction in a model without including the main effects?  What if your hypothesis is only about the interaction, do you still need to include the main effects?",,2011-05-20 01:19:45.107
27800,10008,1506.0,1,,CC BY-SA 3.0,a636a9f6-56e9-4a4a-b514-bbbdc8628926,Including the Interaction but not the Main Effects in a Model,,2011-05-20 01:19:45.107
27808,10008,,25,,,e98ca262-ea36-4578-bfbb-edc1a72adb28,,http://twitter.com/#!/StackStats/status/71412951185235969,2011-05-20 03:12:29.113
27839,10008,,4,user88,CC BY-SA 3.0,6ce9f82e-fcb2-45dc-bfe0-16d6be1bb924,Including the interaction but not the main effects in a model,edited title,2011-05-20 09:20:21.180
27970,10069,2666.0,2,,CC BY-SA 3.0,a1336a00-51f7-4503-bea7-c598c55dae2e,"In my experience, not only is it necessary to have all lower order effects in the model when they are connected to higher order effects, but it is also important to properly model (e.g., allowing to be nonlinear) main effects that are seemingly unrelated to the factors in the interactions of interest.  That's because interactions between x1 and x2 can be stand-ins for main effects of x3 and x4.  I.e. sometimes *seem* to be needed because they are collinear with omitted variables or omitted nonlinear (e.g., spline) terms.",,2011-05-21 12:31:20.447
29493,10541,2690.0,2,,CC BY-SA 3.0,6044637d-d46a-4f80-8894-da665d912571,"Does any know the reference/link where i can find the matlab implementation of gap statistics for clustering as mentioned in [this][1] paper.


  [1]: http://gremlin1.gdcb.iastate.edu/MIP/gene/MicroarrayData/gapstatistics.pdf",,2011-06-05 05:32:14.513
29494,10541,2690.0,1,,CC BY-SA 3.0,6044637d-d46a-4f80-8894-da665d912571,Gap Statistics matlab Implementation,,2011-06-05 05:32:14.513
29495,10541,2690.0,3,,CC BY-SA 3.0,6044637d-d46a-4f80-8894-da665d912571,<clustering><matlab><mathematical-statistics>,,2011-06-05 05:32:14.513
29496,10541,,25,,,b96194d6-17ac-4792-8c40-8c864fccbd98,,http://twitter.com/#!/StackStats/status/77256861539250176,2011-06-05 06:14:06.167
29521,10541,,4,user88,CC BY-SA 3.0,a809b720-a5d8-45d7-969f-9988df693472,Gap statistics MATLAB implementation,edited body; edited title,2011-06-05 12:11:37.537
29522,10541,,5,user88,CC BY-SA 3.0,a809b720-a5d8-45d7-969f-9988df693472,"Does any know the reference/link where i can find the MATLAB implementation of gap statistics for clustering as mentioned in [this][1] paper?


  [1]: http://gremlin1.gdcb.iastate.edu/MIP/gene/MicroarrayData/gapstatistics.pdf",edited body; edited title,2011-06-05 12:11:37.537
30737,10911,22.0,3,,CC BY-SA 3.0,c0dec5b6-cdb4-4ac3-99c2-397b1aef27d2,<multilevel-analysis><confidence-intervals>,,2011-06-16 16:58:13.537
30739,10911,22.0,2,,CC BY-SA 3.0,c0dec5b6-cdb4-4ac3-99c2-397b1aef27d2,"Imagine that you repeat an experiment three times. In each experiment, you collect triplicate measurements. The triplicates tend to be fairly close together, compared to the differences among the three experimental means. Computing the grand mean is pretty easy. But how can one compute a confidence interval for the grand mean?

Sample data:

Experiment 1:	34,	41,	39

Experiment 2:	45,	51,	52

Experiment 3:	29,	31,	35

Assume that the replicate values within an experiment follow a Gaussian distribution, as does the mean values of each experiment. The SD of variation within an experiment is smaller than the SD among experimental means.

The simple approach is to first compute the mean of each experiment: 38.0, 49.3, and 31.7, and then compute the mean, and its 95% confidence interval, of those three values. Using this method, the grand mean is 39.7 with the 95% confidence interval ranging from 17.4 to 61.9. 

The problem with that approach is that it totally ignores the variation among triplicates. I wonder if there isn't a good way to account for that variation. ",,2011-06-16 16:58:13.537
31121,10911,22.0,5,,CC BY-SA 3.0,a1c18211-9322-4c90-93ae-9b2e53bdb965,"Imagine that you repeat an experiment three times. In each experiment, you collect triplicate measurements. The triplicates tend to be fairly close together, compared to the differences among the three experimental means. Computing the grand mean is pretty easy. But how can one compute a confidence interval for the grand mean?

Sample data:

Experiment 1:	34,	41,	39

Experiment 2:	45,	51,	52

Experiment 3:	29,	31,	35

Assume that the replicate values within an experiment follow a Gaussian distribution, as does the mean values of each experiment. The SD of variation within an experiment is smaller than the SD among experimental means. Assume also that there is no ordering of the three values in each experiment. The left-to-right order of the three values in each row is entirely arbitrary.

The simple approach is to first compute the mean of each experiment: 38.0, 49.3, and 31.7, and then compute the mean, and its 95% confidence interval, of those three values. Using this method, the grand mean is 39.7 with the 95% confidence interval ranging from 17.4 to 61.9. 

The problem with that approach is that it totally ignores the variation among triplicates. I wonder if there isn't a good way to account for that variation. ",Explained that the left-to-right order of the replicates in each row is arbitrary.,2011-06-20 15:10:13.723
33635,412,668.0,38,,,275be647-00fe-46a4-811b-2844fbf9f5dc,"[{""Id"":919,""DisplayName"":""whuber""}]",from http://stats.stackexchange.com/questions/12964/introductory-materials-in-statistical-analysis-and-data-visualisation,2011-07-13 18:53:43.480
34460,10911,668.0,6,,CC BY-SA 3.0,e3f36c61-5f49-455b-a259-c476f4660b60,<confidence-interval><multilevel-analysis>,edited tags,2011-07-19 21:18:45.377
36699,4714,60.0,5,,CC BY-SA 3.0,5b5afa53-4981-4e4a-a881-ab648c8234be,Rev. [Thomas Bayes](http://en.wikipedia.org/wiki/Thomas_Bayes) for discovering Bayes' theorem,added 7 characters in body,2011-08-09 17:16:30.820
37746,13058,1124.0,2,,CC BY-SA 3.0,0f962e58-a484-4f48-9a63-a6041886afd2,"Anybody have any experience with software (preferably free, preferably open source) that will take an image of data plotted on cartesian coordinates (a standard, everyday plot) and extract the coordinates of the points plotted on the graph?

Essentially, this is a data-mining problem and a **reverse** data-visualization problem.",,2011-08-18 04:14:22.583
37744,13058,1124.0,3,,CC BY-SA 3.0,0f962e58-a484-4f48-9a63-a6041886afd2,<data-visualization><data-mining><software>,,2011-08-18 04:14:22.583
37745,13058,1124.0,1,,CC BY-SA 3.0,0f962e58-a484-4f48-9a63-a6041886afd2,Software needed to scrape data from graph,,2011-08-18 04:14:22.583
37748,13060,1805.0,2,,CC BY-SA 3.0,900d659c-78e5-4ef0-bcdc-cc38d92cd8dd,"Check out the [digitize][1] package for [R][2].  Its designed to solve exactly this sort of problem.


  [1]: http://cran.r-project.org/web/packages/digitize/index.html
  [2]: http://cran.r-project.org/",,2011-08-18 05:14:07.900
37752,13058,,25,,,cd66f7ee-f31b-4427-b17d-bd6ecb46d669,,http://twitter.com/#!/StackStats/status/104079744433270784,2011-08-18 06:38:39.437
39549,13631,4221.0,1,,CC BY-SA 3.0,a67f7563-8add-48b4-98c6-aa3af25404e7,Forecasting binary time series,,2011-09-01 14:56:28.933
39547,13631,4221.0,3,,CC BY-SA 3.0,a67f7563-8add-48b4-98c6-aa3af25404e7,<forecasting><binary-data>,,2011-09-01 14:56:28.933
39548,13631,4221.0,2,,CC BY-SA 3.0,a67f7563-8add-48b4-98c6-aa3af25404e7,"I have a binary time series with 1 when the car is not moving, and 0 when the car is moving. I want to make a forecast for a time horizon up to 36 hours ahead and for each hour. 

My first approach was to use a Naive Bayes using the following inputs: t-24 (daily seasonal), t-48 (weekly seasonal), hour of the day. However, the results are not very good.

Which articles or software do you recommend for this problem?

Thanks,
Ricardo Bessa",,2011-09-01 14:56:28.933
39550,13631,668.0,6,,CC BY-SA 3.0,db02cef5-5db9-4265-836b-d90858e51908,<time-series><forecasting><binary-data>,edited tags,2011-09-01 15:05:15.883
39554,13631,,25,,,23f2eb64-be55-40c1-b49c-2315201d8b97,,http://twitter.com/#!/StackStats/status/109289232429883392,2011-09-01 15:39:17.967
39697,13631,668.0,6,,CC BY-SA 3.0,ef643b3b-2265-4631-99e4-24acd48794ff,<r><time-series><forecasting><binary-data>,deleted 26 characters in body; edited tags,2011-09-02 18:54:00.667
39698,13631,668.0,5,,CC BY-SA 3.0,ef643b3b-2265-4631-99e4-24acd48794ff,"I have a binary time series with 1 when the car is not moving, and 0 when the car is moving. I want to make a forecast for a time horizon up to 36 hours ahead and for each hour. 

My first approach was to use a Naive Bayes using the following inputs: t-24 (daily seasonal), t-48 (weekly seasonal), hour of the day. However, the results are not very good.

Which articles or software do you recommend for this problem?",deleted 26 characters in body; edited tags,2011-09-02 18:54:00.667
43119,14729,5898.0,3,,CC BY-SA 3.0,b4530a21-db63-4663-ad43-8b417207fe2f,<r><linear-model><matrix-decomposition><matrix-inverse>,,2011-10-01 17:46:37.323
43118,14729,5898.0,1,,CC BY-SA 3.0,b4530a21-db63-4663-ad43-8b417207fe2f,testing for linear dependence among the columns of a matrix,,2011-10-01 17:46:37.323
43120,14729,5898.0,2,,CC BY-SA 3.0,b4530a21-db63-4663-ad43-8b417207fe2f,"I have a correlation matrix of security returns whose determinant is zero. (This is a bit surprising since the sample correlation matrix and the corresponding covariance matrix should theoretically be positive semi-definite.)

My hypothesis is that at least one security is linearly dependent on other securities. Is there a function in R that sequentially tests each column a matrix for linear dependence?

For example, one approach would be to build up a correlation matrix one security at a time and calculate the determinant at each step. When the determinant = 0 then stop as you have identified the security who is a linear combination of other securities.

Any other techniques to identify linear dependence in such a matrix are appreciated.",,2011-10-01 17:46:37.323
179775,55576,21833.0,1,,CC BY-SA 3.0,aaf7aaeb-ac21-4d4d-b4be-c8c1715d8b8d,Generating random numbers based on partial correlation data,,2013-09-18 12:09:15.653
179776,55576,21833.0,3,,CC BY-SA 3.0,aaf7aaeb-ac21-4d4d-b4be-c8c1715d8b8d,<correlation><matlab><random-generation><partial-correlation>,,2013-09-18 12:09:15.653
43122,14729,5898.0,5,,CC BY-SA 3.0,67de931f-9e3f-43e6-a759-f35741eb39d5,"I have a correlation matrix of security returns whose determinant is zero. (This is a bit surprising since the sample correlation matrix and the corresponding covariance matrix should theoretically be positive definite.)

My hypothesis is that at least one security is linearly dependent on other securities. Is there a function in R that sequentially tests each column a matrix for linear dependence?

For example, one approach would be to build up a correlation matrix one security at a time and calculate the determinant at each step. When the determinant = 0 then stop as you have identified the security who is a linear combination of other securities.

Any other techniques to identify linear dependence in such a matrix are appreciated.",deleted 5 characters in body,2011-10-01 17:54:45.020
43138,14729,674.0,4,,CC BY-SA 3.0,b9f630bd-8fd3-43be-be43-353b98b2b8c2,Testing for linear dependence among the columns of a matrix,edited title,2011-10-01 19:39:48.100
43333,14790,2081.0,2,,CC BY-SA 3.0,9d1571c5-e38f-43bd-b1d4-75ee38f08767,"You seem to ask a really provoking question: how to detect, given a singular correlation (or covariance, or sum-of-squares-and-cross-product) matrix, which column is linearly dependent on which. I tentatively suppose that **sweep operation** could help. Here is my probe in SPSS (not R) to illustrate.

Let's generate some data:

            v1        v2        v3         v4          v5
        -1.64454	.35119	 -.06384	-1.05188	 .25192
        -1.78520   -.21598	 1.20315	  .40267	1.14790
         1.36357   -.96107	 -.46651	  .92889   -1.38072
         -.31455   -.74937	 1.17505	 1.27623   -1.04640
         -.31795	.85860	  .10061	  .00145	 .39644
         -.97010	.19129	 2.43890	 -.83642	-.13250
         -.66439	.29267	 1.20405	  .90068   -1.78066
          .87025   -.89018	 -.99386	-1.80001	 .42768
        -1.96219   -.27535	  .58754	  .34556	 .12587
        -1.03638   -.24645	 -.11083	  .07013	-.84446

Let's create some linear dependancy between V2, V4 and V5:

    compute V4 = .4*V2+1.2*V5.
    execute.

So, we modified our column V4.

    matrix.
    get X. /*take the data*/
    compute M = sscp(dat). /*SSCP matrix, X'X; it is singular*/
    print rank(M). /*with rank 5-1=4, because there's 1 group of interdependent columns*/
    loop i= 1 to 5. /*Start iterative sweep operation on M from column 1 to column 5*/
    -compute M = sweep(M,i).
    -print M. /*That's printout we want to trace*/
    end loop.
    end matrix.

The printouts of M in 5 iterations:

    M
         .06660028    -.12645565    -.54275426    -.19692972    -.12195621
         .12645565    3.20350385    -.08946808    2.84946215    1.30671718
         .54275426    -.08946808    7.38023317   -3.51467361   -2.89907198
         .19692972    2.84946215   -3.51467361   13.88671851   10.62244471
         .12195621    1.30671718   -2.89907198   10.62244471    8.41646486
    
    M
         .07159201     .03947417    -.54628594    -.08444957    -.07037464
         .03947417     .31215820    -.02792819     .88948298     .40790248
         .54628594     .02792819    7.37773449   -3.43509328   -2.86257773
         .08444957    -.88948298   -3.43509328   11.35217042    9.46014202
         .07037464    -.40790248   -2.86257773    9.46014202    7.88345168
    
    M
        .112041875    .041542117    .074045215   -.338801789   -.282334825
        .041542117    .312263922    .003785470    .876479537    .397066281
        .074045215    .003785470    .135542964   -.465602725   -.388002270
        .338801789   -.876479537    .465602725   9.752781632   8.127318027
        .282334825   -.397066281    .388002270   8.127318027   6.772765022
    
    M
       .1238115070   .0110941027   .0902197842   .0347389906   .0000000000
       .0110941027   .3910328733  -.0380581058  -.0898696977  -.3333333333
       .0902197842  -.0380581058   .1577710733   .0477405054   .0000000000
       .0347389906  -.0898696977   .0477405054   .1025348498   .8333333333
       .0000000000   .3333333333   .0000000000  -.8333333333   .0000000000
    
    M
       .1238115070   .0110941027   .0902197842   .0347389906   .0000000000
       .0110941027   .3910328733  -.0380581058  -.0898696977   .0000000000
       .0902197842  -.0380581058   .1577710733   .0477405054   .0000000000
       .0347389906  -.0898696977   .0477405054   .1025348498   .0000000000
       .0000000000   .0000000000   .0000000000   .0000000000   .0000000000

Notice that eventually column 5 got full of zeros. This means (as I understand it) that V5 is linearly tied with some of *preceeding* columns. Which columns? Look at iteration where column 5 is last not full of zeroes - iteration 4. We see there that V5 is tied with V2 and V4 with coefficients -.3333 and .8333: V5 = -.3333*V2+.8333*V4, which corresponds to what we've done with the data: V4 = .4*V2+1.2*V5.

That's how we knew which column is linearly tied with which other. I didn't check how helpful is the above approach in more general case with many groups of interdependancies in the data. In the above example it appeared helpful, though.",,2011-10-03 09:34:57.197
43335,14790,2081.0,5,,CC BY-SA 3.0,f91a351c-14c3-4445-bf1c-1c2ced8329a6,"You seem to ask a really provoking question: how to detect, given a singular correlation (or covariance, or sum-of-squares-and-cross-product) matrix, which column is linearly dependent on which. I tentatively suppose that **sweep operation** could help. Here is my probe in SPSS (not R) to illustrate.

Let's generate some data:

            v1        v2        v3         v4          v5
        -1.64454	.35119	 -.06384	-1.05188	 .25192
        -1.78520   -.21598	 1.20315	  .40267	1.14790
         1.36357   -.96107	 -.46651	  .92889   -1.38072
         -.31455   -.74937	 1.17505	 1.27623   -1.04640
         -.31795	.85860	  .10061	  .00145	 .39644
         -.97010	.19129	 2.43890	 -.83642	-.13250
         -.66439	.29267	 1.20405	  .90068   -1.78066
          .87025   -.89018	 -.99386	-1.80001	 .42768
        -1.96219   -.27535	  .58754	  .34556	 .12587
        -1.03638   -.24645	 -.11083	  .07013	-.84446

Let's create some linear dependancy between V2, V4 and V5:

    compute V4 = .4*V2+1.2*V5.
    execute.

So, we modified our column V4.

    matrix.
    get X. /*take the data*/
    compute M = sscp(X). /*SSCP matrix, X'X; it is singular*/
    print rank(M). /*with rank 5-1=4, because there's 1 group of interdependent columns*/
    loop i= 1 to 5. /*Start iterative sweep operation on M from column 1 to column 5*/
    -compute M = sweep(M,i).
    -print M. /*That's printout we want to trace*/
    end loop.
    end matrix.

The printouts of M in 5 iterations:

    M
         .06660028    -.12645565    -.54275426    -.19692972    -.12195621
         .12645565    3.20350385    -.08946808    2.84946215    1.30671718
         .54275426    -.08946808    7.38023317   -3.51467361   -2.89907198
         .19692972    2.84946215   -3.51467361   13.88671851   10.62244471
         .12195621    1.30671718   -2.89907198   10.62244471    8.41646486
    
    M
         .07159201     .03947417    -.54628594    -.08444957    -.07037464
         .03947417     .31215820    -.02792819     .88948298     .40790248
         .54628594     .02792819    7.37773449   -3.43509328   -2.86257773
         .08444957    -.88948298   -3.43509328   11.35217042    9.46014202
         .07037464    -.40790248   -2.86257773    9.46014202    7.88345168
    
    M
        .112041875    .041542117    .074045215   -.338801789   -.282334825
        .041542117    .312263922    .003785470    .876479537    .397066281
        .074045215    .003785470    .135542964   -.465602725   -.388002270
        .338801789   -.876479537    .465602725   9.752781632   8.127318027
        .282334825   -.397066281    .388002270   8.127318027   6.772765022
    
    M
       .1238115070   .0110941027   .0902197842   .0347389906   .0000000000
       .0110941027   .3910328733  -.0380581058  -.0898696977  -.3333333333
       .0902197842  -.0380581058   .1577710733   .0477405054   .0000000000
       .0347389906  -.0898696977   .0477405054   .1025348498   .8333333333
       .0000000000   .3333333333   .0000000000  -.8333333333   .0000000000
    
    M
       .1238115070   .0110941027   .0902197842   .0347389906   .0000000000
       .0110941027   .3910328733  -.0380581058  -.0898696977   .0000000000
       .0902197842  -.0380581058   .1577710733   .0477405054   .0000000000
       .0347389906  -.0898696977   .0477405054   .1025348498   .0000000000
       .0000000000   .0000000000   .0000000000   .0000000000   .0000000000

Notice that eventually column 5 got full of zeros. This means (as I understand it) that V5 is linearly tied with some of *preceeding* columns. Which columns? Look at iteration where column 5 is last not full of zeroes - iteration 4. We see there that V5 is tied with V2 and V4 with coefficients -.3333 and .8333: V5 = -.3333*V2+.8333*V4, which corresponds to what we've done with the data: V4 = .4*V2+1.2*V5.

That's how we knew which column is linearly tied with which other. I didn't check how helpful is the above approach in more general case with many groups of interdependancies in the data. In the above example it appeared helpful, though.",deleted 2 characters in body,2011-10-03 09:44:41.570
44941,15281,3641.0,3,,CC BY-SA 3.0,308ec5ab-4a6f-4041-bbfe-4a01e09300f0,<change-point><structural-change>,,2011-10-13 11:46:03.773
44939,15281,3641.0,2,,CC BY-SA 3.0,308ec5ab-4a6f-4041-bbfe-4a01e09300f0,"Is there a specific method to detect change points(structural breaks) in a timeseries?(stocks prices)

Thanks",,2011-10-13 11:46:03.773
44940,15281,3641.0,1,,CC BY-SA 3.0,308ec5ab-4a6f-4041-bbfe-4a01e09300f0,How to detect change(shift) in a timeseries,,2011-10-13 11:46:03.773
44975,15281,668.0,6,,CC BY-SA 3.0,492894dc-a353-47ec-943c-4cafd3a9248f,<time-series><change-point><structural-change>,edited tags,2011-10-13 17:35:25.130
45075,15281,3641.0,4,,CC BY-SA 3.0,e84cc2a1-4c13-4b3b-9cb6-e53e712672c0,How to detect structural change in a timeseries,edited title,2011-10-14 11:36:29.290
45752,15542,4911.0,3,,CC BY-SA 3.0,ff1de0f7-ac9e-4895-b7ee-071f6b935b5a,<machine-learning><clustering><bioinformatics>,,2011-10-18 21:24:39.543
45751,15542,4911.0,1,,CC BY-SA 3.0,ff1de0f7-ac9e-4895-b7ee-071f6b935b5a,"What are the ""hot algorithms"" for machine learning?",,2011-10-18 21:24:39.543
45750,15542,4911.0,2,,CC BY-SA 3.0,ff1de0f7-ac9e-4895-b7ee-071f6b935b5a,"this is a naive question from someone starting to learn machine learning. I'm reading these days the book ""Machine Learning: An algorithmic perspective"" from Marsland. I find it useful as an introductory book, but now I would like to go into advanced algorithms, those that are currently giving the best results. I'm mostly interested in bioinformatics: clustering of biological networks and finding patterns in biological sequences, particularly applied to single nucleotide polymorphism (SNP) analysis. Could you recommend me some reivews or books to read?

Many thanks!
",,2011-10-18 21:24:39.543
45776,15542,,25,,,580c95ca-cc39-4224-a465-f9a877f814f8,,http://twitter.com/#!/StackStats/status/126418768904720385,2011-10-18 22:05:57.933
47868,16209,5196.0,2,,CC BY-SA 3.0,73dde68c-0572-477b-8721-a2746443264a,"How to convert the x below to into a vector like y?

    x <- [""a"", ""b"", ""b"", ""c"", ...]

    y <- [1, 2, 2, 3, ...]

",,2011-11-06 09:47:15.640
47866,16209,5196.0,3,,CC BY-SA 3.0,73dde68c-0572-477b-8721-a2746443264a,<r>,,2011-11-06 09:47:15.640
47867,16209,5196.0,1,,CC BY-SA 3.0,73dde68c-0572-477b-8721-a2746443264a,How to convert an vector of enumerable strings into a vector of numbers?,,2011-11-06 09:47:15.640
47878,16212,1927.0,2,,CC BY-SA 3.0,56c96588-9bef-4359-af3d-80193163a6b0,"Here is a possibility, very similar than that of @Roman Lustrik, but just a little bit more automatic.

Say that
       > x <- c(""a"", ""b"", ""b"", ""c"")
       
Then

       > x <- as.factor(x)
       > levels(x) <- 1:length(levels(x))
       > x <- as.numeric(x)
       
makes the job:

       > print(x)
       [1] 1 2 2 3
",,2011-11-06 11:12:09.377
47880,16209,5196.0,5,,CC BY-SA 3.0,8a95de3c-2fbb-4839-ba1f-aaacddcebab9,"How to convert the x below to into a vector like y?

    x <- [""a"", ""b"", ""b"", ""c"", ...]

    y <- [1, 2, 2, 3, ...]


**UPDATE:**

I end up with:

    levels(x) <- 1:length(levels(x))",added 63 characters in body,2011-11-06 11:21:43.040
48046,16209,668.0,4,,CC BY-SA 3.0,530f9c82-47d1-4163-9446-238f4028c866,How to convert a vector of enumerable strings into a vector of numbers?,edited title,2011-11-07 22:02:53.807
48171,16313,5234.0,1,,CC BY-SA 3.0,2a13055a-934e-4f80-99a6-bc2164c1deda,How does the Goodman-Kruskal Gamma test and the Kendall-Tau or Spearman-Rho test compare?,,2011-11-09 02:39:58.810
48170,16313,5234.0,3,,CC BY-SA 3.0,2a13055a-934e-4f80-99a6-bc2164c1deda,<ranking><spearman-rho><rank-correlation>,,2011-11-09 02:39:58.810
48172,16313,5234.0,2,,CC BY-SA 3.0,2a13055a-934e-4f80-99a6-bc2164c1deda,"In my work, we are comparing predicted rankings versus true rankings for some sets of data. Up until recently, we've been using Kendall-Tau alone. A group working on a similar project suggested we try to use the [Goodman-Kruskal Gamma test](http://en.wikipedia.org/wiki/Gamma_test_(statistics)) instead, and that they preferred it. I was wondering what the differences between the different rank correlation algorithms were.

The best I've found was [this answer](http://stats.stackexchange.com/questions/3943/kendall-tau-or-spearmans-rho/3946#3946), which claims Spearman is used in place of usual linear correlations, and that Kendall-Tau is less direct and more closely resembles Goodman-Kruskal Gamma. The data I'm working with doesn't seem to have any obvious linear correlations, and the data is heavily skewed and non-normal.

Also, Spearman generally reports better correlation than Kendall-Tau for our data, and I was wondering what that says about the data specifically. I'm not a statistician, so some of the papers I'm reading on these things just seem like jargon to me, sorry.",,2011-11-09 02:39:58.810
48196,16313,,4,user88,CC BY-SA 3.0,d4d06604-0256-40e1-b28b-5231f2773d50,How does the Goodman-Kruskal gamma test and the Kendall tau or Spearman rho test compare?,edited title,2011-11-09 07:54:20.863
48222,16313,,25,,,dd2db29b-245f-454e-a911-f38913dd9a7a,,http://twitter.com/#!/StackStats/status/134228649456041984,2011-11-09 11:19:38.553
74072,24506,7341.0,2,,CC BY-SA 3.0,3d76502f-1e6d-42d0-b5a7-2ce4d22e779a,"I'm using JMP to analyze some sample data to make predictions about the population.  My sample is from a destructive QC test, so I obviously want to minimize my sample.  I have a response (my Y) and a known factor (a very strong and consistent correlation that is measurable by non-destructive means) but the exact relationship between them varies from lot to lot (the slope and y offset vary).

So, in JMP, I am fitting a line and then showing the ""confidence limits for an individual predicted value"" which I believe gives me an indicator of how the population is likely to behave.  So I'm using that plot to make disposition decisions.  I want to automate this process, perhaps using R, but I'm a total novice at R.  I could do the math if I was just dealing with a mean and standard deviation, but I don't know how to do it with a fit line and a known factor.  Can someone please give me either the general information on how to get the confidence limits around the line, or else tell me how to do the whole thing in R?

Thankss much.",,2012-05-04 17:02:04.137
48230,16337,2081.0,2,,CC BY-SA 3.0,4dab8f73-9a2f-455e-a67a-64698eb4d463,"**Spearman rho vs Kendall tau**. These two are so much computationally different that you *cannot* directly compare their magnitudes. Spearman is usually higher by 1/4 to 1/3 and this makes one incorrectly conclude that Spearman is ""better"" for a particular dataset. The difference between rho and tau is in their ideology, *proportion-of-variance* for rho and *probability* for tau. Rho is a usual Pearson r applied for ranked data, and like r, is more sensitive to points with large moments (that is, deviations from cloud centre) than to points with small moments. Therefore rho is quite sensitive to the shape of the cloud: the coefficient for rhombic cloud will be higher than the coefficient for dumbbelled cloud. Tau is an extension of Gamma and is equally sensitive to all points, so it is less sensitive to peculiarities in shape of the ranked cloud. Tau is more ""general"" than rho, for rho is warranted only when you believe the underlying relationship between the variables is monotonic. Rho is comparable with r in magnitude; tau is not.

**Kendall tau as Gamma**. Tau is just a standardized form of Gamma. Several related measures all have numerator $P-Q$ but differ in normalizing *denominator*:

 - Gamma: $P+Q$
 - Somer's D(""x dependent""): $P+Q+T_x$
 - Somer's D(""y dependent""): $P+Q+T_y$
 - Somer's D(""symmetric""): arithmetic mean of the above two
 - Kendall's Tau-b corr. (most suitable for square tables): geometric mean of those two
 - Kendall's Tau-c corr. (most suitable for rectangular tables): $N^2(k-1)/2k$
 - Kendall's Tau-a corr. (makes nо adjustment for ties): $N(N-1)/2 = P+Q+T_x+T_y+T_{xy}$

where $P$ - number of pairs of observations with ""concordance"", $Q$ - with ""inversion""; $T_x$ - number of ties by variable X, $T_y$ - by variable Y, $T_{xy}$ – by both variables; $N$ - number of observations, $k$ - number of distinct values in that variable where this number is less.
",,2011-11-09 11:41:29.170
48231,16337,2081.0,5,,CC BY-SA 3.0,06b7aa90-7328-487a-8a0c-bc7f02816ac5,"**Spearman rho vs Kendall tau**. These two are so much computationally different that you *cannot* directly compare their magnitudes. Spearman is usually higher by 1/4 to 1/3 and this makes one incorrectly conclude that Spearman is ""better"" for a particular dataset. The difference between rho and tau is in their ideology, *proportion-of-variance* for rho and *probability* for tau. Rho is a usual Pearson r applied for ranked data, and like r, is more sensitive to points with large moments (that is, deviations from cloud centre) than to points with small moments. Therefore rho is quite sensitive to the shape of the cloud (upon ranling data): the coefficient for rhombic cloud will be higher than the coefficient for dumbbelled cloud. Tau is an extension of Gamma and is equally sensitive to all points, so it is less sensitive to peculiarities in shape of the ranked cloud. Tau is more ""general"" than rho, for rho is warranted only when you believe the underlying relationship between the variables is monotonic. Rho is comparable with r in magnitude; tau is not.

**Kendall tau as Gamma**. Tau is just a standardized form of Gamma. Several related measures all have numerator $P-Q$ but differ in normalizing *denominator*:

 - Gamma: $P+Q$
 - Somer's D(""x dependent""): $P+Q+T_x$
 - Somer's D(""y dependent""): $P+Q+T_y$
 - Somer's D(""symmetric""): arithmetic mean of the above two
 - Kendall's Tau-b corr. (most suitable for square tables): geometric mean of those two
 - Kendall's Tau-c corr. (most suitable for rectangular tables): $N^2(k-1)/2k$
 - Kendall's Tau-a corr. (makes nо adjustment for ties): $N(N-1)/2 = P+Q+T_x+T_y+T_{xy}$

where $P$ - number of pairs of observations with ""concordance"", $Q$ - with ""inversion""; $T_x$ - number of ties by variable X, $T_y$ - by variable Y, $T_{xy}$ – by both variables; $N$ - number of observations, $k$ - number of distinct values in that variable where this number is less.
",added 20 characters in body,2011-11-09 11:56:10.417
48232,16337,2081.0,5,,CC BY-SA 3.0,8988fdf2-6004-4296-8897-c482a39e4aec,"**Spearman rho vs Kendall tau**. These two are so much computationally different that you *cannot* directly compare their magnitudes. Spearman is usually higher by 1/4 to 1/3 and this makes one incorrectly conclude that Spearman is ""better"" for a particular dataset. The difference between rho and tau is in their ideology, *proportion-of-variance* for rho and *probability* for tau. Rho is a usual Pearson r applied for ranked data, and like r, is more sensitive to points with large moments (that is, deviations from cloud centre) than to points with small moments. Therefore rho is quite sensitive to the shape of the cloud: the coefficient for rhombic cloud will be higher than the coefficient for dumbbelled cloud. Tau is an extension of Gamma and is equally sensitive to all points, so it is less sensitive to peculiarities in shape of the ranked cloud. Tau is more ""general"" than rho, for rho is warranted only when you believe the underlying relationship between the variables is monotonic. Rho is comparable with r in magnitude; tau is not.

**Kendall tau as Gamma**. Tau is just a standardized form of Gamma. Several related measures all have numerator $P-Q$ but differ in normalizing *denominator*:

 - Gamma: $P+Q$
 - Somer's D(""x dependent""): $P+Q+T_x$
 - Somer's D(""y dependent""): $P+Q+T_y$
 - Somer's D(""symmetric""): arithmetic mean of the above two
 - Kendall's Tau-b corr. (most suitable for square tables): geometric mean of those two
 - Kendall's Tau-c corr. (most suitable for rectangular tables): $N^2(k-1)/2k$
 - Kendall's Tau-a corr. (makes nо adjustment for ties): $N(N-1)/2 = P+Q+T_x+T_y+T_{xy}$

where $P$ - number of pairs of observations with ""concordance"", $Q$ - with ""inversion""; $T_x$ - number of ties by variable X, $T_y$ - by variable Y, $T_{xy}$ – by both variables; $N$ - number of observations, $k$ - number of distinct values in that variable where this number is less.
",edited body,2011-11-09 12:06:16.160
48338,16366,5249.0,3,,CC BY-SA 3.0,2d928bb0-4a91-4fe9-bd6b-3f44175f6ec7,<text-mining>,,2011-11-10 03:08:12.977
48336,16366,5249.0,2,,CC BY-SA 3.0,2d928bb0-4a91-4fe9-bd6b-3f44175f6ec7,"I'm confused about how to calculate the perplexity of a holdout sample when doing Latent Dirichlet Allocation (LDA). The papers on the topic breeze over it, making me think I'm missing something obvious.. So..

Perplexity is seen as a good measure of performance for LDA. The idea is that you keep a holdout sample, train your LDA on the rest of the data, then calculate the perplexity of the holdout. 

The perplexity could be given by the forumla:

$per(D_{test})=exp\{-\frac{\sum_{d=1}^{M}\log p(\mathbb{w}_d)}{\sum_{d=1}^{M}N_d}\} $

(taken from [Image retrieval on large-scale image databases, Horster et al][2])
Here  $M$ is the number of documents (in the test sample, presumably), $\mathbb{w}_d$ represents the words in document $d$, $N_d$ the number of words in document $d$.

It is not clear to me how to sensibly calcluate $p(\mathbb{w}_d)$, since we don't have topic mixtures for the held out documents. Ideally, we would integrate over the Dirichlet prior for all possible topic mixtures and use the topic multinomials we learned. Calculating this integral doesn't seem an easy task however.

Alternatively, we could attempt to learn an optimal topic mixture for each held out document (given our learned topics) and use this to calculate the perplexity. This would be doable, however it's not as trivial as papers such as Horter et al and Blei et al seem to suggest, and it's not immediately clear to me that the result will be equivalent to the ideal case above.





  [1]: https://i.stack.imgur.com/yWP97.jpg
  [2]: http://doi.acm.org/10.1145/1282280.1282283",,2011-11-10 03:08:12.977
48337,16366,5249.0,1,,CC BY-SA 3.0,2d928bb0-4a91-4fe9-bd6b-3f44175f6ec7,How to calculate perplexity of a holdout with LDA,,2011-11-10 03:08:12.977
48346,16366,2081.0,4,,CC BY-SA 3.0,0c8204ea-50fa-4151-8add-d2897f059886,How to calculate perplexity of a holdout with Latent Dirichlet Allocation,edited title,2011-11-10 06:24:54.233
54652,18345,5448.0,2,,CC BY-SA 3.0,101d508e-840d-48b2-92b9-c6d503b7121a,"The LR (likelihood ratio) test actually is testing the hypothesis that a specified subset of the parameters equal some pre-specified values.  In the case of model selection, generally (but not always) that means some of the parameters equal zero.  If the models are nested, the parameters in the larger model that are not in the smaller model are the ones being tested, with values specified implicitly by their exclusion from the smaller model.  If the models aren't nested, you aren't testing this any more, because BOTH models have parameters that aren't in the other model, so the LR test statistic doesn't have the asymptotic $\chi^2$ distribution that it (usually) does in the nested case.

AIC, on the other hand, is not used for formal testing.  It is used for informal comparisons of models with differing numbers of parameters.   The penalty term in the expression for AIC is what allows this comparison.  But no assumptions are made about the functional form of the asymptotic distribution of the differences between the AIC of two non-nested models when doing the model comparison, and the difference between two AICs is not treated as a test statistic.",,2012-01-01 17:41:56.327
74283,24506,,4,user88,CC BY-SA 3.0,216d616c-d992-426c-a52a-bac828ae0063,Confidence interval for values for a fitted line,edited title,2012-05-05 20:47:03.430
181537,55722,15827.0,4,,CC BY-SA 3.0,2c950a00-ef5e-4b19-b2c0-aaa310f0a991,Looking for a good and complete probability and statistics book,small fixes to English,2013-09-25 11:17:31.700
48372,16366,674.0,5,,CC BY-SA 3.0,86ea3e53-d91e-492f-bd85-6c9f9e3cf955,"I'm confused about how to calculate the perplexity of a holdout sample when doing Latent Dirichlet Allocation (LDA). The papers on the topic breeze over it, making me think I'm missing something obvious...

Perplexity is seen as a good measure of performance for LDA. The idea is that you keep a holdout sample, train your LDA on the rest of the data, then calculate the perplexity of the holdout. 

The perplexity could be given by the formula:

$per(D_{test})=exp\{-\frac{\sum_{d=1}^{M}\log p(\mathbb{w}_d)}{\sum_{d=1}^{M}N_d}\} $

(Taken from [Image retrieval on large-scale image databases, Horster et al][2].)

Here  $M$ is the number of documents (in the test sample, presumably), $\mathbb{w}_d$ represents the words in document $d$, $N_d$ the number of words in document $d$.

It is not clear to me how to sensibly calcluate $p(\mathbb{w}_d)$, since we don't have topic mixtures for the held out documents. Ideally, we would integrate over the Dirichlet prior for all possible topic mixtures and use the topic multinomials we learned. Calculating this integral doesn't seem an easy task however.

Alternatively, we could attempt to learn an optimal topic mixture for each held out document (given our learned topics) and use this to calculate the perplexity. This would be doable, however it's not as trivial as papers such as Horter et al and Blei et al seem to suggest, and it's not immediately clear to me that the result will be equivalent to the ideal case above.





  [1]: https://i.stack.imgur.com/yWP97.jpg
  [2]: http://doi.acm.org/10.1145/1282280.1282283",edited title,2011-11-10 08:29:44.023
48373,16366,674.0,4,,CC BY-SA 3.0,86ea3e53-d91e-492f-bd85-6c9f9e3cf955,How to calculate perplexity of a holdout with Latent Dirichlet Allocation?,edited title,2011-11-10 08:29:44.023
48807,15542,1406.0,5,,CC BY-SA 3.0,6e8e2806-e31a-4065-9d89-28d74b6c48b9,"This is a naive question from someone starting to learn machine learning. I'm reading these days the book ""Machine Learning: An algorithmic perspective"" from Marsland. I find it useful as an introductory book, but now I would like to go into advanced algorithms, those that are currently giving the best results. I'm mostly interested in bioinformatics: clustering of biological networks and finding patterns in biological sequences, particularly applied to single nucleotide polymorphism (SNP) analysis. Could you recommend me some reviews or books to read?
",deleted 16 characters in body,2011-11-14 12:28:30.437
48901,16537,1831.0,2,,CC BY-SA 3.0,8f71eff4-ad42-4cd0-969c-1a8414bd0cd0,"[Deep Learning][1] got a lot of focus since 2006. It's basically an approach to train deep neural networks and is leading to really impressive results on very hard datasets (like document clustering or object recognition). Some people are talking about the second neural network renaissance (eg in [this Google talk][2] by Schmidhuber).

If you want to be impressed you should look at this Science paper [Reducing the Dimensionality of Data with Neural Networks,][3] Hinton & Salakhutdinov.

(There is so much work going on right now in that area, that there is only two upcoming books I know about that will treat it: [Large scale machine learning][4], Langford et al and [Machine Learning: a probabilistic perspective][5] by Kevin Murphy.)

If you want to know more, check out what the main deep learning groups are doing: [Stanford][6], [Montreal][7] and most importantly [Toronto #1][8] and [Toronto #2][9].


  [1]: http://deeplearning.net/
  [2]: http://www.youtube.com/watch?v=rkCNbi26Hds
  [3]: http://www.mit.edu/~rsalakhu/papers/science.pdf
  [4]: http://www.cambridge.org/aus/catalogue/catalogue.asp?isbn=9780521192248
  [5]: http://www.cs.ubc.ca/~murphyk/MLbook/index.html
  [6]: http://www.cs.stanford.edu/people/ang/
  [7]: http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html
  [8]: http://www.cs.toronto.edu/~hinton/
  [9]: http://www.utstat.toronto.edu/~rsalakhu/",,2011-11-14 20:26:28.373
50409,16998,5479.0,1,,CC BY-SA 3.0,504e401b-ec61-46a6-abcf-1c7bb1558bc1,Large Text Corpus,,2011-11-24 21:22:19.287
50407,16998,5479.0,3,,CC BY-SA 3.0,504e401b-ec61-46a6-abcf-1c7bb1558bc1,<correlation><dataset>,,2011-11-24 21:22:19.287
50408,16998,5479.0,2,,CC BY-SA 3.0,504e401b-ec61-46a6-abcf-1c7bb1558bc1,"I am looking for large (>1000) text corpus to download. Preferably with **world news** or some kind of **reports**. I have only found one with patents. Any suggestions?
Thanks in advance.",,2011-11-24 21:22:19.287
50413,17000,65.0,2,,CC BY-SA 3.0,0a6dbb03-dc99-4e2f-866c-7d4ec6c57dcc,Do not the Wikileaks texts suit you?,,2011-11-24 21:48:24.163
50417,16998,,4,user88,CC BY-SA 3.0,01530515-c877-4b58-9c85-1ae101686dbf,Where to find a large text corpus?,added 2 characters in body; edited tags; edited title,2011-11-24 22:07:16.037
50415,16998,,6,user88,CC BY-SA 3.0,01530515-c877-4b58-9c85-1ae101686dbf,<dataset>,added 2 characters in body; edited tags; edited title,2011-11-24 22:07:16.037
50416,16998,,5,user88,CC BY-SA 3.0,01530515-c877-4b58-9c85-1ae101686dbf,"I am looking for large (>1000) text corpus to download. Preferably with **world news** or some kind of **reports**. I have only found one with patents. Any suggestions?

Thanks in advance.",added 2 characters in body; edited tags; edited title,2011-11-24 22:07:16.037
50440,16998,,25,,,5b68229b-b463-43f3-978c-ca093f894619,,http://twitter.com/#!/StackStats/status/139891353785077760,2011-11-25 02:21:12.780
50539,16998,668.0,16,,,36fa3f61-2e7c-4e71-879f-c7ddba16c861,,,2011-11-25 15:52:54.157
50540,17000,668.0,16,,,07569919-7a93-4df4-9c3d-7a3460f88c84,,,2011-11-25 15:52:54.157
52708,4705,1209.0,8,,CC BY-SA 3.0,46ee557c-6802-44e1-952b-383358c4564e,What are the most important statisticians and what is it that made them famous? (Reply just one scientist per answer please),Rollback to [0998ef5d-278a-43dd-bada-15885d12473c],2011-12-13 10:15:04.773
52709,4705,1209.0,9,,CC BY-SA 3.0,46ee557c-6802-44e1-952b-383358c4564e,<methodology><analysis>,Rollback to [0998ef5d-278a-43dd-bada-15885d12473c],2011-12-13 10:15:04.773
54620,18335,5038.0,3,,CC BY-SA 3.0,c7afb6f4-5687-4296-a138-723d0b135298,<aic><likelihood-ratio>,,2012-01-01 12:15:03.760
54618,18335,5038.0,2,,CC BY-SA 3.0,c7afb6f4-5687-4296-a138-723d0b135298,"Both the likelihood ratio test and the AIC are tools for choosing between two models and both are based on the log-likelihood.

But, why the likelihood ratio test can't be used to choose between two non-nested models while AIC can?",,2012-01-01 12:15:03.760
54619,18335,5038.0,1,,CC BY-SA 3.0,c7afb6f4-5687-4296-a138-723d0b135298,Non-nested model selection,,2012-01-01 12:15:03.760
61671,20667,5911.0,2,,CC BY-SA 3.0,f352f93e-00b4-4f87-8218-2c840a03ce9e,"Is there any datasets of 2 dimensional datapoints (each datapoint is a vector of two values (x,y)) following different distributions and forms ? Or is there any codes to generate such a datapoints ? I want to use them to plot/visualise how some clustering algorithms performs on them. Here are some examples:

 - http://www.cise.ufl.edu/~jmishra/clustering/ClusterImages/KMeans4.jpg
 - http://www.aishack.in/wp-content/uploads/2010/07/kmeans-example.jpg
 - http://www.ti.uni-bielefeld.de/html/research/ngpca/spiral.1.png
 - http://courses.ee.sun.ac.za/Pattern_Recognition_813/lectures/6_em/img19.png
 - http://3.bp.blogspot.com/_k1D0z3ucw7o/SITL8Et0QAI/AAAAAAAADSw/Bp_N8c9i5SE/s320/Sigma0.25.png
 - http://www.newfolderconsulting.com/prtdoc/prtDocDataGen_01.png
 - ... etc",,2012-02-16 21:14:21.930
54677,18345,5448.0,5,,CC BY-SA 3.0,bd59794b-5fc9-4117-ae72-3fbc438ad24e,"The LR (likelihood ratio) test actually is testing the hypothesis that a specified subset of the parameters equal some pre-specified values.  In the case of model selection, generally (but not always) that means some of the parameters equal zero.  If the models are nested, the parameters in the larger model that are not in the smaller model are the ones being tested, with values specified implicitly by their exclusion from the smaller model.  If the models aren't nested, you aren't testing this any more, because BOTH models have parameters that aren't in the other model, so the LR test statistic doesn't have the asymptotic $\chi^2$ distribution that it (usually) does in the nested case.

AIC, on the other hand, is not used for formal testing.  It is used for informal comparisons of models with differing numbers of parameters.   The penalty term in the expression for AIC is what allows this comparison.  But no assumptions are made about the functional form of the asymptotic distribution of the differences between the AIC of two non-nested models when doing the model comparison, and the difference between two AICs is not treated as a test statistic.

I'll add that there is some disagreement over the use of AIC with non-nested models, as the theory is worked out for nested models.  Hence my emphasis on ""not...formal"" and ""not...test statistic.""  I use it for non-nested models, but not in a hard-and-fast way, more as an important, but not the sole, input into the model building process.",added 344 characters in body,2012-01-01 23:02:09.460
54688,18335,,25,,,b0f27a0f-b1b7-4ed5-bab8-234df6a0a12e,,http://twitter.com/#!/StackStats/status/153663247508193280,2012-01-02 02:25:48.097
55317,4705,668.0,6,,CC BY-SA 3.0,d229089b-e579-4c0a-bc11-80e7ef91842e,<methodology>,edited tags,2012-01-07 01:53:14.437
55832,10008,668.0,38,,,72919f37-cbd1-4a46-b360-3b0324fc2554,"[{""Id"":919,""DisplayName"":""whuber""}]",from http://stats.stackexchange.com/questions/20862/covariate-present-in-a-logistic-regression-model-as-a-effect-modifier-but-not-a,2012-01-10 14:02:57.437
60422,20234,12900.0,2,vzn,CC BY-SA 3.0,36b42512-b757-4ba4-a493-04059dcd742f,"hi all recently there was a ML-like question over on cstheory stackexchange & I posted an answer recommending powells method, gradient descent or genetic algorithms or other <a href=""http://en.wikipedia.org/wiki/Approximation_algorithm"">""approximation algorithms"".</a> in a comment someone told me these methods were ""heuristics"" and _not_ ""approximation algorithms"" and frequently did not come close to the theoretical optimum (because they ""frequently get stuck in local minima"").

my question, do others agree with that? also it seems to me there is a sense in which heuristic algorithms can be guaranteed to come close to theoretical optimums if they are set up to explore a large part of the search space (eg setting parameters/step sizes small), although I havent seen that in a paper. does anyone know if this has been shown or proven in a paper somewhere? (if not for a large class of algorithms maybe for a small class say NNs etc)",,2012-02-10 19:03:03.517
60423,20234,12900.0,3,vzn,CC BY-SA 3.0,36b42512-b757-4ba4-a493-04059dcd742f,<neural-networks><approximation>,,2012-02-10 19:03:03.517
60421,20234,12900.0,1,vzn,CC BY-SA 3.0,36b42512-b757-4ba4-a493-04059dcd742f,"are machine learning techniques ""approximation algorithms""?",,2012-02-10 19:03:03.517
60441,20240,1073.0,2,,CC BY-SA 3.0,d7ce0bc2-0c64-459b-97b1-6ac4e2d77d1a,"I think you're mixing multiple important concepts. Let me try to clarify a couple of things:

 - There are metaheuristic methods, which are methods that iteratively try to improve a candidate solution. Examples of this are tabu search, simulated annealing, genetic algorithms, etc. Observe that while there can be many cases where these methods work nicely, there isn't any deep understanding of when these methods work and when they don't. And more importantly when they don't get to the solution, we can be arbitrarily far from it. Problems solved by metaheuristic methods tend to be discrete in nature, because there are far better tools to handle continuous problems. But every now and then you see metaheuristics for continuous problems, too.

 - There are numerical optimization methods, people in this community carefully the nature of the function that is to be optimized and the    restrictions of the solution (into groups like convex optimization,     quadratic programming, linear programming etc) and apply algorithms  that have been shown to work for that type of function, and that type of restrictions. When people in this area say ""shown to work"" they mean a proof. The situation is that these types of methods work in continuous problems. But when your problem falls in this category,  this is definitely the tool to use.

 - There are discrete optimization methods, which tend to be things that in nature are connected to algorithms to well studied discrete problems: such as shortest paths, max flow, etc. People in this area also care that their algorithms really work (proofs). There are a subset of people in this group that study really hard problems for which no fast algorithm is expected to exist. They then study approximation algorithms, which are fast algorithms for which they are able to show that their solution is within a constant factor of the true optimum. This is called ""approximation algorithms"". These people also show their results as proofs.

So... to answer your question, I do not think that metaheuristics are approximation algorithms. It doesn't seem to me as something connected to opinion, it is just fact.



",,2012-02-10 21:45:51.503
61390,20561,786.0,1,,CC BY-SA 3.0,abd789e5-91d5-4535-beaf-e8f6f3929d78,How to deal with Gaps/NaNs in time series data when using Matlab for autocorrelation and neural networks?,,2012-02-15 19:25:44.330
61389,20561,786.0,2,,CC BY-SA 3.0,abd789e5-91d5-4535-beaf-e8f6f3929d78,"I have a time series of measurements (heights). In the observation period, the measurement process went down for some time points. So the resulting data is a vector with NaNs where there were gaps in the data. Using MATLAB, this is causing me a problem when computing the autocorrelation (`autocorr`) and applying neural networks (`nnstart`). 

How should these Gaps/NaNs be dealt with? Should I just remove these from the vector? Or replace their entry with an interpolated value? (if so how in MATLAB)",,2012-02-15 19:25:44.330
61388,20561,786.0,3,,CC BY-SA 3.0,abd789e5-91d5-4535-beaf-e8f6f3929d78,<time-series><dataset><matlab><autocorrelation><missing-data>,,2012-02-15 19:25:44.330
61396,20561,,4,user88,CC BY-SA 3.0,a2b288f3-30a7-4464-9d8f-e344352b7a44,How to deal with gaps/NaNs in time series data when using Matlab for autocorrelation and neural networks?,edited title,2012-02-15 19:44:22.593
61431,20561,786.0,5,,CC BY-SA 3.0,5125c547-a0bd-4175-8914-95746e330ebe,"I have a time series of measurements (heights-one dimensional series). In the observation period, the measurement process went down for some time points. So the resulting data is a vector with NaNs where there were gaps in the data. Using MATLAB, this is causing me a problem when computing the autocorrelation (`autocorr`) and applying neural networks (`nnstart`). 

How should these Gaps/NaNs be dealt with? Should I just remove these from the vector? Or replace their entry with an interpolated value? (if so how in MATLAB)",clarification,2012-02-15 23:19:08.517
61672,20667,5911.0,1,,CC BY-SA 3.0,f352f93e-00b4-4f87-8218-2c840a03ce9e,"2D artificial data of different distributions and forms, existing datasets or generation code",,2012-02-16 21:14:21.930
61673,20667,5911.0,3,,CC BY-SA 3.0,f352f93e-00b4-4f87-8218-2c840a03ce9e,<dataset><distributions>,,2012-02-16 21:14:21.930
68714,22797,7769.0,2,,CC BY-SA 3.0,5510d113-64ee-4412-a1a2-b5da2133932f,"For a simple 2 variables (say X and Y) cointegration test, how does it affect our analysis, if we perform regression on X and Y with and without the intercept, and then test the spread for stationarity.

I am doing this analysis for stocks.",,2012-04-02 07:39:49.013
68716,22797,7769.0,3,,CC BY-SA 3.0,5510d113-64ee-4412-a1a2-b5da2133932f,<regression><cointegration>,,2012-04-02 07:39:49.013
68715,22797,7769.0,1,,CC BY-SA 3.0,5510d113-64ee-4412-a1a2-b5da2133932f,"Cointegration:- Regression with and without intercepts, and testing spreads for stationarity",,2012-04-02 07:39:49.013
61824,28,,5,,CC BY-SA 3.0,0660907c-db69-4602-a1dd-036eae33de32,"Last year, I read a blog post from [Bendan O'Connor][1] entitled [""Statistics vs. Machine Learning, fight!""][2] that discussed some of the differences between the two fields.  [Andrew Gelman responded to favorably to this][3]:

Simon Blomberg: 
> From R's fortunes
> package: To paraphrase provocatively,
> 'machine learning is statistics minus
> any checking of models and
> assumptions'.
> -- Brian D. Ripley (about the difference between machine learning
> and statistics) useR! 2004, Vienna
> (May 2004) :-) Season's Greetings!

Andrew Gelman:

> In that case, maybe we should get rid
> of checking of models and assumptions
> more often. Then maybe we'd be able to
> solve some of the problems that the
> machine learning people can solve but
> we can't!

There was also the [**""Statistical Modeling: The Two Cultures""** paper][4] by Leo Breiman in 2001 which argued that Statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the *predictive accuracy* of models.

Has the Statistics field changed over the last decade in response to these critiques?  Do the *two cultures* still exist or has Statistics grown to embrace machine learning techniques such as neural networks and support vector machines?


  [1]: http://anyall.org/
  [2]: http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/
  [3]: http://andrewgelman.com/2008/12/machine_learnin/
  [4]: http://www.stat.osu.edu/~bli/dmsl/papers/Breiman.pdf",gelman's blog moved and the old redirects seem broken; insert new url,2012-02-17 15:49:06.140
62108,20667,0.0,36,,,60b0d735-4c7a-489f-9339-c31be0d24988,,from http://stackoverflow.com/questions/9319168/2d-artificial-data-of-different-distributions-and-forms-existing-datasets-or-ge,2012-02-20 07:47:45.927
62127,20667,674.0,6,,CC BY-SA 3.0,f8736e43-ac8b-4d9b-8ab5-eb55a18aa69a,<distributions><data-visualization><clustering><dataset>,edited tags,2012-02-20 09:06:30.837
62361,20667,221.0,5,,CC BY-SA 3.0,620cdcf1-8998-46dc-aaea-e6344367280e,"I am looking for datasets of 2 dimensional datapoints (each datapoint is a vector of two values (x,y)) following different distributions and forms ? Or is there any codes to generate such a datapoints ? I want to use them to plot/visualise how some clustering algorithms performs on them. Here are some examples:

 - [star like cloud data][1]
 - [four clusters, one easy seperable][2] 
 - [a spiral (no cluster)][3]
 - [a ring][4]
 - [two barely seperated clouds][5]
 - [two parallel clusters forming a spiral][6]
 - ... etc


  [1]: http://www.cise.ufl.edu/~jmishra/clustering/ClusterImages/KMeans4.jpg
  [2]: http://www.aishack.in/wp-content/uploads/2010/07/kmeans-example.jpg
  [3]: http://www.ti.uni-bielefeld.de/html/research/ngpca/spiral.1.png
  [4]: http://%20http://courses.ee.sun.ac.za/Pattern_Recognition_813/lectures/6_em/img19.png
  [5]: http://3.bp.blogspot.com/_k1D0z3ucw7o/SITL8Et0QAI/AAAAAAAADSw/Bp_N8c9i5SE/s320/Sigma0.25.png
  [6]: http://www.newfolderconsulting.com/prtdoc/prtDocDataGen_01.png",changed title to accomodate for the application to cluster analysis; changed bare links to SE-provided description-link-style,2012-02-21 12:47:33.070
62362,20667,221.0,4,,CC BY-SA 3.0,620cdcf1-8998-46dc-aaea-e6344367280e,Looking for 2D artificial data to demonstrate properties of clustering algorithms,changed title to accomodate for the application to cluster analysis; changed bare links to SE-provided description-link-style,2012-02-21 12:47:33.070
62370,20667,,16,user88,,3a92734b-4ea6-49c5-8d6e-828f5ef4960c,,,2012-02-21 13:06:45.223
62692,20240,0.0,36,,,66326469-f9aa-4b4d-824e-a686eca4faad,,from http://machinelearning.stackexchange.com/questions/104/are-machine-learning-techniques-approximation-algorithms/105#105,2012-02-22 19:46:04.020
62693,20234,0.0,36,,,77630c49-3505-4c0c-a665-b4eaed309e80,,from http://machinelearning.stackexchange.com/questions/104/are-machine-learning-techniques-approximation-algorithms,2012-02-22 19:46:04.020
62861,20234,,4,user88,CC BY-SA 3.0,ed8a9760-3a2a-4fa4-a9ef-7b93b58e501c,"Are machine learning techniques ""approximation algorithms""?",edited title,2012-02-22 23:58:08.800
62931,20234,,25,,,ca22530d-6b53-4fa8-bd52-7b31ffd6ee48,,http://twitter.com/#!/StackStats/status/172646050895638528,2012-02-23 11:36:40.773
63076,20234,,6,,CC BY-SA 3.0,14ebc707-d316-463d-b232-01b8744d6045,<machine-learning><optimization><approximation>,edited tags,2012-02-24 09:22:17.303
67006,28,568.0,5,,CC BY-SA 3.0,0f0a7774-d8d2-4f92-b536-c32f261ebc21,"Last year, I read a blog post from [Bendan O'Connor][1] entitled [""Statistics vs. Machine Learning, fight!""][2] that discussed some of the differences between the two fields.  [Andrew Gelman responded to favorably to this][3]:

Simon Blomberg: 
> From R's fortunes
> package: To paraphrase provocatively,
> 'machine learning is statistics minus
> any checking of models and
> assumptions'.
> -- Brian D. Ripley (about the difference between machine learning
> and statistics) useR! 2004, Vienna
> (May 2004) :-) Season's Greetings!

Andrew Gelman:

> In that case, maybe we should get rid
> of checking of models and assumptions
> more often. Then maybe we'd be able to
> solve some of the problems that the
> machine learning people can solve but
> we can't!

There was also the [**""Statistical Modeling: The Two Cultures""** paper][4] by Leo Breiman in 2001 which argued that Statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the *predictive accuracy* of models.

Has the Statistics field changed over the last decade in response to these critiques?  Do the *two cultures* still exist or has Statistics grown to embrace machine learning techniques such as neural networks and support vector machines?


  [1]: http://anyall.org/
  [2]: http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/
  [3]: http://andrewgelman.com/2008/12/machine_learnin/
  [4]: http://projecteuclid.org/euclid.ss/1009213726",fixed broken link,2012-03-22 14:45:13.073
68281,22674,7714.0,3,,CC BY-SA 3.0,e6199626-9024-4a98-8136-48c0a4e24c12,<machine-learning><graph-theory>,,2012-03-30 03:51:41.627
68282,22674,7714.0,1,,CC BY-SA 3.0,e6199626-9024-4a98-8136-48c0a4e24c12,Belief Propagation for MRF with complex cliques,,2012-03-30 03:51:41.627
68283,22674,7714.0,2,,CC BY-SA 3.0,e6199626-9024-4a98-8136-48c0a4e24c12,"Is there a belief propagation algorithm for exact inference on a MRF with complex clique structures (i.e. ones involving more than 2 neighbours)?

For MRF's with cliques that only involve pairwise interaction, you could just search out far enough and cluster to form an acyclic graph and run the usual BP.  With more complex cliques, this seems impossible to me as clustering might involve cutting through a clique with multiple members on either side.  Is there a workaround for this?  Perhaps some clever conditioning arguments?",,2012-03-30 03:51:41.627
68284,22674,7714.0,4,,CC BY-SA 3.0,514f4d04-808d-4cf8-8392-2494309ec6f2,Belief Propagation on MRF with complex cliques,edited title,2012-03-30 03:59:07.250
68314,22674,,4,user88,CC BY-SA 3.0,64237466-458f-4543-8422-abbd0306dfd0,Belief propagation on MRF with complex cliques,edited title,2012-03-30 09:56:54.120
68343,4187,,6,,CC BY-SA 3.0,48123a9a-35c5-4f90-8bf4-a17c096bed74,<best-practices><big-list>,tagging with big-list,2012-03-30 13:30:25.143
184201,56859,155.0,6,,CC BY-SA 3.0,c9516594-9ccd-488f-8fc3-414b330e5282,<terminology><descriptive-statistics><inference>,edited tags; edited title,2013-10-05 05:52:04.663
69401,23019,7739.0,2,,CC BY-SA 3.0,c2a12930-342d-44b6-a3f2-64ae387ff8ac,"Essentially, I have two collinear variables which could be seen as either random or as fixed effects, a variable I'm trying to predict, and a variable that's assuredly a random effect.
 
__Dependent var:__ Number of neuron spikes (`FiringRate`) in a specific region of mousebrain

__Fixed effects:__

__1)__ `Time` at which data sample was taken (on a linear scale in days -- so day two would be 2, day 5 would be 5, and so on)

__2)__ The `Age` of the mouse in days (so there's definitely collinearity between this and the `Time` variable, but there are enough mice of different ages to make this worthwhile as a separate variable)

__Random effect:__ `Subject` -- ""Name"" (ID number) of the mouse

Essentially, I'm wondering if it would be appropriate to run two LMEs. In the first, I'd treat `Age` and `Subject` as random variables in order to control for the effects of `Age` and see if Time is a significant predictor of the # of spikes (dependent variable). In the second, I'd enter `Time` and `Subject` as random variables to see if `Age` was a significant predictor.

    library(lme4)
    a = lmer(FiringRate ~ Time + (1|Age) + (1|Subject))
    b = lmer(FiringRate ~ Age + (1|Time) + (1|Subject))
    ",,2012-04-05 23:08:58.800
69399,23019,7739.0,3,,CC BY-SA 3.0,c2a12930-342d-44b6-a3f2-64ae387ff8ac,<r><mixed-model><multiple-regression><random-effects-model><fixed-effects-model>,,2012-04-05 23:08:58.800
69400,23019,7739.0,1,,CC BY-SA 3.0,c2a12930-342d-44b6-a3f2-64ae387ff8ac,"R: How to ""control"" for another variable in Linear Mixed Effects Regression model?",,2012-04-05 23:08:58.800
69402,23019,7739.0,5,,CC BY-SA 3.0,1e3333e1-8c49-41cd-a463-f7500c498c11,"Essentially, I have two collinear variables which could be seen as either random or as fixed effects, a dependent variable I'm fitting the model to, and a variable that's assuredly a random effect.
 
__Dependent var:__ Number of neuron spikes (`FiringRate`) in a specific region of mousebrain

__Fixed effects:__

__1)__ `Time` at which data sample was taken (on a linear scale in days -- so day two would be 2, day 5 would be 5, and so on)

__2)__ The `Age` of the mouse in days (so there's definitely collinearity between this and the `Time` variable, but there are enough mice of different ages to make this worthwhile as a separate variable)

__Random effect:__ `Subject` -- ""Name"" (ID number) of the mouse

Essentially, I'm wondering if it would be appropriate to run two LMEs. In the first, I'd treat `Age` and `Subject` as random variables in order to control for the effects of `Age` and see if Time is a significant predictor of the # of spikes (dependent variable). In the second, I'd enter `Time` and `Subject` as random variables to see if `Age` was a significant predictor.

    library(lme4)
    a = lmer(FiringRate ~ Time + (1|Age) + (1|Subject))
    b = lmer(FiringRate ~ Age + (1|Time) + (1|Subject))
    ",added 13 characters in body,2012-04-05 23:15:21.473
69413,23019,7739.0,5,,CC BY-SA 3.0,f45fc943-f923-46a8-98be-ac2dc340c533,"Essentially, I have two collinear variables which could be seen as either random or as fixed effects, a dependent variable I'm fitting the model to, and a variable that's assuredly a random effect.
 
__Dependent var:__ Number of neuron spikes (`FiringRate`) in a specific region of mousebrain

__Fixed effects:__

__1)__ `Time` at which data sample was taken (on a linear scale in days -- so day two would be 2, day 5 would be 5, and so on)

__2)__ The `Age` of the mouse in days (so there's definitely collinearity between this and the `Time` variable, but there are enough mice of different ages to make this worthwhile as a separate variable)

__Random effect:__ `Subject` -- ""Name"" (ID number) of the mouse

Essentially, I'm wondering if it would be appropriate to run two LMEs. In the first, I'd treat `Age` and `Subject` as random variables in order to control for the effects of `Age` (and thus the collinearity between `Age` and `Time`) and see if Time is a significant predictor of the # of spikes (dependent variable). In the second, I'd enter `Time` and `Subject` as random variables to see if `Age` was a significant predictor.

    library(lme4)
    a = lmer(FiringRate ~ Time + (1|Age) + (1|Subject))
    b = lmer(FiringRate ~ Age + (1|Time) + (1|Subject))
    ",added 53 characters in body,2012-04-06 01:52:06.693
69601,23019,,25,,,6d9c677e-c38f-4861-8712-a0a9fef76ce8,,http://twitter.com/#!/StackStats/status/188501092475207682,2012-04-07 05:38:57.223
69629,23087,5643.0,1,,CC BY-SA 3.0,c97fe292-fd58-4503-af06-80b1895e9644,Moving-average model error terms,,2012-04-07 12:48:41.467
69630,23087,5643.0,3,,CC BY-SA 3.0,c97fe292-fd58-4503-af06-80b1895e9644,<regression><time-series><arima>,,2012-04-07 12:48:41.467
69628,23087,5643.0,2,,CC BY-SA 3.0,c97fe292-fd58-4503-af06-80b1895e9644,"This is a basic question on Box-Jenkins MA models. As I understand it, an MA model is basically a linear regression of time-series values $Y$ against previous error terms $e_t,..., e_{t-n}$. That is, the observation $Y$ is first regressed against its previous values $Y_{t-1}, ..., Y_{t-n}$ and then one or more $Y - \hat{Y}$ values are used as the error terms for the MA model.

But how are the error terms calculated in an ARIMA(0, 0, 2) model? If the MA model is used without an autoregressive part and thus no estimated value, how can I possibly have an error term?",,2012-04-07 12:48:41.467
73060,6788,1790.0,5,,CC BY-SA 3.0,a8fcd97d-005b-4620-af03-99dc676537af,"I recently stumbled upon the concept of [**sample complexity**][1], and was wondering if there are any texts, papers or tutorials that provide:

 1. An introduction to the concept
 2. An analysis of the sample complexity of established and popular classification methods or kernel methods.
 3. Advice or information on how to measure it in practice.

Any help with the topic would be greatly appreciated.


  [1]: http://www.google.com/search?q=%22sample%20complexity%22",deleted 4 characters in body,2012-04-29 17:50:04.333
73597,6788,1790.0,5,,CC BY-SA 3.0,ff29749c-cfbd-48f2-ba62-6789ec0a24c0,"I recently stumbled upon the concept of [**sample complexity**][1], and was wondering if there are any texts, papers or tutorials that provide:

 1. An introduction to the concept (rigorous or informal)
 2. An analysis of the sample complexity of established and popular classification methods or kernel methods.
 3. Advice or information on how to measure it in practice.

Any help with the topic would be greatly appreciated.


  [1]: http://www.google.com/search?q=%22sample%20complexity%22",added 23 characters in body,2012-05-02 14:19:06.373
74073,24506,7341.0,1,,CC BY-SA 3.0,3d76502f-1e6d-42d0-b5a7-2ce4d22e779a,Confidence Interval for Values for a Fitted Line,,2012-05-04 17:02:04.137
74074,24506,7341.0,3,,CC BY-SA 3.0,3d76502f-1e6d-42d0-b5a7-2ce4d22e779a,<r><confidence-interval><jmp>,,2012-05-04 17:02:04.137
95592,30864,4890.0,2,,CC BY-SA 3.0,138b717d-80e6-415f-afe9-8938d2a1c11c,"A low rank approximation $\hat{X}$ of $X$ can be decomposed into a matrix square root as $G=U_{r}\lambda_{r}^\frac{1}{2}$ where the eigen decomposition of $X$ is $U\lambda U^T$, thereby reducing the number of features, which can be represented by $G$ based on the rank-r approximation as $\hat{X}=GG^T$. Note that the subscript $r$  represents the number of eigen-vectors and eigen-values used in the approximation. Hence, it does reduce the number of features to represent the data. In some examples low-rank approximations are considered as basis or latent variable(dictionary) based expansions of the original data, under special constraints like orthogonality, non-negativity(non-negative matrix factorization) etc. ",,2012-08-28 00:53:18.257
74371,24602,3662.0,2,,CC BY-SA 3.0,1d2f615d-41f5-427e-afe1-76f1ce974595,"If your using linear regression I would recommend using [the rms package](http://biostat.mc.vanderbilt.edu/wiki/Main/Rrms) in R. It is very easy to use and has lots of nice features. 

Here's an example:

    # Load package (remember to install.packages(""rms"") or this will fail the first time)
    library(rms)
    
    # Get a dataset to experiment with
    data(mtcars)
    mtcars$am <- factor(mtcars$am, levels=0:1, labels=c(""Automatic"", ""Manual""))
    
    # The rms package needs this work properly
    dd <- datadist(mtcars)
    options(datadist=""dd"")
    
    # Do the regression
    f <- ols(mpg~wt, data=mtcars, x=T, y=T)
    
    # Plot regular mean confidence interval
    p <- Predict(f, wt=seq(2.5, 4, by=.001), conf.type=""mean"")
    plot(p, ylim=c(10, 30), col=""lightblue"")
    
    # Plot wide confidence interval
    p <- Predict(f, wt=seq(2.5, 4, by=.001), conf.type=""individual"")
    plot(p, ylim=c(10, 30), col=""lightblue"")

Gives this output:

![Plain line][1]

Now usually you want to test the linearity assumption:

    # Try the model with a restricted cubic spline
    f <- ols(mpg~rcs(wt, 3), data=mtcars, x=T, y=T)
    anova(f)

Gives this output:

    > anova(f)
                    Analysis of Variance          Response: mpg 
    
     Factor     d.f. Partial SS MS         F     P     
     wt          2   922.04230  461.021149 65.54 <.0001
      Nonlinear  1    74.31705   74.317047 10.56 0.0029
     REGRESSION  2   922.04230  461.021149 65.54 <.0001
     ERROR      29   204.00489    7.034651             

And if you plot the graphs with the same code as a bove you get this picture:

![Regression with a spline][2]

If you want to make your formula more complicated just add that variable:

    f <- ols(mpg~rcs(wt, 3)+am, data=mtcars, x=T, y=T)
    p <- Predict(f, wt=seq(2.5, 4, by=.001), am=levels(mtcars$am), conf.type=""individual"")
    plot(p)

I don't know anything about JMP, it shouldn't be too difficult but I recommend learning R because it gives you an incredible freedom. 

Hope this helped.

  [1]: https://i.stack.imgur.com/UAvh7.png
  [2]: https://i.stack.imgur.com/jkftF.png
",,2012-05-06 10:01:28.687
75872,25072,7421.0,2,,CC BY-SA 3.0,6bf1d89b-922e-4d86-bf5e-2095145cefaa,"Before it is pointed, I am aware that a [very similar question was already asked][1]. Still, I am in doubt regarding the concept.

More specifically, it is mentioned by the most voted answer that:

>In terms of a **simple rule of thumb**, I'd suggest that you:

>1. Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.

>2. Run principal components analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.

**Question 1:**

I am having difficulties on understanding based on the results I obtained from R where exactly I am inputing my *theoretical model of latent factors*. I am using the functions from [statsmethods][2]. On both **factanal()** and **princomp()** the inputs were the same: A table where each row represented one data point and the columns consisted of different attributes I was interested on reducing. Thus, this add to my confusion on where is this pre assumed model play its role. I noticed that for factor analysis function I used parallel analysis also suggested by the site using the nScree() function to determine the number of factors and I specified if I wanted a varimax (orthogonal) or promax (oblique) rotation. Is that what is it mean by the model? Being ale to choose the amount of factors and the type of rotation?

**Question 2:** 

I bought a book to study about this from Richard L. Gorsuch. On this book there is something that the author caught attention on the difference between PCA (Principal Component Analysis) and EFA (Exploratory Factor Analysis): It is mentioned that PCA is for **population** while EFA is for **sample**. Is that true? I didn't see that being mentioned on any discussion I read so far. Is it irrelevant? 

**Question 3:** 

I noticed that all those methods seems to impose the normal distribution constraint. I also read that for larger sets this constraint can be ignored. Is that true or PCA, EFA and CFA are sensible to distribution constraint violations?


**Question 4:** Where from the results of PCA and EFA should I note that one is talking about latent factors (EFA) and the other is just clustering on components (factors) the variables? The outputs from R looks the same to me. Is it just the way I perceive what the factors being shown as output? I noted that both show me the table where I can see which I can observe which of my variables are expressed the most of my factors. **What is the difference on the interpretation I should have on which variable belongs to which factor** in respect to PCA and EFA? EFA is saying those with higher expression seems to be more explained by that latent factor while PCA is trying to say that factor is holding those variables from what is it observed?

**Question 5**
Finally the last question is regarding CFA (Confirmatory Factor Analysis). 

On the same function website the following image is being shown:

![Confirmatory Factor Analysis][3]

I read that CFA is usually followed after EFA for hypothesis testing. In that sense, EFA tells you which are the latent factors (which are the output factors) and then you use CFA assuming those factors you observed from EFA for hypothesis testing? 

**Question 6** 

For EFA one of the available rotations on the literature is direct oblimium. I heard that it can accounts for both promax and varimax so 'it takes the best of two words'. Is that true? I am also trying to find a function that employs them on R, since the one suggested on the site does not. I would be happy to get any suggestion on this one.


---

I hope it is noted that this question is way more specific on the doubts regarding EFA and PCA and also adds to CFA so not to get closed for being repeated on the subject. If at least one of the questions is answered I am more than happy too as to clarify the confusion in my head.

Thank you.


  [1]: http://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi
  [2]: http://www.statmethods.net/advstats/factor.html
  [3]: https://i.stack.imgur.com/NNrvh.gif",,2012-05-14 05:55:32.040
75874,25072,7421.0,3,,CC BY-SA 3.0,6bf1d89b-922e-4d86-bf5e-2095145cefaa,<pca><factor-analysis><confirmatory-factor>,,2012-05-14 05:55:32.040
75873,25072,7421.0,1,,CC BY-SA 3.0,6bf1d89b-922e-4d86-bf5e-2095145cefaa,"Differences on Exploratory Factor Analysis, Confirmatory Factor Analysis and Principal Component Analysis",,2012-05-14 05:55:32.040
75875,25072,7421.0,5,,CC BY-SA 3.0,8a8f3f42-91aa-4ffe-a355-a9e3b9b871b0,"Before it is pointed, I am aware that a [very similar question was already asked][1]. Still, I am in doubt regarding the concept.

More specifically, it is mentioned by the most voted answer that:

>In terms of a **simple rule of thumb**, I'd suggest that you:

>1. Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.

>2. Run principal components analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.

**Question 1:**

I am having difficulties on understanding based on the results I obtained from R where exactly I am inputing my *theoretical model of latent factors*. I am using the functions from [statsmethods][2]. On both **factanal()** and **princomp()** the inputs were the same: A table where each row represented one data point and the columns consisted of different attributes I was interested on reducing. Thus, this add to my confusion on where is this pre assumed model play its role. I noticed that for factor analysis function I used parallel analysis also suggested by the site using the nScree() function to determine the number of factors and I specified if I wanted a varimax (orthogonal) or promax (oblique) rotation. Is that what is it mean by the model? Being able to choose the amount of factors and the type of rotation?

**Question 2:** 

I bought a book to study about this from Richard L. Gorsuch. On this book there is something that the author caught attention on the difference between PCA (Principal Component Analysis) and EFA (Exploratory Factor Analysis): It is mentioned that PCA is for **population** while EFA is for **sample**. Is that true? I didn't see that being mentioned on any discussion I read so far. Is it irrelevant? 

**Question 3:** 

I noticed that all those methods seems to impose the normal distribution constraint. I also read that for larger sets this constraint can be ignored. Is that true or PCA, EFA and CFA are sensible to distribution constraint violations?


**Question 4:** Where from the results of PCA and EFA should I note that one is talking about latent factors (EFA) and the other is just clustering on components (factors) the variables? The outputs from R looks the same to me. Is it just the way I perceive what the factors being shown as output? I noted that both show me the table where I can see which I can observe which of my variables are expressed the most of my factors. **What is the difference on the interpretation I should have on which variable belongs to which factor** in respect to PCA and EFA? EFA is saying those with higher expression seems to be more explained by that latent factor while PCA is trying to say that factor is holding those variables from what is it observed?

**Question 5**
Finally the last question is regarding CFA (Confirmatory Factor Analysis). 

On the same function website the following image is being shown:

![Confirmatory Factor Analysis][3]

I read that CFA is usually followed after EFA for hypothesis testing. In that sense, EFA tells you which are the latent factors (which are the output factors) and then you use CFA assuming those factors you observed from EFA for hypothesis testing? 

**Question 6** 

For EFA one of the available rotations on the literature is direct oblimium. I heard that it can accounts for both promax and varimax so 'it takes the best of two words'. Is that true? I am also trying to find a function that employs them on R, since the one suggested on the site does not. I would be happy to get any suggestion on this one.


---

I hope it is noted that this question is way more specific on the doubts regarding EFA and PCA and also adds to CFA so not to get closed for being repeated on the subject. If at least one of the questions is answered I am more than happy too as to clarify the confusion in my head.

Thank you.


  [1]: http://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi
  [2]: http://www.statmethods.net/advstats/factor.html
  [3]: https://i.stack.imgur.com/NNrvh.gif",added 1 characters in body,2012-05-14 06:09:44.487
75876,25072,7421.0,5,,CC BY-SA 3.0,a6360364-d39c-4c01-ba65-1432330c9363,"Before it is pointed, I am aware that a [very similar question was already asked][1]. Still, I am in doubt regarding the concept.

More specifically, it is mentioned by the most voted answer that:

>In terms of a **simple rule of thumb**, I'd suggest that you:

>1. Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.

>2. Run principal components analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.

**Question 1:**

I am having difficulties on understanding based on the results I obtained from R where exactly I am inputing my *theoretical model of latent factors*. I am using the functions from [statsmethods][2]. On both **factanal()** and **princomp()** the inputs were the same: A table where each row represented one data point and the columns consisted of different attributes I was interested on reducing. Thus, this add to my confusion on where is this pre assumed model play its role. I noticed that for factor analysis function I used parallel analysis also suggested by the site using the nScree() function to determine the number of factors and I specified if I wanted a varimax (orthogonal) or promax (oblique) rotation. Is that what is it mean by the model? Being able to choose the amount of factors and the type of rotation?

The results being provided as visual graphs for both PCA and EFA also doesn't seem to highlight this difference which adds to my confusion. Where does this distinction can be observed on them?

![PCA][3]
PCA 

![EFA][4]
EFA

**Question 2:** 

I bought a book to study about this from Richard L. Gorsuch. On this book there is something that the author caught attention on the difference between PCA (Principal Component Analysis) and EFA (Exploratory Factor Analysis): It is mentioned that PCA is for **population** while EFA is for **sample**. Is that true? I didn't see that being mentioned on any discussion I read so far. Is it irrelevant? 

**Question 3:** 

I noticed that all those methods seems to impose the normal distribution constraint. I also read that for larger sets this constraint can be ignored. Is that true or PCA, EFA and CFA are sensible to distribution constraint violations?


**Question 4:** Where from the results of PCA and EFA should I note that one is talking about latent factors (EFA) and the other is just clustering on components (factors) the variables? The outputs from R looks the same to me. Is it just the way I perceive what the factors being shown as output? I noted that both show me the table where I can see which I can observe which of my variables are expressed the most of my factors. **What is the difference on the interpretation I should have on which variable belongs to which factor** in respect to PCA and EFA? EFA is saying those with higher expression seems to be more explained by that latent factor while PCA is trying to say that factor is holding those variables from what is it observed?

**Question 5**
Finally the last question is regarding CFA (Confirmatory Factor Analysis). 

On the same function website the following image is being shown:

![Confirmatory Factor Analysis][5]

I read that CFA is usually followed after EFA for hypothesis testing. In that sense, EFA tells you which are the latent factors (which are the output factors) and then you use CFA assuming those factors you observed from EFA for hypothesis testing? 

**Question 6** 

For EFA one of the available rotations on the literature is direct oblimium. I heard that it can accounts for both promax and varimax so 'it takes the best of two words'. Is that true? I am also trying to find a function that employs them on R, since the one suggested on the site does not. I would be happy to get any suggestion on this one.


---

I hope it is noted that this question is way more specific on the doubts regarding EFA and PCA and also adds to CFA so not to get closed for being repeated on the subject. If at least one of the questions is answered I am more than happy too as to clarify the confusion in my head.

Thank you.


  [1]: http://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi
  [2]: http://www.statmethods.net/advstats/factor.html
  [3]: https://i.stack.imgur.com/SGK56.jpg
  [4]: https://i.stack.imgur.com/Eg4MN.jpg
  [5]: https://i.stack.imgur.com/NNrvh.gif",added few images to highlight the doubts,2012-05-14 06:17:10.343
75877,25072,,4,user88,CC BY-SA 3.0,dc2b5e8c-78d1-4bac-ab88-b383a27a4ab8,"Differences on exploratory factor analysis, confirmatory factor analysis and principal component analysis",edited title,2012-05-14 07:04:25.323
75924,25087,8363.0,2,,CC BY-SA 3.0,c7fff30b-43b7-429b-9cd5-7187003b5008,I will just address question 2.  I have some doubts about how well the author knows his subject if he really said it the way you have presented it.  PCA is applied to the sample just like EFA and CFA.  It simply takes a list of n possibly related factors looks at how the points (samples) scatter in n-dimension space and then gets the first principal component as the linear combination that explains more of the variability in the data than any other linear combination.  Then the second looks at orthogonal directions to the first to find theone out of those that explains the most of the remaining variability and so on with the 3rd and 4th.  So sometimes one can take just 1-3 components to describe most of the variation in the data.  That is why factor analysis and principal componet analysis are described according to 1 and 2 in your statement.,,2012-05-14 14:04:26.687
76761,25072,7421.0,5,,CC BY-SA 3.0,23930539-bc39-4796-ab75-2b19cc5cc9c7,"Before it is pointed, I am aware that a [very similar question was already asked][1]. Still, I am in doubt regarding the concept.

More specifically, it is mentioned by the most voted answer that:

>In terms of a **simple rule of thumb**, I'd suggest that you:

>1. Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.

>2. Run principal components analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables. 

**Question 1:**

I am having difficulties on understanding based on the results I obtained from R where exactly I am inputing my *theoretical model of latent factors*. I am using the functions from [statsmethods][2]. On both **factanal()** and **princomp()** the inputs were the same: A table where each row represented one data point and the columns consisted of different attributes I was interested on reducing. Thus, this add to my confusion on where is this pre assumed model play its role. I noticed that for factor analysis function I used parallel analysis also suggested by the site using the nScree() function to determine the number of factors and I specified if I wanted a varimax (orthogonal) or promax (oblique) rotation. Is that what is it mean by the model? Being able to choose the amount of factors and the type of rotation?

The results being provided as visual graphs for both PCA and EFA also doesn't seem to highlight this difference which adds to my confusion. Where does this distinction can be observed on them?

![PCA][3]
PCA 

![EFA][4]
EFA

**Question 2:** -- Answered

I bought a book to study about this from Richard L. Gorsuch. On this book there is something that the author caught attention on the difference between PCA (Principal Component Analysis) and EFA (Exploratory Factor Analysis): It is mentioned that PCA is for **population** while EFA is for **sample**. Is that true? I didn't see that being mentioned on any discussion I read so far. Is it irrelevant? 

**Question 3:** 

I noticed that all those methods seems to impose the normal distribution constraint. I also read that for larger sets this constraint can be ignored. Is that true or PCA, EFA and CFA are sensible to distribution constraint violations?


**Question 4:** Where from the results of PCA and EFA should I note that one is talking about latent factors (EFA) and the other is just clustering on components (factors) the variables? The outputs from R looks the same to me. Is it just the way I perceive what the factors being shown as output? I noted that both show me the table where I can see which I can observe which of my variables are expressed the most of my factors. **What is the difference on the interpretation I should have on which variable belongs to which factor** in respect to PCA and EFA? EFA is saying those with higher expression seems to be more explained by that latent factor while PCA is trying to say that factor is holding those variables from what is it observed?

**Question 5**
Finally the last question is regarding CFA (Confirmatory Factor Analysis). 

On the same function website the following image is being shown:

![Confirmatory Factor Analysis][5]

I read that CFA is usually followed after EFA for hypothesis testing. In that sense, EFA tells you which are the latent factors (which are the output factors) and then you use CFA assuming those factors you observed from EFA for hypothesis testing? 

**Question 6** 

For EFA one of the available rotations on the literature is direct oblimium. I heard that it can accounts for both promax and varimax so 'it takes the best of two words'. Is that true? I am also trying to find a function that employs them on R, since the one suggested on the site does not. I would be happy to get any suggestion on this one.


---

I hope it is noted that this question is way more specific on the doubts regarding EFA and PCA and also adds to CFA so not to get closed for being repeated on the subject. If at least one of the questions is answered I am more than happy too as to clarify the confusion in my head.

Thank you.


  [1]: http://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi
  [2]: http://www.statmethods.net/advstats/factor.html
  [3]: https://i.stack.imgur.com/SGK56.jpg
  [4]: https://i.stack.imgur.com/Eg4MN.jpg
  [5]: https://i.stack.imgur.com/NNrvh.gif",added 12 characters in body,2012-05-18 19:22:13.883
76764,10069,2666.0,5,,CC BY-SA 3.0,9acb4028-2693-4cd1-af47-24768fb023a9,"In my experience, not only is it necessary to have all lower order effects in the model when they are connected to higher order effects, but it is also important to properly model (e.g., allowing to be nonlinear) main effects that are seemingly unrelated to the factors in the interactions of interest.  That's because interactions between x1 and x2 can be stand-ins for main effects of x3 and x4.  Interactions sometimes *seem* to be needed because they are collinear with omitted variables or omitted nonlinear (e.g., spline) terms.",added 8 characters in body,2012-05-18 19:29:16.353
79109,26070,5911.0,2,,CC BY-SA 3.0,0530a116-5968-42eb-9c51-f5b323f99168,I have seen somewhere that classical distances (like euclidean distance) becomes weekly discriminant when we have multidimensional and sparse data. Why ? Do you have an example of two sparse data vectors where the distance do not performs well ? In this case which similarity should we use ?,,2012-06-01 13:55:13.253
79110,26070,5911.0,1,,CC BY-SA 3.0,0530a116-5968-42eb-9c51-f5b323f99168,Euclidean distance is usually not good for sparse data?,,2012-06-01 13:55:13.253
79111,26070,5911.0,3,,CC BY-SA 3.0,0530a116-5968-42eb-9c51-f5b323f99168,<clustering><dataset><data-mining><sparse><euclidean>,,2012-06-01 13:55:13.253
79128,26070,668.0,6,,CC BY-SA 3.0,1666741c-0682-467a-a68f-f4557767bbcb,<clustering><data-mining><sparse><euclidean>,added 7 characters in body; edited tags,2012-06-01 15:30:45.033
79127,26070,668.0,5,,CC BY-SA 3.0,1666741c-0682-467a-a68f-f4557767bbcb,I have seen somewhere that classical distances (like Euclidean distance) become weakly discriminant when we have multidimensional and sparse data. Why? Do you have an example of two sparse data vectors where the Euclidean distance does not perform well? In this case which similarity should we use?,added 7 characters in body; edited tags,2012-06-01 15:30:45.033
79262,26070,,25,,,20af2220-ff35-46b9-96ca-49f1b2005d47,,http://twitter.com/#!/StackStats/status/208879359052886016,2012-06-02 11:14:54.757
80964,26657,190.0,2,,CC BY-SA 3.0,15956cab-12e6-4234-8f3e-0d8be45ac328,"Here is a simple toy example illustrating the effect of dimensionality in a discrimination problem. 

**Framework** Assume that $\xi$ is a gaussian vector with mean $\nu$ and diagonal covariance $\sigma Id$ ($\sigma$ is known) and that you want to test the simple hypothesis 

$$H_0: \;\nu=0,\; Vs \; H_{\theta}: \; \nu=\theta $$
(for a given $\theta\in \mathbb{R}^n$) $\theta$ is not necessarily known in advance. 

**Test statistic with energy**. The intuition you certainlly have is that it is a good idea to evaluate the norm/energy $\mathcal{E}_n=\frac{1}{n}\sum_{i=1}^n\xi_i^2$ of you observation $\xi$ to build a test statistic.  Actually you can construct a standardized centered (under $H_0$) version $T_n$ of the energy $T_n=\frac{\sum_i\xi_i^2-\sigma^2}{\sqrt{2n\sigma^4}}$. That makes a critical region at level $\alpha$ of the form $\{T_n\geq v_{1-\alpha}\}$ for a well chosen $v_{1-\alpha}$



**Power of the test and dimension.** In this case it is an easy probabiliy exercice to show the following formula for the power of your test: 

>  $$P_{\theta}(T\leq v_{1-\alpha})=P\left (Z\leq \frac{v_{1-\alpha}}{\sqrt{1+2\|\theta\|_2^2/(n\sigma^2)}}-\frac{\|\theta\|^2_2}{\sqrt{2n\sigma^4+2\sigma^2\|\theta\|_2^2/(n\sigma^2)}}\right )$$
> with $Z$ a sum of $n$ iid random variables with $\mathbb{E}[Z]=0$ and $Var(Z)=1$. 

This means that the power of your test is increased by the energy of your signal $\|\theta\|^2_2$ and decreased by $n$. 

**Toward a test with a thresholded statistic.** If you do not have much energy in your signal but if you know a linear traformation that can help you to have this energy concentrated in a small part of your signal, then you can build a test statistic that will only evaluate the energy for the small part of your signal. If you known in advance where it is concentrated (for example you known there cannot be high frequencies in your signal) then you can obtain a power in the preceding test with $n$ replaced by a small number and $\|\theta\|^2_2$ allmost the same... If you do not know it in advance you have to estimate it this leads to well known thresholding tests. 

Note that this argument is exactly at the root many papers such as 

 - A Antoniadis, F Abramovich, T Sapatinas, and B Vidakovic. Wavelet methods for testing
in functional analysis of variance models. International Journal on Wavelets and its
applications, 93 :1007–1021, 2004.
 - M. V. Burnashef and Begmatov. On a problem of signal detection leading to stable distribution. Theory of probability and its applications, 35(3) :556–560, 1990.
 - Y. Baraud. Non asymptotic minimax rate of testing in signal detection. Bernoulli, 8 :577–606, 2002.
 - J Fan. Test of significance based on wavelet thresholding and neyman’s truncation. JASA,
91 :674–688, 1996.
 - J. Fan and S-K Lin. Test of significance when data are curves. JASA, 93 :1007–1021, 1998.
 - V. Spokoiny. Adaptative hypothesis testing using wavelets. Annals of Statistics, 24(6) :2477–2498, december 1996.
",,2012-06-12 09:23:27.760
81125,26657,190.0,5,,CC BY-SA 3.0,1ba58f6d-8d11-4ba8-8fc7-f3f092a0cd6a,"Here is a simple toy example illustrating the effect of dimensionality in a discrimination problem. 

**Heuristic.**  The key issue here is that the Euclidian norm gives the same importance to any direction. This constitutes a lack of a priori, and as you certainly know in high dimension there is no free lunch (i.e. if you don't known what you are searching for, then why noise is not what you are searching for). I would say that for any problem there is a limit of information that is necessary to find something else than noise. This limit is related somehow to the ""size"" of the area you are trying to explore with regard to the ""noise"" level (i.e. level of uninformative content). In high dimension if you have the a priori that your signal is sparse then you can remove non sparse vector with metric that fill the space with sparse vector or by using thresholding. 

**Framework** Assume that $\xi$ is a gaussian vector with mean $\nu$ and diagonal covariance $\sigma Id$ ($\sigma$ is known) and that you want to test the simple hypothesis 

$$H_0: \;\nu=0,\; Vs \; H_{\theta}: \; \nu=\theta $$
(for a given $\theta\in \mathbb{R}^n$) $\theta$ is not necessarily known in advance. 

**Test statistic with energy**. The intuition you certainlly have is that it is a good idea to evaluate the norm/energy $\mathcal{E}_n=\frac{1}{n}\sum_{i=1}^n\xi_i^2$ of you observation $\xi$ to build a test statistic.  Actually you can construct a standardized centered (under $H_0$) version $T_n$ of the energy $T_n=\frac{\sum_i\xi_i^2-\sigma^2}{\sqrt{2n\sigma^4}}$. That makes a critical region at level $\alpha$ of the form $\{T_n\geq v_{1-\alpha}\}$ for a well chosen $v_{1-\alpha}$



**Power of the test and dimension.** In this case it is an easy probabiliy exercice to show the following formula for the power of your test: 

>  $$P_{\theta}(T\leq v_{1-\alpha})=P\left (Z\leq \frac{v_{1-\alpha}}{\sqrt{1+2\|\theta\|_2^2/(n\sigma^2)}}-\frac{\|\theta\|^2_2}{\sqrt{2n\sigma^4+2\sigma^2\|\theta\|_2^2/(n\sigma^2)}}\right )$$
> with $Z$ a sum of $n$ iid random variables with $\mathbb{E}[Z]=0$ and $Var(Z)=1$. 

This means that the power of your test is increased by the energy of your signal $\|\theta\|^2_2$ and decreased by $n$. 

**Toward a test with a thresholded statistic.** If you do not have much energy in your signal but if you know a linear traformation that can help you to have this energy concentrated in a small part of your signal, then you can build a test statistic that will only evaluate the energy for the small part of your signal. If you known in advance where it is concentrated (for example you known there cannot be high frequencies in your signal) then you can obtain a power in the preceding test with $n$ replaced by a small number and $\|\theta\|^2_2$ allmost the same... If you do not know it in advance you have to estimate it this leads to well known thresholding tests. 

Note that this argument is exactly at the root many papers such as 

 - A Antoniadis, F Abramovich, T Sapatinas, and B Vidakovic. Wavelet methods for testing
in functional analysis of variance models. International Journal on Wavelets and its
applications, 93 :1007–1021, 2004.
 - M. V. Burnashef and Begmatov. On a problem of signal detection leading to stable distribution. Theory of probability and its applications, 35(3) :556–560, 1990.
 - Y. Baraud. Non asymptotic minimax rate of testing in signal detection. Bernoulli, 8 :577–606, 2002.
 - J Fan. Test of significance based on wavelet thresholding and neyman’s truncation. JASA,
91 :674–688, 1996.
 - J. Fan and S-K Lin. Test of significance when data are curves. JASA, 93 :1007–1021, 1998.
 - V. Spokoiny. Adaptative hypothesis testing using wavelets. Annals of Statistics, 24(6) :2477–2498, december 1996.
",added 775 characters in body,2012-06-13 06:03:39.853
81702,412,668.0,38,,,6aaaae68-e8ba-499c-a44c-f070d563bca3,"[{""Id"":919,""DisplayName"":""whuber""}]",from http://stats.stackexchange.com/questions/30528/concise-book-tutorial-for-statistics-data-analysis,2012-06-15 15:18:35.453
82550,27120,568.0,2,,CC BY-SA 3.0,2f33f795-f5a9-4ceb-aa56-4458ebfbf8de,"As a non-native english speaker I was wondering which of the **square** or **squared** expression I should use. For instance in mean **square** error or mean **squared** error.

According to the Internet, it seems both forms are used indistinctly. Is one expression more square than the other ?",,2012-06-20 16:41:10.737
82551,27120,568.0,1,,CC BY-SA 3.0,2f33f795-f5a9-4ceb-aa56-4458ebfbf8de,Mean square error or mean squared error,,2012-06-20 16:41:10.737
82552,27120,568.0,3,,CC BY-SA 3.0,2f33f795-f5a9-4ceb-aa56-4458ebfbf8de,<mse>,,2012-06-20 16:41:10.737
82556,27120,668.0,6,,CC BY-SA 3.0,57763cab-db74-4cc2-ab81-658839b63241,<terminology><mse>,edited tags,2012-06-20 16:54:18.370
95611,30862,,25,,,9ec383bd-1259-45c3-be63-5a04a7b8cfb8,,http://twitter.com/#!/StackStats/status/240282770423832579,2012-08-28 03:00:51.527
126223,40121,14728.0,2,,CC BY-SA 3.0,08a3103d-cd12-49f2-9aa8-2b4d408b4ec8,"I'm trying to compare some experiment data using JMP. In the `Compare Means` function, there are different tests such as `Each Pair, Student’s t` and `All Pairs, Tukey HSD`, which gives circles of different radius. What's the difference between the tests and how are these radius calculated? I found this [help file][1] but it did not answer my question.


  [1]: http://www.jmp.com/support/help/Additional_Examples_of_the_Oneway_Platform.shtml",,2013-02-10 03:03:22.580
82590,27132,668.0,2,,CC BY-SA 3.0,2205616d-4ab0-4890-ab4e-e1c1d8d5e4be,"**The conceptual uses of ""square"" and ""squared"" are subtly different,** although interchangeable:

* ""Squared"" refers to the past *action* of taking or computing the second power.  E.g., $x^2$ is usually read as ""x-squared,"" not ""x-square."" (The latter is sometimes encountered but I suspect it results from speakers who are accustomed to clipping their phrases or who just haven't heard the terminal dental in ""x-squared."")

* ""Square"" refers to the *result* of taking the second power.  E.g., $x^2$ can be referred to as the ""square of x.""  (The illocution ""squared of x"" is never used.)

These suggest that a person using a phrase like ""mean squared error"" is thinking in terms of a *computation*: take the errors, square them, average those.  The phrase ""mean square error"" has a more conceptual feel to it: average the square errors.  The user of this phrase may be thinking in terms of square errors rather than the errors themselves.  I believe this shows up especially in theoretical literature where the second form, ""square,"" appears more often (I believe: I haven't systematically checked).

Obviously both are equivalent in function and safely interchangeable in practice.  It is interesting, though, that some careful Google queries give substantially different hit counts.  Presently, 

    ""mean squared"" -square -root -Einstein -Relativity

returns about 367,000 results (notice the necessity of ruling out the phrase ""$e=m c^2$"" popularly quoted in certain contexts, which demands the use of ""squared"" instead of ""square"" when written out), while

    ""mean square"" -squared -root  -Einstein -Relativity

(maintaining analogous exclusions for comparability) returns an order of magnitude more, at 3.47 million results.  This (weakly) suggests people favor ""mean square"" over ""mean squared,"" but don't take this too much to heart: ""mean squared"" is used in official SAS documentation, for instance.",,2012-06-20 20:29:04.380
82614,27132,668.0,5,,CC BY-SA 3.0,6b2395e3-2026-4d16-9020-57b702a39355,"**The conceptual uses of ""square"" and ""squared"" are subtly different,** although (almost) interchangeable:

* ""Squared"" refers to the past *action* of taking or computing the second power.  E.g., $x^2$ is usually read as ""x-squared,"" not ""x-square."" (The latter is sometimes encountered but I suspect it results from speakers who are accustomed to clipping their phrases or who just haven't heard the terminal dental in ""x-squared."")

* ""Square"" refers to the *result* of taking the second power.  E.g., $x^2$ can be referred to as the ""square of x.""  (The illocution ""squared of x"" is never used.)

These suggest that a person using a phrase like ""mean squared error"" is thinking in terms of a *computation*: take the errors, square them, average those.  The phrase ""mean square error"" has a more conceptual feel to it: average the square errors.  The user of this phrase may be thinking in terms of square errors rather than the errors themselves.  I believe this shows up especially in theoretical literature where the second form, ""square,"" appears more often (I believe: I haven't systematically checked).

Obviously both are equivalent in function and safely interchangeable in practice.  It is interesting, though, that some careful Google queries give substantially different hit counts.  Presently, 

    ""mean squared"" -square -root -Einstein -Relativity

returns about 367,000 results (notice the necessity of ruling out the phrase ""$e=m c^2$"" popularly quoted in certain contexts, which demands the use of ""squared"" instead of ""square"" when written out), while

    ""mean square"" -squared -root  -Einstein -Relativity

(maintaining analogous exclusions for comparability) returns an order of magnitude more, at 3.47 million results.  This (weakly) suggests people favor ""mean square"" over ""mean squared,"" but don't take this too much to heart: ""mean squared"" is used in official SAS documentation, for instance.",added 9 characters in body,2012-06-21 01:36:39.797
82789,27194,1085.0,1,,CC BY-SA 3.0,8ff84a0c-ae79-4626-9587-e5d450bd6482,what does that mean that 2 time series are colinear (or collinear)?,,2012-06-22 02:20:37.490
82788,27194,1085.0,2,,CC BY-SA 3.0,8ff84a0c-ae79-4626-9587-e5d450bd6482,"I am familiar with the concept of cointegration.

But I hear sometimes people talking about colinearity (or collinearity) for time series.
A set of points is collinear if they are on the same line. But what does that mean for time series?

Is it exactly the same as cointegration of order 1?
Or is there something stronger/different in the concept of collinearity?",,2012-06-22 02:20:37.490
82787,27194,1085.0,3,,CC BY-SA 3.0,8ff84a0c-ae79-4626-9587-e5d450bd6482,<cointegration><multicollinearity>,,2012-06-22 02:20:37.490
82812,27194,674.0,6,,CC BY-SA 3.0,c55e067f-4673-4b7c-92e0-1afbcaeeda95,<time-series><cointegration><multicollinearity>,edited tags; edited title,2012-06-22 07:49:51.370
82811,27194,674.0,4,,CC BY-SA 3.0,c55e067f-4673-4b7c-92e0-1afbcaeeda95,What does that mean that two time series are colinear?,edited tags; edited title,2012-06-22 07:49:51.370
82951,27194,,25,,,5107e24f-1ca2-4d33-a388-cec6427ac4d2,,http://twitter.com/#!/StackStats/status/216252810881282050,2012-06-22 19:34:22.503
87118,16366,,6,,CC BY-SA 3.0,9b4a1ac0-ffdf-4f09-9922-6f94c0ffd96c,<text-mining><topic-models>,edited tags,2012-07-15 20:58:58.220
90550,4714,60.0,5,,CC BY-SA 3.0,85734fde-3036-4b17-b66f-acbf39d8805a,Reverend [Thomas Bayes](http://en.wikipedia.org/wiki/Thomas_Bayes) for discovering Bayes' theorem,added 4 characters in body,2012-08-02 03:09:39.773
91184,1248,,25,,,7ad766be-e930-45b3-a1f0-fa155b7770f8,,http://twitter.com/#!/StackStats/status/232388024548339712,2012-08-06 08:09:57.143
93684,541,5237.0,5,,CC BY-SA 3.0,a9188445-f779-419a-a7b0-8470be848d3a,"ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression. 

In light of their equivalence, is there any reason why ANOVA is used instead of linear regression? 

Note: I am particularly interested in hearing about **technical** reasons for the use of ANOVA instead of linear regression. 

**Edit**

Here is one example using one-way ANOVA. Suppose, you want to know if the average height of male and females is the same. To test for your hypothesis you would collect data from a random   sample of male and females (say 30 each) and perform the ANOVA analysis (i.e., sum of squares for gender and error) to decide whether an effect exists.

You could also use linear regression to test for this as follows:

Define:  $\text{Gender} = 1$ if respondent is a male and $0$ otherwise.
$$
\text{Height} = \text{Intercept} + \beta * \text{Gender} + \text{error}
$$
where: $\text{error}\sim\mathcal N(0,\sigma^2)$

Then a test of whether $\beta = 0$ is a an equivalent test for your hypothesis.",added mathjax,2012-08-18 16:21:38.253
94058,30434,9605.0,2,,CC BY-SA 3.0,6d4d3822-fb7e-41d1-983b-98cba880abe7,"This is indeed something often glossed over.

Some people are doing something a bit cheeky: holding out a proportion of the words in each document, and giving using predictive probabilities of these held-out words given the document-topic mixtures as well as the topic-word mixtures. This is obviously not ideal as it doesn't evaluate performance on any held-out documents.

To do it properly with held-out documents you do as suggested need to ""integrate over the Dirichlet prior for all possible topic mixtures"". http://people.cs.umass.edu/~wallach/talks/evaluation.pdf reviews a few methods for tackling this slightly unpleasant integral. I'm just about to try and implement this myself in fact, so good luck!",,2012-08-20 14:56:05.707
95589,30862,1805.0,1,,CC BY-SA 3.0,ce4200a8-b09a-418d-aa4f-4da3f8770a83,Why bother with low rank approximations?,,2012-08-28 00:12:57.667
95590,30862,1805.0,3,,CC BY-SA 3.0,ce4200a8-b09a-418d-aa4f-4da3f8770a83,<r><matrix>,,2012-08-28 00:12:57.667
95588,30862,1805.0,2,,CC BY-SA 3.0,ce4200a8-b09a-418d-aa4f-4da3f8770a83,"If you have a matrix with n rows and m columns, you can use SVD or other methods to calculate a [low-rank approximation][1] of the given matrix.

However, the low rank approximation will still have n rows and m columns. How can low-rank-approximations be useful for machine learning and natural language processing, given that you are left with the same number of features?


  [1]: http://en.wikipedia.org/wiki/Low-rank_approximation",,2012-08-28 00:12:57.667
104635,33598,,6,user10525,CC BY-SA 3.0,c8b4acc1-5c74-4c86-bb89-b20e3917ffa3,<interpretation><chow-test>,Attach image and related tag,2012-10-16 09:32:48.907
131932,41914,13918.0,3,,CC BY-SA 3.0,dd90e082-7edf-4155-b4dd-caff8a793247,<estimation><forecasting>,,2013-03-12 11:17:01.283
95694,30864,4890.0,5,,CC BY-SA 3.0,91755515-9824-400a-92e9-f0a2c70617d7,"A low rank approximation $\hat{X}$ of $X$ can be decomposed into a matrix square root as $G=U_{r}\lambda_{r}^\frac{1}{2}$ where the eigen decomposition of $X$ is $U\lambda U^T$, thereby reducing the number of features, which can be represented by $G$ based on the rank-r approximation as $\hat{X}=GG^T$. Note that the subscript $r$  represents the number of eigen-vectors and eigen-values used in the approximation. Hence, it does reduce the number of features to represent the data. In some examples low-rank approximations are considered as basis or latent variable (dictionary) based expansions of the original data, under special constraints like orthogonality, non-negativity (non-negative matrix factorization) etc.",added 1 characters in body,2012-08-28 14:22:41.110
95922,30957,9446.0,3,,CC BY-SA 3.0,c4ff9d11-fdf7-4b61-9012-7ba7a6c1758c,<time-series><forecasting><simulation><arima>,,2012-08-29 14:56:24.737
95920,30957,9446.0,2,,CC BY-SA 3.0,c4ff9d11-fdf7-4b61-9012-7ba7a6c1758c,"I have a fairly long time-series of annual abundances ($N_t$) of a wildlife species (73 years of abundances).  To forecast the population’s trajectory, I have used ARIMA modeling.  Examination of the ACF and PACF of the first-order differenced time-series suggested a 10-year cycle exists.  So I used a span 10 seasonal difference to account for this periodic pattern.  Therefore, the response variable was:
$$
Y_t=(\sqrt{N_t}-\sqrt{N_{t-1}})-(\sqrt{N_{t-10}}-\sqrt{N_{t-11}})
$$
Typically, I would have used a logarithmic transformation but it resulted in heteroscedastic residuals.  Examination of the ACF and PACF of $Y_t$ indicated a multiplicative seasonal structure so I fit the model:
$$
ARIMA(0,1,1)(0,1,1)_{10}
$$
using the Forecast Package in `R`....`library(forecast)`.

Example code for fitting the model:

    m1=Arima(y,order=c(0,1,1),seasonal=list(order=c(0,1,1),period=10),include.mean=FALSE)

The residuals of this model were normally distributed, not autocorrelated, and homoscedastic.

I have been using the fitted model from above for some additional simulation work using the `simulate.Arima` function.  However, I would like to initialize the simulation with a different time-series.  The `arima.sim` function allows this but the `arima.sim` function doesn't seem to handle seasonal ARIMA models.  With the `simulate.Arima` function one can use the `future=TRUE` option to simulate values that are ""future to and conditional on the data"" in the model `m1`.  Can the data in the model object `m1` simply be replaced to create a simulation that is conditional on different data?

For example:

    # Create a new model object for simulation.
    m.sim=m1
    # Replace the data in the model object with the new data.
    m.sim$x=new
    # Simulation conditional on the new data.
    sim.forecasts=replicate(1000,simulate.Arima(m.sim,future=TRUE,bootstrap=TRUE))",,2012-08-29 14:56:24.737
95921,30957,9446.0,1,,CC BY-SA 3.0,c4ff9d11-fdf7-4b61-9012-7ba7a6c1758c,Initialize ARIMA simulations with different time-series,,2012-08-29 14:56:24.737
95934,30960,132.0,2,,CC BY-SA 3.0,3ff51ceb-268a-40aa-ac58-9fe8d021bafd,"You can ""fit"" the model to different data and then simulate:

    m2 <- Arima(z,model=m1)
    simulate.Arima(m2,future=TRUE,bootstrap=TRUE)

`m2` will have the same parameters as `m1` (they are not re-estimated), but the residuals, etc., are computed on the new data.

However, I am concerned with your model. Seasonal models are for when the seasonality is fixed and known. With animal population data, you almost certainly have aperiodic population cycling. This is a well-known phenomenon and can easily be handled with non-seasonal ARIMA models. Look at the literature on the Canadian lynx data for discussion.

By all means, use the square root, but then I would use a non-seasonal ARIMA model. Provided the AR order is greater than 1, it is possible to have cycles. See 

You can do all this in one step:

    m1 <- auto.arima(y, lambda=0.5)

Then proceed with your simulations as above.
",,2012-08-29 17:25:54.740
95964,30957,9446.0,6,,CC BY-SA 3.0,0a2d2468-c0dc-4c33-8d45-5001fd329475,<time-series><forecasting><simulation><arima><ecology>,edited tags,2012-08-29 21:08:35.373
96965,2509,668.0,38,,,8e86171a-d1d1-4947-a6e0-d478293a0868,"[{""Id"":919,""DisplayName"":""whuber""}]",from http://stats.stackexchange.com/questions/35647/how-are-eigenvectors-and-principal-components-related,2012-09-04 12:24:34.043
98179,31575,6404.0,1,,CC BY-SA 3.0,3e31d65c-12e2-4dd1-b090-0669c1336496,Estimating Markov Transition Probabilities from sequence data,,2012-09-11 15:29:12.027
98180,31575,6404.0,3,,CC BY-SA 3.0,3e31d65c-12e2-4dd1-b090-0669c1336496,<r><matlab><markov-process>,,2012-09-11 15:29:12.027
98178,31575,6404.0,2,,CC BY-SA 3.0,3e31d65c-12e2-4dd1-b090-0669c1336496,"I have a full set of sequences (432 observations to be precise) of 4 states $A-D$: eg

$$Q=\left(\begin{array}{c c c c c}
A& C& D&D  & B\\
B& A& A&C & A\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
B& C& A&D & A\\
    \end{array}\right)$$

Is there a way of calculating the transition matrix $$P_{ij}(Y_{t}=j|Y_{t-1}=i)$$ in Matlab or R or similar?  I think the HMM package might help.  Any thoughts?",,2012-09-11 15:29:12.027
98185,31575,6404.0,5,,CC BY-SA 3.0,fbdf5795-cef6-4cf0-a73e-b9a654dc1ae8,"I have a full set of sequences (432 observations to be precise) of 4 states $A-D$: eg

$$Y=\left(\begin{array}{c c c c c}
A& C& D&D  & B\\
B& A& A&C & A\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
B& C& A&D & A\\
    \end{array}\right)$$

Is there a way of calculating the transition matrix $$P_{ij}(Y_{t}=j|Y_{t-1}=i)$$ in Matlab or R or similar?  I think the HMM package might help.  Any thoughts?

eg: http://stats.stackexchange.com/questions/14360/estimating-markov-chain-probabilities",added 92 characters in body,2012-09-11 15:37:08.833
98207,31587,7007.0,2,,CC BY-SA 3.0,fa47d38a-ff7d-4060-96cc-07de9ab271b7,"Please, check the comments above. Here is a quick implementation in R.

    x <- c(1,2,1,1,3,4,4,1,2,4,1,4,3,4,4,4,3,1,3,2,3,3,3,4,2,2,3)
    p <- matrix(nrow = 4, ncol = 4, 0)
    for (t in 1:(length(x) - 1)) p[x[t], x[t + 1]] <- p[x[t], x[t + 1]] + 1
    for (i in 1:4) p[i, ] <- p[i, ] / sum(p[i, ])

Results:

    > p
              [,1]      [,2]      [,3]      [,4]
    [1,] 0.1666667 0.3333333 0.3333333 0.1666667
    [2,] 0.2000000 0.2000000 0.4000000 0.2000000
    [3,] 0.1428571 0.1428571 0.2857143 0.4285714
    [4,] 0.2500000 0.1250000 0.2500000 0.3750000

P.S. Does anyone know why R doesn't have a `++` like operator?",,2012-09-11 17:05:10.090
98218,31575,6404.0,5,,CC BY-SA 3.0,bf8763a4-fe47-4841-9bb9-c107d3fc60f4,"I have a full set of sequences (432 observations to be precise) of 4 states $A-D$: eg

$$Y=\left(\begin{array}{c c c c c c c}
A& C& D&D  & B & A &C\\
B& A& A&C & A&- &-\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
B& C& A&D & A & B & A\\
    \end{array}\right)$$

**EDIT**:  The observation sequences are of unequal lengths! Does this change anything?

Is there a way of calculating the transition matrix $$P_{ij}(Y_{t}=j|Y_{t-1}=i)$$ in Matlab or R or similar?  I think the HMM package might help.  Any thoughts?

eg: http://stats.stackexchange.com/questions/14360/estimating-markov-chain-probabilities",added 129 characters in body,2012-09-11 18:08:32.263
98232,31575,,4,user88,CC BY-SA 3.0,9a82b108-2b89-444f-b582-1cbd1948627f,Estimating Markov transition probabilities from sequence data,edited title,2012-09-11 19:44:59.607
126180,40104,14684.0,1,,CC BY-SA 3.0,fd79b115-4bed-4397-9e4c-70d83be1859a,The sum of two independent Poisson random variables,,2013-02-09 19:31:13.290
98233,31587,7007.0,5,,CC BY-SA 3.0,2a0d8cfe-809f-4c73-bc7e-f760847ef3cd,"Please, check the comments above. Here is a quick implementation in R.

    x <- c(1,2,1,1,3,4,4,1,2,4,1,4,3,4,4,4,3,1,3,2,3,3,3,4,2,2,3)
    p <- matrix(nrow = 4, ncol = 4, 0)
    for (t in 1:(length(x) - 1)) p[x[t], x[t + 1]] <- p[x[t], x[t + 1]] + 1
    for (i in 1:4) p[i, ] <- p[i, ] / sum(p[i, ])

Results:

    > p
              [,1]      [,2]      [,3]      [,4]
    [1,] 0.1666667 0.3333333 0.3333333 0.1666667
    [2,] 0.2000000 0.2000000 0.4000000 0.2000000
    [3,] 0.1428571 0.1428571 0.2857143 0.4285714
    [4,] 0.2500000 0.1250000 0.2500000 0.3750000

A (probably dumb) implementation in MATLAB (which I have never used, so I don't know if this is going to work. I've just googled ""declare vector matrix MATLAB"" to get the syntax):

    x = [ 1, 2, 1, 1, 3, 4, 4, 1, 2, 4, 1, 4, 3, 4, 4, 4, 3, 1, 3, 2, 3, 3, 3, 4, 2, 2, 3 ]

    n = size(x)

    p = [ 0, 0, 0, 0; 0, 0, 0, 0; 0, 0, 0, 0; 0, 0, 0, 0 ]

    for t = 1:n
      p(x(t), x(t + 1)) = p(x(t), x(t + 1)) + 1
    end

    for i = 1:4
      p(i, :) = p(i, :) / sum(p(i, :))
    end

P.S. Does anyone know why R doesn't have a `++` like operator?",added 368 characters in body,2012-09-11 19:48:22.687
98888,31587,6404.0,5,,CC BY-SA 3.0,f562f3dd-e1c5-4272-ad63-c75d645e668c,"Please, check the comments above. Here is a quick implementation in R.

    x <- c(1,2,1,1,3,4,4,1,2,4,1,4,3,4,4,4,3,1,3,2,3,3,3,4,2,2,3)
    p <- matrix(nrow = 4, ncol = 4, 0)
    for (t in 1:(length(x) - 1)) p[x[t], x[t + 1]] <- p[x[t], x[t + 1]] + 1
    for (i in 1:4) p[i, ] <- p[i, ] / sum(p[i, ])

Results:

    > p
              [,1]      [,2]      [,3]      [,4]
    [1,] 0.1666667 0.3333333 0.3333333 0.1666667
    [2,] 0.2000000 0.2000000 0.4000000 0.2000000
    [3,] 0.1428571 0.1428571 0.2857143 0.4285714
    [4,] 0.2500000 0.1250000 0.2500000 0.3750000

A (probably dumb) implementation in MATLAB (which I have never used, so I don't know if this is going to work. I've just googled ""declare vector matrix MATLAB"" to get the syntax):

    x = [ 1, 2, 1, 1, 3, 4, 4, 1, 2, 4, 1, 4, 3, 4, 4, 4, 3, 1, 3, 2, 3, 3, 3, 4, 2, 2, 3 ]

    n = length(x)-1

    p = zeros(4,4)

    for t = 1:n
      p(x(t), x(t + 1)) = p(x(t), x(t + 1)) + 1
    end
    
   
    for i = 1:4
      p(i, :) = p(i, :) / sum(p(i, :))
    end

P.S. Does anyone know why R doesn't have a `++` like operator?",quick clean-up. Matlab's size command returns the 2D length parameters of an array. so length returns the longest dimension,2012-09-15 21:47:52.567
98889,31587,,24,,CC BY-SA 3.0,f562f3dd-e1c5-4272-ad63-c75d645e668c,,Proposed by 8686 approved by 686 edit id of 1740,2012-09-15 21:47:52.567
99657,32038,11013.0,1,,CC BY-SA 3.0,7f1ab9d0-3fe6-463a-8e13-a9306f703a97,Minimum number of levels for a random effects factor?,,2012-09-20 01:56:50.007
99659,32038,11013.0,2,,CC BY-SA 3.0,7f1ab9d0-3fe6-463a-8e13-a9306f703a97,"I'm using a mixed model in `R` (`lme4`) to analyze some repeated measures data. I have a response variable (fiber content of feces) and 3 fixed effects (body mass, etc.). My study only has 6 participants, with 16 repeated measures for each one (though two only have 12 repeats). The subjects are lizards that were given different combinations of food in different 'treatments'. 

My question is: can I use subject ID as a random effect? 

I know this is the usual course of action in longitudinal mixed effects models, to take account of the randomly sampled nature of the subjects and the fact that observations within subjects will be more closely correlated than those between subjects. But, treating subject ID as a random effect involves estimating a mean and variance for this variable. Since I have only 6 subjects (6 levels of this factor), is this enough to get an accurate characterization of the mean and variance? Does the fact that I have quite a few repeated measurements for each subject help in this regard (I don't see how it matters)? Finally, If I can't use subject ID as a random effect, will including it as a fixed effect allow me to control for the fact that I have repeated measures?",,2012-09-20 01:56:50.007
99658,32038,11013.0,3,,CC BY-SA 3.0,7f1ab9d0-3fe6-463a-8e13-a9306f703a97,<r><mixed-model><random-effects-model>,,2012-09-20 01:56:50.007
99721,32053,346.0,2,,CC BY-SA 3.0,30eaff0c-aafb-434e-a926-1bcd3ebff6e0,"Short answer: Yes, you can use ID as random effect with 6 levels.

Slightly longer answer: The [mixed modeling FAQ for r-sig-mixed][1] says (among other things) the following under the headline ""*Should I treat factor xxx as fixed or random?*"":

> One point of particular relevance to 'modern' mixed model estimation
> (rather than 'classical' method-of-moments estimation) is that, for
> practical purposes, there must be a reasonable number of
> random-effects levels (e.g. blocks) — more than 5 or 6 at a minimum.

So you are at the lower bound, but on the right side of it.

  [1]: http://glmm.wikidot.com/faq",,2012-09-20 12:35:29.537
99843,32038,11013.0,5,,CC BY-SA 3.0,8c3cd3e6-f264-44f7-8db5-c54c0c84646a,"I'm using a mixed model in `R` (`lme4`) to analyze some repeated measures data. I have a response variable (fiber content of feces) and 3 fixed effects (body mass, etc.). My study only has 6 participants, with 16 repeated measures for each one (though two only have 12 repeats). The subjects are lizards that were given different combinations of food in different 'treatments'. 

My question is: can I use subject ID as a random effect? 

I know this is the usual course of action in longitudinal mixed effects models, to take account of the randomly sampled nature of the subjects and the fact that observations within subjects will be more closely correlated than those between subjects. But, treating subject ID as a random effect involves estimating a mean and variance for this variable. Since I have only 6 subjects (6 levels of this factor), is this enough to get an accurate characterization of the mean and variance? Does the fact that I have quite a few repeated measurements for each subject help in this regard (I don't see how it matters)? Finally, If I can't use subject ID as a random effect, will including it as a fixed effect allow me to control for the fact that I have repeated measures?

Edit 1: I'd just like to clarify that when I say ""can I"" use subject ID as a random effect, I mean ""is it a good idea to"". I know I can fit the model with a factor with just 2 levels, but surely this would be in-defensible? I'm asking at what point does it become sensible to think about treating subjects as random effects? It seems like the literature advises that 5-6 levels is a lower bound. It seems to me that the estimates of the mean and variance of the random effect would not be very precise until there were 15+ factor levels.",added 541 characters in body,2012-09-20 21:50:02.310
100597,32317,8208.0,2,,CC BY-SA 3.0,71de4ded-8258-483a-8e75-61f0856afb15,"I'm looking for a good Java library implementing several clustering algorithms.

I'll have to cluster some programs execution traces and I still don't know which algorithms I am going to need, so I'd like to use a library providing lot of them and that makes it easy to swap algorithms.

So far I had a look at Weka but I don't know whether there is a more complete library available I'm missing.",,2012-09-25 20:11:11.667
100598,32317,8208.0,1,,CC BY-SA 3.0,71de4ded-8258-483a-8e75-61f0856afb15,Good clustering Java library,,2012-09-25 20:11:11.667
100599,32317,8208.0,3,,CC BY-SA 3.0,71de4ded-8258-483a-8e75-61f0856afb15,<clustering><java>,,2012-09-25 20:11:11.667
126208,40104,,25,,,e86d4231-d4fd-4c35-9595-e506e3459a0d,,http://twitter.com/#!/StackStats/status/300410953932091394,2013-02-10 01:08:47.493
100830,32388,2105.0,2,,CC BY-SA 3.0,42b1fb9f-22db-4de7-8c71-cfa1abdc70f5,"Related to question [here][1].

I've been trying to teach myself about Network Analysis, and developing DAG charts in R. Let's say that I have the following data.

    dat=data.frame(sold=c(0,0,0,1,0,1), won=c(1,0,0,1,0,1), bid=c(5,3,2,5,3,4))
    dat

Given what I'm trying to analyze, I know that the DAG plot should be as follows:

    bid => won => sold

However, when I utilize the bnlearn package to generate the plot, it comes out as follows. It just can't be correct, and should  be in the opposite direction.

    library(""bnlearn"")
    library(""Rgraphviz"")
    
    bn.hc <- hc(dat, score = ""bic"")
    graphviz.plot(bn.hc)

![enter image description here][2]

Can anyone help with diagnosing the problem in R? Is there something I'm missing in the code? or of my understanding of BN's? is this an issue w/ what I pass as the algorithm to use in 'score'?


  [1]: http://stats.stackexchange.com/questions/37930/prediction-with-bayesian-networks-in-r
  [2]: https://i.stack.imgur.com/Zg0rM.png",,2012-09-26 20:56:22.507
100831,32388,2105.0,1,,CC BY-SA 3.0,42b1fb9f-22db-4de7-8c71-cfa1abdc70f5,Odd results from Bayesian Network in R,,2012-09-26 20:56:22.507
100832,32388,2105.0,3,,CC BY-SA 3.0,42b1fb9f-22db-4de7-8c71-cfa1abdc70f5,<r><bayesian><bayesian-network>,,2012-09-26 20:56:22.507
100835,32388,2105.0,5,,CC BY-SA 3.0,0a641658-e5e9-46c6-8f45-c8e406626a5c,"Related to question [here][1].

I've been trying to teach myself about Network Analysis, and developing DAG charts in R. Let's say that I have the following data.

    dat=data.frame(sold=c(0,0,0,1,0,1), won=c(1,0,0,1,0,1), bid=c(5,3,2,5,3,4))
    dat

Given what I'm trying to analyze, I know that the DAG plot should be as follows:

    bid => won => sold

However, when I utilize the bnlearn package to generate the plot, it comes out as follows. It just can't be correct, and should  be in the opposite direction.

    library(""bnlearn"")
    library(""Rgraphviz"")
    
    bn.hc <- hc(dat, score = ""bic"")
    graphviz.plot(bn.hc)

![enter image description here][2]

Now, I know that's just the data that I provided it to learn on, but I've messed around with the variable values, and it never turns our the way it should. Basically, a bid should determine whether you win, and whether you win should determine whether you can sell it. Just doesn't make sense.

Isn't there some way to specify what variable is the response variable?

Can anyone help with diagnosing the problem in R? Is there something I'm missing in the code? or of my understanding of BN's? is this an issue w/ what I pass as the algorithm to use in 'score'?


  [1]: http://stats.stackexchange.com/questions/37930/prediction-with-bayesian-networks-in-r
  [2]: https://i.stack.imgur.com/Zg0rM.png",added 296 characters in body,2012-09-26 21:08:45.927
100837,32388,2105.0,5,,CC BY-SA 3.0,2d8c527c-d569-4ec4-a9e9-17b38f669605,"Related to question [here][1].

I've been trying to teach myself about Network Analysis, and developing DAG charts in R. Let's say that I have the following data.

    dat=data.frame(sold=c(0,0,0,1,0,1), won=c(1,0,0,1,0,1), bid=c(5,3,2,5,3,4))
    dat

Given what I'm trying to analyze, I know that the DAG plot should be as follows:

    bid => won => sold

However, when I utilize the bnlearn package to generate the plot, it comes out as follows. It just can't be correct, and should  be in the opposite direction.

    library(""bnlearn"")
    library(""Rgraphviz"")
    
    bn.hc <- hc(dat, score = ""bic"")
    graphviz.plot(bn.hc)

![enter image description here][2]

Now, I know that's just the data that I provided it to learn on, but I've messed around with the variable values, and it never turns our the way it should. Basically, a bid should determine whether you win, and whether you win should determine whether you can sell it. Just doesn't make sense.

Isn't there some way to specify what variable is the response variable? In my case, the response variable should be sold, and there be no arcs from sold to another node.

Can anyone help with diagnosing the problem in R? Is there something I'm missing in the code? or of my understanding of BN's? is this an issue w/ what I pass as the algorithm to use in 'score'?


  [1]: http://stats.stackexchange.com/questions/37930/prediction-with-bayesian-networks-in-r
  [2]: https://i.stack.imgur.com/Zg0rM.png",added 98 characters in body,2012-09-26 21:14:48.620
100843,32388,2105.0,5,,CC BY-SA 3.0,2e452461-f36b-4c20-81a9-c47932851467,"Related to question [here][1].

I've been trying to teach myself about Network Analysis, and developing DAG charts in R. Let's say that I have the following data.

    dat=data.frame(sold=c(0,0,0,1,0,1), won=c(1,0,0,1,0,1), bid=c(5,3,2,5,3,4))
    dat

Given what I'm trying to analyze, I know that the DAG plot should be as follows:

    bid => won => sold

However, when I utilize the bnlearn package to generate the plot, it comes out as follows. It just can't be correct, and should  be in the opposite direction.

    library(""bnlearn"")
    library(""Rgraphviz"")
    
    bn.hc <- hc(dat, score = ""bic"")
    graphviz.plot(bn.hc)

![enter image description here][2]

Now, I know that's just the data that I provided it to learn on, but I've messed around with the variable values, and it never turns our the way it should. Basically, a bid should determine whether you win, and whether you win should determine whether you can sell it. Just doesn't make sense.

Isn't there some way to specify what variable is the response variable? In my case, the response variable should be sold, and there should be no arcs from sold to another node.

Can anyone help with diagnosing the problem in R? Is there something I'm missing in the code? or of my understanding of BN's? is this an issue w/ what I pass as the algorithm to use in 'score'?


  [1]: http://stats.stackexchange.com/questions/37930/prediction-with-bayesian-networks-in-r
  [2]: https://i.stack.imgur.com/Zg0rM.png",added 7 characters in body,2012-09-26 21:23:34.353
100858,32388,,4,user88,CC BY-SA 3.0,eb569b4d-70c4-4c88-9247-b279f2edf766,Odd results from Bayesian network in R,edited title,2012-09-26 23:02:24.670
104633,33598,11643.0,3,,CC BY-SA 3.0,428994e0-0b5b-442d-9427-ed2c06426e4a,<chow-test>,,2012-10-16 09:30:00.727
104632,33598,11643.0,1,,CC BY-SA 3.0,428994e0-0b5b-442d-9427-ed2c06426e4a,How to identify structural change using a Chow test on Eviews,,2012-10-16 09:30:00.727
104631,33598,11643.0,2,,CC BY-SA 3.0,428994e0-0b5b-442d-9427-ed2c06426e4a,"I have this little problem and I would appreciate some help.

As part of my master thesis, I have to identify a trend in a univariate (GDP) time series for different countries.  I have to separate the trend and the stochastic element in it for each country.

I have managed to do so by doing:

variable c @trend  // for each country.

And then running a AR(1) on the residuals  // for each country.


However, now I need to identify structural breaks in one of these countries.  I've been reading and searching all over the internet and books and I've found that the test most people use to identify these structural changes is the Chow Test.

I know how to run the test, but I have't been able to figure out how to interpret the results, and decide whether there is a structural break or not.

Here there is an example of the results:


 https://i.stack.imgur.com/1kCqG.jpg


What puzzles me the most is the fact that, regardless the point I choose to break the series, I always get 

Prob. F(2,47)  0.0016 //or any very significant value, with the same degrees of freedom.


Can someone please help me understand how I should interpret these results in order to identify where the breaks lie?

Thank you!

Hernan.


",,2012-10-16 09:30:00.727
104634,33598,,5,user10525,CC BY-SA 3.0,c8b4acc1-5c74-4c86-bb89-b20e3917ffa3,"I have this little problem and I would appreciate some help.

As part of my master thesis, I have to identify a trend in a univariate (GDP) time series for different countries.  I have to separate the trend and the stochastic element in it for each country.

I have managed to do so by doing:

variable c @trend  // for each country.

And then running a AR(1) on the residuals  // for each country.


However, now I need to identify structural breaks in one of these countries.  I've been reading and searching all over the internet and books and I've found that the test most people use to identify these structural changes is the Chow Test.

I know how to run the test, but I have't been able to figure out how to interpret the results, and decide whether there is a structural break or not.

Here there is an example of the results:


![enter image description here][1]


What puzzles me the most is the fact that, regardless the point I choose to break the series, I always get 

Prob. F(2,47)  0.0016 //or any very significant value, with the same degrees of freedom.


Can someone please help me understand how I should interpret these results in order to identify where the breaks lie?


  [1]: https://i.stack.imgur.com/RL9Lz.jpg",Attach image and related tag,2012-10-16 09:32:48.907
106474,34166,668.0,3,,CC BY-SA 3.0,02e02bfa-e679-43b0-add7-da3c37f51507,<decision-theory><paradox>,,2012-10-25 20:10:18.553
106477,34166,668.0,2,,CC BY-SA 3.0,02e02bfa-e679-43b0-add7-da3c37f51507,"##The situation##
Some researchers would like to put you to sleep.  Depending on the secret toss of a fair coin, they will briefly awaken you either once (Heads) or twice (Tails).  After each waking, they will put you back to sleep with a drug that makes you forget that awakening.  When you are awakened, to what degree should *you* believe that the outcome of the coin toss was Heads?

*(OK, maybe you don’t want to be the subject of this experiment!  Suppose instead that Sleeping Beauty (SB) agrees to it (with the full approval of the Magic Kingdom’s Institutional Review Board, of course).  She’s about to go to sleep for one hundred years, so what are one or two more days, anyway?)*

![Maxfield Parrish illustration][1]

*[Detail of a [Maxfield Parrish](http://en.wikipedia.org/wiki/Maxfield_Parrish) illustration.]*

##Are you a Halfer or a Thirder?##

**The Halfer position.**  Simple! The coin is fair--and SB knows it--so she should believe there's a one-half chance of heads.

**The Thirder position.** Were this experiment to be repeated many times, then the coin will be heads only one third of the time SB is awakened.  Her probability for heads will be one third.

###Thirders have a problem###

Most, but not all, people who have written about this are thirders.  But:

* On Sunday evening, just before SB falls asleep, she must believe the chance of heads is one-half: that’s what it means to be a fair coin.

* Whenever SB awakens, *she has learned absolutely nothing she did not know Sunday night.*  What rational argument can she give, then, for stating that her belief in heads is now one-third and not one-half?

##Some attempted explanations##

* SB would necessarily lose money if she were to bet on heads with any odds other than 1/3.  (Vineberg, *inter alios*)

* One-half really is correct: just use the Everettian “many-worlds” interpretation of Quantum Mechanics!  (Lewis).

* SB updates her belief based on self-perception of her “temporal location” in the world.  (Elga, *i.a.*)

* SB is confused: “[It] seems more plausible to say that her epistemic state upon waking up should not include a definite degree of belief in heads. … The real issue is how one deals with known, unavoidable, cognitive malfunction.”  [Arntzenius]

------

##The question##

**Accounting for what has already been written on this subject** (see the references as well as a [previous post](http://stats.stackexchange.com/a/23812)), **how can this paradox be resolved in a statistically rigorous way?**  Is this even possible?

-----

##References##

**Arntzenius, Frank** (2002).  [*Reflections on Sleeping Beauty*](**Arntzenius, Frank** (2002).  *Reflections on Sleeping Beauty*.  Analysis 62.1 pp 53-62.).  Analysis 62.1 pp 53-62.

**Bradley, DJ** (2010).  [*Confirmation in a Branching World: The Everett Interpretation and Sleeping Beauty*](http://philpapers.org/archive/BRACIB.1.pdf).  Brit. J. Phil. Sci. 0 (2010), 1–21.

Elga, Adam (2000).  Self-locating belief and the Sleeping Beauty Problem.  Analysis 60 pp 143-7.

**Franceschi, Paul** (2005).  [*Sleeping Beauty and the Problem of World Reduction*](http://philsci-archive.pitt.edu/2175/1/sb-en.pdf).  Preprint.

**Groisman, Berry** (2007).  [*The end of Sleeping Beauty’s nightmare*](http://philsci-archive.pitt.edu/3624/1/SB_b.groisman_last.pdf).  Preprint.

**Lewis, D** (2001).  *Sleeping Beauty: reply to Elga*.  Analysis 61.3 pp 171-6.

**Papineau, David** and **Victor Dura-Vila** (2008).  *A Thirder and an Everettian: a reply to Lewis’s ‘Quantum Sleeping Beauty’*.

**Pust, Joel** (2008).  *Horgan on Sleeping Beauty*.  Synthese 160 pp 97-101.

**Vineberg, Susan** (undated, perhaps 2003).  *Beauty’s Cautionary Tale*.


  [1]: https://i.stack.imgur.com/zLmrR.png",,2012-10-25 20:10:18.553
106475,34166,668.0,1,,CC BY-SA 3.0,02e02bfa-e679-43b0-add7-da3c37f51507,The Sleeping Beauty Paradox,,2012-10-25 20:10:18.553
106482,34166,,25,,,eaf12d0a-d02a-48dd-9879-1e96591b3fdd,,http://twitter.com/#!/StackStats/status/261574473898139648,2012-10-25 21:06:29.000
106563,34166,668.0,5,,CC BY-SA 3.0,2dbe05fd-1fd1-4c51-82a5-a640314ac466,"###The situation###
Some researchers would like to put you to sleep.  Depending on the secret toss of a fair coin, they will briefly awaken you either once (Heads) or twice (Tails).  After each waking, they will put you back to sleep with a drug that makes you forget that awakening.  When you are awakened, to what degree should *you* believe that the outcome of the coin toss was Heads?

*(OK, maybe you don’t want to be the subject of this experiment!  Suppose instead that Sleeping Beauty (SB) agrees to it (with the full approval of the Magic Kingdom’s Institutional Review Board, of course).  She’s about to go to sleep for one hundred years, so what are one or two more days, anyway?)*

![Maxfield Parrish illustration][1]

*[Detail of a [Maxfield Parrish](http://en.wikipedia.org/wiki/Maxfield_Parrish) illustration.]*

###Are you a Halfer or a Thirder?###

**The Halfer position.**  Simple! The coin is fair--and SB knows it--so she should believe there's a one-half chance of heads.

**The Thirder position.** Were this experiment to be repeated many times, then the coin will be heads only one third of the time SB is awakened.  Her probability for heads will be one third.

###Thirders have a problem###

Most, but not all, people who have written about this are thirders.  But:

* On Sunday evening, just before SB falls asleep, she must believe the chance of heads is one-half: that’s what it means to be a fair coin.

* Whenever SB awakens, *she has learned absolutely nothing she did not know Sunday night.*  What rational argument can she give, then, for stating that her belief in heads is now one-third and not one-half?

###Some attempted explanations###

* SB would necessarily lose money if she were to bet on heads with any odds other than 1/3.  (Vineberg, *inter alios*)

* One-half really is correct: just use the Everettian “many-worlds” interpretation of Quantum Mechanics!  (Lewis).

* SB updates her belief based on self-perception of her “temporal location” in the world.  (Elga, *i.a.*)

* SB is confused: “[It] seems more plausible to say that her epistemic state upon waking up should not include a definite degree of belief in heads. … The real issue is how one deals with known, unavoidable, cognitive malfunction.”  [Arntzenius]

------

###The question###

Accounting for what has already been written on this subject (see the references as well as a [previous post](http://stats.stackexchange.com/a/23812)), how can this paradox be resolved in a statistically rigorous way?  Is this even possible?

-----

###References###

Arntzenius, Frank (2002).  [*Reflections on Sleeping Beauty*](http://www.joelvelasco.net/teaching/3865/arntzenius%20-%20reflections%20on%20sleeping%20beauty.pdf) Analysis 62.1 pp 53-62.

Bradley, DJ (2010).  [*Confirmation in a Branching World: The Everett Interpretation and Sleeping Beauty*](http://philpapers.org/archive/BRACIB.1.pdf).  Brit. J. Phil. Sci. 0 (2010), 1–21.

Elga, Adam (2000).  Self-locating belief and the Sleeping Beauty Problem.  Analysis 60 pp 143-7.

Franceschi, Paul (2005).  [*Sleeping Beauty and the Problem of World Reduction*](http://philsci-archive.pitt.edu/2175/1/sb-en.pdf).  Preprint.

Groisman, Berry (2007).  [*The end of Sleeping Beauty’s nightmare*](http://philsci-archive.pitt.edu/3624/1/SB_b.groisman_last.pdf).  Preprint.

Lewis, D (2001).  *Sleeping Beauty: reply to Elga*.  Analysis 61.3 pp 171-6.

Papineau, David and Victor Dura-Vila (2008).  *A Thirder and an Everettian: a reply to Lewis’s ‘Quantum Sleeping Beauty’*.

Pust, Joel (2008).  *Horgan on Sleeping Beauty*.  Synthese 160 pp 97-101.

Vineberg, Susan (undated, perhaps 2003).  *Beauty’s Cautionary Tale*.


  [1]: https://i.stack.imgur.com/zLmrR.png",Formatting,2012-10-26 11:47:02.827
109645,35097,9886.0,3,,CC BY-SA 3.0,2bd8d41b-4f7f-47e8-a585-249f9544130b,<bayesian><frequentist>,,2012-11-11 15:56:03.667
109644,35097,9886.0,1,,CC BY-SA 3.0,2bd8d41b-4f7f-47e8-a585-249f9544130b,What's wrong with last xkcd (Frequentists vs. Bayesians)?,,2012-11-11 15:56:03.667
109643,35097,9886.0,2,,CC BY-SA 3.0,2bd8d41b-4f7f-47e8-a585-249f9544130b,"![xkcd comic number 1132][2]

[This xkcd comic (Frequentists vs. Bayesians)][1] makes fun of a frequentist statistician who derives an obviously wrong result.

However it seems to me that his reasoning is actually correct in the sense that it follows  the standard frequentist methodology. 

So my question is ""does he correctly apply the frequentist methodology?"" 

 - If no: what would be a correct frequentist inference in this scenario? How to integrate ""prior knowledge"" about the sun stability in the frequentist methodology?
 - If yes: wtf? ;-)

  [1]: http://xkcd.com/1132
  [2]: https://i.stack.imgur.com/tStr4.png",,2012-11-11 15:56:03.667
109649,35097,,25,,,c297e61f-84b2-430d-8662-6c6a70b5354f,,http://twitter.com/#!/StackStats/status/267657811683594240,2012-11-11 15:59:07.593
109743,35097,,4,user88,CC BY-SA 3.0,ac6424b2-7acc-49e5-9954-e3a1578becd9,What's wrong with XKCD's Frequentists vs. Bayesians comic?,edited title,2012-11-12 00:38:54.837
109862,35160,12273.0,2,,CC BY-SA 3.0,ec3ce893-d398-4833-91c0-9405cb0bb83a,"The main issue is that the first experiment (Sun gone nova) is not repeatable, which makes it highly unsuitable for frequentist methodology that interprets probability as estimate of how frequent an event is giving that we can repeat the experiment many times. In contrast, bayesian probability is interpreted as our degree of belief giving all available prior knowledge, making it suitable for common sense reasoning about one-time events. The dice throw experiment is repeatable, but I find it very unlikely that any frequentist would intentionally ignore the influence of the first experiment and be so confident in significance of the obtained results.

Although it seems that author mocks frequentist reliance on repeatable experiments and their distrust of priors, giving the unsuitability of the experimental setup to the frequentist methodology I would say that real theme of this comic is not frequentist methodology but blind following of unsuitable methodology in general. Whether it's funny or not is up to you (for me it is) but I think it more misleads than clarifies the differences between the two approaches.
",,2012-11-12 16:27:57.197
110145,35249,11884.0,3,,CC BY-SA 3.0,fdca3fc8-044c-401f-b119-72b321271ed7,<clustering><pca><factor-analysis>,,2012-11-13 22:29:58.533
110143,35249,11884.0,2,,CC BY-SA 3.0,fdca3fc8-044c-401f-b119-72b321271ed7,"First of all, sorry for the strange title, I had no idea how to describe my problem better.

My issue is the following, I think it is pretty much limited to geosciences.

I have several properties for every sample, which are divided by depth.

For instance:

(ID  var1_0-20cm, var1_20-50cm, var1_50-100cm, var2_0-20cm, var2_20-50, ....)

(1, 2.3, 2.1, 2.6, 10.5, 10.9, 15.0,...)    
(2, 2.0, 1.1, 1.1, 5.5, 5.9, 5.0,...)   
(3, 1.0, 0.0, 0.0, 3.5, 1.9, 1.0,...)

Basically these are geological layers going from surface down to 100 cm depth.
I am trying to decrease the number of variables, either with PCA or factor analysis.
The issue is, that I would like to handle properties together, no matter what the depth is.

(For instance I do not want to get rid of a layer in between the surface and the bottom layer.)

Is there any way to handle them together, or group them for PCA or whatever. I tried to find some relevant information, but I think the problem is limited to a small portion of the science(maybe I am wrong), so I could not find anything usefull.

Or any good idea or solution would be amazing and highly appreciated :)

I hope I described the issue well :) I am not native english so sorry for the mistakes :)

Thanks 

-v",,2012-11-13 22:29:58.533
110144,35249,11884.0,1,,CC BY-SA 3.0,fdca3fc8-044c-401f-b119-72b321271ed7,Handling variables subset based on depth for PCA,,2012-11-13 22:29:58.533
110151,35249,11884.0,4,,CC BY-SA 3.0,1281397b-0e37-44c9-8182-8cb81ab2c001,Reducing no of variables subsetted based on depth for PCA,edited title,2012-11-13 23:27:08.820
113396,13060,1805.0,5,,CC BY-SA 3.0,313325ca-afe3-4d2f-9287-5a8ccfc6dc6b,"Check out the [digitize][1] package for [R][2].  Its designed to solve exactly this sort of problem.

/edit:  No longer on CRAN, but you can still [get it from R-Forge][3].


  [1]: http://cran.r-project.org/web/packages/digitize/index.html
  [2]: http://cran.r-project.org/
  [3]: https://r-forge.r-project.org/R/?group_id=594",added 128 characters in body,2012-11-30 21:49:31.210
116360,37182,11446.0,1,,CC BY-SA 3.0,361c0f6d-7473-4840-bda3-25d69bd7da92,How to specify in r spatial covariance structure similar to SAS sp(pow) in a marginal model?,,2012-12-14 15:06:25.837
116361,37182,11446.0,3,,CC BY-SA 3.0,361c0f6d-7473-4840-bda3-25d69bd7da92,<r><sas><spatial><covariance><panel-data>,,2012-12-14 15:06:25.837
126224,40121,14728.0,1,,CC BY-SA 3.0,08a3103d-cd12-49f2-9aa8-2b4d408b4ec8,"In JMP, when compare means, how is comparison circle radius calculated?",,2013-02-10 03:03:22.580
126225,40121,14728.0,3,,CC BY-SA 3.0,08a3103d-cd12-49f2-9aa8-2b4d408b4ec8,<students-t><tukey-hsd-test><jmp>,,2013-02-10 03:03:22.580
136699,43458,16452.0,1,,CC BY-SA 3.0,7b50b83b-d2fd-46f6-abcb-83f47dbc465c,How to check if removing a sample makes a difference in mean and stdev values?,,2013-04-02 14:45:37.780
116359,37182,11446.0,2,,CC BY-SA 3.0,361c0f6d-7473-4840-bda3-25d69bd7da92,"I'm currently translating existing code from SAS to R. I have this SAS code :

    Proc mixed data=ALBI;
    class NUM_PAT;
    model CD4t=T /s ;
    repeated / sub=NUM_PAT type=sp(pow)(T);

The SAS spatial power covariance structure is useful for unequally spaced longitudinal measurements where the correlations decline as a function of time (as shown by the picture below).

![Spatial Power Covariance Structure][1]


  [1]: https://i.stack.imgur.com/s7RnV.png

I think I have to use gls( ) from {nlme} since I don't have any random effects. My guess is that I need to use corSpatial, but I don't know how.

Thanks for any help.

",,2012-12-14 15:06:25.837
116378,37182,,25,,,1ace1c95-3a94-42f6-a597-07471ea9f2e1,,http://twitter.com/#!/StackStats/status/279617248447057921,2012-12-14 16:02:01.877
118326,37748,13370.0,1,,CC BY-SA 3.0,f4028a8c-e2e0-40a3-b550-6a026eead2e0,What is the computational complexity of the EM algorithm?,,2012-12-27 07:48:43.813
118325,37748,13370.0,3,,CC BY-SA 3.0,f4028a8c-e2e0-40a3-b550-6a026eead2e0,<machine-learning><computational-statistics>,,2012-12-27 07:48:43.813
118324,37748,13370.0,2,,CC BY-SA 3.0,f4028a8c-e2e0-40a3-b550-6a026eead2e0,"In general, and more specifically for Bernoulli mixture model (aka Latent Class Analysis).",,2012-12-27 07:48:43.813
118612,37819,12314.0,2,,CC BY-SA 3.0,f162557b-d4e1-4221-a537-6fd720ed017a,"Is it okay to feed $I(0)$ variables into the Johansen procedure? I've read three sources that seem to state that this is not what you're supposed to do. However, whenever I've done this, I notice that $\Pi$ is full rank and so it leads me to a VAR and therefore I don't see any problem with this. ",,2012-12-29 16:08:10.207
118613,37819,12314.0,3,,CC BY-SA 3.0,f162557b-d4e1-4221-a537-6fd720ed017a,<time-series><econometrics><cointegration><autoregressive><stationarity>,,2012-12-29 16:08:10.207
118614,37819,12314.0,1,,CC BY-SA 3.0,f162557b-d4e1-4221-a537-6fd720ed017a,Putting stationary variables through Johansen procedure,,2012-12-29 16:08:10.207
118646,33598,,25,,,3fe604be-c89b-4dc9-9cfb-54e04cccd200,,http://twitter.com/#!/StackStats/status/285189143577911296,2012-12-30 01:02:45.163
119017,37182,11446.0,5,,CC BY-SA 3.0,59968bd8-832b-4092-9e9b-3d79e50fc875,"I'm currently translating existing code from SAS to R. I'm working on longitudinal data (CD4 count over time). I have the following SAS code :

    Proc mixed data=df;
    class NUM_PAT;
    model CD4t=T /s ;
    repeated / sub=NUM_PAT type=sp(pow)(T);

The SAS spatial power covariance structure is useful for unequally spaced longitudinal measurements where the correlations decline as a function of time (as shown by the picture below). 
![Spatial Power Covariance Structure][1]


  [1]: https://i.stack.imgur.com/s7RnV.png

I think I have to use gls( ) from {nlme} since I don't have any random effects. As R 'only' provides ""spherical"", ""exponential"", ""gaussian"", ""linear"", and ""rational"" as correlation spatial structures, my guess is that I need to use corSpatial plus a weights argument.

I tried the following code, but it doesn't work :

    gls(CD4t~T, data=df, na.action = (na.omit), method = ""ML"",
    corr=corCompSymm(form=~1|NUM_PAT), weighhts=varConstPower(form=~1|T))

What am I doing wrong ?

Thanks for any help.
",added 404 characters in body,2013-01-02 13:47:28.253
119166,37182,11446.0,6,,CC BY-SA 3.0,685798ad-4d7e-420b-95cd-68bb7a7fbc39,<r><sas><spatial><panel-data><generalized-least-squares>,edited tags,2013-01-03 14:44:37.783
119184,37981,13403.0,3,,CC BY-SA 3.0,4740b865-675b-4493-950c-7b499e06cf3e,<density-function><descriptive-statistics><skewness><kurtosis>,,2013-01-03 16:00:17.050
119185,37981,13403.0,2,,CC BY-SA 3.0,4740b865-675b-4493-950c-7b499e06cf3e,"Happy New Year Everyone,

I've tried looking this up, but haven't found a good answer. It is a question of definitions.

I would like to describe the ""peakedness"" and tail ""heaviness"" of several skewed probability density functions.

The features I want to describe, would they be called ""kurtosis""? I've only seen the word ""kurtosis"" used for symmetric distributions?

Thank you
",,2013-01-03 16:00:17.050
119183,37981,13403.0,1,,CC BY-SA 3.0,4740b865-675b-4493-950c-7b499e06cf3e,kurtosis and skewness - descriptive statistics,,2013-01-03 16:00:17.050
119192,37981,5237.0,5,,CC BY-SA 3.0,f2639eda-6477-49d6-848a-4dc25b1ba6f2,"I would like to describe the ""peakedness"" and tail ""heaviness"" of several skewed probability density functions.

The features I want to describe, would they be called ""kurtosis""? I've only seen the word ""kurtosis"" used for symmetric distributions?",removed peripheral info,2013-01-03 16:36:19.167
119284,37981,,25,,,68ceccc1-0525-4b45-bfe2-19923ce67043,,http://twitter.com/#!/StackStats/status/287001087658057728,2013-01-04 01:02:46.207
123965,33598,,4,user88,CC BY-SA 3.0,89d9b1fa-41b8-463a-9318-77eb8962d615,How to identify structural change using a Chow test on Eviews?,edited title,2013-01-29 08:33:00.967
125943,40030,1790.0,3,,CC BY-SA 3.0,592c5e1a-1ae8-4ae7-8c1c-3f231cb84663,<cross-validation><stratification>,,2013-02-07 20:58:31.927
125941,40030,1790.0,1,,CC BY-SA 3.0,592c5e1a-1ae8-4ae7-8c1c-3f231cb84663,Understanding stratified CV,,2013-02-07 20:58:31.927
125942,40030,1790.0,2,,CC BY-SA 3.0,592c5e1a-1ae8-4ae7-8c1c-3f231cb84663,"What is the difference between stratified CV and CV?

Wikipedia says: 

> In stratified k-fold cross-validation, the folds are selected so that
> the mean response value is approximately equal in all the folds. In
> the case of a dichotomous classification, this means that each fold
> contains roughly the same proportions of the two types of class
> labels.

But I am still confused. 

1. What does `mean response value` mean in this context?
2. Why is # 1 important? 
3. How does one achieve #1 in practice?

",,2013-02-07 20:58:31.927
126179,40104,14684.0,2,,CC BY-SA 3.0,fd79b115-4bed-4397-9e4c-70d83be1859a,"Using wikipedia I found a way to calculate the probability mass function resulting from the sum of two Poisson random variables. However, I think that the approach I have is wrong.

Let $X_1, X_2$ be two independent Poisson random variables with mean $\lambda_1, \lambda_2$, and

$S_2 =  a_1 X_1+a_2 X_2,$

where the $a_1$ and $a_2$ are constants, then the probability-generating function of $S_2$ is given by

$G_{S_2}(z) = \operatorname{E}(z^{S_2})= \operatorname{E}(z^{a_1 X_1+a_2 X_2}) G_{X_1}(z^{a_1})G_{X_2}(z^{a_2}).$

Now, using the fact that the probability-generating function for a Poisson random variable is

$G_{X_i}(z) = \textrm{e}^{\lambda_i(z - 1)}, $

we can write the probability-generating function of the sum of the two independent Poisson random variables as

$G_{S_2}(z) =  \textrm{e}^{\lambda_1(z^{a_1} - 1)}\textrm{e}^{\lambda_2(z^{a_2} - 1)}$
$= \textrm{e}^{\lambda_1(z^{a_1} - 1)+\lambda+2(z^{a_2} - 1)}.$


It seems that the probability mass function of $S_2$ is recovered by taking derivatives of $G_{S_2}(z)$
$\operatorname{Pr}(S_2 = k) = \frac{G_{S_2}^{(k)}(0)}{k!}$, where $G_{S_2}^{(k)} = \frac{d^k G_{S_2}(z)}{ d z^k}.$

Is this is correct? I have the feeling I cannot just take the derivative to obtain the probability mass function, because of the constants $a_1$ and $a_2$. Is this right? Is there an alternative approach?

If this is correct can I now obtain an approximation of the cumulative distribution by truncating the infinite sum over all k?


Please help!

Michel",,2013-02-09 19:31:13.290
126178,40104,14684.0,3,,CC BY-SA 3.0,fd79b115-4bed-4397-9e4c-70d83be1859a,<distributions><poisson-distribution>,,2013-02-09 19:31:13.290
126226,40104,14684.0,5,,CC BY-SA 3.0,46622481-6c90-4396-b656-7125761b0a4c,"Using wikipedia I found a way to calculate the probability mass function resulting from the sum of two Poisson random variables. However, I think that the approach I have is wrong.

Let $X_1, X_2$ be two independent Poisson random variables with mean $\lambda_1, \lambda_2$, and

$S_2 =  a_1 X_1+a_2 X_2,$

where the $a_1$ and $a_2$ are constants, then the probability-generating function of $S_2$ is given by

$G_{S_2}(z) = \operatorname{E}(z^{S_2})= \operatorname{E}(z^{a_1 X_1+a_2 X_2}) G_{X_1}(z^{a_1})G_{X_2}(z^{a_2}).$

Now, using the fact that the probability-generating function for a Poisson random variable is

$G_{X_i}(z) = \textrm{e}^{\lambda_i(z - 1)}, $

we can write the probability-generating function of the sum of the two independent Poisson random variables as

$G_{S_2}(z) =  \textrm{e}^{\lambda_1(z^{a_1} - 1)}\textrm{e}^{\lambda_2(z^{a_2} - 1)}$
$= \textrm{e}^{\lambda_1(z^{a_1} - 1)+\lambda_2(z^{a_2} - 1)}.$


It seems that the probability mass function of $S_2$ is recovered by taking derivatives of $G_{S_2}(z)$
$\operatorname{Pr}(S_2 = k) = \frac{G_{S_2}^{(k)}(0)}{k!}$, where $G_{S_2}^{(k)} = \frac{d^k G_{S_2}(z)}{ d z^k}.$

Is this is correct? I have the feeling I cannot just take the derivative to obtain the probability mass function, because of the constants $a_1$ and $a_2$. Is this right? Is there an alternative approach?

If this is correct can I now obtain an approximation of the cumulative distribution by truncating the infinite sum over all k?


Please help!

Michel",changed \lambda+2 into \lambda_2,2013-02-10 03:12:19.067
126237,40121,14728.0,5,,CC BY-SA 3.0,9e6cf058-4c95-4a26-befa-68bdadf5c7a4,"I'm trying to compare several sets of experiment data, by comparing means. I read there are several different tests such as `Each Pair, Student’s t` and `All Pairs, Tukey HSD`, which give different circles of different radius, an example shown below 

oops I can't post image so the link is [here][1]

How are the circles defined? How do I calculate the radius? And is there a rule what test one should use for what kind of data?


  [1]: https://i.stack.imgur.com/Ce579.gif",added 33 characters in body; edited tags; edited title,2013-02-10 05:58:03.727
126238,40121,14728.0,4,,CC BY-SA 3.0,9e6cf058-4c95-4a26-befa-68bdadf5c7a4,"different tests for compare means, how to calculate comparison circle radius?",added 33 characters in body; edited tags; edited title,2013-02-10 05:58:03.727
126239,40121,14728.0,6,,CC BY-SA 3.0,9e6cf058-4c95-4a26-befa-68bdadf5c7a4,<mean><students-t><tukey-hsd-test>,added 33 characters in body; edited tags; edited title,2013-02-10 05:58:03.727
126266,40121,,5,user88,CC BY-SA 3.0,eddbf5fd-6c76-4c2d-86f0-dd9509456155,"I'm trying to compare several sets of experiment data, by comparing means. I read there are several different tests such as *Each Pair, Student’s t* and *All Pairs, Tukey HSD*, which give different circles of different radius, an example shown below 

![enter image description here][1]

How are the circles defined? How do I calculate the radius? And is there a rule what test one should use for what kind of data?


  [1]: https://i.stack.imgur.com/QCKE1.gif",deleted 14 characters in body; edited title,2013-02-10 12:16:41.200
126267,40121,,4,user88,CC BY-SA 3.0,eddbf5fd-6c76-4c2d-86f0-dd9509456155,A test for comparing means,deleted 14 characters in body; edited title,2013-02-10 12:16:41.200
126296,40121,,4,user88,CC BY-SA 3.0,b4a45ecc-788f-4df1-8e95-477e2d39e355,Comparing many means in JMP,edited tags; edited title,2013-02-10 19:53:22.873
126297,40121,,6,user88,CC BY-SA 3.0,b4a45ecc-788f-4df1-8e95-477e2d39e355,<mean><students-t><tukey-hsd-test><jmp>,edited tags; edited title,2013-02-10 19:53:22.873
128647,40859,15044.0,2,,CC BY-SA 3.0,549a71af-215b-44f7-9334-b477d57d3d74,"I was naively validating my binomial logit models by testing on a test dataset. I had randomly divided the available data (~2000 rows) into training (~1500) and validation (~500) datasets. 

I now read a post in another thread ( Frank Harrell) that causes me to question my approach:


> Data splitting is not very reliable unless you have more than 15,000
> observations. In other words, if you split the data again, accuracy
> indexes will vary too much from what you obtained with the first
> split.

How serious is this worry and what are ways around it? The OP speaks of ""resampling"" but not sure how that works here for validation. ",,2013-02-22 08:40:44.930
128648,40859,15044.0,1,,CC BY-SA 3.0,549a71af-215b-44f7-9334-b477d57d3d74,Validation: Data splitting into training vs. test datasets,,2013-02-22 08:40:44.930
128649,40859,15044.0,3,,CC BY-SA 3.0,549a71af-215b-44f7-9334-b477d57d3d74,<cross-validation>,,2013-02-22 08:40:44.930
128656,40859,,25,,,d6fedac7-1e29-4233-ab25-80aee0406c2c,,http://twitter.com/#!/StackStats/status/304899083087261696,2013-02-22 10:23:00.777
128677,40859,15044.0,5,,CC BY-SA 3.0,ab3dfcae-f22f-475e-a51e-cf4620a282f5,"I was naively validating my binomial logit models by testing on a test dataset. I had randomly divided the available data (~2000 rows) into training (~1500) and validation (~500) datasets. 

I now read a post in another thread ( Frank Harrell) that causes me to question my approach:


> Data splitting is not very reliable unless you have more than 15,000
> observations. In other words, if you split the data again, accuracy
> indexes will vary too much from what you obtained with the first
> split.

How serious is this worry and what are ways around it? The OP speaks of ""resampling"" but not sure how that works here for validation. 

Edit: Adding context as per @Bernhard's comment below:

http://stats.stackexchange.com/questions/15618/comparing-logistic-regression-models",added 145 characters in body,2013-02-22 11:38:06.397
128681,40870,1923.0,1,,CC BY-SA 3.0,77f91c26-5e31-4057-ad99-c1ee2ac19234,FRD calculation in target-decoy matching context,,2013-02-22 12:56:28.503
128682,40870,1923.0,3,,CC BY-SA 3.0,77f91c26-5e31-4057-ad99-c1ee2ac19234,<terminology><false-discovery-rate>,,2013-02-22 12:56:28.503
128680,40870,1923.0,2,,CC BY-SA 3.0,77f91c26-5e31-4057-ad99-c1ee2ac19234,"A common strategy in mass spectrometry of biological molecules is to upload observed spectra to a server so that they can be matched to a LARGE database of theoretical spectra of known molecules (a.k.a. *target* database). In order to control for false positives, a *decoy* database consisting of incorrect/irrelevant spectra is used.

I have been reading more into this subject and have come up some questions regarding the calculation of the FDR measure from this target-decoy strategy. The basic idea of the FDR value is very intuitive: 

$FDR = \frac{FP}{FP + TP}$

where FP and TP stands for false and true positives respectively. This makes perfect sense to me; if I'm trying to guess some peoples' names out of a phone book, and get 8 right and 2 wrong, I would have 2 out of 10 false guesses, and thus my false discovery rate would be 20%.

However reading [this tutorial][1] on how this is done in large scale on the servers, I got introduced to two different calculations, depending on whether or not the *target* and *decoy* databases are concatenated (page 2).


I don't think that this is a typo as I found other occurrences <sup>*</sup> of the mysterious factor 2 in front of FP in scientific literature. However the motivation behind this is never explained (at least I couldn't find it). 

I would appreciate some insight on where this doubling comes from. Likewise I wonder whether or not FDR calculation this way **assumes** that the error rate for each spectra match is the same for the target database and decoy database (i.e. *assuming* that getting 25 decoy hits *implies* 25 target hits are also false positives). It's not really clear for me why the error rate has to be the same for the two databases. Any comments on this subject is also appreciated.

<sub>* one such reference is Elias et al Nature Methods - 2, 667 - 675 (2005) </sub>

  [1]: http://www.proteored.org/pme6/fdr_calculation_for_pme6.pdf
  [2]: doi:10.1038/nmeth785",,2013-02-22 12:56:28.503
128683,40870,1923.0,4,,CC BY-SA 3.0,5fa7b4b3-310d-4ecb-9eaf-cee08c9eb81b,False discovery rate calculation in target-decoy matching context,corrected spelling error in the title,2013-02-22 13:08:41.170
129877,41244,15330.0,2,,CC BY-SA 3.0,511b413e-d9c0-4366-b124-bdc5e1c143fc,"Given N flips of the same coin and k occurences of 'heads', what is the probability distribution function of heads-probability?",,2013-03-01 05:09:26.123
129876,41244,15330.0,1,,CC BY-SA 3.0,511b413e-d9c0-4366-b124-bdc5e1c143fc,Probability of heads in a biased coin,,2013-03-01 05:09:26.123
129875,41244,15330.0,3,,CC BY-SA 3.0,511b413e-d9c0-4366-b124-bdc5e1c143fc,<probability>,,2013-03-01 05:09:26.123
129878,41244,15330.0,5,,CC BY-SA 3.0,4267f8bb-bde3-4837-a942-67dc98101b4b,"Given N flips of the same coin resulting in k occurences of 'heads', what is the probability density function of heads-probability?",added 4 characters in body,2013-03-01 05:22:01.993
129975,41244,15330.0,5,,CC BY-SA 3.0,08419f6a-88e7-453e-bc72-83ae8f11d8dc,"Given N flips of the same coin resulting in k occurences of 'heads', what is the probability density function of heads-probability of the coin?",added 12 characters in body,2013-03-01 20:14:36.900
131931,41914,13918.0,1,,CC BY-SA 3.0,dd90e082-7edf-4155-b4dd-caff8a793247,Lewandowski algorithm demand forecasting,,2013-03-12 11:17:01.283
131930,41914,13918.0,2,,CC BY-SA 3.0,dd90e082-7edf-4155-b4dd-caff8a793247,"I came across the Lewandowski method of demand forecasting in JDA Demand. Please help me understand at a high level the methodology it uses. I found a paper by Robert Hyndman titled 
""A state space framework for automatic forecasting using exponential smoothing methods"" and it uses this method as one of methods they compare the algorithm to. Currently for us this a black box, we want to get some high level understand so that we can better fine tune the parameters they have provided as part of the software. It would be great if you can share some thoughts about the Lewandowski algorithm and point to some references that I could use for further research.
",,2013-03-12 11:17:01.283
131956,41914,13918.0,5,,CC BY-SA 3.0,679db353-69e1-451f-a62f-e4b390b56067,"I came across the Lewandowski method of demand forecasting in JDA Demand. Please help me understand at a high level the methodology it uses. I found a paper by Robert Hyndman titled 
""A state space framework for automatic forecasting using exponential smoothing methods"" and it uses this method as one of methods they compare their algorithm to in the paper. Currently for us this is a black box, we want to get some high level understanding so that we can better fine tune the parameters they have provided as part of the software. It would be great if you can share some thoughts about the Lewandowski algorithm and point to some references that I could use for further research.
",fixed grammar,2013-03-12 13:45:22.893
133836,42513,15991.0,3,,CC BY-SA 3.0,d2af7f18-8d21-4ab5-8ea7-be61e96e4a0d,<self-study><data-visualization><mean><histogram>,,2013-03-19 21:23:49.697
133834,42513,15991.0,1,,CC BY-SA 3.0,d2af7f18-8d21-4ab5-8ea7-be61e96e4a0d,Plotting mean in histogram,,2013-03-19 21:23:49.697
133835,42513,15991.0,2,,CC BY-SA 3.0,d2af7f18-8d21-4ab5-8ea7-be61e96e4a0d,"I have a Really Stupid Question:
Is it ""okay"" to add a vertical line to a histogram to visualize the mean value? 
It seems okay to me, but I've never seen this in textbooks and the likes, so I'm wondering if there's some sort of convention not to do that? The graph is for a term paper, I just want to make sure I don't accidentally break some super important unspoken stats rule. :)",,2013-03-19 21:23:49.697
133842,42517,594.0,2,,CC BY-SA 3.0,d6d7588e-228d-407b-8f1f-e3d345964ead,"Of course, why not? 

![enter image description here][1]

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

For example, instead of drawing a line right across the plot, you might mark information along the bottom of it.

  [1]: https://i.stack.imgur.com/xNNl9.png",,2013-03-19 21:39:28.337
133845,42517,594.0,5,,CC BY-SA 3.0,61fdef87-78be-4467-8cc2-e23666081aa9,"Of course, why not? 

![histogram with mean][1]

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

For example, instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][2]

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)

  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/0gvWk.png",added 154 characters in body,2013-03-19 21:51:28.330
133846,42517,594.0,5,,CC BY-SA 3.0,43ee8aa1-9b0f-42ce-91c1-b0b45443ee17,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)


  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png",added 154 characters in body,2013-03-19 21:56:37.443
133847,42517,594.0,5,,CC BY-SA 3.0,1d50281d-7afd-49cd-a461-3ee46545d298,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot][4]

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)



You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)


  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [4]: https://i.stack.imgur.com/l8IT2.png",added 377 characters in body,2013-03-19 22:08:02.433
136701,43458,16452.0,2,,CC BY-SA 3.0,7b50b83b-d2fd-46f6-abcb-83f47dbc465c,"I'd like to ask if someone could help me with the following problem:

we have measured the same sample 5 times and we would like to check if there are significant differences in mean and stdev values if we use:

- All 5 datapoints
- Only the last 4 datapoints
- Only the last 3 datapoints

We have performed ANOVA analysis but we are not sure about the results because we might not have homocedasticity.

Which tests would you do to investigate this issue?

Thanks in advance for your help. ",,2013-04-02 14:45:37.780
142853,45457,5237.0,5,,CC BY-SA 3.0,2aaeebd3-83b7-4d5c-a6df-09343d242bcb,"I have a the following time series 


      Price      BrokerID 632 Behaviour  BrokerID 680 Behaviour ...BrokerID XYZ Behaviour

      5.6          IP                       SP                   
      5.7          BP                       IP
      5.8          SP                       BP
      5.83         IP                       SP

where `IP` is idle position, `BP` is buying position, and `SP` is selling position. I want to use Broker behaviour as the known variable and price as the hidden variable and predict it using HMM. But my question is how to find the emission matrix between a character vector (broker behaviour) and price numeric vector? ","Added tag ""r"" according to question's comments",2013-04-29 20:18:56.040
142852,45457,,24,,CC BY-SA 3.0,0baa5beb-9aaf-4cb5-9dd3-ea57ce80ee88,,Proposed by 22468 approved by -1 edit id of 3308,2013-04-29 20:18:56.040
133848,42517,594.0,5,,CC BY-SA 3.0,3c147044-0097-40f6-b533-d98b7f5d55da,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot][4]

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

(My plots are generated in R.)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)


  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [4]: https://i.stack.imgur.com/l8IT2.png",added 377 characters in body,2013-03-19 22:13:47.097
133854,42513,,25,,,469cb11c-f850-48a3-8f45-e98f68a54194,,http://twitter.com/#!/StackStats/status/314145220960858114,2013-03-19 22:43:51.923
133944,42517,594.0,5,,CC BY-SA 3.0,d4e679e2-99bb-46bf-8a3b-384bd82936fd,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot with jitter][5]

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

(My plots are generated in R.)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)


  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [5]: https://i.stack.imgur.com/vRujH.png",put jitter on the rugplot to overcome discreteness caused by rounding x.,2013-03-20 09:44:06.130
134100,42517,594.0,5,,CC BY-SA 3.0,835123de-d2d5-47a2-9332-cecba1c7151c,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot with jitter][5]  
(I have jittered the data locations slightly because the values were rounded to integers and you couldn't see the relative density well.)

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

(My plots are generated in R.)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)


  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [5]: https://i.stack.imgur.com/vRujH.png",added 141 characters in body,2013-03-20 22:23:23.613
134104,42513,166.0,4,,CC BY-SA 3.0,343090b4-75cc-4813-aef5-756205203d2f,Is it appropriate to plot the mean in a histogram?,updated title to more closely reflect the content of the question,2013-03-20 23:07:00.543
134978,42885,2615.0,3,,CC BY-SA 3.0,f40b3e36-51ab-4967-922d-9a16233f0e8a,<matlab><2sls>,,2013-03-25 15:11:36.577
134976,42885,2615.0,2,,CC BY-SA 3.0,f40b3e36-51ab-4967-922d-9a16233f0e8a,"I have one endogenous variable and two instruments for it, and I want to calculate my beta with the direct (one step) matrix formula

beta = [X'ZZ'X]^-1 X'ZZ'Y

But if I have two instruments for one endogenous variable X and Z are not the same length.

Any ideas?
Thanks!",,2013-03-25 15:11:36.577
134977,42885,2615.0,1,,CC BY-SA 3.0,f40b3e36-51ab-4967-922d-9a16233f0e8a,2SLS with two instruments for one endogenous variable in matlab,,2013-03-25 15:11:36.577
134980,42885,2615.0,5,,CC BY-SA 3.0,1e657c01-28d8-436e-b68a-ca8d5b51e08d,"I have one endogenous variable and two instruments for it, and I want to calculate my beta with the direct (one step) matrix formula

beta_2sls = (X' * Z * (Z' * X)^(-1) * Z' *  X) ^ (-1) * X' * Z *(Z' * Z)^(-1) * Z' * Y)

But if I have two instruments for one endogenous variable X and Z are not the same length.

Any ideas?
Thanks!",changed 2sls formula,2013-03-25 15:22:38.943
135820,42885,503.0,5,,CC BY-SA 3.0,71f0ef3e-86c6-44d4-94bb-08dc5fa4358d,"I have one endogenous variable and two instruments for it, and I want to calculate my beta with the direct (one step) matrix formula

$\beta_2sls = X' Z(Z'X)^{-1}Z'X^{-1}X'Z(Z'Z)^{-1}Z'Y$

But if I have two instruments for one endogenous variable X and Z are not the same length.

Any ideas?
Thanks!",made equation into LaTeX,2013-03-29 02:44:43.590
136700,43458,16452.0,3,,CC BY-SA 3.0,7b50b83b-d2fd-46f6-abcb-83f47dbc465c,<hypothesis-testing>,,2013-04-02 14:45:37.780
184211,56859,,25,,,f96991ca-a5c1-404d-810d-5cf303bc6566,,http://twitter.com/#!/StackStats/status/386417699796287488,2013-10-05 09:08:53.620
138269,42517,594.0,5,,CC BY-SA 3.0,5da066cb-c400-49e8-a055-b91b19e53eea,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot with jitter][5]  
(I have jittered the data locations slightly because the values were rounded to integers and you couldn't see the relative density well.)

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)

\--

My plots are generated in R.

Edit: 

As @gung surmised, `abline(v=mean...` was used to draw the mean-line across the plot and `rug` was used to draw the data values (though I actually used `rug(jitter(...` because the data was rounded to integers).

Here's a way to do the boxplot in between the histogram and the axis: 

    hist(Davis2[,2],n=30)
    boxplot(Davis2[,2],
      add=TRUE,horizontal=TRUE,at=-0.75,border=""darkred"",boxwex=1.5,outline=FALSE)

Of course, you won't have that data, so here's a similar-looking example with built in data:

    hist((islands)^(1/4),n=30)
    boxplot((islands)^(1/4),
      add=TRUE,horizontal=TRUE,at=-.25,border=""darkred"",boxwex=.6,outline=FALSE)

  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [5]: https://i.stack.imgur.com/vRujH.png

",added 686 characters in body,2013-04-10 03:14:22.220
138271,42517,594.0,5,,CC BY-SA 3.0,c144c1cc-6c94-4785-9ba3-69a753082500,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot with jitter][5]  
(I have jittered the data locations slightly because the values were rounded to integers and you couldn't see the relative density well.)

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)

\--

My plots are generated in R.

Edit: 

As @gung surmised, `abline(v=mean...` was used to draw the mean-line across the plot and `rug` was used to draw the data values (though I actually used `rug(jitter(...` because the data was rounded to integers).

Here's a way to do the boxplot in between the histogram and the axis: 

    hist(Davis2[,2],n=30)
    boxplot(Davis2[,2],
      add=TRUE,horizontal=TRUE,at=-0.75,border=""darkred"",boxwex=1.5,outline=FALSE)

Of course, you won't have that data, so here's a similar-looking example with built in data:

    hist((islands)^(1/4),n=30)
    boxplot((islands)^(1/4),
      add=TRUE,horizontal=TRUE,at=-.25,border=""darkred"",boxwex=.6,outline=FALSE)

However, it's not a general solution - I don't guarantee it will always work as well as it does here (note I already changed the `at` and `boxwex` options). If you don't write an intelligent function to take care of everything, it's necessary to pay attention to what everything does to make sure it's doing what you want.

  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [5]: https://i.stack.imgur.com/vRujH.png

",added 326 characters in body,2013-04-10 03:24:08.257
138272,42517,594.0,5,,CC BY-SA 3.0,a5aa4ab9-39df-41eb-b3f2-a64d2c23e8cc,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot with jitter][5]  
(I have jittered the data locations slightly because the values were rounded to integers and you couldn't see the relative density well.)

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)

\--

My plots are generated in R.

Edit: 

As @gung surmised, `abline(v=mean...` was used to draw the mean-line across the plot and `rug` was used to draw the data values (though I actually used `rug(jitter(...` because the data was rounded to integers).

Here's a way to do the boxplot in between the histogram and the axis: 

    hist(Davis2[,2],n=30)
    boxplot(Davis2[,2],
      add=TRUE,horizontal=TRUE,at=-0.75,border=""darkred"",boxwex=1.5,outline=FALSE)

I'm not going to list what everything there is for, but you can check the arguments in the help (`?boxplot`) to find out what they're for, and play with them yourself.

Of course, you won't have that data, so here's a similar-looking example with built in data:

    hist((islands)^(1/4),n=30)
    boxplot((islands)^(1/4),
      add=TRUE,horizontal=TRUE,at=-.25,border=""darkred"",boxwex=.6,outline=FALSE)

However, it's not a general solution - I don't guarantee it will always work as well as it does here (note I already changed the `at` and `boxwex` options). If you don't write an intelligent function to take care of everything, it's necessary to pay attention to what everything does to make sure it's doing what you want.

  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [5]: https://i.stack.imgur.com/vRujH.png

",added 171 characters in body,2013-04-10 03:32:56.743
138288,42517,594.0,5,,CC BY-SA 3.0,9686431b-415d-469d-8f49-669b2c417010,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot with jitter][5]  
(I have jittered the data locations slightly because the values were rounded to integers and you couldn't see the relative density well.)

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)

\--

My plots are generated in R.

Edit: 

As @gung surmised, `abline(v=mean...` was used to draw the mean-line across the plot and `rug` was used to draw the data values (though I actually used `rug(jitter(...` because the data was rounded to integers).

Here's a way to do the boxplot in between the histogram and the axis: 

    hist(Davis2[,2],n=30)
    boxplot(Davis2[,2],
      add=TRUE,horizontal=TRUE,at=-0.75,border=""darkred"",boxwex=1.5,outline=FALSE)

I'm not going to list what everything there is for, but you can check the arguments in the help (`?boxplot`) to find out what they're for, and play with them yourself.

Of course, you won't have that data, so here's a similar-looking example with built in data:

    hist((islands)^(1/4),n=30)
    boxplot((islands)^(1/4),
      add=TRUE,horizontal=TRUE,at=-.25,border=""darkred"",boxwex=.6,outline=FALSE)

However, it's not a general solution - I don't guarantee it will always work as well as it does here (note I already changed the `at` and `boxwex` options\*). If you don't write an intelligent function to take care of everything, it's necessary to pay attention to what everything does to make sure it's doing what you want.

\* -- an appropriate value for `at` is -0.5 times the value of `boxwex`; that would be a good default if you write a function to do it; `boxwex` would need to  be scaled in a way that relates to the y-scale (height) of the boxplot; I'd suggest 0.04 to 0.05 times the upper y-limit might usually be okay.

  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [5]: https://i.stack.imgur.com/vRujH.png

",added 237 characters in body,2013-04-10 04:20:42.457
138344,42517,594.0,5,,CC BY-SA 3.0,cdce9796-71f7-45c5-99ef-c8a9b10dd391,"Of course, why not? 

![histogram with mean][1]

Here's an example (one of dozens I found with a simple google search):

![hist with mean and median][2]

(Image source is is the measuring usability blog, [here](http://www.measuringusability.com/average-times.php).)

I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.

Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:

![histogram with marginal boxplot][3]

There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, [here](https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg). 

Sometimes people mark in the data:

![histogram rugplot with jitter][4]  
(I have jittered the data locations slightly because the values were rounded to integers and you couldn't see the relative density well.)

There's an example of this kind, done in STATA, on [this page](https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples) (see the third one [here](https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png))

Histograms are better with a little extra information - [they can be misleading on their own](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753)

You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)

\--

One last plot:

![histogram with stripchart][5]

\--

My plots are generated in R.

Edit: 

As @gung surmised, `abline(v=mean...` was used to draw the mean-line across the plot and `rug` was used to draw the data values (though I actually used `rug(jitter(...` because the data was rounded to integers).

Here's a way to do the boxplot in between the histogram and the axis: 

    hist(Davis2[,2],n=30)
    boxplot(Davis2[,2],
      add=TRUE,horizontal=TRUE,at=-0.75,border=""darkred"",boxwex=1.5,outline=FALSE)

I'm not going to list what everything there is for, but you can check the arguments in the help (`?boxplot`) to find out what they're for, and play with them yourself.

However, it's not a general solution - I don't guarantee it will always work as well as it does here (note I already changed the `at` and `boxwex` options\*). If you don't write an intelligent function to take care of everything, it's necessary to pay attention to what everything does to make sure it's doing what you want.

Here's how to create the data I used (I was trying to find whether Theil regression was really able to handle several influential outliers). It just happened to be data I was playing with when I first answered this question.

     library(""car"")
     add <- data.frame(sex=c(""F"",""F""),
           weight=c(150,130),height=c(NA,NA),repwt=c(55,50),repht=c(NA,NA))
     Davis2 <- rbind(Davis,add)

\* -- an appropriate value for `at` is around -0.5 times the value of `boxwex`; that would be a good default if you write a function to do it; `boxwex` would need to  be scaled in a way that relates to the y-scale (height) of the boxplot; I'd suggest 0.04 to 0.05 times the upper y-limit might often be okay.

Code for the marginal stripchart:

     hist(Davis2[,2],n=30)
     stripchart(jitter(Davis2[,2],amount=.5),
           method=""jitter"",jitter=.5,pch=16,cex=.05,add=TRUE,at=-.75,col='purple3')

  [1]: https://i.stack.imgur.com/xNNl9.png
  [2]: https://i.stack.imgur.com/fUqwi.jpg
  [3]: https://i.stack.imgur.com/0gvWk.png
  [4]: https://i.stack.imgur.com/vRujH.png
  [5]: https://i.stack.imgur.com/7sHku.png
",added data and an extra plot,2013-04-10 08:42:35.867
139547,44370,8063.0,3,,CC BY-SA 3.0,d83a57ab-0946-4b80-9986-8fccab06f0c1,<r><microarray>,,2013-04-15 13:42:17.450
139548,44370,8063.0,1,,CC BY-SA 3.0,d83a57ab-0946-4b80-9986-8fccab06f0c1,way to test for enrichment of differentially expressed genes in a genomic location,,2013-04-15 13:42:17.450
139549,44370,8063.0,2,,CC BY-SA 3.0,d83a57ab-0946-4b80-9986-8fccab06f0c1,"I have an experiment where I expect a certain genomic location to influence gene expression levels of nearby genes. I have data for expression levels (Agilent 4x44 microarrays, Drosophila) in two groups - one where I expect expression to be affected and the other wild-type and I would like to run a test for overrepresentation of differentially expressed genes in a genomic location.

My main problem is that I couldn't find a package (R/bioconductor) that would do it out of the box easily, so if you know about such a package, please let me know. In the meantime, this is what I figured out: I would run a sliding window over the whole genome and simply count number of differentially expressed genes in each window - this should tell me where I have the most differentially expressed genes in the genome. However, it will be dependent on gene density, so to obtain some sort of background distribution, I would run permutations of the samples (or p values), say, 1000 times, and check how often I am likely to find this number of windows with that number of differentially expressed genes compared to the observed numbers. Does this sound right?

I should add that while I know the location that would mess up things, I cannot exclude that any other genomic region would not be affected as well. So I have to test the whole genome.

Please advise on this approach and/or propose a better one...",,2013-04-15 13:42:17.450
139558,44370,8063.0,4,,CC BY-SA 3.0,6d99f698-33b2-4c33-affa-f819f5c73d9c,A way to test for enrichment of differentially expressed genes in a genomic location,edited title,2013-04-15 14:26:51.783
140326,44635,728.0,3,,CC BY-SA 3.0,bf522207-b2dc-4a54-8fb5-ae1af03fc385,<hypothesis-testing><ranking>,,2013-04-18 18:32:01.767
140324,44635,728.0,2,,CC BY-SA 3.0,bf522207-b2dc-4a54-8fb5-ae1af03fc385,"We can test the symmetry of a distribution around $0$ by Wilcoxon sign rank test, based on its sample. 

But if we want to test if a distribution is symmetric around its mean, based on its sample $X_1, \dots, X_n$, is it valid to first normalize $X_i$ by the sample mean as $Y_i := X_i - \bar{X}$, and then apply Wilcoxon sign rank test to $Y_i$'s?

If not, what are some ways?

Thanks and regards!",,2013-04-18 18:32:01.767
140325,44635,728.0,1,,CC BY-SA 3.0,bf522207-b2dc-4a54-8fb5-ae1af03fc385,Testing symmetry of a distribution around its mean,,2013-04-18 18:32:01.767
140748,44772,17076.0,2,,CC BY-SA 3.0,346fc206-855b-487f-b7ca-73ba38240b2f,"Like many, I have stumbled across this site in an attempt to answer a stats question and I like what I see! What a great resource!

I am doubting myself on which analysis to run for the following:
18 participants were evaluated at 4 time points with different conditions at each time.
They were given scores (on a discrete visual analog scale) by 2 raters.

Is that a 2-way repeated measures ANOVA? Some variation of Friedman test?

Ugh - my brain is fried from a rough week. Thanks for the help!",,2013-04-20 21:00:03.237
140750,44772,17076.0,3,,CC BY-SA 3.0,346fc206-855b-487f-b7ca-73ba38240b2f,<anova><repeated-measures><multilevel-analysis>,,2013-04-20 21:00:03.237
140749,44772,17076.0,1,,CC BY-SA 3.0,346fc206-855b-487f-b7ca-73ba38240b2f,"Appropriate Analysis for ordinal variable, repeated 4 times under different conditions, by the same 2 raters",,2013-04-20 21:00:03.237
140767,44772,,24,,CC BY-SA 3.0,812b209f-806b-4401-a5fd-ba496547fb7f,,Proposed by 22468 approved by 930 edit id of 3243,2013-04-20 22:27:20.340
140768,44772,16174.0,5,,CC BY-SA 3.0,812b209f-806b-4401-a5fd-ba496547fb7f,"I am doubting myself on which analysis to run for the following:
18 participants were evaluated at 4 time points with different conditions at each time.
They were given scores (on a discrete visual analog scale) by 2 raters.

Is that a 2-way repeated measures ANOVA? Some variation of Friedman test?
",Removed text not related to question,2013-04-20 22:27:20.340
140830,16537,,24,,CC BY-SA 3.0,5271a08a-9cf5-4dc9-a772-d937b700e477,,"Proposed by 9007 approved by 805, 930 edit id of 3247",2013-04-21 07:54:21.450
142801,45457,17179.0,2,,CC BY-SA 3.0,ca8dd658-0510-4d78-a3b0-44a4d22d9762,"I have a the following time series 


      Price      BrokerID 632 Behaviour  BrokerID 680 Behaviour ...BrokerID XYZ Behaviour

      5.6          IP                       SP                   
      5.7          BP                       IP
      5.8          SP                       BP
      5.83         IP                       SP

where IP =Idle position , buying and ,selling position .I want to use Broker behaviour as the known variable and price as the hidden variable and predict it using HMM .But my question is how to find the emission matrix between a character vector (broker behaviour) and price numeric vector ",,2013-04-29 17:03:08.587
142803,45457,17179.0,3,,CC BY-SA 3.0,ca8dd658-0510-4d78-a3b0-44a4d22d9762,<machine-learning><hidden-markov-model>,,2013-04-29 17:03:08.587
140831,16537,,5,,CC BY-SA 3.0,5271a08a-9cf5-4dc9-a772-d937b700e477,"[Deep Learning][1] got a lot of focus since 2006. It's basically an approach to train deep neural networks and is leading to really impressive results on very hard datasets (like document clustering or object recognition). Some people are talking about the second neural network renaissance (eg in [this Google talk][2] by Schmidhuber).

If you want to be impressed you should look at this Science paper [Reducing the Dimensionality of Data with Neural Networks,][3] Hinton & Salakhutdinov.

(There is so much work going on right now in that area, that there is only two upcoming books I know about that will treat it: [Large scale machine learning][4], Langford et al and [Machine Learning: a probabilistic perspective][5] by Kevin Murphy.)

If you want to know more, check out what the main deep learning groups are doing: [Stanford][6], [Montreal][7] and most importantly [Toronto #1][8] and [Toronto #2][9].


  [1]: http://deeplearning.net/
  [2]: http://www.youtube.com/watch?v=rkCNbi26Hds
  [3]: http://www.utstat.toronto.edu/~rsalakhu/papers/science.pdf
  [4]: http://www.cambridge.org/aus/catalogue/catalogue.asp?isbn=9780521192248
  [5]: http://www.cs.ubc.ca/~murphyk/MLbook/index.html
  [6]: http://www.cs.stanford.edu/people/ang/
  [7]: http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html
  [8]: http://www.cs.toronto.edu/~hinton/
  [9]: http://www.utstat.toronto.edu/~rsalakhu/",Update link,2013-04-21 07:54:21.450
141367,44772,17076.0,5,,CC BY-SA 3.0,acf6ad29-8615-459d-9b1b-b991014c8698,"I am doubting myself on which analysis to run for the following:
18 participants were evaluated at 4 time points with different conditions at each time.
They were given scores (on a discrete visual analog scale) by 2 raters.

The scores were calculated for a pair of participants: the pairs changed at each time point.
I do know which participant comprises each pair.

Is that a 2-way repeated measures ANOVA? Some variation of Friedman test?
",added 146 characters in body,2013-04-23 17:52:53.203
141488,40870,1923.0,5,,CC BY-SA 3.0,36929b06-7550-4410-9fea-2365df8c3e7f,"A common strategy in mass spectrometry of biological molecules is to upload observed spectra to a server so that they can be matched to a LARGE database of theoretical spectra of known molecules (a.k.a. *target* database). In order to control for false positives, a *decoy* database consisting of incorrect/irrelevant spectra is used.

I have been reading more into this subject and have come up some questions regarding the calculation of the FDR measure from this target-decoy strategy. The basic idea of the FDR value is very intuitive: 

$FDR = \frac{FP}{FP + TP}$

where FP and TP stands for false and true positives respectively. This makes perfect sense to me; if I'm trying to guess some peoples' names out of a phone book, and get 8 right and 2 wrong, I would have 2 *false* out of 10 *total* guesses, and thus my false discovery rate would be 20%.

However reading [this tutorial][1] on how this is done in large scale on the servers, I got introduced to two different calculations, depending on whether or not the *target* and *decoy* databases are concatenated (page 2).


I don't think that this is a typo as I found other occurrences <sup>*</sup> of the mysterious factor 2 in front of FP in scientific literature. However the motivation behind this is never explained (at least I couldn't find it). 

I would appreciate some insight on where this doubling comes from. Likewise I wonder whether or not FDR calculation this way **assumes** that the error rate for each spectra match is the same for the target database and decoy database (i.e. *assuming* that getting 25 decoy hits *implies* 25 target hits are also false positives). It's not really clear for me why the error rate has to be the same for the two databases. Any comments on this subject is also appreciated.

<sub>* one such reference is Elias et al Nature Methods - 2, 667 - 675 (2005) </sub>

  [1]: http://www.proteored.org/pme6/fdr_calculation_for_pme6.pdf
  [2]: doi:10.1038/nmeth785",added 10 characters in body,2013-04-24 08:39:29.520
142246,45279,9095.0,2,,CC BY-SA 3.0,378f4df1-33d6-4625-a2f8-59f5b2c56a72,"I am using latent class analysis to cluster a sample of observations based on a set of binary variables. I am using R and the package poLCA. In LCA, you must specify the number of clusters you want to find. In practice, people usually run several models, each specifying a different number of classes, and then use various criteria to determine which is the ""best"" explanation of the data. 

I often find it very useful to look across the various models to try to understand how observations classified in model with class=(i) are distributed by the model with class = (i+1). At the very least you can sometimes find very robust clusters that exist regardless of the number of classes in the model. 

I would like a way to graph these relationships, to more easily communicate these complex results in papers and to colleagues who aren't statistically oriented. I imagine this is very easy to do in R using some kind of simple network graphics package, but I simply don't know how.

Could anyone please point me in the right direction. Below is code to reproduce an example dataset. Each vector xi represents the classification of 100 observations, for number of possible classes i = 1, 2, 3, 4, 5. 

    x1 <- sample(1:1, 100, replace=T)
    x2 <- sample(1:2, 100, replace=T)
    x3 <- sample(1:3, 100, replace=T)
    x4 <- sample(1:4, 100, replace=T)
    x5 <- sample(1:5, 100, replace=T)
    
    results <- cbind (x1, x2, x3, x4, x5)

I imagine there is a way to produce a graph where the nodes are classifications and the edges reflect (by weights, or color maybe) the % of observations moving from classifications from one model to the next. E.g. 

![enter image description here][1]


  [1]: https://i.stack.imgur.com/muEii.png",,2013-04-26 17:31:28.260
142248,45279,9095.0,3,,CC BY-SA 3.0,378f4df1-33d6-4625-a2f8-59f5b2c56a72,<data-visualization><mixture-distribution><latent-class>,,2013-04-26 17:31:28.260
142247,45279,9095.0,1,,CC BY-SA 3.0,378f4df1-33d6-4625-a2f8-59f5b2c56a72,Visualizing results from multiple latent class models,,2013-04-26 17:31:28.260
142251,45280,17326.0,3,,CC BY-SA 3.0,a11eeec7-7e28-49f6-ac1b-7ccb111b0075,<hypothesis-testing><continuous-data>,,2013-04-26 17:33:27.823
142250,45280,17326.0,1,,CC BY-SA 3.0,a11eeec7-7e28-49f6-ac1b-7ccb111b0075,Statistical test to show association of any kind between two variables,,2013-04-26 17:33:27.823
142249,45280,17326.0,2,,CC BY-SA 3.0,a11eeec7-7e28-49f6-ac1b-7ccb111b0075,"I have two continuous variables which I have data from a physics exspirment.

I want to test for association between the two variables but without assuming a monotonic relationship. I also only have 6 data point each with a large error associated with it and want the test to take this into consideration.

Does anyone know of a statistical test of this type?",,2013-04-26 17:33:27.823
142279,45279,,25,,,38accfd7-0742-4a77-9a4d-e3d0c4906ba1,,http://twitter.com/#!/StackStats/status/327874572810596353,2013-04-26 19:59:24.240
142767,45279,9095.0,5,,CC BY-SA 3.0,22c59796-74fa-454a-8989-7f029494d459,"I am using latent class analysis to cluster a sample of observations based on a set of binary variables. I am using R and the package poLCA. In LCA, you must specify the number of clusters you want to find. In practice, people usually run several models, each specifying a different number of classes, and then use various criteria to determine which is the ""best"" explanation of the data. 

I often find it very useful to look across the various models to try to understand how observations classified in model with class=(i) are distributed by the model with class = (i+1). At the very least you can sometimes find very robust clusters that exist regardless of the number of classes in the model. 

I would like a way to graph these relationships, to more easily communicate these complex results in papers and to colleagues who aren't statistically oriented. I imagine this is very easy to do in R using some kind of simple network graphics package, but I simply don't know how.

Could anyone please point me in the right direction. Below is code to reproduce an example dataset. Each vector xi represents the classification of 100 observations, in a model with i possible classes. I want to graph how observations (rows) move from class to class across the columns. 

    x1 <- sample(1:1, 100, replace=T)
    x2 <- sample(1:2, 100, replace=T)
    x3 <- sample(1:3, 100, replace=T)
    x4 <- sample(1:4, 100, replace=T)
    x5 <- sample(1:5, 100, replace=T)
    
    results <- cbind (x1, x2, x3, x4, x5)

I imagine there is a way to produce a graph where the nodes are classifications and the edges reflect (by weights, or color maybe) the % of observations moving from classifications from one model to the next. E.g. 

![enter image description here][1]


  [1]: https://i.stack.imgur.com/muEii.png",added 71 characters in body,2013-04-29 14:34:50.910
142802,45457,17179.0,1,,CC BY-SA 3.0,ca8dd658-0510-4d78-a3b0-44a4d22d9762,Predicting High Frequency Finance time series with HMM,,2013-04-29 17:03:08.587
142851,45457,16174.0,6,,CC BY-SA 3.0,0baa5beb-9aaf-4cb5-9dd3-ea57ce80ee88,<r><machine-learning><hidden-markov-model>,"Added tag ""r"" according to question's comments",2013-04-29 20:18:56.040
143035,45534,17447.0,3,,CC BY-SA 3.0,bc3a2f15-7af9-4188-8fca-181330e1cdac,<density-function><cumulative-distribution-function><paradox>,,2013-04-30 15:18:37.443
143036,45534,17447.0,1,,CC BY-SA 3.0,bc3a2f15-7af9-4188-8fca-181330e1cdac,question about harrington paradox (my coursework),,2013-04-30 15:18:37.443
143037,45534,17447.0,2,,CC BY-SA 3.0,bc3a2f15-7af9-4188-8fca-181330e1cdac,"2. Model  
The firm and enforcement agency interact in more than one domain. This may arise because a single agency is responsible for enforcing more than one regulation or because it enforces the same regulation at more than one constituent plant of a multi-plant firm.
For simplicity we will assume that the number of domains is two and that they are ex ante identical. In each domain the firm is required to comply with a regulation. If it complies it inflicts no environmental damage otherwise it inflicts damage d, which is commonly observed. The cost to the ith firm of compliance in domain j [ h1, 2j will be denoted cij where ci 1 and ci 2 are independent, privately observed draws from a distribution f(c) with associated cumulative F(c). F is common knowledge.   
If the agency observes non-compliance by a firm in either domain it can take that firm to court (‘‘pursue’’ the firm), in which case the firm is subject to a penalty L which is exogenous. Penalties are assumed to be restricted in the sense that
F(L) < 1. This implies that a policy of full-pursuit, whereby the agency pursues all 3
violations, will not generate full-compliance.
The firm and enforcement agency are both risk neutral and aim to maximise
expected profit and minimise expected environmental damage respectively.


can someone explain to me what F(L) < 1 implies?

if you need the context behind this model, please tell me ill explain that as well",,2013-04-30 15:18:37.443
143039,45536,15663.0,2,,CC BY-SA 3.0,9b4a0dca-4916-404c-bb2b-1dc5ef87265c,"It means that the fine is lower than the compliance cost.

This is what Harrington Paradox (http://en.wikipedia.org/wiki/Harrington_paradox) show: 

In the case of rational economics entities a firm will maximize its profit. This is not what is observed in reality. In theory, if the fine is lower than compliance cost a rationnal entity will not pay.  In reality the fine is lower than compliance cost, but firms pay.

This suggest image concern ( or altruism....)

",,2013-04-30 15:36:32.853
143062,45543,17454.0,1,,CC BY-SA 3.0,6c1a6483-bd0a-48d6-9b6b-e62295e7e01f,Is the square root of the symmetric Kullback-Leibler divergence a metric?,,2013-04-30 17:27:21.667
143063,45543,17454.0,3,,CC BY-SA 3.0,6c1a6483-bd0a-48d6-9b6b-e62295e7e01f,<kullback-leibler><metric>,,2013-04-30 17:27:21.667
143061,45543,17454.0,2,,CC BY-SA 3.0,6c1a6483-bd0a-48d6-9b6b-e62295e7e01f,"It is well known that the square root of the Jensen-Shannon divergence is a true metric, but how about the symmetric KL: D(P||Q)+D(Q||P)? I have reasons to believe that it also is a true metric but cannot find any references on that other than anecdotal comments such as that it behaves more like a metric when used.",,2013-04-30 17:27:21.667
143065,45543,17454.0,5,,CC BY-SA 3.0,f880a338-7a31-4312-8890-7530c0a2a895,"It is well known that the square root of the Jensen-Shannon divergence is a true metric, but how about the square root of symmetric KL: D(P||Q)+D(Q||P)? I have reasons to believe that it also is a true metric but cannot find any references on that other than anecdotal comments such as that it behaves more like a metric when used.",added 15 characters in body,2013-04-30 17:36:18.407
143112,45543,17454.0,5,,CC BY-SA 3.0,95c95e40-e4e7-4bd2-8f99-a223d24d12b8,"It is well known that the square root of the Jensen-Shannon divergence is a true metric, but how about the square root of symmetric KL: D(P||Q)+D(Q||P)? I have reasons to believe that it also is a true metric but cannot find any references on that other than anecdotal comments such as that it behaves more like a metric when used.

Updated

Kullback-Leibler divergence: $D(P||Q) = \sum_i p_i\log(p_i/q_i)$

Jensen-Shannon divergence: $J(P,Q) = \big(D(P||(P+Q)/2)+D(Q||(P+Q)/2)\big)/2$

Symmetric KL divergence: $S(P,Q) = D(P||Q)+D(Q||P) = \sum_i (p_i-q_i)\log(p_i/q_i)$

Square root of symmetric KL: $d_{KL}(P,Q) = \sqrt{S(P,Q)}$ 

Is $d_{KL}$ a metric?

",Added math to define the question more exactly,2013-04-30 20:30:42.553
143211,45543,17454.0,5,,CC BY-SA 3.0,a14a6e7f-686a-4e3a-92f3-bde20248f427,"It is well known that the square root of the Jensen-Shannon divergence is a true metric, but how about the square root of symmetric KL: D(P||Q)+D(Q||P)? I have reasons to believe that it also is a true metric but cannot find any references on that other than anecdotal comments such as that it behaves more like a metric when used.

Update 1

Kullback-Leibler divergence: $D(P||Q) = \sum_i p_i\log(p_i/q_i)$

Jensen-Shannon divergence: $J(P,Q) = \big(D(P||(P+Q)/2)+D(Q||(P+Q)/2)\big)/2$

Symmetric KL divergence: $S(P,Q) = D(P||Q)+D(Q||P) = \sum_i (p_i-q_i)\log(p_i/q_i)$

Square root of symmetric KL: $d_{KL}(P,Q) = \sqrt{S(P,Q)}$ 

Is $d_{KL}$ a metric?

Update 2
I think the following upper and lower bounds hold:

$\sum_i (p_i-q_i)^2 \leq  \sum_i (p_i-q_i)\log(p_i/q_i) \leq  \sum_i \log(p_i/q_i)^2$

Both of the square root of the bounds are metrics, I suppose, since they are the square of the Euclidean distances in the probability space and the log-prob space respectively. 


",added 337 characters in body,2013-05-01 09:20:46.710
143216,45543,17454.0,5,,CC BY-SA 3.0,a8c02a64-4ffa-402a-90ec-4a95cc887ee7,"It is well known that the square root of the Jensen-Shannon divergence is a true metric, but how about the square root of symmetric KL: D(P||Q)+D(Q||P)? I have reasons to believe that it also is a true metric but cannot find any references on that other than anecdotal comments such as that it behaves more like a metric when used.

Update 1

Kullback-Leibler divergence: $D(P||Q) = \sum_i p_i\log(p_i/q_i)$

Jensen-Shannon divergence: $J(P,Q) = \big(D(P||(P+Q)/2)+D(Q||(P+Q)/2)\big)/2$

Symmetric KL divergence: $S(P,Q) = D(P||Q)+D(Q||P) = \sum_i (p_i-q_i)\log(p_i/q_i)$

Square root of symmetric KL: $d_{KL}(P,Q) = \sqrt{S(P,Q)}$ 

Is $d_{KL}$ a metric?

Update 2

I think the following upper and lower bounds hold:

$\sum_i (p_i-q_i)^2 \leq  \sum_i (p_i-q_i)\log(p_i/q_i) \leq  \sum_i \log(p_i/q_i)^2$

Both of the square root of the bounds are metrics, I suppose, since they are the square of the Euclidean distances in the probability space and the log-prob space respectively. 


",Added more info on the problem,2013-05-01 09:36:54.540
144862,46070,17678.0,5,,CC BY-SA 3.0,28ffc670-5a1f-4cd3-a0c8-42876df5ba81,"I think this is a basic question, but maybe I am confusing the concepts.

Suppose I fit an ARIMA model to a time series using, for example, the function auto.arima() in the R forecast package. The model assumes constant variance. How do I obtain that variance? Is it the variance of the residuals?

If I use the model for forecasting, I know that it gives me the conditional mean. I'd like to know the (constant) variance as well.

Thank you.

Bruno

<hr>

<h2>Update 1:</h2>

I added some code below. The variance given by `sigma2` isn't close to the one calculated from the data as if they were i.i.d samples. I'm still wondering if `sigma2` is the right option. See figure below for time series plot.

    demand.train <- c(10.06286, 9.56286, 10.51914, 12.39571, 14.72857, 15.89429, 15.89429, 17.06143, 
                  17.72857, 16.56286, 14.23000, 15.39571, 13.06286, 15.39571, 15.39571, 16.56286,
                  16.21765, 15.93449, 14.74856, 14.46465, 15.38132)
    timePoints.train <- c(""Q12006"", ""Q22006"", ""Q32006"", ""Q12007"", ""Q22007"", ""Q32007"", ""Q12008"", ""Q22008"",
                          ""Q32008"", ""Q12009"", ""Q22009"", ""Q32009"", ""Q12010"", ""Q22010"", ""Q32010"", ""Q12011"",
                          ""Q22011"", ""Q32011"", ""Q12012"", ""Q22012"", ""Q32012"")
    
    plot(1:length(timePoints.train), demand.train, type=""o"", xaxt=""n"", ylim=c(0, max(demand.train) + 2), 
         ylab=""Demand"", xlab=""Quadrimestre"")

    title(main=""Time Series Demand of Product C"", font.main=4)
    axis(1, at=1:length(timePoints.train), labels=timePoints.train)
    box()
    
    ### ARIMA Fit
    library(forecast)
    
    # Time series
    demandts.freq <- 3
    demandts.train <- ts(demand.train, frequency=demandts.freq, start=c(2006, 1))
    
    # Model fitting
    demandts.train.arima <- auto.arima(demandts.train, max.p=10, max.q=10, max.P=10, max.Q=10, max.order=10)
    print(demandts.train.arima)
    summary(demandts.train.arima)
    demandts.train.arima.fit <- fitted(demandts.train.arima)
    
    # Forecast ARIMA (conditional means)
    demandts.arima.forecast <- forecast(demandts.train.arima, h = 3, level=95)
    print(demandts.arima.forecast)
    
    # Constant variance from ARIMA
    demandts.arima.var <- demandts.train.arima$sigma2
    print(demandts.arima.var)
    
    # Variance from data
    print(var(demandts.train))


![Time Series Plot][1]


  [1]: https://i.stack.imgur.com/E5gv0.png",added 2007 characters in body,2013-05-08 15:15:58.943
143509,45279,9095.0,5,,CC BY-SA 3.0,97786d81-40b7-4360-8038-a077d49f35c3,"I am using latent class analysis to cluster a sample of observations based on a set of binary variables. I am using R and the package poLCA. In LCA, you must specify the number of clusters you want to find. In practice, people usually run several models, each specifying a different number of classes, and then use various criteria to determine which is the ""best"" explanation of the data. 

I often find it very useful to look across the various models to try to understand how observations classified in model with class=(i) are distributed by the model with class = (i+1). At the very least you can sometimes find very robust clusters that exist regardless of the number of classes in the model. 

I would like a way to graph these relationships, to more easily communicate these complex results in papers and to colleagues who aren't statistically oriented. I imagine this is very easy to do in R using some kind of simple network graphics package, but I simply don't know how.

Could anyone please point me in the right direction. Below is code to reproduce an example dataset. Each vector xi represents the classification of 100 observations, in a model with i possible classes. I want to graph how observations (rows) move from class to class across the columns. 

    x1 <- sample(1:1, 100, replace=T)
    x2 <- sample(1:2, 100, replace=T)
    x3 <- sample(1:3, 100, replace=T)
    x4 <- sample(1:4, 100, replace=T)
    x5 <- sample(1:5, 100, replace=T)
    
    results <- cbind (x1, x2, x3, x4, x5)

I imagine there is a way to produce a graph where the nodes are classifications and the edges reflect (by weights, or color maybe) the % of observations moving from classifications from one model to the next. E.g. 

![enter image description here][1]


  [1]: https://i.stack.imgur.com/muEii.png

UPDATE: Having some progress with the igraph package. Starting from the code above...

poLCA results recycle the same numbers to describe class membership, so you need to do a bit of recoding. 

    N<-ncol(results) 
    n<-0
    for(i in 2:N) {
    results[,i]<- (results[,i])+((i-1)+n)
    n<-((i-1)+n)
    }

Then you need to get all the cross-tabulations and their frequencies, and rbind them into one matrix defining all the edges. There is probably a much more elegant way to do this. 

    results <-as.data.frame(results)
    
    g1           <- count(results,c(""x1"", ""x2""))
      
    g2           <- count(results,c(""x2"", ""x3""))
    colnames(g2) <- c(""x1"", ""x2"", ""freq"")
      
    g3           <- count(results,c(""x3"", ""x4""))
    colnames(g3) <- c(""x1"", ""x2"", ""freq"")
      
    g4           <- count(results,c(""x4"", ""x5""))
    colnames(g4) <- c(""x1"", ""x2"", ""freq"")
    
    results <- rbind(g1, g2, g3, g4)
      
    library(igraph)
      
    g1 <- graph.data.frame(results, directed=TRUE)
    
    tkplot(g1)

",added 1064 characters in body,2013-05-02 15:18:21.187
143513,45279,9095.0,5,,CC BY-SA 3.0,ee984bb1-4ecb-4131-90d5-d0ad162e975d,"I am using latent class analysis to cluster a sample of observations based on a set of binary variables. I am using R and the package poLCA. In LCA, you must specify the number of clusters you want to find. In practice, people usually run several models, each specifying a different number of classes, and then use various criteria to determine which is the ""best"" explanation of the data. 

I often find it very useful to look across the various models to try to understand how observations classified in model with class=(i) are distributed by the model with class = (i+1). At the very least you can sometimes find very robust clusters that exist regardless of the number of classes in the model. 

I would like a way to graph these relationships, to more easily communicate these complex results in papers and to colleagues who aren't statistically oriented. I imagine this is very easy to do in R using some kind of simple network graphics package, but I simply don't know how.

Could anyone please point me in the right direction. Below is code to reproduce an example dataset. Each vector xi represents the classification of 100 observations, in a model with i possible classes. I want to graph how observations (rows) move from class to class across the columns. 

    x1 <- sample(1:1, 100, replace=T)
    x2 <- sample(1:2, 100, replace=T)
    x3 <- sample(1:3, 100, replace=T)
    x4 <- sample(1:4, 100, replace=T)
    x5 <- sample(1:5, 100, replace=T)
    
    results <- cbind (x1, x2, x3, x4, x5)

I imagine there is a way to produce a graph where the nodes are classifications and the edges reflect (by weights, or color maybe) the % of observations moving from classifications from one model to the next. E.g. 

![enter image description here][1]


UPDATE: Having some progress with the igraph package. Starting from the code above...

poLCA results recycle the same numbers to describe class membership, so you need to do a bit of recoding. 

    N<-ncol(results) 
    n<-0
    for(i in 2:N) {
    results[,i]<- (results[,i])+((i-1)+n)
    n<-((i-1)+n)
    }

Then you need to get all the cross-tabulations and their frequencies, and rbind them into one matrix defining all the edges. There is probably a much more elegant way to do this. 

    results <-as.data.frame(results)
    
    g1           <- count(results,c(""x1"", ""x2""))
      
    g2           <- count(results,c(""x2"", ""x3""))
    colnames(g2) <- c(""x1"", ""x2"", ""freq"")
      
    g3           <- count(results,c(""x3"", ""x4""))
    colnames(g3) <- c(""x1"", ""x2"", ""freq"")
      
    g4           <- count(results,c(""x4"", ""x5""))
    colnames(g4) <- c(""x1"", ""x2"", ""freq"")
    
    results <- rbind(g1, g2, g3, g4)
      
    library(igraph)
      
    g1 <- graph.data.frame(results, directed=TRUE)
    
    plot.igraph(g1, layout=layout.reingold.tilford)

![enter image description here][2]

Time to play more with the igraph options I guess. 

  [1]: https://i.stack.imgur.com/muEii.png
  [2]: https://i.stack.imgur.com/iCJ2Z.png",added 169 characters in body,2013-05-02 15:29:34.723
143827,45804,17580.0,3,,CC BY-SA 3.0,f09d1a96-91a7-4fc4-9d6d-412b74fb16e0,<similarities><genetic-algorithms><function><kolmogorov-smirnov-test>,,2013-05-03 21:05:29.093
143828,45804,17580.0,2,,CC BY-SA 3.0,f09d1a96-91a7-4fc4-9d6d-412b74fb16e0,"I am a first-year grad student in Computer Science, and I need some help with a problem that I think is statistically oriented. I have taken a statistics course, but it was abysmal and I haven't had time to rectify that. But anyway, my problem stems from a project I'm working on involving genetic programming, where I'm randomly generating functions. Please bear with my description, as it's been a while since I've had a formal theory course too.


I have two continuous (but not onto) functions **F** and **G**, both of which map **N** variables to a single output. The domain of the input variables is the integers between -100 and 100. The range of the output is the Real numbers. I want to find some statistical measure of how ""similar"" the two functions are; given the finite inputs (of which there will be 201^N possible), how much variance(?) there is between the two functions outputs. Two identical functions should return no variance, and two wildly different functions should return a high variance.


Since **N** will typically be greater than 6, I can't iterate through all the possible inputs and compare the outputs, so I figured I could take some sampling at regular intervals (e.g. every multiple of 10, so that it's only 10^N). But here's about where I realize I have no idea what I'm doing. How do I determine if two numbers are ""highly variant"" from each other? What sample size do I need to use to have confidence in my results?

My current approach is to compare the functions with a two-sided Kolmogorov-Smirnov Test. Since that test doesn't seem to scale well to multi-variate problems, I've taken advantage of my limited domains to just treat the problem as having a single variable by concatenating my variables. So the first value of the variable is (-100:100:100:100:100:100), the second is (-100:100:100:100:100:099), and the last is (100:100:100:100:100:100). Does that even make sense?",,2013-05-03 21:05:29.093
143826,45804,17580.0,1,,CC BY-SA 3.0,f09d1a96-91a7-4fc4-9d6d-412b74fb16e0,Finding the similarity between two functions,,2013-05-03 21:05:29.093
144685,46070,17678.0,3,,CC BY-SA 3.0,52ab564e-92d6-4305-a134-592e75304928,<r><variance><arima>,,2013-05-07 19:48:58.193
144684,46070,17678.0,1,,CC BY-SA 3.0,52ab564e-92d6-4305-a134-592e75304928,Variance of a Time Series Fitted to an ARIMA Model,,2013-05-07 19:48:58.193
144683,46070,17678.0,2,,CC BY-SA 3.0,52ab564e-92d6-4305-a134-592e75304928,"I think this is a basic question, but maybe I am confusing the concepts.

Suppose I fit an ARIMA model to a time series using, for example, the function auto.arima() in the R forecast package. The model assumes constant variance. How do I obtain that variance? Is it the variance of the residuals?

If I use the model for forecasting, I know that it gives me the conditional mean. I'd like to know the (constant) variance as well.

Thank you.

Bruno",,2013-05-07 19:48:58.193
144776,46070,,4,user88,CC BY-SA 3.0,c529a8a4-82b1-44e8-a534-933108bf0c22,Variance of a time series fitted to an ARIMA model,edited title,2013-05-08 07:52:15.497
150746,47846,17994.0,2,,CC BY-SA 3.0,5f0c0ec7-5a81-4865-bd25-b59c43fcee75,"I am looking for some information about the difference between binomial, negative binomial and Poisson regression and for which situations are these regression best fitted. 

Are there any tests I can preform in SPSS that can tell me which of these regressions is the best for my situation?

Also, how do I run a Poisson or negative binomial in SPSS, since there are no options like that as I can see in the regression part?

if you have any useful links I would appreciate it very much.

Thank you,
",,2013-06-02 09:36:07.877
150744,47846,17994.0,3,,CC BY-SA 3.0,5f0c0ec7-5a81-4865-bd25-b59c43fcee75,<binomial-distribution><poisson-distribution><negative-binomial-distribution>,,2013-06-02 09:36:07.877
185573,57248,10278.0,2,,CC BY-SA 3.0,aa37ca6b-1998-4188-87ae-176c040d5644,"If you treat the variable as ordinal you are assuming a gene-dosage effect.
This is essentially a two degree of freedom test.
If you treat the variable as nominal you are not assuming any gene-dosage effect and instead comparing the mean of the three genotype groups this is a one degree of freedom test.
Hence the gene-dosage model (treating genotypes as ordinal) is more powerful. ",,2013-10-10 19:25:29.377
144891,46070,17678.0,5,,CC BY-SA 3.0,7837fdca-d405-4deb-9d02-5135bb23d80b,"I think this is a basic question, but maybe I am confusing the concepts.

Suppose I fit an ARIMA model to a time series using, for example, the function auto.arima() in the R forecast package. The model assumes constant variance. How do I obtain that variance? Is it the variance of the residuals?

If I use the model for forecasting, I know that it gives me the conditional mean. I'd like to know the (constant) variance as well.

Thank you.

Bruno

<hr>

<h2>Update 1:</h2>

I added some code below. The variance given by `sigma2` isn't close to the one calculated from the fitted values. I'm still wondering if `sigma2` is the right option. See figure below for time series plot.

    demand.train <- c(10.06286, 9.56286, 10.51914, 12.39571, 14.72857, 15.89429, 15.89429, 17.06143, 
                  17.72857, 16.56286, 14.23000, 15.39571, 13.06286, 15.39571, 15.39571, 16.56286,
                  16.21765, 15.93449, 14.74856, 14.46465, 15.38132)
    timePoints.train <- c(""Q12006"", ""Q22006"", ""Q32006"", ""Q12007"", ""Q22007"", ""Q32007"", ""Q12008"", ""Q22008"",
                          ""Q32008"", ""Q12009"", ""Q22009"", ""Q32009"", ""Q12010"", ""Q22010"", ""Q32010"", ""Q12011"",
                          ""Q22011"", ""Q32011"", ""Q12012"", ""Q22012"", ""Q32012"")
    
    plot(1:length(timePoints.train), demand.train, type=""o"", xaxt=""n"", ylim=c(0, max(demand.train) + 2), 
         ylab=""Demand"", xlab=""Quadrimestre"")

    title(main=""Time Series Demand of Product C"", font.main=4)
    axis(1, at=1:length(timePoints.train), labels=timePoints.train)
    box()
    
    ### ARIMA Fit
    library(forecast)
    
    # Time series
    demandts.freq <- 3
    demandts.train <- ts(demand.train, frequency=demandts.freq, start=c(2006, 1))
    
    # Model fitting
    demandts.train.arima <- auto.arima(demandts.train, max.p=10, max.q=10, max.P=10, max.Q=10, max.order=10)
    print(demandts.train.arima)
    summary(demandts.train.arima)
    demandts.train.arima.fit <- fitted(demandts.train.arima)
    
    # Forecast ARIMA (conditional means)
    demandts.arima.forecast <- forecast(demandts.train.arima, h = 3, level=95)
    print(demandts.arima.forecast)
    
    # Constant variance from ARIMA
    demandts.arima.var <- demandts.train.arima$sigma2
    print(demandts.arima.var)
    
    # Variance from fitted values
    print(var(demandts.train.arima.fit))


![Time Series Plot][1]


  [1]: https://i.stack.imgur.com/E5gv0.png",deleted 11 characters in body,2013-05-08 17:27:21.287
145762,46384,15839.0,2,,CC BY-SA 3.0,b353bccf-17d3-47e7-a797-4cb6190acfe4,What is the difference of gaussian HMM and gaussian mixture HMM (the emission is gaussian or  gaussian mixture)? I want to know if it is the same thing. What is the point when estimating the parameters using Baum Welch algorithm.,,2013-05-13 01:51:18.740
145761,46384,15839.0,1,,CC BY-SA 3.0,b353bccf-17d3-47e7-a797-4cb6190acfe4,gaussian mixture HMM,,2013-05-13 01:51:18.740
145763,46384,15839.0,3,,CC BY-SA 3.0,b353bccf-17d3-47e7-a797-4cb6190acfe4,<hidden-markov-model>,,2013-05-13 01:51:18.740
147491,46894,18085.0,3,,CC BY-SA 3.0,83503990-ae8e-4d1e-a7ed-192f418499be,<multiple-imputation><vector-autoregression>,,2013-05-19 14:17:34.503
147492,46894,18085.0,2,,CC BY-SA 3.0,83503990-ae8e-4d1e-a7ed-192f418499be,"I have a question concerning the coefficients of VAR models used on multiple imputed data (high missigness in some variables: up to 40%).
In particular I would like to know how the coefficients are related to the explained variance. 

I have used vector autoregression on multiple imputed data (m=10) and have then combined the estimated coefficient with rubin's rule.
However, what confuses me is the fact that my imputation variance is quite small in relationship to the estimates and variance of coefficients, but the difference between the explained variance is huge (17% to 0.04%) between models.

My idea is that since the highest imputation variance across all systems is at the constant (around a third of the variance value but 3-4 times higher then in other coefficients) and that this critically affects the explained variance.
But thats just a guess.

I would be very happy if somebody could help me here.",,2013-05-19 14:17:34.503
147493,46894,18085.0,1,,CC BY-SA 3.0,83503990-ae8e-4d1e-a7ed-192f418499be,Imputation variance and explained variance (in vector autoregression),,2013-05-19 14:17:34.503
147590,37981,,4,user88,CC BY-SA 3.0,1bc3c281-c8b2-41db-8463-ad3b761d435a,"""Peakedness"" of a skewed probability density function",edited title,2013-05-19 22:47:17.833
149359,47447,18356.0,3,,CC BY-SA 3.0,4482f89f-84bb-4753-aecd-48204430063f,<time-series><count-data><intervention-analysis>,,2013-05-27 18:56:55.533
149361,47447,18356.0,1,,CC BY-SA 3.0,4482f89f-84bb-4753-aecd-48204430063f,How to fit a simple count time series INAR(1) model in R,,2013-05-27 18:56:55.533
149360,47447,18356.0,2,,CC BY-SA 3.0,4482f89f-84bb-4753-aecd-48204430063f,"I am trying to perform a simple time series analysis with count time series data. My data is a sequence of small integer values like 0,1,2 and 3. I learned from various sources that INAR model would be appropriate with such data. 

My question is whether anyone knows R codes for fitting a simple INAR(1) model (regressing time series data on a binary dummy variable).  

Appreciate any assistance.",,2013-05-27 18:56:55.533
149550,47497,18382.0,1,,CC BY-SA 3.0,1999dc19-2add-44c8-b460-caf0eada062f,R Code for yeo-johnson transformation,,2013-05-28 08:54:55.873
149551,47497,18382.0,2,,CC BY-SA 3.0,1999dc19-2add-44c8-b460-caf0eada062f,"I have writen a code for boxcox transformation (see below). But now I want to do a yeo-johnson Transformation because datc$plot contains zeros.I try, but I dont find a solution.

lambda.fm1 <- boxcox(datc$plot ~ datc$cond.evlot*datc$cond.dl*datc$version), family=""yjPower"")
lambda.max <- lambda.fm1$x[which.max(lambda.fm1$y)]
require(car)
datc$plott <- bcPower(datc$plot, lambda = lambda.max, jacobian.adjusted = FALSE)
",,2013-05-28 08:54:55.873
149552,47497,18382.0,3,,CC BY-SA 3.0,1999dc19-2add-44c8-b460-caf0eada062f,<r><data-transformation>,,2013-05-28 08:54:55.873
149565,47497,10579.0,5,,CC BY-SA 3.0,82a992cc-5956-4e98-bf56-4dbd49e5b6cf,"I have writen a code for `boxcox` transformation (see below). But now I want to do a yeo-johnson Transformation because `datc$plot` contains zeros. I try, but I dont find a solution.

    lambda.fm1 <- boxcox(datc$plot ~ datc$cond.evlot*datc$cond.dl*datc$version), 
                         family=""yjPower"")
    lambda.max <- lambda.fm1$x[which.max(lambda.fm1$y)]
    require(car)
    datc$plott <- bcPower(datc$plot, lambda = lambda.max, jacobian.adjusted = FALSE)

",Improved formatting.,2013-05-28 10:05:13.463
149564,47497,,24,,CC BY-SA 3.0,82a992cc-5956-4e98-bf56-4dbd49e5b6cf,,"Proposed by 13680 approved by 686, 930 edit id of 3536",2013-05-28 10:05:13.463
149612,47497,594.0,5,,CC BY-SA 3.0,ab54965b-f8af-4965-a62c-06be401b71b6,"I have writen code for a Box-Cox transformation (see below). But now I want to do a Yeo-Johnson transformation because `datc$plot` contains zeros. I try, but I don't find a solution.

    lambda.fm1 <- boxcox(datc$plot ~ datc$cond.evlot*datc$cond.dl*datc$version), 
                         family=""yjPower"")
    lambda.max <- lambda.fm1$x[which.max(lambda.fm1$y)]
    require(car)
    datc$plott <- bcPower(datc$plot, lambda = lambda.max, jacobian.adjusted = FALSE)

",formatting,2013-05-28 12:03:05.920
149614,47497,594.0,5,,CC BY-SA 3.0,f80ec0c1-8bea-474e-9fa1-a619b07c2d38,"I have writen code for a Box-Cox transformation (see below). But now I want to do a Yeo-Johnson transformation because `datc$plot` contains zeros. I tried, but I didn't find a solution.

    lambda.fm1 <- boxcox(datc$plot ~ datc$cond.evlot*datc$cond.dl*datc$version), 
                         family=""yjPower"")
    lambda.max <- lambda.fm1$x[which.max(lambda.fm1$y)]
    require(car)
    datc$plott <- bcPower(datc$plot, lambda = lambda.max, jacobian.adjusted = FALSE)

",added 3 characters in body,2013-05-28 12:08:47.473
149616,47497,594.0,4,,CC BY-SA 3.0,2c7fcb84-9f44-4c70-aa72-1fbacc34ff25,R Code for Yeo-Johnson transformation,Also fixed title,2013-05-28 12:18:56.140
150745,47846,17994.0,1,,CC BY-SA 3.0,5f0c0ec7-5a81-4865-bd25-b59c43fcee75,"Difference between binomial, negative binomial and Poisson regression",,2013-06-02 09:36:07.877
161768,50739,10492.0,3,,CC BY-SA 3.0,2b322f2f-26bd-4ba5-8f8a-b2ac9b44b523,<regression><anova>,,2013-07-14 10:14:44.800
161767,50739,10492.0,1,,CC BY-SA 3.0,2b322f2f-26bd-4ba5-8f8a-b2ac9b44b523,Linear Regression and ANOVA,,2013-07-14 10:14:44.800
150749,47846,594.0,5,,CC BY-SA 3.0,8150fce4-ce77-4c73-a3ca-1ef3175a1915,"I am looking for some information about the difference between binomial, negative binomial and Poisson regression and for which situations are these regression best fitted. 

Are there any tests I can perform in SPSS that can tell me which of these regressions is the best for my situation?

Also, how do I run a Poisson or negative binomial in SPSS, since there are no options like that as I can see in the regression part?

if you have any useful links I would appreciate it very much.

Thank you,
",edited body,2013-06-02 09:43:45.347
150753,47846,15827.0,6,,CC BY-SA 3.0,a03bcc94-a823-4c77-bd0a-3769a110b6a0,<spss><binomial-distribution><poisson-distribution><negative-binomial-distribution>,deleted 15 characters in body; edited tags,2013-06-02 09:48:13.397
150752,47846,15827.0,5,,CC BY-SA 3.0,a03bcc94-a823-4c77-bd0a-3769a110b6a0,"I am looking for some information about the difference between binomial, negative binomial and Poisson regression and for which situations are these regression best fitted. 

Are there any tests I can perform in SPSS that can tell me which of these regressions is the best for my situation?

Also, how do I run a Poisson or negative binomial in SPSS, since there are no options such as I can see in the regression part?

If you have any useful links I would appreciate it very much.


",deleted 15 characters in body; edited tags,2013-06-02 09:48:13.397
151234,47981,16990.0,2,,CC BY-SA 3.0,a6d65662-ed58-4380-99eb-0d7556c7551a,"In a Wilcoxon signed-ranks statistical significance test, we came across some data that produces a $p$-value of $0.04993$. With a threshold of $p < 0.05$, is this result enough to reject the null hypothesis, or is it safer to say the test was inconclusive, since if we round the p-value to 3 decimal places it becomes $0.050$?
",,2013-06-04 09:21:32.970
151236,47981,16990.0,3,,CC BY-SA 3.0,a6d65662-ed58-4380-99eb-0d7556c7551a,<hypothesis-testing><statistical-significance><p-value><wilcoxon>,,2013-06-04 09:21:32.970
151235,47981,16990.0,1,,CC BY-SA 3.0,a6d65662-ed58-4380-99eb-0d7556c7551a,Is a p-value of 0.04993 enough to reject null hypothesis?,,2013-06-04 09:21:32.970
151641,48103,11200.0,1,,CC BY-SA 3.0,8c37cf76-3157-4afe-9d35-8d50006e2b57,Fit a sine to data,,2013-06-05 18:23:47.483
151642,48103,11200.0,3,,CC BY-SA 3.0,8c37cf76-3157-4afe-9d35-8d50006e2b57,<r><regression><fitting>,,2013-06-05 18:23:47.483
151640,48103,11200.0,2,,CC BY-SA 3.0,8c37cf76-3157-4afe-9d35-8d50006e2b57,"although I read [this](http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r) post, I still have no idea how to apply this to my own data and hope that someone can help me out.

I have the following data:

    y <- c(11.622967, 12.006081, 11.760928, 12.246830, 12.052126, 12.346154, 12.039262, 12.362163, 12.009269, 11.260743, 10.950483, 10.522091,  9.346292,  7.014578,  6.981853,  7.197708,  7.035624,  6.785289, 7.134426,  8.338514,  8.723832, 10.276473, 10.602792, 11.031908, 11.364901, 11.687638, 11.947783, 12.228909, 11.918379, 12.343574, 12.046851, 12.316508, 12.147746, 12.136446, 11.744371,  8.317413, 8.790837, 10.139807,  7.019035,  7.541484,  7.199672,  9.090377,  7.532161,  8.156842,  9.329572, 9.991522, 10.036448, 10.797905)
    t <- 18:65

And now I simply want to fit a sine wave

$$
y(t)=A\cdot sin(\omega t+\phi) +C.
$$

with the four unknowns $A$, $\omega$, $\phi$ and $C$ to it.

The rest of my code looks is the following

    res <- nls(y ~ A*sin(omega*ct+phi)+C, data=data.frame(t,y), start=list(A=1,omega=1,phi=1,C=1))
    co <- coef(res)

    fit <- function(x, a, b, c, d) {a*sin(b*x+c)+d}

    # Plot result
    plot(x=t, y=y)
    curve(fit(x, a=co[""A""], b=co[""omega""], c=co[""phi""], d=co[""C""]), add=TRUE ,lwd=2, col=""steelblue"")

But the result is really poor.

I would very much appreciate any help.

Cheers.",,2013-06-05 18:23:47.483
151646,48103,,5,,CC BY-SA 3.0,89db7ecb-ac8c-4ca3-86ef-570b37820c46,"Although I read [this](http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r) post, I still have no idea how to apply this to my own data and hope that someone can help me out.

I have the following data:

    y <- c(11.622967, 12.006081, 11.760928, 12.246830, 12.052126, 12.346154, 12.039262, 12.362163, 12.009269, 11.260743, 10.950483, 10.522091,  9.346292,  7.014578,  6.981853,  7.197708,  7.035624,  6.785289, 7.134426,  8.338514,  8.723832, 10.276473, 10.602792, 11.031908, 11.364901, 11.687638, 11.947783, 12.228909, 11.918379, 12.343574, 12.046851, 12.316508, 12.147746, 12.136446, 11.744371,  8.317413, 8.790837, 10.139807,  7.019035,  7.541484,  7.199672,  9.090377,  7.532161,  8.156842,  9.329572, 9.991522, 10.036448, 10.797905)
    t <- 18:65

And now I simply want to fit a sine wave

$$
y(t)=A\cdot sin(\omega t+\phi) +C.
$$

with the four unknowns $A$, $\omega$, $\phi$ and $C$ to it.

The rest of my code looks is the following

    res <- nls(y ~ A*sin(omega*t+phi)+C, data=data.frame(t,y), start=list(A=1,omega=1,phi=1,C=1))
    co <- coef(res)

    fit <- function(x, a, b, c, d) {a*sin(b*x+c)+d}

    # Plot result
    plot(x=t, y=y)
    curve(fit(x, a=co[""A""], b=co[""omega""], c=co[""phi""], d=co[""C""]), add=TRUE ,lwd=2, col=""steelblue"")

But the result is really poor.

![Sine fit][1]

I would very much appreciate any help.

Cheers.


  [1]: https://i.stack.imgur.com/IS0ae.png",added 64 characters in body,2013-06-05 18:43:38.480
151711,48125,18416.0,2,,CC BY-SA 3.0,d5f99bb9-5a14-4fbc-8033-cad74168df97,"Is $||Y-X\beta||_2^2 + \lambda\beta^T K\beta$ , the standard loss-function in kernel ridge regression, or is it different? Also, is the gaussian kernel a standard choice used for the kernel, in practice? If not, which kernels are used more often than not? Also, is $\lambda$ the only parameter to be tuned via cross-validation or is the kernel parameter like $\sigma$ in a gaussian kernel, also tuned via cross validation in practice? Please confirm and/or correct my understanding of Kernel ridge regression.",,2013-06-06 01:38:15.400
151710,48125,18416.0,3,,CC BY-SA 3.0,d5f99bb9-5a14-4fbc-8033-cad74168df97,<regression><machine-learning><mathematical-statistics><kernel><ridge-regression>,,2013-06-06 01:38:15.400
151712,48125,18416.0,1,,CC BY-SA 3.0,d5f99bb9-5a14-4fbc-8033-cad74168df97,kernel ridge regression loss:,,2013-06-06 01:38:15.400
151713,48125,,25,,,5cbdf2e5-f98f-4fdc-93e1-fce0232c44ef,,http://twitter.com/#!/StackStats/status/342460977054359553,2013-06-06 02:00:34.250
162783,50982,1790.0,3,,CC BY-SA 3.0,6ac4b172-e012-4139-a772-da53984ea8c5,<cross-validation><model-selection>,,2013-07-17 13:51:44.960
162781,50982,1790.0,2,,CC BY-SA 3.0,6ac4b172-e012-4139-a772-da53984ea8c5,"Say that I have two learning methods $A$ and $B$ and that I estimate their generalization performance with something like repeated cross validation. From this process I get a **distribution of scores** $P_A$ and $P_B$ for each method (e.g. their ROC AUC values).

Looking at these distributions, it could be that $\mu_A \ge \mu_B$  but that $\sigma_A \ge \sigma_B$ (i.e. the expected generalization performance of $A$ is higher than $B$, but that there is more uncertainty about this estimation).

What **mathematical methods** can I use to compare $P_A$ and $P_B$ and eventually make an informed decision about which model to use?

**Note:** For the sake of simplicity, I am referring to two methods $A$ and $B$ here, but I am interested in methods that can be used to compare the distribution of scores of ~1000 learning methods (e.g. from a grid search) and eventually make a final decision about which model to use.

",,2013-07-17 13:51:44.960
151741,48133,594.0,2,,CC BY-SA 3.0,236d20af-4071-465a-931d-c6599a033225,"If you just want a good estimate of $\omega$ and don't care much about
its standard error:

    ssp <- spectrum(y)  
    per <- 1/ssp$freq[ssp$spec==max(ssp$spec)]
    reslm <- lm(y ~ sin(2*pi/per*t)+cos(2*pi/per*t))
    summary(reslm)

    rg <- diff(range(y))
    plot(y~t,ylim=c(min(y)-0.1*rg,max(y)+0.1*rg))
    lines(fitted(reslm)~t,col=4,lty=2)   # dashed blue line is sin fit

    # including 2nd harmonic really improves the fit
    reslm2 <- lm(y ~ sin(2*pi/per*t)+cos(2*pi/per*t)+sin(4*pi/per*t)+cos(4*pi/per*t))
    summary(reslm2)
    lines(fitted(reslm2)~t,col=3)    # solid green line is periodic with second harmonic

![sine plot][1]

(A better fit still would perhaps account for the outliers in that series in some way, reducing their influence.)

\---

If you want some idea of the uncertainty in $\omega$, you could use profile likelihood ([pdf1][2], [pdf2][3] - references on getting approximate CIs or SEs from profile likelihood or its variants aren't hard to locate)

(Alternatively, you could feed these estimates into nls ... and start it already converged.)

  [1]: https://i.stack.imgur.com/ZF1P2.png
  [2]: http://www.math.umt.edu/patterson/ProfileLikelihoodCI.pdf
  [3]: http://www.utstat.toronto.edu/reid/research/slacpaper.pdf",,2013-06-06 08:03:29.833
151973,48125,18416.0,5,,CC BY-SA 3.0,8486a4d5-54f0-4dd4-bb19-548a6901aa42,"Is $||Y-X\beta||_2^2 + \lambda\beta^T K\beta$ , the standard loss-function in kernel ridge regression, or is it different? Also, is the gaussian kernel a standard choice used for the kernel, in practice? If not, which kernels are used more often than not? Also, is $\lambda$ the only parameter to be tuned via cross-validation or is the kernel parameter like $\sigma$ in a gaussian kernel, also tuned via cross validation in practice? Please confirm and/or correct my understanding of Kernel ridge regression",deleted 1 characters in body,2013-06-07 00:01:06.023
151974,48125,18416.0,4,,CC BY-SA 3.0,8486a4d5-54f0-4dd4-bb19-548a6901aa42,Kernel Ridge Regression,deleted 1 characters in body,2013-06-07 00:01:06.023
151987,48125,18416.0,5,,CC BY-SA 3.0,76802a5f-ddd0-411c-906f-1c3840ab7b1d,"Is $||Y-X\beta||_2^2 + \lambda\beta^T K\beta$ , the standard loss-function in kernel ridge regression, or is it different? Also, is the gaussian kernel a standard choice used for the kernel, in practice? If not, which kernels are used more often than not? Also, is $\lambda$ the only parameter to be tuned via cross-validation or is the kernel parameter like $\sigma$ in a gaussian kernel, also tuned via cross validation in practice? Please confirm and/or correct my understanding of Kernel ridge regression.",added 1 characters in body,2013-06-07 03:19:45.887
152013,28,15827.0,5,,CC BY-SA 3.0,65e61767-ac5a-4e89-94ab-e1fe72978eb2,"Last year, I read a blog post from [Brendan O'Connor][1] entitled [""Statistics vs. Machine Learning, fight!""][2] that discussed some of the differences between the two fields.  [Andrew Gelman responded favorably to this][3]:

Simon Blomberg: 
> From R's fortunes
> package: To paraphrase provocatively,
> 'machine learning is statistics minus
> any checking of models and
> assumptions'.
> -- Brian D. Ripley (about the difference between machine learning
> and statistics) useR! 2004, Vienna
> (May 2004) :-) Season's Greetings!

Andrew Gelman:

> In that case, maybe we should get rid
> of checking of models and assumptions
> more often. Then maybe we'd be able to
> solve some of the problems that the
> machine learning people can solve but
> we can't!

There was also the [**""Statistical Modeling: The Two Cultures""** paper][4] by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the *predictive accuracy* of models.

Has the statistics field changed over the last decade in response to these critiques?  Do the *two cultures* still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?


  [1]: http://anyall.org/
  [2]: http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/
  [3]: http://andrewgelman.com/2008/12/machine_learnin/
  [4]: http://projecteuclid.org/euclid.ss/1009213726",deleted 2 characters in body,2013-06-07 06:38:10.327
152255,48125,18416.0,4,,CC BY-SA 3.0,433c0527-8558-45f1-b6c0-da59b6df4ae0,Loss for Kernel Ridge Regression,edited body,2013-06-08 00:58:52.503
152254,48125,18416.0,5,,CC BY-SA 3.0,433c0527-8558-45f1-b6c0-da59b6df4ae0,"Is $||Y-X\beta||_2^2 + \lambda\beta^T K\beta$ , the standard loss-function in kernel ridge regression, or is it different? Also, is the gaussian kernel a standard choice used for the kernel, in practice? If not, which kernels are used more often than not? Also, is $\lambda$ the only parameter to be tuned via cross-validation or is the kernel parameter like $\sigma$ in a gaussian kernel, also tuned via cross validation in practice? Please confirm and/or correct my understanding of Kernel ridge regression!",edited body,2013-06-08 00:58:52.503
153465,48597,18905.0,2,,CC BY-SA 3.0,7c7e28b4-1e41-4561-856d-75bb4d56cd6b,"I've just finished an animal experiment. I compared 1 control group and 1 experimental group, the only difference betweeen the two is type of diet. For statistic analysis I use independent t-test, and the result shows no significant differences between the two group. However the data shows the tendency that the experimental group has more benefit in all variables measured. So, what should I say about my data? All data are normally distributed.
My supervisor said that maybe because I used very small number of sample (each group n=8) so I could not gain any significant differences. He suggested me to do some ""probability test"" or something to extrapolate my data (which is I don't have any clue what he talked about).
So, is there any statistic analysis that I can use as what my supervisor told me to do?
Thank you.",,2013-06-13 04:46:26.887
153466,48597,18905.0,1,,CC BY-SA 3.0,7c7e28b4-1e41-4561-856d-75bb4d56cd6b,"T-test shows no differences, but the experiment group shows tendency more benefit in all variables measured than control group",,2013-06-13 04:46:26.887
153467,48597,18905.0,3,,CC BY-SA 3.0,7c7e28b4-1e41-4561-856d-75bb4d56cd6b,<t-test>,,2013-06-13 04:46:26.887
153469,48597,5237.0,5,,CC BY-SA 3.0,91bcb256-f8de-4f83-9d90-296de7831b27,"I've just finished an animal experiment. I compared 1 control group and 1 experimental group, the only difference between the two is type of diet. For statistical analysis I used the independent groups t-test, and the result showed no significant differences between the two groups. However, the data shows the tendency that the experimental group has more benefit in all variables measured. So, what should I say about my data? All data are normally distributed.  

My supervisor said that maybe because I used very small sample (each group n=8) that I could not find any significant differences. He suggested me to do some ""probability test"" or something to extrapolate my data (unfortunately, I don't have any clue what he was talking about).  

So, is there any statistical analysis that I can use like what my supervisor told me to do?",light editing,2013-06-13 05:13:29.993
153470,48597,5237.0,6,,CC BY-SA 3.0,91bcb256-f8de-4f83-9d90-296de7831b27,<t-test><statistical-power>,light editing,2013-06-13 05:13:29.993
162782,50982,1790.0,1,,CC BY-SA 3.0,6ac4b172-e012-4139-a772-da53984ea8c5,Comparing distributions of generalization performance,,2013-07-17 13:51:44.960
185660,57270,22593.0,3,,CC BY-SA 3.0,1a8e6a02-0e64-4b5f-857f-40ecb23b1df6,<distributions><correlation><pca><group-differences>,,2013-10-11 00:34:33.700
153679,48658,1926.0,2,,CC BY-SA 3.0,d34abced-098f-457b-87b5-afdba2aa3e67,"I am currently taking the PGM course by Daphne Koller on Coursera. In that, we generally model a Bayesian Network as a cause and effect directed graph of the variables which are part of the observed data. But on PyMC tutorials and examples I generally see that it not quite modeled in the same way as the PGM or atleast I am confused. In PyMC the parents of any observed real world variable are often the parameters of the distribution that you use to model the variable. 

Now my question really is a practical one. Suppose I have 3 variables for which data is observed (A, B, C) (lets assume they are all continuous variables just for the sake of it). From some domain knowledge, one can say that A and B cause C. So we have a BN here - A, B are the parents and C is the children.
now from the BN equation P(A, B, C) = P(C | A, B) * P(A) * P(B)

I can say A and B are some normal distributions with some mu and sigma, but how do I model P(C | A, B) ?
The general idea I want to learn, is how do I learn this BN using PyMC so that I can query the BN. Or do I have to augment the BN with parameters of the model  in some fashion.

Is this problem solvable using pymc? or have I got some fundamentals wrong?

Any help would be appreciated!
",,2013-06-13 19:42:13.623
153680,48658,1926.0,3,,CC BY-SA 3.0,d34abced-098f-457b-87b5-afdba2aa3e67,<bayesian><inference><bayesian-network><pymc>,,2013-06-13 19:42:13.623
153678,48658,1926.0,1,,CC BY-SA 3.0,d34abced-098f-457b-87b5-afdba2aa3e67,Bayesian network inference using pymc (Beginner's confusion),,2013-06-13 19:42:13.623
155155,2509,668.0,6,,CC BY-SA 3.0,ffed9afa-d5ba-4792-869b-d61bb7184c09,<pca><intuition>,edited tags,2013-06-19 22:14:36.010
156536,45280,,5,,CC BY-SA 3.0,cc162914-6a6a-4799-b014-b576d1c68ea2,"I have two continuous variables which I have data from a physics experiment.

I want to test for association between the two variables but without assuming a monotonic relationship. I also only have 6 data point each with a large error associated with it and want the test to take this into consideration.

Does anyone know of a statistical test of this type?",edited body,2013-06-25 20:19:10.577
157640,41244,15827.0,5,,CC BY-SA 3.0,907560b8-a088-4a19-9e4e-1427cf677e0c,"Given $N$ flips of the same coin resulting in $k$ occurrences of 'heads', what is the probability density function of heads-probability of the coin?",added 5 characters in body,2013-06-30 08:48:04.803
157776,45457,,4,user88,CC BY-SA 3.0,3cecd1b2-7f2a-428e-98f8-8107d16f3f7f,Predicting high frequency finance time series with HMM,edited title,2013-06-30 20:45:35.737
158536,49879,19492.0,3,,CC BY-SA 3.0,ffad237c-16fa-4c55-9b55-f2d9ea6efbfd,<data-visualization><copula>,,2013-07-03 11:48:24.817
158535,49879,19492.0,1,,CC BY-SA 3.0,ffad237c-16fa-4c55-9b55-f2d9ea6efbfd,What is an adaptive copula?,,2013-07-03 11:48:24.817
158534,49879,19492.0,2,,CC BY-SA 3.0,ffad237c-16fa-4c55-9b55-f2d9ea6efbfd,"My basic question is: What is an adaptive copula?

I have slides from a presentation (unfortunately, I cannot ask the author of the slides) about adaptive copulae and I am not getting, what this means resp. what this is good for?

Here are the slides:
![sl1][1]
![sl2][2]
Then the slides continue with a change-point Test. I am wondering what this is about and why I need this in connection to copulae?

The slides end with an adaptively estimated parameter plot:
![sl3][3]
![sl4][4]

This seems to show, that my estimates are lagged behind. Any other interpretations, comments would be great!
  [1]: https://i.stack.imgur.com/0F76A.png
  [2]: https://i.stack.imgur.com/F3H0r.png
  [3]: https://i.stack.imgur.com/qJXPm.png
  [4]: https://i.stack.imgur.com/jYIy9.png",,2013-07-03 11:48:24.817
158660,49906,5821.0,2,,CC BY-SA 3.0,4c57d016-2ed0-4a83-a64f-1770b24567d7,"I am interested in the modeling of binary response data in paired observations. We aim to make inference about the effectiveness of a pre-post intervention in a group, potentially adjusting for several covariates and determining whether there is effect modification by a group that received particularly different training as part of an intervention.

Given data of the following form:

    id phase resp
    1  pre   1
    1  post  0
    2  pre   0
    2  post  0
    3  pre   1
    3  post  0

And a $2 \times 2$ contingency table of paired response information:

\begin{array}{cc|cc}
& & \mbox{Pre} & \\ 
& & \mbox{Correct} & \mbox{Incorrect} \\ \hline
\mbox{Post} & \mbox{Correct} & a & b&\\
 & \mbox{Incorrect} & c& d&\\
\end{array}

We're interested in the test of hypothesis: $\mathcal{H}_0: \theta_c = 1$.

McNemar's Test gives: $Q = \frac{(b-c)^2}{b+c} \sim \chi^2_1$ under $\mathcal{H}_0$ (asymptotically). This is intuitive because, under the null, we would expect an equal proportion of the discordant pairs ($b$ and $c$) to be favoring a positive effect ($b$) or a negative effect ($c$). With the probability of positive case definition defined $p  =\frac{b}{b+c}$ and $n=b+c$. The odds of observing a positive discordant pair is $\frac{p}{1-p}=\frac{b}{c}$.

On the other hand, conditional logistic regression uses a different approach to test the same hypothesis, by maximizing the conditional likelihood:

$$\mathcal{L}(X ; \beta) = \prod_{j=1}^n \frac{\exp(\beta X_{j,2})}{\exp(\beta X_{j,1}) +  \exp(\beta X_{j,2})}$$

where $\exp(\beta) = \theta_c$.

So, what's the relationship between these tests? How can one do a simple test of the contingency table presented earlier? Looking at calibration of p-values from clogit and McNemar's approaches under the null, you'd think they were completely unrelated!

    library(survival)
    n <- 100
    do.one <- function(n) {
      id <- rep(1:n, each=2)
      ph <- rep(0:1, times=n)
      rs <- rbinom(n*2, 1, 0.5)
      c(
        'pclogit' = coef(summary(clogit(rs ~ ph + strata(id))))[5],
        'pmctest' = mcnemar.test(table(ph,rs))$p.value
      )
    }
    
    out <- replicate(1000, do.one(n))
    plot(t(out), main='Calibration plot of pvalues for McNemar and Clogit tests', 
      xlab='p-value McNemar', ylab='p-value conditional logistic regression')

![enter image description here][1]


  [1]: https://i.stack.imgur.com/HC8YV.jpg",,2013-07-03 17:50:20.467
158661,49906,5821.0,3,,CC BY-SA 3.0,4c57d016-2ed0-4a83-a64f-1770b24567d7,<logistic><mcnemar-test>,,2013-07-03 17:50:20.467
158659,49906,5821.0,1,,CC BY-SA 3.0,4c57d016-2ed0-4a83-a64f-1770b24567d7,Relationship between McNemar's test and conditional logistic regression,,2013-07-03 17:50:20.467
158841,49906,,25,,,465f7e5a-ff9f-4ce0-830a-898d4ffaac6b,,http://twitter.com/#!/StackStats/status/352653364342964224,2013-07-04 05:01:28.550
161769,50739,10492.0,2,,CC BY-SA 3.0,2b322f2f-26bd-4ba5-8f8a-b2ac9b44b523,"I found two very useful posts about the difference between linear regression analysis and ANOVA and how to visualise them:

http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared

http://stats.stackexchange.com/questions/5278/how-to-visualize-what-anova-does

As stated in the first post, to test whether the average height of male and females is the same you can use a regression model (height = alpha + beta * gender + error) and test whether beta = 0. If beta = 0, then there's no difference in the height between males and females. However, I am not quite sure how this is tested when you have three groups. Image the following example:

    height (y) -  group (x)
    5 - A
    6 - A
    7 - A
    6 - A
    30 - B
    32 - B
    34 - B
    33 - B 
    20 - C
    19 - C
    21 - C
    22 - C

The regression model would look like:

height = a + b * group + e

I quickly visualized the data (see image below)

They way I understood the regression model is that it would now test whether
any of the three slopes (AB, AC or BC) has a slope b which is significantly different from 0. If that's the case one can conclude like in an ANOVA that there is at least one group which height is significantly different from one or more groups. Afterwards, one could use a post-hoc test of course to test which of the groups really differ. Is my understanding of how the regression models tests this hypothesis correct?


![enter image description here][1]


  [1]: https://i.stack.imgur.com/6LD5Q.png

",,2013-07-14 10:14:44.800
163054,51047,17056.0,2,,CC BY-SA 3.0,9f1a3b93-8f46-4468-b8c7-3c27aa4225d9,"Over the past few weeks I have been trying to understand MCMC and the Metropolis-Hastings algorithm(s). Every time I think I understand it I realise that I am wrong. Most of the code examples I find on-line implement something that is not consistent with the description. i.e.: They say they implement Metropolis-Hastings but they actually implement random-walk metropolis. Others (almost always) silently skip the implementation of the Hastings correction ratio because they are using a symmetric proposal distribution. Actually, I haven't found a single simple example that calculates the ratio so far. That makes me even more confused. Can someone give me code examples (in any language) of the following:

 - Vanilla Non-Random Walk Metropolis-Hastings Algorithm with Hastings correction ratio calculation (even if this will end up being 1 when using a symmetric proposal distribution).
 - Vanilla Random Walk Metropolis-Hastings algorithm.
 - Vanilla Independent Metropolis-Hastings algorithm.

No need to provide the Metropolis algorithms because if I am not mistaken the only difference between Metropolis and Metropolis-Hastings is that the first ones always sample from a symmetric distribution and thus they don't have the Hastings correction ratio.
No need to give detailed explanation of the algorithms. I do understand the basics but I am kinda confused with all the different names for the different variations of the Metropolis-Hastings algorithm but also with how you practically implement the Hastings correction ratio on the Vanilla non-random-walk MH. Thank you.",,2013-07-18 06:53:11.527
163052,51047,17056.0,3,,CC BY-SA 3.0,9f1a3b93-8f46-4468-b8c7-3c27aa4225d9,<markov-chain-montecarlo><metropolis-hastings>,,2013-07-18 06:53:11.527
163053,51047,17056.0,1,,CC BY-SA 3.0,9f1a3b93-8f46-4468-b8c7-3c27aa4225d9,"Confused with MCMC Metropolis-Hastings variations: Random-Walk, Non-Random-Walk, Independent, Metropolis",,2013-07-18 06:53:11.527
163433,51047,17056.0,5,,CC BY-SA 3.0,39482c02-1b61-48bd-ba8f-f39d3d2b2007,"Over the past few weeks I have been trying to understand MCMC and the Metropolis-Hastings algorithm(s). Every time I think I understand it I realise that I am wrong. Most of the code examples I find on-line implement something that is not consistent with the description. i.e.: They say they implement Metropolis-Hastings but they actually implement random-walk metropolis. Others (almost always) silently skip the implementation of the Hastings correction ratio because they are using a symmetric proposal distribution. Actually, I haven't found a single simple example that calculates the ratio so far. That makes me even more confused. Can someone give me code examples (in any language) of the following:

 - Vanilla Non-Random Walk Metropolis-Hastings Algorithm with Hastings correction ratio calculation (even if this will end up being 1 when using a symmetric proposal distribution).
 - Vanilla Random Walk Metropolis-Hastings algorithm.
 - Vanilla Independent Metropolis-Hastings algorithm.

No need to provide the Metropolis algorithms because if I am not mistaken the only difference between Metropolis and Metropolis-Hastings is that the first ones always sample from a symmetric distribution and thus they don't have the Hastings correction ratio.
No need to give detailed explanation of the algorithms. I do understand the basics but I am kinda confused with all the different names for the different variations of the Metropolis-Hastings algorithm but also with how you practically implement the Hastings correction ratio on the Vanilla non-random-walk MH. Please don't copy paste links that partially answer my questions because most likely I have already seen them. Those links led me to this confusion. Thank you.",added 147 characters in body,2013-07-19 04:46:14.300
163920,3646,19377.0,6,,CC BY-SA 3.0,465b7c78-99f7-410c-a6e4-079f2265c8aa,<correlation><nonparametric><spearman-rho><kendall-tau>,Added tags,2013-07-20 13:29:05.953
163921,3646,,24,,CC BY-SA 3.0,465b7c78-99f7-410c-a6e4-079f2265c8aa,,"Proposed by 27403 approved by 6029, 686 edit id of 4251",2013-07-20 13:29:05.953
163990,50982,1790.0,5,,CC BY-SA 3.0,606d052a-71e9-4806-ab20-7e003ee37b07,"Say that I have two learning methods $A$ and $B$ and that I estimate their generalization performance with something like repeated cross validation. From this process I get a **distribution of scores** $P_A$ and $P_B$ for each method (e.g. their ROC AUC values).

Looking at these distributions, it could be that $\mu_A \ge \mu_B$  but that $\sigma_A \ge \sigma_B$ (i.e. the expected generalization performance of $A$ is higher than $B$, but that there is more uncertainty about this estimation).

I think this is called the **bias-variance tradeoff**.

What **mathematical methods** can I use to compare $P_A$ and $P_B$ and eventually make an informed decision about which model to use?

**Note:** For the sake of simplicity, I am referring to two methods $A$ and $B$ here, but I am interested in methods that can be used to compare the distribution of scores of ~1000 learning methods (e.g. from a grid search) and eventually make a final decision about which model to use.

",added 58 characters in body,2013-07-20 20:56:04.300
163991,50982,1790.0,33,,,7fc5a35e-9151-4d96-b6f9-38bec6a245b2,,692,2013-07-20 21:03:13.030
163992,50982,1790.0,5,,CC BY-SA 3.0,3a99a5c4-0922-43cd-b50d-30c6c35fb6c3,"Say that I have two learning methods $A$ and $B$ and that I estimate their generalization performance with something like repeated cross validation or nested cross validation. From this process I get a **distribution of scores** $P_A$ and $P_B$ for each method (e.g. their ROC AUC values).

Looking at these distributions, it could be that $\mu_A \ge \mu_B$  but that $\sigma_A \ge \sigma_B$ (i.e. the expected generalization performance of $A$ is higher than $B$, but that there is more uncertainty about this estimation).

I think this is called the **bias-variance tradeoff**.

What **mathematical methods** can I use to compare $P_A$ and $P_B$ and eventually make an informed decision about which model to use?

**Note:** For the sake of simplicity, I am referring to two methods $A$ and $B$ here, but I am interested in methods that can be used to compare the distribution of scores of ~1000 learning methods (e.g. from a grid search) and eventually make a final decision about which model to use.

",added 27 characters in body,2013-07-20 21:03:38.780
164036,50982,,25,,,7fad73ce-3648-47e9-a434-3b7786d48dbf,,http://twitter.com/#!/StackStats/status/358730969500495873,2013-07-20 23:31:42.820
164736,51496,19870.0,3,,CC BY-SA 3.0,a1c050e7-5c3f-4a10-98ce-c447dd6b6b34,<classification><predictive-models><cart>,,2013-07-23 03:50:02.683
164735,51496,19870.0,1,,CC BY-SA 3.0,a1c050e7-5c3f-4a10-98ce-c447dd6b6b34,crt decision tree node mutually exclusive,,2013-07-23 03:50:02.683
164734,51496,19870.0,2,,CC BY-SA 3.0,a1c050e7-5c3f-4a10-98ce-c447dd6b6b34,"I have been trying to understand the results of a CRT decision tree, my question is if the terminal nodes should be mutually exclusive? I am asking this because by reading the terminal nodes some variables seems to overlap each other. 

For instance some terminal nodes ""share"" the same profession:

Node 23: Carpenter, Plumber, Sole trader, Truck Driver 

Node 24: Plumber, Truck Driver, Teacher, Retired. 

Probably I am reading the results incorrectly because it should not happen, at least in theory. 

Best Regards

",,2013-07-23 03:50:02.683
167013,52126,20367.0,3,,CC BY-SA 3.0,0855c6cc-9d31-4d3e-959d-fbda2d699811,<time-series><statistical-significance><survey><likert>,,2013-07-30 00:44:55.017
164747,51496,,5,,CC BY-SA 3.0,f56759ea-e22d-471c-ab9a-c108958b0967,"I have been trying to understand the results of a CRT decision tree, my question is if the terminal nodes should be mutually exclusive? I am asking this because by reading the terminal nodes some variables seems to overlap each other. 

For instance some terminal nodes ""share"" the same profession:

Node 23: carpenter, plumber, sole trader, truck driver 

Node 24: plumber, truck driver, teacher, retired. 

Probably I am reading the results incorrectly because it should not happen, at least in theory.",deleted 21 characters in body; edited title,2013-07-23 05:40:31.833
164748,51496,,4,,CC BY-SA 3.0,f56759ea-e22d-471c-ab9a-c108958b0967,Should CRT decision tree node be mutually exclusive?,deleted 21 characters in body; edited title,2013-07-23 05:40:31.833
165013,51577,20097.0,3,,CC BY-SA 3.0,64fca8fc-e10f-43c6-a37a-a05ee99b5588,<regression><time-series><svm><libsvm>,,2013-07-23 19:27:17.943
165011,51577,20097.0,2,,CC BY-SA 3.0,64fca8fc-e10f-43c6-a37a-a05ee99b5588,"I try to predict values for regression in LIBSVM. My data is in time series. I use gridregression.m file in LIBSVM to find optimal parameters c, g and p. Gridregression.m file use cross validation to find optimal parameters, but is it ok to use cross validation in time series? 

When I use parameters from gridregression.m, sometimes the MSE is not better then the default values. (  cmd= '-s 3 -t 2' is sometimes better )",,2013-07-23 19:27:17.943
165012,51577,20097.0,1,,CC BY-SA 3.0,64fca8fc-e10f-43c6-a37a-a05ee99b5588,LIBSVM parameter search in time series,,2013-07-23 19:27:17.943
165246,51644,3733.0,3,,CC BY-SA 3.0,2b580020-bb7f-477a-a198-0b1c34c0e22c,<mixed-model><random-effects-model>,,2013-07-24 09:19:26.983
165244,51644,3733.0,2,,CC BY-SA 3.0,2b580020-bb7f-477a-a198-0b1c34c0e22c,"I have been always tought that random effects only influence the variance (error), and that fixed effects influence only the mean. But I have found an example where random effects influence also the mean - the coefficient estimate:

    require(nlme)
    set.seed(128)
    n <- 100
    k <- 5
    cat <- as.factor(rep(1:k, each = n))
    cat_i <- 1:k # intercept per kategorie
    x <- rep(1:n, k)
    sigma <- 0.2
    alpha <- 0.001
    y <- cat_i[cat] + alpha * x + rnorm(n*k, 0, sigma)
    plot(x, y)
    
    # simulate missing data
    y[c(1:(n/2), (n*k-n/2):(n*k))] <- NA
    
    m1 <- lm(y ~ x)
    summary(m1)

    m2 <- lm(y ~ cat + x)
    summary(m2)
    
    m3 <- lme(y ~ x, random = ~ 1|cat, na.action = na.omit)
    summary(m3)

You can see that estimate of coefficient for `x` from model `m1` is -0.013780, while from model `m2` it is 0.0011713 - both significantly different from zero.

Note that when I remove the line simulating missing data, the results are the same (it is full matrix).

Why is that?

PS: please note I am not a professional statistician, so if you are about to respond with a lot of math then please make also some simple summary for dummies :-)",,2013-07-24 09:19:26.983
165245,51644,3733.0,1,,CC BY-SA 3.0,2b580020-bb7f-477a-a198-0b1c34c0e22c,Adding random effect influences coefficient estimates,,2013-07-24 09:19:26.983
165321,51644,3733.0,5,,CC BY-SA 3.0,a189ec0c-3bce-4611-8337-c0143eae6d30,"I have been always tought that random effects only influence the variance (error), and that fixed effects influence only the mean. But I have found an example where random effects influence also the mean - the coefficient estimate:

    require(nlme)
    set.seed(128)
    n <- 100
    k <- 5
    cat <- as.factor(rep(1:k, each = n))
    cat_i <- 1:k # intercept per kategorie
    x <- rep(1:n, k)
    sigma <- 0.2
    alpha <- 0.001
    y <- cat_i[cat] + alpha * x + rnorm(n*k, 0, sigma)
    plot(x, y)
    
    # simulate missing data
    y[c(1:(n/2), (n*k-n/2):(n*k))] <- NA
    
    m1 <- lm(y ~ x)
    summary(m1)

    m2 <- lm(y ~ cat + x)
    summary(m2)
    
    m3 <- lme(y ~ x, random = ~ 1|cat, na.action = na.omit)
    summary(m3)

You can see that estimate of coefficient for `x` from model `m1` is -0.013780, while from model `m3` it is 0.0011713 - both significantly different from zero.

Note that when I remove the line simulating missing data, the results are the same (it is full matrix).

Why is that?

PS: please note I am not a professional statistician, so if you are about to respond with a lot of math then please make also some simple summary for dummies :-)",edited body,2013-07-24 13:17:41.980
165423,51644,6204.0,5,,CC BY-SA 3.0,92408bb6-e87a-49c2-b16b-324fe5eca3fe,"I have always been taught that random effects only influence the variance (error), and that fixed effects only influence the mean. But I have found an example where random effects influence also the mean - the coefficient estimate:

    require(nlme)
    set.seed(128)
    n <- 100
    k <- 5
    cat <- as.factor(rep(1:k, each = n))
    cat_i <- 1:k # intercept per kategorie
    x <- rep(1:n, k)
    sigma <- 0.2
    alpha <- 0.001
    y <- cat_i[cat] + alpha * x + rnorm(n*k, 0, sigma)
    plot(x, y)
    
    # simulate missing data
    y[c(1:(n/2), (n*k-n/2):(n*k))] <- NA
    
    m1 <- lm(y ~ x)
    summary(m1)

    m2 <- lm(y ~ cat + x)
    summary(m2)
    
    m3 <- lme(y ~ x, random = ~ 1|cat, na.action = na.omit)
    summary(m3)

You can see that the estimated coefficient for `x` from model `m1` is -0.013780, while from model `m3` it is 0.0011713 - both significantly different from zero.

Note that when I remove the line simulating missing data, the results are the same (it is full matrix).

Why is that?

PS: please note I am not a professional statistician, so if you are about to respond with a lot of math then please make also some simple summary for dummies :-)",fixed some grammar/spelling,2013-07-24 17:33:40.770
165424,51644,,24,,CC BY-SA 3.0,92408bb6-e87a-49c2-b16b-324fe5eca3fe,,"Proposed by 8451 approved by 7290, 919 edit id of 4365",2013-07-24 17:33:40.770
166141,51895,9384.0,3,,CC BY-SA 3.0,2ca19789-f375-46bb-b686-ce8f2dbffc6c,<r><classification><unbalanced-classes><oversampling>,,2013-07-26 19:31:50.407
166143,51895,9384.0,1,,CC BY-SA 3.0,2ca19789-f375-46bb-b686-ce8f2dbffc6c,SMOTE throws error for multi class imbalance problem,,2013-07-26 19:31:50.407
167011,52126,20367.0,2,,CC BY-SA 3.0,0855c6cc-9d31-4d3e-959d-fbda2d699811,"I have 5 surveys of the same group of students over a semester.  Each survey uses a 5 point Likert scale.  The first and last survey contain some questions dealing with the begining and end of the class (first impressions, final impressions), but most of the questions are identical for all 4 or 5 of the surveys.

I want to evaluate the statistical significance of changes to students' responses over time.  Unfortunately statistics is not my strong suit.  I know of the t-test, but that seems to only be applicable to two groups of data (please correct me if I'm wrong).  How should I go about evaluating this data?  Is a repeated measures one way ANOVA appropriate?",,2013-07-30 00:44:55.017
167017,52126,20367.0,4,,CC BY-SA 3.0,95966e90-1735-4356-8b7c-289f62b6f8ed,How to evaluate Likert scale data changes over multiple surveys of the same group?,edited title,2013-07-30 01:13:16.480
167029,52126,,4,,CC BY-SA 3.0,e803cf51-4e85-43d0-876e-40b140db677f,How to evaluate likert scale data changes over multiple surveys of the same group?,Improved Formating,2013-07-30 04:55:26.877
185574,57242,674.0,4,,CC BY-SA 3.0,88f07a34-44d5-4dc4-9cdd-040669d35479,Estimating multivariate normal distribution by observing variance in different directions,edited title,2013-10-10 19:27:37.740
166142,51895,9384.0,2,,CC BY-SA 3.0,2ca19789-f375-46bb-b686-ce8f2dbffc6c,"I am trying to use SMOTE to correct imbalance in my multi-class classification problem.
Although SMOTE works perfectly on the iris dataset as per the SMOTE help document, it does not work on a similar dataset.
Here is how my data looks. Note it has three classes with values 1, 2, 3.

    > data
       looking risk every status
    1        0    1     0      1
    2        0    0     0      1
    3        0    0     0      2
    4        0    0     0      1
    5        0    0     0      1
    6        3    0     0      1
    7        0    0     0      1
    8        0    0     0      1
    9        0    1     0      1
    10       0    0     0      1
    11       0    0     0      3
    12       0    0     0      1
    13       0    0     0      1
    14       0    0     0      1
    15       0    0     0      2

It is in the form of dataframe, same as iris:

    > class(data)
    [1] ""data.frame""

Here is my code using SMOTE and the error that it throws:

    > newData <- SMOTE(status ~ ., data, perc.over = 600,perc.under=100)
    Error in scale.default(T, T[i, ], ranges) : subscript out of bounds
    In addition: Warning messages:
    1: In FUN(newX[, i], ...) :
      no non-missing arguments to max; returning -Inf
    2: In FUN(newX[, i], ...) :
      no non-missing arguments to max; returning -Inf
    3: In FUN(newX[, i], ...) :
      no non-missing arguments to max; returning -Inf
    4: In FUN(newX[, i], ...) : no non-missing arguments to min; returning Inf
    5: In FUN(newX[, i], ...) : no non-missing arguments to min; returning Inf
    6: In FUN(newX[, i], ...) : no non-missing arguments to min; returning Inf",,2013-07-26 19:31:50.407
166648,50982,0.0,34,,,980eb327-c8a1-46c8-9ce3-f9be1de75085,,692,2013-07-28 22:42:36.070
166923,52099,20363.0,2,,CC BY-SA 3.0,befbdb5d-a6d4-4dc3-a85f-649b7b809872,"I am trying to investigate the following problem using multinomial likelihoods and could really do with some advice regarding it's appropriateness and implementation in R

A sequence is generated by selecting with replacement from a bag of n differently coloured balls and consists of the number of occurrences of each colour in the selection (i.e. each sequence is a vector of length n with each element a count corresponding to the number of occurrences of a particular colour in the sequence). The process is then repeated a number of times to generate a group of unique sequences (duplicate sequences are rejected).

If a single sequence is selected at random as the test subject and a multinomial model is generated for each of the other sequences, using the colour count proportions as probabilities, can the likelihood be calculated for each multinomial model in the group using the test sequence as the data and would the greatest likelihood indicate the most alike sequence from the group?

I have tried implementing this in R but am struggling with a couple of points.

1) Calculating the likelihood fails if the number of colours is large since the factorial term falls out of bounds.
2) If the number of occurrences of each colour relative to the total number of colours is small then the probability is small and the product of the p^x terms tends to zero.

I hope this makes sense and somebody is able to offer some advice. 

Thank you in advance for your help.",,2013-07-29 19:04:43.263
166921,52099,20363.0,1,,CC BY-SA 3.0,befbdb5d-a6d4-4dc3-a85f-649b7b809872,Multinomial likelihood for large number of groups,,2013-07-29 19:04:43.263
166922,52099,20363.0,3,,CC BY-SA 3.0,befbdb5d-a6d4-4dc3-a85f-649b7b809872,<r><multinomial-distribution>,,2013-07-29 19:04:43.263
166949,52099,16174.0,4,,CC BY-SA 3.0,43b940e2-dddc-4463-94ea-559a8b0de478,Multinomial likelihood for large number of groups.,"tag ""likelihood"", readability",2013-07-29 20:16:35.787
166950,52099,16174.0,6,,CC BY-SA 3.0,43b940e2-dddc-4463-94ea-559a8b0de478,<r><multinomial-distribution><likelihood>,"tag ""likelihood"", readability",2013-07-29 20:16:35.787
166951,52099,,24,,CC BY-SA 3.0,43b940e2-dddc-4463-94ea-559a8b0de478,,Proposed by 22468 approved by -1 edit id of 4466,2013-07-29 20:16:35.787
166947,52099,15827.0,4,,CC BY-SA 3.0,cf9bc69f-59a8-43cf-9e26-20edab46ba75,Multinomial likelihood for large number of groups,"tag ""likelihood"", readability",2013-07-29 20:16:35.787
166948,52099,16174.0,5,,CC BY-SA 3.0,43b940e2-dddc-4463-94ea-559a8b0de478,"I am trying to investigate the following problem using multinomial likelihoods and could really do with some advice regarding it's appropriateness and implementation in R.

A sequence is generated by selecting with replacement from a bag of n differently coloured balls and consists of the number of occurrences of each colour in the selection (i.e. each sequence is a vector of length n with each element a count corresponding to the number of occurrences of a particular colour in the sequence). The process is then repeated a number of times to generate a group of unique sequences (duplicate sequences are rejected).

If a single sequence is selected at random as the test subject and a multinomial model is generated for each of the other sequences, using the colour count proportions as probabilities, can the likelihood be calculated for each multinomial model in the group using the test sequence as the data and would the greatest likelihood indicate the most alike sequence from the group?

I have tried implementing this in R but am struggling with a couple of points.

1. Calculating the likelihood fails if the number of colours is large since the factorial term falls out of bounds.  
2. If the number of occurrences of each colour relative to the total number of colours is small then the probability is small and the product of the $p^x$ terms tends to zero.  

I hope this makes sense and somebody is able to offer some advice. 

Thank you in advance for your help.","tag ""likelihood"", readability",2013-07-29 20:16:35.787
166952,52099,15827.0,5,,CC BY-SA 3.0,cf9bc69f-59a8-43cf-9e26-20edab46ba75,"I am trying to investigate the following problem using multinomial likelihoods and could really do with some advice regarding its appropriateness and implementation in R.

A sequence is generated by selecting with replacement from a bag of n differently coloured balls and consists of the number of occurrences of each colour in the selection (i.e. each sequence is a vector of length n with each element a count corresponding to the number of occurrences of a particular colour in the sequence). The process is then repeated a number of times to generate a group of unique sequences (duplicate sequences are rejected).

If a single sequence is selected at random as the test subject and a multinomial model is generated for each of the other sequences, using the colour count proportions as probabilities, can the likelihood be calculated for each multinomial model in the group using the test sequence as the data and would the greatest likelihood indicate the most alike sequence from the group?

I have tried implementing this in R but am struggling with a couple of points.

1. Calculating the likelihood fails if the number of colours is large since the factorial term falls out of bounds.  
2. If the number of occurrences of each colour relative to the total number of colours is small then the probability is small and the product of the $p^x$ terms tends to zero.  

I hope this makes sense and somebody is able to offer some advice. 
","tag ""likelihood"", readability",2013-07-29 20:16:35.787
167012,52126,20367.0,1,,CC BY-SA 3.0,0855c6cc-9d31-4d3e-959d-fbda2d699811,How to evaluate Likert scale data changes over multiple serveys of the same group?,,2013-07-30 00:44:55.017
186114,57399,6813.0,1,,CC BY-SA 3.0,9fb279df-c7cf-483c-a537-7a1203d9c279,Modelling probabilties within friend sets,,2013-10-13 18:37:21.500
167026,52126,594.0,5,,CC BY-SA 3.0,e22c5de6-f20e-4e5c-a4ed-b9ec5188cc3d,"I have five surveys of the same group of students over a semester.  Each survey uses a 5-point Likert scale.  The first and last survey contain some questions dealing with the beginning and end of the class (first impressions, final impressions), but most of the questions are identical for all four or five of the surveys.

I want to evaluate the statistical significance of changes to students' responses over time.  Unfortunately statistics is not my strong suit.  I know of the t-test, but that seems to only be applicable to two groups of data (please correct me if I'm wrong).  How should I go about evaluating this data?  Is a repeated measures one-way ANOVA appropriate?
",Improved Formating,2013-07-30 04:55:26.877
167030,52126,,6,,CC BY-SA 3.0,e803cf51-4e85-43d0-876e-40b140db677f,<regression><anova><likert>,Improved Formating,2013-07-30 04:55:26.877
167025,52126,594.0,4,,CC BY-SA 3.0,e22c5de6-f20e-4e5c-a4ed-b9ec5188cc3d,How to evaluate Likert scale data changes over multiple surveys of the same group?,Improved Formating,2013-07-30 04:55:26.877
167028,52126,,5,,CC BY-SA 3.0,e803cf51-4e85-43d0-876e-40b140db677f,"I have `5` surveys of the same group of students over a semester.  Each survey uses a `5` point likert scale.  The first and last survey contain some questions dealing with the beginning and end of the class (first impressions, final impressions), but most of the questions are identical for all 4 or 5 of the surveys.

I want to evaluate the statistical significance of changes to students' responses over time.  Unfortunately statistics is not my strong suit.  I know of the `t-test`, but that seems to only be applicable to two groups of data (please correct me if I'm wrong).  How should I go about evaluating this data?  Is a repeated measures one way ANOVA appropriate?",Improved Formating,2013-07-30 04:55:26.877
167027,52126,,24,,CC BY-SA 3.0,e803cf51-4e85-43d0-876e-40b140db677f,,Proposed by 27576 approved by -1 edit id of 4472,2013-07-30 04:55:26.877
168110,52449,18845.0,2,,CC BY-SA 3.0,9c086957-2d21-409a-9930-bcae298f80da,"I've found two definitions in the literature for the autocorrelation time of a weakly stationary time series:

$$
\tau_a = 1+2\sum_{k=1}^\infty \rho_k \quad \text{versus} \quad \tau_b = 1+2\sum_{k=1}^\infty \left|\rho_k\right|
$$

where $\rho_k = \frac{\text{Cov}[X_t,X_{t+h}]}{\text{Var}[X_t]}$ is the autocorrelation at lag $k$.  

One application of the autocorrelation time is to find the ""effective sample size"": if you have $n$ observations of a time series, and you know its autocorrelation time $\tau$, then you can pretend that you have

$$
n_\text{eff} = \frac{n}{\tau}
$$

independent samples instead of $n$ correlated ones for the purposes of finding the mean.  Estimating $\tau$ from data is non-trivial, but there are a few ways of doing it (see [Thompson 2010](http://arxiv.org/abs/1011.0175)).

The definition without absolute values, $\tau_a$, seems more common in the literature; but it admits the possibility of $\tau_a<1$.  Using R and the ""coda"" package:

    require(coda)
    ts.uncorr <- arima.sim(model=list(),n=10000)         # white noise 
    ts.corr <- arima.sim(model=list(ar=-0.5),n=10000)    # AR(1)
    effectiveSize(ts.uncorr)                             # Sanity check
        # result should be close to 10000
    effectiveSize(ts.corr)
        # result is in the neighborhood of 30000... ???

The ""effectiveSize"" function in ""coda"" uses a definition of the autocorrelation time equivalent to $\tau_a$, above.  There are some other R packages out there that compute effective sample size or autocorrelation time, and all the ones I've tried give results consistent with this:  that an AR(1) process with a negative AR coefficient has *more* effective samples than the correlated time series.  This seems strange.  

Obviously, this can never happen in the $\tau_b$ definition of autocorrelation time.

What is the correct definition of autocorrelation time?  Is there something wrong with my understanding of effective sample sizes?  The $n_\text{eff} > n$ result shown above seems like it must be wrong... what's going on?",,2013-08-02 14:46:27.663
168111,52449,18845.0,1,,CC BY-SA 3.0,9c086957-2d21-409a-9930-bcae298f80da,Definition of autocorrelation time (for effective sample size),,2013-08-02 14:46:27.663
168109,52449,18845.0,3,,CC BY-SA 3.0,9c086957-2d21-409a-9930-bcae298f80da,<r><time-series><correlation>,,2013-08-02 14:46:27.663
168161,52449,,25,,,c8f8e9d8-4c26-45ad-9f90-8307cfabccb3,,http://twitter.com/#!/StackStats/status/363355683162304512,2013-08-02 17:48:40.210
168575,52567,728.0,2,,CC BY-SA 3.0,44b5e16d-f404-48b7-a340-add207e85da9,"In linear regression, $Y= X\beta$, why is $X$ called the design matrix? Can $X$ be designed or constructed arbitrarily to some degree as in art? Thanks!",,2013-08-04 18:26:28.673
168576,52567,728.0,1,,CC BY-SA 3.0,44b5e16d-f404-48b7-a340-add207e85da9,"Meaning of ""design"" in design matrix?",,2013-08-04 18:26:28.673
168574,52567,728.0,3,,CC BY-SA 3.0,44b5e16d-f404-48b7-a340-add207e85da9,<regression>,,2013-08-04 18:26:28.673
169744,52871,18447.0,2,,CC BY-SA 3.0,6a4385d8-75a6-4576-9047-18129738dddb,"I want to analyze a multilevel multidimensional model in WinBUGS. the model is as below (N students responding to J items of a test, students are nested within J schools):

    model{
    		#responses
    for(i in 1:N){
    		for(j in 1:K){
    		logit(p[i,j])<- a1[j]*t[i,1]+a2[j]*t[i,2]-b[j]
    		y[i,j]~dbern(p[i,j]  }#for j
    				
    		t[i,1:2]~dmnorm(mu[sc[i],1:2],tau.p[1:2,1:2])
    				}#for i
    			
    		#school level
    	for(j in 1:J){	
    		mu[j,1:2]~dmnorm(m[j,1:2],tau.s[1:2,1:2])
    		
    				}#for j of school
    		#priors
    		for(j in 1:J){
    		m[j,1:2]~dmnorm(m0[1:2],cov[1:2,1:2])
    		}
    		tau.p[1:2,1:2]~dwish(cov[1:2,1:2],2)
    		tau.s[1:2,1:2]~dwish(cov[1:2,1:2],2)
    		sigma.p[1:2,1:2]<-inverse(tau.p[,])
    		sigma.s[1:2,1:2]<-inverse(tau.s[,])
    		s2p<-sum(sigma.p[,])
    		s2s<-sum(sigma.s[,])
    		rho<-(s2s)/(s2s+s2p)
    		a1[1]~dlnorm(0,4)
    		a2[1]<-0
    		b[1]~dnorm(0,1)
    		for(s in 2:K) {
    	a1[s]~dlnorm(0,4)
    	a2[s]~dlnorm(0,4)
    	b[s]~dnorm(0,1)
    		
    					}#for s of items 	
    		}#for model

I've set these functions as initial values:

    ini<-function(){
    list(tau.p=matrix(rgamma(4,100,100),2,2),
    tau.s=matrix(rgamma(4,100,100),2,2),
    t=rmvnorm(2362,mean=c(0,0),sigma=diag(2)),
    m=rmvnorm(116,mean=c(0,0),sigma=diag(2)),
    mu=rmvnorm(116,mean=c(0,0),sigma=diag(2)),
    a1=rlnorm(45,0, 0.4),
    a2=c(NA,rlnorm(44,0, 0.4)),
    b=rnorm(45))
    }
I use rube package in R to check and run my analysis and everything looks fine. When I run the model I receive ""Trap 66 (postcondition violated)"" or ""undefined real result"". I think the problem is from the initials but I have no idea how to solve it.

Any idea?
",,2013-08-09 03:06:43.727
169746,52871,18447.0,3,,CC BY-SA 3.0,6a4385d8-75a6-4576-9047-18129738dddb,<r><hierarchical-bayesian><winbugs>,,2013-08-09 03:06:43.727
169745,52871,18447.0,1,,CC BY-SA 3.0,6a4385d8-75a6-4576-9047-18129738dddb,Trap 66 in WinBUGS in a hierarchical Bayesian modeling,,2013-08-09 03:06:43.727
171096,53261,449.0,2,,CC BY-SA 3.0,e75aac2b-9bd3-4627-94a6-275cbe8682cb,"I was recently looking at a paper in the journal *Psychological Science* and came across this:

*F*(1, 71) = 4.5, *p* = .037, $\mu^2$ = .06

*F*(1, 71) = 0.08, *p* = .78, $\mu^2$ = .001

I was wondering what the $\mu^2$ is in the above. Typically in APA the third thing should be either the MSE or it should be a standardized effect size (or you should have all 4). I'm guessing it's a standardized effect size of some sort but I'm not familiar with it and searching the net has turned up nothing. The actual effect, as near as I can tell from the graph, is about 12 for the first one.

Is this an effect size I haven't heard of yet or a typo in the article?

Farrelly, D., Slater, R., Elliott, H. R., Walden, H. R. and Wetherell, M. A. (2013) Competitors Who Choose to Be Red Have Higher Testosterone Levels. *Psychological Science*, DOI:10.1177/0956797613482945",,2013-08-14 03:06:30.697
171095,53261,449.0,1,,CC BY-SA 3.0,e75aac2b-9bd3-4627-94a6-275cbe8682cb,What is the $\mu^2$ squared effect size?,,2013-08-14 03:06:30.697
186161,57416,22507.0,2,,CC BY-SA 3.0,1e4a3ab0-f21d-4abb-96e6-feeab411acf7,"Multinomial test, if I understand you correctly.",,2013-10-14 02:36:19.860
169749,52871,18447.0,5,,CC BY-SA 3.0,39a8bbe3-e716-41d6-b8f6-0681415ddb55,"I want to analyze a multilevel multidimensional model in WinBUGS. the model is as below (N students responding to J items of a test, students are nested within J schools):

    model{
    		#responses
    for(i in 1:N){
    		for(j in 1:K){
    		logit(p[i,j])<- a1[j]*t[i,1]+a2[j]*t[i,2]-b[j]
    		y[i,j]~dbern(p[i,j]  }#for j
    				
    		t[i,1:2]~dmnorm(mu[sc[i],1:2],tau.p[1:2,1:2])
    				}#for i
    			
    		#school level
    	for(j in 1:J){	
    		mu[j,1:2]~dmnorm(m[j,1:2],tau.s[1:2,1:2])
    		
    				}#for j of school
    		#priors
    		for(j in 1:J){
    		m[j,1:2]~dmnorm(m0[1:2],cov[1:2,1:2])
                   #m0=c(0,0) #cov=diag(2)
    		}
    		tau.p[1:2,1:2]~dwish(cov[1:2,1:2],2)
    		tau.s[1:2,1:2]~dwish(cov[1:2,1:2],2)
    		sigma.p[1:2,1:2]<-inverse(tau.p[,])
    		sigma.s[1:2,1:2]<-inverse(tau.s[,])
    		s2p<-sum(sigma.p[,])
    		s2s<-sum(sigma.s[,])
    		rho<-(s2s)/(s2s+s2p)
    		a1[1]~dlnorm(0,4)
    		a2[1]<-0
    		b[1]~dnorm(0,1)
    		for(s in 2:K) {
    	a1[s]~dlnorm(0,4)
    	a2[s]~dlnorm(0,4)
    	b[s]~dnorm(0,1)
    		
    					}#for s of items 	
    		}#for model

I've set these functions as initial values:

    ini<-function(){
    list(tau.p=matrix(rgamma(4,100,100),2,2),
    tau.s=matrix(rgamma(4,100,100),2,2),
    t=rmvnorm(2362,mean=c(0,0),sigma=diag(2)),
    m=rmvnorm(116,mean=c(0,0),sigma=diag(2)),
    mu=rmvnorm(116,mean=c(0,0),sigma=diag(2)),
    a1=rlnorm(45,0, 0.4),
    a2=c(NA,rlnorm(44,0, 0.4)),
    b=rnorm(45))
    }
I use rube package in R to check and run my analysis and everything looks fine. When I run the model I receive ""Trap 66 (postcondition violated)"" or ""undefined real result"". I think the problem is from the initials but I have no idea how to solve it.

Any idea?",added 42 characters in body,2013-08-09 03:48:10.500
169753,52871,18447.0,5,,CC BY-SA 3.0,cb38a76a-5d35-4f26-b8cc-1814f7c11b3a,"I want to analyze a multilevel multidimensional model in WinBUGS. the model is as below (N=2362 students responding to K=45 items of a test, students are nested within J=116 schools):

    model{
    		#responses
    for(i in 1:N){
    		for(j in 1:K){
    		logit(p[i,j])<- a1[j]*t[i,1]+a2[j]*t[i,2]-b[j]
    		y[i,j]~dbern(p[i,j]  }#for j
    				
    		t[i,1:2]~dmnorm(mu[sc[i],1:2],tau.p[1:2,1:2])
    				}#for i
    			
    		#school level
    	for(j in 1:J){	
    		mu[j,1:2]~dmnorm(m[j,1:2],tau.s[1:2,1:2])
    		
    				}#for j of school
    		#priors
    		for(j in 1:J){
    		m[j,1:2]~dmnorm(m0[1:2],cov[1:2,1:2])
                   #m0=c(0,0) #cov=diag(2)
    		}
    		tau.p[1:2,1:2]~dwish(cov[1:2,1:2],2)
    		tau.s[1:2,1:2]~dwish(cov[1:2,1:2],2)
    		sigma.p[1:2,1:2]<-inverse(tau.p[,])
    		sigma.s[1:2,1:2]<-inverse(tau.s[,])
    		s2p<-sum(sigma.p[,])
    		s2s<-sum(sigma.s[,])
    		rho<-(s2s)/(s2s+s2p)
    		a1[1]~dlnorm(0,4)
    		a2[1]<-0
    		b[1]~dnorm(0,1)
    		for(s in 2:K) {
    	a1[s]~dlnorm(0,4)
    	a2[s]~dlnorm(0,4)
    	b[s]~dnorm(0,1)
    		
    					}#for s of items 	
    		}#for model

I've set these functions as initial values:

    ini<-function(){
    list(tau.p=matrix(rgamma(4,100,100),2,2),
    tau.s=matrix(rgamma(4,100,100),2,2),
    t=rmvnorm(N,mean=c(0,0),sigma=diag(2)),
    m=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
    mu=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
    a1=rlnorm(K,0, 0.4),
    a2=c(NA,rlnorm(K-1,0, 0.4)),
    b=rnorm(45,0,0.5))
    }
I use rube package in R to check and run my analysis and everything looks fine. When I run the model I receive ""Trap 66 (postcondition violated)"" or ""undefined real result"". I think the problem is from the initials but I have no idea how to solve it.

Any idea?
",added 13 characters in body,2013-08-09 04:12:10.890
169871,52910,3731.0,3,,CC BY-SA 3.0,1c09ff82-129e-430e-939d-b84b5c0f65cd,<regression><linear-model><multicollinearity>,,2013-08-09 13:48:43.087
169870,52910,3731.0,1,,CC BY-SA 3.0,1c09ff82-129e-430e-939d-b84b5c0f65cd,What are typically encountered condition numbers in social science?,,2013-08-09 13:48:43.087
169869,52910,3731.0,2,,CC BY-SA 3.0,1c09ff82-129e-430e-939d-b84b5c0f65cd,"As part of my thesis, I'm proving (or attempting to prove...) a few asymptotic results. Because these results depend on the condition number, I'd like to have some idea about the typical sizes of a condition numbers that crop up in social science research. That way, I can give some guidance about how large the sample size has to be before we reach the happy land of asymptopia.  

The setup is as follows. For the standard Generalized Least Squares (GLS) model

$$Y = X\beta + e \quad \quad \quad e \sim N(0, V\sigma^2) $$

where $V$ is assumed to be known and positive definite, we define 

$$ X^- = (X^\top X)^{-1} X^\top \quad \quad \quad U = (I-XX^-)V$$
and the condition number $\kappa$ 

$$ \kappa = \frac{ \lambda_{\text{max}} }{ \lambda_{\text{min}} } $$

where the $\lambda_\star$ values are the maximum and minimum eigenvalues of the matrix $U$.  

Does anyone have pointers to references for the sizes of condition numbers in social science research? I don't even know where to look. Any pointers for either 

 1. GLS models (as I've posed it),   
 2. REML/ML models where $V$ is
    estimated and then conditioned upon, or 
 3. fixed effect only models
    where $V$ is the identity matrix

would be most welcome!",,2013-08-09 13:48:43.087
169872,52910,3731.0,5,,CC BY-SA 3.0,232d5027-98a5-42a7-bea8-21bb6599d718,"As part of my thesis, I'm proving (or attempting to prove...) a few asymptotic results. Because these results depend on the condition number, I'd like to have some idea about the typical sizes of a condition numbers that crop up in social science research. That way, I can give some guidance about how large the sample size has to be before we reach the happy land of asymptopia.  

I'd be happy for any guidance. 

**My very specific** setup is as follows. For the standard Generalized Least Squares (GLS) model

$$Y = X\beta + e \quad \quad \quad e \sim N(0, V\sigma^2) $$

where $V$ is assumed to be known and positive definite, we define 

$$ X^- = (X^\top X)^{-1} X^\top \quad \quad \quad U = (I-XX^-)V$$
and the condition number $\kappa$ 

$$ \kappa = \frac{ \lambda_{\text{max}} }{ \lambda_{\text{min}} } $$

where the $\lambda_\star$ values are the maximum and minimum eigenvalues of the matrix $U$.  

Does anyone have pointers to references for the sizes of condition numbers in social science research? I don't even know where to look. Any pointers for either 

 1. OLS estimators (used incorrectly in a GLS context as posed above)
 2. GLS estimators (correctly analyzed) 
 3. REML/ML estimators where $V$ is
    estimated and then conditioned upon, or 
 4. OLS fixed effect only models
    where $V$ is the identity matrix

would be most welcome!","improved formatting, clarified that my definition of a condition number is non-standard",2013-08-09 14:00:16.667
171094,53261,449.0,3,,CC BY-SA 3.0,e75aac2b-9bd3-4627-94a6-275cbe8682cb,<effect-size>,,2013-08-14 03:06:30.697
171099,53264,17249.0,2,,CC BY-SA 3.0,1b55933c-6460-4fff-8508-50bba11fee23,"I can only think of this referring to $\eta^2$, the proportion of variance explained in the dependent variable by the grouping variable (in this case, a binary variable). This would be indeed the same value as the $R^2$ obtained if the difference between the two groups was estimated using simple linear regression:

$y_i=\beta_0+\beta_1group_i+\epsilon_i$",,2013-08-14 03:24:19.637
185659,57270,22593.0,1,,CC BY-SA 3.0,1a8e6a02-0e64-4b5f-857f-40ecb23b1df6,Simulating groups different with respect to the orthogonal complement in R,,2013-10-11 00:34:33.700
171100,53264,17249.0,5,,CC BY-SA 3.0,a212ce65-b0c8-4321-a136-3a63f4d0db73,"I can only think of this referring to $\eta^2$, the proportion of variance explained in the dependent variable by the grouping variable (in this case, a binary variable). This would be indeed the same value as the $R^2$ obtained if the difference between the two groups was estimated using simple linear regression:

$y_i=\beta_0+\beta_1group_i+\epsilon_i$

I can see from the paper that the second F test is actually that of an interaction term, and since it has 1 degree of freedom, I am deducing that the second factor was also a binary variable. In this case, the $\eta^2$'s are partial $\eta^2$'s, which are the proportion of variance explained by the grouping variable (or the interaction term) controlling for the other grouping variable. In this more complex case, the partial $\eta^2$'s are the same as the partial $R^2$'s obtained from the multiple linear regression:

$y_i=\beta_0+\beta_1group_{1i}+\beta_2group_{1i}+\beta_3 \cdot group_{1i} \cdot group_{2i} + \epsilon_i$",added 626 characters in body,2013-08-14 03:30:42.040
171102,53264,17249.0,5,,CC BY-SA 3.0,ae3d442b-7faa-4944-9a35-776cd498365c,"I can only think of this referring to $\eta^2$, computed as:

$\eta^2={SS_{effect} \over SS_{total}}$

This is the proportion of variance explained in the dependent variable by the grouping variable (in this case, a binary variable). This would be indeed the same value as the $R^2$ obtained if the difference between the two groups was estimated using simple linear regression:

$y_i=\beta_0+\beta_1group_i+\epsilon_i$

I can see from the paper that the second F test is actually that of an interaction term, and since it has 1 degree of freedom, I am deducing that the second factor was also a binary variable. In this case, the $\eta^2$'s are partial $\eta^2$'s, which are the proportion of variance explained by the grouping variable (or the interaction term) controlling for the other grouping variable. In this more complex case, the partial $\eta^2$'s are the same as the partial $R^2$'s obtained from the multiple linear regression:

$y_i=\beta_0+\beta_1group_{1i}+\beta_2group_{1i}+\beta_3 \cdot group_{1i} \cdot group_{2i} + \epsilon_i$",added formula to eta²,2013-08-14 03:57:01.523
171106,53261,155.0,5,,CC BY-SA 3.0,2712bc6f-255d-4bba-8b80-7484f1ea94cd,"I was recently looking at a paper in the journal *Psychological Science* and came across this:

*F*(1, 71) = 4.5, *p* = .037, $\mu^2$ = .06

*F*(1, 71) = 0.08, *p* = .78, $\mu^2$ = .001

I was wondering what the $\mu^2$ is in the above. Typically in APA the third thing should be either the MSE or it should be a standardized effect size (or you should have all 4). I'm guessing it's a standardized effect size of some sort but I'm not familiar with it and searching the net has turned up nothing. The actual effect, as near as I can tell from the graph, is about 12 for the first one.

Is this an effect size I haven't heard of yet or a typo in the article?

Farrelly, D., Slater, R., Elliott, H. R., Walden, H. R. and Wetherell, M. A. (2013) Competitors Who Choose to Be Red Have Higher Testosterone Levels. *Psychological Science*, DOI:10.1177/0956797613482945

Here's a screen shot of the text (p.2)

![enter image description here][1]


  [1]: https://i.stack.imgur.com/h5BtM.png",thought the screen shot might be useful.,2013-08-14 04:04:45.867
171480,53384,20838.0,1,,CC BY-SA 3.0,5aa121c9-6908-487b-b82f-9a46cda3e493,Bootstrapping residuals: Am I doing it right?,,2013-08-15 21:35:20.763
171481,53384,20838.0,3,,CC BY-SA 3.0,5aa121c9-6908-487b-b82f-9a46cda3e493,<time-series><bootstrap><residuals>,,2013-08-15 21:35:20.763
171479,53384,20838.0,2,,CC BY-SA 3.0,5aa121c9-6908-487b-b82f-9a46cda3e493,"**First of all:** 
From what I understood, bootstrapping residuals works as follows:

 1. Fit model to data 
 2. Calculate the residuals
 3. Resample the residuals and add them to 1.
 4. Fit model to new dataset from 3.
 5. Repeat `n` times, but always add the resampled residuals to the fit
    from 1.

Is that correct so far?

---
**What I want to do** is something slightly different:

I want to estimate parameter and prediction uncertainty for an algorithm that estimates some environmental variable.
 
What I have is a error-free time-series (from a simulation) of that variable, `x_true`, to which I add some noise, `x_noise`, in order to generate a synthetic dataset `x`.
I then try to find optimal parameters by fitting my algorithm with the sum of squares `sum((x_estimate - x_true)^2)` (! not `x_estimate - x` !) as an objective function. In order to see how my algorithm performs and to create samples of my parameters' distributions, I want to resample `x_noise`, add it to `x_true`, fit my model again, rinse and repeat. Is that a valid approach to assess parameter uncertainty? Can I interpret the fits to the bootstrapped datasets as prediction uncertainty, or do I have to follow the procedure I posted above?

Sorry if I'm not very clear with terminology, English isn't my first language and I'm pretty new to this.",,2013-08-15 21:35:20.763
171488,53384,20838.0,5,,CC BY-SA 3.0,8eef2ba8-8e8e-4738-a9af-d1569f0bc044,"**First of all:** 
From what I understood, bootstrapping residuals works as follows:

 1. Fit model to data 
 2. Calculate the residuals
 3. Resample the residuals and add them to 1.
 4. Fit model to new dataset from 3.
 5. Repeat `n` times, but always add the resampled residuals to the fit
    from 1.

Is that correct so far?

---
**What I want to do** is something slightly different:

I want to estimate parameter and prediction uncertainty for an algorithm that estimates some environmental variable.
 
What I have is a error-free time-series (from a simulation) of that variable, `x_true`, to which I add some noise, `x_noise`, in order to generate a synthetic dataset `x`.
I then try to find optimal parameters by fitting my algorithm with the sum of squares `sum((x_estimate - x_true)^2)` (! not `x_estimate - x` !) as an objective function. In order to see how my algorithm performs and to create samples of my parameters' distributions, I want to resample `x_noise`, add it to `x_true`, fit my model again, rinse and repeat. Is that a valid approach to assess parameter uncertainty? Can I interpret the fits to the bootstrapped datasets as prediction uncertainty, or do I have to follow the procedure I posted above?

Sorry if I'm not very clear with terminology, English isn't my first language and I'm pretty new to this.

/edit: I think I haven't really made clear what my model does. Think of it as essentially something like a de-noising method. It's not a predictive model, it's an algorithm that tries to extract the underlying signal of a noisy time-series of environmental data.",added 266 characters in body,2013-08-15 22:13:08.573
171503,53384,20838.0,6,,CC BY-SA 3.0,f4b44eb1-97b7-4c31-aeea-f628927ae5a0,<time-series><matlab><bootstrap><residuals>,added tag,2013-08-15 23:33:09.623
171505,53391,20312.0,1,,CC BY-SA 3.0,148e1d2c-00af-4152-8635-e634fb590995,How do I calculate random baseline?,,2013-08-15 23:38:00.327
171506,53391,20312.0,3,,CC BY-SA 3.0,148e1d2c-00af-4152-8635-e634fb590995,<machine-learning>,,2013-08-15 23:38:00.327
171504,53391,20312.0,2,,CC BY-SA 3.0,148e1d2c-00af-4152-8635-e634fb590995,"I am a bit confused as to how to calculate random baseline. If I understand correctly the random baseline is calculated by adding up the squared probabilities of all the classes. The random baseline classifier thus picks a class at random, instead of choosing the most frequent one.

I have 7classes, each with # of items and a total of X. How do I find the probabilities? I have a clue, but I don't want to be mistaken as this is for an important piece of work for me. Thanks! :)",,2013-08-15 23:38:00.327
171507,53384,20838.0,5,,CC BY-SA 3.0,44eed4c4-3e25-407a-9b37-1281ec9d6142,"**First of all:** 
From what I understood, bootstrapping residuals works as follows:

 1. Fit model to data 
 2. Calculate the residuals
 3. Resample the residuals and add them to 1.
 4. Fit model to new dataset from 3.
 5. Repeat `n` times, but always add the resampled residuals to the fit
    from 1.

Is that correct so far?

---
**What I want to do** is something slightly different:

I want to estimate parameter and prediction uncertainty for an algorithm that estimates some environmental variable.
 
What I have is a error-free time-series (from a simulation) of that variable, `x_true`, to which I add some noise, `x_noise`, in order to generate a synthetic dataset `x`.
I then try to find optimal parameters by fitting my algorithm with the sum of squares `sum((x_estimate - x_true)^2)` (! not `x_estimate - x` !) as an objective function. In order to see how my algorithm performs and to create samples of my parameters' distributions, I want to resample `x_noise`, add it to `x_true`, fit my model again, rinse and repeat. Is that a valid approach to assess parameter uncertainty? Can I interpret the fits to the bootstrapped datasets as prediction uncertainty, or do I have to follow the procedure I posted above?

Sorry if I'm not very clear with terminology, English isn't my first language and I'm pretty new to this.

/edit: I think I haven't really made clear what my model does. Think of it as essentially something like a de-noising method. It's not a predictive model, it's an algorithm that tries to extract the underlying signal of a noisy time-series of environmental data.


/edit^2: **For the MATLAB-Users** out there, I wrote down some quick & dirty linear regression example of what I mean.

This is what I believe ""ordinary"" bootstrapping of residuals is (please correct me if I'm wrong): http://pastebin.com/C0CJp3d1

This is what I want to do: http://pastebin.com/6748SLib",fixed one code example,2013-08-15 23:57:54.450
171508,53384,,25,,,5673a4b4-4fcc-45f3-9636-9e898a52a0ce,,http://twitter.com/#!/StackStats/status/368161180998975488,2013-08-16 00:04:00.307
171510,53391,15827.0,5,,CC BY-SA 3.0,6dd71773-15a3-4b18-b028-ab77c12f9003,"I am a bit confused as to how to calculate random baseline. If I understand correctly the random baseline is calculated by adding up the squared probabilities of all the classes. The random baseline classifier thus picks a class at random, instead of choosing the most frequent one.

I have 7 classes, each with # of items and a total of X. How do I find the probabilities? ","""important for you"" not a criterion for anyone else; the question stands well on its own",2013-08-16 00:52:39.100
171553,53404,20820.0,1,,CC BY-SA 3.0,69df8ff1-fdef-4002-b43d-fdc69c550f5b,why do we use one-tail test [F-test] in anova?,,2013-08-16 06:36:58.590
171552,53404,20820.0,2,,CC BY-SA 3.0,69df8ff1-fdef-4002-b43d-fdc69c550f5b,"state reason for using one tail in the analysis of variance test ?



why do we use one-tail test  [F-test] in anova ?",,2013-08-16 06:36:58.590
171554,53404,20820.0,3,,CC BY-SA 3.0,69df8ff1-fdef-4002-b43d-fdc69c550f5b,<anova><sums-of-squares><f-distribution>,,2013-08-16 06:36:58.590
171582,53404,20820.0,6,,CC BY-SA 3.0,ac1b0109-213a-4c0c-81d1-2e59b27cc5e8,<anova><self-study><sums-of-squares><f-distribution>,edited tags,2013-08-16 09:21:43.467
171673,53439,5208.0,3,,CC BY-SA 3.0,5e2a9f0d-40f5-4148-8e2e-af5b54a8c897,<machine-learning>,,2013-08-16 15:35:05.747
171672,53439,5208.0,1,,CC BY-SA 3.0,5e2a9f0d-40f5-4148-8e2e-af5b54a8c897,How would you frame this as a machine learning problem?,,2013-08-16 15:35:05.747
171671,53439,5208.0,2,,CC BY-SA 3.0,5e2a9f0d-40f5-4148-8e2e-af5b54a8c897,"I have a trading software that buys and sells loans. There's an auction site where borrowers ask for some money and lenders bid on them until the borrower is fully funded and the auction ends. There's lots of information on each loan request. My trading bot always bids at the highest possible interest rate, if it is outbid, then it just re-bids slightly lower. Once I win the loan parts, I can sell them at a markup. Right now, I sell at the minimum markup, so that with fees I barely make a profit.

What I'm not sure is what markup I should sell? The lower the markup the faster my loan parts sell, but I will get less profit too. On what loans should I bid? Should I bid on a loan auction with a higher interest rate, but which is not going to end for several days, thereby leaving my money stale, or should I bid on an auction with a lower interest rate, but which is going to end very soon. Sometimes in the former case, the borrower might decide to take the loan and not wait until the end of the auction, thereby I could secure a better interest rate than just bidding on the loan auction due to end soon.

I was thinking of framing this problem as reinforcement learning, but I'm not sure how to do it. My goal is to maximiz the profit I make from trading loans. Any ideas?",,2013-08-16 15:35:05.747
171792,53471,11155.0,2,,CC BY-SA 3.0,5cf3d5ab-3ab2-40b1-bc6e-63dae8ab0233,I think adaptive in this context just means the reestimation on a rolling basis. So the parameter should not change until there is a change point. Then the true parameter increases and stays constant after it decreases again because of the second change point. The estimated parameter is evaluated compared to the true parameter: How fast does it get the change point? How fast does it *adapt* to the new environment?,,2013-08-17 07:53:38.020
171810,49879,,25,,,69536709-1587-453a-ab99-d05538c6e3f0,,http://twitter.com/#!/StackStats/status/368705372057374721,2013-08-17 12:06:25.727
171972,40104,4656.0,4,,CC BY-SA 3.0,6355eb14-36ae-4ea1-8219-56329f6b50f9,The _weighted_ sum of two independent Poisson random variables,"added the word ""weighted"" in the title to distinguish this question from the more usual one",2013-08-18 13:02:31.897
172127,53384,20838.0,5,,CC BY-SA 3.0,15adea5f-93f7-454f-9215-8852efdc7a71,"**First of all:** 
From what I understood, bootstrapping residuals works as follows:

 1. Fit model to data 
 2. Calculate the residuals
 3. Resample the residuals and add them to 1.
 4. Fit model to new dataset from 3.
 5. Repeat `n` times, but always add the resampled residuals to the fit
    from 1.

Is that correct so far?

---
**What I want to do** is something slightly different:

I want to estimate parameter and prediction uncertainty for an algorithm that estimates some environmental variable.
 
What I have is a error-free time-series (from a simulation) of that variable, `x_true`, to which I add some noise, `x_noise`, in order to generate a synthetic dataset `x`.
I then try to find optimal parameters by fitting my algorithm with the sum of squares `sum((x_estimate - x_true)^2)` (! not `x_estimate - x` !) as an objective function. In order to see how my algorithm performs and to create samples of my parameters' distributions, I want to resample `x_noise`, add it to `x_true`, fit my model again, rinse and repeat. Is that a valid approach to assess parameter uncertainty? Can I interpret the fits to the bootstrapped datasets as prediction uncertainty, or do I have to follow the procedure I posted above?

Sorry if I'm not very clear with terminology, English isn't my first language and I'm pretty new to this.

/edit: I think I haven't really made clear what my model does. Think of it as essentially something like a de-noising method. It's not a predictive model, it's an algorithm that tries to extract the underlying signal of a noisy time-series of environmental data.


/edit^2: **For the MATLAB-Users** out there, I wrote down some quick & dirty linear regression example of what I mean.

This is what I believe ""ordinary"" bootstrapping of residuals is (please correct me if I'm wrong): http://pastebin.com/C0CJp3d1

This is what I want to do: http://pastebin.com/mbapsz4c",Updated a comment in second code example,2013-08-19 09:04:57.950
176726,54724,10957.0,2,,CC BY-SA 3.0,f021eb83-6877-4ab9-a762-66d6629e4d51,"I came across a simple question on comparing flexible models (i.e. splines) vs. inflexible models (e.g. linear regression) under different scenarios. The question is:

In general, do we expect the performance of a flexible statistical learning method to perform better or worse than an inflexible method when:

1. The number of predictors p is extremely large, and the number of observations n is small?
2. The variance of the error terms, i.e. σ2 = Var(e), is extremely high?

I think for (1), when n is small, inflexible models are better (not sure). For (2), I don't know which model is (relatively) better.",,2013-09-04 20:24:41.247
176727,54724,10957.0,1,,CC BY-SA 3.0,f021eb83-6877-4ab9-a762-66d6629e4d51,flexible and inflexible models in machine learning,,2013-09-04 20:24:41.247
174630,54234,21204.0,2,,CC BY-SA 3.0,8c05e30e-ddee-40f1-9ee2-9d06b83164b1,"I have a question regarding to time series forecasting. In particular I've been working with a Bayesian approach, but I think the question is independent from that.

I have several time series which are very stable in time, except on specific dates that they have sudden changes. The problem is that if I use a forecasting technique that looks at the past to predict the future, such as ARIMA, the days after the sudden changes have high impact on the forecast.

Thus, to give a simple example, suppose I'm predicting $x_{t+1} = \sum \beta_j x_j, j<t+1$, I would like to add another weight witch accounts for the probability of $x_j$, something like $x_{t+1} = \sum f(x_j)\beta_j x_j, j<t+1$ where $f(x_j)$ is proportional to $P(x_j)$. 

Thus, a sudden change has low probability and should not contribute to the prediction.

Does anyone know how to deal with these kind of problems? I'm trying to implement this in a Bayesian model, but I'm now sure how I should do it.

Anyway, thanks

Pablo",,2013-08-28 15:56:16.150
174632,54234,21204.0,3,,CC BY-SA 3.0,8c05e30e-ddee-40f1-9ee2-9d06b83164b1,<time-series><forecasting><arima><outliers>,,2013-08-28 15:56:16.150
174631,54234,21204.0,1,,CC BY-SA 3.0,8c05e30e-ddee-40f1-9ee2-9d06b83164b1,Weighting time series coefficients using model's likelihood,,2013-08-28 15:56:16.150
174889,50739,,24,,CC BY-SA 3.0,04563081-e021-43d5-97e2-6d148f1cfe08,,Proposed by 27581 approved by 919 edit id of 4944,2013-08-29 13:44:27.413
174890,50739,,5,,CC BY-SA 3.0,04563081-e021-43d5-97e2-6d148f1cfe08,"I found two very useful posts about the difference between linear regression analysis and ANOVA and how to visualise them:

http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared

http://stats.stackexchange.com/questions/5278/how-to-visualize-what-anova-does

As stated in the first post, to test whether the average height of male and females is the same you can use a regression model ($y = \alpha + \beta x + \epsilon$, where $y$ denotes height and $x$ denotes gender) and test whether $\beta = 0$. If $\beta = 0$, then there is no difference in the height between males and females. However, I am not quite sure how this is tested when you have three groups. Imagine the following example:

    height (y) -  group (x)
    5          -  A
    6          -  A
    7          -  A
    6          -  A
    30         -  B
    32         -  B
    34         -  B
    33         -  B 
    20         -  C
    19         -  C
    21         -  C
    22         -  C

The regression model would look like:

$$y = a+ b x + \epsilon$$

I quickly visualized the data (see image below)

They way I understood the regression model is that it would now test whether
any of the three slopes (AB, AC or BC) has a slope $b$ which is significantly different from 0. If that's the case one can conclude like in an ANOVA that there is at least one group in which height is significantly different from one or more groups. Afterwards, one could use a post-hoc test of course to test which of the groups really differ. Is my understanding of how the regression models tests this hypothesis correct?


![enter image description here][1]


  [1]: https://i.stack.imgur.com/6LD5Q.png

",Formatted question,2013-08-29 13:44:27.413
175755,54506,21322.0,1,,CC BY-SA 3.0,921ae92f-7cd6-4240-b71c-c77635a51121,How to analyse this data?,,2013-09-02 04:48:52.793
175756,54506,21322.0,3,,CC BY-SA 3.0,921ae92f-7cd6-4240-b71c-c77635a51121,<regression><logistic>,,2013-09-02 04:48:52.793
175754,54506,21322.0,2,,CC BY-SA 3.0,921ae92f-7cd6-4240-b71c-c77635a51121,"I am conducting an experiment investigating lineup accuracy and witness confidence.  

A long story short: we want to know what the pattern of false positives, hits and misses on a lineup task are under different lineup conditions and how confidence may vary with/independently of accuracy.  Logically, witness confidence may also be affected by the different conditions, and we'd like to know this as well.  

The between subjects variables are: Gender (male, female), ethnicity (asian, caucasian), and lineup type (sequential- where people see each lineup member one at a time and make a decision about each one, and simultaneous- where people see all the lineup members and make a decision about whether they see the perpetrator or not)

The within subjects variables are: Photo type (same vs different photo of the person), lineup ethnicity (asian vs caucasian lineups), confidence (5 levels of a likert scale from 1 ""not confidence at all"" to 5 ""extremely confident)

The dependent variable is accuracy in terms of hits, misses and false positives (these could be coded as 0 or 1?) and correct recognition (hits-false positives)

One of the problems is that we want to know the relationship between confidence and accuracy, which would necessitate that confidence is an independent variable, however we also want to know if the other variables might affect confidence (such as ethnicity or lineup type), so I'm having trouble figuring out the best way to analyse this data.  

Does anyone have any answers for me?  Someone suggested maybe logistic regression, but they weren't really sure.  I'm really not used to dealing with categorical data, so am in need of help!  ",,2013-09-02 04:48:52.793
175781,54506,,25,,,909d9002-9e7f-4383-a489-621b14f3fd09,,http://twitter.com/#!/StackStats/status/374458404137607168,2013-09-02 09:06:55.293
176102,54574,11283.0,2,,CC BY-SA 3.0,e5f2574b-278c-4706-9f6e-313c24b7df6b,"I have calculated log-likelihood distances between 50 sequences according to the Formula (1): 

$$
D(X_i,X_j)= 1/2(\log p(X_i|Mod_j)+\log p(X_j|Mod_i)),  
$$
where $
p(X_i|Mod_j)
$ is the likelihood of sequence X_i being produced by model Mod_j, where $Mod_j$ is a corresponding Markov model of the given $Seq_j$, defined by its Transition Probability Matrix and Start Probabilities Vector.  The measure is symmetrical as seen from the definition. To make the measure more ""legible"" and similar to the traditional measures, I compute distance=(1-D) from formula (1). Thus, D(X_i,X_i) = 0 and the distance increases if the likelihood decreases. 

Now, I have a 50x50 Distance Matrix.I have run a ""meaningfullness"" check, and it seemed ok for me - i.e. more similar sequences had smaller distance and very different ones had very large distance. The distances seemed to satisfy the triangle inequality. However, I have noticed that:

1)  the shorter sequences seem to be ""closer"" to all other sequences than longer ones. It seems that this distance measure is biased to favor short distances. 

2) I have tried PAM-clustering with the distance matrix by converting my distance matrix to dist object in R by using as.dist(), and my results were very bad, even for 2 clusters or 49 ( max avg.silhouette width produced by R function pam was 0.28). With some numbers of clusters the avg.silhouette widths were even negative. 

I am coming to conclusion that my way of computing medoids is invalid/conceptually wrong. What could be the problem? Can log-likelihood distance matrix be used with medoids clustering at all? 
",,2013-09-03 10:48:31.337
176104,54574,11283.0,3,,CC BY-SA 3.0,e5f2574b-278c-4706-9f6e-313c24b7df6b,<r><distance-functions><likelihood><markov-chain><k-medoids>,,2013-09-03 10:48:31.337
176103,54574,11283.0,1,,CC BY-SA 3.0,e5f2574b-278c-4706-9f6e-313c24b7df6b,Log-likelihood distance measure validity for clustering,,2013-09-03 10:48:31.337
176107,49906,15473.0,6,,CC BY-SA 3.0,5395e6c4-dbee-4d21-be55-51849baf6fd4,<logistic><mcnemar-test><clogit>,add one tag,2013-09-03 10:49:18.003
176108,49906,,24,,CC BY-SA 3.0,5395e6c4-dbee-4d21-be55-51849baf6fd4,,"Proposed by 21599 approved by 17230, 686 edit id of 5155",2013-09-03 10:49:18.003
176126,54574,,5,,CC BY-SA 3.0,cbf2f1a2-17ef-42ca-b16e-5a4b6b07621b,"I have calculated log-likelihood distances between 50 sequences according to the Formula (1): 

$$
D(X_i,X_j)= 1/2(\log p(X_i|Mod_j)+\log p(X_j|Mod_i)),  
$$
where $
p(X_i|Mod_j)
$ is the likelihood of sequence $X_i$ being produced by model $Mod_j$, where $Mod_j$ is a corresponding Markov model of the given $Seq_j$, defined by its Transition Probability Matrix and Start Probabilities Vector.  The measure is symmetrical as seen from the definition. To make the measure more ""legible"" and similar to the traditional measures, I compute distance$=(1-D)$ from formula (1). Thus, $D(X_i,X_i) = 0$ and the distance increases if the likelihood decreases. 

Now, I have a 50x50 Distance Matrix.I have run a ""meaningfullness"" check, and it seemed ok for me - i.e. more similar sequences had smaller distance and very different ones had very large distance. The distances seemed to satisfy the triangle inequality. However, I have noticed that:

1)  the shorter sequences seem to be ""closer"" to all other sequences than longer ones. It seems that this distance measure is biased to favor short distances. 

2) I have tried PAM-clustering with the distance matrix by converting my distance matrix to dist object in `R` by using as.dist(), and my results were very bad, even for 2 clusters or 49 ( max avg.silhouette width produced by `R` function pam was 0.28). With some numbers of clusters the avg.silhouette widths were even negative. 

I am coming to conclusion that my way of computing medoids is invalid/conceptually wrong. What could be the problem? Can log-likelihood distance matrix be used with medoids clustering at all? 
",formatted the maths parts,2013-09-03 11:43:15.713
176125,54574,,24,,CC BY-SA 3.0,cbf2f1a2-17ef-42ca-b16e-5a4b6b07621b,,"Proposed by 26338 approved by 17230, 22047 edit id of 5168",2013-09-03 11:43:15.713
176151,54574,11283.0,5,,CC BY-SA 3.0,7a7b098b-f4a4-4f7b-a183-f2768cef65f1,"I have calculated log-likelihood distances between 50 sequences according to the Formula (1): 

$$
D(X_i,X_j)= 1/2(\log p(X_i|Mod_j)+\log p(X_j|Mod_i)),  
$$
where $
p(X_i|Mod_j)
$ is the likelihood of sequence $X_i$ being produced by model $Mod_j$, where $Mod_j$ is a corresponding Markov model of the given $Seq_j$, defined by its Transition Probability Matrix and Start Probabilities Vector.  The measure is symmetrical as seen from the definition. To make the measure more ""legible"" and similar to the traditional measures, I compute distance$=(1-D)$ from formula (1). Thus, $D(X_i,X_i) = 0$ and the distance increases if the likelihood decreases. 

Now, I have a 50x50 Distance Matrix.I have run a ""meaningfullness"" check, and it seemed ok for me - i.e. more similar sequences had smaller distance and very different ones had very large distance. The distances seemed to satisfy the triangle inequality. However, I have noticed that:

1)  the shorter sequences seem to be ""closer"" to all other sequences than longer ones. It seems that this distance measure is biased to favor short distances. 

2) I have tried PAM-clustering with the distance matrix by converting my distance matrix to dist object in `R` by using as.dist(), and my results were very bad, even for 2 clusters or 49 ( max avg.silhouette width produced by `R` function pam was 0.28). With some numbers of clusters the avg.silhouette widths were even negative. 

I am coming to conclusion that my way of computing medoids is invalid/conceptually wrong. What could be the problem? Can log-likelihood distance matrix be used with medoids clustering at all? 

edit: I am including the heatmap of the distance matrix, where x- and y-axis represent sequences (1 through 50th). It looks strange to me but I cannot pinpoint what exactly doesn't feel right. 

![heatmap][1]


  [1]: https://i.stack.imgur.com/RcSBc.png",included image,2013-09-03 12:22:26.347
176340,54622,12744.0,1,,CC BY-SA 3.0,ff32c643-c67e-43cf-81b8-9210d8867740,"Do ""true"" multi-level models require Bayesian methods?",,2013-09-03 21:37:13.153
176339,54622,12744.0,2,,CC BY-SA 3.0,ff32c643-c67e-43cf-81b8-9210d8867740,"I've been recently learning about mixed effects models (e.g. via Fitzmaurice, Laird, and Ware 's book *Applied Longitudinal Analysis*) as well as Bayesian hierarchical models (e.g. via Gelman and Hill's book *Data Analysis Using Regression and Multilevel/Hierarchical Models*) 

One curious thing I've noticed:  The Bayesian literature tends to emphasize that their models can handle covariates at multiple level of analysis.  For example, if the clustering is by person, and each person is measured in multiple ""trials,"" then the Bayesian hierarchical models can investigate the main effects of covariates both at the subject and trial level, as well as interactions across ""levels.""

However, I have not seen these kinds of models in the textbooks introducing frequentist methods.    

I'm not sure if this is a coincidence, or an example of where Bayesian methods can do ""more complicated things.""  Is it possible to use mixed effects models (e.g. the lme4 or nlme packages in the R statistical software) to investigate interactions of covariates across ""levels"" of analysis?



",,2013-09-03 21:37:13.153
176341,54622,12744.0,3,,CC BY-SA 3.0,ff32c643-c67e-43cf-81b8-9210d8867740,<multilevel-analysis><mixed-model><hierarchical-bayesian>,,2013-09-03 21:37:13.153
176350,54624,503.0,2,,CC BY-SA 3.0,3dc1b12b-fe5f-4f8d-a2b6-9abb55692113,"Yes it is. I don't know the commands in `R` but in `SAS PROC MIXED` you can have variables at either level in the MODEL statement and you can include interactions. e.g., a split plot design 

    proc mixed;
       class A B Block;
       model Y = A B A*B;
       random Block A*Block;
    run;

where A is assigned to whole plots and B is assigned to subplots.


",,2013-09-03 22:08:41.237
176398,54637,21382.0,1,,CC BY-SA 3.0,1b932e59-71cc-4d8e-9ebc-9e42822b7b88,how to get pooled p-values on tests done in multiple imputed datasets?,,2013-09-04 01:06:26.173
176399,54637,21382.0,3,,CC BY-SA 3.0,1b932e59-71cc-4d8e-9ebc-9e42822b7b88,<r><spss><p-value><multiple-imputation><pooling>,,2013-09-04 01:06:26.173
176397,54637,21382.0,2,,CC BY-SA 3.0,1b932e59-71cc-4d8e-9ebc-9e42822b7b88,"Using Amelia in R, I obtained multiple imputed datasets. After that, I performed a repeated measures test in SPSS. Now, I want to pool test results. I know that I can use Rubin's rules (implemented through any multiple imputation package in R) to pool means and standard erros, but how do I pool p-values? Is it possible? Is there a function in R to do so?
Thanks in advance.",,2013-09-04 01:06:26.173
176403,54637,,24,,CC BY-SA 3.0,55d21b0f-722c-413f-8362-eae00df3a582,,"Proposed by 21599 approved by 601, 805 edit id of 5211",2013-09-04 01:41:45.727
176404,54637,15473.0,4,,CC BY-SA 3.0,55d21b0f-722c-413f-8362-eae00df3a582,How to get pooled p-values on tests done in multiple imputed datasets?,Fixed grammar,2013-09-04 01:41:45.727
176402,54637,15473.0,5,,CC BY-SA 3.0,55d21b0f-722c-413f-8362-eae00df3a582,"Using Amelia in R, I obtained multiple imputed datasets. After that, I performed a repeated measures test in SPSS. Now, I want to pool test results. I know that I can use Rubin's rules (implemented through any multiple imputation package in R) to pool means and standard errors, but how do I pool p-values? Is it possible? Is there a function in R to do so?
Thanks in advance.",Fixed grammar,2013-09-04 01:41:45.727
176728,54724,10957.0,3,,CC BY-SA 3.0,f021eb83-6877-4ab9-a762-66d6629e4d51,<machine-learning>,,2013-09-04 20:24:41.247
176733,54724,16174.0,4,,CC BY-SA 3.0,53b4720d-700c-43c1-84e2-57f73d508e02,Flexible and inflexible models in machine learning,"Sentence case style in title, tag, LaTeX",2013-09-04 20:39:44.190
176734,54724,16174.0,5,,CC BY-SA 3.0,53b4720d-700c-43c1-84e2-57f73d508e02,"I came across a simple question on comparing flexible models (i.e. splines) vs. inflexible models (e.g. linear regression) under different scenarios. The question is:

In general, do we expect the performance of a flexible statistical learning method to perform better or worse than an inflexible method when:

1. The number of predictors $p$ is extremely large, and the number of observations $n$ is small?  
2. The variance of the error terms, i.e. $σ^2 = \text{Var}(e)$, is extremely high?

I think for (1), when $n$ is small, inflexible models are better (not sure). For (2), I don't know which model is (relatively) better.","Sentence case style in title, tag, LaTeX",2013-09-04 20:39:44.190
177138,54836,20304.0,3,,CC BY-SA 3.0,5966db7f-40ab-4f21-8d87-0b85bae4b157,<gibbs><dirichlet-distribution>,,2013-09-06 15:56:05.860
177136,54836,20304.0,2,,CC BY-SA 3.0,5966db7f-40ab-4f21-8d87-0b85bae4b157,"I am trying to implement LDA using the collapsed Gibbs sampler from 
http://www.uoguelph.ca/~wdarling/research/papers/TM.pdf

the main algorithm is shown below

![enter image description here][1]


  [1]: https://i.stack.imgur.com/X9OwX.png

I'm a bit confused about the notation in the inner-most loop. n_dk refers to the count of the number of words assigned to topic k in document d, however I'm not sure which document d this is referring to.  Is it the document that *word* (from the next outer loop) is in?  Furthermore, the paper does not show how to get the hyperparameters alpha and beta.  Should these be guessed and then tuned?  Furthermore, I don't understand what the *W* refers to in the inner-most loop (or the beta without the subscript).

Could anyone enlighten me?",,2013-09-06 15:56:05.860
177137,54836,20304.0,1,,CC BY-SA 3.0,5966db7f-40ab-4f21-8d87-0b85bae4b157,Implementing LDA - notation confusion,,2013-09-06 15:56:05.860
177139,54836,,6,,CC BY-SA 3.0,b94f0d74-74b1-42dd-bb01-e733da209010,<gibbs><dirichlet-distribution><topic-models>,edited tags,2013-09-06 16:08:33.727
177161,54836,2081.0,4,,CC BY-SA 3.0,57659a03-60cf-45f9-876b-fa8d3f8007b8,Implementing Latent Dirichlet Allocation - notation confusion,edited title,2013-09-06 17:45:26.307
177442,54915,21523.0,2,,CC BY-SA 3.0,3523ce2f-d9d7-47b8-b2e8-861dc04bb4a2,"I'm trying to implement Pettitt test in R following papers like this:

http://www.ias.ac.in/jess/forthcoming/JESS-D-13-00049.pdf (pag. 5, 6)

or this:

http://www.igu.in/17-3/paper-2.pdf

But, I'm misunderstandig something because testing with some data I think that output is not correct.

Here are the code:

    pettitt<-function(x,alpha=0.99) {
    # Pettitt AN. 1979 A non-parametric approach to the change point detection.
    # x is a vector
    # alpha, integer, level of significance
    x<-na.omit(x)
    o<-rank(x)
    s<-c()
    L<-length(x)
    for (i in 1:(L-1)) {
          s<-c(s,
        2*(colSums(as.matrix(o[1:i])))
           -(i*(L+1))
      )
    }
    vc<-sqrt((-1)*log(alpha)*(L^3 + L^2)/6)
    output<-list(abs(s),vc)
    return(output)
}

Testing with larain and tempdub dataset from TSA package:

<pre><code>
library(TSA)
data(larain)
data(tempdub)
pettitt(larain)
[[1]]
  [1]  78 118 180  76  30  30 144  90 124 148 224 334 314 298 362 444 356 334
 [19] 300 302 194 121  83  55  45  57  25  95 175 195 193 287 181 231 175 213
 [37] 301 331 421 345 392 322 282 354 372 274 194 130 188 248 175  97  85 153
 [55] 105 171 181 189 245 297 401 375 449 557 467 551 594 576 602 490 406 354
 [73] 262 266 362 248 244 214 208 200 247 147  89  13   9  15  97   5   9  83
 [91]   3  95 123  63  31  12  44   6  48  34  72 108 208 164 170 282 214 148
[109] 202 140 104   6 102  86

[[2]]
[1] 50.69224

> max(pettitt(larain)[[1]])
[1] 602


pettitt(tempdub)
[[1]]
  [1]  83 161 226 235 164  60  80 169 220 219 188  74  57 177 266 281 228 147
 [19]  19  82 125 140 102  41 100 197 235 254 233 141   1  97 144 153 112  26
 [37]  73 206 255 258 235 137  28  49  98 101  46  29 149 252 281 274 247 160
 [55]  43  70 115 126  79  22 157 248 317 328 287 224  96  27  86  79  27  82
 [73] 225 348 407 406 351 256 125  10  58  77  32  61 200 314 381 386 353 216
 [91] 124  40  35  70  35  36 173 302 365 386 321 242 131  10  51  38  19 146
[109] 241 319 342 359 330 223  89  45 113 144 111   2 123 228 280 275 250 177
[127]  34  50  89 102  59  22 131 248 334 359 302 198  73  46  83 100  73

[[2]]
[1] 70.96777

> max(pettitt(tempdub)[[1]])
[1] 407

</pre></code>

I don't know if I lost something in pettitt test or there are error in my code.",,2013-09-08 13:37:22.690
177443,54915,21523.0,1,,CC BY-SA 3.0,3523ce2f-d9d7-47b8-b2e8-861dc04bb4a2,Implementing Pettitt test in R,,2013-09-08 13:37:22.690
177444,54915,21523.0,3,,CC BY-SA 3.0,3523ce2f-d9d7-47b8-b2e8-861dc04bb4a2,<r><time-series><heteroscedasticity>,,2013-09-08 13:37:22.690
177450,54915,5237.0,5,,CC BY-SA 3.0,871b4b5f-75a3-473b-aa10-e831ccf77240,"I'm trying to implement Pettitt test in R following papers like this [pdf](http://www.ias.ac.in/jess/forthcoming/JESS-D-13-00049.pdf) (pp. 5 & 6), or this [pdf](http://www.igu.in/17-3/paper-2.pdf). But, I'm misunderstanding something, because having tested it with some data, I think that output is not correct.

Here is the code:

    pettitt <- function(x, alpha=0.99) {
    # Pettitt AN. 1979 A non-parametric approach to the change point detection.
    # x is a vector
    # alpha, integer, level of significance
    x <- na.omit(x)
    o <- rank(x)
    s <- c()
    L <- length(x)
    for (i in 1:(L-1)) {
          s <- c(s, 2*(colSums(as.matrix(o[1:i]))) - (i*(L+1)) )
    }
    vc <- sqrt((-1) * log(alpha) * (L^3 + L^2)/6)
    output <- list(abs(s), vc)
    return(output)
    }

Testing with `larain` and `tempdub` dataset from `TSA package`:

    library(TSA)
    data(larain)
    data(tempdub)
    pettitt(larain)
    [[1]]
      [1]  78 118 180  76  30  30 144  90 124 148 224 334 314 298 362 444 356 334
     [19] 300 302 194 121  83  55  45  57  25  95 175 195 193 287 181 231 175 213
     [37] 301 331 421 345 392 322 282 354 372 274 194 130 188 248 175  97  85 153
     [55] 105 171 181 189 245 297 401 375 449 557 467 551 594 576 602 490 406 354
     [73] 262 266 362 248 244 214 208 200 247 147  89  13   9  15  97   5   9  83
     [91]   3  95 123  63  31  12  44   6  48  34  72 108 208 164 170 282 214 148
    [109] 202 140 104   6 102  86
    
    [[2]]
    [1] 50.69224
    
    > max(pettitt(larain)[[1]])
    [1] 602
    
    pettitt(tempdub)
    [[1]]
      [1]  83 161 226 235 164  60  80 169 220 219 188  74  57 177 266 281 228 147
     [19]  19  82 125 140 102  41 100 197 235 254 233 141   1  97 144 153 112  26
     [37]  73 206 255 258 235 137  28  49  98 101  46  29 149 252 281 274 247 160
     [55]  43  70 115 126  79  22 157 248 317 328 287 224  96  27  86  79  27  82
     [73] 225 348 407 406 351 256 125  10  58  77  32  61 200 314 381 386 353 216
     [91] 124  40  35  70  35  36 173 302 365 386 321 242 131  10  51  38  19 146
    [109] 241 319 342 359 330 223  89  45 113 144 111   2 123 228 280 275 250 177
    [127]  34  50  89 102  59  22 131 248 334 359 302 198  73  46  83 100  73
    
    [[2]]
    [1] 70.96777
    
    > max(pettitt(tempdub)[[1]])
    [1] 407


I don't know if I lost something in pettitt test or there are error in my code.",light editing & formatting,2013-09-08 14:04:35.343
177453,54915,,25,,,151022fa-addc-4e75-8ebe-706f9a22c114,,http://twitter.com/#!/StackStats/status/376723347809517569,2013-09-08 15:06:59.810
177852,16337,2081.0,5,,CC BY-SA 3.0,ecb1723a-e780-466a-bcf9-8affd469662d,"**Spearman rho vs Kendall tau**. These two are so much computationally different that you *cannot* directly compare their magnitudes. Spearman is usually higher by 1/4 to 1/3 and this makes one incorrectly conclude that Spearman is ""better"" for a particular dataset. The difference between rho and tau is in their ideology, *proportion-of-variance* for rho and *probability* for tau. Rho is a usual Pearson r applied for ranked data, and like r, is more sensitive to points with large moments (that is, deviations from cloud centre) than to points with small moments. Therefore rho is quite sensitive to the shape of the cloud after ranking: the coefficient for an oblong rhombic cloud will be higher than the coefficient for an oblong dumbbelled cloud (because sharp edges of the first are large moments). Tau is an extension of Gamma and is equally sensitive to all points, so it is less sensitive to peculiarities in shape of the ranked cloud. Tau is more ""general"" than rho, for rho is warranted only when you believe the underlying relationship between the variables is monotonic. Rho is comparable with r in magnitude; tau is not.

**Kendall tau as Gamma**. Tau is just a standardized form of Gamma. Several related measures all have numerator $P-Q$ but differ in normalizing *denominator*:

 - Gamma: $P+Q$
 - Somer's D(""x dependent""): $P+Q+T_x$
 - Somer's D(""y dependent""): $P+Q+T_y$
 - Somer's D(""symmetric""): arithmetic mean of the above two
 - Kendall's Tau-b corr. (most suitable for square tables): geometric mean of those two
 - Kendall's Tau-c corr. (most suitable for rectangular tables): $N^2(k-1)/2k$
 - Kendall's Tau-a corr. (makes nо adjustment for ties): $N(N-1)/2 = P+Q+T_x+T_y+T_{xy}$

where $P$ - number of pairs of observations with ""concordance"", $Q$ - with ""inversion""; $T_x$ - number of ties by variable X, $T_y$ - by variable Y, $T_{xy}$ – by both variables; $N$ - number of observations, $k$ - number of distinct values in that variable where this number is less.
",added 87 characters in body,2013-09-10 10:14:19.363
177859,16337,2081.0,5,,CC BY-SA 3.0,834fd617-762f-49fa-b3a5-cafe5a705a0c,"**Spearman rho vs Kendall tau**. These two are so much computationally different that you *cannot* directly compare their magnitudes. Spearman is usually higher by 1/4 to 1/3 and this makes one incorrectly conclude that Spearman is ""better"" for a particular dataset. The difference between rho and tau is in their ideology, *proportion-of-variance* for rho and *probability* for tau. Rho is a usual Pearson r applied for ranked data, and like r, is more sensitive to points with large moments (that is, deviations from cloud centre) than to points with small moments. Therefore rho is quite sensitive to the shape of the cloud after ranking: the coefficient for an oblong rhombic cloud will be higher than the coefficient for an oblong dumbbelled cloud (because sharp edges of the first are large moments). Tau is an extension of Gamma and is equally sensitive to all points, so it is less sensitive to peculiarities in shape of the ranked cloud. Tau is more ""general"" than rho, for rho is warranted only when you believe the underlying relationship between the variables is monotonic. Rho is comparable with r in magnitude; tau is not.

**Kendall tau as Gamma**. Tau is just a standardized form of Gamma. Several related measures all have numerator $P-Q$ but differ in normalizing *denominator*:

 - Gamma: $P+Q$
 - Somer's D(""x dependent""): $P+Q+T_x$
 - Somer's D(""y dependent""): $P+Q+T_y$
 - Somer's D(""symmetric""): arithmetic mean of the above two
 - Kendall's Tau-b corr. (most suitable for square tables): geometric mean of those two
 - Kendall's Tau-c corr. (most suitable for rectangular tables): $N^2(k-1)/(2k)$
 - Kendall's Tau-a corr. (makes nо adjustment for ties): $N(N-1)/2 = P+Q+T_x+T_y+T_{xy}$

where $P$ - number of pairs of observations with ""concordance"", $Q$ - with ""inversion""; $T_x$ - number of ties by variable X, $T_y$ - by variable Y, $T_{xy}$ – by both variables; $N$ - number of observations, $k$ - number of distinct values in that variable where this number is less.
",added 2 characters in body,2013-09-10 10:46:32.390
177900,55043,18198.0,2,,CC BY-SA 3.0,b5f3d14a-c2ed-4993-b73e-c41a04b6ae94,"My problem is similar to this one but I am looking for a different solution: (so if it should be merged just let me know).

http://stats.stackexchange.com/questions/21742/measuring-whats-lost-in-pca-dimensionality-reduction

I my application we have a correlation matrix of dimension 30 upon which we conduct a PCA analysis and retain the first three eigenvectors on the basis that they typically contain 90+% of the variation. 

However this has always struck me as a little arbitrary, I would like to test whether these smaller eigenvectors do actually contain a ""signal"" rather than white noise.

I suppose one very simple method would be to split the data up and see if these smaller eignevectors maintain a similar shape, but I would like to find a more scientifically robust way to test this hypothesis.
",,2013-09-10 13:00:25.933
177901,55043,18198.0,1,,CC BY-SA 3.0,b5f3d14a-c2ed-4993-b73e-c41a04b6ae94,"Testing whether small Eigenvalues produce a ""signal""",,2013-09-10 13:00:25.933
177902,55043,18198.0,3,,CC BY-SA 3.0,b5f3d14a-c2ed-4993-b73e-c41a04b6ae94,<pca><information-theory>,,2013-09-10 13:00:25.933
177953,55043,18198.0,6,,CC BY-SA 3.0,b36dd6d7-23d1-4200-9a85-5102a479f8bb,<pca><information-theory><ica>,edited tags,2013-09-10 15:29:32.983
178298,55150,21630.0,1,,CC BY-SA 3.0,42df28a9-3aab-430b-9843-3528cc18147c,Mathematical definition of causality,,2013-09-12 01:13:21.980
185696,57278,20144.0,1,,CC BY-SA 3.0,5d3d56c9-9330-4d2b-88f3-518b100b01d3,"Exponential family parameter estimation and fitting, references",,2013-10-11 06:43:41.090
185938,57348,594.0,5,,CC BY-SA 3.0,6ff91b81-0395-40da-ad4b-4532e4975f87,"If 
 
$$P = [0,0.9,0,0.1]$$

$$Q = [0,1,0,0]$$
 
Then $$KL(P||Q) = 0 + \ln(0.9/1)\cdot0.9 + 0 + 0 = -0.094$$ 
 
This shouldn't be possible from the Gibbs inequality. What am I misunderstanding? ",added 13 characters in body,2013-10-12 11:42:48.307
178297,55150,21630.0,2,,CC BY-SA 3.0,42df28a9-3aab-430b-9843-3528cc18147c,"Let Y and X be random variables. E(Y|X) is the conditional mean of Y given X. We say Y is not causally related to X if E(Y|X) does not depend on X, i.e., it is equal to E(Y). Now, let's go along with this definiton of causality for a second. By the law of iterated expectations, E(XE(Y|X)) = E(E(XY|X)) = E(XY). This means that if E(Y|X) does not depend on X, if it is equal to E(Y), then E(X)E(Y) = E(XY). In other words: 

If X and Y are not causally related, then X and Y are uncorrelated! - This makes no sense and I know this must be wrong. What is my mistake?

Kind regards,

Christian",,2013-09-12 01:13:21.980
178299,55150,21630.0,3,,CC BY-SA 3.0,42df28a9-3aab-430b-9843-3528cc18147c,<econometrics><conditional-expectation>,,2013-09-12 01:13:21.980
178303,55150,21630.0,5,,CC BY-SA 3.0,5a0417cf-2e26-40b7-af28-f00ff2387346,"Let Y and X be random variables. E(Y|X) is the conditional mean of Y given X. We say Y is not causally related to X if E(Y|X) does not depend on X, i.e., it is equal to E(Y). Now, let's go along with this definiton of causality for a second. By the law of iterated expectations, E(XE(Y|X)) = E(E(XY|X)) = E(XY). This means that if E(Y|X) does not depend on X, if it is equal to E(Y), then E(X)E(Y) = E(XY). 

In econometrics we generally assume E(Y|X) = b0 + b1*X. So E(Y|X) = E(Y) is equivalent to b1 = 0. The logic applies in this specific scenario too.

In other words: 

If X and Y are not causally related, then X and Y are uncorrelated! - This makes no sense and I know this must be wrong. What is my mistake?

Kind regards,

Christian",added 154 characters in body,2013-09-12 01:22:02.703
178311,55150,5237.0,5,,CC BY-SA 3.0,7c2dc3e9-2c38-4f8f-8cdb-c3397fee19cb,"Let $Y$ and $X$ be random variables. $E(Y|X)$ is the conditional mean of $Y$ given $X$. We say $Y$ is not causally related to $X$ if $E(Y|X)$ does not depend on $X$, i.e., it is equal to $E(Y)$. Now, let's go along with this definition of causality for a second. By the law of iterated expectations, $E(XE(Y|X)) = E(E(XY|X)) = E(XY)$. This means that if $E(Y|X)$ does not depend on $X$, if it is equal to $E(Y)$, then $E(X)E(Y) = E(XY)$. 

In econometrics we generally assume $E(Y|X) = b_0 + b_1X$. So $E(Y|X) = E(Y)$ is equivalent to $b_1 = 0$. The logic applies in this specific scenario too.

In other words: 

If $X$ and $Y$ are not causally related, then $X$ and $Y$ are uncorrelated! - This makes no sense and I know this must be wrong. What is my mistake?
",formatted; removed signature,2013-09-12 01:41:23.783
178312,55150,21630.0,5,,CC BY-SA 3.0,14307d4b-f114-4f0b-b384-869cd9b1fa77,"Let $Y$ and $X$ be random variables. $E(Y|X)$ is the conditional mean of $Y$ given $X$. We say $Y$ is not causally related to $X$ if $E(Y|X)$ does not depend on $X$, which implies it is equal to $E(Y)$. Now, let's go along with this definition of causality for a second. By the law of iterated expectations, $E(XE(Y|X)) = E(E(XY|X)) = E(XY)$. This means that if $E(Y|X)$ does not depend on $X$, if it is equal to $E(Y)$, then $E(X)E(Y) = E(XY)$. 

In other words: 

If $X$ and $Y$ are not causally related, then $X$ and $Y$ are uncorrelated! - This makes no sense and I know this must be wrong. Have I defined causality incorrectly? What have I done wrong? 

In econometrics we generally assume $E(Y|X) = b_0 + b_1X$. So $E(Y|X) = E(Y)$ is equivalent to $b_1 = 0$. The logic applies in this specific scenario too.
",added 51 characters in body,2013-09-12 02:08:52.787
178317,55150,,25,,,241a0024-1c80-4cea-8a54-7fd75db59461,,http://twitter.com/#!/StackStats/status/377991721180987392,2013-09-12 03:07:03.497
178427,55182,17573.0,2,,CC BY-SA 3.0,d9871688-2e22-4bd2-820f-8fb0ce846fdf,"You have defined causality incorrectly, yes.  Probably, you have heard the saying ""correlation isn't causation.""  You have essentially defined causality as correlation.  The problem is worse than that, though.  Causality is not a statistical or probabilistic concept at all, at least as those topics are normally taught.  There is no statistical or probabilistic definition of causality: nothing involving conditional expectations or conditional distributions or suchlike.  It is hard to pick up this fact from courses in statistics or econometrics, though.

Unfortunately, we tend to do a better job saying what causality isn't than what causality is.  Causality always and everywhere comes from theory, from a priori reasoning, from assumptions.  You mentioned econometrics.  If you have been taught instrumental variables competently, then you know that causal effects can only be measured if you have an ""exclusion restriction.""  And you know that exclusion restrictions always come from theory.

You said you wanted math, though.  The guy you want to read is [Judea Pearl][1].  It's not easy math, and the math sometimes wanders off into philosophy, but that's because causality is a hard subject.  Here is [a page][2] with more links on the subject.

If the math there is too hard, let me know, and I will see if I can find an easier presentation.


  [1]: http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdfview_1&handle=euclid.ssu/1255440554
  [2]: http://vserver1.cscs.lsa.umich.edu/~crshalizi/notebooks/causality.html",,2013-09-12 13:18:37.003
178431,55182,17573.0,5,,CC BY-SA 3.0,d318bda4-2a10-4535-a249-bd6eebb6c455,"You have defined causality incorrectly, yes.  Probably, you have heard the saying ""correlation isn't causation.""  You have essentially defined causality as correlation.  The problem is worse than that, though.  Causality is not a statistical or probabilistic concept at all, at least as those topics are normally taught.  There is no statistical or probabilistic definition of causality: nothing involving conditional expectations or conditional distributions or suchlike.  It is hard to pick up this fact from courses in statistics or econometrics, though.

Unfortunately, we tend to do a better job saying what causality isn't than what causality is.  Causality always and everywhere comes from theory, from a priori reasoning, from assumptions.  You mentioned econometrics.  If you have been taught instrumental variables competently, then you know that causal effects can only be measured if you have an ""exclusion restriction.""  And you know that exclusion restrictions always come from theory.

You said you wanted math, though.  The guy you want to read is [Judea Pearl][1].  It's not easy math, and the math sometimes wanders off into philosophy, but that's because causality is a hard subject.  Here is [a page][2] with more links on the subject.  Here is [a free online book][3] I just came across.  Finally, here is [a previous question][4] where I gave an answer you might find useful.


  [1]: http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdfview_1&handle=euclid.ssu/1255440554
  [2]: http://vserver1.cscs.lsa.umich.edu/~crshalizi/notebooks/causality.html
  [3]: http://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/
  [4]: http://stats.stackexchange.com/questions/59588/what-is-endogeneity-and-what-does-it-mean-substantively-as-an-extension-what-is",added additional relevant links,2013-09-12 13:38:24.510
178452,55150,,24,,CC BY-SA 3.0,2a5d9eac-f1f9-4cce-aadd-981efbf470c7,,"Proposed by 25212 approved by 686, 601 edit id of 5387",2013-09-12 14:01:36.937
178451,55150,17573.0,6,,CC BY-SA 3.0,2a5d9eac-f1f9-4cce-aadd-981efbf470c7,<econometrics><causality><conditional-expectation>,Added tag causal inference,2013-09-12 14:01:36.937
178525,55209,20222.0,3,,CC BY-SA 3.0,6030c099-6008-47b8-ba82-1b3c70a4349f,<distributions><statistical-significance><confidence-interval><kolmogorov-smirnov-test>,,2013-09-12 19:12:06.933
178526,55209,20222.0,2,,CC BY-SA 3.0,6030c099-6008-47b8-ba82-1b3c70a4349f,"As a non-statistician, I need help in interpreting a customer specified two-part reliability requirement that I think involves KS.

Requirement Part 1

R[4 years] must be greater than or equal to 0.95 and

R[8 years] must be greater than or equal to 0.85

I have plotted the reliability (survival) function of a 2-parameter Weibull distribution that meets the above requirement in Plot A below. The shape parameter is 1.664 and the characteristic life is 23.844 for this distribution.


![Plot A]![enter image description here][1]


Requirement Part 2

The confidence level shall be 90% when demonstrating the Part 1 requirement via product life testing.

It’s the Part 2 that I’m a bit shaky on. On page 8-54 of MIL-HDBK-338B (http://www.sre.org/pubs/Mil-Hdbk-338B.pdf) there is a table showing KS critical “d” values as a function samples size, N and significance level, alpha (also note the plot on page 8-57). From this table I took a d value of 0.264 based on a signficance value of 0.10 and a sample size of 20. Plot B below shows my result. My interpretation of Plot B is that after running a life test on 20 samples that if the resulting reliability plot does not fall below the lower boundary shown in Plot B then we have met the requirements.

![Plot B]![enter image description here][2]

I have two questions:

1. Did I translate the Part 2 requirement properly when I used an alpha of 0.10 to obtain the KS critical value of 0.264 ?  In other words, does a 90% confidence equal a 0.10 significance within the KS context ? If not, can someone provide guidance ?

2. How would *you* interpret Plot B ?

Many thanks.


  [1]: https://i.stack.imgur.com/bY5JN.png
  [2]: https://i.stack.imgur.com/ilZqf.png",,2013-09-12 19:12:06.933
178527,55209,20222.0,1,,CC BY-SA 3.0,6030c099-6008-47b8-ba82-1b3c70a4349f,Interpretation of Kolmogorov-Smirnov Critical Value Generated Distributions,,2013-09-12 19:12:06.933
178714,55260,13459.0,2,,CC BY-SA 3.0,61e5ebd1-a4c6-465d-9694-7b23e87c2e5f,"I have a dataframe with 2 million rows and approximately 200 columns/features. Approximately 30-40% of the entries are blank. I am trying to find important features for a binary response variable.  The predictors may be categorical or continuous. 

I started with applying logistic regression, but having so much missing entries I feel that this is not a good approach as glm discard all records which have any item blank. So I am now looking to apply tree based algorithms (`rpart` or `gbm`) which are capable to handle missing data in a better way. 

Since my data is too big for `rpart` or `gbm`, I decided to randomly fetch 10,000 records from original data, apply `rpart` on that, and keep building a pool of important variables. However, even this 10,000 records seem to be too much for the `rpart` algorithm. 

What can I do in this situation? Is there any switch that I can use to make it fast? Or it is impossible to apply `rpart` on my data. 

I am using the following rpart command:

    varimp = rpart(fmla,  dat=tmpData, method = ""class"")$variable.importance

",,2013-09-13 15:45:00.397
178716,55260,13459.0,3,,CC BY-SA 3.0,61e5ebd1-a4c6-465d-9694-7b23e87c2e5f,<r><regression><cart><big-data>,,2013-09-13 15:45:00.397
178715,55260,13459.0,1,,CC BY-SA 3.0,61e5ebd1-a4c6-465d-9694-7b23e87c2e5f,which regression tree to use for large data?,,2013-09-13 15:45:00.397
178735,55260,5237.0,5,,CC BY-SA 3.0,6df3ac67-9fb1-42aa-9d67-eaba2cda024d,"I have a dataframe with 2 million rows and approximately 200 columns / features. Approximately 30-40% of the entries are blank. I am trying to find important features for a binary response variable.  The predictors may be categorical or continuous. 

I started with applying logistic regression, but having so much missing entries I feel that this is not a good approach as glm discard all records which have any item blank. So I am now looking to apply tree based algorithms (`rpart` or `gbm`) which are capable to handle missing data in a better way. 

Since my data is too big for `rpart` or `gbm`, I decided to randomly fetch 10,000 records from original data, apply `rpart` on that, and keep building a pool of important variables. However, even this 10,000 records seem to be too much for the `rpart` algorithm. 

What can I do in this situation? Is there any switch that I can use to make it fast? Or it is impossible to apply `rpart` on my data. 

I am using the following rpart command:

    varimp = rpart(fmla,  dat=tmpData, method = ""class"")$variable.importance

",switched tag; light editing,2013-09-13 17:07:10.740
178737,55260,5237.0,6,,CC BY-SA 3.0,6df3ac67-9fb1-42aa-9d67-eaba2cda024d,<r><regression><cart><large-data>,switched tag; light editing,2013-09-13 17:07:10.740
178736,55260,5237.0,4,,CC BY-SA 3.0,6df3ac67-9fb1-42aa-9d67-eaba2cda024d,Which regression tree to use for large data?,switched tag; light editing,2013-09-13 17:07:10.740
179054,55361,227.0,2,,CC BY-SA 3.0,ea423552-8c7b-4505-a48b-1794fd331c90,"I'm working through Think Bayes (free here: http://www.greenteapress.com/thinkbayes/) and I'm on exercise 3.1. Here's a summary of the problem:

""A railroad numbers its locomotives in order 1..N. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad has.""

This solution is found with the likelihood function and exponential prior like so:

    class Train(Suite):
      def __init__(self, hypos, alpha=1.0):
        # Create an exponential prior
        Pmf.__init__(self)
        for hypo in hypos:
          self.Set(hypo, hypo**(-alpha))
        self.Normalize()
      def Likelihood(self, data, hypo):
        if hypo < data:
          return 0
        else:
          return (1.0/hypo)

Conceptually this is saying, if we see a train number larger than one of our hypotheses (1...1000) then every hypothesis that's smaller has a zero chance of being correct. The rest of the hypotheses have a 1/number_of_trains chance of showing us a train with this number.

In the exercise I'm working on the author then adds on a little extra. This assumes there's only one company. In real life however you'd have a mixture of big and small companies and bigger companies (both equally likely). However, this would mean that you're more likely to see a train from a bigger company since they'd have more trains.

Now the question is how to reflect this in the likelihood function?

This isn't Stack Overflow so I'm not really asking for coding help, but instead perhaps just help about how I might think about this problem in terms of a likelihood function.",,2013-09-15 23:02:42.580
179055,55361,227.0,1,,CC BY-SA 3.0,ea423552-8c7b-4505-a48b-1794fd331c90,Locomotive problem with various size companies,,2013-09-15 23:02:42.580
179056,55361,227.0,3,,CC BY-SA 3.0,ea423552-8c7b-4505-a48b-1794fd331c90,<bayesian><conditional-probability><bayes>,,2013-09-15 23:02:42.580
179324,55436,21778.0,3,,CC BY-SA 3.0,2d9d4004-d3f1-4abd-aded-1fab527cffbd,<classification><predictive-models>,,2013-09-16 22:19:43.457
179323,55436,21778.0,1,,CC BY-SA 3.0,2d9d4004-d3f1-4abd-aded-1fab527cffbd,Creating a high predictive value classifier,,2013-09-16 22:19:43.457
179325,55436,21778.0,2,,CC BY-SA 3.0,2d9d4004-d3f1-4abd-aded-1fab527cffbd,"I have a two-class classification problem with n-dimensional data. I would like to train a classifier (preferably but not necessarily linear) with 100% positive predictive value. In other words, I want the model to completely avoid one of the classes. For this application a low-ish sensitivity is OK as long as PPV is ~100%.  
Do you have any suggestions of good techniques to use? 
Thank you!",,2013-09-16 22:19:43.457
179774,55576,21833.0,2,,CC BY-SA 3.0,aaf7aaeb-ac21-4d4d-b4be-c8c1715d8b8d,"I need to generate random numbers based on already existing partial correlation data (not correlation or covariance data). Specifically, a 168*12 matrix based on a 12*12 partial correlation matrix. The idea is to simulate a data matrix that can be used for testing a few components of a project.

Any help in this regard would be appreciated. I have looked around but have not found any threads that talk about doing this with partial correlation data.

If someone has ideas about implementation in MATLAB, that would be a bonus!

Thanks a lot in advance!",,2013-09-18 12:09:15.653
179885,55609,21842.0,2,,CC BY-SA 3.0,81b91f73-5152-46a9-b8b8-32a8b136aa2c,"I am a newbie here. well I am trying to find a method or a formula to forecast meals per day, which have ̀5 meals to upload on flights, sales, wastage and passengers are what I have to consider, the old template is not completed yet , and its not so good to forecast, and I can't think of other formulas or methods to forecast , I have the sales in the past few months.. anyone can suggest me which methods can solve this problem? well I am using MS.Excel to calculate or if there is a program to suggest me it would be great :) ..thanks a lot for helping and sorry for my bad english :)",,2013-09-18 18:04:46.113
179887,55609,21842.0,3,,CC BY-SA 3.0,81b91f73-5152-46a9-b8b8-32a8b136aa2c,<forecasting><excel>,,2013-09-18 18:04:46.113
179886,55609,21842.0,1,,CC BY-SA 3.0,81b91f73-5152-46a9-b8b8-32a8b136aa2c,how to forecast daily sale using Excel,,2013-09-18 18:04:46.113
179924,55617,21846.0,1,,CC BY-SA 3.0,f71e727d-faa6-41dc-a2b3-0cb8efc827f0,regression with rank order as dependent variable,,2013-09-18 19:30:10.233
179923,55617,21846.0,2,,CC BY-SA 3.0,f71e727d-faa6-41dc-a2b3-0cb8efc827f0,"I have data on 44 firms that have all been ranked by an expert. The ""best"" firm has rank 1, the second best has rank 2, ..., the last one has rank 44. 
I have a bunch of explanatory variables and would like to explain the rank of the firm on the basis of these variables. My inclination is to use a regression model, but am concerned about the fact that the dependent variable is limited, it can only be a positive discrete number. 

I have thought about ordinal regression, but that seems impossible since I would have as many categories as I have observations. 

What regression models would be possible? (preferably to be run in R)

thanks,
Peter
",,2013-09-18 19:30:10.233
179922,55617,21846.0,3,,CC BY-SA 3.0,f71e727d-faa6-41dc-a2b3-0cb8efc827f0,<regression><multiple-regression><ordinal-data>,,2013-09-18 19:30:10.233
179931,55617,,4,,CC BY-SA 3.0,c72f5abb-33e2-4530-8e0d-584193f7cbce,Regression with rank order as dependent variable,edited title,2013-09-18 19:53:54.687
180251,55722,21885.0,3,,CC BY-SA 3.0,541541c7-b766-4959-9575-bad302cfcdbb,<self-study><references><basic-concepts>,,2013-09-19 22:14:08.257
180252,55722,21885.0,1,,CC BY-SA 3.0,541541c7-b766-4959-9575-bad302cfcdbb,Looking for a good and complete probabilty and statistics book,,2013-09-19 22:14:08.257
180253,55722,21885.0,2,,CC BY-SA 3.0,541541c7-b766-4959-9575-bad302cfcdbb,"I never had the  opportunity to visit a stats course from a math faculty. I am looking for a probability theory and statistics book that is complete and self sufficient. By complete I mean that it contains all the proofs and not just states results. By self sufficient I mean that I am not required to read another book to be able to understand the book. Of course it can require college level (math student) calculus and linear algebra.

I have looked at multiple books and it didn't like any of them.

http://www.amazon.com/Probability-Statistics-Edition-Morris-DeGroot/dp/0321500466 is not complete enough. It just states a lot of stuff without the derivation.  Besides that I like it.

http://www.amazon.de/All-Statistics-Statistical-Inference-Springer/dp/0387402721
Didn't like it at all. Almost no explanations.

Weighing the Odds from David Willams is more formal  degroot  and seems to be is complete and  self sufficient. However, I find the style strange. He also invents new terms that only he seems to use. All the stuff that is explained in DeGroot too is explained better there.

If you know a great book in German that's also fine as I am german



",,2013-09-19 22:14:08.257
180263,55722,,5,,CC BY-SA 3.0,49d9fd74-8fe8-41cd-b829-6bf7a24ff93f,"I never had the  opportunity to visit a stats course from a math faculty. I am looking for a probability theory and statistics book that is complete and self sufficient. By complete I mean that it contains all the proofs and not just states results. By self sufficient I mean that I am not required to read another book to be able to understand the book. Of course it can require college level (math student) calculus and linear algebra.

I have looked at multiple books and it didn't like any of them.

 * DeGroot & Schervish (2011) *[Probability and Statistics (4th Edition)](http://www.amazon.com/Probability-Statistics-Edition-Morris-DeGroot/dp/0321500466)* Pearson

   This is not complete enough. It just states a lot of stuff without the derivation.  Besides that I like it.

 * Wasserman (2004) *[All of Statistics: A Concise Course in Statistical Inference](http://www.amazon.de/All-Statistics-Statistical-Inference-Springer/dp/0387402721)* Springer.

   Didn't like it at all. Almost no explanations.

""Weighing the Odds"" from David Willams is more formal than DeGroot and seems to be complete and self sufficient. However, I find the style strange. He also invents new terms that only he seems to use. All the stuff that is explained in DeGroot too is explained better there.

If you know a great book in German that's also fine as I am German.
",tidy up the links and some of the text,2013-09-19 22:45:38.583
180325,55576,21833.0,6,,CC BY-SA 3.0,fd875972-05b1-4263-a5cd-d3d9489b3c08,<time-series><correlation><matlab><random-generation><partial-correlation>,Elaborated on the question,2013-09-20 06:23:02.580
180324,55576,21833.0,5,,CC BY-SA 3.0,fd875972-05b1-4263-a5cd-d3d9489b3c08,"I need to generate random numbers based on already existing partial correlation data (not correlation or covariance data). Specifically, a 168*12 matrix based on a 12*12 partial correlation matrix. The idea is to simulate a data matrix that can be used for testing a few components of a project.

Any help in this regard would be appreciated. I have looked around but have not found any threads that talk about doing this with partial correlation data.

If someone has ideas about implementation in MATLAB, that would be a bonus!

Thanks a lot in advance!


Additions:
Apologies for any ambiguity. 

-What I mean by partial correlation matrix is a matrix containing the partial correlations, calculated for any two pairs by partialling out effect of all other pairs.

-The goal is: given a matrix of partial correlation values, is there a way I can generate a data set (168*12) that would have these partial correlation values?

-If there is a method to convert partial correlation to correlation values, that would be appreciated as well.

Thanks again!",Elaborated on the question,2013-09-20 06:23:02.580
181533,56091,2802.0,2,,CC BY-SA 3.0,2fe33530-03ff-46a5-8fff-35bb520ac699,"If you are searching for proofs, I have been working for some time on a free stats textbook that collects lots of proofs of elementary and less elementary facts that are difficult to find in probability and statistics books (because they are scattered here and there). You can have a look at it at http://www.statlect.com/
",,2013-09-25 11:04:55.647
185717,57284,,3,user14650,CC BY-SA 3.0,d0e821d4-743c-40ef-83e9-66dad2846efa,<r><confidence-interval><wilcoxon-signed-rank>,,2013-10-11 09:06:48.623
181536,55722,15827.0,5,,CC BY-SA 3.0,2c950a00-ef5e-4b19-b2c0-aaa310f0a991,"I never had the  opportunity to visit a stats course from a math faculty. I am looking for a probability theory and statistics book that is complete and self-sufficient. By complete I mean that it contains all the proofs and not just states results. By self-sufficient I mean that I am not required to read another book to be able to understand the book. Of course it can require college level (math student) calculus and linear algebra.

I have looked at multiple books and I didn't like any of them.

 * DeGroot & Schervish (2011) *[Probability and Statistics (4th Edition)](http://www.amazon.com/Probability-Statistics-Edition-Morris-DeGroot/dp/0321500466)* Pearson

   This is not complete enough. It just states a lot of stuff without the derivation.  Besides that I like it.

 * Wasserman (2004) *[All of Statistics: A Concise Course in Statistical Inference](http://www.amazon.de/All-Statistics-Statistical-Inference-Springer/dp/0387402721)* Springer.

   Didn't like it at all. Almost no explanations.

""Weighing the Odds"" from David Williams is more formal than DeGroot and seems to be complete and self-sufficient. However, I find the style strange. He also invents new terms that only he seems to use. All the stuff that is explained in DeGroot too is explained better there.

If you know a great book in German that's also fine as I am German.
",small fixes to English,2013-09-25 11:17:31.700
182197,56273,22126.0,2,,CC BY-SA 3.0,3d58bdc2-ec6e-4e7f-99ad-17ba229c5149,"I have a question, that is very important for me! :(
It is written in book ""basic statistics for business and economics"" for organizing data into a frequency distribution:
step 1: Decide on the number of classes. The goal is to use just enough groupings or classes to reveal the shape of the distribution. Some judgment is needed here. A useful recipe to determine the number of classes (k) is the ""2 to the k rule"". This guide suggests you select the smallest number (k) for the number of classes such that 2k (in words, 2 raised to the power of k) is greater than the number of observations (n). [n<=2k​]

I want to know, how can I prove this formula?",,2013-09-27 11:50:54.613
182198,56273,22126.0,1,,CC BY-SA 3.0,3d58bdc2-ec6e-4e7f-99ad-17ba229c5149,Frequency Distribution,,2013-09-27 11:50:54.613
182196,56273,22126.0,3,,CC BY-SA 3.0,3d58bdc2-ec6e-4e7f-99ad-17ba229c5149,<statistical-significance><mathematical-statistics><descriptive-statistics>,,2013-09-27 11:50:54.613
182206,56273,16474.0,6,,CC BY-SA 3.0,319e43fe-dd7e-42a2-9fdb-0e62559f4987,<descriptive-statistics>,deleted 27 characters in body; edited tags,2013-09-27 12:15:34.770
182205,56273,16474.0,5,,CC BY-SA 3.0,319e43fe-dd7e-42a2-9fdb-0e62559f4987,"I have a question, that is very important for me! :(
It is written in book ""basic statistics for business and economics"" for organizing data into a frequency distribution:
step 1: Decide on the number of classes. The goal is to use just enough groupings or classes to reveal the shape of the distribution. Some judgment is needed here. A useful recipe to determine the number of classes (k) is the ""2 to the $k$ rule"". This guide suggests you select the smallest number ($k$) for the number of classes such that $2^k$ is greater than the number of observations ($n$). [$n<=2^k$​]

I want to know, how can I prove this formula?",deleted 27 characters in body; edited tags,2013-09-27 12:15:34.770
182207,56273,10060.0,5,,CC BY-SA 3.0,3e747219-1ad9-44c0-9c6f-e2bafd746d21,"I have a question, that is very important for me! :(
It is written in book *Basic Statistics for Business and Economics* for organizing data into a frequency distribution:
step 1: Decide on the number of classes. The goal is to use just enough groupings or classes to reveal the shape of the distribution. Some judgment is needed here. A useful recipe to determine the number of classes (k) is the ""2 to the $k$ rule"". This guide suggests you select the smallest number ($k$) for the number of classes such that $2^k$ is greater than the number of observations ($n$). [$n<=2^k$​]

I want to know, how can I prove this formula?",Capitalize book title.,2013-09-27 12:18:04.597
182220,56273,16043.0,5,,CC BY-SA 3.0,1a14aade-2587-428f-8a0a-c50714a3cd03,"I have a question related to the book *Basic Statistics for Business and Economics* for organizing data into a frequency distribution:
>Step 1: Decide on the number of classes. The goal is to use just enough groupings or classes to reveal the shape of the distribution. Some judgment is needed here. A useful recipe to determine the number of classes ($k$) is the ""2 to the $k$ rule"". This guide suggests you select the smallest number ($k$) for the number of classes such that $2^k$ is greater than the number of observations ($n$): [$n \le 2^k$​]

I want to know, how can I prove this formula?",latex and formatting,2013-09-27 12:51:41.120
182221,56273,,24,,CC BY-SA 3.0,1a14aade-2587-428f-8a0a-c50714a3cd03,,"Proposed by 22311 approved by 686, -1 edit id of 5508",2013-09-27 12:51:41.120
182222,56273,449.0,5,,CC BY-SA 3.0,83a1d797-6620-4831-844b-04ba9bc564f0,"I have a question that is very important to me related to the book *Basic Statistics for Business and Economics* for organizing data into a frequency distribution:
>Step 1: Decide on the number of classes. The goal is to use just enough groupings or classes to reveal the shape of the distribution. Some judgment is needed here. A useful recipe to determine the number of classes ($k$) is the ""2 to the $k$ rule"". This guide suggests you select the smallest number ($k$) for the number of classes such that $2^k$ is greater than the number of observations ($n$): [$n \le 2^k$​]

I want to know, how can I prove this formula?",latex and formatting,2013-09-27 12:51:41.120
182549,56372,21108.0,2,,CC BY-SA 3.0,928b4018-77a8-4e62-b27e-5b398cf42ba5,"I just made an implementation of P(A/B)/P(¬A/B) for a ""people who bought this also bought..."" algorithm.


I'm doing it by 

    P(A/B) = count_users(bought_A_and_B)/count_users(bougth_A)
    P(¬A/B) = count_users(bougth_B_but_not_A)/count_users(did_not_buy_A)

Then dividing the top one by the bottom one I get a score which makes absolute sense, but what kind of correlation am I calculating? What is this method called? Where can I read more about it?",,2013-09-28 23:42:17.863
182550,56372,21108.0,1,,CC BY-SA 3.0,928b4018-77a8-4e62-b27e-5b398cf42ba5,Item correlation for recommender system,,2013-09-28 23:42:17.863
182548,56372,21108.0,3,,CC BY-SA 3.0,928b4018-77a8-4e62-b27e-5b398cf42ba5,<correlation><recommender-system>,,2013-09-28 23:42:17.863
182781,56445,22189.0,3,,CC BY-SA 3.0,cff5421d-f6ce-497f-b7bd-92ac3fe5b2bf,<distributions><experiment-design><wilcoxon>,,2013-09-30 08:12:44.097
182782,56445,22189.0,2,,CC BY-SA 3.0,cff5421d-f6ce-497f-b7bd-92ac3fe5b2bf,"I have following problem: 
Within an independent groups 1-factor design I have two independent groups, with a sample size of 20 each. The data of the treatment group is not normally distributed, whereas the data for the control group is (checked with Shapiro-Wilk Normality Test). Now I want to check if the differences of the means of both groups are significant. 
What is the appropriate test for this? I think it should be the Wilcoxon Rank Sum and Signed Rank Test, but I am not sure...

Could please anybody help me?

Thank you very much in advance.",,2013-09-30 08:12:44.097
182783,56445,22189.0,1,,CC BY-SA 3.0,cff5421d-f6ce-497f-b7bd-92ac3fe5b2bf,"Testing for significance between means, having one normal distributed sample and one non normal distributed",,2013-09-30 08:12:44.097
182822,56445,,5,,CC BY-SA 3.0,c5668d6b-ad14-42cc-a570-ec75a045e1d0,"I have following problem: 

Within an independent groups 1-factor design I have two independent groups, with a sample size of 20 each. The data of the treatment group is not normally distributed, whereas the data for the control group is (checked with Shapiro-Wilk Normality Test). Now I want to check if the differences of the means of both groups are significant. 
What is the appropriate test for this? I think it should be the Wilcoxon Rank Sum and Signed Rank Test, but I am not sure...

Could please anybody help me?
",formatting and remove sign off,2013-09-30 11:14:45.493
182832,56445,,25,,,92218eee-abef-4e0e-a4d7-0b3e3fe1d76e,,http://twitter.com/#!/StackStats/status/384650938159747072,2013-09-30 12:08:24.857
182881,543,,5,,CC BY-SA 3.0,8def996c-7bd9-42fc-8aff-c43f96c20ae0,"As an economist, the analysis of variance (ANOVA) is taught and usually understood in relation to linear regression (e.g. in Arthur Goldberger's *A Course in Econometrics*). Economists/Econometricians typically view ANOVA as uninteresting and prefer to move straight to regression models. From the perspective of linear (or even generalised linear) models, ANOVA assigns coefficients into batches, with each batch corresponding to a ""source of variation"" in ANOVA terminology.

Generally you can replicate the inferences you would obtain from ANOVA using regression but not always OLS regression. Multilevel models are needed for analysing hierarchical data structures such as ""split-plot designs,"" where between-group effects are compared to group-level errors, and within-group effects are compared to data-level errors. [Gelman's paper][1] &#91;1&#93; goes into great detail about this problem and effectively argues that ANOVA is an important statistical tool that should still be taught for it's own sake.

In particular Gelman argues that ANOVA is a way of understanding and structuring multilevel models. Therefore ANOVA is not an alternative to regression but as a tool for summarizing complex high-dimensional inferences and for exploratory data analysis. 

Gelman is a well-respected statistician and some credence should be given to his view. However, almost all of the empirical work that I do would be equally well served by linear regression and so I firmly fall into the camp of viewing it as a little bit pointless. Some disciplines with complex study designs (e.g. psychology) may find ANOVA useful.

&#91;1&#93; Gelman, A. (2005). Analysis of variance: why it is more important than ever (with discussion). *Annals of Statistics* 33, 1–53. [doi:10.1214/009053604000001048][1]

[1]:http://dx.doi.org/10.1214%2F009053604000001048",direct link didn't work; reverting,2013-09-30 15:35:44.163
182882,543,,24,,CC BY-SA 3.0,8def996c-7bd9-42fc-8aff-c43f96c20ae0,,"Proposed by 30872 approved by 5836, 6029 edit id of 5524",2013-09-30 15:35:44.163
183210,56580,20190.0,2,,CC BY-SA 3.0,328921c4-74fc-406e-a600-f30d2a86f8ee,Why log transformation may improve results of svm prediction(regression)? Does svm based on assumption of normal distribution or something else? ,,2013-10-01 18:59:27.453
183208,56580,20190.0,1,,CC BY-SA 3.0,328921c4-74fc-406e-a600-f30d2a86f8ee,SVM and log transformation,,2013-10-01 18:59:27.453
183209,56580,20190.0,3,,CC BY-SA 3.0,328921c4-74fc-406e-a600-f30d2a86f8ee,<regression><svm><prediction>,,2013-10-01 18:59:27.453
183357,56580,20190.0,5,,CC BY-SA 3.0,20ce087a-37e6-42fb-b53c-6f878030e573,"Why log transformation may improve results of svm prediction(regression)? Does svm based on assumption of normal distribution or something else?

update1. I use Radial basis function kernel.",update1,2013-10-02 07:18:28.333
183372,56580,20190.0,5,,CC BY-SA 3.0,5417088c-ac32-4c5d-8504-32b80be14cce,"Why log(natural logarithm) transformation may improve results of svm prediction(**regression**, eps-svm)? Does svm based on assumption of normal distribution or something else?

update1. I use Radial basis function kernel.",added 32 characters in body,2013-10-02 08:21:38.333
183373,56580,20190.0,4,,CC BY-SA 3.0,49eabf32-78ec-4140-94c4-8e463af06d28,Support Vector Machine(SVM) and log transformation,edited title,2013-10-02 08:37:02.353
183375,56580,20470.0,5,,CC BY-SA 3.0,b76257e8-d3ce-464f-970a-b2d80ba51234,"Why may log(natural logarithm) transformation improve results of SVM prediction(**regression**, eps-svm)? Is SVM based on the assumption of normal distribution or something else?

update1. I use Radial basis function kernel.",grammar / typo,2013-10-02 08:49:37.283
183374,56580,,24,,CC BY-SA 3.0,b76257e8-d3ce-464f-970a-b2d80ba51234,,Proposed by 28740 approved by 930 edit id of 5527,2013-10-02 08:49:37.283
183580,56684,,2,Ben,CC BY-SA 3.0,e6d624b4-359d-4fbe-805e-caab3eebd6ac,"I am working with two highly skewed Bernoulli distributions where 96-99+% of the samples are in the ""false"" category, and the rest are in the ""true"" category (sort of speak). I am looking for a two-sided test of difference of proportions between the two samples. I can often achieve 500+ ""trues"" and tens or hundreds of thousands of ""falses"" in a reasonable time but I'm not sure if approximation to the normal distribution can withstand this extreme skewness.

I initially thought I might need something non-parametric, but here, I actually know the distribution.

I have been using a student's t-test, while paying attention to sample size estimation, but past experience has led me to be skeptical of its results. Thanks for your help.",,2013-10-02 21:14:55.710
183579,56684,,3,Ben,CC BY-SA 3.0,e6d624b4-359d-4fbe-805e-caab3eebd6ac,<normal-distribution>,,2013-10-02 21:14:55.710
183578,56684,,1,Ben,CC BY-SA 3.0,e6d624b4-359d-4fbe-805e-caab3eebd6ac,Significance test for highly skewed Bernoulli distribution,,2013-10-02 21:14:55.710
183882,56768,11490.0,3,,CC BY-SA 3.0,b500505a-003b-49b8-ad92-3e6859863a67,<time-series><logistic>,,2013-10-03 21:13:50.837
183883,56768,11490.0,2,,CC BY-SA 3.0,b500505a-003b-49b8-ad92-3e6859863a67,"Suppose we have yearly data representing the market share of three companies,
say A, B and C. In other words, we have observations:

$$
 A_t, \; B_t \;\; \text{and} \;\; C_t \;\; \text{where} \; \; A_t+B_t+C_t = 1  
$$
for $t = 1, \dots,T$.

Suppose that in year $t$ the market share of company A has changed by $\Delta A_t = A_t - A_{t-1}$. Is there any way of estimating how that change can be sub-divided into market share lost to or acquired from companies B and C? My actual problem includes 5 companies, but I guess that the solution shouldn't change too much.  
Thanks!",,2013-10-03 21:13:50.837
183881,56768,11490.0,1,,CC BY-SA 3.0,b500505a-003b-49b8-ad92-3e6859863a67,Estimating hidden transfers of market share,,2013-10-03 21:13:50.837
183925,56780,15280.0,1,,CC BY-SA 3.0,9751f311-a012-4d08-9982-678aaa929071,Problem with proof of Conditional expectation as best predictor,,2013-10-04 00:24:13.043
183923,56780,15280.0,3,,CC BY-SA 3.0,9751f311-a012-4d08-9982-678aaa929071,<mathematical-statistics><conditional-probability><proof><conditional-expectation>,,2013-10-04 00:24:13.043
184930,57053,5237.0,5,,CC BY-SA 3.0,801ef2d3-22c6-4d65-94b9-f6989f650e9e,"How can I describe descriptive statistics for a dummy variable (gender of worker in a shop)? Let's say this is the info that I have: 

    mean :         0.47
    median :       0
    max :          1
    min :          0
    std. dev :     0.4998
    skewness :     0.101
    kurtosis :     1.01
    jarque bera : 85.67
    probability :  0

I know that some of the information is useless since it's a dummy variable. So how do I interpret it in words?",clarified issue in title; added tags; edited for English; formatted,2013-10-08 13:09:13.483
183924,56780,15280.0,2,,CC BY-SA 3.0,9751f311-a012-4d08-9982-678aaa929071,"I have an issue with the proof of

>> $E(Y|X) \in \arg \min_{g(x)} E\Big[\big(Y - g(x)\big)^2\Big]$

which very likely reveal a deeper misunderstanding of expectations and conditional expectations.

The proof I know goes as follows ( another version of this proof can be found [here][1])

\begin{align*}
&\arg \min_{g(x)} E\Big[\big(Y - g(x)\big)^2\Big]\\
 = &\arg \min_{g(x)} E \Big[ \big(Y - E(Y|X) + E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ \big(Y - E(Y|X)\big)^2 - 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
\end{align*}

The proof then typically continues with an argument showing that $-2 E\Big[ \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big)\Big] = 0$, and hence

\begin{align*}
\arg \min_{g(x)} E\Big[\big(Y - g(x)\big)^2\Big] = \arg \min_{g(x)} E \Big[\big(E(Y|X)  - g(X)\big)^2\Big]
\end{align*}

which can be seen to be minimized when $g(X) = E(Y|X)$.

My puzzles about the proof are the following:

 1. Consider

>> $E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]$.

 It seems to me that, independently of any argument showing that the first term is always equal to zero, one can see that setting $g(X) = E(Y|X)$ minimizes the expression as it implies $\big(E(Y|X)  - g(X)\big) =0$ and hence

>> $E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big] = E( 0 + 0)$ = 0.

But if this is true, then the one might repeat the proof replacing $E(Y|X)$ by any other function of $x$, say $h(x)$, and get to the conclusion that it is $h(x)$ that minimizes the expression. So there must be something I misunderstand (right?).

2. I have some doubts about the meaning of $E[(Y−g(x))^2]$ in the statement of the problem. How should the notation be interpreted? Does it mean

>>$E_X[(Y−g(x))^2]$, $E_Y[(Y−g(x))^2]$ or $E_{XY}[(Y−g(x))^2]$?

  [1]: http://www.econ.uiuc.edu/~wsosa/econ507/CEF.pdf",,2013-10-04 00:24:13.043
183932,56783,20473.0,2,,CC BY-SA 3.0,01d25739-5b2a-425a-8433-3c6a82e5bec7,"*(This is an adaptation from Granger & Newbold(1986) ""Forecasting Economic Time Series"").*  

By construction, your _error cost function_ is $\left[Y-g(X)\right]^2$. This incorporates a critical assumption (that the error cost function is symmetric around zero) -a different error cost function would not necessarily have the conditional expected value as the $\arg \min$ of its expected value.
You cannot minimize your error cost function because it contains unknown quantities. So you decide to minimize its expected value instead. Then your objective function becomes

$$E\left[Y-g(X)\right]^2 = \int_{-\infty}^{\infty}\left[y-g(X)\right]^2f_{Y|X}(y|x)dy $$

which I believe answers also your second question. It is intuitive that the expected value will be of $Y$ conditional on $X$, since we are trying to estimate/forecast $Y$ based on $X$. Decompose the square to obtain

$$E\left[Y-g(X)\right]^2 = \int_{-\infty}^{\infty}y^2f_{Y|X}(y|x)dy  -2g(X)\int_{-\infty}^{\infty}yf_{Y|X}(y|x)dy \\+ \Big[g(X)\Big]^2\int_{-\infty}^{\infty}f_{Y|X}(y|x)dy$$

The first term does not contain $g(X)$ so it does not affect minimization, and it can be ignored. The integral in the second term equals the conditional expected value of $Y$ given $X$, and the integral in the last term equals unity. So 

$$\arg \min_{g(x)} E\left[Y-g(X)\right]^2 = \arg \min_{g(x)} \Big\{ -2g(X)E(Y\mid X) + \Big[g(X)\Big]^2 \Big\}$$

The first derivative w.r.t $g(X)$ is $-2E(Y\mid X) + 2g(X)$ leading to the first order condition for minimization $g(X) = E(Y\mid X)$ while the second derivative is equal to $2>0$ which is sufficient for a minimum.",,2013-10-04 01:05:36.887
183937,56784,594.0,2,,CC BY-SA 3.0,6f674147-8b56-4cd3-ac40-a5939190d812,"Leaving aside the obvious issue of the low power of the chi-square in this sort of circumstance, imagine doing a chi-square test for some density with unspecified parameters.

For concreteness, let's say an exponential distribution with unknown mean and a sample size of say 100. 

In order to get a reasonable number of expected observations per bin some account would need to be taken of the data (e.g. if we chose to put 6 bins below the mean and 4 above it, that would still be using data-based bin boundaries). 

But this would presumably affect the distribution of the test statistic under the null. 

I have seen plenty of discussion about the fact that - *if* the parameters are estimated by maximum likelihood from the *binned* data you lose 1 d.f per estimated parameter (an issue dating right back to Fisher vs Karl Pearson) - but I don't recall reading anything about finding the bin boundaries themselves based on the data. (If you estimate them from the unbinned data, then with $k$ bins the distribution of the test statistic lies somewhere between a $\chi^2_{k}$ and a $\chi^2_{k-p}$.) 

Does this data-based choice of bins substantively impact significance level or power? Are there some approaches that matter more than others? If there is much of an effect, is it something that goes away in large samples?

If it does have a substantive impact, this would seem to make the use of a chi-squared test when parameters are unknown almost useless in many cases (in spite of still being advocated in quite a few texts), unless you had a good a-priori estimate of the parameter.

Discussion of the issues or pointers to references (preferably with a mention of their conclusions) would be useful.
",,2013-10-04 01:48:35.417
183938,56784,594.0,1,,CC BY-SA 3.0,6f674147-8b56-4cd3-ac40-a5939190d812,Impact of data-based bin boundaries on a chi-square goodness of fit test?,,2013-10-04 01:48:35.417
183939,56784,594.0,3,,CC BY-SA 3.0,6f674147-8b56-4cd3-ac40-a5939190d812,<chi-squared-test><binning>,,2013-10-04 01:48:35.417
183940,56784,594.0,5,,CC BY-SA 3.0,acdec6a2-daea-4796-82fb-5650a3030d52,"Leaving aside the obvious issue of the low power of the chi-square in this sort of circumstance, imagine doing a chi-square goodness of test for some density with unspecified parameters, by binning the data.

For concreteness, let's say an exponential distribution with unknown mean and a sample size of say 100. 

In order to get a reasonable number of expected observations per bin some account would need to be taken of the data (e.g. if we chose to put 6 bins below the mean and 4 above it, that would still be using data-based bin boundaries). 

But this use of bins based on seeing the data would presumably affect the distribution of the test statistic under the null. 

I have seen plenty of discussion about the fact that - *if* the parameters are estimated by maximum likelihood from the *binned* data - you lose 1 d.f per estimated parameter (an issue dating right back to Fisher vs Karl Pearson) - but I don't recall reading anything about finding the bin boundaries themselves based on the data. (If you estimate them from the unbinned data, then with $k$ bins the distribution of the test statistic lies somewhere between a $\chi^2_{k}$ and a $\chi^2_{k-p}$.) 

Does this data-based choice of bins substantively impact significance level or power? Are there some approaches that matter more than others? If there is much of an effect, is it something that goes away in large samples?

If it does have a substantive impact, this would seem to make the use of a chi-squared test when parameters are unknown almost useless in many cases (in spite of still being advocated in quite a few texts), unless you had a good a-priori estimate of the parameter.

Discussion of the issues or pointers to references (preferably with a mention of their conclusions) would be useful.
",added 2 characters in body,2013-10-04 01:58:21.100
184932,57053,5237.0,6,,CC BY-SA 3.0,801ef2d3-22c6-4d65-94b9-f6989f650e9e,<categorical-data><interpretation><descriptive-statistics><basic-concepts>,clarified issue in title; added tags; edited for English; formatted,2013-10-08 13:09:13.483
184952,57065,22477.0,2,,CC BY-SA 3.0,a3a40789-426a-4c30-967b-12e4a66c0c51,"We've run a split test of a new product feature and want to measure if the uplift on revenue is significant. Our observations are definitely not normally distributed (most of our users don't spend, and within those that do, it is heavily skewed towards lots of small spenders and a few very big spenders).

We've decided on using bootstrapping to compare the means, to get round the issue of the data not being normally distributed (side-question: is this a legitimate use of bootstrapping?)

My question is, do I need to trim outliers from the data set (eg the few very big spenders) before I run the bootstrapping, or does that not matter?

Any advice would be much appreciated,

Thanks,

Fred",,2013-10-08 14:28:42.660
183946,56784,594.0,5,,CC BY-SA 3.0,33db5069-1b24-44e3-b579-3efdc0ce01dc,"Leaving aside the obvious issue of the low power of the chi-square in this sort of circumstance, imagine doing a chi-square goodness of test for some density with unspecified parameters, by binning the data.

For concreteness, let's say an exponential distribution with unknown mean and a sample size of say 100. 

In order to get a reasonable number of expected observations per bin some account would need to be taken of the data (e.g. if we chose to put 6 bins below the mean and 4 above it, that would still be using data-based bin boundaries). 

But this use of bins based on seeing the data would presumably affect the distribution of the test statistic under the null. 

I have seen plenty of discussion about the fact that - *if* the parameters are estimated by maximum likelihood from the *binned* data - you lose 1 d.f per estimated parameter (an issue dating right back to Fisher vs Karl Pearson) - but I don't recall reading anything about finding the bin boundaries themselves based on the data. (If you estimate them from the unbinned data, then with $k$ bins the distribution of the test statistic lies somewhere between a $\chi^2_{k}$ and a $\chi^2_{k-p}$.) 

Does this data-based choice of bins substantively impact significance level or power? Are there some approaches that matter more than others? If there is much of an effect, is it something that goes away in large samples?

If it does have a substantive impact, this would seem to make the use of a chi-squared test when parameters are unknown almost useless in many cases (in spite of still being advocated in quite a few texts), unless you had a good a-priori estimate of the parameter.

Discussion of the issues or pointers to references (preferably with a mention of their conclusions) would be useful.

Edit, pretty much an aside to the main question:

It occurs to me that there are potential solutions for the specific case of the exponential\* (and the uniform come to think of it), but I am still interested in the more general issue of the impact choosing bin boundaries.

\* For example, for the exponential, one might use the smallest observation (say it is equal to $m$) to get a very rough idea of where to place the bins (since the smallest observation is exponential with mean $\mu/n$), and then test the remaining $n-1$ differences ($x_i - m$) for exponentiality. Of course that might yield a very poor estimate of $\mu$, and hence poor bin choices, though I suppose one might use the argument recursively in order to take the lowest two or three observations from which to choose reasonable bins and then test the differences above the largest for exponentiality)
",added 881 characters in body,2013-10-04 02:51:16.323
183954,56780,,25,,,542001ad-303c-429f-b511-8ce7900c7370,,http://twitter.com/#!/StackStats/status/385964605274865664,2013-10-04 03:08:27.487
183961,56780,15280.0,5,,CC BY-SA 3.0,6af01165-7cff-4a55-af1b-21089a5f836d,"I have an issue with the proof of

>> $E(Y|X) \in \arg \min_{g(X)} E\Big[\big(Y - g(X)\big)^2\Big]$

which very likely reveal a deeper misunderstanding of expectations and conditional expectations.

The proof I know goes as follows ( another version of this proof can be found [here][1])

\begin{align*}
&\arg \min_{g(X)} E\Big[\big(Y - g(x)\big)^2\Big]\\
 = &\arg \min_{g(X)} E \Big[ \big(Y - E(Y|X) + E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ \big(Y - E(Y|X)\big)^2 - 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
\end{align*}

The proof then typically continues with an argument showing that $-2 E\Big[ \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big)\Big] = 0$, and hence

\begin{align*}
\arg \min_{g(x)} E\Big[\big(Y - g(x)\big)^2\Big] = \arg \min_{g(x)} E \Big[\big(E(Y|X)  - g(X)\big)^2\Big]
\end{align*}

which can be seen to be minimized when $g(X) = E(Y|X)$.

My puzzles about the proof are the following:

 1. Consider

>> $E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]$.

 It seems to me that, independently of any argument showing that the first term is always equal to zero, one can see that setting $g(X) = E(Y|X)$ minimizes the expression as it implies $\big(E(Y|X)  - g(X)\big) =0$ and hence

>> $E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big] = E( 0 + 0)$ = 0.

But if this is true, then the one might repeat the proof replacing $E(Y|X)$ by any other function of $X$, say $h(X)$, and get to the conclusion that it is $h(X)$ that minimizes the expression. So there must be something I misunderstand (right?).

2. I have some doubts about the meaning of $E[(Y−g(X))^2]$ in the statement of the problem. How should the notation be interpreted? Does it mean

>>$E_X[(Y−g(X))^2]$, $E_Y[(Y−g(X))^2]$ or $E_{XY}[(Y−g(X))^2]$?

  [1]: http://www.econ.uiuc.edu/~wsosa/econ507/CEF.pdf",capitalize X's,2013-10-04 03:53:34.993
183994,56784,594.0,6,,CC BY-SA 3.0,e45e6c72-4470-4329-9aad-033a3ab39ac3,<chi-squared-test><goodness-of-fit><binning>,edited tags,2013-10-04 07:28:52.303
184026,56784,594.0,5,,CC BY-SA 3.0,371731ce-884c-4dae-95ba-ed7fc482d1fb,"Leaving aside the obvious issue of the low power of the chi-square in this sort of circumstance, imagine doing a chi-square goodness of test for some density with unspecified parameters, by binning the data.

For concreteness, let's say an exponential distribution with unknown mean and a sample size of say 100. 

In order to get a reasonable number of expected observations per bin some account would need to be taken of the data (e.g. if we chose to put 6 bins below the mean and 4 above it, that would still be using data-based bin boundaries). 

But this use of bins based on seeing the data would presumably affect the distribution of the test statistic under the null. 

I have seen plenty of discussion about the fact that - *if* the parameters are estimated by maximum likelihood from the *binned* data - you lose 1 d.f per estimated parameter (an issue dating right back to Fisher vs Karl Pearson) - but I don't recall reading anything about finding the bin boundaries themselves based on the data. (If you estimate them from the unbinned data, then with $k$ bins the distribution of the test statistic lies somewhere between a $\chi^2_{k}$ and a $\chi^2_{k-p}$.) 

Does this data-based choice of bins substantively impact significance level or power? Are there some approaches that matter more than others? If there is much of an effect, is it something that goes away in large samples?

If it does have a substantive impact, this would seem to make the use of a chi-squared test when parameters are unknown almost useless in many cases (in spite of still being advocated in quite a few texts), unless you had a good a-priori estimate of the parameter.

Discussion of the issues or pointers to references (preferably with a mention of their conclusions) would be useful.

---

Edit, pretty much an aside to the main question:

It occurs to me that there are potential solutions for the specific case of the exponential\* (and the uniform come to think of it), but I am still interested in the more general issue of the impact choosing bin boundaries.

\* For example, for the exponential, one might use the smallest observation (say it is equal to $m$) to get a very rough idea of where to place the bins (since the smallest observation is exponential with mean $\mu/n$), and then test the remaining $n-1$ differences ($x_i - m$) for exponentiality. Of course that might yield a very poor estimate of $\mu$, and hence poor bin choices, though I suppose one might use the argument recursively in order to take the lowest two or three observations from which to choose reasonable bins and then test the differences above the largest for exponentiality)
",edited tags,2013-10-04 10:18:14.337
184027,56783,20473.0,5,,CC BY-SA 3.0,c6c8a449-b83c-4c81-9fa6-9bb6fc418ee7,"*(This is an adaptation from Granger & Newbold(1986) ""Forecasting Economic Time Series"").*  

By construction, your _error cost function_ is $\left[Y-g(X)\right]^2$. This incorporates a critical assumption (that the error cost function is symmetric around zero) -a different error cost function would not necessarily have the conditional expected value as the $\arg \min$ of its expected value.
You cannot minimize your error cost function because it contains unknown quantities. So you decide to minimize its expected value instead. Then your objective function becomes

$$E\left[Y-g(X)\right]^2 = \int_{-\infty}^{\infty}\left[y-g(X)\right]^2f_{Y|X}(y|x)dy $$

which I believe answers also your second question. It is intuitive that the expected value will be of $Y$ conditional on $X$, since we are trying to estimate/forecast $Y$ based on $X$. Decompose the square to obtain

$$E\left[Y-g(X)\right]^2 = \int_{-\infty}^{\infty}y^2f_{Y|X}(y|x)dy  -2g(X)\int_{-\infty}^{\infty}yf_{Y|X}(y|x)dy \\+ \Big[g(X)\Big]^2\int_{-\infty}^{\infty}f_{Y|X}(y|x)dy$$

The first term does not contain $g(X)$ so it does not affect minimization, and it can be ignored. The integral in the second term equals the conditional expected value of $Y$ given $X$, and the integral in the last term equals unity. So 

$$\arg \min_{g(x)} E\left[Y-g(X)\right]^2 = \arg \min_{g(x)} \Big\{ -2g(X)E(Y\mid X) + \Big[g(X)\Big]^2 \Big\}$$

The first derivative w.r.t $g(X)$ is $-2E(Y\mid X) + 2g(X)$ leading to the first order condition for minimization $g(X) = E(Y\mid X)$ while the second derivative is equal to $2>0$ which is sufficient for a minimum.

**ADDENDUM:The logic of the ""add and subtract"" proof approach.**  

The OP is puzzled by the approach stated in the question, because it seems tautological. It isn't, because while using the tactic of adding and subtracting makes a _specific part_ of the objective function zero for an arbitrary choice of the term that is added and subtracted, it does NOT equalize the _value function_ , namely the value of the objective function evaluated at the candidate minimizer.

For the choice $g(X) = E(Y \mid X)$ we have the value function $ V\left(E(Y\mid X)\right) = E\Big[ (Y-E(Y \mid X))^2\mid X\Big]$
For the arbitrary choice $g(X) = h(X)$we have the value funtion $ V\left(h(X)\right) = E\Big[ (Y-h(X))^2\mid X\Big]$.

I claim that 

$$V\left(E(Y\mid X)\right) \le V\left(h(X)\right)$$
$$\Rightarrow E(Y^2\mid X) -2E\Big [(YE(Y \mid X))\mid X\Big] + E\Big [(E(Y \mid X))^2\mid X\Big] \\\le E(Y^2\mid X) -2E\Big [(Yh(X))\mid X\Big] + E\Big [(h(X))^2\mid X\Big]$$

The first term of the LHS and the RHS cancel out. Also note that the outer expectation is conditional on $X$. By the properties of conditional expectations we end up with

$$...\Rightarrow  -2E(Y \mid X)\cdot E\Big (Y\mid X\Big) + \Big [E(Y \mid X)\Big]^2 \le  -2E(Y\mid X)h(X) + \Big [h(X)\Big]^2$$

$$\Rightarrow  0 \le  \Big [E(Y \mid X)\Big]^2-2E(Y\mid X)h(X) + \Big [h(X)\Big]^2$$

$$\Rightarrow  0 \le  \Big [E(Y \mid X) - h(x)\Big]^2$$
which holds with strict inequality if $h(x) \neq E(Y \mid X)$. So $E(Y \mid X)$ is the global and unique minimizer.

But this also says that the ""add-and-subtract"" approach is not the most illuminating way of proof here.

",Added an explanation for  the proof indicated by the OP,2013-10-04 10:26:39.823
184954,57065,22477.0,1,,CC BY-SA 3.0,a3a40789-426a-4c30-967b-12e4a66c0c51,Bootstrapping - do I need to remove outliers first?,,2013-10-08 14:28:42.660
184953,57065,22477.0,3,,CC BY-SA 3.0,a3a40789-426a-4c30-967b-12e4a66c0c51,<outliers>,,2013-10-08 14:28:42.660
184968,57065,,25,,,342b7d4b-5d7c-4cda-b681-40c4baad3246,,http://twitter.com/#!/StackStats/status/387596378970808321,2013-10-08 15:12:32.667
185716,57284,,1,user14650,CC BY-SA 3.0,d0e821d4-743c-40ef-83e9-66dad2846efa,How to set confidence level for wilcoxsign_test (package coin)?,,2013-10-11 09:06:48.623
185940,57349,22630.0,4,,CC BY-SA 3.0,72b6caf7-36bf-4b95-acb6-e41fc8c61312,"What does ""Mean of each pixel over all images"" mean?",Added more details,2013-10-12 11:51:10.477
184043,56784,594.0,5,,CC BY-SA 3.0,dbca9084-7715-41a6-8d29-6e4b896051ab,"Leaving aside the obvious issue of the low power of the chi-square in this sort of circumstance, imagine doing a chi-square goodness of test for some density with unspecified parameters, by binning the data.

For concreteness, let's say an exponential distribution with unknown mean and a sample size of say 100. 

In order to get a reasonable number of expected observations per bin some account would need to be taken of the data (e.g. if we chose to put 6 bins below the mean and 4 above it, that would still be using data-based bin boundaries). 

But this use of bins based on seeing the data would presumably affect the distribution of the test statistic under the null. 

I have seen plenty of discussion about the fact that - *if* the parameters are estimated by maximum likelihood from the *binned* data - you lose 1 d.f per estimated parameter (an issue dating right back to Fisher vs Karl Pearson) - but I don't recall reading anything about finding the bin boundaries themselves based on the data. (If you estimate them from the unbinned data, then with $k$ bins the distribution of the test statistic lies somewhere between a $\chi^2_{k}$ and a $\chi^2_{k-p}$.) 

Does this data-based choice of bins substantively impact significance level or power? Are there some approaches that matter more than others? If there is much of an effect, is it something that goes away in large samples?

If it does have a substantive impact, this would seem to make the use of a chi-squared test when parameters are unknown almost useless in many cases (in spite of still being advocated in quite a few texts), unless you had a good a-priori estimate of the parameter.

Discussion of the issues or pointers to references (preferably with a mention of their conclusions) would be useful.

---

Edit, pretty much an aside to the main question:

It occurs to me that there are potential solutions for the specific case of the exponential\* (and the uniform come to think of it), but I am still interested in the more general issue of the impact choosing bin boundaries.

\* For example, for the exponential, one might use the smallest observation (say it is equal to $m$) to get a very rough idea of where to place the bins (since the smallest observation is exponential with mean $\mu/n$), and then test the remaining $n-1$ differences ($x_i - m$) for exponentiality. Of course that might yield a very poor estimate of $\mu$, and hence poor bin choices, though I suppose one might use the argument recursively in order to take the lowest two or three observations from which to choose reasonable bins and then test the differences of the remaining observations above the largest of those smallest order statistics for exponentiality)
",added 65 characters in body,2013-10-04 13:09:48.050
184047,56780,15280.0,5,,CC BY-SA 3.0,b578444c-ed0f-4f43-8e70-a55e8eff0531,"I have an issue with the proof of

>> $E(Y|X) \in \arg \min_{g(X)} E\Big[\big(Y - g(X)\big)^2\Big]$

which very likely reveal a deeper misunderstanding of expectations and conditional expectations.

The proof I know goes as follows ( another version of this proof can be found [here][1])

\begin{align*}
&\arg \min_{g(X)} E\Big[\big(Y - g(x)\big)^2\Big]\\
 = &\arg \min_{g(X)} E \Big[ \big(Y - E(Y|X) + E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ \big(Y - E(Y|X)\big)^2 - 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
\end{align*}

The proof then typically continues with an argument showing that $-2 E\Big[ \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big)\Big] = 0$, and hence

\begin{align*}
\arg \min_{g(x)} E\Big[\big(Y - g(x)\big)^2\Big] = \arg \min_{g(x)} E \Big[\big(E(Y|X)  - g(X)\big)^2\Big]
\end{align*}

which can be seen to be minimized when $g(X) = E(Y|X)$.

My puzzles about the proof are the following:

 1. Consider

>> $E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]$.

 It seems to me that, independently of any argument showing that the first term is always equal to zero, one can see that setting $g(X) = E(Y|X)$ minimizes the expression as it implies $\big(E(Y|X)  - g(X)\big) =0$ and hence

>> $E \Big[ -2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big] = E( 0 + 0)$ = 0.

But if this is true, then one might repeat the proof replacing $E(Y|X)$ by any other function of $X$, say $h(X)$, and get to the conclusion that it is $h(X)$ that minimizes the expression. So there must be something I misunderstand (right?).

2. I have some doubts about the meaning of $E[(Y−g(X))^2]$ in the statement of the problem. How should the notation be interpreted? Does it mean

>>$E_X[(Y−g(X))^2]$, $E_Y[(Y−g(X))^2]$ or $E_{XY}[(Y−g(X))^2]$?

  [1]: http://www.econ.uiuc.edu/~wsosa/econ507/CEF.pdf",deleted 4 characters in body,2013-10-04 14:03:01.287
184155,56780,15280.0,5,,CC BY-SA 3.0,30315ba8-73f9-4496-97dc-99aa5aace307,"I have an issue with the proof of

>> $E(Y|X) \in \arg \min_{g(X)} E\Big[\big(Y - g(X)\big)^2\Big]$

which very likely reveal a deeper misunderstanding of expectations and conditional expectations.

The proof I know goes as follows ( another version of this proof can be found [here][1])

\begin{align*}
&\arg \min_{g(X)} E\Big[\big(Y - g(x)\big)^2\Big]\\
 = &\arg \min_{g(X)} E \Big[ \big(Y - E(Y|X) + E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ \big(Y - E(Y|X)\big)^2 + 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
=&\arg \min_{g(x)} E \Big[ 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
\end{align*}

The proof then typically continues with an argument showing that $2 E\Big[ \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big)\Big] = 0$, and hence

\begin{align*}
\arg \min_{g(x)} E\Big[\big(Y - g(x)\big)^2\Big] = \arg \min_{g(x)} E \Big[\big(E(Y|X)  - g(X)\big)^2\Big]
\end{align*}

which can be seen to be minimized when $g(X) = E(Y|X)$.

My puzzles about the proof are the following:

 1. Consider

>> $E \Big[ 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]$.

 It seems to me that, independently of any argument showing that the first term is always equal to zero, one can see that setting $g(X) = E(Y|X)$ minimizes the expression as it implies $\big(E(Y|X)  - g(X)\big) =0$ and hence

>> $E \Big[ 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big] = E( 0 + 0)$ = 0.

But if this is true, then one might repeat the proof replacing $E(Y|X)$ by any other function of $X$, say $h(X)$, and get to the conclusion that it is $h(X)$ that minimizes the expression. So there must be something I misunderstand (right?).

2. I have some doubts about the meaning of $E[(Y−g(X))^2]$ in the statement of the problem. How should the notation be interpreted? Does it mean

>>$E_X[(Y−g(X))^2]$, $E_Y[(Y−g(X))^2]$ or $E_{XY}[(Y−g(X))^2]$?

  [1]: http://www.econ.uiuc.edu/~wsosa/econ507/CEF.pdf",sign error,2013-10-04 23:15:58.010
184194,56859,6805.0,1,,CC BY-SA 3.0,bee0fd00-b9fd-4418-a516-c201cc38df00,"What are descriptive and inferential statistics, and how do they differ?",,2013-10-05 04:59:21.093
184193,56859,6805.0,3,,CC BY-SA 3.0,bee0fd00-b9fd-4418-a516-c201cc38df00,<descriptive-statistics><inference>,,2013-10-05 04:59:21.093
184195,56859,6805.0,2,,CC BY-SA 3.0,bee0fd00-b9fd-4418-a516-c201cc38df00,"My understanding was that descriptive statistics quantitatively described features of a data sample, while inferential statistics made inferences about the populations from which samples were drawn.

However, the [wikipedia page for statistical inference][1] states:

> For the most part, statistical inference makes propositions about
> populations, using data drawn from the population of interest via some
> form of random sampling.

The ""for the most part"" has thrown me. Are there examples of inferential statistics that don't make propositions about populations?


  [1]: http://en.wikipedia.org/wiki/Statistical_inference",,2013-10-05 04:59:21.093
184197,56859,6805.0,5,,CC BY-SA 3.0,69619d75-9e53-46c4-bf88-749a35faec51,"My understanding was that descriptive statistics quantitatively described features of a data sample, while inferential statistics made inferences about the populations from which samples were drawn.

However, the [wikipedia page for statistical inference][1] states:

> For the most part, statistical inference makes propositions about
> populations, using data drawn from the population of interest via some
> form of random sampling.

The ""for the most part"" has made me think I perhaps don't properly understand these concepts. Are there examples of inferential statistics that don't make propositions about populations?


  [1]: http://en.wikipedia.org/wiki/Statistical_inference",added 55 characters in body,2013-10-05 05:30:28.103
184199,56860,155.0,2,,CC BY-SA 3.0,391fdd35-ce53-4ab1-8ba9-afbd94e441a7,"Coming from a behavioural sciences background, I associate this terminology particularly with introductory statistics textbooks. In this context the distinction is that :

* **Descriptive statistics**  are functions of  the sample data that are intrinsically interesting in describing some feature of the data. Classic descriptive statistics include mean, min, max, standard deviation, median, skew, kurtosis.
* **Inferential statistics** are a function of the sample data that assists you to draw an inference regarding an hypothesis about a population parameter. Classic inferential statistics include z, t, $\chi^2$, F-ratio, etc.

The important point is that any statistic, inferential or descriptive, is a function of the sample data. A parameter is a function of the population, where the term population is the same as saying the underlying data generating process.

From this perspective the status of a given function of the data as a descriptive or inferential statistic depends on the purpose for which you are using it. 

That said, some statistics are clearly more useful in describing  relevant features of the data, and some are well suited to aiding inference. 

* **Inferential statistics:**  Standard test statistics like t and z, for a given data generating process, where the null hypothesis is false, the expected value is strongly influenced by sample size. Most researchers would not see such statistics as estimating a population parameter of intrinsic interest.
* **Descriptive statistics**: In contrast descriptive statistics do estimate population parameters that are typically of intrinsic interest. For example the sample mean and standard deviation provide estimates of the equivalent population parameters. Even descriptive statistics like the minimum and maximum provide information about equivalent or similar population parameters, although of course in this case, much more care is required. Furthermore, many descriptive statistics might be biased or otherwise less than ideal estimators. However, they still have some utility in estimating a population parameter of interest.

So from this perspective, the important things to understand are:

* **statistic**: function of the sample data
* **parameter**: function of the population (data generating process)
* **estimator**: function of the sample data used to provide an estimate of a parameter
* **inference**: process of reaching a conclusion about a parameter

Thus, you could either define the distinction between descriptive and inferential based on the intention of the researcher using the statistic, or you could define a statistic based on how it is typically used.
",,2013-10-05 05:51:35.693
184200,56859,155.0,4,,CC BY-SA 3.0,c9516594-9ccd-488f-8fc3-414b330e5282,What is the difference between descriptive and inferential statistics?,edited tags; edited title,2013-10-05 05:52:04.663
184253,56875,947.0,2,,CC BY-SA 3.0,2c419e31-10fa-4d70-bf04-ab6eb4cea370,"I have become aware that a group at a large corporation is developing an econometrics model to forecast sales of their product.  They are using this model solely to estimate sales in specified stress test economic scenarios where they are given what the economic environment will be like, including real GDP contraction, rising unemployment rate, etc...  Because of the nature of those scenarios, they think the most proper way to construct this model is to focus solely on the 2008-2009 period capturing the main period of the recent financial crisis.  They have monthly data, so that gives them 24 monthly data points.  Given that GDP's frequency is really quarterly, on this one variable it gives them only 8 true datapoints.  But, they extrapolate it into 24 month observation. 

For the record, if they chose to, they have good internal data going back to 2001 and up to the current period.  But, as mentioned they decided to focus instead solely on the 2008-2009 period.  

I will also answer this question as I have built many such econometrics models.  And, I invite others to debate and rebutt my answer... and to post your own better answer. ",,2013-10-05 14:58:52.307
184252,56875,947.0,1,,CC BY-SA 3.0,2c419e31-10fa-4d70-bf04-ab6eb4cea370,Can you develop an econometrics model for stress test purpose only focusing on 2008-2009 data?,,2013-10-05 14:58:52.307
184251,56875,947.0,3,,CC BY-SA 3.0,2c419e31-10fa-4d70-bf04-ab6eb4cea370,<time-series><econometrics>,,2013-10-05 14:58:52.307
184258,56875,947.0,5,,CC BY-SA 3.0,dc8c2207-9257-4949-a24c-4ea93f23b76a,"I have become aware that a group at a large corporation is developing an econometrics model to forecast sales of their product.  They are using this model solely to estimate sales in specified stress test economic scenarios where they are given what the economic environment will be like, including real GDP contraction, rising unemployment rate, etc... out to 2016.  Because of the nature of those scenarios, they think the most proper way to construct this model is to focus solely on the 2008-2009 period capturing the main period of the recent financial crisis.  They have monthly data, so that gives them 24 monthly data points.  Given that GDP's frequency is really quarterly, on this one variable it gives them only 8 true datapoints.  But, they extrapolate it into 24 month observations. 

For the record, if they chose to, they have good internal data going back to 2001 and up to the current period.  But, as mentioned they decided to focus instead solely on the 2008-2009 period.  

I will also answer this question as I have built many such econometrics models.  And, I invite others to debate and rebutt my answer... and to post your own better answer. ",added 14 characters in body,2013-10-05 15:12:25.113
184283,56684,0.0,36,,,edb576dd-3c9c-4173-b1af-d509360e3ca2,,from http://math.stackexchange.com/questions/512624/significance-test-for-highly-skewed-bernoulli-distribution,2013-10-05 16:31:40.173
184376,56911,1506.0,1,,CC BY-SA 3.0,57f47e46-f22c-432c-9d09-e3068f7eee00,Detect Pattern in Residual Plot,,2013-10-06 06:53:48.957
184377,56911,1506.0,2,,CC BY-SA 3.0,57f47e46-f22c-432c-9d09-e3068f7eee00,"I wish to automatically (not by visual inspection) detect where large deviations occur in a residual plot from a regression. For example, suppose I have the residual plot below:


![enter image description here][1]


I want to automatically detect the observations from about 30:35 deviate from a normal residual pattern.  Some clues are that the magnitude is quite large and the residuals do not appear independent in this region.  How can I go about this?


  [1]: https://i.stack.imgur.com/IWgZV.png",,2013-10-06 06:53:48.957
184378,56911,1506.0,3,,CC BY-SA 3.0,57f47e46-f22c-432c-9d09-e3068f7eee00,<regression><residuals>,,2013-10-06 06:53:48.957
184435,56911,,4,user88,CC BY-SA 3.0,89e85066-fb9c-459e-a134-57d51b5f56b8,Detecting patterns in residual plot,edited title,2013-10-06 13:08:49.547
184444,56928,16046.0,1,,CC BY-SA 3.0,81814c07-2601-49e8-bb84-cfcf1a73f3d4,Reference for Hierarchical Bayesian Modelling,,2013-10-06 15:21:17.703
184442,56928,16046.0,3,,CC BY-SA 3.0,81814c07-2601-49e8-bb84-cfcf1a73f3d4,<references><hierarchical-bayesian>,,2013-10-06 15:21:17.703
184443,56928,16046.0,2,,CC BY-SA 3.0,81814c07-2601-49e8-bb84-cfcf1a73f3d4,"I am currently reading ""Bayesian Data Analysis"" by Gelman et. al. and my main goal was to learn about Hierarchical modeling on chapter 5. I read until chapter 4 and the book is written terribly for a taste of a math student as it is pretty sketchy and engineering oriented. 

I decided to not to continue anymore with this book and I would be very grateful if somebody could introduce a reference with a more rigorous approach to the topic.

",,2013-10-06 15:21:17.703
184462,56768,11490.0,33,,,95a9c40e-d2f7-4519-85cc-2621ef94cbbb,,806,2013-10-06 17:15:36.623
184469,56768,,25,,,cedb0df9-a14d-448f-a400-5fce1cb28076,,http://twitter.com/#!/StackStats/status/386916228692209664,2013-10-06 18:09:52.167
184474,56928,,4,user88,CC BY-SA 3.0,7be19069-5d02-4fed-ae65-39df9fdd61b5,Reference for hierarchical Bayesian modelling,added 1 characters in body; edited title,2013-10-06 18:22:25.083
184475,56928,,5,user88,CC BY-SA 3.0,7be19069-5d02-4fed-ae65-39df9fdd61b5,"I am currently reading ""Bayesian Data Analysis"" by Gelman et. al. and my main goal was to learn about Hierarchical modelling on chapter 5. I read until chapter 4 and the book is written terribly for a taste of a math student as it is pretty sketchy and engineering oriented. 

I decided to not to continue anymore with this book and I would be very grateful if somebody could introduce a reference with a more rigorous approach to the topic.

",added 1 characters in body; edited title,2013-10-06 18:22:25.083
184516,56928,15827.0,5,,CC BY-SA 3.0,c3f47328-186d-41a2-be2f-63b0e6391849,"I am currently reading ""Bayesian Data Analysis"" by Gelman et al. and my main goal was to learn about Hierarchical modelling on chapter 5. I read until chapter 4 and the book is written terribly for a taste of a math student as it is pretty sketchy and engineering oriented. 

I decided to not to continue anymore with this book and I would be very grateful if somebody could introduce a reference with a more rigorous approach to the topic.

",deleted 1 characters in body,2013-10-06 22:45:52.387
184545,56955,22423.0,1,,CC BY-SA 3.0,7db1e6d2-ce4a-4628-a8d2-b6e92221d5da,How to combine data from 5 surveys from the same population spanning 10 years,,2013-10-07 02:44:46.310
184544,56955,22423.0,3,,CC BY-SA 3.0,7db1e6d2-ce4a-4628-a8d2-b6e92221d5da,<sampling><survey><meta-analysis><population><weighted-sampling>,,2013-10-07 02:44:46.310
184546,56955,22423.0,2,,CC BY-SA 3.0,7db1e6d2-ce4a-4628-a8d2-b6e92221d5da,"I have results from 5 surveys each 2 years apart and let us assume that no subjects are selected in more than one survey.

The sampling method used in these surveys are biased and I have sampling weights calculated(with respect to the population) for each data point in each study.

The question is, how would I be able to combine the 5 datasets and have the weights recalculated so as to obtain one giant dataset for analysis on this population?

Also, what should I do if subjects appear in more than one survey? ",,2013-10-07 02:44:46.310
185759,57297,18198.0,1,,CC BY-SA 3.0,b510d6c8-caec-4002-917b-283ff85acac8,Testing whether two Eigen decompositions are equal,,2013-10-11 12:13:44.087
185762,57287,,25,,,bc00f02f-2f08-4677-9bc9-86d3fe74cc67,,http://twitter.com/#!/StackStats/status/388639436827025408,2013-10-11 12:17:17.037
184573,56955,22423.0,5,,CC BY-SA 3.0,1b526a42-1226-40d3-8fb4-97f89adb779d,"I have results from 5 surveys each 2 years apart and let us assume that no subjects are selected in more than one survey.

The sampling method used in these surveys are biased and I have sampling weights calculated(with respect to the population) for each data point in each study.

The question is, how would I be able to combine the 5 datasets and have the weights recalculated so as to obtain one giant dataset for analysis on this population?

Also, what should I do if subjects appear in more than one survey? 

# Updates/Further Elaboration: #

thank you @user30523, here are some more infomation that might be useful:

Suppose I wish to find out the estimated distribution of height across the population using these 5 datasets. 

In some data, younger people are oversampled because of the location where the survey are conducted. Let's assume the weights are calculated with respect to their age. 

Eg. assuming 2% of the population are 15 years old, and the location of the survey is at a mall where 15-year-olds made up 5% of all shoppers, then sampling weight for an subject aged 15 in that survey would be calculated as 0.02 / 0.05 = 0.4. For simplicity, each person in the mall has equal chance of being surveyed and all participants complied when asked.

Given that 5 surveys are conducted in 5 different malls and each has their set of weights calculated in the same way, how would I then be able to combine all 5 datasets and recalculate the sampling weights?

P.S: I'm new to the topic on sampling weights so do correct me if I have made errors in the way I have calculated the weights.",added 1103 characters in body,2013-10-07 07:09:40.857
184600,56970,22372.0,2,,CC BY-SA 3.0,45ba5452-56ce-4511-b70d-da164b64e467,"I'm building a logit model using R and I'm getting a result of 88.9% of accuracy (verified using the ROC [in rattle, evaluation tab] using 30% of my 34k dataset).

What kind of tests would be interesting to do to certify myself that it's a good model?",,2013-10-07 09:44:35.967
184602,56970,22372.0,3,,CC BY-SA 3.0,45ba5452-56ce-4511-b70d-da164b64e467,<r><regression><hypothesis-testing><logit>,,2013-10-07 09:44:35.967
184601,56970,22372.0,1,,CC BY-SA 3.0,45ba5452-56ce-4511-b70d-da164b64e467,test a logit model in R,,2013-10-07 09:44:35.967
184606,56970,1406.0,4,,CC BY-SA 3.0,af8bf362-0d50-4621-ab43-e226a75a7361,How to test a logit model in R?,edited title,2013-10-07 09:48:27.890
184773,57012,20773.0,3,,CC BY-SA 3.0,e30f56bd-53f7-490c-817b-5e0a7924a4b7,<data-imputation><threshold><sensitivity>,,2013-10-07 20:36:21.153
184771,57012,20773.0,2,,CC BY-SA 3.0,e30f56bd-53f7-490c-817b-5e0a7924a4b7,"I have observations taken with different sensitivity thresholds and minimum detection levels, i.e. Lab A is less sensitive and has a minimum detection level of .2 and Lab B is more sensitive and has a minimum detection level of .02. Each row corresponds to a unique measurement taken by either lab:

    Obs | Lab A | Lab B
    ---------------------
     1  |  .6   |  NA
     2  |  0    |  NA
     3  |  NA   |  .53
     4  |  .2   |  NA
     5  |  NA   |  .07

I think I would like something like:

    Obs | LabA  | LabB  | NewLab
    ----------------------------
     1  |  .6   |  NA   |  .64
     2  |  0    |  NA   |  .13
     3  |  NA   |  .53  |  .53
     4  |  .2   |  NA   |  .21
     5  |  NA   |  .07  |  .07

What techniques are available to standardize the values such that there is not a large loss of information?

 1. Obviously, I could take the values from Lab B and replace anything less than .2 with 0 and then round them, but I want to avoid throwing away information if possible.
 2. One person suggested to add random noise to the values of Lab A, but I'm not sure of the benefit of this vs. simply imputing the missing values from Lab B.",,2013-10-07 20:36:21.153
184772,57012,20773.0,1,,CC BY-SA 3.0,e30f56bd-53f7-490c-817b-5e0a7924a4b7,How to compare different sensitivity thresholds and detection limits?,,2013-10-07 20:36:21.153
184780,57015,22454.0,3,,CC BY-SA 3.0,55b254fa-2a3d-4def-b47b-cedd6e7c6628,<hypothesis-testing><statistical-significance><p-value>,,2013-10-07 20:50:53.017
184782,57015,22454.0,1,,CC BY-SA 3.0,55b254fa-2a3d-4def-b47b-cedd6e7c6628,Testing statistical significance in two conditions,,2013-10-07 20:50:53.017
184781,57015,22454.0,2,,CC BY-SA 3.0,55b254fa-2a3d-4def-b47b-cedd6e7c6628,"I am measuring two variables $x$ and $y$ in two different conditions. In the first condition, my hypothesis is that $\bar{x} > \bar{y}$ and in the second condition that $\bar{x} < \bar{y}$. Now that I have $N$ samples from both variables, how can I test whether my hypotheses are true? I am not sure if I can safely assume that $x$ and $y$ are independent from each other. Neither do I know from what kind of distributions they are sampled from. The sample size I have is small. I have read several introductions to statistics for the past few days, but never saw a worked out example for this kind of situations. All help appreciated.",,2013-10-07 20:50:53.017
184821,57026,22458.0,3,,CC BY-SA 3.0,c3011751-2e16-4c3e-a7a6-c6e4c2944a16,<machine-learning><svm><kernel><hyperparameter>,,2013-10-08 01:16:28.460
184820,57026,22458.0,1,,CC BY-SA 3.0,c3011751-2e16-4c3e-a7a6-c6e4c2944a16,Relationship between the kernel and the value of C in SVM's,,2013-10-08 01:16:28.460
184819,57026,22458.0,2,,CC BY-SA 3.0,c3011751-2e16-4c3e-a7a6-c6e4c2944a16,"How exactly does the value of C relate across different kernels that we can use for SVM's? As in, how does it vary when changing the polynomial degree of a kernel or while using a gaussian kernel?",,2013-10-08 01:16:28.460
184825,57026,5237.0,5,,CC BY-SA 3.0,a89e8068-5550-4d3c-98b6-5d2560f574d5,"How exactly does the value of C relate across different kernels that we can use for SVM's? As in, how does it vary when changing the polynomial degree of a kernel or while using a Gaussian kernel?",light editing,2013-10-08 02:09:52.270
184832,57026,,25,,,b3edbe93-c571-483f-ae7d-61688b411ec7,,http://twitter.com/#!/StackStats/status/387415183624519680,2013-10-08 03:12:32.320
184922,57053,22475.0,2,,CC BY-SA 3.0,eae6a2b9-8dfe-4f27-8dd8-d5497599071c,"How did I describe descriptive statistics for dummy variable (gender of worker in a shop)? let say this is the info that I have: 

mean : 0.47
median : 0
max : 1
min : 0
std. dev : 0.4998
skewness : 0.101
kurtosis : 1.01
jarque bera : 85.67
probability : 0

i know that some of the informations are useless since it's a dummy variable. so how do i interpret it in words?",,2013-10-08 12:31:47.380
184920,57053,22475.0,3,,CC BY-SA 3.0,eae6a2b9-8dfe-4f27-8dd8-d5497599071c,<descriptive-statistics>,,2013-10-08 12:31:47.380
184921,57053,22475.0,1,,CC BY-SA 3.0,eae6a2b9-8dfe-4f27-8dd8-d5497599071c,Descriptive statistics,,2013-10-08 12:31:47.380
184929,57055,22.0,2,,CC BY-SA 3.0,9ac3a97b-0747-4081-b1e2-f55528d3bc26,"I think only one descriptive statistic is needed: ""47% are male"" (assuming 0 encodes female and 1 encodes male). No other statistics are really helpful to describe those data. If you thought these were a randomish sample of a larger population, you could compute the confidence interval for that proportion. 
",,2013-10-08 13:08:34.837
184931,57053,5237.0,4,,CC BY-SA 3.0,801ef2d3-22c6-4d65-94b9-f6989f650e9e,Interpretation of descriptive statistics for dummy variable,clarified issue in title; added tags; edited for English; formatted,2013-10-08 13:09:13.483
185951,57271,20473.0,5,,CC BY-SA 3.0,65103292-f647-47a7-8a42-77fba3010ccc,"Write your system explicitly for time $t$ as (""$L$"" for ""loss"", as a positive quantity, and ""$G$"" for ""gain"")
$$ A_t - A_{t-1} = - L^A_{t} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - L^B_{t} + G_{t-1}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - L^C_{t} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$

The following three relations hold exactly:
$$  L^A_{t} = G_{t}^{A\rightarrow B} +  G_{t}^{A\rightarrow C} $$
$$  L^B_{t} = G_{t}^{B\rightarrow A} +  G_{t}^{B\rightarrow C} $$
$$  L^C_{t} = G_{t}^{C\rightarrow A} +  G_{t}^{C\rightarrow B} $$

If you substitute in the first three you obtain

$$ A_t - A_{t-1} = - G_{t}^{A\rightarrow B} -  G_{t}^{A\rightarrow C} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - G_{t}^{B\rightarrow A} -  G_{t}^{B\rightarrow C} + G_{t}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - G_{t}^{C\rightarrow A} -  G_{t}^{C\rightarrow B} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$

You have $6$ unknown quantities to estimate _per time period_. There is just not enough information to do that. So you need assumptions that will impose structure (=restrictions) on the situation, and will permit you to estimate _something_. What? Let's say you assume that there is a relatively stable ""churn"" from one company to another, as a linear function of their market share in the previous period. This assumption brings in a set of unknown coefficients to be estimated (which will then give you an estimate of ""hidden transfers of market share""). Write $G_{t}^{A\rightarrow B} = a_bA_{t-1}$ (market share lost from $A$ to $B$ as a linear function of $A$'s market share in period $t-1$).
Your equations will become

$$ A_t - A_{t-1} = - a_bA_{t-1} -  a_cA_{t-1} + b_aB_{t-1}+c_aC_{t-1} $$

$$ B_t - B_{t-1} = - b_aB_{t-1} -  b_cB_{t-1} + a_bA_{t-1}+c_bC_{t-1}$$

$$ C_t - C_{t-1} = - c_aC_{t-1} -  c_bC_{t-1} + a_cA_{t-1}+ b_cB_{t-1}$$

We have turned a set of mathematical identities into a _model_. It is doubtful that this model will hold exactly for each $t$, so you should add a stochastic error term. Rearranging we obtain a first-order Vector Autoregression (VAR): 

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
1-a_b-a_c  & b_a & c_a \\
a_b & 1-b_a-b_c & c_b \\
a_c & b_c & 1-c_a-c_b \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

or, to homogenize notation,

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
\gamma_{11}  & \gamma_{12} & \gamma_{13} \\
\gamma_{21} & \gamma_{22} & \gamma_{23} \\
\gamma_{31} & \gamma_{32} & \gamma_{33} \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

subject to the equality restrictions
$$  \begin{matrix}
\gamma_{11} +  \gamma_{21} + \gamma_{31} =1 \\
\gamma_{12} +  \gamma_{22} + \gamma_{32} =1  \\
\gamma_{13} +  \gamma_{23} + \gamma_{33} =1  \\
\end{matrix} $$

So you have essentially $6$ unknown coefficients and a sample of $T-1$ observations (for each company).  
Note that these restrictions _imply_ the ""add up to unity"" restriction $A_t+B_t+C_t =1$ for each $t$, so this last one does not impose any additional structure on the unknown coefficients -but it does imply a relation between the error terms, namely that $u^A_{t} + u^B_{t}  +u^C_{t} =0$. Any additional assumptions on the three error terms should either come from knowledge of the specific real world phenomenon under study, and/or through a statistical specification search.

The for example an estimation for the hidden transfers of market share will be, for example

$$\hat G_{t}^{A\rightarrow B} = \hat \gamma_{21}A_{t-1}$$

etc.

Of course you may find that such a model does not fit your data sample well - for example you expect that all estimated coefficients should be positive and smaller than or equal to unity, but the estimation procedure may not give you that. But this is what we do: we come up with specification hypotheses and we test them against the data - ""success"" is never guaranteed. Then you should try to come up with a different model.

",Cange time-indexing and added an example,2013-10-12 14:00:57.927
184993,57012,20773.0,5,,CC BY-SA 3.0,e8cab215-3ee1-433f-be26-5f92422567ae,"I have observations taken with different sensitivity thresholds and minimum detection levels, i.e. Lab A is less sensitive and has a minimum detection level of .2 and Lab B is more sensitive and has a minimum detection level of .02. Each row corresponds to a unique measurement taken by either lab:

    Obs | Lab A | Lab B
    ---------------------
     1  |  .6   |  NA
     2  |  0    |  NA
     3  |  NA   |  .53
     4  |  .2   |  NA
     5  |  NA   |  .07

I think I would like something like:

    Obs | LabA  | LabB  | NewLab
    ----------------------------
     1  |  .6   |  NA   |  .64
     2  |  0    |  NA   |  .13
     3  |  NA   |  .53  |  .53
     4  |  .2   |  NA   |  .21
     5  |  NA   |  .07  |  .07

What techniques are available to standardize the values such that there is not a large loss of information?

 1. Obviously, I could take the values from Lab B and replace anything less than .2 with 0 and then round them, but I want to avoid throwing away information if possible.
 2. One person suggested to add random noise to the values of Lab A, but I'm not sure of the benefit of this vs. simply imputing the missing values from Lab B.

*Edit 1:*
There are no observations for which both Lab A and Lab B values are present, one will always be missing.",added 119 characters in body,2013-10-08 16:25:34.013
184996,56372,21108.0,5,,CC BY-SA 3.0,b18542c4-d61f-47e2-b5be-760f17240604,"I just made an implementation of P(A|B)/P(¬A|B) for a ""people who bought this also bought..."" algorithm.


I'm doing it by 

    P(A|B) = count_users(bought_A_and_B)/count_users(bought_A)
    P(¬A|B) = count_users(bought_B_but_not_A)/count_users(did_not_buy_A)

Then dividing the top one by the bottom one I get a score which makes absolute sense, but what kind of correlation am I calculating? What is this method called? Where can I read more about it?

[EDIT] When the number of users who bought item B but not item A is zero I just assume the correlation is 0. The same goes on when the number of users who bought A is zero.",edited body,2013-10-08 16:35:30.503
184997,56372,21108.0,5,,CC BY-SA 3.0,7cc7e87d-a414-49ef-8ecf-f4c24763154c,"I just made an implementation of P(A|B)/P(¬A|B) for a ""people who bought this also bought..."" algorithm.


I'm doing it by 

    P(A|B) = count_users(bought_A_and_B)/count_users(bought_A)
    P(¬A|B) = count_users(bought_B_but_not_A)/count_users(did_not_buy_A)

Then dividing the top one by the bottom one I get a score which makes absolute sense, but what kind of correlation am I calculating? What is this method called? Where can I read more about it?

[EDIT] This is not for using in a production environment, it is just some algorithm which appeared out of the blue in an online course I'm taking, I was just wondering where it could come from. Also, when the number of users who bought item B but not item A is zero I just skip the pair until I get more data. The same goes on when the number of users who bought A is zero.",edited body,2013-10-08 16:41:37.913
185022,57086,22034.0,2,,CC BY-SA 3.0,d1f0ea48-e92f-4b74-b688-7899e3b89c34,"I've read through the following posts that answered the question I was going to ask:

http://stats.stackexchange.com/questions/59741/use-random-forest-model-to-make-predictions-from-sensor-data

http://stats.stackexchange.com/questions/64201/decision-tree-for-output-prediction

Here's what I've done so far: I compared Logistic Regression to Random Forests and RF outperformed Logistic. Now the medical researchers I work with want to turn my RF results into a medical diagnostic tool. For example:

If you are an Asian Male between 25 and 35, have Vitamin D below xx and Blood Pressure above xx, you have a 76% chance of developing disease xxx. 

However, RF doesn't lend itself to simple mathematical equations (see above links). So here's my question: what ideas do you all have for using RF to develop a diagnostic tool (without having to export hundreds of trees). 

Here's a few of my ideas:

1. Use RF for variable selection, then use Logistic (using all possible interactions) to make the diagnostic equation.
2. Somehow aggregate the RF forest into one ""mega-tree,"" that somehow averages the node splits across trees. 
3. Similar to #2 and #1, use RF to select variables (say m variables total), then build hundreds of classification trees, all of which uses every m variable, then pick the best single tree. 

Any other ideas? Also, doing #1 is easy, but any ideas on how to implement #2 and #3?",,2013-10-08 18:41:35.193
185024,57086,22034.0,3,,CC BY-SA 3.0,d1f0ea48-e92f-4b74-b688-7899e3b89c34,<random-forest><prediction>,,2013-10-08 18:41:35.193
185023,57086,22034.0,1,,CC BY-SA 3.0,d1f0ea48-e92f-4b74-b688-7899e3b89c34,Ideas for outputting a prediction equation for Random Forests,,2013-10-08 18:41:35.193
185032,57015,,24,,CC BY-SA 3.0,e5712be2-23eb-4c93-a80d-dc6d4e8e7e04,,Proposed by anonymous approved by 686 edit id of 5573,2013-10-08 20:07:05.960
185031,57015,0.0,5,,CC BY-SA 3.0,e5712be2-23eb-4c93-a80d-dc6d4e8e7e04,"I am measuring two variables $x$ and $y$ in two different conditions. In the first condition, my hypothesis is that $\bar{x} > \bar{y}$ and in the second condition that $\bar{x} < \bar{y}$. Now that I have $N$ samples from both variables, how can I test whether my hypotheses are true? I am not sure if I can safely assume that $x$ and $y$ are independent from each other. Neither do I know from what kind of distributions they are sampled from. The sample size I have is small. I have read several introductions to statistics for the past few days, but never saw a worked out example for this kind of situations. All help appreciated.

Edit: Like Michael Mayer wrote, there is a binary grouping variable ""condition"". Sorry for a bit unclear question.",more information to question,2013-10-08 20:07:05.960
185072,57065,594.0,5,,CC BY-SA 3.0,05dead74-dd75-430d-b197-517eccc833bf,"We've run a split test of a new product feature and want to measure if the uplift on revenue is significant. Our observations are definitely not normally distributed (most of our users don't spend, and within those that do, it is heavily skewed towards lots of small spenders and a few very big spenders).

We've decided on using bootstrapping to compare the means, to get round the issue of the data not being normally distributed (side-question: is this a legitimate use of bootstrapping?)

My question is, do I need to trim outliers from the data set (e.g. the few very big spenders) before I run the bootstrapping, or does that not matter?

",deleted 54 characters in body,2013-10-08 23:07:21.240
185106,57110,22494.0,1,,CC BY-SA 3.0,91783daf-4317-40a8-b965-aa6c2b05ebf2,how to remove seasonality from daily electricity demand,,2013-10-09 03:35:34.333
185105,57110,22494.0,3,,CC BY-SA 3.0,91783daf-4317-40a8-b965-aa6c2b05ebf2,<r><time-series><arima>,,2013-10-09 03:35:34.333
185107,57110,22494.0,2,,CC BY-SA 3.0,91783daf-4317-40a8-b965-aa6c2b05ebf2,"how to remove seasonality from daily electricity demand (arima)

I tried to build a model to forecast daily electricity demand in R, and plot my data as per blow.
![Daily electricity demand][1]

I tried to remove seasonality with the following,
demand.xts.diff<-diff(demand.xts,lag=1,difference=1)
demand.xts.diff<-diff(demand.xts,lag=7,difference=1)

I also tried to use lag=365,366 (I am not sure what lag to use, due tot he leap year issue), but none of them successfully removed seasonality, and the ACF and PACF are as per blow

![ACF][2]
![PACF][3]

Anybody with experience of modelling daily electricity demand, please give some advice, any advice is appreciated.


  [1]: https://i.stack.imgur.com/pz5OS.png
  [2]: https://i.stack.imgur.com/OV7T4.png
  [3]: https://i.stack.imgur.com/HcXKi.png",,2013-10-09 03:35:34.333
185108,57110,22494.0,5,,CC BY-SA 3.0,dd8cd3d8-2a49-425a-bfcd-c9aef89c5ade,"how to remove seasonality from daily electricity demand (arima)

I tried to build a model to forecast daily electricity demand in R, and plot my data as per blow. My understanding is there is weekly (high demand on Tue, Wed, and low demand on Sat, Sun) and annul seasonality (high demand on Winter and lower on Summer). 
![Daily electricity demand][1]

I tried to remove seasonality with the following,
demand.xts.diff<-diff(demand.xts,lag=1,difference=1)
demand.xts.diff<-diff(demand.xts,lag=7,difference=1)

I also tried to use lag=365,366 (I am not sure what lag to use, due tot he leap year issue), but none of them successfully removed seasonality, and the ACF and PACF are as per blow

![ACF][2]
![PACF][3]

Anybody with experience of modelling daily electricity demand, please give some advice, any advice is appreciated.


  [1]: https://i.stack.imgur.com/pz5OS.png
  [2]: https://i.stack.imgur.com/OV7T4.png
  [3]: https://i.stack.imgur.com/HcXKi.png",added 158 characters in body,2013-10-09 03:51:49.790
185109,57110,17249.0,5,,CC BY-SA 3.0,4a926a73-1c9f-402e-9939-7c998bc7b336,"I want to remove seasonality from daily electricity demand (a time series). My understanding is there is weekly (high demand on Tue, Wed, and low demand on Sat, Sun) and annual seasonality (high demand on Winter and lower on Summer). I tried to build a model to forecast daily electricity demand in R, and plot my data as shown below:
![Daily electricity demand][1]

I tried to remove seasonality with the following:

    demand.xts.diff<-diff(demand.xts,lag=1,difference=1)
    demand.xts.diff<-diff(demand.xts,lag=7,difference=1)

I also tried to use `lag=365` and `lag=366` (I am not sure what lag to use, due to the leap year issue), but none of them successfully removed seasonality. The ACF and PACF are shown below:

![ACF][2]
![PACF][3]

Any advice is appreciated.


  [1]: https://i.stack.imgur.com/pz5OS.png
  [2]: https://i.stack.imgur.com/OV7T4.png
  [3]: https://i.stack.imgur.com/HcXKi.png","adjusted code, improved text",2013-10-09 04:06:14.310
185110,57110,17249.0,4,,CC BY-SA 3.0,4a926a73-1c9f-402e-9939-7c998bc7b336,How to remove seasonality from daily electricity demand,"adjusted code, improved text",2013-10-09 04:06:14.310
185111,57110,,24,,CC BY-SA 3.0,4a926a73-1c9f-402e-9939-7c998bc7b336,,"Proposed by 24808 approved by 805, 919 edit id of 5577",2013-10-09 04:06:14.310
185124,57110,,25,,,10fd3468-9968-462a-8da3-b79c2136f4e3,,http://twitter.com/#!/StackStats/status/387823266498637824,2013-10-09 06:14:06.873
185156,57126,22503.0,3,,CC BY-SA 3.0,8aff1f06-eb93-4019-bd27-180b130704ae,<hypothesis-testing><statistical-significance><bootstrap>,,2013-10-09 09:52:16.150
185157,57126,22503.0,2,,CC BY-SA 3.0,8aff1f06-eb93-4019-bd27-180b130704ae,"Wondering if anyone has an opinion on whether bootstrapping the difference in means is the right method given I have a situation with extreme data points. I've decided to use this as I don't think a t test is appropriate

I have about 30k observations per group (3 groups)

my situation is about spend, and I have extreme outliers
the outliers aren't quite like an ""income"" distribution. i.e. Most users (95%+) will spend zero, a subset of users will spend 5 - 10 dollars. some will spend about 20 or 50 dollars and then a select few will spend 500+, with a couple of users spending 5000 or 10000+

I am trying to test which group brought in the most revenue per user


can anyone offer any advice on which statistical test is best suited?",,2013-10-09 09:52:16.150
185155,57126,22503.0,1,,CC BY-SA 3.0,8aff1f06-eb93-4019-bd27-180b130704ae,is bootstrapping the right method for extreme distributions,,2013-10-09 09:52:16.150
185162,57128,22505.0,1,,CC BY-SA 3.0,7a7db1d9-a337-4ae2-be08-a265e834bb00,Poolong regression results in SPSS,,2013-10-09 10:06:51.963
185163,57128,22505.0,3,,CC BY-SA 3.0,7a7db1d9-a337-4ae2-be08-a265e834bb00,<regression><pooling>,,2013-10-09 10:06:51.963
185161,57128,22505.0,2,,CC BY-SA 3.0,7a7db1d9-a337-4ae2-be08-a265e834bb00,"please help me to solve the following issue:

1. I run my linear regression model many times (let's say 1000 times) with two variables: y - continuous dependent variable, x - continuous independent variable (mean of several consequent measurements).
2. The independent variable in each model was randomly drawn using its mean and standard deviation
3. I have the regression coefficient and standard error for this independent variable in each of the models.

Somehow I have to combine these results into one regression result. As far as I know the regression coefficients of 1000 models can be just averaged. However, this is not really clear to me how can I estimate the total variance of 1000 models.

Thank you!

",,2013-10-09 10:06:51.963
185167,57128,674.0,5,,CC BY-SA 3.0,304a609f-22cf-4718-9137-9406f5713b96,"I have to solve the following issue:

1. I run my linear regression model many times (let's say 1000 times) with two variables: y - continuous dependent variable, x - continuous independent variable (mean of several consequent measurements).
2. The independent variable in each model was randomly drawn using its mean and standard deviation
3. I have the regression coefficient and standard error for this independent variable in each of the models.

Somehow I have to combine these results into one regression result. As far as I know the regression coefficients of 1000 models can be just averaged. However, this is not really clear to me how can I estimate the total variance of 1000 models.

",deleted 22 characters in body; edited tags; edited title,2013-10-09 10:08:41.917
185169,57128,674.0,6,,CC BY-SA 3.0,304a609f-22cf-4718-9137-9406f5713b96,<regression><spss><pooling>,deleted 22 characters in body; edited tags; edited title,2013-10-09 10:08:41.917
185168,57128,674.0,4,,CC BY-SA 3.0,304a609f-22cf-4718-9137-9406f5713b96,Pooling regression results in SPSS,deleted 22 characters in body; edited tags; edited title,2013-10-09 10:08:41.917
185193,57137,21896.0,1,,CC BY-SA 3.0,a12ba676-3572-4938-b1a6-f4308be97151,Negative Binomial Regression: is parameter theta (R) the reciprocal of parameter kappa (SAS)?,,2013-10-09 12:00:22.943
185192,57137,21896.0,3,,CC BY-SA 3.0,a12ba676-3572-4938-b1a6-f4308be97151,<r><sas><negative-binomial-distribution>,,2013-10-09 12:00:22.943
185363,57177,594.0,5,,CC BY-SA 3.0,c97a4854-dc00-4e21-8001-c6a5480e273d,"As for qualitative differences, the lognormal and gamma are, as you say, quite similar. 

Indeed, in practice they're often used to model the same phenomena (some people will use a gamma where others use a lognormal). They are both, for example, constant-coefficient-of-variation models (the CV for the lognormal is $\sqrt{e^{\sigma^2} -1}$, for the gamma it's $1/\sqrt \alpha$).

[How can it be constant if it depends on a parameter, you ask? It applies when you model the scale (location for the log scale); for the lognormal, $\mu$ acts as a scale parameter, while for the gamma, the scale is the parameter that isn't the shape parameter (or its reciprocal if you use the shape-rate parameterization). I'll call the scale parameter for the gamma distribution $\beta$. Gamma GLMs model the mean ($\mu=\alpha\beta$) while holding $\alpha$ constant; in that case $\mu$ is also a scale parameter. A model with varying $\mu$ and constant $\alpha$ or $\sigma$ respectively will have constant CV.]

You might find it instructive to look at the density of their *logs*, which often shows a very clear difference.

The log of a lognormal random variable is ... normal. It's symmetric.

The log of a gamma random variable is left-skew. Depending on the value of the shape parameter, it may be quite skew or nearly symmetric.

Here's an example, with both lognormal and gamma having mean 1 and variance 1/4. The top plot shows the densities (gamma in green, lognormal in blue), and the lower one shows the densities of the logs:

![gamma and lognormal, densitiy and density of log][1]

(Plotting the log of the density of the logs is also useful. That is, taking a log-scale on the y-axis above)

This difference implies that the gamma has more of a tail on the left, and less of a tail on the right; the far right tail of the lognormal is heavier and its left tail lighter. And indeed, if you look at the skewness, of the lognormal and gamma, for a given coefficient of variation, the lognormal is more right skew ($(e^{\sigma^2}+2) \text{CV}$) than the gamma ($2\text{CV}$).


  [1]: https://i.stack.imgur.com/I9ARM.png",added 664 characters in body,2013-10-10 00:27:42.183
185194,57137,21896.0,2,,CC BY-SA 3.0,a12ba676-3572-4938-b1a6-f4308be97151,"after some frantic googling I do believe the answer is yes, but more so I am frustrated that the relation between the two parameter seems to be nowhere described explicitely so I do it here. (I hope this isn't against the rules of stackexchange.)

[This very nice article](http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1141&context=usdeptcommercepub&sei-redir=1&referer=http%3A%2F%2Fscholar.google.nl%2Fscholar_url%3Fhl%3Dnl%26q%3Dhttp%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%253Farticle%253D1141%2526context%253Dusdeptcommercepub%26sa%3DX%26scisig%3DAAGBfm3vz9gDbxRveIafikl02v0aeUyu0w%26oi%3Dscholarr%26ei%3DDj9VUqWlL6LG0QXe1oHwAg%26ved%3D0CDAQgAMoADAA#search=%22http%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1141%26context%3Dusdeptcommercepub%22) states: we will denote the random variable Y having a negative binomial distribution as Y ~ NB($\mu, \kappa$) with a parameterization such that E(Y) = $\mu$, var(Y) = $\mu + \kappa \mu^2$.

I take this latter equation as the definition of $\kappa$.

[Apparently](http://books.google.nl/books?id=Ohks0xwvyT4C&pg=PA196&lpg=PA196&dq=kappa+parameter+negative+binomial+proc+glimmix&source=bl&ots=PYKpaGQ8VN&sig=5sNEB-7H7ZocErTKhi35ORKd2lA&hl=nl&sa=X&ei=lEBVUqCnNcTJ0QXppYGoAg&ved=0CDYQ6AEwAA#v=onepage&q=kappa%20parameter%20negative%20binomial%20proc%20glimmix&f=false) this kappa is implemented in SAS.

Now turning to R, the function `glm.nb` in the `MASS` package contains a parameter $\mu$ which is obviously the same $\mu$ as above and a parameter $\theta$. The question is how $\theta$ and $\kappa$ are related. The documentation for `glm.nb` only refers to it as an ""additional parameter"". The answer to [this](http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r) stackexchange question directly implies that $\theta = 1/\kappa$, but [this](http://stats.stackexchange.com/questions/30360/what-is-the-distribution-of-theta-in-a-negative-binomial-model-glm-nb-with-r?rq=1) question seems to suggest that $\theta = \kappa$. 

The help page for negative binomial in R is nice and introduces a parameter called `size` that equals $1/\kappa$. Fitting `glm.nb` on random data generated by `rnbinom` for various choices of $\mu$ and `size` seems to support the thesis that $\theta = 1/\kappa$ (i.e. that $\theta$ = `size`) but also that for large values of size the estimation is poor.

Summarizing: I do believe that $\theta = 1/\kappa$ but it would be nice if there were an easily googlable place on the internet stating this explicitly. Maybe one of the answers to this questions can serve as such a place? ",,2013-10-09 12:00:22.943
185201,57137,21896.0,5,,CC BY-SA 3.0,dcd9c3b8-6f71-448a-90ad-3d615ee3fef3,"After some frantic googling I do believe the answer is yes, but more so I am frustrated that the relation between the two parameter seems to be nowhere described explicitely so I do it here. (I hope this isn't against the rules of stackexchange.)

[This very nice article](http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1141&context=usdeptcommercepub&sei-redir=1&referer=http%3A%2F%2Fscholar.google.nl%2Fscholar_url%3Fhl%3Dnl%26q%3Dhttp%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%253Farticle%253D1141%2526context%253Dusdeptcommercepub%26sa%3DX%26scisig%3DAAGBfm3vz9gDbxRveIafikl02v0aeUyu0w%26oi%3Dscholarr%26ei%3DDj9VUqWlL6LG0QXe1oHwAg%26ved%3D0CDAQgAMoADAA#search=%22http%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1141%26context%3Dusdeptcommercepub%22) states: we will denote the random variable Y having a negative binomial distribution as Y ~ NB($\mu, \kappa$) with a parameterization such that E(Y) = $\mu$, var(Y) = $\mu + \kappa \mu^2$.

I take this latter equation as the definition of $\kappa$.

[Apparently](http://books.google.nl/books?id=Ohks0xwvyT4C&pg=PA196&lpg=PA196&dq=kappa+parameter+negative+binomial+proc+glimmix&source=bl&ots=PYKpaGQ8VN&sig=5sNEB-7H7ZocErTKhi35ORKd2lA&hl=nl&sa=X&ei=lEBVUqCnNcTJ0QXppYGoAg&ved=0CDYQ6AEwAA#v=onepage&q=kappa%20parameter%20negative%20binomial%20proc%20glimmix&f=false) this kappa is implemented in SAS.

Now turning to R, the function `glm.nb` in the `MASS` package contains a parameter $\mu$ which is obviously the same $\mu$ as above and a parameter $\theta$. The question is how $\theta$ and $\kappa$ are related. The documentation for `glm.nb` only refers to it as an ""additional parameter"". The answers to [this](http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r) and [this](http://stats.stackexchange.com/questions/10457/interpreting-negative-binomial-regression-output-in-r?rq=1) stackexchange questions directly imply that $\theta = 1/\kappa$, but [this](http://stats.stackexchange.com/questions/30360/what-is-the-distribution-of-theta-in-a-negative-binomial-model-glm-nb-with-r?rq=1) question seems to suggest that $\theta = \kappa$. 

The help page for negative binomial in R is nice and introduces a parameter called `size` that equals $1/\kappa$. Fitting `glm.nb` on random data generated by `rnbinom` for various choices of $\mu$ and `size` seems to support the thesis that $\theta = 1/\kappa$ (i.e. that $\theta$ = `size`) but also that for large values of size the estimation is poor.

Summarizing: I do believe that $\theta = 1/\kappa$ but it would be nice if there were an easily googlable place on the internet stating this explicitly. Maybe one of the answers to this questions can serve as such a place? ",added 118 characters in body,2013-10-09 12:06:18.343
185204,57137,21896.0,5,,CC BY-SA 3.0,155ab217-d115-4841-b575-8c1a25007ac4,"After some frantic googling I do believe the answer is yes, but more so I am frustrated that the relation between the two parameters seems to be nowhere described explicitely so I do it here. (I hope this isn't against the rules of stackexchange.)

[This very nice article](http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1141&context=usdeptcommercepub&sei-redir=1&referer=http%3A%2F%2Fscholar.google.nl%2Fscholar_url%3Fhl%3Dnl%26q%3Dhttp%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%253Farticle%253D1141%2526context%253Dusdeptcommercepub%26sa%3DX%26scisig%3DAAGBfm3vz9gDbxRveIafikl02v0aeUyu0w%26oi%3Dscholarr%26ei%3DDj9VUqWlL6LG0QXe1oHwAg%26ved%3D0CDAQgAMoADAA#search=%22http%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1141%26context%3Dusdeptcommercepub%22) states: we will denote the random variable Y having a negative binomial distribution as Y ~ NB($\mu, \kappa$) with a parameterization such that E(Y) = $\mu$, var(Y) = $\mu + \kappa \mu^2$.

I take this latter equation as the definition of $\kappa$.

[Apparently](http://books.google.nl/books?id=Ohks0xwvyT4C&pg=PA196&lpg=PA196&dq=kappa+parameter+negative+binomial+proc+glimmix&source=bl&ots=PYKpaGQ8VN&sig=5sNEB-7H7ZocErTKhi35ORKd2lA&hl=nl&sa=X&ei=lEBVUqCnNcTJ0QXppYGoAg&ved=0CDYQ6AEwAA#v=onepage&q=kappa%20parameter%20negative%20binomial%20proc%20glimmix&f=false) this kappa is implemented in SAS.

Now turning to R, the function `glm.nb` in the `MASS` package contains a parameter $\mu$ which is obviously the same $\mu$ as above and a parameter $\theta$. The question is how $\theta$ and $\kappa$ are related. The documentation for `glm.nb` only refers to it as an ""additional parameter"". The answers to [this](http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r) and [this](http://stats.stackexchange.com/questions/10457/interpreting-negative-binomial-regression-output-in-r?rq=1) stackexchange questions directly imply that $\theta = 1/\kappa$, but [this](http://stats.stackexchange.com/questions/30360/what-is-the-distribution-of-theta-in-a-negative-binomial-model-glm-nb-with-r?rq=1) question seems to suggest that $\theta = \kappa$. 

The [help page for negative binomial in R](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/NegBinomial.html) is nice and introduces a parameter called `size` that equals $1/\kappa$. Fitting `glm.nb` on random data generated by `rnbinom` for various choices of $\mu$ and `size` seems to support the thesis that $\theta = 1/\kappa$ (i.e. that $\theta$ = `size`) but also that for large values of size the estimation is poor.

Summarizing: I do believe that $\theta = 1/\kappa$ but it would be nice if there were an easily googlable place on the internet stating this explicitly. Maybe one of the answers to this questions can serve as such a place? ",added 79 characters in body,2013-10-09 12:15:59.433
185219,57012,20773.0,5,,CC BY-SA 3.0,13522325-3b5c-4627-aaea-0f0edaca604a,"I have observations taken with different sensitivity thresholds and minimum detection levels, i.e. Lab A is less sensitive and has a minimum detection level of .2 and Lab B is more sensitive and has a minimum detection level of .02. 

*Edit 2: I have taken $N$ samples and have had them processed by two different labs (for stupid political reasons). Both labs send me the results and I discover that Lab A has a minimum detection level of .2 and Lab B has a minimum detection level of .02. See example:*

Each row corresponds to a unique measurement taken by either lab:

    Obs | Lab A | Lab B
    ---------------------
     1  |  .6   |  NA
     2  |  0    |  NA
     3  |  NA   |  .53
     4  |  .2   |  NA
     5  |  NA   |  .07

*Edit 2: I would like to be able to use and combine results from both labs, as if they were on the same scale. The problem is that the labs used to process the samples have very different thresholds for detection and have different sensitivity levels.*

I think I would like something like:

    Obs | LabA  | LabB  | NewLab
    ----------------------------
     1  |  .6   |  NA   |  .64
     2  |  0    |  NA   |  .13
     3  |  NA   |  .53  |  .53
     4  |  .2   |  NA   |  .21
     5  |  NA   |  .07  |  .07

What techniques are available to standardize the values such that there is not a large loss of information?

 1. Obviously, I could take the values from Lab B and replace anything less than .2 with 0 and then round them, but I want to avoid throwing away information if possible.
 2. One person suggested to add random noise to the values of Lab A, but I'm not sure of the benefit of this vs. simply imputing the missing values from Lab B.

*Edit 1:*
There are no observations for which both Lab A and Lab B values are present, one will always be missing.

*Edit 2:*
What can I do to get results from both labs on a similar scale?",added 548 characters in body,2013-10-09 13:36:19.723
185273,57156,19822.0,1,,CC BY-SA 3.0,bdeaca58-ef69-4246-95f9-685b96ec5013,How to perform unsupervised Random Forest classification using Breimans code?,,2013-10-09 17:22:59.850
185272,57156,19822.0,3,,CC BY-SA 3.0,bdeaca58-ef69-4246-95f9-685b96ec5013,<machine-learning><classification><random-forest>,,2013-10-09 17:22:59.850
185274,57156,19822.0,2,,CC BY-SA 3.0,bdeaca58-ef69-4246-95f9-685b96ec5013,"I am working with the Breimans random Forest code (http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_manual.htm#c2) for classification of satellite data (supervised learning). I am using a training and test dataset having sample size of 2000 and variable size 10. The data is classified in to two classes, A and B. In supervised learning mode, the algorithm is performing well with very low classification error (<2%). Now i want to try the unsupervised classification with no class lables in the test data set and see how the algorithm is able to predict the classes? Is there a way to implement unsupervised classification using the Breimans code? Will the error from this method will be higher than supervised classification?
The data and run parameter setting in the algorithm are given below

DESCRIBE DATA
1       mdim=10,ntrain=2000,nclass=2,maxcat=1,
1	ntest=2000,labelts=1,labeltr=1,

SET RUN PARAMETERS
2	mtry0=3,ndsize=1,jbt=500,look=100,lookcls=1,
2	jclasswt=0,mdim2nd=0,mselect=0,",,2013-10-09 17:22:59.850
185293,57160,15764.0,3,,CC BY-SA 3.0,55f4fbfc-b592-4a25-8c46-3a903de800ac,<time-series><python><arima><arma>,,2013-10-09 19:09:36.590
185381,57187,9792.0,3,,CC BY-SA 3.0,6084d71e-6338-4e2c-96e9-f23881bbbbe2,<r><splines><lm><centering>,,2013-10-10 02:44:04.330
185380,57187,9792.0,1,,CC BY-SA 3.0,6084d71e-6338-4e2c-96e9-f23881bbbbe2,Centering when using splines in R,,2013-10-10 02:44:04.330
185446,57211,503.0,2,,CC BY-SA 3.0,ecb27f3c-e9d4-4aed-a527-8017b743b9b6,"Using eigenvalues > 1 is only *one* indication of how many factors to retain. Other reasons include the scree test, getting a reasonable proportion of variance explained and (most importantly) substantive sense. 

That said, the rule came about because the average eigenvalue will be 1, so > 1 is ""higher than average"". 

On your second question: Are you asking how to know how many factors (latent variables) to retain? Or are you asking about which observed variables to retain?

If the former, see above and see any book on factor analysis. If the latter, each factor is a linear combination of *all* the observed variables (although some contribute very little). ",,2013-10-10 10:52:34.780
185294,57160,15764.0,2,,CC BY-SA 3.0,55f4fbfc-b592-4a25-8c46-3a903de800ac,"I'd like to have a [*seasonal* ARIMA model](https://www.otexts.org/fpp/8/9) implemented with Statsmodels ARIMA. Specifically, I'd like to log before the weekly seasonality and then be able to make forecasts.

Perhaps an example with ARIMA's [from_formula](http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.arima_model.ARIMA.from_formula.html#statsmodels.tsa.arima_model.ARIMA.from_formula) method could accomplish this. I'd also love to be able to do this with patsy.

Here's my sample code for logging before the weekly seasonality, and then transforming back to compare to the original time series (I've also skipped checking the validity of the model through testing stationarity and the residuals):
    
    import pandas as pd
    import numpy as np
    from statsmodels.tsa.arima_model import ARIMA
    
    # ts is a time series
    logged_ts = np.log(ts)
    # Differencing by the week forces us to drop the first 7 values.
    diffed_logged_ts = (logged_ts - logged_ts.shift(7))[7:]

    p = 0
    d = 1
    q = 1

    arima = ARIMA(diffed_logged_ts.values, [p, d, q], exog=None, dates=diffed_logged_ts.index, freq='D', missing='none')
    diffed_logged_results = arima.fit(trend='c', disp=False)
    predicted_diffed_logged = diffed_logged_results.predict(exog=None, dynamic=False)
    predicted_diffed_logged_ts = pd.Series(predicted_diffed_logged, index=diffed_logged_ts.index[d:])
    predicted_diffed_logged_ts = np.exp(logged_ts.shift(7) + diffed_logged_ts.shift(d) + predicted_diffed_logged_ts)
  
    concatenated = pd.concat([ts, predicted_diffed_logged_ts], axis=1, keys=['original', 'predicted'])
    print concatenated[-7:]

What do you think of this approach? I hope there's a less error-prone way coming in a future version of Statsmodels. Could someone tag this question with ""statsmodels""? Thanks!
",,2013-10-09 19:09:36.590
185292,57160,15764.0,1,,CC BY-SA 3.0,55f4fbfc-b592-4a25-8c46-3a903de800ac,ARIMA with seasonality in Statsmodels,,2013-10-09 19:09:36.590
185295,57161,20739.0,1,,CC BY-SA 3.0,484423cf-7729-4409-9591-7f374c2684cd,Performing binary logistic regression with equal number of cases and non-cases,,2013-10-09 19:25:15.053
185296,57161,20739.0,3,,CC BY-SA 3.0,484423cf-7729-4409-9591-7f374c2684cd,<logistic><sampling>,,2013-10-09 19:25:15.053
185297,57161,20739.0,2,,CC BY-SA 3.0,484423cf-7729-4409-9591-7f374c2684cd,"The best way to ask my question is to present an example scenario:

Let's say that the outcome of interest is lung cancer (1 = lung cancer; 0 = no lung cancer) and we have 200k records (where 20k patients have lung cancer (cases) and 180k patients do NOT have lung cancer (non-cases)). Since only 10% of patients (20/200k) in our sample data have lung cancer, we use a random sample of 20k from the patients that do NOT have lung cancer. By doing so, we would have a sample of 20k patients with lung cancer and 20k patients without lung cancer in our sample (the sample is reduced from 200k to 40k records).

Are there any benefits to performing binary logistic regression with equal number of cases and non-cases when the actual distribution of the outcome is not equal? Or does this bias our model estimates/predictive power?

Thanks in advance!           ",,2013-10-09 19:25:15.053
185301,57161,20739.0,5,,CC BY-SA 3.0,cc2c17bc-ea01-45cf-8f70-7e5683c45310,"The best way to ask my question is to present an example scenario:

Let's say that the outcome of interest is lung cancer (1 = lung cancer; 0 = no lung cancer) and a researcher has 200k records (where 20k patients have lung cancer (cases) and 180k patients do NOT have lung cancer (non-cases)). Since only 10% of patients (20/200k) in the sample data have lung cancer, a researcher uses a random sample of 20k from the patients that do NOT have lung cancer. By doing so, the researcher would have a sample of 20k patients with lung cancer and 20k patients without lung cancer in their sample (the sample is reduced from 200k to 40k records).

Are there any benefits to performing binary logistic regression with equal number of cases and non-cases when the actual distribution of the outcome is not equal? Or does this bias model estimates/predictive power?

Thanks in advance!           ",added 21 characters in body,2013-10-09 19:33:35.397
185303,57164,19264.0,2,,CC BY-SA 3.0,6c124c80-8c41-40cd-b530-5197075e7dbb,"I have an experimentally observed distribution that looks very similar to a gamma or lognormal distribution. I've read that the [lognormal distribution](http://en.wikipedia.org/wiki/Log-normal_distribution) is the maximum entropy probability distribution for a random variate $X$ for which the mean and variance of $ln(X)$ are fixed. Does the gamma distribution have any similar properties?

What methods can I use to determine the true PDF of my data?",,2013-10-09 19:51:40.443
185304,57164,19264.0,1,,CC BY-SA 3.0,6c124c80-8c41-40cd-b530-5197075e7dbb,Gamma vs. lognormal distributions,,2013-10-09 19:51:40.443
185305,57164,19264.0,3,,CC BY-SA 3.0,6c124c80-8c41-40cd-b530-5197075e7dbb,<density-function><gamma-distribution><lognormal-distribution>,,2013-10-09 19:51:40.443
185310,57164,19264.0,5,,CC BY-SA 3.0,50309f10-b9dd-4cfb-8582-bf7aa5de8bcc,I have an experimentally observed distribution that looks very similar to a gamma or lognormal distribution. I've read that the [lognormal distribution](http://en.wikipedia.org/wiki/Log-normal_distribution) is the maximum entropy probability distribution for a random variate $X$ for which the mean and variance of $ln(X)$ are fixed. Does the gamma distribution have any similar properties?,deleted 64 characters in body,2013-10-09 20:34:07.890
185312,57167,21952.0,1,,CC BY-SA 3.0,2760c52f-dd31-4e2b-853f-2728f6b5853c,simultaneous equations,,2013-10-09 20:44:49.813
185313,57167,21952.0,3,,CC BY-SA 3.0,2760c52f-dd31-4e2b-853f-2728f6b5853c,<multiple-regression><econometrics>,,2013-10-09 20:44:49.813
185311,57167,21952.0,2,,CC BY-SA 3.0,2760c52f-dd31-4e2b-853f-2728f6b5853c,"I have the following relationships

Y ~ X1 + X2 + X3 + X4 

and

X1 ~ Z1 + Z2 + Z3 + Z4

X2 ~ Z1 + Z2 + Z3 + Z4

X3 ~ Z1 + Z2 + Z3 + Z4

X4 ~ Z1 + Z2 + Z3 + Z4

where Y and Z1, Z2, Z3, Z4 are endogenous (Say while the Z's play a role in determining Y, the values of Z's are fixed depending upont he values of Y - Kind of like advertising expense has an impact on sales revenue but at the same time managers determine the advertisement expense on the expected sales revenue). So all the variable are changing simultaneously. Can anyone help me on how I can estimate this relationship? Thank you",,2013-10-09 20:44:49.813
185322,57015,,24,,CC BY-SA 3.0,90fcbbe2-f915-43ea-af06-3856c0dec8f4,,"Proposed by anonymous approved by 601, 805 edit id of 5584",2013-10-09 21:19:16.310
185321,57015,0.0,5,,CC BY-SA 3.0,90fcbbe2-f915-43ea-af06-3856c0dec8f4,"I am measuring two unpaired variables $x$ and $y$ in two different conditions ($x$ and $y$ are magnitudes of some special magnetic signals). In the first condition, my hypothesis is that $\bar{x} > \bar{y}$ and in the second condition that $\bar{x} < \bar{y}$. Now that I have $N$ samples from both variables, how can I test whether my hypotheses are true? I am not sure if I can safely assume that $x$ and $y$ are independent from each other. Neither do I know from what kind of distributions they are sampled from. The sample size I have is small. I have read several introductions to statistics for the past few days, but never saw a worked out example for this kind of situations. All help appreciated.

Edit: Like Michael Mayer wrote, there is a binary grouping variable ""condition"". Sorry for a bit unclear question.",added more information,2013-10-09 21:19:16.310
185335,57175,20320.0,2,,CC BY-SA 3.0,8e648ebf-f69f-4779-9b4e-4dd7d569b2c7,"I have just delved into the basics of maximum likelihood estimation and expectation maximization. The latter is really difficult to follow and I am having a tough time in figuring how I can apply EM method for parameter estimation by minimizing the error given in [paper][1]. The estimation problem is formulated as 

$x_n= s_n(\theta_0) + w_n  Eq(1)$

where $s_n$ is the signal of interest;  $w_n$ is a Gaussian white noise. The parameter estimate is obtained after minimization of $J$ (eq3)

$$\hat{\theta}= \arg  \min_{\theta} J(\theta) Eq(2)$$ and $$J = \sum ||u_{i-1} - u_i||^2$$ Eq(3)

where $u_n$ is a function,  $u_n = x_n - s_n(\theta)$. Minimizing the nonlinear prediction error of $u_n$ will give us the parameter vector $\theta$ since $u_n$ will converge to $\theta_0$. The search is initialized by taking random values of initial guesses of the parameters and doing a random search, stopping when $J$ is minimized for a given number of iterations, the parameters are incremented after each iteration by a very small number. I have read considerably on number of papers on EM but they are more complicated since they use Kalman filters and Kalman smoothers as the estimate. I don't think that I will be needing Kalman filter for estimation since minimization of $J$ serves as an estimator. My objective is to formulate the recursive search for optimal $J$ as EM method. 

 1. Can somebody give ideas if I need to find the derivative of the objective function and how I can plugin this step in EM method. 
 2. I really do not understand how to begin the formulation of Eq(3) in ML format.


  [1]: http://www.eurasip.org/Proceedings/Eusipco/Eusipco2000/SESSIONS/THUAM/OR1/CR1473.PDF",,2013-10-09 22:19:42.207
185337,57175,20320.0,3,,CC BY-SA 3.0,8e648ebf-f69f-4779-9b4e-4dd7d569b2c7,<estimation><matlab><maximum-likelihood>,,2013-10-09 22:19:42.207
185336,57175,20320.0,1,,CC BY-SA 3.0,8e648ebf-f69f-4779-9b4e-4dd7d569b2c7,On how to formulate and apply maximum likelihood,,2013-10-09 22:19:42.207
185340,57175,20320.0,5,,CC BY-SA 3.0,1c87e67b-2a11-4c9e-9ee3-04443f77788e,"I have just delved into the basics of maximum likelihood estimation and expectation maximization. The latter is really difficult to follow and I am having a tough time in figuring how I can apply EM method for parameter estimation by minimizing the error given in [paper][1]. The estimation problem is formulated as 

$x_n= s_n(\theta_0) + w_n  Eq(1)$

where $s_n$ is the signal of interest;  $w_n$ is a Gaussian white noise. The parameter estimate is obtained after minimization of $J$ (eq3)

$$\hat{\theta}= \arg  \min_{\theta} J(\theta) Eq(2)$$ and $$J = \sum ||u_{i-1} - u_i||^2$$ Eq(3)

where $u_n$ is a function,  $u_n = x_n - s_n(\theta)$. Minimizing the nonlinear prediction error of $u_n$ will give us the parameter vector $\theta$ since $u_n$ will converge to $\theta_0$. The search is initialized by taking random values of initial guesses of the parameters and doing a random search, stopping when $J$ is minimized for a given number of iterations, the parameters are incremented after each iteration by a very small number. I have read considerably on number of papers on EM but they are more complicated since they use Kalman filters and Kalman smoothers as the estimate. I don't think that I will be needing Kalman filter for estimation since minimization of $J$ serves as an estimator. My objective is to formulate the recursive search for optimal $J$ as EM method. The maximum-likelihood solution for the parameters ,
under the additive white Gaussian noise assumption, corresponds
to minimization of the norm as follows $\theta_{ML} = arg min \sum ||u_{i-1} - u_i||^2  Eq(4)$

 1. Can somebody give ideas of of solving Eq(4) as ML. 
 2. I really do not understand how to begin the formulation of Eq(3) in ML format.


  [1]: http://www.eurasip.org/Proceedings/Eusipco/Eusipco2000/SESSIONS/THUAM/OR1/CR1473.PDF",added 136 characters in body,2013-10-09 22:31:06.690
185341,57175,20320.0,5,,CC BY-SA 3.0,69c42ac2-3f27-4592-8ed8-47377a960c8a,"I have just delved into the basics of maximum likelihood estimation and expectation maximization. The latter is really difficult to follow and I am having a tough time in figuring how I can apply EM method for parameter estimation by minimizing the error given in [paper][1]. The estimation problem is formulated as 

$x_n= s_n(\theta_0) + w_n  Eq(1)$

where $s_n$ is the signal of interest;  $w_n$ is a Gaussian white noise. The parameter estimate is obtained after minimization of $J$ (eq3)

$$\hat{\theta}= \arg  \min_{\theta} J(\theta) Eq(2)$$ and $$J = \sum ||u_{i-1} - u_i||^2$$ Eq(3)

where $u_n$ is a function,  $u_n = x_n - s_n(\theta)$. Minimizing the nonlinear prediction error of $u_n$ will give us the parameter vector $\theta$ since $u_n$ will converge to $\theta_0$. The search is initialized by taking random values of initial guesses of the parameters and doing a random search, stopping when $J$ is minimized for a given number of iterations, the parameters are incremented after each iteration by a very small number. I have read considerably on number of papers on EM but they are more complicated since they use Kalman filters and Kalman smoothers as the estimate. I don't think that I will be needing Kalman filter for estimation since minimization of $J$ serves as an estimator. My objective is to formulate the recursive search for optimal $J$ as EM method. The maximum-likelihood solution for the parameters ,
under the additive white Gaussian noise assumption, corresponds
to minimization of the norm as follows $\theta_{ML} = arg min \sum ||u_{i-1} - u_i||^2  Eq(4)$

 1. Can somebody give ideas of of solving Eq(4) as ML i.e how do I minimize J using ML? 
 2. I really do not understand how to begin the formulation of Eq(3) in ML format.


  [1]: http://www.eurasip.org/Proceedings/Eusipco/Eusipco2000/SESSIONS/THUAM/OR1/CR1473.PDF",added 33 characters in body,2013-10-09 22:41:43.693
185342,57177,594.0,2,,CC BY-SA 3.0,47be9cb5-bfa4-465c-983b-32385d25aa69,"As for qualitative differences, the lognormal and gamma are, as you say, quite similar. 

Indeed, in practice they're often used to model the same phenomena (some people will use a gamma where others use a lognormal). They are both, for example, constant-coefficient-of-variation models (the CV for the lognormal is $\sqrt{e^{\sigma^2} -1}$, for the gamma it's $1/\sqrt \alpha$).

You might find it instructive to look at the density of their *logs*, which often shows a very clear difference.

The log of a lognormal random variable is ... normal. It's symmetric.

The log of a gamma random variable is left-skew. Depending on the value of the shape parameter, it may be quite skew or nearly symmetric.

This difference implies that the gamma has more of a tail on the left, and less of a tail on the right; the far right tail of the lognormal is heavier and its left tail lighter. And indeed, if you look at the skewness, of the lognormal and gamma, for a given coefficient of variation, the lognormal is more right skew ($(e^{\sigma^2}+2) \text{CV}$) than the gamma ($2\text{CV}$).
",,2013-10-09 22:43:42.927
185407,57192,155.0,6,,CC BY-SA 3.0,ce295a26-359f-4f9a-94e1-272c2f31aedc,<normal-distribution><excel>,added 1 characters in body; edited tags; edited title,2013-10-10 06:05:50.533
185406,57192,155.0,4,,CC BY-SA 3.0,ce295a26-359f-4f9a-94e1-272c2f31aedc,How to check for normal distribution using Excel for performing a t-test?,added 1 characters in body; edited tags; edited title,2013-10-10 06:05:50.533
185954,57355,20927.0,3,,CC BY-SA 3.0,65f12dac-758c-453a-a31d-c32136036024,<bonferroni>,,2013-10-12 14:02:41.167
185953,57355,20927.0,1,,CC BY-SA 3.0,65f12dac-758c-453a-a31d-c32136036024,SPSS-independent-sample-t-test,,2013-10-12 14:02:41.167
185348,57175,20320.0,5,,CC BY-SA 3.0,aef0b883-d326-48bf-a27d-13915f5a8fdc,"I have just delved into the basics of maximum likelihood estimation and expectation maximization. The latter is really difficult to follow and I am having a tough time in figuring how I can apply EM method for parameter estimation by minimizing the error given in [paper][1]. The estimation problem is formulated as 

$x_n= s_n(\theta_0) + w_n  Eq(1)$


where $s_n$ is the signal of interest;  $w_n$ is a Gaussian white noise. The parameter estimate is obtained after minimization of $J$ (eq3)

$$\hat{\theta}= \arg  \min_{\theta} J(\theta) Eq(2)$$ and $$J = \sum ||u_{i-1} - u_i||^2$$ Eq(3)

where $u_n$ is a function,  $u_n = x_n - s_n(\theta)$. Minimizing the nonlinear prediction error of $u_n$ will give us the parameter vector $\theta$ since $u_n$ will converge to $\theta_0$. So, $u_n$ generally becomes a data series. 

In the original way, the search is initialized by taking random values of initial guesses of the parameters and doing a random search, stopping when $J$ is minimized for a given number of iterations, the parameters are incremented after each iteration by a very small number. I have read considerably on number of papers on EM but they are more complicated since they use Kalman filters and Kalman smoothers as the estimate. I don't think that I will be needing Kalman filter for estimation since minimization of $J$ serves as an estimator. My objective is to formulate the recursive search for optimal $J$ as EM method. The maximum-likelihood solution for the parameters ,
under the additive white Gaussian noise assumption, corresponds
to minimization of the norm as follows $\theta_{ML} = arg min u_n Eq(4)$

 1. Can somebody give ideas of of solving Eq(4) as ML i.e how do I minimize J using ML? 
 2. I really do not understand how to replace Eq (3) by ML based minimization since I cannot understand the formulation of the likelihood and other technical information. 


  [1]: http://www.eurasip.org/Proceedings/Eusipco/Eusipco2000/SESSIONS/THUAM/OR1/CR1473.PDF",added 137 characters in body,2013-10-09 22:54:40.457
185350,57156,15827.0,5,,CC BY-SA 3.0,725a81ee-f648-4672-b464-dc6b081442fc,"I am working with Breiman's random forest code (http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_manual.htm#c2) for classification of satellite data (supervised learning). I am using a training and test dataset having sample size of 2000 and variable size 10. The data is classified into two classes, A and B. In supervised learning mode, the algorithm is performing well with very low classification error (<2%). Now I want to try the unsupervised classification with no class labels in the test data set and see how the algorithm is able to predict the classes. Is there a way to implement unsupervised classification using Breiman's code? Will the error from this method will be higher than supervised classification?
The data and run parameter setting in the algorithm are given below

DESCRIBE DATA
1       mdim=10,ntrain=2000,nclass=2,maxcat=1,
1	ntest=2000,labelts=1,labeltr=1,

SET RUN PARAMETERS
2	mtry0=3,ndsize=1,jbt=500,look=100,lookcls=1,
2	jclasswt=0,mdim2nd=0,mselect=0,",basic English fixes; Breiman's is possessive,2013-10-09 22:56:14.480
185349,57156,15827.0,4,,CC BY-SA 3.0,725a81ee-f648-4672-b464-dc6b081442fc,How to perform unsupervised Random Forest classification using Breiman's code?,basic English fixes; Breiman's is possessive,2013-10-09 22:56:14.480
185355,57177,594.0,5,,CC BY-SA 3.0,a0a601e5-4a67-4a0c-9a80-adad523795f9,"As for qualitative differences, the lognormal and gamma are, as you say, quite similar. 

Indeed, in practice they're often used to model the same phenomena (some people will use a gamma where others use a lognormal). They are both, for example, constant-coefficient-of-variation models (the CV for the lognormal is $\sqrt{e^{\sigma^2} -1}$, for the gamma it's $1/\sqrt \alpha$).

You might find it instructive to look at the density of their *logs*, which often shows a very clear difference.

The log of a lognormal random variable is ... normal. It's symmetric.

The log of a gamma random variable is left-skew. Depending on the value of the shape parameter, it may be quite skew or nearly symmetric.

Here's an example, with both lognormal and gamma having mean 1 and variance 1/4. The top plot shows the densities (gamma in green, lognormal in blue), and the lower one shows the densities of the logs:

![gamma and lognormal, densitiy and density of log][1]

(Plotting the log of the density of the logs is also useful.)

This difference implies that the gamma has more of a tail on the left, and less of a tail on the right; the far right tail of the lognormal is heavier and its left tail lighter. And indeed, if you look at the skewness, of the lognormal and gamma, for a given coefficient of variation, the lognormal is more right skew ($(e^{\sigma^2}+2) \text{CV}$) than the gamma ($2\text{CV}$).


  [1]: https://i.stack.imgur.com/I9ARM.png",added 373 characters in body,2013-10-09 23:14:21.953
185358,57183,22541.0,2,,CC BY-SA 3.0,361515a0-5bc8-4025-a674-dbaa6c10c840,"Students have completed a test containing 20 questions at both T1 and T2, with an intervention in the interval. Scores for each question are either 0 (incorrect) or 1 (correct). I am interested in knowing whether the improvement in students' scores was significantly greater for some questions than for others. I am thinking that this may involve an extension of the McNemar test, but open to all suggestions. Thanks!",,2013-10-09 23:52:28.193
185359,57183,22541.0,1,,CC BY-SA 3.0,361515a0-5bc8-4025-a674-dbaa6c10c840,Which test should I use to assess for the statistical signficance of changes in multiple binomial dependent variables from T1 to T2?,,2013-10-09 23:52:28.193
185360,57183,22541.0,3,,CC BY-SA 3.0,361515a0-5bc8-4025-a674-dbaa6c10c840,<repeated-measures><multivariate-analysis><binomial-distribution><mcnemar-test>,,2013-10-09 23:52:28.193
185361,57184,1741.0,2,,CC BY-SA 3.0,6cfb513d-5907-41f1-bd6d-6e6d62b3f513,"Here there are some thoughts:

 1. All black-box models might be inspected in some way. You can compute the variable importance for each feature for example or you can also plot the predicted response and the actual one for each feature ([link][1]);
 2. You might think about some pruning of the ensemble. Not all the trees in the forest are necessary and you might use just a few. Paper: [Search for the Smallest Random Forest, Zhang]. Otherwise just Google ""ensemble pruning"", and have a look at ""Ensemble Methods: Foundations and Algorithms
"" [Chapter 6][2];
 3. You can build a single model by feature selection as you said. Otherwise you can also try to use Domingos' method in [Knowledge acquisition from examples via multiple models] that consists in building a new dataset with black-box predictions and build a decision tree on top of it. 
 4. As mentioned in [this][3] Stack Exchange's answer, a tree model might seem interpretable but it is prone to high changes just because of small perturbations of the training data. Thus, it is better to use a black-box model. The final aim of an end user is to understand why a new record is classified as a particular class. You might think about some feature importances just for that particular record.
 
I would go for 1. or 2.

  [1]: http://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest/21457#21457
  [2]: http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/publication.htm
  [3]: http://stats.stackexchange.com/questions/32125/how-to-make-random-forests-more-interpretable/32132#32132",,2013-10-10 00:04:43.820
185405,57192,155.0,5,,CC BY-SA 3.0,ce295a26-359f-4f9a-94e1-272c2f31aedc,"I want to know **how to check a data set for normality in Excel, just to verify that the requirements for using a t-test are being met**.  

For the right tail, is it appropriate to just calculate a mean and standard deviation, add 1, 2 & 3 standard deviations from the mean to create a range then compare that to the normal 68/95/99.7 for the standard normal distribution after using the norm.dist function in excel to test each standard deviation value.

Or is there a better way to test for normality?",added 1 characters in body; edited tags; edited title,2013-10-10 06:05:50.533
185408,57196,668.0,2,,CC BY-SA 3.0,f0bb99df-2174-45dc-a31b-421819814e44,"You have the right idea.  This can be done systematically, comprehensively, and with relatively simple calculations.  A graph of the results is called a *normal probability plot* (or sometimes a Q-Q plot).  From it you can see *much* more detail than appears in other graphical representations, especially [histograms](http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-a-histogram/51753#51753), and with a little practice you can even learn to determine ways to re-express your data to make them closer to Normal in situations where that is warranted.

Here is an example:

![Spreadsheet with probability plot][1]

Data are in column `A` (and named `Data`).  The rest is all calculation, although you can control the ""hinge rank"" value used to fit a reference line to the plot.

This plot is a scatterplot comparing the data to values that would be attained by numbers drawn independently from a standard Normal distribution.  When the points line up along the diagonal, they are close to Normal; horizontal departures (along the data axis) indicate departures from normality.  In this example the points are remarkably close to the reference line; the largest departure occurs at the highest value, which is about $1.5$ units to the left of the line.  Thus we see at a glance that these data are very close to Normally distributed but perhaps have a slightly ""light"" right tail.  This is perfectly fine for applying a t-test.

The comparison values on the vertical axis are computed in two steps.  First each data value is ranked from $1$ through $n$, the amount of data (shown in the `Count` field in cell `F22`).  These are proportionally converted to values in the range $0$ to $1$.  A good formula to use is $\left(\text{rank}+1/6\right)/\left(n+1/3\right).$  Then these are converted to standard Normal values via the `NormSInv` function.  These values appear in the `Normal score` column.  The plot at the right is an XY scatterplot of `Normal Score` against the data.  (In some references you will see the transpose of this plot, which perhaps is more natural, but Excel prefers to place the leftmost column on the horizontal axis and the rightmost column on the vertical axis, so I have let it do what it prefers.)

![Spreadsheet: normal score calculation][2]

(As you can see, I simulated these data with independent random draws from a Normal distribution with mean $5$ and standard deviation $2$.  It is therefore no surprise that the probability plot looks so nice.)  There really are only two formulas to type in, which you propagate downward to match the data: they appear in cells `B2:C2` and rely on the `Count` value computed in cell `F2`.  That's really all there is to it, apart from the plotting.

The rest of this sheet is not necessary but it's helpful for judging the plot: it provides a robust estimate of a reference line.  This is done by picking two points equally far in from the left and right of the plot and connecting them with a line.  In the example these points are the third lowest and third highest, as determined by the $3$ in the `Hinge Rank` cell, `F3`.  As a bonus, its slope and intercept are robust estimates of the standard deviation and mean of the data, respectively.

To plot the reference line, two extreme points are computed and added to the plot: their calculation occurs in columns `I:J`, labeled `X` and `Y`.

![Spreadsheet: reference line calculation][3]


  [1]: https://i.stack.imgur.com/giJwL.png
  [2]: https://i.stack.imgur.com/kS738.png
  [3]: https://i.stack.imgur.com/ZdEYB.png",,2013-10-10 06:11:44.377
185364,57175,20320.0,5,,CC BY-SA 3.0,0307b09e-8a99-40af-ac04-ebd005fe6055,"I have just delved into the basics of maximum likelihood estimation and expectation maximization. The latter is really difficult to follow and I am having a tough time in figuring how I can apply EM method for parameter estimation by minimizing the error given in [paper][1]. The estimation problem is formulated as 

$x_n= s_n(\theta_0) + w_n  Eq(1)$


where $s_n$ is the signal of interest;  $w_n$ is a Gaussian white noise. The parameter estimate is obtained after minimization of $J$ (eq3)

$$\hat{\theta}= \arg  \min_{\theta} J(\theta) Eq(2)$$ and $$J = \sum ||u_{i-1} - u_i||^2$$ Eq(3)

where $u_n$ is a function,  $u_n = x_n - s_n(\theta)$. Minimizing $u_n$ will give us the parameter vector $\theta$ since $u_n$ will converge to $\theta_0$. So, $u_n$ generally becomes a data series. In the paper they have used nearest neighbor search for minimizing J and hence the representation of Eq(3) as a nearest neighbor search. 

In the original way, the search is initialized by taking random values of initial guesses of the parameters and doing a random search, stopping when $J$ is minimized for a given number of iterations, the parameters are incremented after each iteration by a very small number. 

My question is how do I replace EQ(3) and formulate the recursive search for optimal $J$ as EM method. The maximum-likelihood solution for the parameters ,
under the additive white Gaussian noise assumption, corresponds
to minimization of the norm as follows $\theta_{ML} = arg min u_n Eq(4)$

 1. Can somebody give ideas of of solving Eq(4) as ML i.e how do I minimize J using ML? 
 2. I really do not understand how to replace Eq (3) by ML based minimization since I cannot understand the formulation of the likelihood and other technical information. 
Thank you.

  [1]: http://www.eurasip.org/Proceedings/Eusipco/Eusipco2000/SESSIONS/THUAM/OR1/CR1473.PDF",deleted 125 characters in body,2013-10-10 00:42:12.537
185368,57185,,1,user10619,CC BY-SA 3.0,9005c5f8-25d5-4cc3-a5c9-0447eba79eb3,What is the difference between the concept and treatment of measurement error under the psycumetry and under statistical analysis?,,2013-10-10 00:46:42.743
185367,57185,,2,user10619,CC BY-SA 3.0,9005c5f8-25d5-4cc3-a5c9-0447eba79eb3,There is some confusion with respect to the measurement error. What is the definition in statistics and definition in psychometry? Does bias imply the same thing under two disciplines?,,2013-10-10 00:46:42.743
185366,57185,,3,user10619,CC BY-SA 3.0,9005c5f8-25d5-4cc3-a5c9-0447eba79eb3,<bias><measurement-error>,,2013-10-10 00:46:42.743
185370,57185,594.0,4,,CC BY-SA 3.0,061f74cd-3dbd-49e8-b1f2-dbd600ca568f,What is the difference between the concept and treatment of measurement error under the psychometry and under statistical analysis?,added 4 characters in body; edited title,2013-10-10 00:48:12.213
185369,57185,594.0,5,,CC BY-SA 3.0,061f74cd-3dbd-49e8-b1f2-dbd600ca568f,"There is some confusion with respect to the measurement error. What is the definition in statistics and definition in psychometry? 

Does bias imply the same thing under two disciplines?",added 4 characters in body; edited title,2013-10-10 00:48:12.213
185374,57186,22542.0,3,,CC BY-SA 3.0,7fcd301a-12a1-4a3c-95b5-99dc883358a0,<sample-size><proportion><group-differences>,,2013-10-10 01:51:45.710
185373,57186,22542.0,1,,CC BY-SA 3.0,7fcd301a-12a1-4a3c-95b5-99dc883358a0,Sample sizes for differences between three groups,,2013-10-10 01:51:45.710
185372,57186,22542.0,2,,CC BY-SA 3.0,7fcd301a-12a1-4a3c-95b5-99dc883358a0,"Quick question - I am doing a study of two schools, and at each school I am sampling three groups.

I will be asking each group various questions about how they feel about the school, etc.

I want to be able to detect:

a) a difference of 20% in responses between the schools (e.g., proportion of students for whom current school was first school of preference), and

b) *within* each school, 20% difference in responses between the three different groups (similar questions, but for students who enrolled in different eras).

In both instances, I would like power of 80% for 0.05 significance.

So, if I do what I think is the appropriate calculation, I effectively come up with
 
n = (0.5(0.84 + 1.96)^2)/0.2^2
= 98

Can I just assume then that I need three groups of 99 within each school? I guess I'm confused because nobody ever seems to talk about calculating sample sizes when comparing more than two groups.  

Furthermore, is there anything wrong with sampling the three groups of 98 at each school and assuming that the total sample of 294 at each school will be sufficient to detect the 20% difference between the two schools?

Thank you for the assistance, and I apologise if my english is not clear :)",,2013-10-10 01:51:45.710
185376,57186,10060.0,5,,CC BY-SA 3.0,3b9a5776-68dd-46b0-9f21-fca716f0eaf8,"Quick question - I am doing a study of two schools, and at each school I am sampling three groups.

I will be asking each group various questions about how they feel about the school, etc.

I want to be able to detect:

a) a difference of 20% in responses between the schools (e.g., proportion of students for whom current school was first school of preference), and

b) *within* each school, 20% difference in responses between the three different groups (similar questions, but for students who enrolled in different eras).

In both instances, I would like power of 80% for 0.05 significance.

So, if I do what I think is the appropriate calculation, I effectively come up with
 
$n = \frac{0.5\times(0.84+1.96)^2}{0.2^2} = 98$

Can I just assume then that I need three groups of 99 within each school? I guess I'm confused because nobody ever seems to talk about calculating sample sizes when comparing more than two groups.  

Furthermore, is there anything wrong with sampling the three groups of 98 at each school and assuming that the total sample of 294 at each school will be sufficient to detect the 20% difference between the two schools?

Thank you for the assistance, and I apologise if my english is not clear :)",Change formula to LaTeX expression,2013-10-10 02:27:45.560
185377,57186,5237.0,5,,CC BY-SA 3.0,dca2c6aa-f48c-4a37-822b-17bb865cc526,"I am doing a study of two schools, and at each school I am sampling three groups. I will be asking each group various questions about how they feel about the school, etc. I want to be able to detect:

1. a difference of 20% in responses between the schools (e.g., proportion of students for whom current school was first school of preference), and
2. *within* each school, 20% difference in responses between the three different groups (similar questions, but for students who enrolled in different eras).

In both instances, I would like power of 80% for 0.05 significance.

So, if I do what I think is the appropriate calculation, I effectively come up with:
\begin{align} 
n &= \frac{0.5(0.84 + 1.96)^2}{0.2^2}  \\
  &= 98
\end{align}
Can I just assume then that I need three groups of 99 within each school? I guess I'm confused because nobody ever seems to talk about calculating sample sizes when comparing more than two groups.  

Furthermore, is there anything wrong with sampling the three groups of 98 at each school and assuming that the total sample of 294 at each school will be sufficient to detect the 20% difference between the two schools?
",changed tag; formatted; removed peripheral comments; light editing,2013-10-10 02:27:55.877
185378,57186,5237.0,6,,CC BY-SA 3.0,dca2c6aa-f48c-4a37-822b-17bb865cc526,<proportion><statistical-power><group-differences>,changed tag; formatted; removed peripheral comments; light editing,2013-10-10 02:27:55.877
185379,57187,9792.0,2,,CC BY-SA 3.0,6084d71e-6338-4e2c-96e9-f23881bbbbe2,"I am having trouble understanding why centering seems to only work with simple linear models and not with splines for example. I am using centering to report the estimated group differences at different $x$, but also statistical values (ignoring multiple comparisons for the moment).

    set.seed(1)
    
    # simulate data
    N <- 10
    x <- rep(seq(0.2,1,0.2),N)
    group <- factor(rep(c('I','II'),each=length(x)/N))
    y <- -x^2 + 2*x*as.numeric(group) + rnorm(length(x),mean=0,sd=0.1)
    d <- data.frame(group,x,y)
    
    # fit a linear model with x-group interaction
    l <- lm(y~x*group,data=d)
    d$lmfit <- fitted(l)
    coef(l)['groupII'] # group difference at x==0
    #     groupII 
    #  -0.1097071 
    
    library(ggplot2)
    ggplot(d,aes(x,y,colour=group)) + geom_point() + geom_line(aes(x,lmfit,colour=group))

The plot confirms the reported small group difference `groupII` of 0.05 at $x=0$ if we were to extrapolate back to 0.

Now let us centre the data at $x=1$ and estimate the group difference there.
    
    # center data at x==1 and refit
    l <- lm(y~I(x-1)*group,data=d)
    coef(l)['groupII'] # group difference at x==1
    #   groupII 
    #  2.08525 

In agreement with the plot the difference is about 2.

Now let us fit a spline model.    
    
    # fit data with splines
    library(splines)
    l <- lm(y~ns(x,2)*group,data=d)
    d$lmsplinefit <- fitted(l)
    coef(l)['groupII'] # group difference at x==0.2
    #     groupII 
    #  0.2987893 
    # compare to: d$lmsplinefit[6] - d$lmsplinefit[1]
    
    ggplot(d,aes(x,y,colour=group)) + geom_point() + geom_line(aes(x,lmsplinefit,colour=group))
    

Interestingly, the spline fit reports the group difference at the first $x$, i.e. $x=0.2$.

If we try to centre at $x=1$ we get the same result, i.e. the difference at $x=0.2$.

    l <- lm(y~ns(I(x-1),2)*group,data=d)
    coef(l)['groupII']
    # same result as un-centered data, i.e. 0.2987893

Why is that? And is there a way to show the group difference at a different $x$? Btw, centering $x$ manually before the model fit does not make a difference.",,2013-10-10 02:44:04.330
185382,57188,594.0,2,,CC BY-SA 3.0,42db66eb-ab96-4de8-b768-18f2560cc2bb,"

Here's the probability function and distribution of the proportion of True (plus the normal approximation at which the chi-square will be exact) in a sample of size 10,000 and a proportion of True of only 1% (right below the low end of your suggested total sample size, and with expected number of True only one fifth of your suggested minimum):


![enter image description here][1]

Don't forget that you'll have about five times the expected successes shown here; your approximation will be much better than this.

A straight two sample proportions test or a chi-square test should do just fine. Indeed, one tenth of your proportion of True's would be just fine. One *hundredth*, you'd just go to exact methods.

  [1]: https://i.stack.imgur.com/Se0Qx.png",,2013-10-10 02:45:38.470
185388,57189,22545.0,3,,CC BY-SA 3.0,41d9cae3-1d83-4d0a-be24-7e11018d0589,<probability><variance><expected-value><mathematical-statistics>,,2013-10-10 04:00:27.487
185387,57189,22545.0,1,,CC BY-SA 3.0,41d9cae3-1d83-4d0a-be24-7e11018d0589,Theoretical expected value and variance,,2013-10-10 04:00:27.487
185386,57189,22545.0,2,,CC BY-SA 3.0,41d9cae3-1d83-4d0a-be24-7e11018d0589,"Let $X$ be a random variable having expected value $\mu$ and variance $\sigma^2$. Find the Expected Value and Variance of $Y = \frac{X−\mu}{\sigma}$.

I would like to show some progress I've made so far, but honestly I've been thinking about this problem for the past few days but just have no idea where to start. Any hint or insight on a starting point would be much appreciated.

Thanks!",,2013-10-10 04:00:27.487
185389,57190,449.0,2,,CC BY-SA 3.0,6f725d2f-b050-45ee-adc9-e08170e488fd,"You can use multi-level logistic regression. You've only got one dependent variable, correctness. You have multiple independent variables nested within student. In R you can use `lmer` to construct the model. It would look something like.

    m <- lmer( answer ~ treatment * Q + (treatment * Q | student), family = 'binomial', data = mydata)

That would allow for there to be random effects of question and treatment within student as well as overall correctness variability within student but also be able to assess fixed effects if treatment and question. What seem to really want to know is all of the treatment by question interactions.

In order to analyze all of the questions with any kind of reliability you really should have a lot of students taking the test (hundreds). The general effect of treatment could be assessed with fewer.",,2013-10-10 04:04:00.123
185390,57189,594.0,6,,CC BY-SA 3.0,0ac0ceb1-76d7-42d8-a91a-c29fd3e6744a,<probability><self-study><variance><expected-value><mathematical-statistics>,edited tags,2013-10-10 04:04:07.093
185391,57191,594.0,2,,CC BY-SA 3.0,ab2371ee-34c4-4a1a-8472-e7a71cb3eddc,"Have you seen the following basic properties of expectation and variance?

(I'd be very surprised if some version of these hadn't been discussed)


$\text{E}(aX+b) = a\text{E}(X)+b$

$\text{Var}(aX+b) = a^2\text{Var}(X)$


http://en.wikipedia.org/wiki/Expected_value#Linearity

http://en.wikipedia.org/wiki/Variance#Basic_properties

If you apply these properties, or better, the versions you'll already have been given, the problem is trivial.

If you still can't see it, try finding $\text{E}(X-\mu)$ first and work from there.",,2013-10-10 04:06:18.590
185394,57192,22031.0,3,,CC BY-SA 3.0,09a9dc49-6c22-4325-b904-3662a9f0f6b1,<normal-distribution>,,2013-10-10 04:41:49.360
185393,57192,22031.0,1,,CC BY-SA 3.0,09a9dc49-6c22-4325-b904-3662a9f0f6b1,Check for Normal Distribution in Excel,,2013-10-10 04:41:49.360
185392,57192,22031.0,2,,CC BY-SA 3.0,09a9dc49-6c22-4325-b904-3662a9f0f6b1,"I'm looking for a way to check a data set for normality in Excel, just to verify that the requirements for using a t-test are being met.  

For the right tail, is it appropriate to just calculate a mean and standard deviation, add 1, 2 & 3 standard deviations from the mean to create a range then compare that to the normal 68/95/99.7 for the standard normal distribution after using the norm.dist function in excel to test each standard deviation value.

Or is there a better way to test for normality?",,2013-10-10 04:41:49.360
185395,57186,22542.0,5,,CC BY-SA 3.0,2f9baea0-e9e5-474c-b160-d0a349e681e4,"I am doing a study of two schools, and at each school I am sampling three groups. I am trying to determine why the person chose to go to that school and not another. I will be asking each group various questions about the school and options they may  have had for other schools, etc. I want to be able to detect:

1. a difference of 20% in responses between the schools (e.g., proportion of students for whom current school was first school of preference, or proportion for whom there was no other choice), and
2. *within* each school, 20% difference in responses between the three different groups (similar questions, but for students who enrolled in different eras).

In both instances, I would like power of 80% for 0.05 significance.

So, if I do what I think is the appropriate calculation, I effectively come up with:
\begin{align} 
n &= \frac{0.5(0.84 + 1.96)^2}{0.2^2}  \\
  &= 98
\end{align}
Can I just assume then that I need three groups of 99 within each school? I guess I'm confused because nobody ever seems to talk about calculating sample sizes when comparing more than two groups.  

Furthermore, is there anything wrong with sampling the three groups of 98 at each school and assuming that the total sample of 294 at each school will be sufficient to detect the 20% difference between the two schools?
",added 163 characters in body,2013-10-10 05:24:59.993
185410,57197,22507.0,2,,CC BY-SA 3.0,82ac84ed-08f3-440d-b0ce-ad1fc46d5877,"If you select the equal number of cases and non-cases, it will bias the model.  For example, suppose that the features have zero correlation with the outcome, and the dataset is very large.  The model will predict the same probability of lung cancer for all patients.  If you select equal number of positive and negative example, the predicted probability will be 0.5, while in reality it is 0.1 .",,2013-10-10 06:21:56.940
186091,57378,668.0,10,,,1feab5a9-64fe-4d66-a743-7a136cd6487b,"{""OriginalQuestionIds"":[31],""Voters"":[{""Id"":919,""DisplayName"":""whuber""}]}",101,2013-10-13 16:31:36.877
186094,57394,16046.0,1,,CC BY-SA 3.0,e7e70389-f554-4886-b5ae-65780f3c30a0,Causality in Time Series,,2013-10-13 16:53:32.577
185397,57193,1717.0,2,,CC BY-SA 3.0,4491d577-0526-465d-b41b-cf0996d4bb31,"When you use EM to obtain maximum likelihood estimates, you need a variable that describes your observations $x_{n}$, latent variables $z_{n}$ that are in some way related to your observations (e.g. in coin tossing experiments, $\{H, T\}$ are the latent variables and in gaussian mixtures, the mixing coefficients $\pi_{i}$ take the role of latent variables) and the parameters $\theta$ that you are trying to estimate.

At the risk of not answering your question at all, I think you want a maximum likelihood estimate of $\theta$ using EM based on known observations $x_{n}$ that are given by the following equation:

$$x_{t} = s_{t}(\theta_{0}) + n_{t}$$

If that is correct, a general idea is the following. Since $n_{t}$ is white noise $N(0, \sigma)$, $x_{t}$ can be described by a Gaussian $p(x_{t}|s_{t},\theta) = N(s_{t}(\theta), \sigma)$. In the EM formulation, $x_{t}$'s are known variables, $s_{t}$'s are latent variables and $\theta$ is the parameter. It is customary to group the variables $x_{n}$ in a variable $X$ and likewise, latent variables $s_{n}$ are grouped in a variable $S$.

As you should know, the EM algorithm consists of 2 steps: expectation and maximization. In the expectation step, we use an expression $Q$ as a proxy for the likelihood $L(\theta|X) = p(X|\theta)$, that is, the probability of 
getting the known data $X$ given a parameter $\theta$. This is the same likelihood used to obtain maximum likelihood estimates. However, in EM we use this $Q$ instead:

$$Q(\theta|\theta^{\text{old}}) = E_{S|X, \theta^{\text{old}}} \log p(X,S|\theta)$$

This odd-looking expression is actually a lower bound of the likelihood $L(\theta|X)$. [Bishop's book][1] contains a good derivation of $Q$.

In order to start the EM magic, you have to choose a random $\theta^{\text{old}}$ and calculate this expectation. Notice that you need $p(X,S|\theta)$ and $p(S|X,\theta^{\text{old}})$. $p(X,S|\theta)$ is equal to $p(X|S,\theta)p(S|\theta)$ and using Bayes' theorem, $p(S|X,\theta^{\text{old}})$ is 
proportional to $p(X|S,\theta^{\text{old}})p(S|\theta^{\text{old}})$.

At this point, I hope it is clear that $p(X|S,\theta)=\prod_{t} p(x_{t}|s_{t},\theta)$, so that part is not hard to calculate. However, $p(S|\theta)$, that is, $\prod_{t}p(s_{t}|\theta)$ is required. I don't know what distribution could be appropriate since this depends on the specifics of your problem so I will assume you know.

By now, you can calculate $Q(\theta|\theta^{\text{old}})$. 

The maximization step is simply:

$$\theta = \text{arg max}_{\theta} Q(\theta|\theta^{\text{old}})$$

This is the new $\theta$ to be used in the expectation step again until convergence. 

That is a general idea of how EM could work in this case. However, maybe you don't know a distribution for $s_{t}$ or it is difficult to calculate the expectation or the maximization step. 

For the big picture, take a look at [this][2] nice explanation.


  [1]: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738
  [2]: https://docs.google.com/viewer?url=http://www.nature.com/nbt/journal/v26/n8/pdf/nbt1406.pdf",,2013-10-10 05:41:08.343
185398,57194,155.0,2,,CC BY-SA 3.0,42f13a0c-bc2c-492d-9e93-7f5340bf88bb,"You could [plot a histogram using the data analysis toolpack in Excel](http://office.microsoft.com/en-au/excel-help/create-a-histogram-HP001098364.aspx). Graphical approaches are more likely to communicate the degree of non-normality, which is typically more relevant for assumption testing (see [this discussion of normality](http://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless). 

The data analysis toolpack in Excel will also give you skewness and kurtosis if you ask for descriptive statistics and choose the ""summary statistics"" option.",,2013-10-10 05:50:58.150
185401,57195,22547.0,2,,CC BY-SA 3.0,1ed5a49f-bbf7-4655-8441-eb3e15fdfd14,"I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). 

I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. 

To start with, I took a subset of the data for the North East. I tried grouping the data by calendar month and plotting the correlation between all points (below). This shows how the 30+ data points from January, February, etc. are correlated between different stations in this area. 

![correlation between daily data during each calendar month][1]

  [1]: https://i.stack.imgur.com/X4YZI.jpg

The plots seem to show that the values measured by the stations in the south of this domain have higher correlation than with others, but the relative density of connections in the networks means that any nuance is completely washed out. I also wonder if those southern stations aren't just the last ones to plot...

Unfortunately, there is simply too much data to make sense of on one plot, and that can't be fixed by reducing the size of the lines. I suspect what I have to do is simplify the graph of connections between the stations (nodes) in my network, probably choosing between `k` nearest neighbours or some sphere of influence approach to chose the stations (I don't know how to do this). I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.

I'm learning my way into this field and R at the same time, and would appreciate suggestions on:

 1. What's the correct name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.
 2. Are there more appropriate methods to show the correlation between multiple data sets separated in space?
 3. ... in particular, methods that are easy to show results from visually?
 4. Are any of these implemented in R?
 5. Do any of these approaches lend themselves to automation?

",,2013-10-10 05:52:03.253
185400,57195,22547.0,1,,CC BY-SA 3.0,1ed5a49f-bbf7-4655-8441-eb3e15fdfd14,Showing spatial and temporal correlation on maps,,2013-10-10 05:52:03.253
185399,57195,22547.0,3,,CC BY-SA 3.0,1ed5a49f-bbf7-4655-8441-eb3e15fdfd14,<r><regression><spatial>,,2013-10-10 05:52:03.253
185402,57193,1717.0,5,,CC BY-SA 3.0,a6a6fcae-bcfd-4b7d-98fb-c48abf5f5e4b,"When you use EM to obtain maximum likelihood estimates, you need a variable that describes your observations $x_{n}$, latent variables $z_{n}$ that are in some way related to your observations (e.g. in coin tossing experiments, $\{H, T\}$ are the latent variables and in gaussian mixtures, the mixing coefficients $\pi_{i}$ take the role of latent variables) and the parameters $\theta$ that you are trying to estimate.

At the risk of not answering your question at all, I think you want a maximum likelihood estimate of $\theta$ using EM based on known observations $x_{n}$ that are given by the following equation:

$$x_{t} = s_{t}(\theta_{0}) + n_{t}$$

If that is correct, a general idea is the following. Since $n_{t}$ is white noise $N(0, \sigma)$, $x_{t}$ can be described by a Gaussian $p(x_{t}|s_{t},\theta) = N(s_{t}(\theta), \sigma)$. In the EM formulation, $x_{t}$'s are known variables, $s_{t}$'s are latent variables and $\theta$ is the parameter. It is customary to group the variables $x_{n}$ in a variable $X$ and likewise, latent variables $s_{n}$ are grouped in a variable $S$.

As you should know, the EM algorithm consists of 2 steps: expectation and maximization. In the expectation step, we use an expression $Q$ as a proxy for the likelihood $L(\theta|X) = p(X|\theta)$, that is, the probability of 
getting the known data $X$ given a parameter $\theta$. This is the same likelihood used to obtain maximum likelihood estimates. However, in EM we use this $Q$ instead:

$$Q(\theta|\theta^{\text{old}}) = E_{S|X, \theta^{\text{old}}} \log p(X,S|\theta)$$

This odd-looking expression is actually a lower bound of the likelihood $L(\theta|X)$. [Bishop's book][1] contains a good derivation of $Q$.

In order to start the EM magic, you have to choose a random $\theta^{\text{old}}$ and calculate this expectation. Notice that you need $p(X,S|\theta)$ and $p(S|X,\theta^{\text{old}})$. $p(X,S|\theta)$ is equal to $p(X|S,\theta)p(S|\theta)$ and using Bayes' theorem, $p(S|X,\theta^{\text{old}})$ is 
proportional to $p(X|S,\theta^{\text{old}})p(S|\theta^{\text{old}})$.

At this point, I hope it is clear that $p(X|S,\theta)=\prod_{t} p(x_{t}|s_{t},\theta)$, so that part is not hard to calculate. However, $p(S|\theta)$, that is, $\prod_{t}p(s_{t}|\theta)$ is required. I don't know what distribution could be appropriate since this depends on the specifics of your problem so I will assume you know.

By now, you can calculate $Q(\theta|\theta^{\text{old}})$. 

The maximization step is simply:

$$\theta = \text{arg max}_{\theta} Q(\theta|\theta^{\text{old}})$$

This is the new $\theta$ to be used in the expectation step again until convergence. 

That is a general idea of how EM could work in this case. However, maybe you don't know a distribution for $s_{t}$ or it is difficult to calculate the expectation or the maximization step. 

For the big picture, take a look at [this][2] nice explanation.

**UPDATE**

I think you changed the question quite a bit. Are you asking how to calculate maximum likelihood estimates? Basically, you apply a derivative to the likelihood on the parameter you want to estimate:

$$\frac{\partial}{\partial \theta}L(\theta|X) = 0$$

solve it and that's pretty much it. See more examples [here][3].


  [1]: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738
  [2]: https://docs.google.com/viewer?url=http://www.nature.com/nbt/journal/v26/n8/pdf/nbt1406.pdf
  [3]: http://en.wikipedia.org/wiki/Maximum_likelihood#Continuous_distribution.2C_continuous_parameter_space",added update,2013-10-10 05:57:41.453
185403,57194,155.0,5,,CC BY-SA 3.0,30640fce-c1c3-4a74-afd6-87aa48cc4d27,"You could [plot a histogram using the data analysis toolpack in Excel](http://office.microsoft.com/en-au/excel-help/create-a-histogram-HP001098364.aspx). Graphical approaches are more likely to communicate the degree of non-normality, which is typically more relevant for assumption testing (see [this discussion of normality](http://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless). 

The data analysis toolpack in Excel will also give you [skewness and kurtosis](http://graphpad.com/guides/prism/6/statistics/index.htm?stat_skewness_and_kurtosis.htm) if you ask for descriptive statistics and choose the ""summary statistics"" option. You might for example consider values of skewness above plus or minus one be a form of substantive non-normality. 

That said, the assumption with t-tests is that the residuals are normally distributed and not the variable. Furthermore, they also quite robust such that even with fairly large amounts of non-normality, p-values are still fairly valid.",added 444 characters in body,2013-10-10 05:59:12.390
185404,57195,22547.0,5,,CC BY-SA 3.0,0179ea7e-9ed5-44c5-89e4-5ecb9259d0a7,"I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). 

I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. 

To start with, I took a subset of the data for the North East. I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. 

![correlation between daily data during each calendar month][1]

  [1]: https://i.stack.imgur.com/X4YZI.jpg

The plots show higher correlation between the stations in the south than between other pairs. Unfortunately, there is simply too much data to make sense of on one plot, and that can't be fixed by reducing the size of the lines. I suspect what I have to do is simplify the graph of connections between the stations (nodes) in my network, probably choosing between `k` nearest neighbours or some sphere of influence approach to chose the stations (I don't know how to do this). I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.

I'm learning my way into this field and R at the same time, and would appreciate suggestions on:

 1. What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.
 2. Are there more appropriate methods to show the correlation between multiple data sets separated in space?
 3. ... in particular, methods that are easy to show results from visually?
 4. Are any of these implemented in R?
 5. Do any of these approaches lend themselves to automation?

",clarified method used and question,2013-10-10 06:01:43.267
185411,57194,155.0,5,,CC BY-SA 3.0,63d64309-796a-4997-a946-5a05af52165f,"You could [plot a histogram using the data analysis toolpack in Excel](http://office.microsoft.com/en-au/excel-help/create-a-histogram-HP001098364.aspx). Graphical approaches are more likely to communicate the degree of non-normality, which is typically more relevant for assumption testing (see [this discussion of normality](http://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless)). 

The data analysis toolpack in Excel will also give you [skewness and kurtosis](http://graphpad.com/guides/prism/6/statistics/index.htm?stat_skewness_and_kurtosis.htm) if you ask for descriptive statistics and choose the ""summary statistics"" option. You might for example consider values of skewness above plus or minus one be a form of substantive non-normality. 

That said, the assumption with t-tests is that the residuals are normally distributed and not the variable. Furthermore, they also quite robust such that even with fairly large amounts of non-normality, p-values are still fairly valid.",added 1 characters in body,2013-10-10 06:24:06.637
185414,57198,22548.0,3,,CC BY-SA 3.0,84c6c1eb-7d1b-4327-b1b5-79df42387088,<regression><least-squares><error>,,2013-10-10 06:34:08.830
185413,57198,22548.0,1,,CC BY-SA 3.0,84c6c1eb-7d1b-4327-b1b5-79df42387088,what is random error in OLS regression? and the differnce with Gaussian noise?,,2013-10-10 06:34:08.830
185412,57198,22548.0,2,,CC BY-SA 3.0,84c6c1eb-7d1b-4327-b1b5-79df42387088,"I'm new in statistics, and I would be thankful if you help me.

in OLS regression:  Y = β0 + β1 X1 + β2 X2 + β3 X3 + β4 X4 + β5 X5 + β6 X6 + ξ 

what is ξ ? is it Gaussian noise or random error? what is a difference why we add it to multiple regression model? In most of papers authors refer it to random error but without clarification.
I need a simple and good reason why authors add it to their model..

",,2013-10-10 06:34:08.830
185415,57199,22551.0,2,,CC BY-SA 3.0,df40d321-4e91-4174-945d-18d0fe55fbbe,"Poisson/Negative binomial can also be used with a binary outcome with offset equal to one. Of course it necessitates that the data be from a prospective design (cohort, rct, etc). Poisson or NB regression gives the more appropriate effect measure (IRR) versus odds ratio from logistic regression.

NB regression is ""safer"" to run than Poisson regression because even if the overdispersion parameter (alpha in Stata) is not statistically significant, the results will be exactly the same as its Poisson regression form.",,2013-10-10 07:32:59.240
185417,57200,15563.0,2,,CC BY-SA 3.0,4d6a5617-4e45-4a9e-a88c-e0fa3cc887c6,"I am using the [Kaggle Scikit][1] data to learn R.

I am using the R e1071 SVM function to predict classes.

When I use:

    svm(train, trainLabels, scale = TRUE, type = NULL, kernel = ""polynomial"")

I obtain this level of accuracy on a sample of the Train data:

    > table(pred, trainLabels)
        trainLabels
    pred   0   1
       0 478   8
    1	12 502

which I interpret as being 98% accurate (8+12) / (478+8+12+502).

Though when I use the same prediction model on the Test data, Kaggle returns a **0.82** score, [based on classification accuracy][2].

Can you explain why I can get such a different accuracy level?


  [1]: https://www.kaggle.com/c/data-science-london-scikit-learn/
  [2]: https://www.kaggle.com/c/data-science-london-scikit-learn/details/evaluation",,2013-10-10 08:09:22.140
185419,57200,15563.0,3,,CC BY-SA 3.0,4d6a5617-4e45-4a9e-a88c-e0fa3cc887c6,<r><machine-learning><svm>,,2013-10-10 08:09:22.140
185418,57200,15563.0,1,,CC BY-SA 3.0,4d6a5617-4e45-4a9e-a88c-e0fa3cc887c6,SVM prediction accuracy drops when using Test data,,2013-10-10 08:09:22.140
185420,57201,21762.0,2,,CC BY-SA 3.0,dc589773-5245-4e18-8535-7d4c012cc359,"A simple approach would be the following: 

1) Take all observations sampled at random under condition A and obtain the *relevant one sided* p-value from Wilcoxon's rank sum test.

2) Do the same for the observations sampled under condition B.

3) If the smaller of the two p-values is below the level $\alpha/2$ and the other p-value is below $\alpha$, then your claim holds at the $\alpha$ level. (This would be the Bonferroni-Holm correction for multiple testing.)

Since the sample sizes are extremely low, you will get a ""significant"" result only if the signal is very strong.",,2013-10-10 08:18:09.707
185422,57198,1406.0,4,,CC BY-SA 3.0,e38bca1c-fa2e-48e6-ba30-01b811878fd2,What is random error in OLS regression? And how is it related to Gaussian noise?,deleted 2 characters in body; edited title,2013-10-10 08:26:56.503
185421,57198,1406.0,5,,CC BY-SA 3.0,e38bca1c-fa2e-48e6-ba30-01b811878fd2,"In OLS regression:  

$$Y=\beta_0+\beta_1 X_1+ \beta_2 X_2+\beta_3 X_3 + \beta_4 X_4+\beta_5 X_5+\beta_6 X_6 + \varepsilon,$$


what is $\varepsilon$? Is it Gaussian noise or random error? What is a difference? Why we add it to multiple regression model? In most of papers authors refer it to random error but without clarification.

I need a simple and good reason why authors add it to their model..

",deleted 2 characters in body; edited title,2013-10-10 08:26:56.503
185423,57202,14799.0,2,,CC BY-SA 3.0,e7fe806b-7c5a-4ea5-8899-9e49d3ea9247,"This may a problem of interpretation, a misunderstanding what a so-called ""direct effect"" coefficient really is.

In regression models with continuous predictor variables and no interaction terms -- that is, with no terms that are constructed as the product of other terms -- each variable's coefficient is the slope of the regression surface in the direction of that variable. It is constant, regardless of the values of the variables, and is obviously a measure of the effect of that variable.

In models with interactions -- that is, with terms that are constructed as the products of other terms -- that interpretation can be made without further qualification only for variables that are **not** involved in any interactions. The coefficient of a variable that **is** involved in interactions is the slope of the regression surface in the direction of that variable **when the values of all the variables that interact with the variable in question are zero**, and the significance test of the coefficient refers to the slope of the regression surface **only in that region of the predictor space**. Since there is no requirement that there actually be data in that region of the space, the apparent direct effect coefficient may bear little resemblance to the slope of the regression surface in the region of the predictor space where data were actually observed. There is no true ""direct effect"" in such cases; the best substitute is probably the ""average effect"": the slope of the regression surface in the direction of the variable in question, taken at each data point and averaged over all data points. For more on this, see http://stats.stackexchange.com/questions/65898/answer/65917",,2013-10-10 08:31:00.453
185433,57205,22558.0,2,,CC BY-SA 3.0,08f26015-a044-4186-b60a-83d32509736b,"I would not touch the data at all. Use this for autocorrelation with NaNs:

http://www.mathworks.com/matlabcentral/fileexchange/43840-autocorrelation-and-partial-autocorrelation-with-nans/content/nanautocorr.m",,2013-10-10 09:58:27.403
185575,57248,10278.0,5,,CC BY-SA 3.0,09c07e23-6e2f-4df3-a2be-238b8fa63408,"If you treat the variable as ordinal you are assuming a gene-dosage effect.
This is essentially a two degree of freedom test.  You are doing a regression.
If you treat the variable as nominal you are not assuming any gene-dosage effect and instead comparing the mean of the three genotype groups this is a one degree of freedom test.
Hence the gene-dosage model (treating genotypes as ordinal) is more powerful.
",added 30 characters in body,2013-10-10 19:34:30.353
185425,57177,594.0,5,,CC BY-SA 3.0,b4c4d007-b8e9-455d-ac98-24914b0d487d,"As for qualitative differences, the lognormal and gamma are, as you say, quite similar. 

Indeed, in practice they're often used to model the same phenomena (some people will use a gamma where others use a lognormal). They are both, for example, constant-coefficient-of-variation models (the CV for the lognormal is $\sqrt{e^{\sigma^2} -1}$, for the gamma it's $1/\sqrt \alpha$).

[How can it be constant if it depends on a parameter, you ask? It applies when you model the scale (location for the log scale); for the lognormal, $\mu$ acts as a scale parameter, while for the gamma, the scale is the parameter that isn't the shape parameter (or its reciprocal if you use the shape-rate parameterization). I'll call the scale parameter for the gamma distribution $\beta$. Gamma GLMs model the mean ($\mu=\alpha\beta$) while holding $\alpha$ constant; in that case $\mu$ is also a scale parameter. A model with varying $\mu$ and constant $\alpha$ or $\sigma$ respectively will have constant CV.]

You might find it instructive to look at the density of their *logs*, which often shows a very clear difference.

The log of a lognormal random variable is ... normal. It's symmetric.

The log of a gamma random variable is left-skew. Depending on the value of the shape parameter, it may be quite skew or nearly symmetric.

Here's an example, with both lognormal and gamma having mean 1 and variance 1/4. The top plot shows the densities (gamma in green, lognormal in blue), and the lower one shows the densities of the logs:

![gamma and lognormal, densitiy and density of log][1]

(Plotting the log of the density of the logs is also useful. That is, taking a log-scale on the y-axis above)

This difference implies that the gamma has more of a tail on the left, and less of a tail on the right; the far right tail of the lognormal is heavier and its left tail lighter. And indeed, if you look at the skewness, of the lognormal and gamma, for a given coefficient of variation, the lognormal is more right skew ($\text{CV}^3+3\text{CV}$) than the gamma ($2\text{CV}$).


  [1]: https://i.stack.imgur.com/I9ARM.png",gave a 'simpler' form for lognormal skewness,2013-10-10 08:40:22.340
185426,57203,633.0,2,,CC BY-SA 3.0,d8b9fa3f-b3f3-4de4-b435-dd770c51c19c,"Yes, the gamma distribution is the maximum entropy distribution for which the mean $E(X)$ and mean-log $E(\log X)$ are fixed.  As with all exponential family distributions, it is the unique maximum entropy distribution for a fixed expected sufficient statistic.",,2013-10-10 09:03:03.037
185427,57202,12683.0,5,,CC BY-SA 3.0,af5bd193-6764-4fd4-a93f-0be534858189,"This may be a problem of interpretation, a misunderstanding of what a so-called ""direct effect"" coefficient really is.

In regression models with continuous predictor variables and no interaction terms -- that is, with no terms that are constructed as the product of other terms -- each variable's coefficient is the slope of the regression surface in the direction of that variable. It is constant, regardless of the values of the variables, and is obviously a measure of the effect of that variable.

In models with interactions -- that is, with terms that are constructed as the products of other terms -- that interpretation can be made without further qualification only for variables that are **not** involved in any interactions. The coefficient of a variable that **is** involved in interactions is the slope of the regression surface in the direction of that variable **when the values of all the variables that interact with the variable in question are zero**, and the significance test of the coefficient refers to the slope of the regression surface **only in that region of the predictor space**. Since there is no requirement that there actually be data in that region of the space, the apparent direct effect coefficient may bear little resemblance to the slope of the regression surface in the region of the predictor space where data were actually observed. There is no true ""direct effect"" in such cases; the best substitute is probably the ""average effect"": the slope of the regression surface in the direction of the variable in question, taken at each data point and averaged over all data points. For more on this, see http://stats.stackexchange.com/questions/65898/answer/65917",fixed typos,2013-10-10 09:11:52.483
185428,57175,12683.0,5,,CC BY-SA 3.0,a1c72a30-45e4-489a-b54d-d8637b93b401,"I have just delved into the basics of maximum likelihood estimation and expectation maximization. The latter is really difficult to follow and I am having a tough time in figuring how I can apply the EM method for parameter estimation by minimizing the error given in [paper][1]. The estimation problem is formulated as 

$x_n= s_n(\theta_0) + w_n  Eq(1)$


where $s_n$ is the signal of interest;  $w_n$ is a Gaussian white noise. The parameter estimate is obtained after minimization of $J$ (eq3)

$$\hat{\theta}= \arg  \min_{\theta} J(\theta) Eq(2)$$ and $$J = \sum ||u_{i-1} - u_i||^2$$ Eq(3)

where $u_n$ is a function,  $u_n = x_n - s_n(\theta)$. Minimizing $u_n$ will give us the parameter vector $\theta$ since $u_n$ will converge to $\theta_0$. So, $u_n$ generally becomes a data series. In the paper they have used nearest neighbor search for minimizing J and hence the representation of Eq(3) as a nearest neighbor search. 

In the original way, the search is initialized by taking random values of initial guesses of the parameters and doing a random search, stopping when $J$ is minimized for a given number of iterations, the parameters are incremented after each iteration by a very small number. 

My question is how do I replace EQ(3) and formulate the recursive search for optimal $J$ as EM method. The maximum-likelihood solution for the parameters,
under the additive white Gaussian noise assumption, corresponds
to minimization of the norm as follows $\theta_{ML} = \arg \min u_n Eq(4)$

 1. Can somebody give ideas on solving Eq(4) as ML; i.e how do I minimize J using ML? 
 2. I really do not understand how to replace Eq (3) by ML-based minimization since I cannot understand the formulation of the likelihood and other technical information.

  [1]: http://www.eurasip.org/Proceedings/Eusipco/Eusipco2000/SESSIONS/THUAM/OR1/CR1473.PDF","fixed typos, improved formatting",2013-10-10 09:14:54.500
185429,57203,633.0,5,,CC BY-SA 3.0,8c96897a-1d96-42b6-b2f6-1b6b9c6d56ff,"Yes, the gamma distribution is the maximum entropy distribution for which the mean $E(X)$ and mean-log $E(\log X)$ are fixed.  As with all exponential family distributions, it is the unique maximum entropy distribution for a fixed expected sufficient statistic.

To answer your question about physical processes that generate these distributions:  The lognormal distribution arises when the logarithm of X is normally distributed, for example, if X is the product of very many small factors.  If X is gamma distributed, it is the sum of many exponentially-distributed variates.  For example, the waiting time for many events of a Poisson process.",added 387 characters in body,2013-10-10 09:15:41.347
185430,57195,,25,,,aff4e00a-efc3-4e59-b7ce-cdf642c08abc,,http://twitter.com/#!/StackStats/status/388231387591237632,2013-10-10 09:15:50.517
185431,57164,12683.0,5,,CC BY-SA 3.0,67c56f53-78be-4009-aaf6-921619b97641,I have an experimentally observed distribution that looks very similar to a gamma or lognormal distribution. I've read that the [lognormal distribution](http://en.wikipedia.org/wiki/Log-normal_distribution) is the maximum entropy probability distribution for a random variate $X$ for which the mean and variance of $\ln(X)$ are fixed. Does the gamma distribution have any similar properties?,improved formatting,2013-10-10 09:20:31.077
185432,57204,4831.0,2,,CC BY-SA 3.0,c696db83-d611-4f5e-ad15-6fb2e477ab34,"You are correct that there's currently no good way to do seasonal ARIMA in statsmodels. Currently, I only have a half-baked solution for doing non-consecutive lags, but it's not public anywhere. It's a bit heavy, computations-wise. Unfortunately, I doubt I'll be able to work on this anytime soon (unless someone would be willing to fund the enhancement...). Contributions in this area would be very welcome.

https://github.com/statsmodels/statsmodels/issues/247
https://github.com/statsmodels/statsmodels/issues/232",,2013-10-10 09:22:00.363
185576,57223,22564.0,5,,CC BY-SA 3.0,66e09a7f-8995-4d05-9731-49469d5f6c4a,"I have a problem like the following:

1) There are six measurements for each individual with large within-subject variance 

2) There are two groups (Treatment and Control)

3) Each group consists of 5 individuals

4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.


The data looks like this:
![http://s10.postimg.org/p9krg6f3t/examp.png][1]

And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. **This ignores within-subject variability**:


     n.simulations<-10000
        pvals=matrix(nrow=n.simulations,ncol=1)
        for(k in 1:n.simulations){
          subject=NULL
          for(i in 1:10){
            subject<-rbind(subject,as.matrix(rep(i,6)))
          }
          #set.seed(42)
          
          #Sample Subject Means
          subject.means<-rnorm(10,100,2)
          
          #Sample Individual Measurements
          values=NULL
          for(sm in subject.means){
            values<-rbind(values,as.matrix(rnorm(6,sm,20)))
          }
          
          out<-cbind(subject,values)
          
          #Split into GroupA and GroupB
          GroupA<-out[1:30,]
          GroupB<-out[31:60,]
          
          #Add effect size to GroupA
          GroupA[,2]<-GroupA[,2]+0
          
          colnames(GroupA)<-c(""Subject"", ""Value"")
          colnames(GroupB)<-c(""Subject"", ""Value"")
          
          #Calculate Individual Means and SDS
          GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
          for(i in 1:length(unique(GroupA[,1]))){
            GroupA.summary[i,1]<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
            GroupA.summary[i,2]<-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
          }
          colnames(GroupA.summary)<-c(""Mean"",""SD"")
          
          
          GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
          for(i in 1:length(unique(GroupB[,1]))){
            GroupB.summary[i,1]<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
            GroupB.summary[i,2]<-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
          }
          colnames(GroupB.summary)<-c(""Mean"",""SD"")
          
          Summary<-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
          colnames(Summary)[1]<-""Group""
          
          pvals[k]<-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
        }


And here is code for plots:


    #Plots
    par(mfrow=c(2,2))
    boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupA[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupA[,1]))){
      m<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      ci<-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupB[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupB[,1]))){
      m<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      ci<-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
            ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
            main=""Individual Averages"")
    stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)
    
    points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(.9,
             t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    
    points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(1.9,
             t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
         main=c(paste(""# sims="", n.simulations),
                paste(""% Sig p-values="",100*length(which(pvals<0.05))/length(pvals)))
    )

Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.

So what is the correct way to analyze this data?


**Bonus:**

The example above is a simplification. For the actual data: 

1) The within-subject variance is positively correlated with the mean. 

2) Values can only be multiples of two. 

3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. 

4) Number of Subjects in each group are not necessarily equal. 

Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.

**EDIT:**

Ok, here is what *actual* data looks like. There is also three groups rather than two:

![enter image description here][2]

dput() of data:

    structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
    3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
    6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
    10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
    12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
    15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
    18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
    22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
    6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
    2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
    12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
    10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
    20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
        NULL, c(""Group"", ""Subject"", ""Value"")))


**EDIT 2:**

In response to Henrik's answer:
So if I instead perform anova followed by TukeyHSD procedure on the individual averages as shown below, I could interpret this as underestimating my p-value by about 3-4x? 

My goal with this part of the question is to understand how I, as a reader of a journal article, can better interpret previous results given their choice of analysis method. For example they have those ""stars of authority"" showing me 0.01>p>.001. So if i accept 0.05 as a reasonable cutoff I should accept their interpretation? The only additional information is mean and SEM.

    #Get Invidual Means
    summary=NULL
    for(i in unique(dat[,2])){
    sub<-which(dat[,2]==i)
    summary<-rbind(summary,cbind(
    dat[sub,1][1],
    dat[sub,2][1],
    mean(dat[sub,3]),
    sd(dat[sub,3])
    )
    )
    }
    colnames(summary)<-c(""Group"",""Subject"",""Mean"",""SD"")
    
    TukeyHSD(aov(summary[,3]~as.factor(summary[,1])+ (1|summary[,2])))
    
    #      Tukey multiple comparisons of means
    #        95% family-wise confidence level
    #    
    #    Fit: aov(formula = summary[, 3] ~ as.factor(summary[, 1]) + (1 | summary[, 2]))
    #    
    #    $`as.factor(summary[, 1])`
    #             diff       lwr       upr     p adj
    #    2-1 -0.672619 -4.943205  3.597967 0.9124024
    #    3-1  7.507937  1.813822 13.202051 0.0098935
    #    3-2  8.180556  2.594226 13.766885 0.0046312

  [1]: https://i.stack.imgur.com/55V9J.png
  [2]: https://i.stack.imgur.com/k1xWd.png",added 1451 characters in body,2013-10-10 19:45:44.693
185579,57249,19264.0,3,,CC BY-SA 3.0,c104c76e-1ff7-4661-8be2-af1d7ad779d7,<gamma-distribution><sum>,,2013-10-10 19:49:21.903
185434,57206,22555.0,2,,CC BY-SA 3.0,5f158851-751e-43d9-8312-58c314a0d914,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta 1 and Beta 2.  If Beta 1 is 0 and Beta 2 is 3 then an assumption can be made that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta 1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta 2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.  It also has the advantage of potentially identifying another distribution.  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Uniform: 0, 1.8
    Exponential: 4, 9
    Normal: 0, 3
    Students-t: 0, 0 to 3 (a range) 

Gamma can be identified from a line, and Beta, Beta J and Beta U from regions.  These are illustrated in Hahn and Shapiro Fig 6-1.

Consider this a rough screening method, but it could indicate where further assessment is worthwhile.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",,2013-10-10 09:59:06.940
185435,57206,22555.0,5,,CC BY-SA 3.0,2c87f975-3fc8-4f90-85e4-326f51c97099,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta 1 and Beta 2.  If Beta 1 is 0 and Beta 2 is 3 then an assumption can be made that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta 1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta 2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying another distribution.  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Uniform: 0, 1.8
    Exponential: 4, 9
    Normal: 0, 3
    Students-t: 0, 0 to 3 (a range) 

Gamma can be identified from a line, and Beta, Beta J and Beta U from regions.  These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",deleted 106 characters in body,2013-10-10 10:09:29.183
185436,57206,22555.0,5,,CC BY-SA 3.0,d971bd91-a11b-4a1d-8ce1-fdafbc03e237,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta 1 and Beta 2.  A Beta 1 of 0 and Beta 2 of 3 suggests that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta 1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta 2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying another distribution.  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Uniform: 0, 1.8
    Exponential: 4, 9
    Normal: 0, 3
    Students-t: 0, 3 to 10 (a range)
    Impossible: 0 to 4 and 1 to 5 (a region).

Gamma can be identified from a line, and Beta, Beta J and Beta U from regions.  These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",deleted 106 characters in body,2013-10-10 10:15:59.297
185437,57207,503.0,2,,CC BY-SA 3.0,461d5e8a-a086-4616-92cd-995d838ccd95,"For measurement error there really isn't a difference in the definitions. Psychometry defines ""true score"" as ""measured score"" + ""error"" and this is the same thing as the statistical definition. The confusion may come from different terminology; that developed because psychometry deals with tests while statistics can deal with almost anything. 

""Bias"" is a bit more complex. @NickCox gave the definition in statistics. In psychometry, it is used (at least some of the time) in a slightly different way, again due to the specialized nature of the subject. A test is biased for/against a group if its predictions work differently in another setting. So, e.g. if we are using SAT scores to predict college GPA, bias would be that one group gets lower/higher GPA with the same SAT score.

In statistics, a scale could be biased against everyone - e.g. if my scale estimates everyone's weight as 5 pounds less than the actual value, that's bias. In the psychometrics definition, that can't be bias. 

BUT psychometricians often use ""bias"" in the statistical sense as well. 
",,2013-10-10 10:19:23.717
185440,57208,13889.0,3,,CC BY-SA 3.0,4f16d70a-15a3-4557-952c-52db1864aa30,<ranking><wilcoxon-signed-rank><auc>,,2013-10-10 10:34:56.943
185438,57208,13889.0,2,,CC BY-SA 3.0,4f16d70a-15a3-4557-952c-52db1864aa30,Suppose I have N methods and M benchmarks. I have an AUC statistic (and some other similar statistics) for each combination of method with benchmark. What test should I use to test if one method is better than the rest? I have seen some authors do pairwise comparisons using a one-sided Wilcoxon signed-rank test but I would prefer to test all methods at once. In any case I'm not sure the assumptions for the one-sided Wilcoxon signed-rank test hold. If the average AUC for each benchmark varies widely can you say the samples are from the same population? Also I'm not sure the distribution of the AUCs is symmetric around the median. Any advice would be welcome.,,2013-10-10 10:34:56.943
185439,57208,13889.0,1,,CC BY-SA 3.0,4f16d70a-15a3-4557-952c-52db1864aa30,Test to rank methods by AUCs on various benchmarks,,2013-10-10 10:34:56.943
185441,57209,21896.0,2,,CC BY-SA 3.0,6fefc0ac-befd-4a0f-b8a1-835c6c8db58b,"Following the suggestion of @Momo I will answer the question myself. What I had forgotten yesterday when I posted this question, is that I can just see what `glm.nb` does by typing ""glm.nb"" into the console. From the code it returns it can be inferred that indeed the variance equals $\mu + \mu^2/\theta$ so that $\theta = 1/\kappa$.

Also I'd like to use the opportunity to advertise this [article](http://www.jstatsoft.org/v27/i08/paper) I found since then, also addressing these matters. 
",,2013-10-10 10:37:03.140
185445,57210,22560.0,3,,CC BY-SA 3.0,36622877-0359-46a6-b09f-15f1cc8bbb8d,<factor-analysis>,,2013-10-10 10:46:06.893
185444,57210,22560.0,1,,CC BY-SA 3.0,36622877-0359-46a6-b09f-15f1cc8bbb8d,Why eigenvalues is greater than 1 in factor analysis,,2013-10-10 10:46:06.893
185443,57210,22560.0,2,,CC BY-SA 3.0,36622877-0359-46a6-b09f-15f1cc8bbb8d,Why we take eigenvalue greater than 1 in factor analysis to retain factors ? And how can we decide which variables are to be chosen as factors ,,2013-10-10 10:46:06.893
185578,57249,19264.0,1,,CC BY-SA 3.0,c104c76e-1ff7-4661-8be2-af1d7ad779d7,General Sum of Gamma Distributions,,2013-10-10 19:49:21.903
185447,57190,449.0,5,,CC BY-SA 3.0,23e7321b-8294-44d4-a6ce-dda2fe4d33b0,"You can use multi-level logistic regression. You've only got one dependent variable, correctness. You have multiple independent variables nested within student. In R you can use `lmer` to construct the model. It would look something like.

    m <- lmer( answer ~ treatment * Q + (treatment * Q | student), family = 'binomial', data = mydata)

That would allow for there to be random effects of question and treatment within student as well as overall correctness variability within student but you would also be able to assess fixed effects of treatment and question. What you seem to really want to know is all of the treatment by question interactions and that model provides them.

In order to analyze all of the questions with any kind of reliability you really should have a lot of students taking the test (hundreds). The general effect of treatment could be assessed with fewer. Also, if you know the categories, the kinds of questions you think differ, then you could replace the individual question variable with that. It would be much more sensible and make this look much less like a fishing expedition.",added 272 characters in body,2013-10-10 10:53:13.073
185450,57212,22190.0,2,,CC BY-SA 3.0,2b89f1ea-c055-4128-acd8-e7a1d66333eb,"I am new to statistics and asked to develop a statistical model, which I had started, they ask me to carry out concordance and discordance now, however I don't know anything about these terms except except that the concordance is the probability that a pair of individuals will both have a certain characteristic, given that one of the pair has the characteristic and the opposite for discordance.
Still I don't know why I have to find them and what would be the appropriate value of both for a decent model.  
Thanks
",,2013-10-10 10:53:56.520
185449,57212,22190.0,1,,CC BY-SA 3.0,2b89f1ea-c055-4128-acd8-e7a1d66333eb,Concordance and Discordance role in modelling,,2013-10-10 10:53:56.520
185448,57212,22190.0,3,,CC BY-SA 3.0,2b89f1ea-c055-4128-acd8-e7a1d66333eb,<regression><logistic><modeling>,,2013-10-10 10:53:56.520
185451,57213,503.0,2,,CC BY-SA 3.0,009467a3-3084-46f7-a392-f277eae83613,"In [this paper][1] I cover concordance and discordance. The paper is about `PROC LOGISTIC` in `SAS` but the section on concordance is more general. Briefly: Look at all possible pairs of observations. A pair is concordant if the observation with the higher observed value also has the higher predicted value. 


  [1]: http://www.nesug.org/proceedings/nesug08/sa/sa09.pdf",,2013-10-10 11:17:56.730
185452,57206,22555.0,5,,CC BY-SA 3.0,cb7e33a9-60c9-4480-9e61-478f8e010ade,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta 1 and Beta 2.  A Beta 1 of 0 and Beta 2 of 3 suggests that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta 1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta 2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying another distribution.  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Uniform:        [0,1.8]                                     [point]
    Exponential:    [4,9]                                       [point] 
    Normal:         [0,3]                                       [point]
    Students-t:     [0,0] to (3,10]                             [line]
    Lognormal:      (0,3.6] to (3,10]                           [line]
    Gamma and Beta: (0,3) to [4,9], [1,0] to [4,5] ex [0,1.8]   [area]
    Impossible:     (0,1),(4,1] to (4,5)                        [area]

    Values of Beta1, Beta2 where brackets mean:
 
    [ and ] : includes
    ( and ) : approaches but does not include.
    ex      : excludes

These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",added 431 characters in body,2013-10-10 11:19:34.070
185453,57206,22555.0,5,,CC BY-SA 3.0,5fd45d87-4f2a-4858-bd1e-a7a0044d8898,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta 1 and Beta 2.  A Beta 1 of 0 and Beta 2 of 3 suggests that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta 1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta 2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying another distribution.  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Uniform:        [0,1.8]                                     [point]
    Exponential:    [4,9]                                       [point] 
    Normal:         [0,3]                                       [point]
    Students-t:     [0,0] to (3,10]                             [line]
    Lognormal:      (0,3.6] to (3,10]                           [line]
    Gamma and Beta: (0,3) to [4,9], [1,0] to [4,5] ex [0,1.8]   [area]
    Impossible:     (0,1) to (4.5], (0,1) to (4,1]              [area]

    Values of Beta1, Beta2 where brackets mean:
 
    [ and ] : includes
    ( and ) : approaches but does not include.
    ex      : excludes

These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",added 431 characters in body,2013-10-10 11:26:56.413
185455,57210,,5,user88,CC BY-SA 3.0,1989dd57-e043-40fd-822b-7b63ec202f11,"Why we take eigenvalue greater than 1 in factor analysis to retain factors?  
 And how can we decide which variables are to be chosen as factors?",added 3 characters in body; edited title,2013-10-10 11:28:00.717
185454,57210,,4,user88,CC BY-SA 3.0,1989dd57-e043-40fd-822b-7b63ec202f11,Why eigenvalues are greater than 1 in factor analysis?,added 3 characters in body; edited title,2013-10-10 11:28:00.717
185456,57214,1428.0,2,,CC BY-SA 3.0,a908955b-f311-40da-9f9e-ba2430ab3eb3,"If the M benchmarks are supposed to yield score identically distributed score estimates (e.g. cross-validation folds) then maybe you can estimate confidence intervals for the mean AUC score for each method by bootstrapping on the M benchmarks of that method and then compare methods by considering non-overlapping confidence intervals. As bootstrapped confidence interval is a non-parametric method, you do not make any assumption on the symmetry of AUCs around the median.",,2013-10-10 11:28:17.020
185457,57212,12683.0,5,,CC BY-SA 3.0,58cf2067-e072-4253-add0-28c3fe86e5e7,"I am new to statistics and asked to develop a statistical model, which I had started, they ask me to carry out concordance and discordance now, however I don't know anything about these terms except except that the concordance is the probability that a pair of individuals will both have a certain characteristic, given that one of the pair has the characteristic and the opposite for discordance.
Still I don't know why I have to find them and what would be the appropriate value of both for a decent model.  
",removed thanks,2013-10-10 11:33:34.190
186095,57394,16046.0,3,,CC BY-SA 3.0,e7e70389-f554-4886-b5ae-65780f3c30a0,<time-series><bayesian><causality>,,2013-10-13 16:53:32.577
186164,57417,22425.0,3,,CC BY-SA 3.0,e6884711-0a34-4459-adbd-ae5d96027db4,<probability><distributions><mathematical-statistics><iid>,,2013-10-14 03:02:05.353
185458,57206,22555.0,5,,CC BY-SA 3.0,2c3c267d-7c08-490c-8066-12b5b43dfeaf,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta 1 and Beta 2.  A Beta 1 of 0 and Beta 2 of 3 suggests that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta 1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta 2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying another distribution.  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Uniform:        [0,1.8]                                     [point]
    Exponential:    [4,9]                                       [point] 
    Normal:         [0,3]                                       [point]
    Students-t:     [0,0] to (3,10]                             [line]
    Lognormal:      (0,3.6] to (3,10]                           [line]
    Gamma and Beta: (0,3) to (4,9), [1,0] to [4,5] ex [0,1.8]   [area]
    Impossible:     (0,1) to (4.5), (0,1) to (4,1]              [area]

    Values of Beta1, Beta2 where brackets mean:
 
    [ and ] : includes
    ( and ) : approaches but does not include.
    ex      : excludes

These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",edited body,2013-10-10 11:40:05.413
185459,57215,449.0,2,,CC BY-SA 3.0,3ab4e791-2952-41cd-805e-6f98e58a0af7,"The wikipedia definition is a fine definition that you can use for your paper if you need one but I think you're missing something.

The $\epsilon$ is random error, which is synonymous with noise. In practice, the random error can be Gaussian distributed, in which case it is Gaussian noise, but it could take on other distributions. If the distribution of $\epsilon$ happens to be Gaussian then you've met one of the theoretical assumptions of the model and things like interval estimation are better justified. If it's not Gaussian then, like Glen_b said, you still have that it's best linear unbiased.

Theoretically, the random error (noise) is supposed to be Gaussian distributed but the outcome could be anything. So, in order to answer your question you'd need to state whether you want to know the distribution of your particular noise or what the distribution of the noise should be. For the former you'd need data.",,2013-10-10 11:41:48.027
185460,57216,22561.0,2,,CC BY-SA 3.0,15f64131-5579-49b9-916c-e25e8d005db8,"The total variance for combined regression results can be estimated using the same approach as in multiple imputations. In the attached file, the formulas for combining the regression results and total variance are presented.

![Combining (pooloing) results of several regression models into one][1]


  [1]: https://i.stack.imgur.com/rFT00.png",,2013-10-10 12:00:04.637
185463,57217,15563.0,3,,CC BY-SA 3.0,2aec6883-726c-4e89-8759-5d33b35fba3b,<r><svm><e1071>,,2013-10-10 12:05:58.447
185461,57217,15563.0,2,,CC BY-SA 3.0,2aec6883-726c-4e89-8759-5d33b35fba3b,"I am using the R e1071 library for the SVM (Support Vector Machine) algorithm.
And I used tune to find out the best Cost and gamma parameters.

Though the plot doesn't seem to provide the actual best prediction.

Here is some details:

    gammalist <- c(0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)
    obj <- tune(svm, Class~., data = trainData, ranges = list(gamma = gammalist, cost = 2^(2:4)), tunecontrol = tune.control(sampling = ""fix"") )
    plot(obj)


Here is the plot obtained:
![enter image description here][1]

The plot leads me to believe 0.02 is roundabout the best gamma.
But I actually tested manually several others and find better results for 0.042.

On a 200 sample, I get 23 errors with gamma=0.042, and 26 errors with gamma=0.02.

How do you explain this?


  [1]: https://i.stack.imgur.com/Al0QG.png",,2013-10-10 12:05:58.447
185462,57217,15563.0,1,,CC BY-SA 3.0,2aec6883-726c-4e89-8759-5d33b35fba3b,R e1071 tune plot does not give me the best gamma?,,2013-10-10 12:05:58.447
185468,57185,15827.0,4,,CC BY-SA 3.0,51c21a53-4894-4c20-96e6-16220fc8de24,What is the difference between the concept and treatment of measurement error in psychometry and in statistics?,"shorter title (without distorting intended meaning, I hope)",2013-10-10 12:36:32.503
185469,57218,21398.0,2,,CC BY-SA 3.0,c3ee4016-6ca0-4ca6-aea6-d299cf321fd4,I have a question. I have a dataset with some missing values that were not MCAR. I imputed them with fully conditional specification method iterations. I then executed my analysis on the basis of the imputed dataset. The results of the original model (with missing values: listwise deletion)  did not change much in the eventual pooled model. My idea would be to go back the missing values dataset. What do you think?,,2013-10-10 12:44:27.567
185471,57218,21398.0,3,,CC BY-SA 3.0,c3ee4016-6ca0-4ca6-aea6-d299cf321fd4,<multiple-imputation>,,2013-10-10 12:44:27.567
185470,57218,21398.0,1,,CC BY-SA 3.0,c3ee4016-6ca0-4ca6-aea6-d299cf321fd4,Multiple imputation - original model,,2013-10-10 12:44:27.567
185473,57219,10547.0,2,,CC BY-SA 3.0,42d6c45c-e692-4b95-b8e6-ec3e27228396,"If all the $Zs$ are also determined by the $Y$ the system of equations cannot be identified. What you need to do is to reduce the equations such that the coefficients can be identified.

I recommend reading:

> Wooldridge, Introductory Econometrics, 3d ed. Chapter 16: Simultaneous
> equations

Here your kind of problems gets explained and there are some pretty nice examples.",,2013-10-10 12:59:56.353
185474,57219,10547.0,5,,CC BY-SA 3.0,493c4cdd-9a31-438b-8502-d852e938d0a6,"If all the $Z$s are also determined by the $Y$ the system of equations, which you have proposed, cannot be identified. What you need to do is to reduce the equations such that the coefficients can be identified.

I recommend reading:

> Wooldridge, Introductory Econometrics, 3d ed. Chapter 16: Simultaneous
> equations

Here, your kind of problems gets explained, and there are some pretty nice examples.

I also recommend reading:

    Rummery,Vella,Verbeek (1998) - Estimating the Returns to Education for Australian Youth via Rank-Order Instrumental Variables

and 

    Vella,Verbeek (1997) - Using Rank Order As An Instrumental Variable - An Applicaton To The Return To Schooling

Vella and Verbeek (also Rummery) estimate smth. like:

$y_i = x_i\beta + z_i\delta + e_i, \ \ \ \ i = 1,...,N$

Here $x_i$ is a $K$ vector of exogenous variables whereas $z_i$ is assumed to be endogenous. Hence the reduced form equation of $z_i$ is given by:

$z_i = x_i\alpha + v_i$

The advantage of this approach is, that you dont need any exclusion restrictions for the $x_i$, which are necessary to make 2SLS/3SLS work.

I've used this approach to solve a three equation system, i.e., i got three equations and in each of them there are two endogenous regressors which are also the dependend variable in some other equation.

I also applied a plug-in style of approach to deal with potential heteroscedasticity. 

There are some issues which are not presented within this papers but I would be happy to talk to you about that.",added 1145 characters in body,2013-10-10 13:10:22.827
186163,57417,22425.0,2,,CC BY-SA 3.0,e6884711-0a34-4459-adbd-ae5d96027db4,"Let X1, X2,...,Xn be discrete random variables. I'm looking for a way to prove the random variables are independent but not identically distributed.

Can anyone suggest some ideas ?",,2013-10-14 03:02:05.353
185477,57220,22562.0,2,,CC BY-SA 3.0,db0186ad-6dd6-4ac7-96e4-defd1999ad41,"I have the following regression

 children = \beta_0 + \beta_1 (log) earnings + \beta_2 grandparents + \epsilon

and \beta_1>0 with p=0.01 and \beta_2>0 with p=0.01, and N is large (N>10.000) and grandparents takes values 0,1,2,3,4.

Then I add the interaction term ((log) earnings*grandparents) to equation 1, such that: 

 children = \beta_0 + \beta_1 (log) earnings + \beta_2 grandparents+ \beta_3 ((log) earnings*grandparents) + \epsilon 

and \beta_1>0 with p=0.01, beta_2 is no longer stat.sign and also \beta_3 is no stat. sign. 

I do not understand how to interpret the results and if the interaction term wipes out the direct effect of grandparents since (log)earnings is always different from 0.

Thanks!



",,2013-10-10 13:11:58.220
185476,57220,22562.0,3,,CC BY-SA 3.0,db0186ad-6dd6-4ac7-96e4-defd1999ad41,<regression><interaction><effects>,,2013-10-10 13:11:58.220
185475,57220,22562.0,1,,CC BY-SA 3.0,db0186ad-6dd6-4ac7-96e4-defd1999ad41,Interaction wipes out my direct effects in regression (non zero variable),,2013-10-10 13:11:58.220
185478,57220,16474.0,5,,CC BY-SA 3.0,921e1d79-1e39-4114-a8a3-ff07f1af83c3,"I have the following regression

 $children = \beta_0 + \beta_1 \log(earnings) + \beta_2 grandparents + \epsilon$

and $\beta_1>0$ with $p$=0.01 and $\beta_2>0$ with $p$=0.01, and N is large (N>10.000) and grandparents takes values 0,1,2,3,4.

Then I add the interaction term ($\log(earnings)*grandparents$) to equation 1, such that: 

 $children = \beta_0 + \beta_1 \log( earnings) + \beta_2 grandparents+ \beta_3 \log( earnings)*grandparents + \epsilon$ 

and $\beta_1>0$ with $p$=0.01, $\beta_2$ is no longer statistically significant and also $\beta_3$ is not statistically significant. 

I do not understand how to interpret the results and if the interaction term wipes out the direct effect of grandparents since $\log(earnings)$ is always different from 0.



",improved formatting,2013-10-10 13:23:13.150
185481,57221,22563.0,3,,CC BY-SA 3.0,bfb7c505-77a9-4e14-abc9-7a6898af3636,<distributions>,,2013-10-10 13:28:20.653
185480,57221,22563.0,1,,CC BY-SA 3.0,bfb7c505-77a9-4e14-abc9-7a6898af3636,What's the approximate distribution? Replace the true mean with sample mean,,2013-10-10 13:28:20.653
185479,57221,22563.0,2,,CC BY-SA 3.0,bfb7c505-77a9-4e14-abc9-7a6898af3636,"If say for a random variable X, I have observation of x1,x2,x3,....,xn. Let m be the sample mean, and s be the sample deviation. Does the new random variable (x-m)/s follow some distribution? It's not t-distribution I guess, since for it to be t distribution, m needs to be replaced by true mean.

Can statistics expert shed some light on this?",,2013-10-10 13:28:20.653
185482,57222,16474.0,2,,CC BY-SA 3.0,a41ca399-164d-4220-a633-503daab41441,"$\beta_2$ in equation 2 is the effect of $grandparents$ when $\log(earnings) = 0$, i.e. $earnings = 1$. This is apperently outside the range of your data, so it is an extrapolation. The easiest way around that is to center $earnings$ before taking the logarithm or creating the interaction term at some meaningfull value withing the range of the data, for example, the median. That way the main effect of $grandparents$ will be the effect of grandparents when one has a median income instead of a fictional income of 1.",,2013-10-10 13:29:02.837
185484,57223,22564.0,1,,CC BY-SA 3.0,508b3919-0da2-43d5-87c6-349b1a53e63e,How to compare two groups with multiple measurements for each individual with R?,,2013-10-10 13:47:37.470
185485,57223,22564.0,3,,CC BY-SA 3.0,508b3919-0da2-43d5-87c6-349b1a53e63e,<r><statistical-significance><t-test><error-propagation>,,2013-10-10 13:47:37.470
185483,57223,22564.0,2,,CC BY-SA 3.0,508b3919-0da2-43d5-87c6-349b1a53e63e,"   
Hello Stack exchange,

I have a problem like the following:

1) There are six measurements for each individual with large within-subject variance 

2) There are two groups (Treatment and Control)

3) Each Group consists of 5 individuals

4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.


The data looks like this:
![http://s10.postimg.org/p9krg6f3t/examp.png][1]

And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. **This ignores within-subject variability**:


     n.simulations<-10000
        pvals=matrix(nrow=n.simulations,ncol=1)
        for(k in 1:n.simulations){
          subject=NULL
          for(i in 1:10){
            subject<-rbind(subject,as.matrix(rep(i,6)))
          }
          #set.seed(42)
          
          #Sample Subject Means
          subject.means<-rnorm(10,100,2)
          
          #Sample Individual Measurements
          values=NULL
          for(sm in subject.means){
            values<-rbind(values,as.matrix(rnorm(6,sm,20)))
          }
          
          out<-cbind(subject,values)
          
          #Split into GroupA and GroupB
          GroupA<-out[1:30,]
          GroupB<-out[31:60,]
          
          #Add effect size to GroupA
          GroupA[,2]<-GroupA[,2]+0
          
          colnames(GroupA)<-c(""Subject"", ""Value"")
          colnames(GroupB)<-c(""Subject"", ""Value"")
          
          #Calculate Individual Means and SDS
          GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
          for(i in 1:length(unique(GroupA[,1]))){
            GroupA.summary[i,1]<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
            GroupA.summary[i,2]<-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
          }
          colnames(GroupA.summary)<-c(""Mean"",""SD"")
          
          
          GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
          for(i in 1:length(unique(GroupB[,1]))){
            GroupB.summary[i,1]<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
            GroupB.summary[i,2]<-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
          }
          colnames(GroupB.summary)<-c(""Mean"",""SD"")
          
          Summary<-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
          colnames(Summary)[1]<-""Group""
          
          pvals[k]<-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
        }


And here is code for plots:


    #Plots
    par(mfrow=c(2,2))
    boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupA[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupA[,1]))){
      m<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      ci<-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupB[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupB[,1]))){
      m<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      ci<-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
            ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
            main=""Individual Averages"")
    stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)
    
    points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(.9,
             t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    
    points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(1.9,
             t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
         main=c(paste(""# sims="", n.simulations),
                paste(""% Sig p-values="",100*length(which(pvals<0.05))/length(pvals)))
    )

Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.

So what is the correct way to analyze this data?


**Bonus:**

The example above is a simplification. For the actual data: 

1) The within-subject variance is positively correlated with the mean. 

2) Values can only be multiples of two. 

3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. 

4) Number of Subjects in each group are not necessarily equal. 

Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.

  [1]: https://i.stack.imgur.com/55V9J.png",,2013-10-10 13:47:37.470
185486,57223,15827.0,5,,CC BY-SA 3.0,0b488283-6a15-44b5-b08c-874f71896025,"I have a problem like the following:

1) There are six measurements for each individual with large within-subject variance 

2) There are two groups (Treatment and Control)

3) Each group consists of 5 individuals

4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.


The data looks like this:
![http://s10.postimg.org/p9krg6f3t/examp.png][1]

And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. **This ignores within-subject variability**:


     n.simulations<-10000
        pvals=matrix(nrow=n.simulations,ncol=1)
        for(k in 1:n.simulations){
          subject=NULL
          for(i in 1:10){
            subject<-rbind(subject,as.matrix(rep(i,6)))
          }
          #set.seed(42)
          
          #Sample Subject Means
          subject.means<-rnorm(10,100,2)
          
          #Sample Individual Measurements
          values=NULL
          for(sm in subject.means){
            values<-rbind(values,as.matrix(rnorm(6,sm,20)))
          }
          
          out<-cbind(subject,values)
          
          #Split into GroupA and GroupB
          GroupA<-out[1:30,]
          GroupB<-out[31:60,]
          
          #Add effect size to GroupA
          GroupA[,2]<-GroupA[,2]+0
          
          colnames(GroupA)<-c(""Subject"", ""Value"")
          colnames(GroupB)<-c(""Subject"", ""Value"")
          
          #Calculate Individual Means and SDS
          GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
          for(i in 1:length(unique(GroupA[,1]))){
            GroupA.summary[i,1]<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
            GroupA.summary[i,2]<-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
          }
          colnames(GroupA.summary)<-c(""Mean"",""SD"")
          
          
          GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
          for(i in 1:length(unique(GroupB[,1]))){
            GroupB.summary[i,1]<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
            GroupB.summary[i,2]<-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
          }
          colnames(GroupB.summary)<-c(""Mean"",""SD"")
          
          Summary<-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
          colnames(Summary)[1]<-""Group""
          
          pvals[k]<-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
        }


And here is code for plots:


    #Plots
    par(mfrow=c(2,2))
    boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupA[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupA[,1]))){
      m<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      ci<-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupB[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupB[,1]))){
      m<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      ci<-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
            ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
            main=""Individual Averages"")
    stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)
    
    points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(.9,
             t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    
    points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(1.9,
             t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
         main=c(paste(""# sims="", n.simulations),
                paste(""% Sig p-values="",100*length(which(pvals<0.05))/length(pvals)))
    )

Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.

So what is the correct way to analyze this data?


**Bonus:**

The example above is a simplification. For the actual data: 

1) The within-subject variance is positively correlated with the mean. 

2) Values can only be multiples of two. 

3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. 

4) Number of Subjects in each group are not necessarily equal. 

Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.

  [1]: https://i.stack.imgur.com/55V9J.png",deleted 30 characters in body,2013-10-10 13:50:36.427
185488,57224,3731.0,2,,CC BY-SA 3.0,ee2f3f02-fd21-4ba3-9722-8e2ed09b0088,"To close the loop for those who don't want to follow the comment thread above:

No. People do not normally compile these things because they are specific to both the particular model chosen and the particular data onto which the model is fit. To talk of a ""typical"" number is not well posed.

If someone would like to post a more comprehensive answer, I will be happy to ""unaccept"" this answer and accept theirs instead. ",,2013-10-10 14:09:23.710
185489,57225,503.0,2,,CC BY-SA 3.0,ae7e0ec3-f2ed-4d09-93b3-6323f5180150,"I think the choice depends on the audience that will read whatever you write.

If they are mostly statistically unsophisticated, I'd say you could use the original data set and put a footnote about how multiple imputation did not change things much. If they are more sophisticated, I'd go with the MI analysis. Even if things don't change ""much"" they change *some* and the MI is a better approach.

Also, be careful that you looked at *all* the output for what changed (or didn't). Not just parameter estimates but their standard errors (or whatever your analysis involves - you didn't say what analysis you did, so it's hard to say what might be affected). ",,2013-10-10 14:16:53.693
185490,57221,594.0,5,,CC BY-SA 3.0,b1323a1c-406c-49bf-bd9e-b9f7a2ca1249,"If say for a random variable $X$, I have observation of $x_1,x_2,x_3,\ldots,x_n$. Let $m$ be the sample mean, and $s$ be the sample standard deviation. Does the new random variable $(X-m)/s$ follow some distribution? It's not $t$-distribution I guess, since for it to be $t$ distributed, $m$ needs to be replaced by true mean.

Can statistics expert shed some light on this?","formatting, spelling/grammar",2013-10-10 14:21:48.783
185494,57226,22566.0,2,,CC BY-SA 3.0,d89df24f-4793-49a7-871c-e5fffca843a2,The above presented formulas are available in the SPSS help: Help > Algorithms > Multiple Imputation: Pooling Algorithms > Rubin's Rules (multiple imputation algorithms) > Combining Results after Multiple Imputation,,2013-10-10 14:44:17.203
185496,57227,9792.0,2,,CC BY-SA 3.0,de5d42f5-a1e6-4faa-8a6b-0dc0e577f67c,"I think the question why splines cannot be centered arose out of a misunderstanding of how splines function. It seems that splines don't model an intercept and thus centering is impossible. It would, however, be great if someone had another solution to estimating the group differences at different time points when modelling more complex dynamics.
",,2013-10-10 15:02:05.507
185497,57223,,25,,,f7504841-04ec-4981-80cf-2f089fe58be2,,http://twitter.com/#!/StackStats/status/388321984008159232,2013-10-10 15:15:50.387
185498,57228,22568.0,2,,CC BY-SA 3.0,c16d3942-e5bb-4e4f-ba17-660ba10e344c,"I am an expert GIS user moving towards R more and more.  I have been using R for some basic regressions and such, but I would like to begin to use and manipulate GIS data in R.

How can I create a basemap graphic similar to the one in this post:
http://stats.stackexchange.com/questions/72421/showing-spatial-and-temporal-correlation-on-maps

Again, I am a beginner in R and haven't found any other related thread here.

thanks,
mike",,2013-10-10 15:23:08.293
185500,57228,22568.0,3,,CC BY-SA 3.0,c16d3942-e5bb-4e4f-ba17-660ba10e344c,<r><gis>,,2013-10-10 15:23:08.293
185499,57228,22568.0,1,,CC BY-SA 3.0,c16d3942-e5bb-4e4f-ba17-660ba10e344c,How to create a GIS basemap in R?,,2013-10-10 15:23:08.293
186162,57417,22425.0,1,,CC BY-SA 3.0,e6884711-0a34-4459-adbd-ae5d96027db4,Independent but not identically distributed,,2013-10-14 03:02:05.353
185503,57229,22567.0,2,,CC BY-SA 3.0,1f5099f9-3c2e-4c5b-bbdf-487714206543,"I have a regression problem where the independent variables are all factors (categorical).  I've been looking at the literature on missing data, and so far it all seems concerned with missing training data.  I was wondering if there is a standard way of dealing with missing data in the *prediction* set.  That is, you have all the data you need to train, but then you need to be able to make a prediction with only partial data. This must have been a studied problem. 

My initial thought is to use an average of the dummy encoded variables, according to how common they are.  As a quick example, say we have a three level factor dummy encoded as

level 1: [1 0]

level 2: [0 1]

level 3: [0 0]

say level i occurs fraction f_i of the time in the training data (so sum(i, f_i)==1)

say the regression has the two coefficients \beta_1 and \beta_2.

Then a missing value in this factor might be estimated as:

\beta_1*f_1 + \beta_2*f_2 + 0*f_3

But given that the ""default"" level encoding are shared across factors, I'm not sure I'm handling level 3 correctly in this case. 

Any suggestions?

Thanks,
Craig



",,2013-10-10 15:23:53.377
185502,57229,22567.0,1,,CC BY-SA 3.0,1f5099f9-3c2e-4c5b-bbdf-487714206543,Dealing with missing data in the prediction set only,,2013-10-10 15:23:53.377
185501,57229,22567.0,3,,CC BY-SA 3.0,1f5099f9-3c2e-4c5b-bbdf-487714206543,<regression><categorical-data><missing-data><prediction>,,2013-10-10 15:23:53.377
185505,57230,22569.0,1,,CC BY-SA 3.0,66b38f64-7127-4af7-b557-c287bd9dc419,Represent data across multiple categories and sub categories,,2013-10-10 15:25:25.017
185506,57230,22569.0,3,,CC BY-SA 3.0,66b38f64-7127-4af7-b557-c287bd9dc419,<data-visualization><categorical-data><barplot>,,2013-10-10 15:25:25.017
185504,57230,22569.0,2,,CC BY-SA 3.0,66b38f64-7127-4af7-b557-c287bd9dc419,"The data contains category and sub category distributions. 

Category A      - 100
   -- Level One -    40
   -- Level Two  -    30
   -- Level Three -   30

Category B    - 50
    -- Level One -   10
    -- Level Two -    15
    -- Level Three -  25

The sub categories are always same (Level one, level two, level three)
The sum of all sub categories is equal to the total of the main category.
Each graph would contain up to 5 main categories. 

What is the best way to represent this data?",,2013-10-10 15:25:25.017
185508,57230,22569.0,5,,CC BY-SA 3.0,6af05a89-0446-489d-8beb-b2828486ac81,"The data contains category and sub category distributions. 
The category are topics in a quiz such as: Music, Sports, Business. 

Each category has three levels to choose from: Basic, Standard and Advanced.

I want to represent this on a graph: 
Ex: A user might take a quiz on Music across different levels. Say the number of questions attempted is 100. The user would have answered them across levels. 40 for basic, 40 for standard and 20 for advanced. 


Each graph would contain up to 5 main categories. 

What is the best way to represent this data?",added 56 characters in body,2013-10-10 15:39:29.993
185511,57231,22381.0,2,,CC BY-SA 3.0,1ce69f54-467b-492d-bd90-3dc682223e0e,"I am using squared return as a proxy to calculate volatility, however i'm not sure whether to use absolute return or percentage return. Under absolute return all volatility estimates are below 1, however under percentage return there is a mix of volatility greater than 1 and less than 1. Percentage return below 1 would end up as a volatility figure less than the percentage return itself, on the other hand percentage return above 1 would end up as a volatility figure greater than percentage return. 

My question is: Doesn't this pose a problem when calculating volatility in that there is an over estimation when the return is above 1?

I am going to use the data to fit an ARMA-GARCH model, would there be any difference if I used percentage or absolute values?",,2013-10-10 15:42:26.310
185510,57231,22381.0,1,,CC BY-SA 3.0,1ce69f54-467b-492d-bd90-3dc682223e0e,Absolute Return vs Percentage Return to Calculate Volatility,,2013-10-10 15:42:26.310
185509,57231,22381.0,3,,CC BY-SA 3.0,1ce69f54-467b-492d-bd90-3dc682223e0e,<arma>,,2013-10-10 15:42:26.310
185512,57231,,6,,CC BY-SA 3.0,ffb63273-b3e1-4a21-8ea1-5bb1b80122cd,<finance><arma><garch>,Added additional tags,2013-10-10 15:45:28.413
185513,57231,,24,,CC BY-SA 3.0,ffb63273-b3e1-4a21-8ea1-5bb1b80122cd,,"Proposed by 3826 approved by 805, 7290 edit id of 5587",2013-10-10 15:45:28.413
185514,57232,2873.0,2,,CC BY-SA 3.0,7c95c624-c1a5-4453-8a80-74872c0d4392,"The `ns` function (and other spline functions) does its own ""centering"" of the data.  Consider this example:

    > library(splines)
    > 
    > s1 <- ns( 1:10, 3 )
    > s2 <- ns( (1:10)-5, 3 )
    > 
    > all.equal(s1,s2)
    [1] ""Attributes: < Component 1: Mean relative difference: 0.9090909 >""
    [2] ""Attributes: < Component 7: Mean relative difference: 0.9090909 >""
    > all.equal(as.vector(s1),as.vector(s2))
    [1] TRUE

So the centering of the data leads to the same splines as the uncentered data (other than the knot information in the attributes).  So centering your variable before computing a spline has no effect.  If you want to compare the values at a point other than 0 then just use the `predict` function to get the actual predictions at the point of interest and compare (subtract).",,2013-10-10 15:46:38.177
185515,57230,594.0,5,,CC BY-SA 3.0,fea96626-e807-47d8-96cc-384b557c5618,"The data contains category and sub-category distributions. 

The categories are topics in a quiz such as: Music, Sports, Business. 

Each category has three levels to choose from: Basic, Standard and Advanced.

I want to represent this on a graph.

For example: A user might take a quiz on Music across different levels. Say the number of questions attempted is 100. The user would have answered them across levels. 40 for basic, 40 for standard and 20 for advanced. 

Each graph would contain up to 5 main categories. 

What is the best way to represent this data?","spelling, formatting etc",2013-10-10 15:47:46.300
185517,57229,5237.0,6,,CC BY-SA 3.0,96854c05-d309-4d74-9f19-a12d68610e10,<regression><categorical-data><cross-validation><missing-data><prediction>,added tag; formatted; removed signature; light editing,2013-10-10 15:50:05.603
185516,57229,5237.0,5,,CC BY-SA 3.0,96854c05-d309-4d74-9f19-a12d68610e10,"I have a regression problem where the independent variables are all factors (categorical).  I've been looking at the literature on missing data, and so far it all seems concerned with missing training data.  I was wondering if there is a standard way of dealing with missing data in the *prediction* set.  That is, you have all the data you need to train, but then you need to be able to make a prediction with only partial data. This must have been a studied problem. 

My initial thought is to use an average of the dummy encoded variables, according to how common they are.  As a quick example, say we have a three level factor dummy encoded as

    level 1: [1 0]
    level 2: [0 1]
    level 3: [0 0]

Say level $i$ occurs fraction $f_i$ of the time in the training data (so $\sum(i, f_i)=1$), and the regression has the two coefficients $\beta_1$ and $\beta_2$.

Then a missing value in this factor might be estimated as:
$$
\beta_1*f_1 + \beta_2*f_2 + 0*f_3
$$
But given that the ""default"" level encoding are shared across factors, I'm not sure I'm handling level 3 correctly in this case. 

",added tag; formatted; removed signature; light editing,2013-10-10 15:50:05.603
185519,57231,22381.0,5,,CC BY-SA 3.0,a1b41d34-9ef5-474f-b2ac-7e6355074355,"I am using squared return as a proxy to calculate volatility, however i'm not sure whether to use absolute return or percentage return. Under absolute return all return estimates are below 1, however under percentage return there is a mix of return greater than 1 and less than 1. Percentage return below 1 would end up as a volatility figure less than the percentage return itself, on the other hand percentage return above 1 would end up as a volatility figure greater than percentage return. 

My question is: Doesn't this pose a problem when calculating volatility in that there is an over estimation when the return is above 1?

I am going to use the data to fit an ARMA-GARCH model, would there be any difference if I used percentage or absolute values?",deleted 8 characters in body,2013-10-10 15:52:02.787
185521,57233,5237.0,2,,CC BY-SA 3.0,2dbe7e85-e74e-4e8c-a427-b39c876d392c,"*I'll let someone else address the estimation of the missing data.  (At first glance, it looks reasonable to me.)*  

You are right that you are not handling level 3 correctly.  The coding scheme that you use in your question set up is known as *reference level coding*.  To use this approach correctly, you need to have an intercept (i.e., $\beta_0$), which estimates the mean of level 3.  I suspect you do have such, even though you didn't list it.  In this case, you would just add the intercept to your final equation.  That is: 
$$
\beta_0 + \beta_1\!*\!f_1 + \beta_2\!*\!f_2 + 0\!*\!f_3
$$
Note that you don't have to do it this way.  You can suppress the intercept and use a dummy for each level of your factor.  Some people prefer this because they think it is conceptually clearer.  However, it is exactly the same as above, just with the labels shifted.  ",,2013-10-10 15:56:30.450
185522,57229,22567.0,5,,CC BY-SA 3.0,e6ce2a49-7902-4cc9-9ab6-f66e260654a6,"I have a regression problem where the independent variables are all factors (categorical).  I've been looking at the literature on missing data, and so far it all seems concerned with missing training data.  I was wondering if there is a standard way of dealing with missing data in the *prediction* set.  That is, you have all the data you need to train, but then you need to be able to make a prediction with only partial data. This must have been a studied problem. 

My initial thought is to use an average of the dummy encoded variables, according to how common they are.  As a quick example, say we have a three level factor dummy encoded as

    level 1: [1 0]
    level 2: [0 1]
    level 3: [0 0]

Say level $i$ occurs fraction $f_i$ of the time in the training data (so $\sum_i{f_i}=1$), and the regression has the two coefficients $\beta_1$ and $\beta_2$.

Then a missing value in this factor might be estimated as:
$$
\beta_1*f_1 + \beta_2*f_2 + 0*f_3
$$
But given that the ""default"" level encoding are shared across factors, I'm not sure I'm handling level 3 correctly in this case. 

",fix summation equation,2013-10-10 15:59:24.710
185523,57234,2873.0,2,,CC BY-SA 3.0,96a3f4ab-c578-4c38-aab4-b646e9b93679,"You should read through the Spatial and possibly SpatialTemporal [Taskviews on CRAN][1].  Those will give you an idea of what packages are available and gives brief descriptions of what they do and how they compare.


  [1]: http://cran.r-project.org/web/views/",,2013-10-10 16:05:27.410
185524,57235,10060.0,2,,CC BY-SA 3.0,cf72a2a8-053a-48a4-9151-721277e79553,"R by itself does not handle GIS type of work but with different add-ons it can be a quite potent GIS device. You'd need to understand the idea of ""package"" (user-contributed scripts) and how to use `install.packages(""whateverPackage"")` command to install them.

I don't use R in GIS enough to show you the whole topography (pun totally intended), but the most commonly used packages I have seen are `map`, `ggmap`, `ggplot2`, `RgoogleMaps`, and `plotGoogleMap`.

Also, check out some sites and tutorials about this topic: [1](http://www.nyu.edu/projects/politicsdatalab/workshops/GISwR.pdf), [2](http://cran.r-project.org/web/views/Spatial.html), [3](http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf), and [4](http://www.icesi.edu.co/CRAN/web/packages/plotGoogleMaps/vignettes/plotGoogleMaps-intro.pdf). These got me started and within a day I could make some silly maps.

Also, this [pdf](http://statacumen.com/teach/SC1/SC1_16_Maps.pdf) probably contain some code pertinent to the map you wish to create. Its $\LaTeX$ format is a bit off, but you can still get some general functionality and key commands.

Good luck!",,2013-10-10 16:08:02.663
185527,57236,22572.0,3,,CC BY-SA 3.0,2ba32bca-4415-4c06-b270-6b1a63c06d83,<survival><kaplan-meier>,,2013-10-10 16:10:12.377
185526,57236,22572.0,1,,CC BY-SA 3.0,2ba32bca-4415-4c06-b270-6b1a63c06d83,"Kaplan Meier - Can I use to assess recovery of function, not just loss?",,2013-10-10 16:10:12.377
185525,57236,22572.0,2,,CC BY-SA 3.0,2ba32bca-4415-4c06-b270-6b1a63c06d83,"In my experience and readings, Kaplan-Meier has always been used to calculate differential survival between a certain number of groups. However, Im looking to assess how time to recovery from a certain event as measured by activity levels. At time zero, everyone is essentially ""dead"" (non-mobile), and with time they regain mobility. 
seems like a ""negative"" kaplan-meier, is that possible? Or should I be looking at a different modeling strategy?",,2013-10-10 16:10:12.377
185528,57228,5237.0,5,,CC BY-SA 3.0,0d624180-8f86-4f18-87d4-e02b3abc0064,"I am an expert GIS user moving towards R more and more.  I have been using R for some basic regressions and such, but I would like to begin to use and manipulate GIS data in R.

How can I create a basemap graphic similar to the one in this post:
http://stats.stackexchange.com/questions/72421/showing-spatial-and-temporal-correlation-on-maps

Again, I am a beginner in R and haven't found any other related thread here.
",removed signature,2013-10-10 16:10:14.737
185529,57235,10060.0,5,,CC BY-SA 3.0,b8bc9203-7a71-467b-88d6-4b29cd99cae4,"R by itself does not handle GIS type of work but with different add-ons it can be a quite potent GIS device. You'd need to understand the idea of ""package"" (user-contributed scripts) and how to use `install.packages(""whateverPackage"")` command to install them.

I don't use R in GIS enough to show you the whole topography (pun totally intended), but the most commonly used packages I have seen are `map`, `ggmap`, `ggplot2`, `RgoogleMaps`, and `plotGoogleMap`.

Also, check out some sites and tutorials about this topic: [1](http://www.nyu.edu/projects/politicsdatalab/workshops/GISwR.pdf), [2](http://cran.r-project.org/web/views/Spatial.html), [3](http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf), and [4](http://www.icesi.edu.co/CRAN/web/packages/plotGoogleMaps/vignettes/plotGoogleMaps-intro.pdf). These got me started and within a day I could make some silly maps.

Lastly, this [pdf](http://statacumen.com/teach/SC1/SC1_16_Maps.pdf) probably contains some codes pertinent to the map you wish to create. Its $\LaTeX$ format is a bit off, but you can still get some general functionality and key commands.

Good luck!",added 2 characters in body,2013-10-10 16:15:27.743
185530,57236,594.0,5,,CC BY-SA 3.0,b1f8b7b3-792e-4bd2-92c1-93c18b2d8940,"In my experience and readings, Kaplan-Meier has always been used to calculate differential survival between a certain number of groups. However, I'm looking to assess how time to recovery from a certain event as measured by activity levels. At time zero, everyone is essentially ""dead"" (non-mobile), and with time they regain mobility. 
seems like a ""negative"" Kaplan-Meier, is that possible? Or should I be looking at a different modeling strategy?",added 1 characters in body,2013-10-10 16:20:04.957
185531,57233,5237.0,5,,CC BY-SA 3.0,43adffab-4dcd-42fa-aaa0-8210755cbf03,"*(I'll let someone else address the estimation of the missing data.  You may want to model the probability that the observation is each level of the unknown factor directly using knowledge of other covariate values, and possibly outside information, e.g., priors etc.  However, at first glance your approach looks reasonable to me.)*  

One note is that I can't tell from your description if you are weighting by *raw frequencies*.  If so, you want to divide these by $N$ to get the marginal *probabilities* instead.  

You are right that you are not handling level 3 correctly.  The coding scheme that you use in your question set up is known as *reference level coding*.  To use this approach correctly, you need to have an intercept (i.e., $\beta_0$), which estimates the mean of level 3.  I suspect you do have such, even though you didn't list it.  In this case, you would just add the intercept to your final equation.  That is: 
$$
\beta_0\!*\!f_3 + \beta_1\!*\!f_1 + \beta_2\!*\!f_2
$$
Note that you are multiplying the intercept (which encodes the reference level) by the marginal probability that the observation is actually the reference level.  ",added 295 characters in body,2013-10-10 16:20:30.040
185534,57237,22570.0,2,,CC BY-SA 3.0,23b9ecac-78b5-4afa-a218-b6bc9d6a27ec,"I'm trying to generate sets of causally connected random variables and started off doing this with a monte carlo approach.

The baseline is a 2-dimensional measured histogram from which I draw random values.

In my concrete examples these variables are acceleration $\bf{a}$ and velocity $\bf{v}$ - so obviously
$v_{i+1} = v_{i} + a_i * dt$
has to hold.

My current naive approach is:

I start with a some $v_0$.
Then I generate a random $a_0$ according to the measured probability of $\bf{a}$ for the value of $v_0$. Using this $a_0$ I can calculate $v_1$ and the whole procedure starts over again.

So when I check the generated accelerations $\bf{a}$ in bins of $\bf{v}$ everything's fine.
But I obviously this does not at all respect the marginal distribution of $\bf{v}$.

I'm kind of familiar with basic monte carlo methods, though lacking some theoretical background as you might guess.
I'd be fine if the two variables where *just* connected by some correlation matrix, but the causal connection between the two gives me headaches.

I didn't manage to find an example for this kind of problem somewhere - I might be googl'ing the wrong terms.
I'd be satisfied if somebody could point me to some literature/example or promising method to get a hold on this.

(Or tell me that's is not really possible given my inputs - that's what I'm guessing occasionally...)
",,2013-10-10 16:26:50.467
185533,57237,22570.0,3,,CC BY-SA 3.0,23b9ecac-78b5-4afa-a218-b6bc9d6a27ec,<monte-carlo><random-generation>,,2013-10-10 16:26:50.467
185532,57237,22570.0,1,,CC BY-SA 3.0,23b9ecac-78b5-4afa-a218-b6bc9d6a27ec,Generating causally dependent random variables,,2013-10-10 16:26:50.467
185535,52871,,5,,CC BY-SA 3.0,26608ba7-0af3-4994-a6e7-c99c2918feeb,"I want to analyze a multilevel multidimensional model in WinBUGS. the model is as below (N=2362 students responding to K=45 items of a test, students are nested within J=116 schools):

    model{
    #responses
    for(i in 1:N){
    	for(j in 1:K){
    		logit(p[i,j])<- a1[j]*t[i,1]+a2[j]*t[i,2]-b[j]
    		y[i,j]~dbern(p[i,j] )
       	}
    	t[i,1:2]~dmnorm(mu[sc[i],1:2],tau.p[1:2,1:2])
    }
    #school level
    for(j in 1:J){  
    	mu[j,1:2]~dmnorm(m[j,1:2],tau.s[1:2,1:2])
    }    
    
    #priors
    for(j in 1:J){
    	m[j,1:2]~dmnorm(m0[1:2],cov[1:2,1:2])
    }
    
    tau.p[1:2,1:2]~dwish(cov[1:2,1:2],2)
    tau.s[1:2,1:2]~dwish(cov[1:2,1:2],2)
    sigma.p[1:2,1:2]<-inverse(tau.p[,])
    sigma.s[1:2,1:2]<-inverse(tau.s[,])
    s2p<-sum(sigma.p[,])
    s2s<-sum(sigma.s[,])
    rho<-(s2s)/(s2s+s2p)
    
    a1[1]~dlnorm(0,4)
    a2[1]<-0
    b[1]~dnorm(0,1)
    for(s in 2:K) {
    	a1[s]~dlnorm(0,4)
    	a2[s]~dlnorm(0,4)
      	b[s]~dnorm(0,1)
    }    
    }

I've set these functions as initial values:

    ini<-function(){
    list(tau.p=matrix(rgamma(4,100,100),2,2),
    tau.s=matrix(rgamma(4,100,100),2,2),
    t=rmvnorm(N,mean=c(0,0),sigma=diag(2)),
    m=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
    mu=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
    a1=rlnorm(K,0, 0.4),
    a2=c(NA,rlnorm(K-1,0, 0.4)),
    b=rnorm(45,0,0.5))
    }
I use rube package in R to check and run my analysis and everything looks fine. When I run the model I receive ""Trap 66 (postcondition violated)"" or ""undefined real result"". I think the problem is from the initials but I have no idea how to solve it.

Any idea?
",tidied up model to make it more legable,2013-10-10 16:37:14.287
185536,52871,,24,,CC BY-SA 3.0,26608ba7-0af3-4994-a6e7-c99c2918feeb,,"Proposed by 13267 approved by 805, 919 edit id of 5588",2013-10-10 16:37:14.287
185539,57238,8414.0,3,,CC BY-SA 3.0,ad6f0a81-99c3-4bab-a025-850786c926b7,<r><regression><mediation>,,2013-10-10 16:41:50.710
185538,57238,8414.0,1,,CC BY-SA 3.0,ad6f0a81-99c3-4bab-a025-850786c926b7,Simulating data to fit a mediation model,,2013-10-10 16:41:50.710
185537,57238,8414.0,2,,CC BY-SA 3.0,ad6f0a81-99c3-4bab-a025-850786c926b7,"I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by [Barron and Kenny (1986)][1] and described elsewhere such as [Judd, Yzerbyt, & Muller (2013)][2], mediation models for outcome `Y`, mediator `med`, and predictor `X` and are governed by the following three regression equations:

1. Y = b11 + b12 * X + e1
2. med = b21 + b22 * X + e2
3. Y = b31 + b32 * X + b32 * med + e3

The indirect effect or mediation effect of `X` on `Y` through `med` can either be defined as b22 * b32 or, equivalently, as b12 - b32.  Under the old framework of testing for mediation, mediation was established by testing b12 in equation 1, b22 in equation 2, and b32 in equation 3.

So far, I have attempted to simulate values of med and Y that are consistent with values of the various regression coefficients using `rnorm` in `R`, such as the code below:

    x <- rep(c(-.5, .5), 50)
    med <- 4 + .7 * x + rnorm(100, sd = 1) 

    # Check the relationship between x and med
    mod <- lm(med ~ x)
    summary(mod)

    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)
    
    # Check the relationships between x, med, and y
    mod <- lm(y ~ x + med)
    summary(mod)

    # Check the relationship between x and y -- not present
    mod <- lm(y ~ x)
    summary(mod)

However, it seems that sequentially generating `med` and `y` using equations 2 and 3 is not enough, since I am left with no relationship between `x` and `y` using this approach.

Can anyone help me find a procedure in R to generate variables `x`, `med`, and `y` that satisfy constraints that I set using equations 1, 2, and 3?

Thanks for your help!

  [1]: https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&_Kenny_1986.pdf
  [2]: http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf",,2013-10-10 16:41:50.710
185540,56372,21108.0,5,,CC BY-SA 3.0,701d356f-e4a4-437b-9bf6-60ad2bac8639,"I just made an implementation of P(A|B)/P(¬A|B) for a ""people who bought this also bought..."" algorithm.


I'm doing it by 

    P(A|B) = count_users(bought_A_and_B)/count_users(bought_A)
    P(¬A|B) = count_users(bought_B_but_not_A)/count_users(did_not_buy_A)

Then dividing the top one by the bottom one I get a score which makes absolute sense, but what kind of correlation am I calculating? What is this method called? Where can I read more about it?

**[EDIT]** This is not for using in a production environment, it is just some algorithm which appeared out of the blue in an online course I'm taking, I was just wondering where it could come from. Also, when the number of users who bought item B but not item A is zero I just skip the pair until I get more data. The same goes on when the number of users who bought A is zero.",added 4 characters in body,2013-10-10 16:58:07.030
185543,57239,22571.0,1,,CC BY-SA 3.0,1cdf7b79-4d01-4eba-9f5d-f41e717dcc59,Quantifying the relationship between two disparate time series,,2013-10-10 16:58:21.140
185541,57239,22571.0,2,,CC BY-SA 3.0,1cdf7b79-4d01-4eba-9f5d-f41e717dcc59,"I have two time series that have a roughly similar trend, though both variables are noisy. This graph shows means and standard errors throughout a season of measurements.

![enter image description here][1]

I'd like to be able to make a quantitative statement about the relationship between these two data sets.

While the two data sets were collected from the same experimental plots, the individual samples from which the means and standard errors were calculated are not meaningfully paired with one another, and you can see that the carbohydrate data set was measured more frequently.

By taking a subset of the carbohydrate measurements that are closest to the microbial biomass measurement dates, I can make a scatterplot showing the means and standard errors that I think gives a fair visual representation of the relationship (TRS.ml is the carbohydrates):

![enter image description here][2]


  [1]: https://i.stack.imgur.com/7ThlT.png
  [2]: https://i.stack.imgur.com/kLvc0.png

This is where I am stuck. I'm not sure how to estimate regression coefficients or calculate an r2 value for a regression of this sort where I have estimates of uncertainty for both variables. Here are some approaches I have been considering:

1. Deming regression. I'm not sure that this would be the right approach. It seems to be more for data sets in which the same technique was used for both variables. If it is, my question is how would I calculate the variance ratio based on the information I have?

2. Regression of all underlying data points. This doesn't really work because the data are not meaningfully paired, so of the 80 or so microbial biomass measurements that underlie the data shown in the graphs here, I can't directly match them to individual measurements of carbohydrates. Matching them arbitrarily seems bad.

3. Regression of carbohydrate means by date against microbial biomass means by date. Basically regress the points in my scatterplot above but throw out the information about the uncertainty. This gives a high r2 driven by the coinciding peaks on July 1st, but to me, seems to overestimate the strength of the relationship.

4. Regression of all microbial biomass values against carbohydrate means by date or vice versa. This allows more of the underlying uncertainty to be incorporated while not forcing the pairing of unrelated data points in an arbitrary way. Again though, it does not incorporate the uncertainty in both variables.

My question is which of these approaches, or any other unlisted approaches, would you recommend for quantifying the relationship between these two time series?",,2013-10-10 16:58:21.140
185542,57239,22571.0,3,,CC BY-SA 3.0,1cdf7b79-4d01-4eba-9f5d-f41e717dcc59,<regression><time-series>,,2013-10-10 16:58:21.140
185544,57240,22527.0,2,,CC BY-SA 3.0,b5234575-4030-42a7-bde9-cb3ae105b349,"I believe you multiply by 2 because you need to control for your database being twice as large. There are other ways to do this calculation such as  

decoy spectra identified /  target spectra identified

The use of the term FDR for this calculation is totally confusing and is why people have started calling it target/decoy rate in the last year or so. It's also doubly confusing as people often fail to specify if they are using a spectra target/decoy or a peptide target/decoy...and there is no way to consistently calculate a protein target/decoy as different programs will weigh peptide --> protein evidence differently...It's a mess...Having said that I will always make this calculation just to double check I or the software has not done something stupid. For that it is very useful. ",,2013-10-10 16:59:34.360
185546,57238,17249.0,5,,CC BY-SA 3.0,7c1412a2-e6c8-486c-b9c2-c4e3cbbaea21,"I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by [Barron and Kenny (1986)][1] and described elsewhere such as [Judd, Yzerbyt, & Muller (2013)][2], mediation models for outcome $Y$, mediator $med$, and predictor $X$ and are governed by the following three regression equations:

1. $Y = b_{11} + b_{12}*X + e_1$
2. $med = b_{21} + b_{22}*X + e_2$
3. $Y = b_{31} + b_{32}*X + b_{32} * med + e_3$

The indirect effect or mediation effect of $X$ on $Y$ through $med$ can either be defined as $b_{22}*b_{32}$ or, equivalently, as $b_{12}-b_{32}$.  Under the old framework of testing for mediation, mediation was established by testing $b_{12}$ in equation 1, $b_{22}$ in equation 2, and $b_{32}$ in equation 3.

So far, I have attempted to simulate values of $med$ and $Y$ that are consistent with values of the various regression coefficients using `rnorm` in `R`, such as the code below:

    x <- rep(c(-.5, .5), 50)
    med <- 4 + .7 * x + rnorm(100, sd = 1) 

    # Check the relationship between x and med
    mod <- lm(med ~ x)
    summary(mod)

    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)
    
    # Check the relationships between x, med, and y
    mod <- lm(y ~ x + med)
    summary(mod)

    # Check the relationship between x and y -- not present
    mod <- lm(y ~ x)
    summary(mod)

However, it seems that sequentially generating $med$ and $Y$ using equations 2 and 3 is not enough, since I am left with no relationship between $X$ and $Y$ using this approach.

Can anyone help me find a procedure in R to generate variables $X$, $med$, and $Y$ that satisfy constraints that I set using equations 1, 2, and 3?

Thanks for your help!

  [1]: https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&_Kenny_1986.pdf
  [2]: http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf",Added LaTeX,2013-10-10 17:03:04.440
185545,57238,,24,,CC BY-SA 3.0,7c1412a2-e6c8-486c-b9c2-c4e3cbbaea21,,Proposed by 24808 approved by 11091 edit id of 5589,2013-10-10 17:03:04.440
185547,57241,5237.0,2,,CC BY-SA 3.0,82a6ee7f-1d2d-4638-8ead-f5e27515c7f3,"This is quite straightforward.  The reason you have no relationship between $x$ and $y$ using your approach is because of the code:  

    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)

If you want some relationship between $x$ and $y$ even when $med$ is included (that is, you want *partial* mediation), you would simply use a non-zero value for $b_{32}$ instead.  For example, you could substitute the following code for the above:  

    y <- 2.5 + 3 * x + .4 * med + rnorm(100, sd = 1)

Thus, $b_{32}$ has been changed from $0$ to $3$.  (Of course some other, specific value would probably be more relevant, depending on your situation, I just picked $3$ off the top of my head.)  ",,2013-10-10 17:09:45.623
185549,57238,5237.0,6,,CC BY-SA 3.0,9f3f096d-cb52-4893-947a-9f3279dc943e,<r><regression><simulation><random-generation><mediation>,added tags; removed thanks,2013-10-10 17:12:22.710
185548,57238,5237.0,5,,CC BY-SA 3.0,9f3f096d-cb52-4893-947a-9f3279dc943e,"I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by [Barron and Kenny (1986)][1] and described elsewhere such as [Judd, Yzerbyt, & Muller (2013)][2], mediation models for outcome $Y$, mediator $med$, and predictor $X$ and are governed by the following three regression equations:

1. $Y = b_{11} + b_{12}*X + e_1$
2. $med = b_{21} + b_{22}*X + e_2$
3. $Y = b_{31} + b_{32}*X + b_{32} * med + e_3$

The indirect effect or mediation effect of $X$ on $Y$ through $med$ can either be defined as $b_{22}*b_{32}$ or, equivalently, as $b_{12}-b_{32}$.  Under the old framework of testing for mediation, mediation was established by testing $b_{12}$ in equation 1, $b_{22}$ in equation 2, and $b_{32}$ in equation 3.

So far, I have attempted to simulate values of $med$ and $Y$ that are consistent with values of the various regression coefficients using `rnorm` in `R`, such as the code below:

    x <- rep(c(-.5, .5), 50)
    med <- 4 + .7 * x + rnorm(100, sd = 1) 

    # Check the relationship between x and med
    mod <- lm(med ~ x)
    summary(mod)

    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)
    
    # Check the relationships between x, med, and y
    mod <- lm(y ~ x + med)
    summary(mod)

    # Check the relationship between x and y -- not present
    mod <- lm(y ~ x)
    summary(mod)

However, it seems that sequentially generating $med$ and $Y$ using equations 2 and 3 is not enough, since I am left with no relationship between $X$ and $Y$ using this approach.

Can anyone help me find a procedure in R to generate variables $X$, $med$, and $Y$ that satisfy constraints that I set using equations 1, 2, and 3?


  [1]: https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&_Kenny_1986.pdf
  [2]: http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf",added tags; removed thanks,2013-10-10 17:12:22.710
185558,57231,22381.0,5,,CC BY-SA 3.0,7b7e38ea-8f9b-46a0-87b0-4ec78100d619,"I am using squared return as a proxy to calculate volatility, however i'm not sure whether to use raw return or percentage return. Under raw return all return estimates are below 1, however under percentage return there is a mix of return greater than 1 and less than 1. Percentage return below 1 would end up as a volatility figure less than the percentage return itself, on the other hand percentage return above 1 would end up as a volatility figure greater than percentage return. 

My question is: Doesn't this pose a problem when calculating volatility in that there is an over estimation when the return is above 1?

I am going to use the data to fit an ARMA-GARCH model, would there be any difference if I used percentage or absolute values?",deleted 10 characters in body; edited title,2013-10-10 18:20:07.200
185557,57231,22381.0,4,,CC BY-SA 3.0,7b7e38ea-8f9b-46a0-87b0-4ec78100d619,Raw Return vs Percentage Return to Calculate Volatility,deleted 10 characters in body; edited title,2013-10-10 18:20:07.200
185550,57233,5237.0,5,,CC BY-SA 3.0,08ae1146-8c96-4524-8bc8-d11ee34eed1c,"*(I'll let someone else address the estimation of the missing data.  You may want to directly model the probability that the observation is each level of the unknown factor using knowledge of other covariate values, and possibly outside information, e.g., priors etc.  There are strategies such as [propensity scores][1] that you might be able to use for this type of thing.  However, at first glance your approach looks reasonable to me.)*  

One note is that I can't tell from your description if you are weighting by *raw frequencies*.  If so, you want to divide these by $N$ to get the marginal *probabilities* instead.  

You are right that you are not handling level 3 correctly.  The coding scheme that you use in your question set up is known as *reference level coding*.  To use this approach correctly, you need to have an intercept (i.e., $\beta_0$), which estimates the mean of level 3.  I suspect you do have such, even though you didn't list it.  In this case, you would just add the intercept to your final equation.  That is: 
$$
\beta_0\!*\!f_3 + \beta_1\!*\!f_1 + \beta_2\!*\!f_2
$$
Note that you are multiplying the intercept (which encodes the reference level) by the marginal probability that the observation is actually the reference level.  


  [1]: http://en.wikipedia.org/wiki/Propensity_score_matching",added 174 characters in body,2013-10-10 17:15:34.363
185551,57238,8414.0,5,,CC BY-SA 3.0,e22a1519-3c6a-4839-b639-37c1dd00c20e,"I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by [Barron and Kenny (1986)][1] and described elsewhere such as [Judd, Yzerbyt, & Muller (2013)][2], mediation models for outcome $Y$, mediator $med$, and predictor $X$ and are governed by the following three regression equations:

1. $Y = b_{11} + b_{12}*X + e_1$
2. $med = b_{21} + b_{22}*X + e_2$
3. $Y = b_{31} + b_{32}*X + b_{32} * med + e_3$

The indirect effect or mediation effect of $X$ on $Y$ through $med$ can either be defined as $b_{22}*b_{32}$ or, equivalently, as $b_{12}-b_{32}$.  Under the old framework of testing for mediation, mediation was established by testing $b_{12}$ in equation 1, $b_{22}$ in equation 2, and $b_{32}$ in equation 3.

So far, I have attempted to simulate values of $med$ and $Y$ that are consistent with values of the various regression coefficients using `rnorm` in `R`, such as the code below:

    x <- rep(c(-.5, .5), 50)
    med <- 4 + .7 * x + rnorm(100, sd = 1) 

    # Check the relationship between x and med
    mod <- lm(med ~ x)
    summary(mod)

    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)
    
    # Check the relationships between x, med, and y
    mod <- lm(y ~ x + med)
    summary(mod)

    # Check the relationship between x and y -- not present
    mod <- lm(y ~ x)
    summary(mod)

However, it seems that sequentially generating $med$ and $Y$ using equations 2 and 3 is not enough, since I am left with no relationship between $X$ and $Y$ in regression equation 1 (which models a simple bivariate relationship between $X$ and $Y$) using this approach.

Can anyone help me find a procedure in R to generate variables $X$, $med$, and $Y$ that satisfy constraints that I set using equations 1, 2, and 3?


  [1]: https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&_Kenny_1986.pdf
  [2]: http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf",Clarified text,2013-10-10 17:19:44.757
185552,57238,8414.0,5,,CC BY-SA 3.0,dfafbd85-c819-4032-b11e-d2f1bce451f0,"I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by [Barron and Kenny (1986)][1] and described elsewhere such as [Judd, Yzerbyt, & Muller (2013)][2], mediation models for outcome $Y$, mediator $med$, and predictor $X$ and are governed by the following three regression equations:

1. $Y = b_{11} + b_{12}*X + e_1$
2. $med = b_{21} + b_{22}*X + e_2$
3. $Y = b_{31} + b_{32}*X + b_{32} * med + e_3$

The indirect effect or mediation effect of $X$ on $Y$ through $med$ can either be defined as $b_{22}*b_{32}$ or, equivalently, as $b_{12}-b_{32}$.  Under the old framework of testing for mediation, mediation was established by testing $b_{12}$ in equation 1, $b_{22}$ in equation 2, and $b_{32}$ in equation 3.

So far, I have attempted to simulate values of $med$ and $Y$ that are consistent with values of the various regression coefficients using `rnorm` in `R`, such as the code below:

    x <- rep(c(-.5, .5), 50)
    med <- 4 + .7 * x + rnorm(100, sd = 1) 

    # Check the relationship between x and med
    mod <- lm(med ~ x)
    summary(mod)

    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)
    
    # Check the relationships between x, med, and y
    mod <- lm(y ~ x + med)
    summary(mod)

    # Check the relationship between x and y -- not present
    mod <- lm(y ~ x)
    summary(mod)

However, it seems that sequentially generating $med$ and $Y$ using equations 2 and 3 is not enough, since I am left with no relationship between $X$ and $Y$ in regression equation 1 (which models a simple bivariate relationship between $X$ and $Y$) using this approach.  This is important because one definition of the indirect (i.e., mediation) effect is $b_{12}-b_{32}$, as I describe above.

Can anyone help me find a procedure in R to generate variables $X$, $med$, and $Y$ that satisfy constraints that I set using equations 1, 2, and 3?


  [1]: https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&_Kenny_1986.pdf
  [2]: http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf",Added more clarifying text,2013-10-10 17:30:50.737
185553,57241,5237.0,5,,CC BY-SA 3.0,35b3dd40-8f3d-4df0-9d1b-0f1c94c59c32,"This is quite straightforward.  The reason you have no relationship between $x$ and $y$ using your approach is because of the code:  

    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)

If you want some relationship between $x$ and $y$ even when $med$ is included (that is, you want *partial* mediation), you would simply use a non-zero value for $b_{32}$ instead.  For example, you could substitute the following code for the above:  

    y <- 2.5 + 3 * x + .4 * med + rnorm(100, sd = 1)

Thus, $b_{32}$ has been changed from $0$ to $3$.  (Of course some other, specific value would probably be more relevant, depending on your situation, I just picked $3$ off the top of my head.)  

--------------
*Edit:*  
With respect to the marginal $x\rightarrow y$ relationship being non-significant, that is just a function of [statistical power][1].  Since the causal force of $x$ is passed entirely through $med$ in your original setup, you have lower power than you might otherwise.  Nonetheless, the effect is still *real* in some sense.  When I ran your original code (after having set the seed using `90` as a value that I again just picked off the top of my head), I did get a significant effect:  

    set.seed(90)
    x <- rep(c(-.5, .5), 50)
    med <- 4 + .7 * x + rnorm(100, sd = 1) 
    
    # Check the relationship between x and med
    mod <- lm(med ~ x)
    summary(mod)
    
    y <- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)
    
    # Check the relationships between x, med, and y
    mod <- lm(y ~ x + med)
    summary(mod)
    
    # Check the relationship between x and y -- not present
    mod <- lm(y ~ x)
    summary(mod)
    
    ...
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept)   3.8491     0.1151  33.431   <2e-16 ***
    x             0.5315     0.2303   2.308   0.0231 *  
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
    
    ...

To get more power, you can increase the $N$ you are using, or use smaller error values (i.e., use `sd=` values less than the default `1` in the `rnorm()` calls).  


  [1]: http://en.wikipedia.org/wiki/Statistical_power",added 1833 characters in body,2013-10-10 17:30:55.270
185554,57223,22564.0,5,,CC BY-SA 3.0,6cf4f662-320a-43a5-8faa-fd40d5e17655,"I have a problem like the following:

1) There are six measurements for each individual with large within-subject variance 

2) There are two groups (Treatment and Control)

3) Each group consists of 5 individuals

4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.


The data looks like this:
![http://s10.postimg.org/p9krg6f3t/examp.png][1]

And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. **This ignores within-subject variability**:


     n.simulations<-10000
        pvals=matrix(nrow=n.simulations,ncol=1)
        for(k in 1:n.simulations){
          subject=NULL
          for(i in 1:10){
            subject<-rbind(subject,as.matrix(rep(i,6)))
          }
          #set.seed(42)
          
          #Sample Subject Means
          subject.means<-rnorm(10,100,2)
          
          #Sample Individual Measurements
          values=NULL
          for(sm in subject.means){
            values<-rbind(values,as.matrix(rnorm(6,sm,20)))
          }
          
          out<-cbind(subject,values)
          
          #Split into GroupA and GroupB
          GroupA<-out[1:30,]
          GroupB<-out[31:60,]
          
          #Add effect size to GroupA
          GroupA[,2]<-GroupA[,2]+0
          
          colnames(GroupA)<-c(""Subject"", ""Value"")
          colnames(GroupB)<-c(""Subject"", ""Value"")
          
          #Calculate Individual Means and SDS
          GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
          for(i in 1:length(unique(GroupA[,1]))){
            GroupA.summary[i,1]<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
            GroupA.summary[i,2]<-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
          }
          colnames(GroupA.summary)<-c(""Mean"",""SD"")
          
          
          GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
          for(i in 1:length(unique(GroupB[,1]))){
            GroupB.summary[i,1]<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
            GroupB.summary[i,2]<-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
          }
          colnames(GroupB.summary)<-c(""Mean"",""SD"")
          
          Summary<-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
          colnames(Summary)[1]<-""Group""
          
          pvals[k]<-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
        }


And here is code for plots:


    #Plots
    par(mfrow=c(2,2))
    boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupA[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupA[,1]))){
      m<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      ci<-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupB[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupB[,1]))){
      m<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      ci<-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
            ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
            main=""Individual Averages"")
    stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)
    
    points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(.9,
             t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    
    points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(1.9,
             t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
         main=c(paste(""# sims="", n.simulations),
                paste(""% Sig p-values="",100*length(which(pvals<0.05))/length(pvals)))
    )

Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.

So what is the correct way to analyze this data?


**Bonus:**

The example above is a simplification. For the actual data: 

1) The within-subject variance is positively correlated with the mean. 

2) Values can only be multiples of two. 

3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. 

4) Number of Subjects in each group are not necessarily equal. 

Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.

**EDIT:**

Ok, here is what *actual* data looks like. There is also three groups rather than two:

![enter image description here][2]

dput() of data:

    structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
    3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
    6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
    10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
    12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
    15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
    18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
    22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
    6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
    2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
    12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
    10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
    20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
        NULL, c(""Group"", ""Subject"", ""Value"")))

  [1]: https://i.stack.imgur.com/55V9J.png
  [2]: https://i.stack.imgur.com/k1xWd.png",added data example,2013-10-10 17:35:10.020
185555,57230,668.0,5,,CC BY-SA 3.0,4f244ce1-6536-4446-bdb5-139a8f9978e6,"The data contain category and sub-category distributions. 

The categories are topics in a quiz such as: Music, Sports, Business. 

Each category has three levels to choose from: Basic, Standard and Advanced.

For example: A user might take a quiz on Music across different levels. Say the number of questions attempted is 100. The user would have answered them across levels. 40 for basic, 40 for standard and 20 for advanced. The data consist of counts of the questions attempted within each category for each user.

What is the best way to represent these data on a graph? Each graph would contain up to 5 main categories. 

",added 61 characters in body,2013-10-10 17:49:32.410
185556,57238,,25,,,3cb050b6-fcd0-4170-8049-f286538ff024,,http://twitter.com/#!/StackStats/status/388367281618960384,2013-10-10 18:15:50.177
185561,57242,22573.0,3,,CC BY-SA 3.0,16b13cf6-06c7-4ea8-9755-e85213458509,<normal-distribution><multivariate-analysis>,,2013-10-10 18:29:33.973
185560,57242,22573.0,1,,CC BY-SA 3.0,16b13cf6-06c7-4ea8-9755-e85213458509,estimate multivariate normal distribution by observing variance in different directions,,2013-10-10 18:29:33.973
186165,57195,5237.0,6,,CC BY-SA 3.0,eb4af092-a8ed-4b6c-8bdf-6f50e3f86525,<r><regression><data-visualization><spatial>,added tag,2013-10-14 03:30:03.973
186492,57513,10594.0,3,,CC BY-SA 3.0,96a35508-4fe5-4957-98d5-5291599c2ef8,<hypothesis-testing><statistical-significance><generalized-linear-model>,,2013-10-15 09:49:58.953
185559,57242,22573.0,2,,CC BY-SA 3.0,16b13cf6-06c7-4ea8-9755-e85213458509,"Assume I am looking for a normal distribution $\mathcal{N}(\mu,\Sigma)$. For simplicity let's say we only have 2 random variables $x$ and $y$ and a known $\mu=0$.

Is it possible to estimate $\Sigma$ by observing the variance along multiple directions?

For example, I measure the variance $\sigma_1$ along the vector $\mathbb{v}_1 = (x_1,y_1)^T$. In another step I obtain a different variance $\sigma_2$ from a different direction $\mathbb{v}_2 = (x_2,y_2)^T$. Ideally one would continue to observe these single variations in different directions and combine them in one multivariate normal distribution.

Does this make sense?",,2013-10-10 18:29:33.973
185562,57243,20473.0,2,,CC BY-SA 3.0,b9af84f5-8211-47e8-99cb-1454cca90874,"It appears you are confusing results that hold for a collection of random variables with the case of _one_ random variable.

When you have a series of observations, $x_1,...,x_n$, then ***if*** they are _contemporaneous_, ($x_{1t},...,x_{nt}$) they are considered as realizations of _n distinct random variables_ (that may be identically and independently distributed, or not). You _cannot_, in this case, assume that all are realizations of the _same_ random variable, because a random variable is a real-valued function: this means that at a specific point in time, it can have only one realization (take one value), otherwise it wouldn't be a function but a correspondence: this is why when we have a _cross-sectional_ sample of size $n$, we say that ""it is comprised of the realization of $n$ random variables"", and not  ""$n$ realizations of the same random variable"".  Note carefully that ""same"" does not just mean ""identically distributed"", but ontologically equal.

Assume now that you have a time-series, and the index $1,...,n$ represents different points in time. Can you say that they are all realizations of the _same_ random variable? Well in principle you can, but here too, we tend to view a time series as a stochastic process of _distinct_ random variables (one for each point in time), that, again, may be identically distributed.  

So in general, when looking at a sample, be it cross-sectional or time series, it is advisable to think of it as a collection of realizations of _many_ random variables.

Now, when we subtract the mean from a random variable, and divide by the standard deviation, we create the ""standardized"" version of the variable, that has mean zero and variance (and standard deviation) unity. This is irrespective of the distribution that this variable follows, because, by standard universal properties of these distribution moments
$$Z = \frac {X-\mu}{\sigma} \Rightarrow E(Z) = \frac {1}{\sigma}E(X) - \frac {\mu}{\sigma} = 0$$

and

$$ \text {Var}(Z) = \text {Var}\left(\frac {X-\mu}{\sigma}\right) = \frac {1}{\sigma^2}\text {Var}(X) = \frac {\sigma^2}{\sigma^2} =1$$

The standardized version $Z$ of _one_ random variable $X$ follows a distribution that belongs to the same family as the distribution of $X$, with different said parameters - the distribution _family_ does not change. So if you don't know the distribution by other means, the distribution of the standardized version will remain unknown.

Now consider the random variable $S_n = \frac 1n\sum_{i=1}^nX_i$. It is for this and like quantities that the various [Central Limit Theorems][1] talk about and tell that they approach a normal distribution asymptotically.


  [1]: http://en.wikipedia.org/wiki/Central_limit_theorem",,2013-10-10 18:52:09.813
185565,57244,20179.0,1,,CC BY-SA 3.0,4f9639fe-3c18-4bec-a679-d6bbc97f3efa,genotype coding in regression,,2013-10-10 18:54:07.593
185564,57244,20179.0,2,,CC BY-SA 3.0,4f9639fe-3c18-4bec-a679-d6bbc97f3efa,"I would like to conduct some analysis on some biological traits with regression model.
The response variable is continuous. One important independent variable is the SNP informaion(wildtype, heterzygous or homozygous ). There are different ways to code it.
It can be treated as a nominal variable, or a ordinal variable(like 1,2,3). Any one familar with the difference and any classical references about it? 
Thank you for any suggestion.

Best",,2013-10-10 18:54:07.593
185563,57244,20179.0,3,,CC BY-SA 3.0,4f9639fe-3c18-4bec-a679-d6bbc97f3efa,<regression><categorical-data><genetics>,,2013-10-10 18:54:07.593
185566,57245,346.0,2,,CC BY-SA 3.0,1ecb6780-da48-4a85-abd6-837295d7c3c0,"I take the freedom to answer the question in the title, how would I analyze this data.

Given that we have replicates within the samples, mixed models immediately come to mind, which should estimate the variability within each individual and control for it.

Hence I fit the model using `lmer` from `lme4`. However, as we are interested in p-values, I use `mixed` from `afex` which obtains those via `pbkrtest` (i.e., Kenward-Rogers approximation for degrees-of-freedom). (afex also already sets the contrast to `contr.sum` which I would use in such a case anyway)

To control for the zero floor effect (i.e., positive skew), I fit two alternative versions transforming the dependent variable either with `sqrt` for mild skew and `log` for stronger skew.

    require(afex)
    
    # read the dput() in as dat <- ...    
    dat <- as.data.frame(dat)
    dat$Group <- factor(dat$Group)
    dat$Subject <- factor(dat$Subject)
    
    (model <- mixed(Value ~ Group + (1|Subject), dat))
    ##        Effect    stat ndf ddf F.scaling p.value
    ## 1 (Intercept) 237.730   1  15         1  0.0000
    ## 2       Group   7.749   2  15         1  0.0049
    
    (model.s <- mixed(sqrt(Value) ~ Group + (1|Subject), dat))
    ##        Effect    stat ndf ddf F.scaling p.value
    ## 1 (Intercept) 418.293   1  15         1  0.0000
    ## 2       Group   4.121   2  15         1  0.0375
    
    (model.l <- mixed(log1p(Value) ~ Group + (1|Subject), dat))
    ##        Effect    stat ndf ddf F.scaling p.value
    ## 1 (Intercept) 458.650   1  15         1  0.0000
    ## 2       Group   2.721   2  15         1  0.0981

The effect is significant for the untransformed and `sqrt` dv. But are these model sensible? Let's plot the residuals.

    png(""qq.png"", 800, 300, units = ""px"", pointsize = 12)
    par(mfrow = c(1, 3))
    par(cex = 1.1)
    par(mar = c(2, 2, 2, 1)+0.1)
    qqnorm(resid(model[[2]]), main = ""original"")
    qqline(resid(model[[2]]))
    qqnorm(resid(model.s[[2]]), main = ""sqrt"")
    qqline(resid(model.s[[2]]))
    qqnorm(resid(model.l[[2]]), main = ""log"")
    qqline(resid(model.l[[2]]))
    dev.off()


![enter image description here][1]

It seems that the model with `sqrt` trasnformation provides a reasonable fit (there still seems to be one outlier, but I will ignore it). So, let's further inspect this model using `multcomp` to get the comparisons among groups:

    require(multcomp)
    
    # using bonferroni-holm correction of multiple comparison
    summary(glht(model.s[[2]], linfct = mcp(Group = ""Tukey"")), test = adjusted(""holm""))
    ##          Simultaneous Tests for General Linear Hypotheses
    ## 
    ## Multiple Comparisons of Means: Tukey Contrasts
    ## 
    ## 
    ## Fit: lmer(formula = sqrt(Value) ~ Group + (1 | Subject), data = data)
    ## 
    ## Linear Hypotheses:
    ##            Estimate Std. Error z value Pr(>|z|)  
    ## 2 - 1 == 0  -0.0754     0.3314   -0.23    0.820  
    ## 3 - 1 == 0   1.1189     0.4419    2.53    0.023 *
    ## 3 - 2 == 0   1.1943     0.4335    2.75    0.018 *
    ## ---
    ## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    ## (Adjusted p values reported -- holm method)
    
    # using default multiple comparison correction (which I don't understand)
    summary(glht(model.s[[2]], linfct = mcp(Group = ""Tukey"")))
    ##          Simultaneous Tests for General Linear Hypotheses
    ## 
    ## Multiple Comparisons of Means: Tukey Contrasts
    ## 
    ## 
    ## Fit: lmer(formula = sqrt(Value) ~ Group + (1 | Subject), data = data)
    ## 
    ## Linear Hypotheses:
    ##            Estimate Std. Error z value Pr(>|z|)  
    ## 2 - 1 == 0  -0.0754     0.3314   -0.23    0.972  
    ## 3 - 1 == 0   1.1189     0.4419    2.53    0.030 *
    ## 3 - 2 == 0   1.1943     0.4335    2.75    0.016 *
    ## ---
    ## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    ## (Adjusted p values reported -- single-step method)

Punchline: group 3 differs from the other two groups which do not differ among each other.

  [1]: https://i.stack.imgur.com/WXUEh.png",,2013-10-10 18:54:30.263
185567,57246,22059.0,2,,CC BY-SA 3.0,c3d7f22c-ff10-4978-954f-139f2a142446,"You could, but it doesn't make to much sense to do it this way.  Since there are ${n+1}\choose 2$ independent dimensions in a covariance matrix, you'd need to measure the variance in ${n+1}\choose 2$ different directions, and then do some linear algebra to reconstruct the covariance matrix.  It's more straightforward just to measure the covariance matrix all at once.  

For example, in $2$ dimensions, you could measure the variance along $(0,1)$, $(0,1)$ and $(1,1)$, then since $v_{(1,1)}=\Sigma_{11}+\Sigma_{22}+2\Sigma_{12}$, you would end up with
$$\Sigma=\left(\array{v_{(1,0)} & \frac{v_{(1,1)}-v_{(1,0)}-v_{(0,1)}}{2}\\ \frac{v_{(1,1)}-v_{(1,0)}-v_{(0,1)}}{2} & v_{(0,1)}}\right)$$

I suppose if somehow you were constrained in such a way that you couldn't sample from the entire distribution, but you were only able to sample from projections onto a single dimension at a time, then this approach could be useful.",,2013-10-10 19:06:15.007
185568,57247,20286.0,2,,CC BY-SA 3.0,5c2e6502-62c3-4dd8-afa1-6c98060e99bd,"Unless you have reason to believe that LabA and LabB would systematically provide different results if they ever measured the same set of samples, your data from the 2 Labs ""are on the same scale"" as far as you can tell from these data. The problem is that the less-sensitive LabA will be unable to report a value for some samples that would have had values reported if LabB had instead done the analysis.

Perhaps the best way to proceed would be to define a class of results called ""<0.2"", and include in that class all 0 readings from LabA and all readings <0.2 from LabB. How you proceed from there depends on ""What is the question of interest?"" as @Glen_b put it in a comment.

All will be much more useful and reliable if it is possible to cross-compare a set of samples analyzed by both Labs, because there may be systematic differences between the 2 Labs' results that you don't suspect.

",,2013-10-10 19:08:35.603
185569,57246,22059.0,5,,CC BY-SA 3.0,38d8e909-0327-457e-b285-688639bff90d,"You could, but it doesn't make too much sense to do it this way.  Since there are ${n+1}\choose 2$ independent dimensions in a covariance matrix, you'd need to measure the variance in ${n+1}\choose 2$ different directions, and then do some linear algebra to reconstruct the covariance matrix.  It's more straightforward just to measure the covariance matrix all at once.  

For example, in $2$ dimensions, you could measure the variance along $(0,1)$, $(0,1)$ and $(1,1)$, then since $v_{(1,1)}=\Sigma_{11}+\Sigma_{22}+2\Sigma_{12}$, you would end up with
$$\Sigma=\left(\array{v_{(1,0)} & \frac{v_{(1,1)}-v_{(1,0)}-v_{(0,1)}}{2}\\ \frac{v_{(1,1)}-v_{(1,0)}-v_{(0,1)}}{2} & v_{(0,1)}}\right)$$

I suppose if somehow you were constrained in such a way that you couldn't sample from the entire distribution, but you were only able to sample from projections onto a single dimension at a time, then this approach could be useful.",added 1 characters in body,2013-10-10 19:12:14.623
185570,57228,503.0,10,,,1b2959fb-abe2-4973-a643-23f97c4f9c6a,"{""Voters"":[{""Id"":7290,""DisplayName"":""gung""},{""Id"":1036,""DisplayName"":""Andy W""},{""Id"":21054,""DisplayName"":""COOLSerdash""},{""Id"":17230,""DisplayName"":""Scortchi""},{""Id"":686,""DisplayName"":""Peter Flom""}]}",102,2013-10-10 19:14:24.500
185572,57244,674.0,5,,CC BY-SA 3.0,9f62081b-1051-4d2f-b0fc-233fd38ea38b,"I would like to conduct some analysis on some biological traits with regression model.
The response variable is continuous. One important independent variable is the SNP information (wildtype, heterozygous, or homozygous). There are different ways to code it.
It can be treated as a nominal or a ordinal variable (like 1, 2, 3). Any one familar with the difference and any classical references about it? 
Thank you for any suggestion.
",deleted 10 characters in body,2013-10-10 19:21:11.417
185571,57244,674.0,4,,CC BY-SA 3.0,9f62081b-1051-4d2f-b0fc-233fd38ea38b,Genotype coding in regression,deleted 10 characters in body,2013-10-10 19:21:11.417
186166,57417,594.0,5,,CC BY-SA 3.0,edcc3254-00af-48e6-a623-d55bae71f2dd,"Let $X_1, X_2,\ldots ,X_n$ be discrete random variables. 

I'm looking for a way to prove the random variables are independent but not identically distributed.

Can anyone suggest some ideas ?",formatting,2013-10-14 03:40:56.357
185577,57249,19264.0,2,,CC BY-SA 3.0,c104c76e-1ff7-4661-8be2-af1d7ad779d7,"I have [read](http://en.wikipedia.org/wiki/Gamma_distribution#Summation) that the sum of gamma distributions with the same scale parameter is another gamma distribution. I've also seen the paper by [Moschopoulos](http://www.ism.ac.jp/editsec/aism/pdf/037_3_0541.pdf) describing a method for the summation of a general set of gamma distributions. I have tried implementing Moschopoulos's method but have yet to have success.

What does the summation of a general set of gamma distributions look like? To make this question concrete, what does it look like for:

$Gamma(3,1) + Gamma(4,2) + Gamma(5,1)$

If the parameters above are not particularly revealing, please suggest others.",,2013-10-10 19:49:21.903
185580,57248,10278.0,5,,CC BY-SA 3.0,f5f0cb95-f9e0-4570-9e2d-5b5a8032d445,"If you treat the variable as ordinal you are assuming a gene-dosage effect.
This is essentially a two degree of freedom test.  You are doing a regression.
If you treat the variable as nominal you are not assuming any gene-dosage effect and instead comparing the mean of the three genotype groups this is a one degree of freedom test.  You are doing ANOVA with 3 categories.
Hence the gene-dosage model (treating genotypes as ordinal) is more powerful because you are using information about the genotype group (whether you 0, 1 or 2 copies of the wild type allele) whereas in the categorical approach your model knows nothing about the genotype groups (they could just be called A, B and C).  Treating the genotype as ordinal is the preferred approach.  Also I should mention that if you believe that for example the wild-type allele is be dominant then you can merge the heterozygous individuals into the wild-type homozygous group and treat them as one group.",better explanation of the difference between the models,2013-10-10 19:49:42.177
185583,57250,22577.0,2,,CC BY-SA 3.0,d83f29b1-d736-4cc9-ab55-597c445edc2b,"The scenario is like this:

I have a cohort with 2000 people, half of them taking DRUG, the other half not taking it. I wanna check the interactions between DRUG and the other variables in the model:

Method 1:

Firstly I got a original model:y1=a1*AGE+b1*BMI+c1*DRUG，[DRUG is binary: yes-1, no-0]; i got a likelihood 1;

If I want to test the interaction of AGE, BMI and DRUG, I need another model:y2=a2*AGE+b2*BMI+c2*DRUG+d*(DRUG*AGE)+e*(DRUG*BMI); i got a likelihood 2;

Then I compare the likelihood of these two models using chi-square test (df=2), and see whether the difference (likelihood 2 minus likelihood) is significant. 

Method 2:

Stratify people into two groups according to DRUG status:

Group 1: for people taking DRUG (n=1000), model 1: y1=a1*AGE+b1*BMI, i got a likelihood 1 (L1);

Group 2: for people not taking DRUG (n=1000), model 2: y2=a2*AGE+b2*BMI, likelihood 2 (L2);

Then I use all the people (n-2000), model 3:y3=a3*AGE+b3*BMI+d*(DRUG*AGE)+e*(DRUG*BMI), likelihood 3 (L3);

So in order to test the interactions, chi-square=L3/(L1*L2). But the question is: What is the degree of freedom (df)??

Can anyone help?? Cannot get the answer... Really thanks so much!!!

Cheers,
GL

",,2013-10-10 19:57:14.333
185582,57250,22577.0,1,,CC BY-SA 3.0,d83f29b1-d736-4cc9-ab55-597c445edc2b,how to determine degree of freedom (for test of interaction),,2013-10-10 19:57:14.333
185581,57250,22577.0,3,,CC BY-SA 3.0,d83f29b1-d736-4cc9-ab55-597c445edc2b,<interaction><degrees-of-freedom><stratification>,,2013-10-10 19:57:14.333
185586,57251,22578.0,3,,CC BY-SA 3.0,0938e44f-219f-4489-95f9-e7698c48dc26,<sampling><modeling><gibbs>,,2013-10-10 19:59:40.663
185585,57251,22578.0,1,,CC BY-SA 3.0,0938e44f-219f-4489-95f9-e7698c48dc26,Convergence theorem for Gibbs sampling,,2013-10-10 19:59:40.663
185584,57251,22578.0,2,,CC BY-SA 3.0,0938e44f-219f-4489-95f9-e7698c48dc26,"The convergence theorem for Gibbs sampling states:

Given a random Vektor $X$ with $X_1,X_2,...X_K$ and the knowlegde about the conditional distribution of $X_k$ we can find the actual distribution using Gibbs Sampling infinitly often.

While doing research on this, for a deeper understanding, I ran across [this][1] answer. Which explains quite well how to pick a single sample using the Method, but I am not able to extend/modify it to fit the convergence theorem, as the result of the given example is one sample (spell) and not a final/actual probability distribution.

**Therefore, how do I have to modify that example to fit the convergence theorem?**

  [1]: http://stats.stackexchange.com/a/10216/31349",,2013-10-10 19:59:40.663
185589,57252,22580.0,1,,CC BY-SA 3.0,6d8450a9-3b67-4af7-b8e6-6884726113e7,is good to standardize when you have an interaction?,,2013-10-10 19:59:51.410
185587,57252,22580.0,2,,CC BY-SA 3.0,6d8450a9-3b67-4af7-b8e6-6884726113e7,"I put this question because while reading the benefits of  standardizing explanatory variables or not, I read GOOD BUT CONTRASTING opinions about standardizing when there are interaction in the model. Some talk about how problems of collinearity are removed when standadizing (which is basically the case of my GLMM). However, other claim that standard errors and p-values of interactions of standardized models are not reliable... 
sooo, any ideas on what is the right thing to do? thanks",,2013-10-10 19:59:51.410
185588,57252,22580.0,3,,CC BY-SA 3.0,6d8450a9-3b67-4af7-b8e6-6884726113e7,<standardization>,,2013-10-10 19:59:51.410
185591,57250,674.0,5,,CC BY-SA 3.0,3d4cd37d-754d-48f9-b8a0-c1c029361a6a,"The scenario is like this:

I have a cohort with 2000 people, half of them taking DRUG, the other half not taking it. I would like to check interactions between DRUG and the other variables in the model:

* **Method 1:**

  Firstly I got a original model: `y1=a1*AGE+b1*BMI+c1*DRUG`，[`DRUG` is binary: yes-1, no-0]; I got a likelihood 1;

  If I want to test the interaction of `AGE`, `BMI` and `DRUG`, I need another model: `y2=a2*AGE+b2*BMI+c2*DRUG+d*(DRUG*AGE)+e*(DRUG*BMI)`; I got a likelihood 2;

  Then I compare the likelihood of these two models using chi-square test (df=2), and see whether the difference (likelihood 2 minus likelihood) is significant. 

* **Method 2:**

  Stratify people into two groups according to DRUG status:

  Group 1: for people taking DRUG (n=1000), model 1: `y1=a1*AGE+b1*BMI`, I got a likelihood 1 (L1);

  Group 2: for people not taking DRUG (n=1000), model 2: `y2=a2*AGE+b2*BMI`, likelihood 2 (L2);

  Then I use all the people (n-2000), model 3:y3=a3*AGE+b3*BMI+d*(DRUG*AGE)+e*(DRUG*BMI), likelihood 3 (L3);

So in order to test the interactions, chi-square=L3/(L1*L2). But the question is: What is the degree of freedom (df)?

Can anyone help? I cannot get the answer.",added 2 characters in body,2013-10-10 20:02:02.067
185590,57250,674.0,4,,CC BY-SA 3.0,3d4cd37d-754d-48f9-b8a0-c1c029361a6a,How to determine degree of freedom for a certain test of interaction?,added 2 characters in body,2013-10-10 20:02:02.067
185592,57249,674.0,4,,CC BY-SA 3.0,8aef8156-defd-400b-80d5-5c5eea124362,General sum of Gamma distributions,edited title,2013-10-10 20:02:49.223
185593,57231,674.0,4,,CC BY-SA 3.0,f877ebf7-12bc-42d6-a66a-b7146390e0b0,Raw return vs. percentage return to calculate volatility,edited title,2013-10-10 20:04:38.187
185595,57252,674.0,4,,CC BY-SA 3.0,ebb28b21-b3ff-4081-ae1f-13f178eba5bd,What are the pros and cons of standardizing variable in presence of an interaction?,added 1 characters in body; edited title,2013-10-10 20:06:49.197
186212,57396,,25,,,5344b91a-6a6e-46e0-876c-5fd69a97c12a,,http://twitter.com/#!/StackStats/status/389682357537878016,2013-10-14 09:21:28.723
185594,57252,674.0,5,,CC BY-SA 3.0,ebb28b21-b3ff-4081-ae1f-13f178eba5bd,"I put this question because while reading the benefits of  standardizing explanatory variables or not, I read *good but contrasting* opinions about standardizing when there are interaction in the model. 

Some talk about how problems of collinearity are removed when standardizing (which is basically the case of my GLMM). However, others claim that standard errors and p-values of interactions of standardized models are not reliable... 

So, any ideas on what is the right thing to do?",added 1 characters in body; edited title,2013-10-10 20:06:49.197
185598,57253,22582.0,3,,CC BY-SA 3.0,3467d558-fd84-4f3b-a2b7-3ce515b743cb,<data-transformation>,,2013-10-10 20:15:15.787
185597,57253,22582.0,1,,CC BY-SA 3.0,3467d558-fd84-4f3b-a2b7-3ce515b743cb,Box Cox Transformation with swift,,2013-10-10 20:15:15.787
185596,57253,22582.0,2,,CC BY-SA 3.0,3467d558-fd84-4f3b-a2b7-3ce515b743cb,"I am trying to do a box-cox transformation with swift. I have a dependent variable, annual foreign sales of companies (in US$ thousands) which contains zeros, for a set of panel data. I have been advised to add a small amount, for example, 0.00001 to the annual foreign sales figures so that I can take the log, but I think box-cox transformation will produce a more appropriate constant than 0.00001. I have done a box-cox transformation on R with the codes below, but it has given me a very large lambda2 of 31162.8.

library(geoR)
boxcoxfit(bornp$ForeignSales, lambda2 = TRUE)
#R output - Fitted parameters:
# lambda lambda2 beta sigmasq 
# -1.023463e+00 3.116280e+04 9.770577e-01 7.140328e-11

My hunch is that the above value of lambda2 is very large, so I am not sure if I need to run the boxcoxfit with my independent variables like below:
boxcoxfit(bornp$ForeignSales, bornp$family bornp$roa bornp$solvencyratio,
lambda2=TRUE)

I am still trying to identify the best set of independent variables, so I am not sure if using the boxcoxfit with independent variables at this stage will work or is best.

I would be very grateful for any advice on the above.
Thanks,
Jen
",,2013-10-10 20:15:15.787
185600,57253,668.0,5,,CC BY-SA 3.0,e29c06a1-f65f-4885-8bf9-f909083fc236,"I am trying to do a box-cox transformation with swift. I have a dependent variable, annual foreign sales of companies (in US\$ thousands) which contains zeros, for a set of panel data. I have been advised to add a small amount, for example, 0.00001 to the annual foreign sales figures so that I can take the log, but I think box-cox transformation will produce a more appropriate constant than 0.00001. I have done a box-cox transformation on R with the codes below, but it has given me a very large lambda2 of 31162.8.

    library(geoR)
    boxcoxfit(bornp$ForeignSales, lambda2 = TRUE)
    #R output - Fitted parameters:
    # lambda lambda2 beta sigmasq 
    # -1.023463e+00 3.116280e+04 9.770577e-01 7.140328e-11

My hunch is that the above value of lambda2 is very large, so I am not sure if I need to run the boxcoxfit with my independent variables like below:

    boxcoxfit(bornp$ForeignSales, bornp$family bornp$roa bornp$solvencyratio,lambda2=TRUE)

I am still trying to identify the best set of independent variables, so I am not sure if using the boxcoxfit with independent variables at this stage will work or is best.

I would be very grateful for any advice on the above.
",added 25 characters in body; edited tags,2013-10-10 20:17:08.960
185599,57253,668.0,6,,CC BY-SA 3.0,e29c06a1-f65f-4885-8bf9-f909083fc236,<r><data-transformation>,added 25 characters in body; edited tags,2013-10-10 20:17:08.960
185601,57206,22555.0,5,,CC BY-SA 3.0,b345b2e3-73e1-4ac8-9399-ec6042bfe5c7,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta 1 and Beta 2.  A Beta 1 of 0 and Beta 2 of 3 suggests that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta 1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta 2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying another distribution.  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Uniform:        [0,1.8]                                 [point]
    Exponential:    [4,9]                                   [point] 
    Normal:         [0,3]                                   [point]
    Students-t:     (0,3) to [0,10]                         [line]
    Lognormal:      (0,3) to [3.6,10]                       [line]
    Gamma:          (0,3) to (4,9)                          [line]
    Beta:           (0,3) to (4,9), (0,1.8) to (4,9)        [area]
    Beta J:         (0,1.8) to (4,9), (0,1.8) to [4,6*]     [area]
    Beta U:         (0,1.8) to (4,6), [0,1] to [4.5)        [area]
    Impossible:     (0,1) to (4.5), (0,1) to (4,1]          [area]
    Undefined:      (0,3) to (3.6,10), (0,10) to (3,6,10)   [area]

    Values of Beta1, Beta2 where brackets mean:
 
    [ ] : includes (closed)
    ( ) : approaches but does not include (open)
     *  : approximate 

These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",updated table of distributions,2013-10-10 20:25:20.590
185602,57206,22555.0,5,,CC BY-SA 3.0,7f406071-d5f9-43d9-9e1e-e44c06d3b1f5,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  If you can get a copy of **Hahn and Shapiro: Statistical Models in Engineering** you can look up the factors Beta 1 and Beta 2 (pages 42 to 49) and the Chart 6-1 of Page 197.  The theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate so called Beta1 and Beta2.  A Beta1 = 0 and Beta2 = 3 suggests that these data are approaching normality.  Given this is a rough test but with limited data any test could be considered a rough one.

Beta1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying other distributions (including Pearson Distributions I, I(U), I(J), II, II(U), III, IV, V, VI, VII).  For example, Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated:

    Where:   0 <= Beta1 <= 4
             1 <= Beta2 <= 10 

    Uniform:        [0,1.8]                                 [point]
    Exponential:    [4,9]                                   [point] 
    Normal:         [0,3]                                   [point]
    Students-t:     (0,3) to [0,10]                         [line]
    Lognormal:      (0,3) to [3.6,10]                       [line]
    Gamma:          (0,3) to (4,9)                          [line]
    Beta:           (0,3) to (4,9), (0,1.8) to (4,9)        [area]
    Beta J:         (0,1.8) to (4,9), (0,1.8) to [4,6*]     [area]
    Beta U:         (0,1.8) to (4,6), [0,1] to [4.5)        [area]
    Impossible:     (0,1) to (4.5), (0,1) to (4,1]          [area]
    Undefined:      (0,3) to (3.6,10), (0,10) to (3.6,10)   [area]

    Values of Beta1, Beta2 where brackets mean:
 
    [ ] : includes (closed)
    ( ) : approaches but does not include (open)
     *  : approximate 

These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",updated table of distributions,2013-10-10 20:32:52.977
185658,57270,22593.0,2,,CC BY-SA 3.0,1a8e6a02-0e64-4b5f-857f-40ecb23b1df6,"In a paper of Journal of Chemometrics (Naes & Mevik 2001), the authors propose to make simulations by creating two groups which are different with respect to the smallest eigen vector direction.

> Blockquote
Here the groups are different with respect to the orthogonal complement to the ﬁve ‘NIR loadings’.
This is achieved in the following way. The constant 0 ⋅ 18 is multiplied by a sixth loading vector
(orthogonal to the other ﬁve) and added to group 2. Both groups had initially the same means as group
1
> Blockquote

How can I compute such a simulation in R? The goal is to obtain group differences which are tied to the ""small eigen-vectors"" space.

Many thanks,

Julien",,2013-10-11 00:34:33.700
185603,57206,22555.0,5,,CC BY-SA 3.0,f6065214-789a-4d69-9be4-a700572f6c72,"This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).

As an alternative, you can look at kurtosis and skewness coefficients.  From **Hahn and Shapiro: Statistical Models in Engineering** some background is provided on the properties Beta1 and Beta2 (pages 42 to 49) and the Fig 6-1 of Page 197.  Additional theory behind this can be found on Wikipedia (see Pearson Distribution).

Basically you need to calculate the so-called properties Beta1 and Beta2.  A Beta1 = 0 and Beta2 = 3 suggests that the data set approaches normality.  This is a rough test but with limited data it could be argued that any test could be considered a rough one.

Beta1 is related to the moments 2 and 3, or variance and [skewness](http://en.wikipedia.org/wiki/Skewness), respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:

    Beta1 = SKEW(...)^2/VAR(...)^3

Beta2 is related to the moments 2 and 4, or the variance and [kurtosis](http://en.wikipedia.org/wiki/Kurtosis), respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:

    Beta2 = KURT(...)/VAR(...)^2

Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying other distributions (including Pearson Distributions I, I(U), I(J), II, II(U), III, IV, V, VI, VII).  For example, many of the commonly used distributions such as Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated from these properties:

    Where:   0 <= Beta1 <= 4
             1 <= Beta2 <= 10 

    Uniform:        [0,1.8]                                 [point]
    Exponential:    [4,9]                                   [point] 
    Normal:         [0,3]                                   [point]
    Students-t:     (0,3) to [0,10]                         [line]
    Lognormal:      (0,3) to [3.6,10]                       [line]
    Gamma:          (0,3) to (4,9)                          [line]
    Beta:           (0,3) to (4,9), (0,1.8) to (4,9)        [area]
    Beta J:         (0,1.8) to (4,9), (0,1.8) to [4,6*]     [area]
    Beta U:         (0,1.8) to (4,6), [0,1] to [4.5)        [area]
    Impossible:     (0,1) to (4.5), (0,1) to (4,1]          [area]
    Undefined:      (0,3) to (3.6,10), (0,10) to (3.6,10)   [area]

    Values of Beta1, Beta2 where brackets mean:
 
    [ ] : includes (closed)
    ( ) : approaches but does not include (open)
     *  : approximate 

These are illustrated in Hahn and Shapiro Fig 6-1.

Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.

There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.



",added 101 characters in body,2013-10-10 20:47:04.627
185605,57254,22583.0,1,,CC BY-SA 3.0,d3cc2c9c-ba44-4df4-ae89-9e27509931c0,variance of compound variable?,,2013-10-10 20:48:01.520
185604,57254,22583.0,2,,CC BY-SA 3.0,d3cc2c9c-ba44-4df4-ae89-9e27509931c0,"here is my situation. I am weighting a packet of material that has 10 individual units in it. In the end of the day I would like to know the average weight and variance of the individual units but the problem is that I cannot weight each unit individually since I would have to destroy the packet to get to the individual units. So in lieu of this, I am trying to make an inference of the individual units from what I know about the packets. I weighed 10 packets (hence I have 100 individual units). I was able to figure out the average weight of the units but am having trouble with the variance. Here is what I have done so far:

$$
\begin{split}
\bar{y}&=\frac{1}{10}\sum^{10}_{i=1}y_i\\
       &=\frac{1}{10}\sum^{10}_{i=1}  (x_{i,1}+x_{i,2}+...+x_{i,10})~since~y_i=x_{i,1}+x_{i,2}+...+x_{i,10}\\
       &=\frac{1}{10}\sum^{100}_{j=1}x_j\\
       &=\frac{1}{10}(100~\bar{x})=10~\bar{x}
\end{split}
$$

thus we have the average of $x$, $\bar{x}=\frac{\bar{y}}{10}.$ But now my challenge is how to do I find variance of $x$ given the variance of $y$? Any suggestions? Thanks!",,2013-10-10 20:48:01.520
185606,57254,22583.0,3,,CC BY-SA 3.0,d3cc2c9c-ba44-4df4-ae89-9e27509931c0,<variance><aggregation><sum>,,2013-10-10 20:48:01.520
185607,57255,19545.0,3,,CC BY-SA 3.0,6b75e78c-ade3-479f-8f43-88efc5713fd3,<ranking><information-retrieval>,,2013-10-10 20:53:13.273
185608,57255,19545.0,2,,CC BY-SA 3.0,6b75e78c-ade3-479f-8f43-88efc5713fd3,"It seems to me that normalized ERR scores (ERR scores of your ranking algorithm divided by ERR score calculated for the ground truth ranking) are more useful than the unscaled ERR scores, but I have not seen normalized scores being reported in the literature. Is there a good reason that the ERR scores are reported in raw rather than normalized format?",,2013-10-10 20:53:13.273
185609,57255,19545.0,1,,CC BY-SA 3.0,6b75e78c-ade3-479f-8f43-88efc5713fd3,Why are ERR scores not normalized?,,2013-10-10 20:53:13.273
185611,57254,22583.0,4,,CC BY-SA 3.0,01077b8e-0e3b-4ca0-bb57-bb4cbc6dd67a,variance of summation/compound variable?,edited title,2013-10-10 20:56:04.337
185612,57256,668.0,2,,CC BY-SA 3.0,96d03b29-904a-447d-8814-545270fc6bdc,"First, **combine any sums having the same scale factor**: a $\Gamma(n, \beta)$ plus a $\Gamma(m,\beta)$ variate form a $\Gamma(n+m,\beta)$ variate.

Next, observe that the characteristic function (cf) of $\Gamma(n, \beta)$ is $(1-i \beta  t)^{-n}$, whence the cf of a sum of these distributions is the product

$$\prod_{j} \frac{1}{(1-i \beta_j  t)^{n_j}}.$$

When the $n_j$ are all *integral,* **this product expands as a partial fraction** into a *linear combination* of $(1-i \beta_j  t)^{-\nu}$ where the $\nu$ are integers between $1$ and $n_j$.  In the example with $\beta_1 = 1, n_1=8$ (from the sum of $\Gamma(3,1)$ and $\Gamma(5,1)$) and $\beta_2 = 2, n_2=4$ we find

$$\frac{1}{(1-i t)^{8}}\frac{1}{(1- 2i t)^{4}} = \\
\frac{1}{(x+i)^8}-\frac{8 i}{(x+i)^7}-\frac{40}{(x+i)^6}+\frac{160 i}{(x+i)^5}+\frac{560}{(x+i)^4}-\frac{1792 i}{(x+i)^3}\\-\frac{5376}{(x+i)^2}+\frac{15360 i}{x+i}+\frac{256}{(2 x+i)^4}+\frac{2048 i}{(2 x+i)^3}-\frac{9216}{(2 x+i)^2}-\frac{30720 i}{2 x+i}.$$

The inverse of taking the cf is the inverse Fourier Transform, which is *linear*: that means we may apply it term by term.  Each term is recognizable as a multiple of the cf of a Gamma distribution and so is readily **inverted to yield the PDF**.  In the example we obtain

$$\frac{e^{-t} t^7}{5040}+\frac{1}{90} e^{-t} t^6+\frac{1}{3} e^{-t} t^5+\frac{20}{3} e^{-t} t^4+\frac{8}{3} e^{-\frac{t}{2}} t^3+\frac{280}{3} e^{-t} t^3\\
-128 e^{-\frac{t}{2}} t^2+896 e^{-t} t^2+2304 e^{-\frac{t}{2}} t+5376 e^{-t} t-15360 e^{-\frac{t}{2}}+15360 e^{-t}$$

for the PDF of the sum.

---

As a test, here is a histogram of $10^4$ results obtained by adding independent draws from the $\Gamma(8,1)$ and $\Gamma(4,2)$ distributions.  On it is superimposed the graph of $10^4$ times the preceding function.  The fit is very good.

![Figure][1]

---

Moschopoulos carries this idea one step further by expanding the cf of the sum into an *infinite* series of Gamma characteristic functions whenever one or more of the $n_i$ is non-integral, and then terminates the infinite series at a point where it is reasonably well approximated.


  [1]: https://i.stack.imgur.com/sOPCo.png",,2013-10-10 20:58:56.470
185613,57248,10278.0,5,,CC BY-SA 3.0,da12c19c-42b1-4077-abaa-451e9490a4cb,"If you treat the variable as ordinal you are assuming a gene-dosage effect.
This is essentially a one degree of freedom test.  You are testing whether the slope of the regression line is significantly different from $0$.  If you treat the variable as nominal you are not assuming any gene-dosage effect and instead you are doing a one way ANOVA with 3 groups so that's a two degrees of freedom test.
The gene-dosage model (treating genotypes as ordinal) is more powerful because you are using information about the genotype group (whether you 0, 1 or 2 copies of the wild type allele) whereas in the categorical approach your model knows nothing about the genotype groups (they could just be called A, B and C).  Treating the genotype as ordinal is the preferred approach.  Also I should mention that if you believe that for example the wild-type allele is dominant then you can merge the heterozygous individuals into the wild-type homozygous group and treat them as one group.",incorporated @andrea's correction about gene-dosage regression test being 1 dof and the 3 group ANOVA being a 2 dof test,2013-10-10 20:59:23.583
185641,57265,2490.0,2,,CC BY-SA 3.0,bfc8d250-9121-4eba-adef-d8272f13a86e,"I'm designing a pretty simple experiment that goes like this. Participants will be shown a series of stimuli and after viewing each one they will answer a few questions where they will make judgments about the stimulus - all Likert items. There are two kinds of stimuli. Probably obvious, but the hypothesis is that there will be a difference between answers for A vs B stimuli. There will be 30 or so stimuli, with an equal number of A and B stimuli. All participants will see all the stimuli (within-subjects).

I'm wondering if there would be a benefit to counterbalancing the order in which they receive the items, vs just showing everyone the same randomized sequence of stimuli (which is easier to setup).

If there's a better method I need to consider, I'd be interested in hearing about it. I also looked into blocking designs, but this is so simple that I don't think those apply here. I'm planning to analyze with t-tests or Mann-Whitney-Wilcoxon.",,2013-10-10 23:12:54.873
185614,57248,10278.0,5,,CC BY-SA 3.0,eb6fa4ca-f86c-4293-8ad1-ed0bc91c0b69,"If you treat the variable as ordinal you are assuming a gene-dosage effect.
This is essentially a one degree of freedom test since you are testing whether the slope of the regression line is significantly different from $0$.  If you treat the variable as nominal you are not assuming any gene-dosage effect and instead you are doing a one way ANOVA with 3 groups so that's a two degrees of freedom test.
The gene-dosage model (treating genotypes as ordinal) is more powerful because you are using information about the genotype groups (whether the group has 0, 1 or 2 copies of the wild type allele) whereas in the categorical approach your model knows nothing about the genotype groups (they could just be called A, B and C).  Treating the genotype as ordinal is the preferred approach.  Also I should mention that if you believe that for example the wild-type allele is dominant then you can merge the heterozygous individuals into the wild-type homozygous group and treat them as one group.",incorporated @andrea's correction about gene-dosage regression test being 1 dof and the 3 group ANOVA being a 2 dof test,2013-10-10 21:06:45.523
185615,57256,668.0,5,,CC BY-SA 3.0,5966dd78-4a79-49c7-92d9-3e55b7aed5fe,"First, **combine any sums having the same scale factor**: a $\Gamma(n, \beta)$ plus a $\Gamma(m,\beta)$ variate form a $\Gamma(n+m,\beta)$ variate.

Next, observe that the characteristic function (cf) of $\Gamma(n, \beta)$ is $(1-i \beta  t)^{-n}$, whence the cf of a sum of these distributions is the product

$$\prod_{j} \frac{1}{(1-i \beta_j  t)^{n_j}}.$$

When the $n_j$ are all *integral,* **this product expands as a partial fraction** into a *linear combination* of $(1-i \beta_j  t)^{-\nu}$ where the $\nu$ are integers between $1$ and $n_j$.  In the example with $\beta_1 = 1, n_1=8$ (from the sum of $\Gamma(3,1)$ and $\Gamma(5,1)$) and $\beta_2 = 2, n_2=4$ we find

$$\frac{1}{(1-i t)^{8}}\frac{1}{(1- 2i t)^{4}} = \\
\frac{1}{(x+i)^8}-\frac{8 i}{(x+i)^7}-\frac{40}{(x+i)^6}+\frac{160 i}{(x+i)^5}+\frac{560}{(x+i)^4}-\frac{1792 i}{(x+i)^3}\\-\frac{5376}{(x+i)^2}+\frac{15360 i}{x+i}+\frac{256}{(2 x+i)^4}+\frac{2048 i}{(2 x+i)^3}-\frac{9216}{(2 x+i)^2}-\frac{30720 i}{2 x+i}.$$

The inverse of taking the cf is the inverse Fourier Transform, which is *linear*: that means we may apply it term by term.  Each term is recognizable as a multiple of the cf of a Gamma distribution and so is readily **inverted to yield the PDF**.  In the example we obtain

$$\frac{e^{-t} t^7}{5040}+\frac{1}{90} e^{-t} t^6+\frac{1}{3} e^{-t} t^5+\frac{20}{3} e^{-t} t^4+\frac{8}{3} e^{-\frac{t}{2}} t^3+\frac{280}{3} e^{-t} t^3\\
-128 e^{-\frac{t}{2}} t^2+896 e^{-t} t^2+2304 e^{-\frac{t}{2}} t+5376 e^{-t} t-15360 e^{-\frac{t}{2}}+15360 e^{-t}$$

for the PDF of the sum.

This is a finite *mixture* of Gamma distributions having scale factors equal to those within the sum and shape factors less than or equal to those within the sum.  Except in special cases (where some cancellation might occur), the number of terms is given by the total shape parameter $n_1 + n_2 + \cdots$ (assuming all the $n_j$ are different).

---

As a test, here is a histogram of $10^4$ results obtained by adding independent draws from the $\Gamma(8,1)$ and $\Gamma(4,2)$ distributions.  On it is superimposed the graph of $10^4$ times the preceding function.  The fit is very good.

![Figure][1]

---

Moschopoulos carries this idea one step further by expanding the cf of the sum into an *infinite* series of Gamma characteristic functions whenever one or more of the $n_i$ is non-integral, and then terminates the infinite series at a point where it is reasonably well approximated.


  [1]: https://i.stack.imgur.com/sOPCo.png",added 349 characters in body,2013-10-10 21:07:59.390
185616,57257,10570.0,2,,CC BY-SA 3.0,b3027061-966f-4f2b-8f35-b3f45dcb01c9,"Your question is not really possible to answer unless you have additional information about the situation you are applying this to.

###Indistinguishable situations

For the purposes of this, we'll assume that $X$, $Y$, and $Z$ are 0-mean multivariate normal distributions in $\mathbb{R}^d$, and we're interested in one or more spectrum $\sigma_i$ (a vector of size $d$ with decreasing values, yada yada). I refer to the components of the spectrum as _eigenvalues_, without specifying that they're the eigenvalues of the covariance matrix.

1. The true distribution is $X$ which has spectrum $\sigma_X$ with all non-zero values. There is no error, and we draw a large number of samples, estimating everything very accurately. Clearly all of the ""small"" eigenvalues still have ""information"" and aren't noise.

2. The true distribution  is $Y$ which has a spectrum $\sigma_Y$ with only 3 non-zero eigenvalues. There's noise, though, so we measure $Y+Z$, where $\sigma_Z$ _does_ have all non-zero eigenvalues. Let's suppose $Y$ and $Z$ are such that $\sigma_{Y+Z} = \sigma_X$. Here, it's obvious that all but the top 3 eigenvalues are ""merely noise"".

My point is just that which parts of the spectrum can be attributed to ""noise"" is not an property of the sample.

###External criteria

There potentially are external criteria that can help you distinguish the above situations, but they're sort of problem specific. For instance, in the [Netflix Challenge](http://www.netflixprize.com/), a very successful technique for predicting movie ratings was based on SVD (which is also the basis of PCA). When using using SVD-based algorithms for a prediction task, one is confronted with the same challenge you have: _""How many non-zero components do I consider? How far do I reduce the dimensionality?""_ The answer is basically [cross validation](http://en.wikipedia.org/wiki/Cross-validation_(statistics)). The more components you consider, the lower your training error is, but the more risk of overfitting. The validation error is a proxy for generalization error. So, you generally get a chart like:

![Training/Validation Error as a function of Model Capacity][1]

If you're not doing a predictive problem, I don't really have useful advice, but I do imagine there might be _something_ you want to measure that can help you define what it _means_ for something to be ""signal"" vs ""noise"" in your application.


  [1]: https://i.stack.imgur.com/XZJfg.png",,2013-10-10 21:11:46.330
185617,57128,,25,,,7fc03bde-06eb-4e68-90db-591afd33ff07,,http://twitter.com/#!/StackStats/status/388412947003084800,2013-10-10 21:17:17.657
185618,57258,6162.0,2,,CC BY-SA 3.0,54f33eda-f5fb-4151-a564-602f7465fd7c,"For information, the random-effect model given by @Henrik:

    > f <- function(x) sqrt(x)
    > library(lme4)
    > ( fit1 <- lmer(f(Value) ~ Group + (1|Subject), data=dat) )
    Linear mixed model fit by REML ['lmerMod']
    Formula: f(Value) ~ Group + (1 | Subject) 
       Data: dat 
    REML criterion at convergence: 296.3579 
    Random effects:
     Groups   Name        Std.Dev.
     Subject  (Intercept) 0.5336  
     Residual             0.8673  
    Number of obs: 108, groups: Subject, 18
    Fixed Effects:
    (Intercept)       Group2       Group3  
        3.03718     -0.07541      1.11886  

is equivalent to a generalized least-squares model with an exchangeable correlation structure for subjects:

    > library(nlme)
    > fit2 <-  gls(f(Value) ~ Group, data=dat, na.action=na.omit, correlation=corCompSymm(form= ~  1 | Subject)) 

The fitted variance matrix is then:

    > getVarCov(fit2)
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
    [1,] 1.03690 0.28471 0.28471 0.28471 0.28471 0.28471
    [2,] 0.28471 1.03690 0.28471 0.28471 0.28471 0.28471
    [3,] 0.28471 0.28471 1.03690 0.28471 0.28471 0.28471
    [4,] 0.28471 0.28471 0.28471 1.03690 0.28471 0.28471
    [5,] 0.28471 0.28471 0.28471 0.28471 1.03690 0.28471
    [6,] 0.28471 0.28471 0.28471 0.28471 0.28471 1.03690
      Standard Deviations: 1.0183 1.0183 1.0183 1.0183 1.0183 1.0183 

As you can see, the diagonal entry corresponds to the total variance in the first model:

    > VarCorr(fit1)
     Groups   Name        Std.Dev.
     Subject  (Intercept) 0.53358 
     Residual             0.86731 
    > 0.53358^2+0.86731^2
    [1] 1.036934

and the covariance corresponds to the within-subject variance:

    > 0.53358^2
    [1] 0.2847076

Actually the gls model is more general because it allows a negative covariance. The advantage of `nlme` is that you can more generally use other repeated correlation structures and also you can specify different variances per group with the `weights` argument.

I think that residuals are different because they are constructed with the random-effects in the first model. In order to get multiple comparisons you can use the `lsmeans` and the `multcomp` packages, but the $p$-values of the hypotheses tests are anticonservative with defaults (too high) degrees of freedom. Unfortunately, the `pbkrtest` package does not apply to `gls`/`lme` models.",,2013-10-10 21:31:42.880
185619,57251,22578.0,5,,CC BY-SA 3.0,78062366-3004-4e84-aa29-8d83d7b4a011,"The convergence theorem for Gibbs sampling states:

Given a random Vektor $X$ with $X_1,X_2,...X_K$ and the knowlegde about the conditional distribution of $X_k$ we can find the actual distribution using Gibbs Sampling infinitly often.

The exact theorem as stated by book (Neural Networks and Learning Machines): 
> The random variable $X_k(n)$
> converges in distribution to the true probabiluty distributions of
> $X_k$ for k=1,2,...,K as n approaches infinity
> 
> $\lim_{n -> \infty}P(x^{(n)}_k \leq x | x_k(0)) = P_{x_k}(x) $ for $k
> = 1,2,...,K$
> 
> Where $P_{X_k}(x)$  is the marginal cummulative distribution function
> of $X_k$

While doing research on this, for a deeper understanding, I ran across [this][1] answer. Which explains quite well how to pick a single sample using the Method, but I am not able to extend/modify it to fit the convergence theorem, as the result of the given example is one sample (spell) and not a final/actual probability distribution.

**Therefore, how do I have to modify that example to fit the convergence theorem?**

  [1]: http://stats.stackexchange.com/a/10216/31349",added in quotation for convergence theorem,2013-10-10 21:36:18.333
185661,57270,,24,,CC BY-SA 3.0,2bae73d1-069e-46ee-bad8-4a8a1acc2752,,Proposed by 22468 approved by -1 edit id of 5592,2013-10-11 01:02:37.520
185620,57259,6162.0,2,,CC BY-SA 3.0,627aad7e-2789-4bbd-b45a-d6076c84bb16,"Now, try to you write down the model: $y_{ijk} = ...$ where $y_{ijk}$ is the $k$-th value for individual $j$ of group $i$. Then look at what happens for the means $\bar y_{ij\bullet}$: you get a classical Gaussian linear model, with variance homogeneity because there are $6$ repeated measures for each subject:

    > xtabs(~Group+Subject, data=dat)
         Subject
    Group 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
        1 6 6 6 6 6 6 6 0 0  0  0  0  0  0  0  0  0  0
        2 0 0 0 0 0 0 0 6 6  6  6  6  6  6  6  0  0  0
        3 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  6  6  6

Thus, since you are interested in mean comparisons only, **you don't need to resort to a random-effect or generalised lead-squares model** - just use a classical (fixed effects) model using the means $\bar y_{ij\bullet}$ as the observations:

    tdat <- transform(dat, tvalue=f(Value))
    dd <- aggregate(tvalue~Group+Subject, data=tdat, FUN=mean)
    fit3 <- lm(tvalue~Group, data=dd)

The ANOVA provides the same answer as @Henrik's approach (and that shows that Kenward-Rogers approximation is correct):

    > anova(fit3)
    Analysis of Variance Table
    
    Response: tvalue
              Df Sum Sq Mean Sq F value  Pr(>F)  
    Group      2 3.3799 1.68994   4.121 0.03747 *

Then you can use `TukeyHSD()` or the `lsmeans` package for multiple comparisons: 

    > TukeyHSD(aov(fit3), ""Group"")
      Tukey multiple comparisons of means
        95% family-wise confidence level
    
    Fit: aov(formula = fit3)
    
    $Group
               diff         lwr       upr     p adj
    2-1 -0.07541248 -0.93627828 0.7854533 0.9719148
    3-1  1.11885667 -0.02896441 2.2666777 0.0565628
    3-2  1.19426915  0.06817536 2.3203629 0.0370434
    
    > library(lsmeans)
    > lsmeans(fit3, pairwise~Group)
    
    $`Group pairwise differences`
             estimate        SE df  t.ratio p.value
    1 - 2  0.07541248 0.3314247 15  0.22754 0.97191
    1 - 3 -1.11885667 0.4418996 15 -2.53193 0.05656
    2 - 3 -1.19426915 0.4335348 15 -2.75472 0.03704
        p values are adjusted using the tukey method for 3 means ",,2013-10-10 21:54:03.493
185621,57260,4779.0,2,,CC BY-SA 3.0,2eee46ca-be30-4256-9202-c772be7bca60,"Looking at this as an outlier problem seems wrong to me.  If ""< 10% of users spend at all"", you need to model that aspect.  Tobit or Heckman regression would be two possibilities.",,2013-10-10 21:55:57.283
185622,57065,5237.0,6,,CC BY-SA 3.0,e878b58f-8160-457e-af1b-b71dd952b5d6,<bootstrap><outliers>,added tag,2013-10-10 22:00:36.617
185623,57261,16703.0,2,,CC BY-SA 3.0,84755109-3175-48b2-b7dd-3b7e0edbe2a0,"I have $N$ pmfs, and for each each $L$ samples. Each sample has a variable amount of $x$ values, but the $x$ values that they have can be matched. So for example $sample_1 \rightarrow\ x_1 = 0, x_2 = 0, x_3 = 0.2, x_4 = 0.4, x_5 = 0.4\\sample_2 \rightarrow\ x_1 = 0.3,x_2=0, x_3 = 0.4, x_4 = 0.3,x_5=0$.

I'm using a python program (https://pypi.python.org/pypi/dirichlet/0.7) to calculate the mle from the samples which is a port of Thomas P. Minka's Matlab Fastfit code ([Estimating a Dirichlet Distribution][1]). The problem is that, for fitting, it sums over $logp+psi(\sum^k a_k) - logp$. Since some of the $x$ values are 0, some logp values are -inf. Therefore, summing over this makes everything -inf. 

How can I deal with 0 values when calculating the mle for a Dirichlet distribution? 


  [1]: http://research.microsoft.com/en-us/um/people/minka/papers/dirichlet/",,2013-10-10 22:12:17.237
185625,57261,16703.0,3,,CC BY-SA 3.0,84755109-3175-48b2-b7dd-3b7e0edbe2a0,<maximum-likelihood><dirichlet-distribution>,,2013-10-10 22:12:17.237
185624,57261,16703.0,1,,CC BY-SA 3.0,84755109-3175-48b2-b7dd-3b7e0edbe2a0,How to handle different amount of x_k values per sample when calculating mle for Dirichlet distribution?,,2013-10-10 22:12:17.237
185626,57253,594.0,5,,CC BY-SA 3.0,ddb36712-60e3-4766-8ae6-92492a025859,"I am trying to do a box-cox transformation with swift. I have a dependent variable, annual foreign sales of companies (in US\$ thousands) which contains zeros, for a set of panel data. I have been advised to add a small amount, for example, 0.00001 to the annual foreign sales figures so that I can take the log, but I think box-cox transformation will produce a more appropriate constant than 0.00001. I have done a box-cox transformation on R with the codes below, but it has given me a very large lambda2 of 31162.8.

    library(geoR)
    boxcoxfit(bornp$ForeignSales, lambda2 = TRUE)
    #R output - Fitted parameters:
    # lambda lambda2 beta sigmasq 
    # -1.023463e+00 3.116280e+04 9.770577e-01 7.140328e-11

My hunch is that the above value of lambda2 is very large, so I am not sure if I need to run the boxcoxfit with my independent variables like below:

    boxcoxfit(bornp$ForeignSales, bornp$family bornp$roa bornp$solvencyratio,lambda2=TRUE)

I am still trying to identify the best set of independent variables, so I am not sure if using the boxcoxfit with independent variables at this stage will work or is best.

Here's the description of the two lambda parameters from the help:

`lambda      ` numerical value(s) for the transformation parameter $\lambda$. Used as the initial value  
`            ` in the function for parameter estimation. If not provided default values are as-  
`            ` sumed. If multiple values are passed the one with highest likelihood is used as  
`            ` initial value.  
`lambda2     ` logical or numerical value(s) of the additional transformation (see DETAILS  
`            ` below). Defaults to `NULL`. If `TRUE` this parameter is also estimated and the initial  
`            ` value is set to the absolute value of the minimum data. A numerical value is  
`            ` provided it is used as the initial value. Multiple values are allowed as for  
`            ` lambda.

I would be very grateful for any advice on the above.",added 820 characters in body,2013-10-10 22:32:03.283
185627,57258,6162.0,5,,CC BY-SA 3.0,082e7bf4-b12d-4c93-b47e-2e3483510057,"For information, the random-effect model given by @Henrik:

    > f <- function(x) sqrt(x)
    > library(lme4)
    > ( fit1 <- lmer(f(Value) ~ Group + (1|Subject), data=dat) )
    Linear mixed model fit by REML ['lmerMod']
    Formula: f(Value) ~ Group + (1 | Subject) 
       Data: dat 
    REML criterion at convergence: 296.3579 
    Random effects:
     Groups   Name        Std.Dev.
     Subject  (Intercept) 0.5336  
     Residual             0.8673  
    Number of obs: 108, groups: Subject, 18
    Fixed Effects:
    (Intercept)       Group2       Group3  
        3.03718     -0.07541      1.11886  

is equivalent to a generalized least-squares model with an exchangeable correlation structure for subjects:

    > library(nlme)
    > fit2 <-  gls(f(Value) ~ Group, data=dat, na.action=na.omit, correlation=corCompSymm(form= ~  1 | Subject)) 

The fitted variance matrix is then:

    > getVarCov(fit2)
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
    [1,] 1.03690 0.28471 0.28471 0.28471 0.28471 0.28471
    [2,] 0.28471 1.03690 0.28471 0.28471 0.28471 0.28471
    [3,] 0.28471 0.28471 1.03690 0.28471 0.28471 0.28471
    [4,] 0.28471 0.28471 0.28471 1.03690 0.28471 0.28471
    [5,] 0.28471 0.28471 0.28471 0.28471 1.03690 0.28471
    [6,] 0.28471 0.28471 0.28471 0.28471 0.28471 1.03690
      Standard Deviations: 1.0183 1.0183 1.0183 1.0183 1.0183 1.0183 

As you can see, the diagonal entry corresponds to the total variance in the first model:

    > VarCorr(fit1)
     Groups   Name        Std.Dev.
     Subject  (Intercept) 0.53358 
     Residual             0.86731 
    > 0.53358^2+0.86731^2
    [1] 1.036934

and the covariance corresponds to the between-subject variance:

    > 0.53358^2
    [1] 0.2847076

Actually the gls model is more general because it allows a negative covariance. The advantage of `nlme` is that you can more generally use other repeated correlation structures and also you can specify different variances per group with the `weights` argument.

I think that residuals are different because they are constructed with the random-effects in the first model. In order to get multiple comparisons you can use the `lsmeans` and the `multcomp` packages, but the $p$-values of the hypotheses tests are anticonservative with defaults (too high) degrees of freedom. Unfortunately, the `pbkrtest` package does not apply to `gls`/`lme` models.",added 1 characters in body,2013-10-10 22:40:30.343
185630,57262,22587.0,3,,CC BY-SA 3.0,ecde6e73-992d-409a-82d3-feae12f3da22,<probability><poisson-distribution>,,2013-10-10 22:54:50.603
185629,57262,22587.0,1,,CC BY-SA 3.0,ecde6e73-992d-409a-82d3-feae12f3da22,Poisson Distribution vs multiplying probabilities,,2013-10-10 22:54:50.603
185644,57264,633.0,5,,CC BY-SA 3.0,aaa94539-10a7-4d48-a8d5-2aa318ce24e9,"The Poisson process that you're using assumes that 0.05 is the expected number of computers failing in one day in an unknown number of total computers (your answer also assumes that this rate is fixed after a computer fails, which implies that computers can fail multiple times, or are replaced immediately, or there are so many of them that this is negligible).

The independent probability that the student is using assumes that there are exactly four computers each of which has a 5% chance of failing.

The wording makes it sound to me like 5% is the chance of any individual computer failing (so the second interpretation).  In that case, we want to know the total number of computers and apply a binomial distribution.  Since the question doesn't give the total number of computers, it can't be answered.

Another possibility is that 5% is the probability that exactly one computer fails, and yet another possibility is that 5% is the probability that at least one computer fails.  In either case you might be able to somehow deduce the Poisson process intensity that gives this value.  From there you could calculate similarly to how you did.",added 407 characters in body,2013-10-10 23:17:04.727
185628,57262,22587.0,2,,CC BY-SA 3.0,ecde6e73-992d-409a-82d3-feae12f3da22,"I am a TA for a stats course for engineers, and I had a really good question from a student today, which I don't know the answer to.

We were going through the following word problem:

""Some computers run continuously for the Toronto Stock Exchange. The probability of a computer to fail in a day is estimated at 5%. Assuming differing computers fail independently, what is the probability that 4 computers fail in a day?""

Since the sampling takes place over an interval, the way I would approach this is using the Poisson distribution, with the average number of computers failing on a day $\equiv\lambda = 0.05$. If four computers fail, then $k = 4$. Thus we can use the poisson distribution:
\begin{align*}
   P(k; \lambda) &= \frac{\lambda^{k} e^{-\lambda}}{k!} \\
   P(k=4; \lambda = 0.05) &= \frac{0.05^{4} e^{-0.05}}{4!} \\
   & = 2.477\times 10^{-7}
\end{align*}

However, a student asked why it would not be appropriate to just multiply the probability of each computer failing. Since the probability of each computer failing each day $\equiv p = 0.05$, and since each computer failure is independent, he argued that,

\begin{align*}
   P(k=4) &= p^4 \\
          &= 0.05^4 = 6.25\times 10^{-6}
\end{align*}

Which one of these approaches is wrong given the question? And why? What underlying assumption of the wrong approach is violated by the question?

Thank you for your help.",,2013-10-10 22:54:50.603
185633,57263,22585.0,3,,CC BY-SA 3.0,798908e6-410c-41a4-b394-e5be05ca8bb8,<regression>,,2013-10-10 23:01:51.663
185632,57263,22585.0,1,,CC BY-SA 3.0,798908e6-410c-41a4-b394-e5be05ca8bb8,How to separate out the regression effect vs treatment effect without a control group?,,2013-10-10 23:01:51.663
185631,57263,22585.0,2,,CC BY-SA 3.0,798908e6-410c-41a4-b394-e5be05ca8bb8,"I'm looking at a dataset that has pre-post test measurements on users' stress, depression and anxiety levels collected from a website's online health assessment. On average, the healthier participants at baseline got worse over time, and the sicker participants at baseline got much better, and the middle group gets a little better. There's definitely a regression effect going on here, but also a treatment effect too. 

As this data was collected based on website usage, there isn't really a control group (all of the ""post"" measurements come from people that have used the online program). There are probably ways that I could synthesize a control group using the people who I can guess didn't make much use out of the treatment (based on number of logins or length of time between logins), but is there a way to separate out the treatment effect from the regression effect when you can't use difference-in-difference techniques using a control group or anything like that? 

Thanks!
",,2013-10-10 23:01:51.663
185634,57264,633.0,2,,CC BY-SA 3.0,da5e2452-da9f-4430-a226-af989872e4f7,"The Poisson process that you're using assumes that 0.05 is the expected number of computers failing in one day in an unknown number of total computers (your answer also assumes that computers failing multiple times is allowed or else there is such a large number of computers that this is negligible).

The independent probability that the student is using assumes that there are exactly four computers each of which has a 5% chance of failing.

The wording makes it sound to me like 5% is the chance of any individual computer failing (so the second interpretation).  In that case, we want to know the total number of computers and apply a binomial distribution.  Since the question doesn't give the total number of computers, it can't be answered.",,2013-10-10 23:04:27.343
185637,57261,,24,,CC BY-SA 3.0,980a7909-9631-4a55-8311-f526126d80e4,,Proposed by 22468 approved by 88 edit id of 5591,2013-10-10 23:06:08.977
185636,57261,16174.0,4,,CC BY-SA 3.0,980a7909-9631-4a55-8311-f526126d80e4,Dealing with 0 values when calculating the mle for a Dirichlet distribution,"shortened title for readability, embedded link for readability",2013-10-10 23:06:08.977
185635,57261,16174.0,5,,CC BY-SA 3.0,980a7909-9631-4a55-8311-f526126d80e4,"I have $N$ pmfs, and for each each $L$ samples. Each sample has a variable amount of $x$ values, but the $x$ values that they have can be matched. So for example:

$$sample_1 \rightarrow\ x_1 = 0, x_2 = 0, x_3 = 0.2, x_4 = 0.4, x_5 = 0.4$$
$$sample_2 \rightarrow\ x_1 = 0.3,x_2=0, x_3 = 0.4, x_4 = 0.3,x_5=0$$

I'm using a [python program][1] to calculate the mle from the samples which is a port of Thomas P. Minka's Matlab Fastfit code ([Estimating a Dirichlet Distribution][2]).  

The problem is that, for fitting, it sums over $logp+psi(\sum^k a_k) - logp$. Since some of the $x$ values are 0, some logp values are -inf. Therefore, summing over this makes everything -inf. 

How can I deal with 0 values when calculating the mle for a Dirichlet distribution? 

[1]:https://pypi.python.org/pypi/dirichlet/0.7
  [2]: http://research.microsoft.com/en-us/um/people/minka/papers/dirichlet/","shortened title for readability, embedded link for readability",2013-10-10 23:06:08.977
185638,57262,,4,user88,CC BY-SA 3.0,f10f3f1f-23ad-4901-94df-c773358d0284,Poisson distribution vs multiplying probabilities,edited title,2013-10-10 23:06:47.840
185640,57264,633.0,5,,CC BY-SA 3.0,ce2856b1-fb8d-4834-914d-b1d67253e027,"The Poisson process that you're using assumes that 0.05 is the expected number of computers failing in one day in an unknown number of total computers (your answer also assumes that this rate is fixed after a computer fails, which implies that computers can fail multiple times, or are replaced immediately, or there are so many of them that this is negligible).

The independent probability that the student is using assumes that there are exactly four computers each of which has a 5% chance of failing.

The wording makes it sound to me like 5% is the chance of any individual computer failing (so the second interpretation).  In that case, we want to know the total number of computers and apply a binomial distribution.  Since the question doesn't give the total number of computers, it can't be answered.

Another possibility is that 5% is the probability that exactly one computer fails, and yet another possibility is that 5% is the probability that at least one computer fails.  In either case you could deduce the Poisson process intensity that gives this value (using the inverse cdf).  From there you could calculate similarly to how you did.",added 407 characters in body,2013-10-10 23:11:27.527
185643,57265,2490.0,3,,CC BY-SA 3.0,bfc8d250-9121-4eba-adef-d8272f13a86e,<experiment-design><counterbalancing>,,2013-10-10 23:12:54.873
185642,57265,2490.0,1,,CC BY-SA 3.0,bfc8d250-9121-4eba-adef-d8272f13a86e,Simple experimental design - should I counterbalance?,,2013-10-10 23:12:54.873
185646,57262,22587.0,4,,CC BY-SA 3.0,a4679d86-a32d-4433-900a-2dacc6f73eab,Poisson distribution vs multiplying probabilities,added 819 characters in body; edited title,2013-10-10 23:20:23.123
185662,57270,5237.0,5,,CC BY-SA 3.0,fa00ad39-4246-43ec-8713-d84a70bdf051,"In a paper of Journal of Chemometrics (Naes & Mevik 2001), the authors propose to make simulations by creating two groups which are different with respect to the smallest eigenvector direction.

>Here the groups are different with respect to the orthogonal complement to the ﬁve ‘NIR loadings’. This is achieved in the following way. The constant 0 ⋅ 18 is multiplied by a sixth loading vector (orthogonal to the other ﬁve) and added to group 2. Both groups had initially the same means as group 1


How can I compute such a simulation in R? The goal is to obtain group differences which are tied to the ""small eigenvectors"" space.",removed signature,2013-10-11 01:02:37.520
185645,57262,22587.0,5,,CC BY-SA 3.0,a4679d86-a32d-4433-900a-2dacc6f73eab,"I am a TA for a stats course for engineers, and I had a really good question from a student today, which I don't know the answer to.

We were going through the following word problem:

""4 computers run continuously for the Toronto Stock Exchange. The probability of a computer failure in a day is estimated at 5%. Assuming differing computers fail independently, what is the probability that all 4 computers fail in a day?""

Since the sampling takes place over an interval, the way I would approach this is using the Poisson distribution, with the average number of computers failing on a day $\equiv\lambda = 0.05$. If four computers fail, then $k = 4$. Thus,
\begin{align*}
   P(k; \lambda) &= \frac{\lambda^{k} e^{-\lambda}}{k!} \\
   P(k=4; \lambda = 0.05) &= \frac{0.05^{4} e^{-0.05}}{4!} \\
   & = 2.477\times 10^{-7}
\end{align*}

However, a student asked why it would not be appropriate to just multiply the probability of each computer failing. Since the probability of each computer failing each day $\equiv p = 0.05$, and since each computer failure is independent, he argued that,

\begin{align*}
   P(k=4) &= p^4 \\
          &= 0.05^4 = 6.25\times 10^{-6}
\end{align*}

Which one of these approaches is wrong given the question? And why? What underlying assumption of the wrong approach is violated by the question?
 
Thank you for your help.

UPDATE: I left out some information in the problem the first time this was posted, and I apologize.",added 819 characters in body; edited title,2013-10-10 23:20:23.123
185647,57266,18040.0,2,,CC BY-SA 3.0,68af776e-8425-4104-843a-215ceac92894,"I've run into this problem before.  Often times it's due to a problem with passing negative values to a distribution that doesn't allow them.  So perhaps your log normal or wishart distributions are getting negative values somehow.  

Another issue is that you are giving priors on your variance parameters that is quite big.  This can cause unexpected values to be passed to your log-normald and wishart variables.  I'd start by severely constraining the variance priors and slowly expand the parameter space.  In my experience fitting BUGS models is both and art and a science.
",,2013-10-10 23:22:27.973
185648,57264,633.0,5,,CC BY-SA 3.0,c96a8328-592b-42bd-972a-b90f27b97f5b,"The Poisson process that you're using assumes that 0.05 is the expected number of computers failing in one day in an unknown number of total computers (your answer also assumes that this rate is fixed after a computer fails, which implies that computers can fail multiple times, or are replaced immediately, or there are so many of them that this is negligible).

The independent probability that the student is using assumes that there are exactly four computers each of which has a 5% chance of failing.

The wording makes it sound to me like 5% is the chance of any individual computer failing (so the second interpretation).  In that case, we want to know the total number of computers and apply a binomial distribution.  Since the question doesn't give the total number of computers, it can't be answered.

Another possibility is that 5% is the probability that exactly one computer fails, and yet another possibility is that 5% is the probability that at least one computer fails.  In either case you can deduce the Poisson process intensity that gives this value.  For the first of these, I get 4.4997552907483822; for the second, I get an intensity of 0.051293294149203306.  From there you could calculate similarly to how you did.

---

Per your update:  You can eliminate the Poisson process since you don't have a fixed rate.  You still have to decide whether 5% is the probability of a given computer failing, in which case the student is right.  If it's the probability of at least one computer failing, or the probability of exactly one computer failing, you'll have to reason back from that number to the probability of any individual computer failing before reasoning forwards.",added 90 characters in body,2013-10-10 23:27:17.640
185651,57267,22591.0,2,,CC BY-SA 3.0,b64f0cdf-b047-40d6-b92f-a9fa8f89a6d7,"Here is my situation:
- A huge amount of data
- 600 features
- Only one class is provided
Now, my question is how can I reduce the number of features to important ones? In another word, all of these features (with data) are intending to predict only one class. but some of features have large impact on the prediction (means their variation come to higher probability). ",,2013-10-10 23:28:22.503
185650,57267,22591.0,1,,CC BY-SA 3.0,b64f0cdf-b047-40d6-b92f-a9fa8f89a6d7,Extract important features,,2013-10-10 23:28:22.503
185649,57267,22591.0,3,,CC BY-SA 3.0,b64f0cdf-b047-40d6-b92f-a9fa8f89a6d7,<machine-learning><feature-selection><statistical-learning>,,2013-10-10 23:28:22.503
185654,57268,18040.0,3,,CC BY-SA 3.0,480333e6-1dc5-482e-95c6-4781d1488162,<r><mixed-model><multilevel-analysis><random-effects-model>,,2013-10-10 23:42:09.300
185653,57268,18040.0,1,,CC BY-SA 3.0,480333e6-1dc5-482e-95c6-4781d1488162,Difference between two different mixed effects models,,2013-10-10 23:42:09.300
185652,57268,18040.0,2,,CC BY-SA 3.0,480333e6-1dc5-482e-95c6-4781d1488162,"I have a question about how to tell two different mixed effects models apart.  In the simple case both involve fitting a model with a random group effect and a covariate. I fit the model with `lme4` in `R`.  Here is a visualization of the two different scenarios.   
![enter image description here][1]

    library(ggplot2)
    library(lme4)
    gen_dat2 <- function(group.m,group.v,int, sl,n){
          x <- vector()
          y <- vector()
          g <- vector()
             for(i in 1:length(group.m)){
             x.t <- rnorm(n,group.m[i],group.v[i])
             y.t <- rnorm(n,group.m[i],group.v[i])+int + sl*x.t 
             x <- c(x,x.t)
             y <- c(y,y.t)
             g <- c(g,rep(i,n))
            }
         return(cbind(x,y,g))
    }

    group.m <- runif(5,1,20)
    group.v <- runif(5,1,1.5)

    dat2 <- data.frame(gen_dat2(group.m,group.v,1,4,14))
    ggplot(dat2,aes(x=x,y=y,colour=as.factor(g),group=g))+geom_point()+stat_smooth(method=""lm"",se=F)
    m2 <- lmer(y~x + (x|g),data=dat2)



Then I can generate and fit the other scenario with similar code:

![enter image description here][2]

     gen_dat <- function(group.m,group.v,int, sl,n){
          x <- vector()
          y <- vector()
          g <- vector()
             for(i in 1:length(group.m)){
             x.t <- rnorm(n,0,1)
             y.t <- rnorm(n,group.m[i],group.v[i])+int + sl*x.t 
             x <- c(x,x.t)
             y <- c(y,y.t)
             g <- c(g,rep(i,n))
            }
         return(cbind(x,y,g))
    }

    group.m <- runif(5,1,20)
    group.v <- runif(5,1,1.5)

    dat1 <- data.frame(gen_dat(group.m,group.v,1,4,14))
    ggplot(dat1,aes(x=x,y=y,colour=as.factor(g),group=g))+geom_point()+stat_smooth(method=""lm"",se=F)
    m1 <- lmer(y~x + (x|g),data=dat1)


My central question is how do I tell these two models apart?  Am I incorrectly fitting the first one, and I need an extra term in there to model the relationships between groups and the x variable as well as y?  Both detect substantial between group variation in the intercept and not much in the slope as I would predict.  But I need a way to tell these two apart.  Any thoughts would be helpful.  

  [1]: https://i.stack.imgur.com/GgxYw.png
  [2]: https://i.stack.imgur.com/pgGm7.png",,2013-10-10 23:42:09.300
185655,57268,18040.0,4,,CC BY-SA 3.0,cf3af755-be00-4568-a10d-a1185bd1c214,Difference between two mixed effects models,edited title,2013-10-10 23:47:36.853
185656,57269,1741.0,2,,CC BY-SA 3.0,09f3a2ce-cf9a-4b24-8296-029e4a18ea93,"Given that your model exhibits good accuracy you can just use it to predict the class labels of records in the unlabeled dataset. However, you cannot evaluate the performances on unlabeled data. 

Be careful that you should assess the quality of your model on the labeled data by cross-validation. It is not enough to check the training error rate. 

If your model is not accurate enough you might think about [semi-supervised learning][1]. The unlabeled data is used in order to improve the quality of your model via inductive learning. The accuracy should always be computed by cross-validation on your labeled data.

Have a look at [ Crimisini et al. Decision Forests: A Unified Framework
for Classification, Regression, Density Estimation, Manifold Learning and
Semi-Supervised Learning ] Chapter 7 about semi-supervised learning and 7.4 about induction with semi-supervised learning.

  [1]: http://en.wikipedia.org/wiki/Semi-supervised_learning",,2013-10-11 00:17:06.150
185657,57262,,25,,,5c74984b-cdcf-4a12-9280-19d0b33d5d4a,,http://twitter.com/#!/StackStats/status/388458238112698368,2013-10-11 00:17:15.923
185663,57270,16174.0,5,,CC BY-SA 3.0,2bae73d1-069e-46ee-bad8-4a8a1acc2752,"In a paper of Journal of Chemometrics (Naes & Mevik 2001), the authors propose to make simulations by creating two groups which are different with respect to the smallest eigen-vector direction.

>Here the groups are different with respect to the orthogonal complement to the ﬁve ‘NIR loadings’.
This is achieved in the following way. The constant 0 ⋅ 18 is multiplied by a sixth loading vector (orthogonal to the other ﬁve) and added to group 2. Both groups had initially the same means as group 1


How can I compute such a simulation in R? The goal is to obtain group differences which are tied to the ""small eigen-vectors"" space.",removed signature,2013-10-11 01:02:37.520
185664,57271,20473.0,2,,CC BY-SA 3.0,63f55c7e-7004-4c5d-a5e1-c1d55a6d016e,"Write your system explicitly for time $t$ as (""$L$"" for ""loss"", as a positive quantity, and ""$G$"" for ""gain"")
$$ A_t - A_{t-1} = - L^A_{t-1} + G_{t-1}^{B\rightarrow A}+G_{t-1}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - L^B_{t-1} + G_{t-1}^{A\rightarrow B}+G_{t-1}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - L^C_{t-1} + G_{t-1}^{A\rightarrow C}+G_{t-1}^{B\rightarrow C}$$

The following three relations hold exactly:
$$  L^A_{t-1} = G_{t-1}^{A\rightarrow B} +  G_{t-1}^{A\rightarrow C} $$
$$  L^B_{t-1} = G_{t-1}^{B\rightarrow A} +  G_{t-1}^{B\rightarrow C} $$
$$  L^C_{t-1} = G_{t-1}^{C\rightarrow A} +  G_{t-1}^{C\rightarrow B} $$

If you substitute in the first three you obtain

$$ A_t - A_{t-1} = - G_{t-1}^{A\rightarrow B} -  G_{t-1}^{A\rightarrow C} + G_{t-1}^{B\rightarrow A}+G_{t-1}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - G_{t-1}^{B\rightarrow A} -  G_{t-1}^{B\rightarrow C} + G_{t-1}^{A\rightarrow B}+G_{t-1}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - G_{t-1}^{C\rightarrow A} -  G_{t-1}^{C\rightarrow B} + G_{t-1}^{A\rightarrow C}+G_{t-1}^{B\rightarrow C}$$

You have $6$ unknown quantities to estimate _per time period_. There is just not enough information to do that. So you need assumptions that will impose structure (=restrictions) on the situation, and will permit you to estimate _something_. What? Let's say you assume that there is a relatively stable ""churn"" from one company to another, as a linear function of their market share in the previous period. This assumption brings in a set of unknown coefficients to be estimated (which will then give you an estimate of ""hidden transfers of market share""). Write $G_{t-1}^{A\rightarrow B} = a_bA_{t-1}$ (market share lost from $A$ to $B$ as a linear function of $A$'s market share in period $t-1$).
Your equations will become

$$ A_t - A_{t-1} = - a_bA_{t-1} -  a_cA_{t-1} + b_aB_{t-1}+c_aC_{t-1} $$

$$ B_t - B_{t-1} = - b_aB_{t-1} -  b_cB_{t-1} + a_bA_{t-1}+c_bC_{t-1}$$

$$ C_t - C_{t-1} = - c_aC_{t-1} -  c_bC_{t-1} + a_cA_{t-1}+ b_cB_{t-1}$$

We have turn a set of mathematical identities into a _model_. It is doubtful that this model will hold exactly for each $t$, so you should add a stochastic error term. Rearranging we obtain a first-order Vector Autoregression (VAR): 

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
1-a_b-a_c  & b_a & c_a \\
a_b & 1-b_a-b_c & c_b \\
a_c & b_c & 1-c_a-c_b \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

or, to homogenize notation,

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
\gamma_{11}  & \gamma_{12} & \gamma_{13} \\
\gamma_{21} & \gamma_{22} & \gamma_{23} \\
\gamma_{31} & \gamma_{32} & \gamma_{33} \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

subject to the equality restrictions
$$  \begin{matrix}
\gamma_{11} +  \gamma_{21} + \gamma_{31} =1 \\
\gamma_{12} +  \gamma_{22} + \gamma_{32} =1  \\
\gamma_{13} +  \gamma_{23} + \gamma_{33} =1  \\
\end{matrix} $$

So you have essentially $6$ unknown coefficients and a sample of $T-1$ observations (for each company). Now, you must also assume what, if any, is the stochastic relation between the three stochastic error terms (are they correlated?) but such additional assumption should either come from knowledge of the specific real world phenomenon under study, or through a statistical specification search.

Of course you may find that such a model does not fit your data sample well - for example you expect that all estimated coefficients should be positive and smaller than or equal to unity, but the estimation procedure may not give you that. But this is what we do: we come up with specification hypotheses and we test them against the data - ""success"" is never guaranteed. Then you should try to come up with a different model.

",,2013-10-11 01:09:15.643
185665,57270,22593.0,5,,CC BY-SA 3.0,9cfc802b-031f-41cb-9b7f-05c7798ccdbd,"In a paper of Journal of Chemometrics (Naes & Mevik 2001 : Understanding the collinearity problem in regression and discriminant analysis), the authors propose to make simulations by creating two groups which are different with respect to the smallest eigenvector direction.

>Here the groups are different with respect to the orthogonal complement to the ﬁve ‘NIR loadings’. This is achieved in the following way. The constant 0 ⋅ 18 is multiplied by a sixth loading vector (orthogonal to the other ﬁve) and added to group 2. Both groups had initially the same means as group 1


How can I compute such a simulation in R? The goal is to obtain group differences which are tied to the ""small eigenvectors"" space.",adding title of the reference,2013-10-11 01:20:54.640
185667,57272,22595.0,16,,,92829f23-e685-4c79-98ae-0b5e5578e95b,,,2013-10-11 01:37:35.967
185666,57272,22595.0,2,,CC BY-SA 3.0,ce3b02d5-a6dc-457e-8862-4052938f945c,"The following are text books I used for my MSEE coursework and research and I found them to be pretty good. 

1. Probability, Statistics and Random Processes for Engineers by Henry Stark and John W. Woods
(Detailed explanation of concepts, good for Communications and Signal Processing people)
2. Probability, Random Variables and Random Processes by Hwei Hsu
(Concise explanation of concepts, has a good amount of solved examples)",,2013-10-11 01:37:35.967
185668,57272,1895.0,5,,CC BY-SA 3.0,922aea72-5169-4bec-b4e0-89cfa0886ed6,"The following are text books I used for my MSEE coursework and research and I found them to be pretty good. 

1. [Probability, Statistics and Random Processes for Engineers](http://www.amazon.com/Probability-Statistics-Processes-Engineers-Edition/dp/0132311232) by Henry Stark and John W. Woods
(Detailed explanation of concepts, good for Communications and Signal Processing people).
2. [Schaum's Outline of Probability, Random Variables and Random Processes](http://www.amazon.com/Schaums-Outline-Probability-Variables-Processes/dp/0071632891) by Hwei Hsu
(Concise explanation of concepts, has a good amount of solved examples).",added 199 characters in body,2013-10-11 01:47:30.337
185671,57273,22596.0,3,,CC BY-SA 3.0,e57f3873-cbcb-4953-bcae-b341f15c33d9,<modeling><python><markov-chain-montecarlo><fitting>,,2013-10-11 02:05:57.810
185670,57273,22596.0,1,,CC BY-SA 3.0,e57f3873-cbcb-4953-bcae-b341f15c33d9,Difficulty with MCMC implementation,,2013-10-11 02:05:57.810
185669,57273,22596.0,2,,CC BY-SA 3.0,e57f3873-cbcb-4953-bcae-b341f15c33d9,"I could really use some guided help! I'm having difficulty understanding an MCMC implementation in terms of modeling a data set. I'm working on generating parameters from stellar light curves, and was asked to look into implementing an MCMC algorithm. A large chuck on the code is written in Python, so I've been trying to use [emcee hammer](http://dan.iel.fm/emcee/) to generate parameter fits. But going through the code, it's just ""clicking"" how the method works.

I have a set of data (time vs flux) of two stars orbiting each other such that from our point of view, they eclipse. There are dips in the light curve to signify this. All I'm attempting to do is get the parameters of the system dependent on the characteristics of these dips.

In the emcee implementation, there are a few functions that I understand: the posterior function which, I believe, simply generates a data set given the set of parameters. Then there's a prior function which, I assume, is the function given a previous set of parameters. Somehow the algorithm chooses whether or not the jump to the posterior parameter set is to be done? I'm guessing that's what the use of the likelihood function is? To describe whether or not to take the jump?

I apologize, I'm quite confused on how this is to be implemented in terms a defined set of data.",,2013-10-11 02:05:57.810
185674,57274,22594.0,2,,CC BY-SA 3.0,8ba01954-a44f-45ab-aa74-443f0bb2d962,"I'm fairly new to statistics and I'm still trying to figure out the best way to analyse the data I have. The experiment has 2 groups of participants who perform 2 repetitions of a task that consists of 5 stages. All participants completed both repetitions for all stages, but one group had 8 participants while the other group only had 6. I have a about 100 dependent variables that I wish to examine, so my data looks a bit like this:

    ID   Group    Repetition    Stage   DV1    DV2     ...
    1    A        1             1       212.9  179.9   ...
    1    A        2             1       144.8  134.7   ...
    2    B        1             1       146.3  156.8   ...
    2    B        2             1       128.6  178.2   ...

Group is a between-subjects factor while Repetition and Stage are within-subjects factors. I would like to determine whether Group and Repetition have a significant effect on each dependent variable within each stage (I am not interested in the effect of stage itself). I'm doing the analysis in R so I have the following code:

    options(contrasts=c(""contr.sum"",""contr.poly""))
    mydata=read.csv(""data.csv"",header=TRUE)
    mydata$Group = factor(mydata$Group)
    mydata$Repetition = factor(mydata$Repetition)
    mydata$Stage = factor(mydata$Stage)
    # for each stage
    mydata = mydata[mydata$Stage==1,]
    for (i in 5:(ncol(mydata))) 
    {
       fit=aov(formula=as.formula(paste(names(mydata)[i],""~ Group * Repetition + Error(ID/Repetition)"")), data=mydata)
    }

My questions are:

 1. Is mixed measures ANOVA a valid test for this data? What's the correct way to test whether my data fits the assumptions of ANOVA in R? If this is not a reliable test, what's a possible alternative?
 2. Have I defined the mixed measures ANOVA in R correctly? The various tutorials I've read define it in different ways so I'm a bit confused.

Thanks in advance for any help.

 ",,2013-10-11 02:25:06.173
185673,57274,22594.0,3,,CC BY-SA 3.0,8ba01954-a44f-45ab-aa74-443f0bb2d962,<r><anova><mixed-model>,,2013-10-11 02:25:06.173
185672,57274,22594.0,1,,CC BY-SA 3.0,8ba01954-a44f-45ab-aa74-443f0bb2d962,Is mixed measures ANOVA the correct test for my data?,,2013-10-11 02:25:06.173
185677,57275,22598.0,3,,CC BY-SA 3.0,7721b4b4-3ec4-4951-b637-8fc93b4d2158,<data-mining><outliers><libsvm>,,2013-10-11 02:49:49.953
185676,57275,22598.0,1,,CC BY-SA 3.0,7721b4b4-3ec4-4951-b637-8fc93b4d2158,Training One Class SVM using LibSVM,,2013-10-11 02:49:49.953
185675,57275,22598.0,2,,CC BY-SA 3.0,7721b4b4-3ec4-4951-b637-8fc93b4d2158,"I hope to use one-class SVM of LIBSVM to train a training samples so as to get a model. Then, I use the model to predict the new test data and the training data is same type or not. In the training process, I have some questions as follows:

  - The training samples is all positive examples or not?
  - Which kernel function can get better result,linear kernel or RBF kernel?
  - What is the effect of nu's values to the model?
",,2013-10-11 02:49:49.953
185680,57276,22310.0,3,,CC BY-SA 3.0,4341a7fb-14e9-46f8-a348-0e2ae8e6bbb7,<structural-equation-modeling>,,2013-10-11 02:51:27.387
185679,57276,22310.0,1,,CC BY-SA 3.0,4341a7fb-14e9-46f8-a348-0e2ae8e6bbb7,"Path analysis, sample sizes, and alternative analysis",,2013-10-11 02:51:27.387
185692,56784,,25,,,71a76e5b-abb4-47d5-9999-0e692934a860,,http://twitter.com/#!/StackStats/status/388548838333878272,2013-10-11 06:17:16.663
185678,57276,22310.0,2,,CC BY-SA 3.0,4341a7fb-14e9-46f8-a348-0e2ae8e6bbb7,"I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence. 

My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But,k there are multiple factors, other than English ivy, that affect soil moisture.

![enter image description here][1]

My questions are:

1) I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?

2) Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?

![The relationship between soil moisture and English ivy cover on cover objects (""the number of overstory trees"" for the left graph) for different levels of the surrounding overstory trees (""English ivy cover on cover objects"" for the left graph][2]

3) Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small **AND** regressions do not reflect my hypothesized causal relationships accurately.

I am using R, so if any recommended codes would be greatly helpful (I am a relatively new R user). 

Thank you in advance for reading and considering my questions.

Sincerely,

Kiyoshi

  [1]: https://i.stack.imgur.com/k65Ag.jpg
  [2]: https://i.stack.imgur.com/ArgZm.jpg",,2013-10-11 02:51:27.387
185682,57249,,25,,,938b1411-cb83-45a2-917f-6ac7b8cd81d4,,http://twitter.com/#!/StackStats/status/388503539091517440,2013-10-11 03:17:16.493
185683,57276,5237.0,5,,CC BY-SA 3.0,87bc8adb-41a9-416f-82d5-d5f4971a32a7,"I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence. 

My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But,k there are multiple factors, other than English ivy, that affect soil moisture.

![enter image description here][1]

My questions are:

1. I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?

2. Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?

 ![The relationship between soil moisture and English ivy cover on cover objects (""the number of overstory trees"" for the left graph) for different levels of the surrounding overstory trees (""English ivy cover on cover objects"" for the left graph][2]

3. Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small **AND** regressions do not reflect my hypothesized causal relationships accurately.

I am using R, so any recommended code would be greatly helpful (I am a relatively new R user, though). 


  [1]: https://i.stack.imgur.com/k65Ag.jpg
  [2]: https://i.stack.imgur.com/ArgZm.jpg","added tags; formatted; removed thanks, signature",2013-10-11 03:19:10.097
185684,57276,5237.0,6,,CC BY-SA 3.0,87bc8adb-41a9-416f-82d5-d5f4971a32a7,<r><structural-equation-modeling><small-sample><path-model>,"added tags; formatted; removed thanks, signature",2013-10-11 03:19:10.097
185685,57274,5237.0,5,,CC BY-SA 3.0,805fe935-d4bd-4413-a39d-100d1ff331c3,"I'm fairly new to statistics and I'm still trying to figure out the best way to analyse the data I have. The experiment has 2 groups of participants who perform 2 repetitions of a task that consists of 5 stages. All participants completed both repetitions for all stages, but one group had 8 participants while the other group only had 6. I have a about 100 dependent variables that I wish to examine, so my data looks a bit like this:

    ID   Group    Repetition    Stage   DV1    DV2     ...
    1    A        1             1       212.9  179.9   ...
    1    A        2             1       144.8  134.7   ...
    2    B        1             1       146.3  156.8   ...
    2    B        2             1       128.6  178.2   ...

Group is a between-subjects factor while Repetition and Stage are within-subjects factors. I would like to determine whether Group and Repetition have a significant effect on each dependent variable within each stage (I am not interested in the effect of stage itself). I'm doing the analysis in R so I have the following code:

    options(contrasts=c(""contr.sum"",""contr.poly""))
    mydata            = read.csv(""data.csv"",header=TRUE)
    mydata$Group      = factor(mydata$Group)
    mydata$Repetition = factor(mydata$Repetition)
    mydata$Stage      = factor(mydata$Stage)
    # for each stage
    mydata = mydata[mydata$Stage==1,]
    for (i in 5:(ncol(mydata))) 
    {
       fit = aov(formula=as.formula(paste(names(mydata)[i], 
                                    ""~ Group * Repetition + Error(ID/Repetition)"")), 
                 data=mydata)
    }

My questions are:

 1. Is mixed measures ANOVA a valid test for this data? What's the correct way to test whether my data fits the assumptions of ANOVA in R? If this is not a reliable test, what's a possible alternative?
 2. Have I defined the mixed measures ANOVA in R correctly? The various tutorials I've read define it in different ways so I'm a bit confused.
",tweaked code for readability; removed thanks,2013-10-11 03:22:11.990
185688,57275,5237.0,4,,CC BY-SA 3.0,6cc8d218-d0ec-49dc-b25e-39b5a9f11723,Training one class SVM using LibSVM,added tag; light editing & formatting,2013-10-11 03:27:59.190
185687,57275,5237.0,5,,CC BY-SA 3.0,6cc8d218-d0ec-49dc-b25e-39b5a9f11723,"I hope to use one-class SVM of LIBSVM to train a training samples so as to get a model. Then, I will use the model to predict whether the new test data and the training data is same type or not. In the training process, I have some questions as follows:

  - Should the training samples all be positive examples or not?
  - Which kernel function can get better result, **linear** kernel or **RBF** kernel?
  - What is the effect of nu's values to the model?
",added tag; light editing & formatting,2013-10-11 03:27:59.190
185686,57275,5237.0,6,,CC BY-SA 3.0,6cc8d218-d0ec-49dc-b25e-39b5a9f11723,<svm><data-mining><outliers><libsvm>,added tag; light editing & formatting,2013-10-11 03:27:59.190
185689,56784,594.0,33,,,12f6317e-ae42-4285-8674-fed749d49e70,,817,2013-10-11 03:40:07.910
185690,57277,20603.0,2,,CC BY-SA 3.0,67019973-7c52-4e39-b61d-50a3236f440e,">Should the training samples all be positive examples or not?

Yes, in one class SVM (and any other outlier detection algorithm) you need just **one** class. If it is **positive** or **negative** depends on your naming convention, but it it more probable, that you will seek for **positive** examples which are underrepresented.

>Which kernel function can get better result, linear kernel or RBF kernel?

""There is no free lunch"". There is no general answer, the reason behind having many kernels (not just linear and rbf) is that they work well in different applications. It is **data dependant** decision, so you will have to test at least those two.

>What is the effect of nu's values to the model?

It corresponds to the bounds on fraction of points becoming support vectors, so it limits the model's complexity (smaller the number of SVs, simplier the model and less prone to overfitting, yet prone to underfitting). As in the http://www.cms.livjm.ac.uk/library/archive/Grid%20Computing/NoveltyDetection/sch00support.pdf paper, it directly corresponds to:

* ""an upper bound on the fraction of outliers""
* ""a lower bound on the fraction of SVs"".",,2013-10-11 05:28:45.147
185695,57278,20144.0,2,,CC BY-SA 3.0,5d3d56c9-9330-4d2b-88f3-518b100b01d3,"first of all, I want to express my apologies if the question is too broad or wrong, but I am in need of references and I have no idea of whom can I ask.

If you are interested, the question comes from a model I built, you can see some details [here](http://physics.stackexchange.com/questions/78524/boltzmann-distribution-with-interaction-between-particles) and [here](http://physics.stackexchange.com/questions/80019/grand-canonical-ensemble-with-interaction-simulation-doubts). In this model I have:
$$f(\mathbb{x}|T,\mu)=\frac{h(\mathbb{x})e^{-\frac{E(\mathbb{x})}{kT}+\mu N(x)}}{\mathcal{Z}(T,\mu)}$$

There, my parameters are $\mu$ and $T$, and $\mathbb{x}=(x_1,\dots,x_M)$ where $x_i\in\{0,1\}$ and I have the restriction $\forall i\in\{1,\dots,M-D+1\}$
$$\sum_{j=0}^{D-1} x_{i+j} \leq 1$$
This is,$h(\mathbb{x})=0$ if that condition is not held.


I have the ""small"" inconvenient of not knowing $\mathcal{Z}(T,\mu)$, so I used a MCMC (Metropolis-Hastings) method to approximate this function. However I face two problems. The first of them is regarding the simulation and the model, and I am on solving it (it depends too much on the initial condition). The second is that these parameters are not fully known and I have no idea of how can I estimate them. I have been reading about bayesian inference and I know a bit of estimator theory but I am no expert (furthermore I don't know if not knowing the partition function can affect). If any of you were able to give me some clue in the form of a book that I can read, I would be eternally grateful.

Thank you very much for your help.",,2013-10-11 06:43:41.090
185694,57278,20144.0,3,,CC BY-SA 3.0,5d3d56c9-9330-4d2b-88f3-518b100b01d3,<references><nonparametric><exponential-family>,,2013-10-11 06:43:41.090
185698,57279,22262.0,2,,CC BY-SA 3.0,1540356b-83ae-43ec-86de-50fd9b8a052c,"What variable selection approach should I consider if I have thousands of predictors with clusters that are extremely correlated? 

For example I might have a predictor set $X:= \{A_1,A_2,A_3,A_4,...,A_{39},B_1,B_2,...,B_{44},C_1,C_2,...\}$ with cardinality $|X| > 2000$. Consider the case where all $\rho(A_i,A_j)$ are very high, and similarly for $B$, $C$, .... 

Correlated predictors aren't correlated ""naturally""; it's a result of the feature engineering process. This is because all $A_i$ are hand engineered from the same underlying data with small variations in hand-engineering methodology, e.g. I use a thinner pass band on $A_2$ than I did for $A_1$ in my denoising approach but everything else is the same.

My goal is to improve out of sample accuracy in my classification model.  

One approach would just be to try everything:  non-negative garotte, ridge, lasso, elastic nets, random subspace learning, PCA/manifold learning, least angle regression and pick the one that's best in my out of sample dataset. But specific methods that are good at dealing with the above would be appreciated.",,2013-10-11 06:57:56.613
185699,57279,22262.0,1,,CC BY-SA 3.0,1540356b-83ae-43ec-86de-50fd9b8a052c,Variable selection with groups of predictors that are highly correlated,,2013-10-11 06:57:56.613
185697,57279,22262.0,3,,CC BY-SA 3.0,1540356b-83ae-43ec-86de-50fd9b8a052c,<machine-learning><forecasting><feature-selection><lasso><multicollinearity>,,2013-10-11 06:57:56.613
185702,57280,22600.0,3,,CC BY-SA 3.0,8c5e8fb0-5841-4ae4-8f67-b4f56b8d8e31,<probability><binomial-distribution><conditional-probability>,,2013-10-11 07:24:30.003
185701,57280,22600.0,1,,CC BY-SA 3.0,8c5e8fb0-5841-4ae4-8f67-b4f56b8d8e31,Binomial Conditional Probability of a an event,,2013-10-11 07:24:30.003
185700,57280,22600.0,2,,CC BY-SA 3.0,8c5e8fb0-5841-4ae4-8f67-b4f56b8d8e31,"## Determining Binomial Condition Probability of a Random Sample ##

I have a question about binomial probability involving a conditional event. This problem keeps tripping me up, because while I know how to calculate the binomial probability that a random variable is a failure, i don't know how to calculate the conditional probability of that variable.


----------


My question is as follows:

**70%** of the total shipments come from **factory A**, of which **10% are defective**.

**30%** of the total shipments come from **factory B**, of which **5% are defective**.

A random shipment comes in, and a sample of 20 pints is taken, and 1 of the pints is defective.

*What is the probability that this shipment came from the Factory A?* 

",,2013-10-11 07:24:30.003
185703,57281,9074.0,2,,CC BY-SA 3.0,f7d71e61-191d-4712-b579-76c0e92f1c19,"This requires a straight-forward application of Bayes' rule: $P(A|Defective)=P(Defective|A)*P(A)/P(Defective) \leftrightarrow \\
P(A|Defective)=0.1*0.7/0.085= 0.07/0.085 \approx 0.824$",,2013-10-11 07:39:11.260
185704,57281,9074.0,5,,CC BY-SA 3.0,43545709-e3ef-462f-8fe6-478d150dbfec,**Edited**: didn't pay attention to the question. Will edit answer later today. ,Didn't read the question thoroughly. ,2013-10-11 08:03:27.483
185706,57281,9074.0,5,,CC BY-SA 3.0,8c153f28-999a-42c0-aa97-f50965737ff3,"**Edit 1**: didn't pay attention to the question. Will edit answer later today. 

**Edit 2**: I've attempted to provide an answer below, however I might be mistaken. Feel free to correct me if I am in error. 

$P(1 defective|A) \approx 0.270 \wedge P(1 defective|B) \approx 0.377 \\
P(A) = 0.7 \wedge P(B) = 0.3 \\
P(1D) = 0.7*0.270+0.3*0.377 = 0.189+0.113 = 0.302 \\
P(A|1D) = P(1D|A)*P(A)/P(1D)=0.270*0.7/0.302 \approx 0.626$",added 354 characters in body,2013-10-11 08:22:50.210
185709,57282,22601.0,2,,CC BY-SA 3.0,51317f1a-2981-446a-8f73-a8521e7811fa,"are there any generic tests to validate if a given sample follows a unimodal distribution, like a gaussian, cauchy's, Student's t or chi^2?

Thanks,

Christian",,2013-10-11 08:38:55.517
185708,57282,22601.0,3,,CC BY-SA 3.0,51317f1a-2981-446a-8f73-a8521e7811fa,<hypothesis-testing><distributions><normal-distribution><chi-squared><t-distribution>,,2013-10-11 08:38:55.517
185707,57282,22601.0,1,,CC BY-SA 3.0,51317f1a-2981-446a-8f73-a8521e7811fa,Generic test for unimodality given sample,,2013-10-11 08:38:55.517
185710,57283,9074.0,2,,CC BY-SA 3.0,382e20a9-2035-43d6-ab1b-46b9b4c6a4d8,"You're asking two questions:

1) Is there a generic test for unimodality? 
2) Is there tests to test whether a sample is derived from a given distribution, say, a normal distribution?

Ad 1): Yes, the Hartigan-Hartigan dip test, [Ann. Statist. 13(1):70-84][1].

Ad 2): There exists a number of special tests, but the [Kolmogorov-Smirnov][2] test is a general-purpose nonparametric test, although with low statistical power. 

Best,

  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176346577
  [2]: http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test",,2013-10-11 08:43:06.977
185711,57271,20473.0,5,,CC BY-SA 3.0,9d6fe1be-d19d-49e1-bf4f-d4569cdd9b74,"Write your system explicitly for time $t$ as (""$L$"" for ""loss"", as a positive quantity, and ""$G$"" for ""gain"")
$$ A_t - A_{t-1} = - L^A_{t-1} + G_{t-1}^{B\rightarrow A}+G_{t-1}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - L^B_{t-1} + G_{t-1}^{A\rightarrow B}+G_{t-1}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - L^C_{t-1} + G_{t-1}^{A\rightarrow C}+G_{t-1}^{B\rightarrow C}$$

The following three relations hold exactly:
$$  L^A_{t-1} = G_{t-1}^{A\rightarrow B} +  G_{t-1}^{A\rightarrow C} $$
$$  L^B_{t-1} = G_{t-1}^{B\rightarrow A} +  G_{t-1}^{B\rightarrow C} $$
$$  L^C_{t-1} = G_{t-1}^{C\rightarrow A} +  G_{t-1}^{C\rightarrow B} $$

If you substitute in the first three you obtain

$$ A_t - A_{t-1} = - G_{t-1}^{A\rightarrow B} -  G_{t-1}^{A\rightarrow C} + G_{t-1}^{B\rightarrow A}+G_{t-1}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - G_{t-1}^{B\rightarrow A} -  G_{t-1}^{B\rightarrow C} + G_{t-1}^{A\rightarrow B}+G_{t-1}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - G_{t-1}^{C\rightarrow A} -  G_{t-1}^{C\rightarrow B} + G_{t-1}^{A\rightarrow C}+G_{t-1}^{B\rightarrow C}$$

You have $6$ unknown quantities to estimate _per time period_. There is just not enough information to do that. So you need assumptions that will impose structure (=restrictions) on the situation, and will permit you to estimate _something_. What? Let's say you assume that there is a relatively stable ""churn"" from one company to another, as a linear function of their market share in the previous period. This assumption brings in a set of unknown coefficients to be estimated (which will then give you an estimate of ""hidden transfers of market share""). Write $G_{t-1}^{A\rightarrow B} = a_bA_{t-1}$ (market share lost from $A$ to $B$ as a linear function of $A$'s market share in period $t-1$).
Your equations will become

$$ A_t - A_{t-1} = - a_bA_{t-1} -  a_cA_{t-1} + b_aB_{t-1}+c_aC_{t-1} $$

$$ B_t - B_{t-1} = - b_aB_{t-1} -  b_cB_{t-1} + a_bA_{t-1}+c_bC_{t-1}$$

$$ C_t - C_{t-1} = - c_aC_{t-1} -  c_bC_{t-1} + a_cA_{t-1}+ b_cB_{t-1}$$

We have turned a set of mathematical identities into a _model_. It is doubtful that this model will hold exactly for each $t$, so you should add a stochastic error term. Rearranging we obtain a first-order Vector Autoregression (VAR): 

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
1-a_b-a_c  & b_a & c_a \\
a_b & 1-b_a-b_c & c_b \\
a_c & b_c & 1-c_a-c_b \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

or, to homogenize notation,

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
\gamma_{11}  & \gamma_{12} & \gamma_{13} \\
\gamma_{21} & \gamma_{22} & \gamma_{23} \\
\gamma_{31} & \gamma_{32} & \gamma_{33} \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

subject to the equality restrictions
$$  \begin{matrix}
\gamma_{11} +  \gamma_{21} + \gamma_{31} =1 \\
\gamma_{12} +  \gamma_{22} + \gamma_{32} =1  \\
\gamma_{13} +  \gamma_{23} + \gamma_{33} =1  \\
\end{matrix} $$

So you have essentially $6$ unknown coefficients and a sample of $T-1$ observations (for each company). Now, you must also assume what, if any, is the stochastic relation between the three stochastic error terms (are they correlated?) but such additional assumption should either come from knowledge of the specific real world phenomenon under study, or through a statistical specification search.

Of course you may find that such a model does not fit your data sample well - for example you expect that all estimated coefficients should be positive and smaller than or equal to unity, but the estimation procedure may not give you that. But this is what we do: we come up with specification hypotheses and we test them against the data - ""success"" is never guaranteed. Then you should try to come up with a different model.

",added 2 characters in body,2013-10-11 08:50:02.827
185713,57283,9074.0,5,,CC BY-SA 3.0,5387d364-ebbf-4a6d-9c8f-26702892d8b5,"You're asking two questions:

 1. Is there a generic test for unimodality?
 2. Is there tests to test whether a sample is derived from a given distribution, say, a normal distribution?

Ad 1): Yes, the Hartigan-Hartigan dip test, [Ann. Statist. 13(1):70-84][1].

Ad 2): There exists a number of special tests, but the [Kolmogorov-Smirnov][2] test is a general-purpose nonparametric test, although with low statistical power. 

Best,

  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176346577
  [2]: http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test",added 1 characters in body,2013-10-11 08:56:00.610
185715,57284,,2,user14650,CC BY-SA 3.0,d0e821d4-743c-40ef-83e9-66dad2846efa,"In R, the function `wilcox.test` takes the argument `conf.level = 0.095` (for example). Giving the same argument to the function `wilcoxsign_test` from the *coin* package returns a warning:

    additional arguments conf.level will be ignored

**What default confidence level does `wilcoxsign_test` use, and how can I change it?**

*Or :* Why do I not need a confidence level for this function?",,2013-10-11 09:06:48.623
185718,56955,,25,,,79a5b88c-fdd1-44d9-8b7c-9d2191b483ba,,http://twitter.com/#!/StackStats/status/388594137152643073,2013-10-11 09:17:16.740
185719,57285,221.0,2,,CC BY-SA 3.0,6008c348-1a85-4007-9c6f-6ea070411500,"The topic is called [Association Rule Learning][1], which is one of the most basic (and rather old-fashioned) ways to build a recommender system. The most widely known algorithms are called A Priori and FP Growth. Every good book about Data Mining should contain a chapter about it.

However, the formula seems to be wrong.

$P(A|B)$ means Probability of A given B, so

    P(A|B)=count_users(bought(A,B)) / count_users(bought_B)

is correct.

Furthermore, the mentioned algorithms do not take into account something like $P(\neg A|B)$, because the fact that a user has not bought A could have multiple meanings

 - user does not like A
 - user does not know that A exists or is sold here
 - user does not bought A although he likes it for one of the thousand seemingly arbitrary motivatiors of human behavior.

because not buying something is an implicit preference. If the user would have stated explicitly that he does not like A (may be in a survey), it is called an explicit preference. In case of implicit negative preferences, the negative preferences are often excluded from the model.

If explicit preferences are given, the overall formula $\frac{P(A|B)}{P(\neg A|B)}$ would make sense and represent the [Odds Ratio][2].


  [1]: http://en.wikipedia.org/wiki/Association_rule_learning
  [2]: http://en.wikipedia.org/wiki/Odds_ratio
  [3]: http://en.wikipedia.org/wiki/Statistical_classification",,2013-10-11 09:40:49.687
185721,57282,,24,,CC BY-SA 3.0,203372b3-cb62-42e9-8c9f-7547cf8ed197,,Proposed by 27581 approved by 930 edit id of 5593,2013-10-11 09:45:32.230
185720,57282,,5,,CC BY-SA 3.0,203372b3-cb62-42e9-8c9f-7547cf8ed197,"Are there any generic tests to validate if a given sample follows a unimodal distribution, like a Gaussian, Cauchy, Student's t or a chi-square?",Slight editing,2013-10-11 09:45:32.230
185722,57220,,24,,CC BY-SA 3.0,18183114-e8f1-4ee4-acd9-42723719772c,,Proposed by 31381 approved by 686 edit id of 5594,2013-10-11 09:47:17.813
185723,57220,22603.0,5,,CC BY-SA 3.0,18183114-e8f1-4ee4-acd9-42723719772c,"I have the following regression

 $children = \beta_0 + \beta_1 \log(earnings) + \beta_2 grandparents + \epsilon$

and $\beta_1>0$ with $p$=0.01 and $\beta_2>0$ with $p$=0.01, and N is large (N>10.000) and grandparents takes values 0,1,2,3,4.

Then I add the interaction term ($\log(earnings)*grandparents$) to equation 1, such that: 

 $children = \beta_0 + \beta_1 \log( earnings) + \beta_2 grandparents+ \beta_3 \log( earnings)*grandparents + \epsilon$ 

and $\beta_1>0$ with $p$=0.01, $\beta_2$ is no longer statistically significant and also $\beta_3$ is not statistically significant. 

I do not understand how to interpret the results and if the interaction term wipes out the direct effect of grandparents since $\log(earnings)$ is always different from 0.


>>>>There is a way to test the stat. sign. of the effect of Grandparents in the interacted model? (Thanks Maarten for your previous answer)
",improve formatting,2013-10-11 09:47:17.813
185724,57286,503.0,2,,CC BY-SA 3.0,d7bb3ae9-1c10-4442-a7f3-15c63d956bd2,"You don't need (and in fact can't) set a confidence limit in `wilcoxsign_test` because the about of the function includes a p value. e.g.the first example in the help file for the function:


    RoundingTimes <- data.frame(
      times = c(5.40, 5.50, 5.55,
                5.85, 5.70, 5.75,
                5.20, 5.60, 5.50,
                5.55, 5.50, 5.40,
                5.90, 5.85, 5.70,
                5.45, 5.55, 5.60,
                5.40, 5.40, 5.35,
                5.45, 5.50, 5.35,
                5.25, 5.15, 5.00,
                5.85, 5.80, 5.70,
                5.25, 5.20, 5.10,
                5.65, 5.55, 5.45,
                5.60, 5.35, 5.45,
                5.05, 5.00, 4.95,
                5.50, 5.50, 5.40,
                5.45, 5.55, 5.50,
                5.55, 5.55, 5.35,
                5.45, 5.50, 5.55,
                5.50, 5.45, 5.25,
                5.65, 5.60, 5.40,
                5.70, 5.65, 5.55,
                6.30, 6.30, 6.25),
      methods = factor(rep(c(""Round Out"", ""Narrow Angle"", ""Wide Angle""), 22)),
      block = factor(rep(1:22, rep(3, 22))))
    
    ### classical global test
    friedman_test(times ~ methods | block, data = RoundingTimes)

gives as output 

    Asymptotic Friedman Test
    
    data:  times by
    	 methods (Narrow Angle, Round Out, Wide Angle) 
    	 stratified by block
    chi-squared = 11.1429, df = 2, p-value =  0.003805

so, since p = 0.0038, you know it is significant at p = 0.05 (and, indeed, much below that). ",,2013-10-11 10:26:57.610
185726,57252,,24,,CC BY-SA 3.0,7a276888-4bca-491e-b87f-e6d872670a74,,Proposed by anonymous approved by 686 edit id of 5595,2013-10-11 10:28:02.303
185725,57252,0.0,5,,CC BY-SA 3.0,7a276888-4bca-491e-b87f-e6d872670a74,"I put this question because while reading the benefits of  standardizing explanatory variables or not, I read *good but contrasting* opinions about standardizing when there are interaction in the model. 

Some talk about how problems of collinearity are removed when standardizing (e.g. http://stats.stackexchange.com/questions/60476/collinearity-diagnostics-problematic-only-when-the-interaction-term-is-included#61022), which is basically the case of my GLMM. However, others claim that standard errors and p-values of interactions of standardized models are not reliable... (e.g.http://stats.stackexchange.com/questions/19216/variables-are-often-adjusted-e-g-standardised-before-making-a-model-when-is or http://quantpsy.org/interact/interactions.htm)

So, any ideas on what is the right thing to do?",I edited a bit to include the links of those contrasting opinions,2013-10-11 10:28:02.303
185729,57287,21624.0,3,,CC BY-SA 3.0,34043a0d-98d4-42c3-8bec-690b0e8b61c3,<bootstrap><convergence>,,2013-10-11 10:37:53.160
185728,57287,21624.0,1,,CC BY-SA 3.0,34043a0d-98d4-42c3-8bec-690b0e8b61c3,How to decide how many times of bootstrap to run regards the sample size?,,2013-10-11 10:37:53.160
185727,57287,21624.0,2,,CC BY-SA 3.0,34043a0d-98d4-42c3-8bec-690b0e8b61c3,"I am using bootstrap for my simulation.

The number of the population is flexible for each case, while the sample size is decide by a certain percentage. For example, I have a 10,000 population, and I decide to use 10% for each iteration of bootstrap, so the sample size is 1,000.

In practice, I found it is hard to decide how many time to run the bootstrap is enough. With less simulation, the results appears insuffice, while with a large number of simulation is simply waste of time.

May I know if there is a method can help me to decide the number of iterations to run?",,2013-10-11 10:37:53.160
186215,57428,20470.0,4,,CC BY-SA 3.0,db7a9836-5766-421a-9970-ebc244a2e876,How to discretise continuous attributes while implementing ID3 Algorithm,grammar / rephrasal,2013-10-14 09:23:33.433
185730,57287,21624.0,5,,CC BY-SA 3.0,57a71b9b-0da4-4f98-87cf-b54f9c19c1bc,"I am using bootstrap for my simulation.

The number of the population is flexible for each case, and the sample size is decided by a certain percentage. For example, I have a 10,000 population, and I decide to use 10% for each iteration of bootstrap, so the sample size is 1,000.

In practice, I found it is hard to decide how many times to run the bootstrap is enough. With less simulation, the results appears insuffice, while with a large number of simulation is redundant.

May I know if there is a method can help me to decide the number of iterations to run?",deleted 11 characters in body,2013-10-11 10:55:07.973
185733,57288,19436.0,1,,CC BY-SA 3.0,164a16f0-fe53-4555-bdff-11aba7d1604b,Reversing Chebyshev inequality argument,,2013-10-11 10:55:26.637
185732,57288,19436.0,2,,CC BY-SA 3.0,164a16f0-fe53-4555-bdff-11aba7d1604b,"One way one could state Chebyshev's inequality is 

> The probability that a realization deviates from the mean more
> than $k$ standard deviations is at most $\frac{1}{k^2}$.

My question is: Can one rigorously reverse this logic and make a statement about the probability that the actual mean is close to the observation. One immediate technical problem is that one needs to define a probability space on possible probability distributions/means.

I'm asking because I think this type of argument (although slightly more convoluted) underlies Vapnik-Chervonenkis theory.  In their textbooks this issue is not discussed at all. They prove a large deviation principle and then simply invert all their inequalities. How does this work?  Does it?",,2013-10-11 10:55:26.637
185731,57288,19436.0,3,,CC BY-SA 3.0,164a16f0-fe53-4555-bdff-11aba7d1604b,<philosophical><statistical-learning>,,2013-10-11 10:55:26.637
185734,57283,9074.0,5,,CC BY-SA 3.0,4c8518f1-f825-4496-b879-0f0e300c91b2,"You're asking two questions:

 1. Is there a generic test for unimodality?
 2. Are there tests to test whether a sample is derived from a given distribution, say, a normal distribution?

Ad 1): Yes, the Hartigan-Hartigan dip test, [Ann. Statist. 13(1):70-84][1].

Ad 2): There exists a number of special tests, but the [Kolmogorov-Smirnov][2] test is a general-purpose nonparametric test, although with low statistical power. 

Best,

  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176346577
  [2]: http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test",Congruence error.,2013-10-11 10:57:12.170
185735,57287,674.0,4,,CC BY-SA 3.0,23c27ad9-b6d8-4426-b8ce-a0a1038deee9,How to decide bootstrap sample size and number of runs?,added 14 characters in body; edited title,2013-10-11 11:01:45.260
185736,57287,674.0,5,,CC BY-SA 3.0,23c27ad9-b6d8-4426-b8ce-a0a1038deee9,"I am using bootstrap for my simulation.

The number of the population is flexible for each case, and the sample size is decided by a certain percentage. For example, I have a 10,000 population, and I decide to use 10% for each iteration of bootstrap, so the sample size is 1,000.

In practice, I found it is hard to decide how many times to run the bootstrap is enough. With less simulation, the results appears insufficiant, while with a large number of simulation they are redundant.

May I know if there is a method that can help me to decide the number of iterations to run?",added 14 characters in body; edited title,2013-10-11 11:01:45.260
185738,57287,21624.0,5,,CC BY-SA 3.0,5a8edc37-ec17-4b17-845e-b2a3b2193e95,"I am using bootstrap for my simulation.

The number of the population is flexible for each case, and the sample size is decided by a certain percentage. For example, I have a 10,000 population, and I decide to use 10% for each iteration of bootstrap, so the sample size is 1,000.

In practice, I found it is hard to decide how many times to run the bootstrap is enough. With less simulation, the results appear insufficiant, while with a large number of simulation they are redundant.

May I know if there is a method that can help me to decide the number of iterations to run?",edited title,2013-10-11 11:07:21.687
185737,57287,21624.0,4,,CC BY-SA 3.0,5a8edc37-ec17-4b17-845e-b2a3b2193e95,How to decide bootstrap number of runs?,edited title,2013-10-11 11:07:21.687
185740,57290,0.0,2,,CC BY-SA 3.0,eaaa1685-5eb8-4f57-900a-a02bc70941bb,,,2013-10-11 11:26:16.130
185739,57289,0.0,2,,CC BY-SA 3.0,1db4b3b2-ab49-42c6-a169-74624870c957,,,2013-10-11 11:26:16.130
185743,57291,22262.0,3,,CC BY-SA 3.0,eb1818b2-f2e4-4aa4-acfb-8ecbee48234f,<r><quantiles><function><package>,,2013-10-11 11:39:19.043
185742,57291,22262.0,1,,CC BY-SA 3.0,eb1818b2-f2e4-4aa4-acfb-8ecbee48234f,Function to find the quantile in a vector corresponding to constant $x$,,2013-10-11 11:39:19.043
185741,57291,22262.0,2,,CC BY-SA 3.0,eb1818b2-f2e4-4aa4-acfb-8ecbee48234f,"Suppose I have constant `x=0.1` in the language `R` and I have a vector `vec = rnorm(200)`. Is there a pre-packaged function to find the quantile of `vec` that corresponds the closest to `x`? 

A solution is as follows:

    x = 0.1
	vec = rnorm(100)
	percentiles = quantile(vec,seq(0,1,by=0.01))
	which(abs(x-percentiles)==min(abs(x-percentiles))) 
	#returns closest match
... but I would like to know if there's a pre-packaged function.",,2013-10-11 11:39:19.043
185744,57292,19125.0,2,,CC BY-SA 3.0,77844d56-f139-4737-badd-5f0492e6b69b,"Your supervisor may very well be right and the small sample size is the problem. You might want to do a bit of reading on [*Power Analysis*](http://psych.wisc.edu/henriques/power.html). An introductory paper is that by Cohen (1992). 

In short, there is a relation between sample size, effect size and power (which is the probability that the test detects a significant effect assuming that there is one). For example, if you have an estimate of the effect size you're looking for (in your example the difference between the means of the two groups) and you want to obtain a statistically significant result regardings this effect with a certain error probability (the $\alpha$-Level), then you can compute the size of the sample that is neccessary. Generally, when you have two of the numbers, you can compute the third one. 

The difficult part is probably to get an idea of the effect size before doing the analysis. After all, ususally that is what one wants to find out about. An interesting discussion on this can be found on the [Cognitive Sciences SE site](http://cogsci.stackexchange.com/questions/3384/how-to-estimate-an-expected-effect-size-for-a-planned-psychology-study?lq=1).

One piece of free software to do power analysis is [G Power](http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/download-and-register). There is also the `pwr`-package for R.

References:

<sub>
Cohen, J. (1992). [A power primer](http://classes.deonandan.com/hss4303/2010/cohen%201992%20sample%20size.pdf). *Psychological Bulletin*, 112(1), 155.
</sub> ",,2013-10-11 11:48:49.933
185857,57325,22622.0,2,,CC BY-SA 3.0,9e44d147-9a69-4e49-baaf-af8f690a9818,"I am running cross-sectional regressions of the type:
Y_c = alpha + beta X_1 + gamma X_2 + delta_1 X_3 + delta_2 X_1 X_3 + delta_3 X_2 X_3 + e_c.
My theoretical model implies that delta_2 should be negative, delta_3 should be positive and the marginal effect of X_3 should be negative. My estimates imply that delta_2 is negative and significant, delta_3 is positive and insignificant, beta is significant, and gamma is insignificant. Building on this evidence, can I calculate the marginal effect of X_3 as delta_1 + delta_2 E(X_1) where E(X_1) is the muean of X_1 justifiying this procedure with the fact that all the terms incorporating X_2 are insignificant?

Thanks for this.",,2013-10-11 20:30:29.607
185746,57293,18198.0,2,,CC BY-SA 3.0,58b3f483-5bb1-4e7b-b60f-63dd24dabf9e,"I am testing various techniques for dealing with strong Multi-collinearity in a regression problem.

There have been various comparison papers written between competing techniques such as Ridge Regression (RR) and Principal Components Regression (PCR). There seems to be no clear winner though with the best technique seemingly problem specific. However one thing that bothers me about the PCR approach is the somewhat arbitrary way in which one simply excludes the smallest eigenvectors as has been proven in Hadi and Ling even the smallest eigenvector may have strong predictive power while the largest eigenvectors may have none.

""Some Cautionary notes on the use of Principal Components Regression"" by Hadi and Ling. 

They also show that the the SSE can be vastly improved by adding seemingly insignificant eigenvectors. 

In their discussion they highlght two papers the try to address this 2nd deficiency Lott(1973) and Gunst and Mason(1973) but it has been shown that the Lott technique fails to pick the ""correct"" eigenvectors in the presence of strong MC, and my problem has strong MS. Do you know of a paper that can select the optimum set of eigenvalues even in the presence of strong MC? 

Or more recent papers that compare PCR and RR?",,2013-10-11 11:56:09.380
185745,57293,18198.0,3,,CC BY-SA 3.0,58b3f483-5bb1-4e7b-b60f-63dd24dabf9e,<pca><ridge-regression>,,2013-10-11 11:56:09.380
185747,57293,18198.0,1,,CC BY-SA 3.0,58b3f483-5bb1-4e7b-b60f-63dd24dabf9e,Selecting Optimal set of Eigenvectors for Principal Components Regression,,2013-10-11 11:56:09.380
185749,57290,2081.0,5,,CC BY-SA 3.0,5c30db97-1fba-48f8-9aa7-156c098ed4bf,"Canonical correlation is between sets of variables. It is the maximized correlation between a linear combination of one set and a linear combination of the other set (the two combinations are called canonical variates). Canonical correlation analysis deals with that latent ""inside"" of a multivariate regression/anova.",added 318 characters in body,2013-10-11 12:01:56.470
185748,57290,,24,,CC BY-SA 3.0,5c30db97-1fba-48f8-9aa7-156c098ed4bf,,"Proposed by 3277 approved by 2116, 930 edit id of 5596",2013-10-11 12:01:56.470
185750,57294,11489.0,2,,CC BY-SA 3.0,d75c4039-8dfe-4e4c-8794-4603664608f3,"Yep, if had you bothered to read the manual of `quantile`, you would have found the function `ecdf` in the ""See Also"" section.

    x <- 0.1
    vec <- rnorm( 100 )
    ecdf( vec )( x )
    # or
    my.ecdf <- ecdf( vec )
    my.ecdf( x )

`ecdf` is a function returning another function -- that in turn is the experimental distribution function of your distribution.",,2013-10-11 12:03:59.843
185751,57295,21762.0,2,,CC BY-SA 3.0,37a8cc39-d0ec-49fe-9026-bb9b8266e077,"A data driven (and thus probably not so very good) approach

Calculate four correlation matrices: One for each layer and one for the pooled data (three lines per sample). If they all look quite similar, run a PCA based on the correlation matrix of the pooled sample and go on with the first few PCs.

Instead of comparing the four correlation matrices, you could also consider the four loading matrices of the corresponding PCAs and compare the loadings of the first few PCs. This is much easier if you have lots of variables.",,2013-10-11 12:05:40.297
185752,57294,11489.0,12,,,d0f471eb-e1c7-4bb3-8dce-cb8191f65a24,"{""Voters"":[{""Id"":14803,""DisplayName"":""January""}]}",via Vote,2013-10-11 12:06:11.553
185753,57293,674.0,5,,CC BY-SA 3.0,cf62b295-c42e-4407-9578-17de1111bb42,"I am testing various techniques for dealing with strong multi-collinearity in a regression problem.

There have been various comparison papers written between competing techniques such as Ridge Regression (RR) and Principal Components Regression (PCR). There seems to be no clear winner though with the best technique seemingly problem specific. However one thing that bothers me about the PCR approach is the somewhat arbitrary way in which one simply excludes the smallest eigenvectors as has been proven in Hadi and Ling even the smallest eigenvector may have strong predictive power while the largest eigenvectors may have none.

> ""[Some Cautionary notes on the use of Principal Components
> Regression][1]"" by Hadi and Ling. ([PDF][2])

They also show that the the SSE can be vastly improved by adding seemingly insignificant eigenvectors. 

In their discussion they highlght two papers that try to address this 2nd deficiency--Lott(1973) and Gunst and Mason(1973--but it has been shown that the Lott technique fails to pick the ""correct"" eigenvectors in the presence of strong MC, and my problem has strong MS. 

Do you know of a paper that can select the optimum set of eigenvalues even in the presence of strong MC? 
Or more recent papers that compare PCR and RR?


  [1]: http://www.jstor.org/discover/10.2307/2685559?uid=3738016&uid=2&uid=4&sid=21102750270477
  [2]: http://www.uvm.edu/~rsingle/stat380/F04/possible/Hadi+Ling-AmStat-1998_PCRegression.pdf",added 223 characters in body,2013-10-11 12:06:53.477
185754,57293,674.0,4,,CC BY-SA 3.0,cf62b295-c42e-4407-9578-17de1111bb42,Selecting optimal set of eigenvectors for Principal Components Regression,added 223 characters in body,2013-10-11 12:06:53.477
185755,57294,11489.0,13,,,e8860c4f-f2d9-42a6-9679-7b59ef267e93,"{""Voters"":[{""Id"":14803,""DisplayName"":""January""}]}",,2013-10-11 12:07:34.730
185757,57296,21398.0,1,,CC BY-SA 3.0,fe559804-4816-4ff1-86a6-e6814817feac,Selection probability weight,,2013-10-11 12:11:24.820
185758,57296,21398.0,3,,CC BY-SA 3.0,fe559804-4816-4ff1-86a6-e6814817feac,<weighted-sampling>,,2013-10-11 12:11:24.820
185756,57296,21398.0,2,,CC BY-SA 3.0,fe559804-4816-4ff1-86a6-e6814817feac,"I have a question on my selection probability weight. Is it a correct weight?

***The research design:*** research areas were divided into strata according to size. Interviews were taken: 50 batches of 10 interviews in each area according to the relative size of strata. 
Clusters were made for each stratum. In each cluster: batches of 10 interviews were sampled in fixed intervals. A random walk selected households and within these households, respondents were randomly chosen.

***The selection probability weight:*** I had no population data on number of households. A selection probability weight was calculated for the within-household selection for each stratum. In each stratum, a weight was calculated and normalized so that the sum of the weights is 500 for each research area. The size of the eventual stratum was divided by the number of people in the stratum eligible for the survey. The result of this calculation was then multiplied by the number of eligible respondents in the household.
",,2013-10-11 12:11:24.820
185761,57297,18198.0,2,,CC BY-SA 3.0,b510d6c8-caec-4002-917b-283ff85acac8,"I have an eigen decomposition of a 30 variable covariance matrix calculated using 5y of daily data and would like to compare it to a different 5y period to see if the eigenvalues are the same. Obviously they will not be exactly the same due to noise in the signal but can I test statistically that they are the the same?

""An asymptotic chi-square test for the equality of two correlation matrices"" by R. Jennrich

The closest match I have found is a paper to test the equivalence on two correlation matrices, but as I am working in the Eigenvector space I would prefer a test that is performed on the eigenvectors (plus the paper is quite old).

Also on a similar topic what is the minimum length of time I can run a PCA analysis over for 30 variables on daily data. Clearly if i can generate more eigenvector decompositions to compare I can be more confident in my results.",,2013-10-11 12:13:44.087
185760,57297,18198.0,3,,CC BY-SA 3.0,b510d6c8-caec-4002-917b-283ff85acac8,<pca>,,2013-10-11 12:13:44.087
185763,57278,20144.0,5,,CC BY-SA 3.0,5ad84eb6-e4d5-4847-8dfb-13c04bbc8eba,"first of all, I want to express my apologies if the question is too broad or wrong, but I am in need of references and I have no idea of whom can I ask.

If you are interested, the question comes from a model I built, you can see some details [here](http://physics.stackexchange.com/questions/78524/boltzmann-distribution-with-interaction-between-particles) and [here](http://physics.stackexchange.com/questions/80019/grand-canonical-ensemble-with-interaction-simulation-doubts). In this model I have:
$$f(\mathbb{x}|T,\mu)=\frac{h(\mathbb{x})e^{-\frac{E(\mathbb{x})}{kT}+\mu N(x)}}{\mathcal{Z}(T,\mu)}$$

There, my parameters are $\mu$ and $T$, and $\mathbb{x}=(x_1,\dots,x_M)$ where $x_i\in\{0,1\}$ and I have the restriction $\forall i\in\{1,\dots,M-D+1\}$
$$\sum_{j=0}^{D-1} x_{i+j} \leq 1$$
This is,$h(\mathbb{x})=0$ if that condition is not held.


I have the ""small"" inconvenient of not knowing $\mathcal{Z}(T,\mu)$, so I used a MCMC (Metropolis-Hastings) method to approximate this function. However I face two problems. The first of them is regarding the simulation and the model, and I am on solving it (it depends too much on the initial condition). The second is that these parameters are not fully known and I have no idea of how can I estimate them. I have been reading about bayesian inference and I know a bit of estimator theory but I am no expert (furthermore I don't know if not knowing the partition function can affect). If any of you were able to give me some clue in the form of a book that I can read, I would be eternally grateful.

Thank you very much for your help.

Thanks to cardinal's comment, I have realized that I didn't explain one thing. It probably makes all more complex but there it goes:
The idea is that $E$ is known in each experiment, actually $E(\mathbf{x}) = \mathbf{E}\cdot\mathbf{x}$. However, $\mathbf{E}$ is not always the same, it represents an external potential for some particles. The ""good"" thing is that $T$, which accounts for the temperature, never changes whatever $\mathbf{E}$ is, so I thought that I could find a way of estimating it, given the fact that I have an empirical distribution of $x_{i}$ (so, a probability that a particle is in the position $i$) given a certain $\mathbf{E}$. So, in a way, what I have is
$$f(\mathbf{x}|T,\mu , \mathbf{E})$$, but I always know $\mathbf{E}$ and I know (can I say this?) that $T,\mu$ are independent of $\mathbf{E}$. I am sorry for not being clear enough before. I am starting to think that nothing of this makes sense...",Added information from the comments,2013-10-11 12:26:27.490
185766,57298,,3,user30602,CC BY-SA 3.0,09f4c732-254b-4e4b-805b-8eb9318eb76a,<density-function><cumulative-distribution-function>,,2013-10-11 12:36:44.077
185764,57298,,2,user30602,CC BY-SA 3.0,09f4c732-254b-4e4b-805b-8eb9318eb76a,"I have a function $f(x)=2ae^{-ax}(1-e^{-ax})$, for $x>0, a>0$. This is a pdf. I need to find $P(X>1)$. I have done all my work in such a way that I should get the same answer whether I use the pdf or the cdf to find this probability. However, I'm getting different answers. Can someone please help me?

**My attempt:** 

(using pdf) $P(X>1)=\int_0^{\infty}2ae^{-ax}(1-e^{-ax})dx = 2e^{-a}-e^{-2a}$ 
(using cdf) $P(X>1)= 1-P(X\leq 1) = 1 - (F_X(1)) = 1-(e^{-ax}(e^{-ax}-2))|_{x=1}=1-2e^{-a}-e^{-2a}$

Why are my answers different? Thanks!",,2013-10-11 12:36:44.077
185765,57298,,1,user30602,CC BY-SA 3.0,09f4c732-254b-4e4b-805b-8eb9318eb76a,Different answers for probability density function and cumulative density function,,2013-10-11 12:36:44.077
185767,57242,22573.0,5,,CC BY-SA 3.0,69e34ff8-77f1-4cf2-b4b3-fe462d860172,"Assume I am looking for a normal distribution $\mathcal{N}(\mu,\Sigma)$. For simplicity let's say we only have 2 random variables $x$ and $y$ and a known $\mu=0$.

Is it possible to estimate $\Sigma$ by observing the variance along multiple directions?

For example, I measure the variance $\sigma_1$ along the vector $\mathbb{v}_1 = (x_1,y_1)^T$. In another step I obtain a different variance $\sigma_2$ from a different direction $\mathbb{v}_2 = (x_2,y_2)^T$. Ideally one would continue to observe these single variations in different directions and combine them in one multivariate normal distribution.

Does this make sense?

EDIT:
Some additional background information might be useful: I have a sensor device with known position and orientation in 2D space (in a future step both may have an uncertainty). The sensor is able to measure only the distance of a point along its orientation. I'm also given the sensor model. So for each distance measure $d_i$, I obtain the standard error $\sigma(d_i)$ which depends on the distance.

Since I'm not able to manipulate the sensor position to my advantage or perform a large number of measurements, I'd like to combine these variances into one covariance matrix in order to make a more reliable prediction of the position of the measured point.

This is just a thought that is still under development with no guaranty to work out correctly. Hence my question of ""making sense""...",added background information to clarify problem,2013-10-11 12:39:25.957
185768,57298,,5,user30602,CC BY-SA 3.0,dcd7f79b-65c2-4adc-90c4-bd6e5aebdee0,"I have a function $f(x)=2ae^{-ax}(1-e^{-ax})$, for $x>0, a>0$. This is a pdf. I need to find $P(X>1)$. I have done all my work in such a way that I should get the same answer whether I use the pdf or the cdf to find this probability. However, I'm getting different answers. Can someone please help me?

**My attempt:** 

(using pdf) $P(X>1)=\int_1^{\infty}2ae^{-ax}(1-e^{-ax})dx = 2e^{-a}-e^{-2a}$ 
(using cdf) $P(X>1)= 1-P(X\leq 1) = 1 - (F_X(1)) = 1-(e^{-ax}(e^{-ax}-2))|_{x=1}=1-2e^{-a}-e^{-2a}$

Why are my answers different? Thanks!",fixed grammar,2013-10-11 12:44:22.840
185771,57299,19395.0,1,,CC BY-SA 3.0,7d28127f-c600-43ac-b3de-3beb623d882e,Before and after data: Which test for average comparison of Likert scale data?,,2013-10-11 13:10:18.780
185770,57299,19395.0,3,,CC BY-SA 3.0,7d28127f-c600-43ac-b3de-3beb623d882e,<normal-distribution><t-test><likert>,,2013-10-11 13:10:18.780
185769,57299,19395.0,2,,CC BY-SA 3.0,7d28127f-c600-43ac-b3de-3beb623d882e,"I have one group of respondents which answer on a scale of 1-5 once before and once after an experiment. I want to see if the experiment made a difference to their responses.

I was told not to use a t-test because of the Likert scale (ordinal data does not seem to fit a t-test) and because my data are not nearly normally distributed (answers to the questions lean heavily to the 1 of the scale (which is not a mistake in the design)).

I am not sure if the Wilcoxon signed-rank test works, because it seems to be designed for differences in groups (as in ""Do men respond differently from women?"").

Any suggestions on what could actually be used here?

(The answer [here](http://stats.stackexchange.com/questions/20245/using-t-test-for-comparing-likert-responses-before-and-after-intervention) refers to a ""special paired t-test"", but does not explain which one)",,2013-10-11 13:10:18.780
185772,57300,21762.0,2,,CC BY-SA 3.0,a57aeeb8-f38f-4bec-8bfe-ccab83f5821e,"Wilcoxon's signed-rank test is usually a good choice in such a situation. It is the paired version of Wilcoxon's rank-sum test (aka Mann-Whitney-U-test). I think you are mixing these two procedures.

Make sure you use an exact/pseudo-exact implementation of the test to account for the highly discrete distribution.",,2013-10-11 13:17:25.363
185773,57301,22059.0,2,,CC BY-SA 3.0,9d9b1065-f9a4-4cea-adee-61e7a059d8ad,"It looks like a simple calculation error:

\begin{align}
F_X(y)&=\int_{x=0}^y 2ae^{-ax}(1-e^{-ax})\\
&=e^{-ax}(e^{-ax}-2)|_{x=0}^y\\
&=e^{-2ay}-2e^{-ay}+1.
\end{align}
",,2013-10-11 13:22:52.120
185774,57298,,5,user30602,CC BY-SA 3.0,dda38a81-a611-4837-acdd-20c4c7bc8f75,"I have a function $f(x)=2ae^{-ax}(1-e^{-ax})$, for $x>0, a>0$. This is a pdf. I need to find $P(X>1)$. I have done all my work in such a way that I should get the same answer whether I use the pdf or the cdf to find this probability. However, I'm getting different answers. Can someone please help me?

**My attempt:** 

(using pdf) $P(X>1)=\int_1^{\infty}2ae^{-ax}(1-e^{-ax})dx = 2e^{-a}-e^{-2a}$ 

(using cdf) $P(X>1)= 1-P(X\leq 1) = 1 - (F_X(1)) = 1-(e^{-ax}(e^{-ax}-2))|_{x=1}=1-2e^{-a}-e^{-2a}$

Why are my answers different? Thanks!",improved formatting,2013-10-11 13:24:58.080
185855,57325,22622.0,3,,CC BY-SA 3.0,9e44d147-9a69-4e49-baaf-af8f690a9818,<regression><interaction>,,2013-10-11 20:30:29.607
185775,57302,750.0,2,,CC BY-SA 3.0,8c193f5c-879b-41d0-9f07-3a3f63ebf992,"You don't need extra terms in the models, less actually. It is plain to see by your plots, but if you look at `summary(m2)` you will see that the variance for random effect for `x` is really small, and the variance for the intercept is quite small as well.

Similarly for the `m1` model, you can see from the plot that the slopes are all the same, but the intercept varies. You can use an F-test to check the model with *only* random intercepts versus the model with random slopes and intercepts you specified.

    m1 <- lmer(y~x + (x|g),data=dat1)
    m1RInt <- lmer(y~x + (1|g),data=dat1)
    anova(m1,m1RInt)

Also just looking at the variance estimates of the random intercepts and effects for `summary(m1)` you would have come to the same conclusion that using random intercepts adds nothing to the model.",,2013-10-11 13:41:25.093
185778,57303,11506.0,3,,CC BY-SA 3.0,1d68e327-b90c-4b86-a7b1-4427a21c3086,<machine-learning><clustering>,,2013-10-11 13:50:46.157
185777,57303,11506.0,1,,CC BY-SA 3.0,1d68e327-b90c-4b86-a7b1-4427a21c3086,Quantitative results of clustering analysis,,2013-10-11 13:50:46.157
185776,57303,11506.0,2,,CC BY-SA 3.0,1d68e327-b90c-4b86-a7b1-4427a21c3086,"Currently, I am doing a clustering analysis for two sets of data. One smaller dataset (about 100 data) got ground truth labels, and one larger dataset (about 2000 data) has no ground truth labels.

For the smaller dataset, obviously, I can obtain quantitative results like accuracy, sensitivity and specificity.

However, for the larger dataset, I have no ground truth and couldn't get any useful quantitative results.

1. The only thing I found useful is the 'mean silhouette value', which can measure the cluster performance. However, it based on some distance measure that can only tell people how separate are the clusters. I am wondering if there are other 'better' or 'more appropriate' quantitative analysis for data without labels.

2. Because the data are without labels, I am also wondering if we can somehow have a 'uncertainty' measure about the clustering results like how confident about the cluster results?

3. For the smaller dataset with labels, except accuracy, sensitivity and specificity, any other quantitative results I can get? For the classification algorithm, we can do a cross-validation, is there any method we can use to do such a cross-validation for clustering? Also, can we get ROC analysis for clustering task?

Thanks!
A.
  ",,2013-10-11 13:50:46.157
185779,57300,21762.0,5,,CC BY-SA 3.0,a58a1e78-8a2c-4633-a9c1-7aff4fe52730,"Wilcoxon's signed-rank test is usually a good choice in such a situation. It is the paired version of Wilcoxon's rank-sum test (aka Mann-Whitney-U-test). I think you are mixing these two procedures.

Make sure you use an exact/pseudo-exact implementation of the test to account for the highly discrete distribution.

EDIT: How you do it in R for x (pre) and y (post)

    library(coin)
    set.seed(2)
    x <- sample(1:2, 20, T)
    y <- sample(2:3, 20, T)
    
    #Basic R gives p value of 0.0007167
    wilcox.test(x-y)	 				
    
    #Coin gives p value of 0.0001221
    wilcoxsign_test(x~y, distribution = exact())
    ",How you do it in R,2013-10-11 13:51:35.963
185782,57304,9522.0,3,,CC BY-SA 3.0,6b560f2a-0d80-4352-8587-49f671a62762,<biostatistics>,,2013-10-11 14:05:31.013
185781,57304,9522.0,1,,CC BY-SA 3.0,6b560f2a-0d80-4352-8587-49f671a62762,Which statistic test to used?,,2013-10-11 14:05:31.013
185780,57304,9522.0,2,,CC BY-SA 3.0,6b560f2a-0d80-4352-8587-49f671a62762,"I have performed an experiment to test the cellular sensitivity to a certain DNA damage agent. We have found 270 genes that were specifically sensitive to the drug and the total number of genes analyzed was 3668. 38 out of the 270 sensitive genes are classified as ""DNA repair genes"". If the number of ""DNA repair genes"" containing in the genome is 112 and the total number of genes in the genome in 3668, are the sensitive genes enrichment in DNA repair genes?
Which statistic may I have to used? I would appreciate if you could also tell me some tool to calculate the pvalue online because I do not much about biostatistic.
Thanks in advanced",,2013-10-11 14:05:31.013
185783,57291,668.0,10,,,dc9f49a6-f361-4a4b-8014-0b0ac3fa8ff6,"{""Voters"":[{""Id"":8413,""DisplayName"":""Momo""},{""Id"":17230,""DisplayName"":""Scortchi""},{""Id"":1036,""DisplayName"":""Andy W""},{""Id"":7290,""DisplayName"":""gung""},{""Id"":919,""DisplayName"":""whuber""}]}",102,2013-10-11 14:07:02.583
185786,57305,22607.0,3,,CC BY-SA 3.0,223ad603-0971-4f74-a672-2c494e949f90,<probability><distributions><density-function><moments>,,2013-10-11 14:20:05.910
185785,57305,22607.0,1,,CC BY-SA 3.0,223ad603-0971-4f74-a672-2c494e949f90,Interpretation of a PDF squared,,2013-10-11 14:20:05.910
185784,57305,22607.0,2,,CC BY-SA 3.0,223ad603-0971-4f74-a672-2c494e949f90,"I have a problem where the crucial variable is the integral of the squared PDF of a random variable, i.e.

  $\int f(x)^2dx$

How should I interpret this property of a distribution?  If $f(x)$ is gaussian, then this is inversely proportional to the variance, $\sigma^2$, but I don't think this is generally true.

(Note that this is also equal to $\int F(x)f'(x)dx$ ).

",,2013-10-11 14:20:05.910
185787,57293,668.0,5,,CC BY-SA 3.0,0b28f8f5-b5f1-4632-8f81-b0c90d3ae611,"I am testing various techniques for dealing with strong multi-collinearity (MC) in a regression problem.

There have been various comparison papers written between competing techniques such as Ridge Regression (RR) and Principal Components Regression (PCR). There seems to be no clear winner though with the best technique seemingly problem specific. However one thing that bothers me about the PCR approach is the somewhat arbitrary way in which one simply excludes the smallest eigenvectors as has been proven in Hadi and Ling even the smallest eigenvector may have strong predictive power while the largest eigenvectors may have none.

> ""[Some Cautionary notes on the use of Principal Components
> Regression][1]"" by Hadi and Ling. ([PDF][2])

They also show that the the SSE can be vastly improved by adding seemingly insignificant eigenvectors. 

In their discussion they highlght two papers that try to address this 2nd deficiency--Lott(1973) and Gunst and Mason(1973)--but it has been shown that the Lott technique fails to pick the ""correct"" eigenvectors in the presence of strong MC, and my problem has strong MC. 

Do you know of a paper that can select the optimum set of eigenvalues even in the presence of strong MC?  Or more recent papers that compare PCR and RR?


  [1]: http://www.jstor.org/discover/10.2307/2685559?uid=3738016&uid=2&uid=4&sid=21102750270477
  [2]: http://www.uvm.edu/~rsingle/stat380/F04/possible/Hadi+Ling-AmStat-1998_PCRegression.pdf",added 5 characters in body,2013-10-11 14:20:18.940
185856,57325,22622.0,1,,CC BY-SA 3.0,9e44d147-9a69-4e49-baaf-af8f690a9818,Marginal effect in model with interactions,,2013-10-11 20:30:29.607
185890,57338,21746.0,2,,CC BY-SA 3.0,883f30d6-db01-40ea-b3bb-d00768bd54f3,"In Neural Nets for the regression problem, we rescale the continuous labels consistently with the output activation function, i.e. normalize them if the logistic sigmoid is used, or adjusted normalize them if tanh is used. At the end we can restore original range but renormalizing the output neurons back.

Should we also normalize input features? And how? For example, if hidden activation differs from the output activation? E.g. if hidden activation is TANH and output activation is LOGISTIC, should the input features be normalized to lie in [0,1] or [-1,1] interval?",,2013-10-11 23:29:41.150
185788,57306,21638.0,2,,CC BY-SA 3.0,60230186-7e87-47df-a2d8-f8290f1245fa,"Standard practice to test for enrichment of gene lists is to do a hypergeometric test or, equivalently, a one-sided [Fisher's exact test][1]. You have the following $2\times2$ contingency table:

$$
\array{& \text{DNA Repair} & \text{Other} \\\text{Sensitive} & 38 & 232 & 270\\\text{Not Sensitive} & 74 & 3324 & 3398 \\ & 112 & 3556}
$$

You can carry out the test in `R` as follows:

    fisher.test(matrix(c(38,74,232,3324),nrow=2,ncol=2),alternative=""greater"")

Which gives a highly significant result:

	Fisher's Exact Test for Count Data

    data:  matrix(c(38, 74, 232, 3324), nrow = 2, ncol = 2) 
    p-value < 2.2e-16
    alternative hypothesis: true odds ratio is greater than 1 
    95 percent confidence interval:
    5.062107      Inf 
    sample estimates:
    odds ratio 
    7.34918

Note that as we are testing for over-representation (rather than under-representation) the `alternative` parameter is set to `""greater""`.

  [1]: http://en.wikipedia.org/wiki/Fisher%27s_exact_test",,2013-10-11 14:48:42.243
185790,57279,22262.0,5,,CC BY-SA 3.0,cdf00a11-c554-4f7f-918f-7810ee18440c,"What variable selection approach should I consider if I have thousands of predictors with clusters that are extremely correlated? 

For example I might have a predictor set $X:= \{A_1,A_2,A_3,A_4,...,A_{39},B_1,B_2,...,B_{44},C_1,C_2,...\}$ with cardinality $|X| > 2000$. Consider the case where all $\rho(A_i,A_j)$ are very high, and similarly for $B$, $C$, .... 

Correlated predictors aren't correlated ""naturally""; it's a result of the feature engineering process. This is because all $A_i$ are hand engineered from the same underlying data with small variations in hand-engineering methodology, e.g. I use a thinner pass band on $A_2$ than I did for $A_1$ in my denoising approach but everything else is the same.

My goal is to improve out of sample accuracy in my classification model.  

One approach would just be to try everything:  non-negative garotte, ridge, lasso, elastic nets, random subspace learning, PCA/manifold learning, least angle regression and pick the one that's best in my out of sample dataset. But specific methods that are good at dealing with the above would be appreciated.

Note that my out of sample data is extensive in terms of sample size. ",added 74 characters in body,2013-10-11 15:05:21.000
185795,57307,9522.0,3,,CC BY-SA 3.0,2d649f53-aed2-41ae-a13a-2007bc0a93ca,<software><fishers-exact-test>,,2013-10-11 15:15:59.620
185794,57307,9522.0,1,,CC BY-SA 3.0,2d649f53-aed2-41ae-a13a-2007bc0a93ca,Any online software to calculate pvalue of Fisher exact test?,,2013-10-11 15:15:59.620
185793,57307,9522.0,2,,CC BY-SA 3.0,2d649f53-aed2-41ae-a13a-2007bc0a93ca,"I would like to do a one-sided Fisher´s exact test for an analysis. I have not any statistic software to obtain the pvalues (no SAS, no SPSS). The 2x2 tables are of this type:
 
Do you know any online statistical software to calculate the pvalues? I have tried with some of them but the results indicate pvalue<0.0001 but I need to know the exact number.
The 2x2 tables are of this type:



 ![enter image description here][1]


  [1]: https://i.stack.imgur.com/BUTLq.png

Thanks a lot in advanced!",,2013-10-11 15:15:59.620
185796,57305,,25,,,b968dc23-f4f0-4d36-8421-9cfaa1f87e3a,,http://twitter.com/#!/StackStats/status/388684734056128512,2013-10-11 15:17:16.730
185797,57304,15827.0,4,,CC BY-SA 3.0,9e8aa6be-64d2-4ab7-bdbc-408ede818fa6,Which statistical test should be used to test for enrichment of gene lists?,more informative title; fixes to English,2013-10-11 15:20:35.360
185798,57304,15827.0,5,,CC BY-SA 3.0,9e8aa6be-64d2-4ab7-bdbc-408ede818fa6,"I have performed an experiment to test the cellular sensitivity to a certain DNA damage agent. We have found 270 genes that were specifically sensitive to the drug and the total number of genes analyzed was 3668. 38 out of the 270 sensitive genes are classified as ""DNA repair genes"". If the number of ""DNA repair genes"" contained in the genome is 112 and the total number of genes in the genome is 3668, are the sensitive genes enrichment in DNA repair genes?
Which statistical test should be used? I would appreciate if you could also tell me some tool to calculate the p-value online. ",more informative title; fixes to English,2013-10-11 15:20:35.360
185799,57308,22507.0,2,,CC BY-SA 3.0,d9982cf4-dc90-4cd4-94a8-c5e5c66d0fe7,"I would do the forward stepwise selection, adding predictors as long as the correlation with residuals is significant, and then do some regularization (ridge, lasso, elastic nets).  There are 2-3 metaparameters: forward stepwise termination constraint, and 1 or 2 regularization parameters. These metaparameters are determined via cross-validation.

If you want to take into account non-linearity, you could try random forest, which produces good results when there are many predictors.  But it is slow.",,2013-10-11 15:33:10.407
185800,57223,22564.0,5,,CC BY-SA 3.0,7487704b-5dae-4252-afaa-917449ff1839,"I have a problem like the following:

1) There are six measurements for each individual with large within-subject variance 

2) There are two groups (Treatment and Control)

3) Each group consists of 5 individuals

4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.


The data looks like this:
![http://s10.postimg.org/p9krg6f3t/examp.png][1]

And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. **This ignores within-subject variability**:


     n.simulations<-10000
        pvals=matrix(nrow=n.simulations,ncol=1)
        for(k in 1:n.simulations){
          subject=NULL
          for(i in 1:10){
            subject<-rbind(subject,as.matrix(rep(i,6)))
          }
          #set.seed(42)
          
          #Sample Subject Means
          subject.means<-rnorm(10,100,2)
          
          #Sample Individual Measurements
          values=NULL
          for(sm in subject.means){
            values<-rbind(values,as.matrix(rnorm(6,sm,20)))
          }
          
          out<-cbind(subject,values)
          
          #Split into GroupA and GroupB
          GroupA<-out[1:30,]
          GroupB<-out[31:60,]
          
          #Add effect size to GroupA
          GroupA[,2]<-GroupA[,2]+0
          
          colnames(GroupA)<-c(""Subject"", ""Value"")
          colnames(GroupB)<-c(""Subject"", ""Value"")
          
          #Calculate Individual Means and SDS
          GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
          for(i in 1:length(unique(GroupA[,1]))){
            GroupA.summary[i,1]<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
            GroupA.summary[i,2]<-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
          }
          colnames(GroupA.summary)<-c(""Mean"",""SD"")
          
          
          GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
          for(i in 1:length(unique(GroupB[,1]))){
            GroupB.summary[i,1]<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
            GroupB.summary[i,2]<-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
          }
          colnames(GroupB.summary)<-c(""Mean"",""SD"")
          
          Summary<-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
          colnames(Summary)[1]<-""Group""
          
          pvals[k]<-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
        }


And here is code for plots:


    #Plots
    par(mfrow=c(2,2))
    boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupA[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupA[,1]))){
      m<-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      ci<-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
            ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
            xlab=""Subject"", ylab=""Value"")
    stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
    #abline(h=mean(GroupB[,2]), lty=2, lwd=3)
    
    for(i in 1:length(unique(GroupB[,1]))){
      m<-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      ci<-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]
      
      points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.2,
               ci[1],i-.2,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
            ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
            main=""Individual Averages"")
    stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)
    
    points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(.9,
             t.test(GroupA.summary[,1])$conf.int[1],.9,
             t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    
    points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
    segments(1.9,
             t.test(GroupB.summary[,1])$conf.int[1],1.9,
             t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
    )
    legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
         main=c(paste(""# sims="", n.simulations),
                paste(""% Sig p-values="",100*length(which(pvals<0.05))/length(pvals)))
    )

Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.

So what is the correct way to analyze this data?


**Bonus:**

The example above is a simplification. For the actual data: 

1) The within-subject variance is positively correlated with the mean. 

2) Values can only be multiples of two. 

3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. 

4) Number of Subjects in each group are not necessarily equal. 

Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.

**EDIT:**

Ok, here is what *actual* data looks like. There is also three groups rather than two:

![enter image description here][2]

dput() of data:

    structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
    3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
    6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
    10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
    12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
    15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
    18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
    22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
    6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
    2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
    12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
    10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
    20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
        NULL, c(""Group"", ""Subject"", ""Value"")))


**EDIT 2:**

In response to Henrik's answer:
So if I instead perform anova followed by TukeyHSD procedure on the individual averages as shown below, I could interpret this as underestimating my p-value by about 3-4x? 

My goal with this part of the question is to understand how I, as a reader of a journal article, can better interpret previous results given their choice of analysis method. For example they have those ""stars of authority"" showing me 0.01>p>.001. So if i accept 0.05 as a reasonable cutoff I should accept their interpretation? The only additional information is mean and SEM.

    #Get Invidual Means
    summary=NULL
    for(i in unique(dat[,2])){
    sub<-which(dat[,2]==i)
    summary<-rbind(summary,cbind(
    dat[sub,1][3],
    dat[sub,2][4],
    mean(dat[sub,3]),
    sd(dat[sub,3])
    )
    )
    }
    colnames(summary)<-c(""Group"",""Subject"",""Mean"",""SD"")
    
    TukeyHSD(aov(summary[,3]~as.factor(summary[,1])+ (1|summary[,2])))
    
    #      Tukey multiple comparisons of means
    #        95% family-wise confidence level
    #    
    #    Fit: aov(formula = summary[, 3] ~ as.factor(summary[, 1]) + (1 | summary[, 2]))
    #    
    #    $`as.factor(summary[, 1])`
    #             diff       lwr       upr     p adj
    #    2-1 -0.672619 -4.943205  3.597967 0.9124024
    #    3-1  7.507937  1.813822 13.202051 0.0098935
    #    3-2  8.180556  2.594226 13.766885 0.0046312




**EDIT 3:**
I think we are getting close to my understanding. Here is the simulation described in the comments to @Stephane:


    #Get Subject Means
    means<-aggregate(Value~Group+Subject, data=dat, FUN=mean)

    #Initialize ""dat2"" dataframe
    dat2<-dat
    
    #Initialize within-Subject sd
    s<-.001
    pvals=matrix(nrow=10000,ncol=2)
    
    for(j in 1:10000){
    #Sample individual measurements for each subject
    temp=NULL
    for(i in 1:nrow(means)){
    temp<-c(temp,rnorm(6,means[i,3], s))
    }
    
    #Set new values
    dat2[,3]<-temp
    
    #Take means of sampled values and fit to model
    dd2 <- aggregate(Value~Group+Subject, data=dat2, FUN=mean)
    fit2 <- lm(Value~Group, data=dd2)
    
    #Save sd and pvalue
    pvals[j,]<-cbind(s,anova(fit2)[[5]][5])
    
    #Update sd
    s<-s+.001
    }
    
    plot(pvals[,1],pvals[,2], xlab=""Within-Subject SD"", ylab=""P-value"")


![enter image description here][6]


  [1]: https://i.stack.imgur.com/55V9J.png
  [2]: https://i.stack.imgur.com/k1xWd.png
  [3]: https://i.stack.imgur.com/55V9J.png
  [4]: https://i.stack.imgur.com/55V9J.png
  [5]: https://i.stack.imgur.com/55V9J.png
  [6]: https://i.stack.imgur.com/gMMDY.png",added simulation code and results,2013-10-11 15:48:11.567
185803,57309,22611.0,3,,CC BY-SA 3.0,044a8796-48fe-4b04-a9b8-a58d5a39ea09,<categorical-data><structural-equation-modeling>,,2013-10-11 16:00:23.980
185802,57309,22611.0,1,,CC BY-SA 3.0,044a8796-48fe-4b04-a9b8-a58d5a39ea09,Covary two dummy variables in SEM?,,2013-10-11 16:00:23.980
185801,57309,22611.0,2,,CC BY-SA 3.0,044a8796-48fe-4b04-a9b8-a58d5a39ea09,"I am running a structural equation model (SEM) in Amos 18, and I want to test the impact of marital status on several latent variables. Marital status is nominal, so I created three dummy variables:

1. Mar_Single: 1 = yes, 0 = no
2. Mar_Married: 1 = yes, 0 = no
3. Mar_Other: 1 = yes, 0 = no

I included Mar_Single and Mar_Married in the SEM, so their coefficients will be interpreted against the omitted (reference) group, Mar_Other. The modification indices suggested fit could be improved significantly if I covary Mar_Single and Mar_Married. Should I do this? In a way, this makes sense because they are perfectly correlated: If Mar_Single = 1 then Mar_Married will always = 0. 

Larry",,2013-10-11 16:00:23.980
185804,57310,20972.0,2,,CC BY-SA 3.0,d52a1b12-de82-4883-8197-7151c98c073f,"Microsoft research have an online tool [here](http://research.microsoft.com/en-us/um/redmond/projects/mscompbio/fisherexacttest). You can also download an Excel add-in from [here](http://www.real-statistics.com/free-download/).

Your result according to the Microsoft tool is 6.511E-017.",,2013-10-11 16:12:37.340
185839,57320,19331.0,2,,CC BY-SA 3.0,2b808cf5-0887-42c0-b297-4d7dc397064d,"The spatial power covariance structure is a generalization of the first-order autoregressive covariance structure.  Where the first-order autoregressive structure assumes the time points are equally spaced, the spatial power structure can account for a continuous time point.  In reality, we could just forget the first-order autoregressive structure entirely, because if we fit the spatial power structure when the data are equally spaced we'll get the same answer as when using the first-order autoregressive structure.

All that aside, the correlation function you're looking for is `corCAR1()`, which is the continuous first-order autoregressive structure.  If you're looking to duplicate what you fit in SAS, then the code you're looking for is:

    gls(CD4t~T, data=df, na.action = (na.omit), method = ""REML"",
        corr=corCAR1(form=~T|NUM_PAT))

Of course, you don't need to specify `method = ""REML""`, since, as in SAS, the default method in `gls()` is already restricted maximum likelihood.",,2013-10-11 19:13:11.093
185805,57254,22583.0,5,,CC BY-SA 3.0,fc5a157b-f9d6-46f5-ae4e-68e95f3552d2,"here is my situation. I am weighting a packet of material that has 10 individual units in it. In the end of the day I would like to know the average weight and variance of the individual units but the problem is that I cannot weight each unit individually since I would have to destroy the packet to get to the individual units. So in lieu of this, I am trying to make an inference of the individual units from what I know about the packets. I weighed 10 packets (hence I have 100 individual units). I was able to figure out the average weight of the units but am having trouble with the variance. Here is what I have done so far:

$$
\begin{split}
\bar{y}&=\frac{1}{10}\sum^{10}_{i=1}y_i\\
       &=\frac{1}{10}\sum^{10}_{i=1}  (x_{i,1}+x_{i,2}+...+x_{i,10})~~[since~y_i=x_{i,1}+x_{i,2}+...+x_{i,10}]\\
       &=\frac{1}{10}\sum^{100}_{j=1}x_j\\
       &=\frac{1}{10}(100~\bar{x})=10~\bar{x}
\end{split}
$$

thus we have the average of $x$, $\bar{x}=\frac{\bar{y}}{10}.$ But now my challenge is how to do I find variance of $x$ given the variance of $y$? Any suggestions? Thanks!

::::UPDATE::::

After some thought I came up with this reasoning:
$$
\begin{split}
\frac{1}{10}var(y)&=var(\bar{y})\\
       &=var(10~\bar{x})\\
       &=100~var(\bar{x})\\
       &=100~\frac{1}{100}var(x)~~[assuming~that~all~x~are~i.i.d.]\\
       &=var(x)
\end{split}
$$

thus we have $var(x)=\frac{1}{10}var(y).$ I am correct in that if we assume that all the individual units share the same common variance and are independent of each other, this result holds?
",added 486 characters in body,2013-10-11 16:15:38.403
185806,52871,18447.0,5,,CC BY-SA 3.0,8ea92f64-b133-4850-96aa-5e9ffd28d1b2,"I want to analyze a multilevel multidimensional model in WinBUGS. the model is as below (N=2362 students responding to K=45 items of a test, students are nested within J=116 schools):

    model{
    #responses
    for(i in 1:N){
    	for(j in 1:K){
    		logit(p[i,j])<- a1[j]*th[i,1]+a2[j]*th[i,2]-b[j]
    		y[i,j]~dbern(p[i,j] )
       	}
    	th[i,1:2]~dmnorm(mu[sc[i],1:2],tau.p[1:2,1:2])
    }
    #school level
    for(j in 1:J){  
    	mu[j,1:2]~dmnorm(m[j,1:2],tau.s[1:2,1:2])
    }    
    
    #priors
    for(j in 1:J){
    	m[j,1:2]~dmnorm(m0[1:2],cov[1:2,1:2])
    }
    
    tau.p[1:2,1:2]~dwish(cov[1:2,1:2],2)
    tau.s[1:2,1:2]~dwish(cov[1:2,1:2],2)
    sigma.p[1:2,1:2]<-inverse(tau.p[,])
    sigma.s[1:2,1:2]<-inverse(tau.s[,])
    s2p<-sum(sigma.p[,])
    s2s<-sum(sigma.s[,])
    rho<-(s2s)/(s2s+s2p)
    
    a1[1]~dlnorm(0,4)
    a2[1]<-0
    b[1]~dnorm(0,1)
    for(s in 2:K) {
    	a1[s]~dlnorm(0,4)
    	a2[s]~dlnorm(0,4)
      	b[s]~dnorm(0,1)
    }    
    }

I've set these functions as initial values:

    ini<-function(){
    list(tau.p=matrix(rgamma(4,100,100),2,2),
    tau.s=matrix(rgamma(4,100,100),2,2),
    th=rmvnorm(N,mean=c(0,0),sigma=diag(2)),
    m=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
    mu=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
    a1=rlnorm(K,0, 0.4),
    a2=c(NA,rlnorm(K-1,0, 0.4)),
    b=rnorm(45,0,0.5))
    }
I use rube package in R to check and run my analysis and everything looks fine. When I run the model I receive ""Trap 66 (postcondition violated)"" or ""undefined real result"". I think the problem is from the initials but I have no idea how to solve it.

Any idea?
",added 4 characters in body,2013-10-11 16:26:08.007
185807,57276,22310.0,5,,CC BY-SA 3.0,4a14ee46-e043-46c6-9976-2f6d4388ebe0,"I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence. 

My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But, there are multiple factors, other than English ivy, that affect soil moisture.

![enter image description here][1]

My questions are:

1. I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?

2. Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?

 ![The relationship between soil moisture and English ivy cover on cover objects (""the number of overstory trees"" for the left graph) for different levels of the surrounding overstory trees (""English ivy cover on cover objects"" for the left graph][2]

3. Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small **AND** regressions do not reflect my hypothesized causal relationships accurately.

I am using R, so any recommended code would be greatly helpful (I am a relatively new R user, though). 


  [1]: https://i.stack.imgur.com/k65Ag.jpg
  [2]: https://i.stack.imgur.com/ArgZm.jpg",deleted 1 characters in body,2013-10-11 16:42:08.147
185810,57311,22612.0,2,,CC BY-SA 3.0,bf3d2c94-b4b2-499c-9d02-838657b178d4,"dependent variables (ordinal): credit rating 1970 (cr70) and credit rating 1980 (cr80).

Here is what i want to do:

regress cr80-cr70 = independent vars.

how could this be done and how could you interpret it!?
if the dependent variable is continuous it would be simple. but can you make a new var from the difference of two ordinal vars, and have that be the dependent var?",,2013-10-11 16:46:12.820
185809,57311,22612.0,1,,CC BY-SA 3.0,bf3d2c94-b4b2-499c-9d02-838657b178d4,change in category variable,,2013-10-11 16:46:12.820
185808,57311,22612.0,3,,CC BY-SA 3.0,bf3d2c94-b4b2-499c-9d02-838657b178d4,<regression>,,2013-10-11 16:46:12.820
185811,57268,18040.0,5,,CC BY-SA 3.0,fca076d2-c4ab-4b54-968a-f4993e8bda16,"I have a question about how to tell two different mixed effects models apart.  In the simple case both involve fitting a model with a random group effect and a covariate. I fit the model with `lme4` in `R`.  Here is a visualization of the two different scenarios.   
![enter image description here][1]

    library(ggplot2)
    library(lme4)
    gen_dat2 <- function(group.m,group.v,int, sl,n){
          x <- vector()
          y <- vector()
          g <- vector()
             for(i in 1:length(group.m)){
             x.t <- rnorm(n,group.m[i],group.v[i])
             y.t <- rnorm(n,group.m[i],group.v[i])+int + sl*x.t 
             x <- c(x,x.t)
             y <- c(y,y.t)
             g <- c(g,rep(i,n))
            }
         return(cbind(x,y,g))
    }

    group.m <- runif(5,1,20)
    group.v <- runif(5,1,1.5)

    dat2 <- data.frame(gen_dat2(group.m,group.v,1,4,14))
    ggplot(dat2,aes(x=x,y=y,colour=as.factor(g),group=g))+geom_point()+stat_smooth(method=""lm"",se=F)
    m2 <- lmer(y~x + (x|g),data=dat2)



Then I can generate and fit the other scenario with similar code:

![enter image description here][2]

     gen_dat <- function(group.m,group.v,int, sl,n){
          x <- vector()
          y <- vector()
          g <- vector()
             for(i in 1:length(group.m)){
             x.t <- rnorm(n,0,1)
             y.t <- rnorm(n,group.m[i],group.v[i])+int + sl*x.t 
             x <- c(x,x.t)
             y <- c(y,y.t)
             g <- c(g,rep(i,n))
            }
         return(cbind(x,y,g))
    }

    group.m <- runif(5,1,20)
    group.v <- runif(5,1,1.5)

    dat1 <- data.frame(gen_dat(group.m,group.v,1,4,14))
    ggplot(dat1,aes(x=x,y=y,colour=as.factor(g),group=g))+geom_point()+stat_smooth(method=""lm"",se=F)
    m1 <- lmer(y~x + (x|g),data=dat1)


My central question is how do I tell these two models apart?  Am I incorrectly fitting the first one, and I need an extra term in there to model the relationships between groups and the x variable as well as y?  Both detect substantial between group variation in the intercept and not much in the slope as I would predict.  But I need a way to tell these two apart.  Any thoughts would be helpful.  


----
Edits:

This has been helpful in me restating the question.  So I want to re-ask the question with an example which I hope will make it clear why I want to be able to tell these two models apart. Let's imagine that Y is the average student test score at a school, and X is spending per student in that school.  Our grouping variables are 5 different school districts.  

Data in the top figure shows that an increase in spending within a district means that test scores increase.  It also shows that between districts there are differences is scores, but that's clearly because some districts spend more student than others.

Data in the second figure show similarly that within a district student scores increase as spending increases.  It also shows that between districts there are differences in test scores.  However we don't know what is driving those differences, unlike in the first set of data.  This is a pretty common situation I've encountered in building models.  The former is not.  

So what I'm asking is what is the appropriate model that captures the following features from the first dataset:

1. Test scores increase as spending per student does
2. There is also variance between districts in student test scores
3. Part of that difference between districts is because of the underlying relationship between spending and test scores, which also varies with district.

More generally stated, how do you handle a scenario where you're building a hierarchical model where the grouping variable is correlated with one of your continuous independent variables (e.g. the first scenario).  I feel like the model I've presented get's at points 1. and 2., but not point 3.  So I'm really seeking a way to tease these two scenarios apart.

Normally I might add an extra level of hierarchy if there was another group level explanatory variable.  Continuing our example, maybe in the 2nd dataset there are differences between district because in some districts parents have more time to spend on homework with students.  So we would add that as a group level predictor in a hierarchical model.  But that wouldn't work in the first scenario. 


  [1]: https://i.stack.imgur.com/GgxYw.png
  [2]: https://i.stack.imgur.com/pgGm7.png",added 2179 characters in body,2013-10-11 16:57:17.710
185814,57312,18198.0,3,,CC BY-SA 3.0,ebfdaff6-5f8b-41f5-8072-dd73d86ea0be,<ridge-regression>,,2013-10-11 16:59:04.847
185813,57312,18198.0,1,,CC BY-SA 3.0,ebfdaff6-5f8b-41f5-8072-dd73d86ea0be,Degrees of Freedom for Ridge regression without knowing the Ridge Parameter?,,2013-10-11 16:59:04.847
185812,57312,18198.0,2,,CC BY-SA 3.0,ebfdaff6-5f8b-41f5-8072-dd73d86ea0be,"There is a very nice post here that gives a neat solution to the problem of finding the ridge parameter when the degrees of freedom are known:

http://stats.stackexchange.com/questions/8309/how-to-calculate-regularization-parameter-in-ridge-regression-given-degrees-of-f

My question is how can you know what the degrees of freedom are before knowing the ridge parameter value is? ( I have added a comment to the other thread but its quite old so thought it best to create a new topic).



",,2013-10-11 16:59:04.847
185886,57336,22624.0,1,,CC BY-SA 3.0,3e33c97c-5173-4d5e-8712-f1829175ea73,How to test if a result is statistically significant?,,2013-10-11 22:55:22.160
185885,57336,22624.0,2,,CC BY-SA 3.0,3e33c97c-5173-4d5e-8712-f1829175ea73,"I am trying to determine if a certain conversion on my site is statistically significant.  I remembered doing this type of stuff in school but I can't seem to remember how to do it now.

For 1st set: n = 7297 and conversion was 2.618%
For 2nd set: n = 6107 and conversion was 2.669%

Any tips on how to do this?",,2013-10-11 22:55:22.160
185893,57305,1895.0,10,,,74c08ada-b6b3-4956-b263-e32229699128,"{""OriginalQuestionIds"":[9926],""Voters"":[{""Id"":2970,""DisplayName"":""cardinal""}]}",101,2013-10-11 23:34:46.400
185815,57259,6162.0,5,,CC BY-SA 3.0,67c1a788-1182-41f1-9cd5-9ee5c43295bd,"Now, try to you write down the model: $y_{ijk} = ...$ where $y_{ijk}$ is the $k$-th value for individual $j$ of group $i$. Then look at what happens for the means $\bar y_{ij\bullet}$: you get a classical Gaussian linear model, with variance homogeneity because there are $6$ repeated measures for each subject:

    > xtabs(~Group+Subject, data=dat)
         Subject
    Group 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
        1 6 6 6 6 6 6 6 0 0  0  0  0  0  0  0  0  0  0
        2 0 0 0 0 0 0 0 6 6  6  6  6  6  6  6  0  0  0
        3 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  6  6  6

Thus, since you are interested in mean comparisons only, **you don't need to resort to a random-effect or generalised least-squares model** - just use a classical (fixed effects) model using the means $\bar y_{ij\bullet}$ as the observations:

    tdat <- transform(dat, tvalue=f(Value))
    dd <- aggregate(tvalue~Group+Subject, data=tdat, FUN=mean)
    fit3 <- lm(tvalue~Group, data=dd)

The ANOVA provides the same answer as @Henrik's approach (and that shows that Kenward-Rogers approximation is correct):

    > anova(fit3)
    Analysis of Variance Table
    
    Response: tvalue
              Df Sum Sq Mean Sq F value  Pr(>F)  
    Group      2 3.3799 1.68994   4.121 0.03747 *

Then you can use `TukeyHSD()` or the `lsmeans` package for multiple comparisons: 

    > TukeyHSD(aov(fit3), ""Group"")
      Tukey multiple comparisons of means
        95% family-wise confidence level
    
    Fit: aov(formula = fit3)
    
    $Group
               diff         lwr       upr     p adj
    2-1 -0.07541248 -0.93627828 0.7854533 0.9719148
    3-1  1.11885667 -0.02896441 2.2666777 0.0565628
    3-2  1.19426915  0.06817536 2.3203629 0.0370434
    
    > library(lsmeans)
    > lsmeans(fit3, pairwise~Group)
    
    $`Group pairwise differences`
             estimate        SE df  t.ratio p.value
    1 - 2  0.07541248 0.3314247 15  0.22754 0.97191
    1 - 3 -1.11885667 0.4418996 15 -2.53193 0.05656
    2 - 3 -1.19426915 0.4335348 15 -2.75472 0.03704
        p values are adjusted using the tukey method for 3 means ",added 1 characters in body,2013-10-11 17:18:12.757
185816,57302,750.0,5,,CC BY-SA 3.0,0646fd40-6972-41a9-9736-37c0cc24afa2,"You don't need extra terms in the models, less actually. It is plain to see by your plots, but if you look at `summary(m2)` you will see that the variance for random effect for `x` is really small, and the variance for the intercept is quite small as well.

Similarly for the `m1` model, you can see from the plot that the slopes are all the same, but the intercept varies. You can use an F-test to check the model with *only* random intercepts versus the model with random slopes and intercepts you specified.

    m1 <- lmer(y~x + (x|g),data=dat1)
    m1RInt <- lmer(y~x + (1|g),data=dat1)
    anova(m1,m1RInt)

Also just looking at the variance estimates of the random intercepts and effects for `summary(m1)` you would have come to the same conclusion that using random slopes adds nothing to the model.",deleted 4 characters in body,2013-10-11 17:19:50.067
185817,57313,12544.0,2,,CC BY-SA 3.0,46fc523c-4c82-4644-830b-63aff0ce567e,"Yes, you should. You should always correlate exogenous variables - their correlations are not part of the model, and if you don't, you're either worsening fit, or getting degrees of freedom (which appear to improve fit) when you shouldn't.

When you do this in regression analysis, they are correlated (and there's no way of stopping them).

Also, you say ""If Mar_Single = 1 then Mar_Married will always = 0.""  Yes, but that doesn't make them perfectly correlated - because if mar_single = 0 mar_married might be 0 or might be 1.

If you use Mplus, it will correlate x-variables by default and will not include these correlations in the null model which it uses to calculate CFI and NFI.  ",,2013-10-11 17:21:17.923
185818,57278,,5,,CC BY-SA 3.0,54ab29a9-ae69-49f4-ba63-6c7f6ec7ec4e,"First of all, I want to express my apologies if the question is too broad or wrong, but I am in need of references and I have no idea whom I can ask.

If you are interested, the question comes from a model I built, you can see some details [here](http://physics.stackexchange.com/questions/78524/boltzmann-distribution-with-interaction-between-particles) and [here](http://physics.stackexchange.com/questions/80019/grand-canonical-ensemble-with-interaction-simulation-doubts). In this model I have:
$$f(\mathbb{x}|T,\mu)=\frac{h(\mathbb{x})e^{-\frac{E(\mathbb{x})}{kT}+\mu N(x)}}{\mathcal{Z}(T,\mu)}$$

There, my parameters are $\mu$ and $T$, and $\mathbb{x}=(x_1,\dots,x_M)$ where $x_i\in\{0,1\}$ and I have the restriction $\forall i\in\{1,\dots,M-D+1\}$
$$\sum_{j=0}^{D-1} x_{i+j} \leq 1$$
This is, $h(\mathbb{x})=0$ if that condition is not held.


I have the ""small"" inconvenience of not knowing $\mathcal{Z}(T,\mu)$, so I used a MCMC (Metropolis-Hastings) method to approximate this function. However I face two problems.

  - The first of them regards the simulation and the model and I am on solving it (it depends too much on the initial condition). 

  - The second is that these parameters are not fully known and I have no idea how can I estimate them. I have been reading about Bayesian inference and I know a bit of estimation theory but I am no expert (furthermore I don't know if not knowing the partition function can affect the result). If any of you were able to give me some clue in the form of a book that I can read, I would be eternally grateful.

Thank you very much for your help.

Thanks to cardinal's comment, I have realized that I didn't explain one thing. It probably makes all more complex but there it goes:
The idea is that $E$ is known in each experiment, actually $E(\mathbf{x}) = \mathbf{E}\cdot\mathbf{x}$. However, $\mathbf{E}$ is not always the same, it represents an external potential for some particles. The ""good"" thing is that $T$, which accounts for the temperature, never changes whatever $\mathbf{E}$ is, so I thought that I could find a way of estimating it, given the fact that I have an empirical distribution of $x_{i}$ (so, a probability that a particle is in the position $i$) given a certain $\mathbf{E}$. So, in a way, what I have is
$$f(\mathbf{x}|T,\mu , \mathbf{E})$$, but I always know $\mathbf{E}$ and I know (can I say this?) that $T,\mu$ are independent of $\mathbf{E}$. I am sorry for not being clear enough before. I am starting to think that nothing of this makes sense...",orthography and type setting,2013-10-11 17:28:42.097
185820,57314,3446.0,1,,CC BY-SA 3.0,62261775-d0aa-4875-bdb1-c54d43828957,Coverage rates of confidence intervals in reality,,2013-10-11 17:31:46.623
185821,57314,3446.0,2,,CC BY-SA 3.0,62261775-d0aa-4875-bdb1-c54d43828957,"One proves mathematically that if assumptions of a model are satisfied, then the coverage rate of a $100p\%$ confidence interval is $100p\%$.  But then statistics gets applied to the world, where model assumptions may not be satisfied.  Are there any studies comparing the coverage rates of confidence intervals applied to the real world with theoretical coverage rates?",,2013-10-11 17:31:46.623
185819,57314,3446.0,3,,CC BY-SA 3.0,62261775-d0aa-4875-bdb1-c54d43828957,<confidence-interval><application>,,2013-10-11 17:31:46.623
185824,57315,5984.0,3,,CC BY-SA 3.0,87accd3a-01a1-47cf-974d-9c515e15e2fe,<r><lme4-nlme><random-effects-model>,,2013-10-11 17:38:34.780
185823,57315,5984.0,1,,CC BY-SA 3.0,87accd3a-01a1-47cf-974d-9c515e15e2fe,What does it mean that random effects are highly correlated?,,2013-10-11 17:38:34.780
185840,57321,6162.0,2,,CC BY-SA 3.0,e112a1e8-2c33-4a31-bd80-ecbceb6f65f6,"> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests.

Let me develop this idea here. The model for the indivual observations is 
$$y_{ijk}= \mu_i + \alpha_{ij} + \epsilon_{ijk}$$, where :

 - $y_{ijk}$ is the $k$-th measurement of individual $j$ of group $i$ 

 - $\alpha_{ij} \sim_{\text{iid}} {\cal N}(0, \sigma^2_b)$ is the random effect for individual $j$ of group $i$ 

 - $\epsilon_{ijk} \sim_{\text{iid}} {\cal N}(0, \sigma^2_w)$ is the within-error

In [my answer to your first question][1], I have suggested you to note that one obtains a classical (fixed effects) Gaussian linear model for the group means $\bar y_{ij\bullet}$. Indeed you can easily check that $\bar y_{ij\bullet} = \mu_i + \delta_{ij}$ with $$\delta_{ij} = \alpha_{ij} + \frac{1}{K}\sum_k \epsilon_{ijk} 
\sim_{\text{iid}} {\cal N}(0, \sigma^2) \quad \text{with } \quad \boxed{\sigma^2=\sigma^2_b+\frac{\sigma^2_w}{K}},$$
assuming $K$ repeated measurements for each individual. This is nothing but the one-way ANOVA model with a fixed factor.

And then I claimed that in order to draw inference about the $\mu_i$ you can simply consider the simple classical linear model whose observations are the group means $\bar y_{ij\bullet}$. I think I spoke too quickly, and **I'd like to know the advice of an expert about this point**. I know it works here, but is it due to the fact that the observed group means $\bar y_{ij\bullet}$ are sufficient statistics for the $\mu_i$ ? (I do not remember the theory of sufficient statistics).

> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests. I would like to use a method where
> the larger the within subject variance the less sure I am about the
> group means or understand why it does not make sense to desire that.

As you see from the boxed formula, the within-variance $\sigma^2_w$ plays a role in the model for the observed group means. 

  [1]: http://stats.stackexchange.com/a/72490/8402",,2013-10-11 19:18:23.050
185894,57333,1895.0,33,,,81a5156c-119c-4bea-84e5-bbd86b21169a,,818,2013-10-11 23:40:09.413
185822,57315,5984.0,2,,CC BY-SA 3.0,87accd3a-01a1-47cf-974d-9c515e15e2fe,"What does it mean when two random effects are highly or perfectly correlated?  
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.


    summary(model.lmer) 
    Random effects:
    Groups   Name                    Variance   Std.Dev.  Corr                 
    popu     (Intercept)             2.5714e-01 0.5070912                      
              amdclipped              4.2505e-04 0.0206167  1.000               
              nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
              amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000

I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand

 - 1)what is doing on statistically  
 - 2)what is going on practically with
   the structure of the response variables.

 



**Example**

Here is an example based on ""[GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana][1]""
by Bolker et al

Download data

    download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
    dat.tf <- read.csv(""Banta_TotalFruits.csv"", header = TRUE)

Set up factors

    dat.tf <- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))

Modeling log(total.fruits+1) with ""population"" (popu) as random effect

    model.lmer <- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)

Accessing the Correlation matrix of the random effects show that everything is perfectly correlated

    attr(VarCorr(model.lmer)$popu,""correlation"")
    
                             (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
    (Intercept)                       1          1            1                      -1
    amdclipped                        1          1            1                      -1
    nutrientHigh                      1          1            1                      -1
    amdclipped:nutrientHigh          -1         -1           -1                       1


I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as

    cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)

Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?






  [1]: http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&ved=0CDYQFjAC&url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&ei=hTNYUpuzBu7J4APN5YHYBg&usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&bvm=bv.53899372,d.dmg
",,2013-10-11 17:38:34.780
185825,57273,22596.0,5,,CC BY-SA 3.0,383215c7-62af-4ddf-a393-d3323ce80b0f,"I could really use some guided help! I'm having difficulty understanding an MCMC implementation in terms of modeling a data set. I'm working on generating parameters from stellar light curves, and was asked to look into implementing an MCMC algorithm. A large chuck on the code is written in Python, so I've been trying to use [emcee hammer](http://dan.iel.fm/emcee/) to generate parameter fits. But going through the code, it's just not ""clicking"" how the method works.

I have a set of data (time vs flux) of two stars orbiting each other such that from our point of view, they eclipse. There are dips in the light curve to signify this. All I'm attempting to do is get the parameters of the system dependent on the characteristics of these dips.

In the emcee implementation, there are a few functions that I understand: the posterior function which, I believe, simply generates a data set given the set of parameters. Then there's a prior function which, I assume, is the function given a previous set of parameters. Somehow the algorithm chooses whether or not the jump to the posterior parameter set is to be done? I'm guessing that's what the use of the likelihood function is? To describe whether or not to take the jump?

I apologize, I'm quite confused on how this is to be implemented in terms a defined set of data.",added 4 characters in body,2013-10-11 17:58:05.320
185828,57316,22615.0,3,,CC BY-SA 3.0,79f8a73c-1353-4fca-b24f-cf8465773f56,<hypothesis-testing>,,2013-10-11 18:00:38.057
185826,57316,22615.0,2,,CC BY-SA 3.0,79f8a73c-1353-4fca-b24f-cf8465773f56,"I have two samples s1 and s2 of count data. The sample size is > 1000 each. The distributions look similar to a Poisson distribution but the variance is much larger than the mean. 

How do I test whether the mean of s1 is larger than the mean of s2?",,2013-10-11 18:00:38.057
185827,57316,22615.0,1,,CC BY-SA 3.0,79f8a73c-1353-4fca-b24f-cf8465773f56,mean difference for count data,,2013-10-11 18:00:38.057
185830,57317,22564.0,1,,CC BY-SA 3.0,e5bf0aba-3241-413e-9b9d-408ecc22b571,"When making inferences about group means, are credible Intervals sensitive to within-subject variance while confidence intervals are not?",,2013-10-11 18:05:20.930
185831,57317,22564.0,2,,CC BY-SA 3.0,e5bf0aba-3241-413e-9b9d-408ecc22b571,"This is a spin off of this question:
http://stats.stackexchange.com/questions/72453/how-to-compare-two-groups-with-multiple-measurements-for-each-individual-with-r

In the answers there (if I understood correctly) I learned that within-subject variance does not effect inferences made about group means and it is ok to simply take the averages of averages to calculate group mean, then calculate within-group variance and use that to perform significance tests. I would like to use a method where the larger the within subject variance the less sure I am about the group means or understand why it does not make sense to desire that.

Here is a plot of the original data along with some simulated data that used the same subject means, but sampled the individual measurements for each subject from a normal distribution using those means and a small within-subject variance (sd=.1). As can be seen the group level confidence intervals (bottom row) are unaffected by this (at least the way I calculated them).

![enter image description here][1]


I also used rjags to estimate the group means in three ways.
1) Use the raw original data
2) Use only the Subject means
3) Use the simulated data with small within-subject sd

The results are below. Using this method we see that the 95% credible intervals are narrower in cases #2 and #3. This meets my intuition of what I would like to occur when making inferences about group means, but I am not sure if this is just some artifact of my model or a property of credible intervals.

Note. To use rjags you need to first install JAGS from here:
http://sourceforge.net/projects/mcmc-jags/files/

![enter image description here][2]

The various code is below.

The original data:

    structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
    1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
    3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
    3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
    6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
    10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
    12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
    15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
    18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
    22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
    6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
    2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
    12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
    10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
    20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
        NULL, c(""Group"", ""Subject"", ""Value"")))



Get subject Means and simulate the data with small within-subject variance:

    #Get Subject Means
    means<-aggregate(Value~Group+Subject, data=dat, FUN=mean)
    
    #Initialize ""dat2"" dataframe
    dat2<-dat
    
    #Sample individual measurements for each subject
    temp=NULL
    for(i in 1:nrow(means)){
      temp<-c(temp,rnorm(6,means[i,3], .1))
    }
    
    #Set Simulated values
    dat2[,3]<-temp


The function to fit the JAGS model:

     require(rjags) 
    
    #Jags fit function
    jags.fit<-function(dat2){
        
      #Create JAGS model
      modelstring = ""
      
      model{
      for(n in 1:Ndata){
      y[n]~dnorm(mu[subj[n]],tau[subj[n]]) T(0, )
      }
      
      for(s in 1:Nsubj){
      mu[s]~dnorm(muG,tauG) T(0, )
      tau[s] ~ dgamma(5,5)
      }
      
      
      muG~dnorm(10,.01) T(0, )
      tauG~dgamma(1,1)
      
      }
      ""
      writeLines(modelstring,con=""model.txt"")
      
    #############  
      
      #Format Data
      Ndata = nrow(dat2)
      subj = as.integer( factor( dat2$Subject ,
                                 levels=unique(dat2$Subject ) ) )
      Nsubj = length(unique(subj))
      y = as.numeric(dat2$Value)
      
      dataList = list(
        Ndata = Ndata ,
        Nsubj = Nsubj ,
        subj = subj ,
        y = y
      )
      
      #Nodes to monitor
      parameters=c(""muG"",""tauG"",""mu"",""tau"")
      
      
      #MCMC Settings
      adaptSteps = 1000             
      burnInSteps = 1000          	
      nChains = 1                  	
      numSavedSteps= nChains*10000      	
      thinSteps=20                   	
      nPerChain = ceiling( ( numSavedSteps * thinSteps ) / nChains ) 			
      
      
      #Create Model
      jagsModel = jags.model( ""model.txt"" , data=dataList, 
                              n.chains=nChains , n.adapt=adaptSteps , quiet=FALSE )
      # Burn-in:
      cat( ""Burning in the MCMC chain...\n"" )
      update( jagsModel , n.iter=burnInSteps )
      
      # Getting DIC data:
      load.module(""dic"")
      
      
      # The saved MCMC chain:
      cat( ""Sampling final MCMC chain...\n"" )
      codaSamples = coda.samples( jagsModel , variable.names=parameters , 
                                  n.iter=nPerChain , thin=thinSteps )  
      
      mcmcChain = as.matrix( codaSamples )
      
      result = list(codaSamples=codaSamples, mcmcChain=mcmcChain)
      
    }


Fit the model to each group of each dataset:

    #Fit to raw data
    groupA<-jags.fit(dat[which(dat[,1]==1),])
    groupB<-jags.fit(dat[which(dat[,1]==2),])
    groupC<-jags.fit(dat[which(dat[,1]==3),])
    
    #Fit to subject mean data
    groupA2<-jags.fit(means[which(means[,1]==1),])
    groupB2<-jags.fit(means[which(means[,1]==2),])
    groupC2<-jags.fit(means[which(means[,1]==3),])
    
    #Fit to simulated raw data (within-subject sd=.1)
    groupA3<-jags.fit(dat2[which(dat2[,1]==1),])
    groupB3<-jags.fit(dat2[which(dat2[,1]==2),])
    groupC3<-jags.fit(dat2[which(dat2[,1]==3),])



Credible interval/highest density interval function:

    #HDI Function
    get.HDI<-function(sampleVec,credMass){ 
      sortedPts = sort( sampleVec )
      ciIdxInc = floor( credMass * length( sortedPts ) )
      nCIs = length( sortedPts ) - ciIdxInc
      ciWidth = rep( 0 , nCIs )
      for ( i in 1:nCIs ) {
        ciWidth[ i ] = sortedPts[ i + ciIdxInc ] - sortedPts[ i ]
      }
      HDImin = sortedPts[ which.min( ciWidth ) ]
      HDImax = sortedPts[ which.min( ciWidth ) + ciIdxInc ]
      HDIlim = c( HDImin , HDImax, credMass )
      return( HDIlim )
    }



First Plot:

    layout(matrix(c(1,1,2,2,3,4),nrow=3,ncol=2, byrow=T))
    
    boxplot(dat[,3]~dat[,2], 
    xlab=""Subject"", ylab=""Value"", ylim=c(0, 1.2*max(dat[,3])),
    col=c(rep(""Red"",length(which(dat[,1]==unique(dat[,1])[1]))/6),
    rep(""Green"",length(which(dat[,1]==unique(dat[,1])[2]))/6),
    rep(""Blue"",length(which(dat[,1]==unique(dat[,1])[3]))/6)
    ),
    main=""Original Data""
    )
    stripchart(dat[,3]~dat[,2], vert=T, add=T, pch=16)
    legend(""topleft"", legend=c(""Group A"", ""Group B"", ""Group C"", ""Individual Means +/- 95% CI""),
    col=c(""Red"",""Green"",""Blue"", ""Grey""), lwd=3, bty=""n"", pch=c(15),
    pt.cex=c(rep(0.1,3),1),
    ncol=3)
    
    for(i in 1:length(unique(dat[,2]))){
      m<-mean(examp[which(dat[,2]==unique(dat[,2])[i]),3])
      ci<-t.test(dat[which(dat[,2]==unique(dat[,2])[i]),3])$conf.int[1:2]
      
      points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.3,
               ci[1],i-.3,
               ci[2], lwd=4, col=""Grey""
      )
    }
    
    
    
    boxplot(dat2[,3]~dat2[,2], 
    xlab=""Subject"", ylab=""Value"", ylim=c(0, 1.2*max(dat2[,3])),
    col=c(rep(""Red"",length(which(dat2[,1]==unique(dat2[,1])[1]))/6),
    rep(""Green"",length(which(dat2[,1]==unique(dat2[,1])[2]))/6),
    rep(""Blue"",length(which(dat2[,1]==unique(dat2[,1])[3]))/6)
    ),
    main=c(""Simulated Data"", ""Same Subject Means but Within-Subject SD=.1"")
    )
    stripchart(dat2[,3]~dat2[,2], vert=T, add=T, pch=16)
    legend(""topleft"", legend=c(""Group A"", ""Group B"", ""Group C"", ""Individual Means +/- 95% CI""),
    col=c(""Red"",""Green"",""Blue"", ""Grey""), lwd=3, bty=""n"", pch=c(15),
    pt.cex=c(rep(0.1,3),1),
    ncol=3)
    
    for(i in 1:length(unique(dat2[,2]))){
      m<-mean(examp[which(dat2[,2]==unique(dat2[,2])[i]),3])
      ci<-t.test(dat2[which(dat2[,2]==unique(dat2[,2])[i]),3])$conf.int[1:2]
      
      points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.3,
               ci[1],i-.3,
               ci[2], lwd=4, col=""Grey""
      )
    }
    
    
    means<-aggregate(Value~Group+Subject, data=dat, FUN=mean)
    
    boxplot(means[,3]~means[,1], col=c(""Red"",""Green"",""Blue""),
    ylim=c(0,1.2*max(means[,3])), ylab=""Value"", xlab=""Group"",
    main=""Original Data""
    )
    stripchart(means[,3]~means[,1], pch=16, vert=T, add=T)
    
    for(i in 1:length(unique(means[,1]))){
      m<-mean(means[which(means[,1]==unique(means[,1])[i]),3])
      ci<-t.test(means[which(means[,1]==unique(means[,1])[i]),3])$conf.int[1:2]
      
      points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.3,
               ci[1],i-.3,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")
    
    
    means2<-aggregate(Value~Group+Subject, data=dat2, FUN=mean)
    
    boxplot(means2[,3]~means2[,1], col=c(""Red"",""Green"",""Blue""),
    ylim=c(0,1.2*max(means2[,3])), ylab=""Value"", xlab=""Group"",
    main=""Simulated Data Group Averages""
    )
    stripchart(means2[,3]~means2[,1], pch=16, vert=T, add=T)
    
    for(i in 1:length(unique(means2[,1]))){
      m<-mean(means[which(means2[,1]==unique(means2[,1])[i]),3])
      ci<-t.test(means[which(means2[,1]==unique(means2[,1])[i]),3])$conf.int[1:2]
      
      points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
      segments(i-.3,
               ci[1],i-.3,
               ci[2], lwd=4, col=""Grey""
      )
    }
    legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3,   col=""Grey"")


Second Plot:


    layout(matrix(c(1,2,3,4,4,4,5,5,5,6,6,6),nrow=4,ncol=3, byrow=T))
    
    #Plot priors
    plot(seq(0,10,by=.01),dgamma(seq(0,10,by=.01),5,5), type=""l"", lwd=4,
         xlab=""Value"", ylab=""Density"",
         main=""Prior on Within-Subject Precision""
    )
    plot(seq(0,10,by=.01),dgamma(seq(0,10,by=.01),1,1), type=""l"", lwd=4,
         xlab=""Value"", ylab=""Density"",
         main=""Prior on Within-Group Precision""
    )
    plot(seq(0,300,by=.01),dnorm(seq(0,300,by=.01),10,100), type=""l"", lwd=4,
         xlab=""Value"", ylab=""Density"",
         main=""Prior on Group Means""
    )
    
    
    #Set overall xmax value
    x.max<-1.1*max(groupA$mcmcChain[,""muG""],groupB$mcmcChain[,""muG""],groupC$mcmcChain[,""muG""],
                   groupA2$mcmcChain[,""muG""],groupB2$mcmcChain[,""muG""],groupC2$mcmcChain[,""muG""],
                   groupA3$mcmcChain[,""muG""],groupB3$mcmcChain[,""muG""],groupC3$mcmcChain[,""muG""]
    )
    
    
    #Plot result for raw data
    #Set ymax
    y.max<-1.1*max(density(groupA$mcmcChain[,""muG""])$y,density(groupB$mcmcChain[,""muG""])$y,density(groupC$mcmcChain[,""muG""])$y)
    
    plot(density(groupA$mcmcChain[,""muG""]),xlim=c(0,x.max), 
         ylim=c(-.1*y.max,y.max), lwd=3, col=""Red"",
         main=""Group Mean Estimates: Fit to Raw Data"", xlab=""Value""
    )
    lines(density(groupB$mcmcChain[,""muG""]), lwd=3, col=""Green"")
    lines(density(groupC$mcmcChain[,""muG""]), lwd=3, col=""Blue"")
    
    hdi<-get.HDI(groupA$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.033*y.max,hdi[2],-.033*y.max, lwd=3, col=""Red"")
    
    hdi<-get.HDI(groupB$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.066*y.max,hdi[2],-.066*y.max, lwd=3, col=""Green"")
    
    hdi<-get.HDI(groupC$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.099*y.max,hdi[2],-.099*y.max, lwd=3, col=""Blue"")
    
    ####
    
    #Plot result for mean data
    
    #x.max<-1.1*max(groupA2$mcmcChain[,""muG""],groupB2$mcmcChain[,""muG""],groupC2$mcmcChain[,""muG""])
    y.max<-1.1*max(density(groupA2$mcmcChain[,""muG""])$y,density(groupB2$mcmcChain[,""muG""])$y,density(groupC2$mcmcChain[,""muG""])$y)
    
    plot(density(groupA2$mcmcChain[,""muG""]),xlim=c(0,x.max), 
         ylim=c(-.1*y.max,y.max), lwd=3, col=""Red"",
         main=""Group Mean Estimates: Fit to Subject Means"", xlab=""Value""
    )
    lines(density(groupB2$mcmcChain[,""muG""]), lwd=3, col=""Green"")
    lines(density(groupC2$mcmcChain[,""muG""]), lwd=3, col=""Blue"")
    
    hdi<-get.HDI(groupA2$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.033*y.max,hdi[2],-.033*y.max, lwd=3, col=""Red"")
    
    hdi<-get.HDI(groupB2$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.066*y.max,hdi[2],-.066*y.max, lwd=3, col=""Green"")
    
    hdi<-get.HDI(groupC2$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.099*y.max,hdi[2],-.099*y.max, lwd=3, col=""Blue"")
    
    
    
    
    ####
    #Plot result for simulated data
    #Set ymax
    #x.max<-1.1*max(groupA3$mcmcChain[,""muG""],groupB3$mcmcChain[,""muG""],groupC3$mcmcChain[,""muG""])
    y.max<-1.1*max(density(groupA3$mcmcChain[,""muG""])$y,density(groupB3$mcmcChain[,""muG""])$y,density(groupC3$mcmcChain[,""muG""])$y)
    
    plot(density(groupA3$mcmcChain[,""muG""]),xlim=c(0,x.max), 
         ylim=c(-.1*y.max,y.max), lwd=3, col=""Red"",
         main=c(""Group Mean Estimates: Fit to Simulated data"", ""(Within-Subject SD=0.1)""), xlab=""Value""
    )
    lines(density(groupB3$mcmcChain[,""muG""]), lwd=3, col=""Green"")
    lines(density(groupC3$mcmcChain[,""muG""]), lwd=3, col=""Blue"")
    
    hdi<-get.HDI(groupA3$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.033*y.max,hdi[2],-.033*y.max, lwd=3, col=""Red"")
    
    hdi<-get.HDI(groupB3$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.066*y.max,hdi[2],-.066*y.max, lwd=3, col=""Green"")
    
    hdi<-get.HDI(groupC3$mcmcChain[,""muG""], .95)
    segments(hdi[1],-.099*y.max,hdi[2],-.099*y.max, lwd=3, col=""Blue"")



  [1]: https://i.stack.imgur.com/eiRJ9.png
  [2]: https://i.stack.imgur.com/H2rnX.png











",,2013-10-11 18:05:20.930
185829,57317,22564.0,3,,CC BY-SA 3.0,e5bf0aba-3241-413e-9b9d-408ecc22b571,<r><confidence-interval><jags><error-propagation>,,2013-10-11 18:05:20.930
185834,57318,18198.0,3,,CC BY-SA 3.0,ba61d32b-7b3d-4449-9ad0-6f0ab1204e89,<ridge-regression>,,2013-10-11 18:05:44.790
185833,57318,18198.0,1,,CC BY-SA 3.0,ba61d32b-7b3d-4449-9ad0-6f0ab1204e89,Iterative method to find Ridge Regression Parameter,,2013-10-11 18:05:44.790
185832,57318,18198.0,2,,CC BY-SA 3.0,ba61d32b-7b3d-4449-9ad0-6f0ab1204e89,"I have seen a method whereby instead of trying to estimate the ridge parameter (k) directly from the data (using one of the many many ridge parameter estimators in the literature) you solve for it iteratively.

The method is simple enough: You simply increase k (in suitably small steps) until the condition number is reduced blow 10.  

At first blush this seems like quite a nice solution to me but I've never seen a Ridge Regression paper/book that uses it. 

Is this method theoretically sound though? Even if (as I suspect) it isn't does it really matter for the average practitioner who just want to produce more stable estimates of their Beta's (the weights in the regression) rather than having them ""blow up"" to grossly unrealistic values when they experience severe MC?

Truly I would like to find a better method than this ideally with a solid theoretical underpinning but its hard to see from a practical view point it can be improved upon?
",,2013-10-11 18:05:44.790
185835,57314,,25,,,b2d52d57-2e32-409a-b05f-0c290b856fab,,http://twitter.com/#!/StackStats/status/388730032749047808,2013-10-11 18:17:16.803
185838,57319,1693.0,3,,CC BY-SA 3.0,e3cb4aac-6b0a-4fca-8b20-1c89c118df41,<regression><suppressor><controlling-for-a-variable>,,2013-10-11 18:20:07.563
185837,57319,1693.0,1,,CC BY-SA 3.0,e3cb4aac-6b0a-4fca-8b20-1c89c118df41,How high must RSQ be for a suppressor effect to show up?,,2013-10-11 18:20:07.563
185836,57319,1693.0,2,,CC BY-SA 3.0,e3cb4aac-6b0a-4fca-8b20-1c89c118df41,"I am modeling an outcome for hospital patients, 'RA' (whether readmitted).  My predictor of interest is 'HHS' (whether referred to Home Health Services such as from a visiting nurse).  Those referred readmit at a 15.2% rate; others, 9.2%, but the former are needier, sicker patients.  Conventional thinking is that if we controlled for severity of illness this difference would not only be washed out but would reverse itself.  In other words, holding constant the severity of illness, having HHS should mean a lower RA rate.

With HHS as the sole predictor, B in a logistic regression = 0.6 (N ~ 25k).  B is reduced to 0.2 with a group of covariates controlled, each accounting for some aspect of severity of illness, but B doesn't fall below zero.

HHS alone explains only about 1% of the variance in RA; with the other predictors, this becomes 4%.* Perhaps this is the problem--that the model is not explaining enough variance for the covariates to ""succeed"" in reversing the sign of the coefficient of interest.  If this is true, is there a way to estimate how high explained variance needs to be for such a theorized suppressor effect to show up?


*Using either of 2 pseudo-RSQ formulas; Cox & Snell's or Menard's [-2LL0 - (-2LL1)] / [-2LL0.]",,2013-10-11 18:20:07.563
186491,57513,10594.0,1,,CC BY-SA 3.0,96a35508-4fe5-4957-98d5-5291599c2ef8,"A categorical variable in glm shows signifcance from analysis of deviance, but each level is not significant in z-test",,2013-10-15 09:49:58.953
185841,57321,6162.0,5,,CC BY-SA 3.0,0ef0f224-ad4f-4234-958b-5846b1dceac2,"> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests.

Let me develop this idea here. The model for the individual observations is 
$$y_{ijk}= \mu_i + \alpha_{ij} + \epsilon_{ijk}$$, where :

 - $y_{ijk}$ is the $k$-th measurement of individual $j$ of group $i$ 

 - $\alpha_{ij} \sim_{\text{iid}} {\cal N}(0, \sigma^2_b)$ is the random effect for individual $j$ of group $i$ 

 - $\epsilon_{ijk} \sim_{\text{iid}} {\cal N}(0, \sigma^2_w)$ is the within-error

In [my answer to your first question][1], I have suggested you to note that one obtains a classical (fixed effects) Gaussian linear model for the group means $\bar y_{ij\bullet}$. Indeed you can easily check that $$\bar y_{ij\bullet} = \mu_i + \delta_{ij}$$ with $$\delta_{ij} = \alpha_{ij} + \frac{1}{K}\sum_k \epsilon_{ijk} 
\sim_{\text{iid}} {\cal N}(0, \sigma^2) \quad \text{where } \quad \boxed{\sigma^2=\sigma^2_b+\frac{\sigma^2_w}{K}},$$
assuming $K$ repeated measurements for each individual. This is nothing but the one-way ANOVA model with a fixed factor.

And then I claimed that in order to draw inference about the $\mu_i$ you can simply consider the simple classical linear model whose observations are the group means $\bar y_{ij\bullet}$. I think I spoke too quickly, and **I'd like to know the advice of an expert about this point**. I know it works here, but is it due to the fact that the observed group means $\bar y_{ij\bullet}$ are sufficient statistics for the $\mu_i$ ? (I do not remember the theory of sufficient statistics).

> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests. I would like to use a method where
> the larger the within subject variance the less sure I am about the
> group means or understand why it does not make sense to desire that.

As you see from the boxed formula, the within-variance $\sigma^2_w$ plays a role in the model for the observed group means. 

  [1]: http://stats.stackexchange.com/a/72490/8402",added 5 characters in body,2013-10-11 19:26:55.980
185842,57321,6162.0,5,,CC BY-SA 3.0,6eff36e6-c83c-485e-9ec8-19ca81963fbd,"> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests.

Let me develop this idea here. The model for the individual observations is 
$$y_{ijk}= \mu_i + \alpha_{ij} + \epsilon_{ijk}$$, where :

 - $y_{ijk}$ is the $k$-th measurement of individual $j$ of group $i$ 

 - $\alpha_{ij} \sim_{\text{iid}} {\cal N}(0, \sigma^2_b)$ is the random effect for individual $j$ of group $i$ 

 - $\epsilon_{ijk} \sim_{\text{iid}} {\cal N}(0, \sigma^2_w)$ is the within-error

In [my answer to your first question][1], I have suggested you to note that one obtains a classical (fixed effects) Gaussian linear model for the subjects means $\bar y_{ij\bullet}$. Indeed you can easily check that $$\bar y_{ij\bullet} = \mu_i + \delta_{ij}$$ with $$\delta_{ij} = \alpha_{ij} + \frac{1}{K}\sum_k \epsilon_{ijk} 
\sim_{\text{iid}} {\cal N}(0, \sigma^2) \quad \text{where } \quad \boxed{\sigma^2=\sigma^2_b+\frac{\sigma^2_w}{K}},$$
assuming $K$ repeated measurements for each individual. This is nothing but the one-way ANOVA model with a fixed factor.

And then I claimed that in order to draw inference about the $\mu_i$ you can simply consider the simple classical linear model whose observations are the subjects means $\bar y_{ij\bullet}$. I think I spoke too quickly, and **I'd like to know the advice of an expert about this point**. I know it works here, but is it due to the fact that the observed subjects means $\bar y_{ij\bullet}$ are sufficient statistics for the $\mu_i$ ? (I do not remember the theory of sufficient statistics).

> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests. I would like to use a method where
> the larger the within subject variance the less sure I am about the
> group means or understand why it does not make sense to desire that.

As you see from the boxed formula, the within-variance $\sigma^2_w$ plays a role in the model for the observed group means. 

  [1]: http://stats.stackexchange.com/a/72490/8402",added 5 characters in body,2013-10-11 19:32:39.250
185843,57317,6162.0,6,,CC BY-SA 3.0,d3f09106-aa38-4a26-a36c-58951f89d14c,<r><confidence-interval><mixed-model><jags><error-propagation>,tag: mixed model,2013-10-11 19:33:19.393
185844,57322,10964.0,1,,CC BY-SA 3.0,a069fd69-1082-45df-8e41-9fcab9caf0ed,Explainging p-value to a sophisticated layman,,2013-10-11 19:42:47.813
185846,57322,10964.0,3,,CC BY-SA 3.0,a069fd69-1082-45df-8e41-9fcab9caf0ed,<p-value>,,2013-10-11 19:42:47.813
185845,57322,10964.0,2,,CC BY-SA 3.0,a069fd69-1082-45df-8e41-9fcab9caf0ed,"I Think I understand the concept of p-value but unfortunately I still have to exert a lot of brain cycles to get my arms around it.

I would like to get an explanation of the p-value that is rigorous enough for a sophisticated layman - something that would be intuitive.",,2013-10-11 19:42:47.813
185849,57323,22618.0,3,,CC BY-SA 3.0,06c7f167-fdfe-4f10-87ac-77e31dcad6c8,<data-mining><recommender-system>,,2013-10-11 19:57:04.153
185848,57323,22618.0,1,,CC BY-SA 3.0,06c7f167-fdfe-4f10-87ac-77e31dcad6c8,Simple recommender system - where to start?,,2013-10-11 19:57:04.153
185847,57323,22618.0,2,,CC BY-SA 3.0,06c7f167-fdfe-4f10-87ac-77e31dcad6c8,"Without going into specifics, I'm currently working on a system that involves 20-25 questions being answered as either Green, Yellow, Orange or Red. After completing a subset of these questions (many questions can be left as defaulting to Green), the system allows our users to choose one outcome out of four, roughly corresponding to the answers they entered (OutcomeGreen, OutcomeYellow, OutcomeOrange or OutcomeRed). The answer that was selected most tends to be a good indicator as to what outcome they will select, but that's not always the case.

After having this system in place for the last 2 years, now I've received a request to have the system itself make a recommendation as to which outcome the user should select. Using data already accumulated over this period, I'd like to get some insight as to which questions/answers tend to be most influential for specific outcomes, and possibly give them more weight when determining what to recommend.

My main dilemma is that my last class on statistics was more than 20 years ago, and just looking through the tags here made me feel that I'm out of my depth. With the description I've provided, and the vast knowledge contained within this SE: 

 - Is there anything I should be looking into (tools, subset of
   CrossValidated tags) that would help gain better insight, and where I
   should look for more information?  
 - Is there a quick way to get up-to-speed on what I'm missing?

**Background:** I'm a developer in many programming languages, and an amateur mathematician (mostly playing around in number theory and linear programming). I'm also a quick learner; I've been learning how to use R in my spare time. I just need some indication as to where I would find info quickly that would help me move forward with this.",,2013-10-11 19:57:04.153
185850,57324,5045.0,2,,CC BY-SA 3.0,8056a22e-d2ba-441c-b727-be0df2d7e7c2,"Take a look at the tooth brushing example at the very start of Chapter 14 of Andrew Vicker's book [What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics][1]. It's starts on page 57 or you can use the table of contents button in the bottom right corner to find it. 


  [1]: http://www.pearsonhighered.com/vickers/",,2013-10-11 20:00:54.043
185851,57322,668.0,10,,,23363b32-864a-4213-beb0-a55014c31fcc,"{""OriginalQuestionIds"":[31],""Voters"":[{""Id"":919,""DisplayName"":""whuber""}]}",101,2013-10-11 20:05:32.083
185852,57322,5045.0,6,,CC BY-SA 3.0,04d4c461-4b0d-462c-b669-08585fd4a99b,<p-value><intuition>,edited tags,2013-10-11 20:10:25.547
185854,57322,1895.0,4,,CC BY-SA 3.0,a6f63b15-15e7-4985-9d1b-083036000919,Explaining p-value to a sophisticated layman,edited body; edited title,2013-10-11 20:28:53.333
185853,57322,1895.0,5,,CC BY-SA 3.0,a6f63b15-15e7-4985-9d1b-083036000919,"I think I understand the concept of p-value but unfortunately I still have to exert a lot of brain cycles to get my arms around it.

I would like to get an explanation of the p-value that is rigorous enough for a sophisticated layman - something that would be intuitive.",edited body; edited title,2013-10-11 20:28:53.333
185888,57324,15827.0,5,,CC BY-SA 3.0,6fe1dd84-7aa2-4398-b335-4e134c923c23,"Take a look at the tooth brushing example at the very start of Chapter 14 of Andrew Vickers' book [What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics][1]. It starts on page 57 or you can use the table of contents button in the bottom right corner to find it. 


  [1]: http://www.pearsonhighered.com/vickers/",small fixes to spelling and punctuation,2013-10-11 23:19:18.117
185858,57325,668.0,5,,CC BY-SA 3.0,e7229950-388a-4f5d-bfef-b0f528a007e7,"I am running cross-sectional regressions of the type

$$Y_c = \alpha + \beta X_1 + \gamma X_2 + \delta_1 X_3 + \delta_2 X_1 X_3 + \delta_3 X_2 X_3 + e_c.$$

**My theoretical model** implies that 

 - $\delta_2$ should be negative, 
 - $\delta_3$ should be positive, and
 -  the marginal effect of $X_3$ should be negative. 

**My estimates** imply that 

 - $\widehat\delta_2$ is negative and significant, 
 - $\widehat\delta_3$ is positive and insignificant, 
 - $\widehat\beta$ is significant, and 
 - $\widehat\gamma$ is insignificant. 

Building on this evidence, can I calculate the marginal effect of $X_3$ as $\delta_1 + \delta_2 E(X_1)$ where $E(X_1)$ is the mean of $X_1$, justifying this procedure with the fact that all the terms incorporating $X_2$ are insignificant?
",Formatting and spelling typos.,2013-10-11 20:35:21.577
185861,57326,20742.0,3,,CC BY-SA 3.0,44ad9cfc-3126-44ef-ad5b-d1ebf910bc9c,<machine-learning><classification><clustering><data-mining><weka>,,2013-10-11 20:36:25.720
185860,57326,20742.0,1,,CC BY-SA 3.0,44ad9cfc-3126-44ef-ad5b-d1ebf910bc9c,Sweeping across multiple classifiers and choosing the best?,,2013-10-11 20:36:25.720
185859,57326,20742.0,2,,CC BY-SA 3.0,44ad9cfc-3126-44ef-ad5b-d1ebf910bc9c,"I'm using Weka to perform classification, clustering, and some regression on a few large data sets. I'm currently trying out all the classifiers (decision tree, SVM, naive bayes, etc.).

Is there an automated way (in Weka or other machine learning toolkit) to sweep through all the available classifier algorithms to find the one that produces the best cross-validated accuracy or other metric? I'm not talking about boosting; rather, I'm looking to just choose the best classifier using a given data set.

I'd like to find the best clustering algorithm, too, for my other clustering problem; perhaps finding the lowest sum-of-squared-error?",,2013-10-11 20:36:25.720
185863,57327,22623.0,1,,CC BY-SA 3.0,ee41bc56-0b27-4b12-be8b-5d1ee7e5f1f6,Binary features for prediction,,2013-10-11 20:40:31.053
185862,57327,22623.0,2,,CC BY-SA 3.0,ee41bc56-0b27-4b12-be8b-5d1ee7e5f1f6,"I have a set of relatively long (~1000) binary features with scalar values [0-10] attached to them. My aim is to write a predictor that learns to map the features to the [0-10] interval to predict new features from given a new binary vector. I used SVM and Lasso with leave-one-out performance analysis, but both always end up predicting the mean value of the distribution (correlates to the histogram of all the feature - scalar distribution). The histograms are also rather norm / rayleigh distributions. Suggestions for algorithms / feature space mapping? My main problem is that I am dealing with binary features for the first time.

Thanks, EL",,2013-10-11 20:40:31.053
185864,57327,22623.0,3,,CC BY-SA 3.0,ee41bc56-0b27-4b12-be8b-5d1ee7e5f1f6,<regression><predictive-models><binary-data>,,2013-10-11 20:40:31.053
185867,57328,13396.0,2,,CC BY-SA 3.0,c3398ae7-d890-41fd-bd59-f74c5eb2c099,"Let's say I want to generate $M$ sequences $p_j$, where $j = 1,\ldots,N$.  I want $\mathbb{E}[ p_j ] \to 0$ and $\mathbb{E}[p_j \, p_k] \to \delta_{j, k}$ as $M \to +\infty$, where the expectation is taken across the $M$ samples.

In practice, I can generate an $M \times N$ matrix of i.i.d. unit normals.  For example, in MATLAB, `Z = randn(M, N)`.  Then I get $p_j$ from the $j$-th column of $Z$.

For a finite value of $M$, the sample mean $\mathbb{E}[ p_j ] \neq 0$, but I can ""fix"" the problem if I remove the sample mean by working with $q_j = p_j - \mathbb{E}[ p_j ]$.

My question is -- how do I continue to improve my sequences, so that I get the 2nd-order moments I want, i.e., $\text{corr}(q_j, q_k) = \delta_{j,k}$ even when $M$ is finite?",,2013-10-11 20:40:44.083
185866,57328,13396.0,1,,CC BY-SA 3.0,c3398ae7-d890-41fd-bd59-f74c5eb2c099,Improve the quality of psuedo-randomly generated uncorrelated unit normals,,2013-10-11 20:40:44.083
185865,57328,13396.0,3,,CC BY-SA 3.0,c3398ae7-d890-41fd-bd59-f74c5eb2c099,<normal-distribution>,,2013-10-11 20:40:44.083
185868,57328,1895.0,4,,CC BY-SA 3.0,1592c43f-94a6-4169-a889-3d588dfcf211,Improving the quality of pseudo-randomly generated uncorrelated unit normals,edited title,2013-10-11 20:43:25.953
185871,57329,10135.0,3,,CC BY-SA 3.0,94b8d6e5-7f30-4380-8932-1296c5fa7d62,<time-series><generalized-linear-model><bic><model-evaluation><out-of-sample>,,2013-10-11 20:54:18.747
185869,57329,10135.0,2,,CC BY-SA 3.0,94b8d6e5-7f30-4380-8932-1296c5fa7d62,"I have two statistical models. Model 1 uses a [GLM][1] approach while model 2 uses a time series approach for fitting. I want to compare these two models. 
    
Model 1 (i.e. GLM) has a better out of sample performance. Model 2 has a better [BIC][2] criteria. So based on out of sample performance, I should pick up model 1 and based on BIC I should pick up model 2 as the preferred model.    

I should add that in this context and for the question I am trying to answer, Both the BIC and out of sample performance are important. The question is how to choose the best model in this case? Should I consider other criteria? Please let me know if you know any good reference with similar cases.


  [1]: http://en.wikipedia.org/wiki/Generalized_linear_model
  [2]: http://en.wikipedia.org/wiki/Bayesian_information_criterion",,2013-10-11 20:54:18.747
185870,57329,10135.0,1,,CC BY-SA 3.0,94b8d6e5-7f30-4380-8932-1296c5fa7d62,BIC vs. Out of sample performance,,2013-10-11 20:54:18.747
185872,57328,13396.0,5,,CC BY-SA 3.0,38868be5-f7b3-453a-81d6-4fbb0d884748,"Let's say I want to generate $N$ sequences $p_j$, where $j = 1,\ldots,N$.  Each sequence has a length of $M$.  I want $\mathbb{E}[ p_j ] \to 0$ and $\text{corr}(p_j, p_k) \to \delta_{j, k}$ as $M \to +\infty$.

In practice, I can generate an $M \times N$ matrix of i.i.d. unit normals.  For example, in MATLAB, `Z = randn(M, N)`.  Then I get $p_j$ from the $j$-th column of $Z$.

For a finite value of $M$, the sample mean $\mathbb{E}[ p_j ] \neq 0$, but I can ""fix"" the problem if I remove the sample mean by working with $q_j = p_j - \mathbb{E}[ p_j ]$.

My question is -- how do I continue to improve my sequences, so that I get the 2nd-order moments I want, i.e., $\text{corr}(q_j, q_k) = \delta_{j,k}$ even when $M$ is finite?",added 36 characters in body,2013-10-11 20:58:04.033
185889,57337,2069.0,2,,CC BY-SA 3.0,41016531-0cc7-4adc-adad-25a187f7d96b,"Percents are a proportion. The traditional way to test differences between proportions is the chi-square test. Based on the information you have given me (7106 and 191 [2.62%] in one half and 5944 and 163 [2.67%] in the other), the chi-square test results in a non-significant value of .88 (p value). Your proportions are 2.67 and 2.62, so it is no surprise that these are not statistically significant, despite your large sample. 
",,2013-10-11 23:20:23.397
185892,57338,21746.0,3,,CC BY-SA 3.0,883f30d6-db01-40ea-b3bb-d00768bd54f3,<regression><machine-learning><neural-networks>,,2013-10-11 23:29:41.150
185891,57338,21746.0,1,,CC BY-SA 3.0,883f30d6-db01-40ea-b3bb-d00768bd54f3,Rescaling input features for Neural Networks (Regression),,2013-10-11 23:29:41.150
185895,57324,5045.0,5,,CC BY-SA 3.0,c4072c16-9f0d-41fd-b33d-38b3081b57d6,"Take a look at the tooth brushing example at the very start of Chapter 14 of Andrew Vickers' book [What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics][1]. It starts on page 57 or you can use the table of contents button in the bottom *left* corner to find it. 


  [1]: http://www.pearsonhighered.com/vickers/",added 1 characters in body,2013-10-11 23:55:03.343
185873,57330,22359.0,2,,CC BY-SA 3.0,43c6ab3b-ef95-402d-96ba-6c4fb3280ba0,"How are the data sets related? IF both data sets are drawn from the same distribution (they describe the same problem) than you can use the labeled set as a ""test set"" for the clustering. Basically you treat the clustering algorithm as a classifier. The only problem is that you must find a match between the output of the clustering algorithm and the actual labels. 

You might use some simple matching (ex: instances labeled GREEN are more often clustered in cluster 2 and BLUE in cluster 1 so cluster 1== BLUE and cluster 2 == GREEN).

More elegantly you can compute the [Mutual Information][1] between the clustering output and actual labels. Mutual Information has a nice property, that one doesn't need to know the exact matching. MI will give high scores if most of the matching are consistent. Think of it as a correlation coefficient between (cluster <-> actual label) relation.

Also check http://en.wikipedia.org/wiki/Cluster_analysis for some measures. The key phrase there is:

>  [...] clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by human (experts). Thus, the benchmark sets can be thought of as a gold standard for evaluation. 

For ROC usually one needs some ""*a posteriori*"" probability, outputted by the classifier, but in your case, the distance between the instance and the cluster center will work. Keep in mind that ROC is computed for a specific label at a time (i.e. one vs all). So for 5 labels you will get 4 independent AUROC values.

IMHO I strongly advise yo to do the CV for clustering if you have labeled data! Iterate it several times and use the mean of your measure as the performance. 

I would also try this: Use some percent (66% usually) of unlabeled data to perform clustering, measure performance using labeled data, repeat the experiment with different randomization (usually 5-10 times) and report mean performance. Unfortunately I don't know if this method will give a good estimate of your real performance. Is it possible that will overfit the labeled data set. This is not a textbook approach, so, use it with caution.


  [1]: http://en.wikipedia.org/wiki/Mutual_information ""Mutual Information""",,2013-10-11 21:08:48.940
185876,57331,22763.0,2,,CC BY-SA 3.0,a0d39d0a-882d-427b-8329-31bcb82ba913,"Say I have some normally distributed data. I have an application where I compute the percentile (or cumulative frequency less than sample) for a particular sample using a CDF function along with the mean $\mu$ and standard deviation $\sigma$ of the samples.

so $$F_X(x) = \frac 12\left[1 + \text{erf} \left (\frac {x - \mu}{\sqrt{2 \sigma^2}}\right)\right]$$

Now I find myself in a situation where I want to determine the cumulative frequency of multiple samples across multiple data sets (finding something akin to an overall percentile of, say, three samples). Now assuming the variables are independent, I can sum the normals using 

($\mu$sum, $\sigma$sum) = ($\mu$x + $\mu$y + $\mu$z), ($\sigma$x + $\sigma$y + $\sigma$z).

Can I then sum the individual samples I care about and compare them to the new summed normal to compute a percentile of the three samples compared to the sum of the normals? Something tells me this doesn't work but I'd like to be sure. So I'm thinking something like computing the CDF using the sum of the samples I'm interested in: x = (samplex + sampley + samplez) and using the $\mu$sum and $\sigma$sum in the CDF function above.",,2013-10-11 21:12:06.117
185874,57331,22763.0,1,,CC BY-SA 3.0,a0d39d0a-882d-427b-8329-31bcb82ba913,CDF (cumulative frequency) of multiple samples in summed normals?,,2013-10-11 21:12:06.117
185875,57331,22763.0,3,,CC BY-SA 3.0,a0d39d0a-882d-427b-8329-31bcb82ba913,<probability><normal-distribution><quantiles>,,2013-10-11 21:12:06.117
185877,57288,,25,,,3638e3e8-e5bb-45e6-be39-ee6f62f84044,,http://twitter.com/#!/StackStats/status/388775673198030848,2013-10-11 21:18:38.317
185879,57255,19545.0,4,,CC BY-SA 3.0,75e1a727-7a4e-4178-ad90-78067cc7c088,Why are ERR (Expected Reciprocal Ranking) scores not normalized?,added 30 characters in body; edited title,2013-10-11 21:22:00.880
185878,57255,19545.0,5,,CC BY-SA 3.0,75e1a727-7a4e-4178-ad90-78067cc7c088,"It seems to me that normalized ERR (Expected Reciprocal Ranking) scores (ERR scores of your ranking algorithm divided by ERR score calculated for the ground truth ranking) are more useful than the unscaled ERR scores, but I have not seen normalized scores being reported in the literature. Is there a good reason that the ERR scores are reported in raw rather than normalized format?",added 30 characters in body; edited title,2013-10-11 21:22:00.880
185880,57332,22359.0,2,,CC BY-SA 3.0,48f22cb6-72e4-479c-ac05-17efcfb2afa4,"Features extracted from image/signal processing tend to get correlated a lot! This is not a very bad thing if you have enough samples.

From my experience, a classifier with small variance tend to work well (ex. logistic regression). They have less chances of overfitting the train data.

Another idea that I employed is the Additive logistic regression [here][1] and [here][2] some references. They are already implemented in Weka. They are slower than the logistic models. In the same time they have the great advantage that they perform a feature selection while learning. Moreover, the model is human friendly so you can see what features are more relevant. 

Hope it helps

  [1]: http://people.csail.mit.edu/torralba/courses/6.869/lectures/lecture6/boosting.pdf
  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1016218223",,2013-10-11 21:23:38.743
185881,57255,668.0,5,,CC BY-SA 3.0,47aae167-23d7-46cf-b996-abab0dfb7ee2,"It seems to me that normalized ERR ([Expected Reciprocal Ranking](http://don-metzler.net/papers/metzler-cikm09.pdf)) scores (ERR scores of your ranking algorithm divided by ERR score calculated for the ground truth ranking) are more useful than the unscaled ERR scores, but I have not seen normalized scores being reported in the literature. Is there a good reason that the ERR scores are reported in raw rather than normalized format?",added 52 characters in body,2013-10-11 21:28:11.360
185882,57333,22359.0,2,,CC BY-SA 3.0,6f2dce8a-a958-4911-9239-18a2e9ae1a08,"Try [this][1] paper. Your answer might be at chapter 3.2, figures 2 and 3. 

Long story short: The same performance can be obtained for different pairs of C and kernel parameters. You shouldn't try to manually tune a SVM.

  [1]: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf",,2013-10-11 21:33:36.120
185883,57334,594.0,2,,CC BY-SA 3.0,33626cfa-a0a9-442f-b7f3-5d1e3175bd61,"There are two issues here:

1) If you're doing a formal hypothesis test (and if you're going as far as quoting a p-value in my book you already are), **what is the formal rejection rule?**

When comparing test statistics to critical values, the critical value is *in the rejection region*. While this formality doesn't matter much when everything is continuous, it does matter when the distribution of the test statistic is discrete.

When comparing p-values and significance levels, the rule is:

>           Reject if $p\leq\alpha$

Please note that, even if you rounded your p-value to 0.05, indeed even if the $p$ value was exactly 0.05, formally, *you should still reject*.

2) In terms of 'what is our p-value telling us', then assuming you can even interpret a p-value as 'evidence against the null' (let's say that opinion on that is somewhat divided), 0.0499 and 0.0501 are not really saying different things.


My suggestion would be to (1) formally reject the null, and perhaps point out that even if it were exactly 0.05 it should still be rejected; (2) note that there's nothing particularly *special* about $\alpha = 0.05$ and it's very close to that borderline -- even a slightly smaller significance level would not lead to rejection.
",,2013-10-11 21:33:38.517
185884,57335,13396.0,2,,CC BY-SA 3.0,fedf1925-c675-4c62-b09a-2ef882ddcd69,"I think I got it.  If $Z \sim \mathcal{N}(0, 1)$ but we want to generate $X$ such that its mean is $\mu$ and covariance matrix is $C$, we decompose $C = L L^T$, and let $X = L Z + \mu$.

Now we just need to carry out the reverse operation.",,2013-10-11 21:39:45.703
185887,57336,22624.0,3,,CC BY-SA 3.0,3e33c97c-5173-4d5e-8712-f1829175ea73,<statistical-significance>,,2013-10-11 22:55:22.160
185896,57339,10987.0,2,,CC BY-SA 3.0,da4c4f67-0ec3-462b-8aba-6a8209d99ea3,"Give your large sample sizes, you could probably use a t-test on the means. If your sample sizes are equal, you are in pretty good shape whether you want to use a pooled estimate of the variance or unpooled (Welch's test). Do a one sided test, if you are sure that the population of s1 has a mean at least as large as the mean of the population of s2.

Note: If the variances are much larger than the means, your counts are not Poisson. But what matters here is the distribution of the sample averages, and that should be nearly normal, unless the data are super-skewed. In that case, you could do a non-parametric test like the Kruskal-Wallis.",,2013-10-11 23:59:32.067
185897,57340,10987.0,2,,CC BY-SA 3.0,9e263e0a-bd67-49dc-85c5-a0b5002b2325,"You could try CART (tree) classification regression. That would select a decision tree algorithm for the outcomes based on the answers to the questions. As a bi-product, it would indicate which questions are most important in predicting outcome.  ",,2013-10-12 00:09:09.067
185898,57293,,25,,,33bebd4a-5653-4f9d-b893-f4dcb92ae76f,,http://twitter.com/#!/StackStats/status/388820966710980608,2013-10-12 00:18:37.153
185899,57335,13396.0,5,,CC BY-SA 3.0,11949f92-962b-4509-8d28-8b703f780f10,"I think I got it.  If $Z \sim \mathcal{N}(0, 1)$ but we want to generate $X$ such that its mean is $\mu$ and covariance matrix is $C$, we decompose $C = L L^T$, and let $X = L Z + \mu$.

Now we just need to carry out the reverse operations.",added 1 characters in body,2013-10-12 01:03:46.547
185902,57341,13549.0,3,,CC BY-SA 3.0,170852f7-f141-4b3b-8fe1-685eea592d7f,<multivariate-analysis><autocorrelation><degrees-of-freedom>,,2013-10-12 02:33:38.043
185900,57341,13549.0,2,,CC BY-SA 3.0,170852f7-f141-4b3b-8fe1-685eea592d7f,"I have a data set where samples are collected once per year for 15 years at a number of sites. I am worried that these data are temporally autocorrelated and was trying to figure out if I need to address that. However, the only time I will be using degrees of freedom with these data is in a perMANOVA. This test calculates a pseudo F-statistic by permuting the rows. I can't figure out if the exchangebility assumption means that I don't need to worry about autocorrelation at all (i.e., permuting rows will simply destroy the temporal structure, which I am not interested in anyway) or if it means that I can't use a perMANOVA even if I accounted for autocorrelation?


Since I had a hard time getting a question about MANCOVA answered, I hope explaining how the test works will help someone help me! Here's the paper describing perMANOVA, just in case (it is open access):

http://www.entsoc.org/PDF/MUVE/6_NewMethod_MANOVA1_2.pdf",,2013-10-12 02:33:38.043
185901,57341,13549.0,1,,CC BY-SA 3.0,170852f7-f141-4b3b-8fe1-685eea592d7f,Temporal autocorrelation in perMANOVA?,,2013-10-12 02:33:38.043
185903,57342,19681.0,2,,CC BY-SA 3.0,4a5e614c-d137-437c-86bf-f7154259e521,"You seem to be aware that the marginal effect of $X_3$ is $\delta_1 + \delta_2 X_1 + \delta_3 X_2$, which is just the derivative of the response with respect to $X_3$.  

Replacing $X_1$ with $E(X_1)$ is a reasonable way to summarize the marginal effect.

However, discarding the final term due to statistical insignificance is nonsense.  There are at least two relatively sensible alternatives:

1.  If your $n$ is so big that you believe the statistical result that $\delta_3$ is insignificant more than you believe your prior belief that $\delta_3$ should be positive, than you could get rid of the $\delta_3 X_1 X_2$ term in your model and refit the coefficients BEFORE using $\delta_1 + \delta_2 X_1$ as your marginal effect.

2.  If you believe that the terms involving $X_2$ need to be in the model, regardless of statistical significance, than you need to keep the $X_2$ term in your marginal effect as well.",,2013-10-12 03:15:41.107
185904,57336,,25,,,4b58278f-58c5-4b2a-a187-2d8ea4454936,,http://twitter.com/#!/StackStats/status/388866266150490112,2013-10-12 03:18:37.353
185905,57249,594.0,5,,CC BY-SA 3.0,3adc2d0c-9aa9-4f70-9a8c-24b69c69d5c3,"I have [read](http://en.wikipedia.org/wiki/Gamma_distribution#Summation) that the sum of gamma distributions with the same scale parameter is another gamma distribution. I've also seen the paper by [Moschopoulos](http://www.ism.ac.jp/editsec/aism/pdf/037_3_0541.pdf) describing a method for the summation of a general set of gamma distributions. I have tried implementing Moschopoulos's method but have yet to have success.

What does the summation of a general set of gamma distributions look like? To make this question concrete, what does it look like for:

$\text{Gamma}(3,1) + \text{Gamma}(4,2) + \text{Gamma}(5,1)$

If the parameters above are not particularly revealing, please suggest others.",added 21 characters in body,2013-10-12 05:29:36.093
185906,57343,22629.0,3,,CC BY-SA 3.0,9d3d183e-fd24-4ff2-88b5-aba553850ba6,<k-means>,,2013-10-12 05:40:58.913
185908,57343,22629.0,1,,CC BY-SA 3.0,9d3d183e-fd24-4ff2-88b5-aba553850ba6,K-Means clustering after first iteration,,2013-10-12 05:40:58.913
185907,57343,22629.0,2,,CC BY-SA 3.0,9d3d183e-fd24-4ff2-88b5-aba553850ba6,In the k-means clustering we initially pick k random centroids and assign the given data to one of these k-centroids(which ever is nearest). After this we create new centroids by taking the mean of the assigned points . However there might be case that the initially selected random centroids may not be nearest to any point in the dataset and hence no points would be assigned to these centroids. So in such case what should be done in the step of creating new centroids ?,,2013-10-12 05:40:58.913
185910,57344,21243.0,2,,CC BY-SA 3.0,38d7a905-fbfc-4754-8a22-3e597e425023,"Actually, this isn't by most definitions a recommender system, and anything you read in the literature about recommender systems might be geared toward solving a similar but slightly different problem (namely, where the input and output space are the same set).

This is, by most conventional definitions, a classification problem, so looking for tags related to Classification might help.

In terms of actual approaches: as Placidia mentioned, CART and random forest methods are quite popular right now. Additionally, a classic method is [Logistic Regression][1], which might be worth checking at (I'm not an R user, but I believe R has an implementation of it provided).

More than that is tough to provide without a knowledge of what your system is doing and what approaches might be valid. I'd recommend refreshing your basic probability and statistics and thinking about the way that your variables might be related, then taking a look at the classification methods included with R (and their respective Wikipedia pages).

  [1]: http://en.wikipedia.org/wiki/Logistic_regression",,2013-10-12 06:34:05.823
185923,57348,6136.0,3,,CC BY-SA 3.0,a87dd8a9-d788-473f-a691-552bf51253cc,<information-theory><kullback-leibler>,,2013-10-12 10:23:30.670
185924,57348,6136.0,1,,CC BY-SA 3.0,a87dd8a9-d788-473f-a691-552bf51253cc,KL-divergence between two categorical/multinomial distributions gives negative values?,,2013-10-12 10:23:30.670
186348,57467,594.0,5,,CC BY-SA 3.0,3b0c3bf3-0cb2-4da4-b05e-994ad5e43f47,"I understand that in a linear regression model like:


$y_i = b_0 + b_1  x_i  + \epsilon_i$

I can have a null and an alternative hypothesis:

$H_0: b_1 = 0$ and $H_1: b_1 \neq 0$. 

And then I can reject $H_0$ or fail to reject $H_0$. But what if I want to accept that $b_1 = 0$?",removed convolution operator,2013-10-14 22:06:52.670
185911,57345,436.0,2,,CC BY-SA 3.0,d3c75148-75b5-4786-b081-636589fee8d5,"I am not sure if there is a ""standard"" thing to do in the case one of the initial centroids is completely off. 

You can easily test this by specifying the initial centroids and see how things evolve!

For instance, R will just give you an error.

Say you do:

    # Set the RNG seed to ensure reproducibility
    set.seed(12345)
    
    # Let's create 3 visually distinct clusters
    n <- c(1000, 500, 850)
    classifier.1 <- c(rnorm(n[1], 10, 0.9), 
                      rnorm(n[2], 25, 2),
                      rnorm(n[3], 35, 2))
    classifier.2 <- c(rnorm(n[1], 5, 1),
                      rnorm(n[2], 10, 0.4),
                      rnorm(n[3], 2, .9))
    
    col = c(""blue"", ""darkgreen"", ""darkred"")
    # Run k-means with 3 clusters and random initial centroids 
    # to check the clusters are correctly recognized
    km <- kmeans(cbind(classifier.1, classifier.2), 3)
    # Plot the data, colored by cluster
    plot(classifier.1, classifier.2, pch=20, col=col[km$cluster])

    # Mark the final centroids
    points(km$centers, pch=20, cex=2, col=""orange"")
    
    # Now impose some obviously ""wrong"" starting centroids
    start.x <- c(10, 25, 3000)
    start.y <- c(10, 10, -10000)
    km.2 <- kmeans(cbind(classifier.1, classifier.2), 
                   centers=cbind(start.x, start.y))
    
Now, R has obviously no issue in discriminating the 3 clusters when you let it choose the initial centroids, but when you run it the second time it will just say:

    Error: empty cluster: try a better set of initial centers

I guess that if you are implementing your own algorithm you may choose to use this behaviour or rather give the user a warning and let the algorithm choose the centroids by itself.

Obviously, as others pointed out, there are algorithms such as [k-means++](http://en.wikipedia.org/wiki/K-means%2B%2B) that help in choosing a good set of starting centroids.

Also, in R you can use the `nstart` parameter of the kmeans function to run several iterations with different centroids: this will improve clustering in certain situations.

**EDIT**: also, note from the R `kmeans` help page

> The algorithm of Hartigan and Wong (1979) is used by default. Note
> that some authors use k-means to refer to a specific algorithm rather
> than the general method: most commonly the algorithm given by MacQueen
> (1967) but sometimes that given by Lloyd (1957) and Forgy (1965). The
> Hartigan–Wong algorithm generally does a better job than either of
> those, but trying several random starts (nstart> 1) is often
> recommended. For ease of programmatic exploration, k=1 is allowed,
> notably returning the center and withinss.
> 
> Except for the Lloyd–Forgy method, k clusters will always be returned
> if a number is specified. If an initial matrix of centres is supplied,
> it is possible that no point will be closest to one or more centres,
> which is currently an error for the Hartigan–Wong method.",,2013-10-12 07:23:28.157
185912,57343,594.0,5,,CC BY-SA 3.0,f25bfe3e-6838-45a2-afa2-0fbcf5492c66,"In *k-means clustering* we initially pick $k$ random centroids and assign the given data to one of these $k$ centroids (which ever is nearest). After this we create new centroids by taking the mean of the assigned points. 

However there might be case that the initially selected random centroids may not be nearest to any point in the dataset and hence no points would be assigned to these centroids. So in such case what should be done in the step of creating new centroids?
",formatting etc,2013-10-12 07:33:13.560
185917,57346,21586.0,2,,CC BY-SA 3.0,5ea62487-0fec-449b-999f-b74d65cbd750,"**It lies in the eye of the beholder.**

Formally, if there is a strict decision rule for your problem, follow it. This means $\alpha$ is given. However, I am not aware of any problem where this is the case (though setting $\alpha=0.05$ is what many practitioners do after Statistics101). 

**So it really boils down to what AlefSin commented before. There cannot be a ""correct answer"" to your question. Report what you got, rounded or not.**

There is a huge literature on the ""significance of significance""; see for example the recent paper of one of the leading German statisticians Walter Krämer on ""The cult of statistical significance - What economists should and should not do to make their data talk"", *Schmollers Jahrbuch* **131**, 455-468, 2011.",,2013-10-12 07:43:00.677
185918,57126,,25,,,899d1c94-2ab6-460b-9c2c-13bc1d6a1694,,http://twitter.com/#!/StackStats/status/388956862500323328,2013-10-12 09:18:37.210
185919,57271,20473.0,5,,CC BY-SA 3.0,41e4efec-e97f-47f8-8b3b-4d15bc82507e,"Write your system explicitly for time $t$ as (""$L$"" for ""loss"", as a positive quantity, and ""$G$"" for ""gain"")
$$ A_t - A_{t-1} = - L^A_{t-1} + G_{t-1}^{B\rightarrow A}+G_{t-1}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - L^B_{t-1} + G_{t-1}^{A\rightarrow B}+G_{t-1}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - L^C_{t-1} + G_{t-1}^{A\rightarrow C}+G_{t-1}^{B\rightarrow C}$$

The following three relations hold exactly:
$$  L^A_{t-1} = G_{t-1}^{A\rightarrow B} +  G_{t-1}^{A\rightarrow C} $$
$$  L^B_{t-1} = G_{t-1}^{B\rightarrow A} +  G_{t-1}^{B\rightarrow C} $$
$$  L^C_{t-1} = G_{t-1}^{C\rightarrow A} +  G_{t-1}^{C\rightarrow B} $$

If you substitute in the first three you obtain

$$ A_t - A_{t-1} = - G_{t-1}^{A\rightarrow B} -  G_{t-1}^{A\rightarrow C} + G_{t-1}^{B\rightarrow A}+G_{t-1}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - G_{t-1}^{B\rightarrow A} -  G_{t-1}^{B\rightarrow C} + G_{t-1}^{A\rightarrow B}+G_{t-1}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - G_{t-1}^{C\rightarrow A} -  G_{t-1}^{C\rightarrow B} + G_{t-1}^{A\rightarrow C}+G_{t-1}^{B\rightarrow C}$$

You have $6$ unknown quantities to estimate _per time period_. There is just not enough information to do that. So you need assumptions that will impose structure (=restrictions) on the situation, and will permit you to estimate _something_. What? Let's say you assume that there is a relatively stable ""churn"" from one company to another, as a linear function of their market share in the previous period. This assumption brings in a set of unknown coefficients to be estimated (which will then give you an estimate of ""hidden transfers of market share""). Write $G_{t-1}^{A\rightarrow B} = a_bA_{t-1}$ (market share lost from $A$ to $B$ as a linear function of $A$'s market share in period $t-1$).
Your equations will become

$$ A_t - A_{t-1} = - a_bA_{t-1} -  a_cA_{t-1} + b_aB_{t-1}+c_aC_{t-1} $$

$$ B_t - B_{t-1} = - b_aB_{t-1} -  b_cB_{t-1} + a_bA_{t-1}+c_bC_{t-1}$$

$$ C_t - C_{t-1} = - c_aC_{t-1} -  c_bC_{t-1} + a_cA_{t-1}+ b_cB_{t-1}$$

We have turned a set of mathematical identities into a _model_. It is doubtful that this model will hold exactly for each $t$, so you should add a stochastic error term. Rearranging we obtain a first-order Vector Autoregression (VAR): 

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
1-a_b-a_c  & b_a & c_a \\
a_b & 1-b_a-b_c & c_b \\
a_c & b_c & 1-c_a-c_b \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

or, to homogenize notation,

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
\gamma_{11}  & \gamma_{12} & \gamma_{13} \\
\gamma_{21} & \gamma_{22} & \gamma_{23} \\
\gamma_{31} & \gamma_{32} & \gamma_{33} \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

subject to the equality restrictions
$$  \begin{matrix}
\gamma_{11} +  \gamma_{21} + \gamma_{31} =1 \\
\gamma_{12} +  \gamma_{22} + \gamma_{32} =1  \\
\gamma_{13} +  \gamma_{23} + \gamma_{33} =1  \\
\end{matrix} $$

So you have essentially $6$ unknown coefficients and a sample of $T-1$ observations (for each company).  
Note that these restrictions _imply_ the ""add up to unity"" restriction $A_t+B_t+C_t =1$ for each $t$, so this last one does not impose any additional structure on the unknown coefficients -but it does imply a relation between the error terms, namely that $u^A_{t} + u^B_{t}  +u^C_{t} =0$. Any additional assumptions on the three error terms should either come from knowledge of the specific real world phenomenon under study, and/or through a statistical specification search.

Of course you may find that such a model does not fit your data sample well - for example you expect that all estimated coefficients should be positive and smaller than or equal to unity, but the estimation procedure may not give you that. But this is what we do: we come up with specification hypotheses and we test them against the data - ""success"" is never guaranteed. Then you should try to come up with a different model.

",Elaboration on the relation between the error terms,2013-10-12 09:38:09.133
185920,57347,6162.0,2,,CC BY-SA 3.0,84b56408-cbde-49f3-a93a-947fd590b617,"There is a natural exact confidence interval for the grandmean in the balanced random one-way ANOVA model $$(y_{ij} \mid \mu_i) \sim_{\text{iid}} {\cal N}(\mu_i, \sigma^2_w), \quad j=1,\ldots,J, 
\qquad 
\mu_i \sim_{\text{iid}} {\cal N}(\mu, \sigma^2_b), \quad i=1,\ldots,I.$$
Indeed, it is easy to check that the distribution of the observed means $\bar{y}_{i\bullet}$ is $\bar{y}_{i\bullet} \sim_{\text{iid}} {\cal N}(\mu, \tau^2)$ with $\tau^2=\sigma^2_b+\frac{\sigma^2_w}{J}$, 
and it is well known that the between sum of squares $SS_b$ has distribution $$SS_b \sim J\tau^2\chi^2_{I-1}$$ and is independent of the overall observed mean $$\bar y_{\bullet\bullet} \sim {\cal N}(\mu, \frac{\tau^2}{I})$$. 
Thus $$\frac{\bar y_{\bullet\bullet}  - \mu}{\frac{1}{\sqrt{I}}\sqrt{\frac{SS_b}{J(I-1)}}}$$ has a Student $t$ distribution with $I-1$ degrees of freedom, wherefrom it is easy to get an exact confidence interval about $\mu$.

Note that all of this is simplify equivalent to the classical interval for a Gaussian mean by considering only the group means $\bar{y}_{i\bullet}$ as the observations. 
Thus the simple approach you mention:

> The simple approach is to first compute the mean of each experiment:
> 38.0, 49.3, and 31.7, and then compute the mean, and its 95% confidence interval, of those three values. Using this method, the
> grand mean is 39.7 with the 95% confidence interval ranging from 17.4
> to 61.9.

is right. And your intuition about the ignored variation:

> The problem with that approach is that it totally ignores the
> variation among triplicates. I wonder if there isn't a good way to
> account for that variation.

is wrong. I also mention the correctness of such a simplification in [http://stats.stackexchange.com/a/72578/8402][1]


  [1]: http://stats.stackexchange.com/a/72578/8402",,2013-10-12 10:02:03.747
185922,57347,6162.0,5,,CC BY-SA 3.0,19fbcc15-2c0f-4fb2-858a-c5f2c9aed0c6,"There is a natural exact confidence interval for the grandmean in the balanced random one-way ANOVA model $$(y_{ij} \mid \mu_i) \sim_{\text{iid}} {\cal N}(\mu_i, \sigma^2_w), \quad j=1,\ldots,J, 
\qquad 
\mu_i \sim_{\text{iid}} {\cal N}(\mu, \sigma^2_b), \quad i=1,\ldots,I.$$
Indeed, it is easy to check that the distribution of the observed means $\bar{y}_{i\bullet}$ is $\bar{y}_{i\bullet} \sim_{\text{iid}} {\cal N}(\mu, \tau^2)$ with $\tau^2=\sigma^2_b+\frac{\sigma^2_w}{J}$, 
and it is well known that the between sum of squares $SS_b$ has distribution $$SS_b \sim J\tau^2\chi^2_{I-1}$$ and is independent of the overall observed mean $$\bar y_{\bullet\bullet} \sim {\cal N}(\mu, \frac{\tau^2}{I})$$. 
Thus $$\frac{\bar y_{\bullet\bullet}  - \mu}{\frac{1}{\sqrt{I}}\sqrt{\frac{SS_b}{J(I-1)}}}$$ has a Student $t$ distribution with $I-1$ degrees of freedom, wherefrom it is easy to get an exact confidence interval about $\mu$.

**Note that all of this is simplify equivalent to the classical interval for a Gaussian mean by considering only the group means $\bar{y}_{i\bullet}$ as the observations**. 
Thus the simple approach you mention:

> The simple approach is to first compute the mean of each experiment:
> 38.0, 49.3, and 31.7, and then compute the mean, and its 95% confidence interval, of those three values. Using this method, the
> grand mean is 39.7 with the 95% confidence interval ranging from 17.4
> to 61.9.

is right. And your intuition about the ignored variation:

> The problem with that approach is that it totally ignores the
> variation among triplicates. I wonder if there isn't a good way to
> account for that variation.

is wrong. I also mention the correctness of such a simplification in [http://stats.stackexchange.com/a/72578/8402][1]


  [1]: http://stats.stackexchange.com/a/72578/8402",added 4 characters in body,2013-10-12 10:21:06.487
185925,57348,6136.0,2,,CC BY-SA 3.0,a87dd8a9-d788-473f-a691-552bf51253cc,"If 
 
 P = [0,0.9,0,0.1] 
 
Q = [0,1,0,0] 
 
Then KL(P||Q) = 0 + ln(0.9/1)*0.9 + 0 + 0 = -0.094 
 
This shouldn't be possible from the Gibbs inequality. What am I misunderstanding? ",,2013-10-12 10:23:30.670
185926,57318,18198.0,5,,CC BY-SA 3.0,21b48cf1-99df-4f20-844c-a1a813d0ac2d,"I have seen a method whereby instead of trying to estimate the ridge parameter (k) directly from the data (using one of the many many ridge parameter estimators in the literature) you solve for it iteratively.

The method is simple enough: You simply increase k (in suitably small steps) until the condition number is reduced blow 10.  

At first blush this seems like quite a nice solution to me but I've never seen a Ridge Regression paper/book that uses it. 

Update OK this is basically the method suggested by Marquardt ""Generalized inverses, Ridge Regression, Biased Linear Estimation and Non-linear Estimation"" the only difference being he used VIF's to measure the MC while this method uses the condition number. McDonald and Galrneau ""A Monte-Carlo Evaluation of some Ridge-Type Estimators"" note that this method is may not be appropriate for all data sets as it does not include the y values (observations). I still have not found a paper where the Marquardt method is tested against other estimators for the ridge parameter does anybody know of such a paper?  

Is this method theoretically sound though? Even if (as I suspect) it isn't does it really matter for the average practitioner who just want to produce more stable estimates of their Beta's (the weights in the regression) rather than having them ""blow up"" to grossly unrealistic values when they experience severe MC?

Truly I would like to find a better method than this ideally with a solid theoretical underpinning but its hard to see from a practical view point it can be improved upon?
",added 612 characters in body,2013-10-12 10:35:46.497
185929,57349,22630.0,3,,CC BY-SA 3.0,936245dc-cf30-48ef-bfc7-f8716be94863,<standard-deviation><neural-networks><mean><image-processing>,,2013-10-12 10:43:14.610
185928,57349,22630.0,1,,CC BY-SA 3.0,936245dc-cf30-48ef-bfc7-f8716be94863,"What does saying ""Mean value of each pixel over all images""?",,2013-10-12 10:43:14.610
185927,57349,22630.0,2,,CC BY-SA 3.0,936245dc-cf30-48ef-bfc7-f8716be94863,"I was reading a paper related to Auto encoders for my project work. It is required to input images as vectors to the neural network. I couldn't understand a certain sentence due to lack of knowledge of statistics (I guess). I googled, but the problem is I don't know what it is exactly and searching the same phrase returns the same kind of documents but not their explanation.

 

> We train on 1.6 million 32*32 color images that have been preprocessed
> by subtracting from each pixel its mean value over all images and then
> dividing by the standard deviation of all pixels over all images.


What does it mean by ""subtracting from each pixel its mean value over all images and then
dividing by the standard deviation of all pixels over all images"".

I would request you to explain in laymen terms since I'm new to all this.",,2013-10-12 10:43:14.610
185930,57321,6162.0,5,,CC BY-SA 3.0,e0dc1c31-7271-42ba-bb17-ee2016f27340,"> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests.

Let me develop this idea here. The model for the individual observations is 
$$y_{ijk}= \mu_i + \alpha_{ij} + \epsilon_{ijk}$$, where :

 - $y_{ijk}$ is the $k$-th measurement of individual $j$ of group $i$ 

 - $\alpha_{ij} \sim_{\text{iid}} {\cal N}(0, \sigma^2_b)$ is the random effect for individual $j$ of group $i$ 

 - $\epsilon_{ijk} \sim_{\text{iid}} {\cal N}(0, \sigma^2_w)$ is the within-error

In [my answer to your first question][1], I have suggested you to note that one obtains a classical (fixed effects) Gaussian linear model for the subjects means $\bar y_{ij\bullet}$. Indeed you can easily check that $$\bar y_{ij\bullet} = \mu_i + \delta_{ij}$$ with $$\delta_{ij} = \alpha_{ij} + \frac{1}{K}\sum_k \epsilon_{ijk} 
\sim_{\text{iid}} {\cal N}(0, \sigma^2) \quad \text{where } \quad \boxed{\sigma^2=\sigma^2_b+\frac{\sigma^2_w}{K}},$$
assuming $K$ repeated measurements for each individual. This is nothing but the one-way ANOVA model with a fixed factor.

And then I claimed that in order to draw inference about the $\mu_i$ you can simply consider the simple classical linear model whose observations are the subjects means $\bar y_{ij\bullet}$. I think I spoke too quickly, and **I'd like to know the advice of an expert about this point**. I know it works here, but is it due to the fact that the observed subjects means $\bar y_{ij\bullet}$ are sufficient statistics for the $\mu_i$ ? (I do not remember the theory of sufficient statistics).

> In the answers there (if I understood correctly) I learned that
> within-subject variance does not effect inferences made about group
> means and it is ok to simply take the averages of averages to
> calculate group mean, then calculate within-group variance and use
> that to perform significance tests. I would like to use a method where
> the larger the within subject variance the less sure I am about the
> group means or understand why it does not make sense to desire that.

As you see from the boxed formula, the within-variance $\sigma^2_w$ plays a role in the model for the observed group means. 

## Update 

About my request in **bold**, I have now thought about it and there's no need of sufficient statistics or something like that. The principle is very general. Assume a model for a sample $y=(y_i)$ and consider the ""submodel"" for the sample $(f_i(y))$ for given functions $f_i$. If $\mu$ is a parameter appearing in both models, then a confidence interval about $\mu$ with respect to the second (sub)model obviously is a confidence interval about $\mu$ with respect to the first model.

  [1]: http://stats.stackexchange.com/a/72490/8402",added 498 characters in body,2013-10-12 10:45:10.757
185933,57350,11772.0,1,,CC BY-SA 3.0,5f21cc50-a414-48a6-98ea-c446e2215d99,Subscript notation in expectations,,2013-10-12 11:04:38.997
185932,57350,11772.0,3,,CC BY-SA 3.0,5f21cc50-a414-48a6-98ea-c446e2215d99,<conditional-expectation><notation>,,2013-10-12 11:04:38.997
185931,57350,11772.0,2,,CC BY-SA 3.0,5f21cc50-a414-48a6-98ea-c446e2215d99,"What is the exact meaning of the subscript notation $\mathbb{E}_X[f(X)]$ in conditional expectations in the framework of measure theory ? These subscripts do not appear in the definition of conditional expectation, but we may see for example in [this page of wikipedia][1]. (Note that it wasn't always the case, [the same page][2] few months ago).

What should be for example the meaning of $\mathbb{E}_X[X+Y]$ with $Y=X+1$ ?

  [1]: http://en.wikipedia.org/wiki/Law_of_total_expectation
  [2]: http://en.wikipedia.org/w/index.php?title=Law_of_total_expectation&oldid=548089336",,2013-10-12 11:04:38.997
185934,57349,22630.0,5,,CC BY-SA 3.0,08cb3963-9706-4bc2-b2c8-b0a9b6cffc02,"I was reading a paper related to Auto encoders for my project work. It is required to input images as vectors to the neural network. I couldn't understand a certain sentence due to lack of knowledge of statistics (I guess). I googled, but the problem is I don't know what it is exactly and searching the same phrase returns the same kind of documents but not their explanation.

 

> We train on 1.6 million 32*32 color images that have been preprocessed
> by subtracting from each pixel its mean value over all images and then
> dividing by the standard deviation of all pixels over all images.


What does it mean by ""subtracting from each pixel its mean value over all images and then
dividing by the standard deviation of all pixels over all images"".

I would request you to explain in laymen terms since I'm new to all this.",edited title,2013-10-12 11:17:00.883
185935,57349,22630.0,4,,CC BY-SA 3.0,08cb3963-9706-4bc2-b2c8-b0a9b6cffc02,"What does saying ""Standard deviation of all pixels over all images""?",edited title,2013-10-12 11:17:00.883
185937,57349,15827.0,4,,CC BY-SA 3.0,a83b77be-fa60-47a6-a773-b180c33c4f95,"What does ""Standard deviation of all pixels over all images"" mean?",small fixes,2013-10-12 11:21:03.220
185936,57349,15827.0,5,,CC BY-SA 3.0,a83b77be-fa60-47a6-a773-b180c33c4f95,"I was reading a paper related to Auto encoders for my project work. It is required to input images as vectors to the neural network. I couldn't understand a certain sentence due to lack of knowledge of statistics (I guess). I Googled, but the problem is I don't know what it is exactly and searching the same phrase returns the same kind of documents but not their explanation.

 

> We train on 1.6 million 32*32 color images that have been preprocessed
> by subtracting from each pixel its mean value over all images and then
> dividing by the standard deviation of all pixels over all images.


What does it mean by ""subtracting from each pixel its mean value over all images and then
dividing by the standard deviation of all pixels over all images"".

I would request you to explain in lay terms since I'm new to all this.",small fixes,2013-10-12 11:21:03.220
185939,57349,22630.0,5,,CC BY-SA 3.0,72b6caf7-36bf-4b95-acb6-e41fc8c61312,"I was reading a paper related to Auto encoders for my project work. It is required to input images as vectors to the neural network. I couldn't understand a certain sentence due to lack of knowledge of statistics (I guess). I Googled, but the problem is I don't know what it is exactly and searching the same phrase returns the same kind of documents but not their explanation.

 Source: http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf

> We train on 1.6 million 32*32 color images that have been preprocessed
> by subtracting from each pixel its mean value over all images and then
> dividing by the standard deviation of all pixels over all images.


What does it mean by ""subtracting from each pixel its mean value over all images and then
dividing by the standard deviation of all pixels over all images"".

My interpretation is: ""Subtracting from each pixel its mean value over all images""
   It means, for a pixel position in an image, subtract the average of values of that pixel position over all images and subtract from the current pixel value.


Am I correct? 

It is somewhat ambiguous to me.

Please explain in some math terms.
",Added more details,2013-10-12 11:51:10.477
185941,57351,20473.0,2,,CC BY-SA 3.0,67c03e84-b311-41e8-8a42-e33bca29b457,"In an expression where more than one random variables are involved, the symbol $E$ alone does not clarify _with respect to which random variable_ is the expected value ""taken"". For example

$$E[h(X,Y)] =? \int_{-\infty}^{\infty} h(X,Y) f_X(x)dx$$
or
$$E[h(X,Y)] =? \int_{-\infty}^{\infty} h(X,Y) f_Y(y)dx$$

*Neither*. When many random variables are involved, and there is no subscript in the $E$ symbol, the expected value is taken with respect to their joint distribution:

$$E[h(X,Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h(x,y) f_{XY}(x,y)dxdy$$


When a subscript is present... in some cases it tells us *on which variable we should condition*. So

$$E_X[(h(X,Y)] = E(h(X,Y)\mid X) = \int_{-\infty}^{\infty} h(x,y) f_{h(X,Y)|X}(h(x,y)\mid x)dh  $$

...But in other cases, it tells us which density to use for the ""averaging""

$$E_X[(h(X,Y)] = \int_{-\infty}^{\infty} h(x,y) f_{X}(x)dx $$ 



Rather confusing I would say, but who said that scientific terminology is totally free of ambiguity? You should look how each author defines the use of such symbols.",,2013-10-12 11:56:27.427
185942,57352,15827.0,2,,CC BY-SA 3.0,9e7812a9-b0c2-418b-863c-7e7c24333d0d,"Each image is composed of 32 $\times$ 32 pixels, so for a given pixel (say row 13, column 31) something measured is averaged over all the images, and the standard deviation (SD for short) for the same something is also calculated. 

(value − mean) / SD is often called a z-score and is a way of standardizing values to take account of mean and SD. Presumably that's done for every pixel, meaning every pixel position. 

It is spelled out that they are ""dividing by the standard deviation of _all_ pixels over _all_ images"" [my emphasis] and that SD would usually be calculated with reference to the corresponding overall mean. However, division by that SD would be dividing by a constant, so it won't have any effect on the images beyond a question of units.",,2013-10-12 12:16:10.913
185943,57350,,25,,,579e4a45-e902-4c7f-a4e5-6a325bfec545,,http://twitter.com/#!/StackStats/status/389002160866091008,2013-10-12 12:18:37.187
185944,57353,5875.0,2,,CC BY-SA 3.0,95872aca-e303-4cfe-8014-23d44d9fb68f,"Let’s remove the two categories with probability $0$ in both distributions. Your example is $P = (0.9, 0.1)$ and $Q = (1,0)$. 

The KL divergence is $KL(P||Q) = \sum_i p_i \log\left( {p_i \over q_i }\right)$. It is not 
$$ 0.9 \times \log\, 0.9 + 0 $$
but 
$$ 0.9 \times \log\, 0.9 + 0.1 \times ( +\infty ) = + \infty.$$
",,2013-10-12 12:19:43.223
185945,57319,1693.0,5,,CC BY-SA 3.0,f0f5db8d-5108-4371-9776-b2c303417941,"I am modeling an outcome for hospital patients, 'RA' (whether readmitted).  My predictor of interest is 'HHS' (whether referred to Home Health Services such as from a visiting nurse).  Those referred readmit at a 15.2% rate; others, 9.2%, but the former are needier, sicker patients.  Conventional thinking is that if we controlled for severity of illness this difference would not only be washed out but would reverse itself.  In other words, holding constant the severity of illness, having HHS should mean a lower RA rate.

With HHS as the sole predictor, B in a logistic regression = 0.6 (N ~ 25k).  B is reduced to 0.2 with a group of covariates controlled, each accounting for some aspect of severity of illness, but B doesn't fall below zero.

HHS alone explains only about 1% of the variance in RA; with the other predictors, this becomes 4%.* Perhaps this is the problem--that the model is not explaining enough variance for the covariates to ""succeed"" in reversing the sign of the coefficient of interest.  If this is true, is there a way to estimate how high explained variance needs to be for such a reversal to show up?


----------

*Using either of 2 pseudo-RSQ formulas; Cox & Snell's or Menard's [-2LL0 - (-2LL1)] / [-2LL0.]",responding to ttnphns's comment,2013-10-12 12:45:05.293
185946,57319,1693.0,4,,CC BY-SA 3.0,f0f5db8d-5108-4371-9776-b2c303417941,How high must RSQ be for a suppressor/reversal effect to show up?,responding to ttnphns's comment,2013-10-12 12:45:05.293
185947,57351,20473.0,5,,CC BY-SA 3.0,0dbe36e0-de54-4dfa-909e-0a7c738f0f6b,"In an expression where more than one random variables are involved, the symbol $E$ alone does not clarify _with respect to which random variable_ is the expected value ""taken"". For example

$$E[h(X,Y)] =? \int_{-\infty}^{\infty} h(x,y) f_X(x)dx$$
or
$$E[h(X,Y)] =? \int_{-\infty}^{\infty} h(x,y) f_Y(y)dx$$

*Neither*. When many random variables are involved, and there is no subscript in the $E$ symbol, the expected value is taken with respect to their joint distribution:

$$E[h(X,Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h(x,y) f_{XY}(x,y)dxdy$$


When a subscript is present... in some cases it tells us *on which variable we should condition*. So

$$E_X[(h(X,Y)] = E(h(X,Y)\mid X) = \int_{-\infty}^{\infty} h(x,y) f_{h(X,Y)|X}(h(x,y)\mid x)dh  $$

...But in other cases, it tells us which density to use for the ""averaging""

$$E_X[(h(X,Y)] = \int_{-\infty}^{\infty} h(x,y) f_{X}(x)dx $$ 



Rather confusing I would say, but who said that scientific terminology is totally free of ambiguity? You should look how each author defines the use of such symbols.",edited body,2013-10-12 13:52:57.420
185950,57354,8671.0,3,,CC BY-SA 3.0,f9231b6f-f3f0-4ad8-a8d9-dbbfedb24550,<machine-learning><statistical-significance><computational-statistics><mutual-information>,,2013-10-12 13:58:25.187
185948,57354,8671.0,2,,CC BY-SA 3.0,f9231b6f-f3f0-4ad8-a8d9-dbbfedb24550,"I have a problem when computing the mutual information between two variables.
Lets consider the following table:

.................k=1 .... k=2.....k=3

c = 1.........10.......20.........5

c = 2..........5 ........3.........20

I want to calculate the mutual information between the class (c) and the cluster (k).

for e.g. for MI(c=1, k=1), I will calculate the P(c,k) as follows:

..............k=1 ...... k~=1

c=1 ........10/63 ........ 25/63

c~=1 ...... 5/63 ....... 23/63

and I can calculate the MI based on the following:
$MI(c,k) = \sum_{c\in\{0,1\}} \sum_{k\in\{0,1\}} p(c,k)log_2 \frac{p(c,k)}{p(c)p(k)}$ 

In this case: I am getting the same MI for (c=1, k=1) and (c=2, k=1). what is the reason for it? How can I calculate the MI for each (c,k) based on the first table?",,2013-10-12 13:58:25.187
185949,57354,8671.0,1,,CC BY-SA 3.0,f9231b6f-f3f0-4ad8-a8d9-dbbfedb24550,mutual information calculations example,,2013-10-12 13:58:25.187
185952,57355,20927.0,2,,CC BY-SA 3.0,65f12dac-758c-453a-a31d-c32136036024,"I want to test mean differences between 4(0,1,2,3) groups for 6 different cell-types. In other words, I would have 6 comparisons to do for each cell-type (0-1,0-2,0-3,1-2,1-3,2-3). I have run a repeated-measures ANOVA before but it only gives me the main effects and interactions. To correct my t-tests for multiple comparisons, I would use Bonferroni, does SPSS let me do that, I just cannot seem to find it except as part of the means-testing using ANOVA? Or do I have to do it manually and how would i do that? alpha-level here is 0.05.Thanks",,2013-10-12 14:02:41.167
185955,57356,6630.0,2,,CC BY-SA 3.0,8dcb9dec-0a31-4b8a-9d40-4ef8a4020ad5,"MI is between two random variables. In this case, $c$ and $k$ are random variables.

If you take $c=1$, it isn't really random anymore, and MI should be zero. Likewise for $k=1$.

(Also, one more thing to keep in mind is that your 'plug-in' estimation of MI is biased.)",,2013-10-12 14:05:53.523
185956,57271,20473.0,5,,CC BY-SA 3.0,8a36d51e-7879-49c2-8e4c-69e65e20c633,"Write your system explicitly for time $t$ as (""$L$"" for ""loss"", as a positive quantity, and ""$G$"" for ""gain"")
$$ A_t - A_{t-1} = - L^A_{t} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - L^B_{t} + G_{t-1}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - L^C_{t} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$

The following three relations hold exactly:
$$  L^A_{t} = G_{t}^{A\rightarrow B} +  G_{t}^{A\rightarrow C} $$
$$  L^B_{t} = G_{t}^{B\rightarrow A} +  G_{t}^{B\rightarrow C} $$
$$  L^C_{t} = G_{t}^{C\rightarrow A} +  G_{t}^{C\rightarrow B} $$

If you substitute in the first three you obtain

$$ A_t - A_{t-1} = - G_{t}^{A\rightarrow B} -  G_{t}^{A\rightarrow C} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$

$$ B_t - B_{t-1} = - G_{t}^{B\rightarrow A} -  G_{t}^{B\rightarrow C} + G_{t}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$

$$ C_t - C_{t-1} = - G_{t}^{C\rightarrow A} -  G_{t}^{C\rightarrow B} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$

You have $6$ unknown quantities to estimate _per time period_. There is just not enough information to do that. So you need assumptions that will impose structure (=restrictions) on the situation, and will permit you to estimate _something_. What? Let's say you assume that there is a relatively stable ""churn"" from one company to another, as a linear function of their market share in the previous period. This assumption brings in a set of unknown coefficients to be estimated (which will then give you an estimate of ""hidden transfers of market share""). Write $G_{t}^{A\rightarrow B} = a_bA_{t-1}$ (market share lost from $A$ to $B$ as a linear function of $A$'s market share in period $t-1$).
Your equations will become

$$ A_t - A_{t-1} = - a_bA_{t-1} -  a_cA_{t-1} + b_aB_{t-1}+c_aC_{t-1} $$

$$ B_t - B_{t-1} = - b_aB_{t-1} -  b_cB_{t-1} + a_bA_{t-1}+c_bC_{t-1}$$

$$ C_t - C_{t-1} = - c_aC_{t-1} -  c_bC_{t-1} + a_cA_{t-1}+ b_cB_{t-1}$$

We have turned a set of mathematical identities into a _model_. It is doubtful that this model will hold exactly for each $t$, so you should add a stochastic error term. Rearranging we obtain a first-order Vector Autoregression (VAR): 

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
1-a_b-a_c  & b_a & c_a \\
a_b & 1-b_a-b_c & c_b \\
a_c & b_c & 1-c_a-c_b \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

or, to homogenize notation,

$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
\gamma_{11}  & \gamma_{12} & \gamma_{13} \\
\gamma_{21} & \gamma_{22} & \gamma_{23} \\
\gamma_{31} & \gamma_{32} & \gamma_{33} \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$

subject to the equality restrictions
$$  \begin{matrix}
\gamma_{11} +  \gamma_{21} + \gamma_{31} =1 \\
\gamma_{12} +  \gamma_{22} + \gamma_{32} =1  \\
\gamma_{13} +  \gamma_{23} + \gamma_{33} =1  \\
\end{matrix} $$

So you have essentially $6$ unknown coefficients and a sample of $T-1$ observations (for each company).  
Note that these restrictions _imply_ the ""add up to unity"" restriction $A_t+B_t+C_t =1$ for each $t$, so this last one does not impose any additional structure on the unknown coefficients -but it does imply a relation between the error terms, namely that $u^A_{t} + u^B_{t}  +u^C_{t} =0$. Any additional assumptions on the three error terms should either come from knowledge of the specific real world phenomenon under study, and/or through a statistical specification search.

Then, an estimation for a hidden transfer of market share will be, for example

$$\hat G_{t}^{A\rightarrow B} = \hat \gamma_{21}A_{t-1}$$

etc.

Of course you may find that such a model does not fit your data sample well - for example you expect that all estimated coefficients should be positive and smaller than or equal to unity, but the estimation procedure may not give you that. But this is what we do: we come up with specification hypotheses and we test them against the data - ""success"" is never guaranteed. Then you should try to come up with a different model.

",deleted 13 characters in body,2013-10-12 14:29:18.637
185958,57355,503.0,10,,,2abda8a7-a69a-4ebe-8616-f147c08d4c4e,"{""OriginalQuestionIds"":[60383],""Voters"":[{""Id"":7290,""DisplayName"":""gung""},{""Id"":686,""DisplayName"":""Peter Flom""}]}",101,2013-10-12 15:21:34.280
185959,57357,14850.0,2,,CC BY-SA 3.0,48e3494a-ab05-4a35-9002-cd279513dc71,"A dependent mixture model (hidden markov model) may be of use, depending on the type of deviations expected. 

Assume that your observations come from two distributions (or states), both of which are normally distributed, but have different mean and variance.

A number of parameters can be estimated: The initial state probabilities (2 parameters), the state transition probabilities between neighbouring data points (4 parameters) and finally the mean and variance of the two distributions (4 parameters).

In R, this model can be estimated using the depmixS4 package:

    library(depmixS4)

    set.seed(3)
    y = rnorm(100)
    y[30:35] <- rnorm(6,mean=4,sd=2)
    plot(1:100,y,""l"")

    m <- depmix(y~1,nstates=2,ntimes=100)
    fm <- fit(m)

    means <- getpars(fm)[c(7,9)]
    lines(1:100,means[fm@posterior$state],lwd=2,col=2)

![enter image description here][1]


See http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf for references


  [1]: https://i.stack.imgur.com/HTvOv.png",,2013-10-12 15:57:43.367
185960,57354,8671.0,6,,CC BY-SA 3.0,69e6a847-3f03-4d88-bbae-399f78e71fcc,<machine-learning><clustering><computational-statistics><mutual-information>,edited tags,2013-10-12 16:19:50.917
185961,57357,15827.0,5,,CC BY-SA 3.0,31b24a99-9d3e-4030-97b8-d97c41108eb4,"A dependent mixture model (hidden Markov model) may be of use, depending on the type of deviations expected. 

Assume that your observations come from two distributions (or states), both of which are normally distributed, but have different mean and variance.

A number of parameters can be estimated: The initial state probabilities (2 parameters), the state transition probabilities between neighbouring data points (4 parameters) and finally the mean and variance of the two distributions (4 parameters).

In R, this model can be estimated using the depmixS4 package:

    library(depmixS4)

    set.seed(3)
    y = rnorm(100)
    y[30:35] <- rnorm(6,mean=4,sd=2)
    plot(1:100,y,""l"")

    m <- depmix(y~1,nstates=2,ntimes=100)
    fm <- fit(m)

    means <- getpars(fm)[c(7,9)]
    lines(1:100,means[fm@posterior$state],lwd=2,col=2)

![enter image description here][1]


See http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf for references


  [1]: https://i.stack.imgur.com/HTvOv.png",edited body,2013-10-12 16:32:41.913
185962,57356,6630.0,5,,CC BY-SA 3.0,7a7c24a9-15cc-4bd5-9fa3-e9dcee737fd4,"MI is between two random variables. In this case, $c$ and $k$ are random variables.

If you take $c=1$, it isn't really random anymore, and MI should be zero. Likewise for $k=1$.

(Also, one more thing to keep in mind is that your 'plug-in' estimation of MI is biased.)

EDIT: MI between $I(c=1)$ vs $I(k=1)$ where $I$ is the indicator function, is a different matter. Your double usage of the same variable confused me.

Now your $c$ only take 2 values, therefore, $I(c=1) = 1 - I(c=2)$. MI is invariant invertible transformation of variables, that's why $MI(I(c=1),I(k=1)) = MI(I(c=2),I(k=1))$.",added 331 characters in body,2013-10-12 16:35:12.603
185965,57358,22637.0,3,,CC BY-SA 3.0,87dc4342-01ae-4a49-82d3-e6c76f3b2d67,<probability>,,2013-10-12 16:48:18.213
185964,57358,22637.0,1,,CC BY-SA 3.0,87dc4342-01ae-4a49-82d3-e6c76f3b2d67,Transformations(CDF technique),,2013-10-12 16:48:18.213
185963,57358,22637.0,2,,CC BY-SA 3.0,87dc4342-01ae-4a49-82d3-e6c76f3b2d67,"Consider the following short example of transformations.

Let the joint density of X and Y be given by the unit square, i.e.

f_{X,Y}(x,y) = \begin{cases} 1\ 0<x<1\ \text{and}\ 0<y<1 \\ \text{elsewhere} \end{cases}

Then the Cumulative Distribution Function of **Z=X+Y** is given by:

F_Z = \begin{cases}0\ \text{for}\ z<0 \\ \int_0^{z} \int_0^{z-x} dydx\ \text{for}\ 0\leq z <1 \\1-\int_{z-1}^1 \int_{z-x}^1 dydx\ \text{for}\ 1\leq z<2 \\1\ \text{for}\ 2\leq{z} \end{cases}

I understand why we have to partition our CDF, what I am having trouble figuring out is why for the interval [1,2) that specific form. What is the intuition here? Thanks.",,2013-10-12 16:48:18.213
185968,57359,346.0,3,,CC BY-SA 3.0,d12c5a00-1e64-4a4a-950b-9ce915331de5,<centering><quadratic-form>,,2013-10-12 17:07:43.177
185967,57359,346.0,1,,CC BY-SA 3.0,d12c5a00-1e64-4a4a-950b-9ce915331de5,How to include a linear and quadratic term when also including interaction with those variables?,,2013-10-12 17:07:43.177
185966,57359,346.0,2,,CC BY-SA 3.0,d12c5a00-1e64-4a4a-950b-9ce915331de5,"When adding a numeric predictor with categorical predictors and their interactions, it is usually considered necessary to center the variables at 0 beforehand. The reasoning is that the main effects are otherwise hard to interpret as they are evaluated with the numeric predictor at 0.

My question now is how to center if one not only includes the original numeric variable (as a linear term) but also the quadratic term of this variable? Here, two different approaches are necessary:

 1. **Centering both variables at their individual mean.** This has the unfortunate downside that the 0 now is at a different position for both variables considering the original variable.
 2. **Centering both variables at the mean of the original variable** (i.e., subtracting the mean from the original variable for the linear term and subtracting the square of the mean of the original variable from the quadratic term). With this approach the 0 would represent the same value of the original variable, but the quadratic variable would not be centered at 0 (i.e., the mean of the variable wouldn't be 0).

I think that approach 2 seems reasonable given the reason for centering after all.  However, I cannot find anything about it (also not in the related questions: [a][1] and [b][2]).

Or is it generally a bad idea to include linear and quadratic terms and their interactions with other variables in a model?


  [1]: http://stats.stackexchange.com/q/67512/442
  [2]: http://stats.stackexchange.com/q/47178/442",,2013-10-12 17:07:43.177
185971,57360,22381.0,1,,CC BY-SA 3.0,29909650-ddbb-40ba-8c5f-b88f39c774a0,Does applying ARMA-GARCH require stationarity?,,2013-10-12 17:14:19.543
186442,57496,12314.0,3,,CC BY-SA 3.0,02a56597-a917-40a1-bfe8-276fce6f92ec,<predictive-models><forecasting><autocorrelation><ordinal-data>,,2013-10-15 06:05:55.207
185970,57360,22381.0,2,,CC BY-SA 3.0,29909650-ddbb-40ba-8c5f-b88f39c774a0,"I am going to use the ARMA-GARCH model for financial time series and was wondering whether the series should be stationary before applying the said model. 
I know to apply ARMA model the series should be stationary, however i'm not sure for ARMA-GARCH since i'm including GARCH errors which imply volatility clustering and non-constant variance and hence non-stationary series no matter what transformation I do.

Are financial time series usually stationary or non-stationary?
I tried applying ADF test to a few volatile series and got p-value<0.01 which seems to indicate stationarity but the principle of volatile series itself tells us that the series isn't stationary.

Can somebody clear that up for me?I'm getting really confused",,2013-10-12 17:14:19.543
185969,57360,22381.0,3,,CC BY-SA 3.0,29909650-ddbb-40ba-8c5f-b88f39c774a0,<finance><arma>,,2013-10-12 17:14:19.543
185973,57360,1406.0,6,,CC BY-SA 3.0,14d9566d-828d-43fe-8d5d-11ea1380f5dd,<time-series><finance><arma>,edited body; edited tags,2013-10-12 17:42:16.337
185972,57360,1406.0,5,,CC BY-SA 3.0,14d9566d-828d-43fe-8d5d-11ea1380f5dd,"I am going to use the ARMA-GARCH model for financial time series and was wondering whether the series should be stationary before applying the said model. 
I know to apply ARMA model the series should be stationary, however I'm not sure for ARMA-GARCH since I'm including GARCH errors which imply volatility clustering and non-constant variance and hence non-stationary series no matter what transformation I do.

Are financial time series usually stationary or non-stationary?
I tried applying ADF test to a few volatile series and got p-value<0.01 which seems to indicate stationarity but the principle of volatile series itself tells us that the series isn't stationary.

Can somebody clear that up for me?I'm getting really confused",edited body; edited tags,2013-10-12 17:42:16.337
185974,57361,20473.0,2,,CC BY-SA 3.0,369a04f4-82cc-49fa-82bb-8b0c0150d175,"Copying from the abstract of [Engle's original paper][1]:  
""These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance"".


  [1]: http://www.jstor.org/stable/1912773",,2013-10-12 17:42:59.303
185975,57362,1406.0,2,,CC BY-SA 3.0,28da99a9-2af5-4033-ac26-15815377fb86,"Yes the the series should be stationary. GARCH models are actually white noise processes with not trivial dependence structure. Classical GARCH(1,1) model is defined as

$$r_t=\sigma_t\varepsilon_t,$$

with 

$$\sigma_t^2=\alpha_0+\alpha_1\varepsilon_{t-1}^2+\beta_1\sigma_{t-1}^2,$$

where $\varepsilon_t$ are independent standard normal variables with unit variance.

Then

$$Er_t=EE(r_t|\varepsilon_{t-1},\varepsilon_{t-2},...)=E\sigma_tE(\varepsilon_t|\varepsilon_{t-1},\varepsilon_{t-2},...)=0$$

and

$$Er_tr_{t-h}=EE(r_tr_{t-h}|\varepsilon_{t-1},\varepsilon_{t-2},...)=Er_{t-h}\sigma_{t}E(\varepsilon_t|\varepsilon_{t-1},\varepsilon_{t-2},...)=0$$

for $h>0$. Hence $r_t$ is a white noise process. However it is possible to show that $r_t^2$ is actually a $ARMA(1,1)$ process. So GARCH(1,1) is stationary process, yet has non-constant conditional variance.
",,2013-10-12 17:56:44.900
185976,57361,20473.0,5,,CC BY-SA 3.0,697b5fff-ed99-4197-a405-82017ef701c2,"Copying from the abstract of [Engle's original paper][1]:  
""These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance"".

Continuing with the references, as the author who introduced GARCH shows (Bollerslev, Tim (1986). ""[Generalized Autoregressive Conditional Heteroskedasticity][2]"", Journal of Econometrics, 31:307-327)
for the GARCH(1,1) process, it suffices that $\alpha_1 + \beta_1 <1$ for 2nd-order stationarity.


  [1]: http://www.jstor.org/stable/1912773
  [2]: http://www.sciencedirect.com/science/article/pii/0304407686900631#",added 377 characters in body,2013-10-12 18:01:48.457
185979,57363,22639.0,1,,CC BY-SA 3.0,d9ae5e17-ced4-42c2-8ce2-70ffd3d9fde7,Machine learning algorithms/approaches for class recommendations?,,2013-10-12 18:03:28.603
185978,57363,22639.0,2,,CC BY-SA 3.0,d9ae5e17-ced4-42c2-8ce2-70ffd3d9fde7,"I am asking a theoretical question about machine learning in terms of clustering. Is it possible, given a set of data of classes that students have taken in a semester to recommend additional classes that students should take if they selected some classes?

I am thinking along the line of forming clusters of classes and figuring out if a particular set of picked classes match with a pre-existing set of classes. Then, recommend the class that are in the set. But I am new to machine learning, and so welcome any other suggestions of algorithms.

In addition, this is not particularly theoretical, so feel free to ignore: but does anyone know any particular software that can accomplish this? I know LensKit is a software to handle recommendations but it seems to need ratings (which I do not have).

I welcome any mathematical manipulations that can turn clusters into ""ratings."" Thanks.",,2013-10-12 18:03:28.603
185977,57363,22639.0,3,,CC BY-SA 3.0,d9ae5e17-ced4-42c2-8ce2-70ffd3d9fde7,<machine-learning><clustering><algorithms><artificial-intelligence>,,2013-10-12 18:03:28.603
185980,57351,20473.0,5,,CC BY-SA 3.0,2d3aca9e-4928-4815-8891-cf8b3e4662eb,"In an expression where more than one random variables are involved, the symbol $E$ alone does not clarify _with respect to which random variable_ is the expected value ""taken"". For example

$$E[h(X,Y)] =? \int_{-\infty}^{\infty} h(x,y) f_X(x)dx$$
or
$$E[h(X,Y)] =? \int_{-\infty}^{\infty} h(x,y) f_Y(y)dx$$

*Neither*. When many random variables are involved, and there is no subscript in the $E$ symbol, the expected value is taken with respect to their joint distribution:

$$E[h(X,Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h(x,y) f_{XY}(x,y)dxdy$$


When a subscript is present... in some cases it tells us *on which variable we should condition*. So

$$E_X[(h(X,Y)] = E(h(X,Y)\mid X) = \int_{-\infty}^{\infty} h(x,y) f_{h(X,Y)|X}(h(x,y)\mid x)dh  $$

...But in other cases, it tells us which density to use for the ""averaging""

$$E_X[(h(X,Y)] = \int_{-\infty}^{\infty} h(x,y) f_{X}(x)dx $$ 



Rather confusing I would say, but who said that scientific notation is totally free of ambiguity or multiple use? You should look how each author defines the use of such symbols.",added 13 characters in body,2013-10-12 18:03:55.497
185981,57361,20473.0,5,,CC BY-SA 3.0,438c108c-b984-4ffb-affb-0d2971fb4f06,"Copying from the abstract of [Engle's original paper][1]:  
""These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance"".

Continuing with the references, as the author who introduced GARCH shows (Bollerslev, Tim (1986). ""[Generalized Autoregressive Conditional Heteroskedasticity][2]"", Journal of Econometrics, 31:307-327)
for the GARCH(1,1) process, it suffices that $\alpha_1 + \beta_1 <1$ for 2nd-order stationarity.

Stationarity (the one needed for estimation procedures), is defined relative to the _unconditional_ distribution and moments.


  [1]: http://www.jstor.org/stable/1912773
  [2]: http://www.sciencedirect.com/science/article/pii/0304407686900631#",added 377 characters in body,2013-10-12 18:07:20.230
185982,57364,10135.0,2,,CC BY-SA 3.0,ca91867f-5b21-43b4-b6f2-bf2a8d26c460,"OK, check out the following plot. ![enter image description here][1]
    
You need to find the area of shaded region. So you need to take double integration over the shaded region. First you fix your $X$ and take your integration with respect to $Y$. Look at the double bar in the middle of that triangle (upper right corner). Its lower part goes from $Y=-X+z$ to its upper part $Y=1$. These are the bounds for the first integration. Now you need to move that little bar in the middle to left and right to cover all the shaded region. In other words, it means that this time, you need to take your integration with respect to $X$. So as you can see, the line $Y=-X+z$ for $1\leq z<2$ intercepts the line $Y=1$ at $X=z-1$. This is the left boundary limit for your integration. Now move that little bar to the right, it should go up to $X=1$, that gives you the upper bound. Hope that helps.

  [1]: https://i.stack.imgur.com/OfF8D.jpg",,2013-10-12 18:10:09.217
185983,57212,,25,,,ba7eeb65-fdcc-4303-8892-f1d10b5eb194,,http://twitter.com/#!/StackStats/status/389092757832491008,2013-10-12 18:18:37.167
185984,57365,22640.0,2,,CC BY-SA 3.0,449888d5-8c6a-4a70-8453-f89009bb79bb,"We have a response variable $Y$ and predictor $X$, and we draw $n$ samples $(Y_1,X_1), \ldots, (Y_n, X_n)$ from the population of interest to do a regression analysis. Under the assumptions of a simple linear regression model, my question is a conceptual one: how do we really think about the response on the $i$th unit, $Y_i$? Do we say it's drawn from the level or subpopulation of individuals with $ X = x_i $, or from the aggregate population over all the values of $X$? Moreover, while we assume that the response $Y$ in every subpopulation defined by $X$ is normal with equal variances, how do we think about the aggregate population from which $Y_i$ is drawn from?  ",,2013-10-12 18:36:45.027
185986,57365,22640.0,3,,CC BY-SA 3.0,449888d5-8c6a-4a70-8453-f89009bb79bb,<regression>,,2013-10-12 18:36:45.027
185985,57365,22640.0,1,,CC BY-SA 3.0,449888d5-8c6a-4a70-8453-f89009bb79bb,"Drawing data from ""population"" for regression analysis",,2013-10-12 18:36:45.027
185988,57354,674.0,5,,CC BY-SA 3.0,73870fc2-e681-446d-8328-f794a4d67f1c,"I have a problem when computing the mutual information between two variables.
Let's consider the following table:

    .................k=1 .... k=2.....k=3    
    c = 1.........10.......20.........5    
    c = 2..........5 ........3.........20

I want to calculate the mutual information between the class (c) and the cluster (k).

E.g., for MI(c=1, k=1), I will calculate the P(c,k) as follows:

    ..............k=1 ...... k~=1    
    c=1 ........10/63 ........ 25/63    
    c~=1 ...... 5/63 ....... 23/63

and I can calculate the MI based on the following:
$MI(c,k) = \sum_{c\in\{0,1\}} \sum_{k\in\{0,1\}} p(c,k)log_2 \frac{p(c,k)}{p(c)p(k)}$ 

In this case: I am getting the same MI for (c=1, k=1) and (c=2, k=1). What is the reason for this? How can I calculate the MI for each (c,k) based on the first table?

",added 36 characters in body; edited title,2013-10-12 18:37:57.323
185987,57354,674.0,4,,CC BY-SA 3.0,73870fc2-e681-446d-8328-f794a4d67f1c,Computing mutual information,added 36 characters in body; edited title,2013-10-12 18:37:57.323
185989,57366,306.0,2,,CC BY-SA 3.0,65a4ab91-1449-4132-8426-01b75e01e43f,"Stationarity is a theoretical concept which is then modified to other forms like Weak Sense Stationarity which can be tested easily. Most of the tests like adf test as you have mentioned test for linear conditions only. the ARCH effects are made for series which do not have autocorrelation in the first order but there is dependence in the squared series. 

The ARMA-GARCH process you talk about, here the second order dependence is removed using the GARCH part and then any dependence in the linear terms is captured by the ARMA process. 

The way to go about is to check for the autocorrelation of the squared series, if there is dependence, then apply the GARCH models and check the residuals for any linear time series properties which can then be modelled using ARMA processes.",,2013-10-12 18:41:16.300
185991,57367,22641.0,3,,CC BY-SA 3.0,1d66db1f-3979-4a43-8aa0-311c2700bc62,<hypothesis-testing>,,2013-10-12 18:47:43.087
185992,57367,22641.0,1,,CC BY-SA 3.0,1d66db1f-3979-4a43-8aa0-311c2700bc62,"Test for differences between (among) related, but not matched, samples",,2013-10-12 18:47:43.087
185990,57367,22641.0,2,,CC BY-SA 3.0,1d66db1f-3979-4a43-8aa0-311c2700bc62,"When two samples are related, or dependent, but the observations are not matched, are there any tests that will determine if the samples (means or otherwise) are different? I've searched extensively and have only found tests for matched samples, which is not what I need.",,2013-10-12 18:47:43.087
185995,57368,21840.0,1,,CC BY-SA 3.0,fef043d1-3990-4e4f-a790-427b00612d5c,Compute probability,,2013-10-12 19:10:33.277
185994,57368,21840.0,3,,CC BY-SA 3.0,fef043d1-3990-4e4f-a790-427b00612d5c,<density-function><joint-distribution>,,2013-10-12 19:10:33.277
185993,57368,21840.0,2,,CC BY-SA 3.0,fef043d1-3990-4e4f-a790-427b00612d5c,"Suppose that X, Y and Z are i.i.d.∼ Uniform(0,1). Let $t > 0$ be a ﬁxed constant.
(i) Compute $P(XY \leq t)$
(ii) Compute $ P(XY \leq t)$
(iii) Compute $ P(XY/Z \leq t)$ 

I found the solution for (i) part undermining the different values of $t$.
$ P(XY \leq t) = \int_0^1\int_0^{yt}dx dy  = t/2 $ ; when $t\leq 1$

I am finding hard time to lake limits for different values of $t$ i.e when $t\leq 1 $ and $t>1$
 Thanks!",,2013-10-12 19:10:33.277
185996,57368,21840.0,5,,CC BY-SA 3.0,bc8a8687-8449-42de-9506-53d4b1b9b3b2,"Suppose that X, Y and Z are i.i.d.∼ Uniform(0,1). Let $t > 0$ be a ﬁxed constant.
(i) Compute $P(X/Y \leq t)$
(ii) Compute $ P(XY \leq t)$
(iii) Compute $ P(XY/Z \leq t)$ 

I found the solution for (i) part undermining the different values of $t$.
$ P(X/Y \leq t) = \int_0^1\int_0^{yt}dx dy  = t/2 $ ; when $t\leq 1$

I am finding hard time to lake limits for different values of $t$ i.e when $t\leq 1 $ and $t>1$
 Thanks!",corrected question,2013-10-12 19:21:00.273
185997,57369,22507.0,2,,CC BY-SA 3.0,762352f5-4845-4539-909c-941efdf021f9,"Clustering is seldom, if any, used for recommendations, since it is too crude.  The most common techniques used are:

- matrix factorization; read, for example, ""Matrix Factorization Techniques for Recommender Systems"" by Koren, Bell, and Volinsky.  If you use R, there is are packages NMFN and gnmf for non-negative matrix factorization.  In your case, this will be the matrix of 0's and 1's.  There are many modifications and versions of this technique.

- KNN. For each class, find classes highly correlative with it. Then predict the probability for this class as a linear regression (or, in your case, logistic regression) of the correlative classes, with relaxation.
- Restricted Boltzmann Machines. This is relatively hard to understand or implement.  Read, for example, ""Restricted Boltzmann Machines for Collaborative Filtering"" by Salakhutdinov, Mnih, and Hinton.  There are no Restricted Boltzmann Machine packages on R.",,2013-10-12 19:31:39.603
185998,57370,22507.0,2,,CC BY-SA 3.0,6527c729-ca31-4d3c-a5be-64ad2608fed0,"I recommend ""The Elements of Statistical Learning"", by Hastie, Tibshirani, and Friedman. Don't just read it, play with some algorithms described by them (most of them are implemented in R, or you could even implement some yourself), and learn their weak and strong points.",,2013-10-12 19:39:43.643
185999,57369,22507.0,5,,CC BY-SA 3.0,9e8cc3c0-5f25-4f43-a1f7-0fd1c1f60e9c,"Clustering is seldom, if ever, used for recommendations, since it is too crude.  The most common techniques used are:

- matrix factorization; read, for example, ""Matrix Factorization Techniques for Recommender Systems"" by Koren, Bell, and Volinsky.  If you use R, there is are packages NMFN and gnmf for non-negative matrix factorization.  In your case, this will be the matrix of 0's and 1's.  There are many modifications and versions of this technique.
- KNN. For each class, find classes highly correlative with it. Then predict the probability for this class as a linear regression (or, in your case, logistic regression) of the correlative classes, with relaxation.
- Restricted Boltzmann Machines. This is relatively hard to understand or implement.  Read, for example, ""Restricted Boltzmann Machines for Collaborative Filtering"" by Salakhutdinov, Mnih, and Hinton.  There are no Restricted Boltzmann Machine packages on R.
- Often, a combination of different approaches (blending) is used, providing better results than each one separately. For example, Netflix uses a blending of Matrix Factorization and Restricted Boltzmann Machines.",added 221 characters in body,2013-10-12 19:45:19.737
186000,57371,22507.0,2,,CC BY-SA 3.0,f2a59895-51d9-48d2-b813-28ffb4f9de02,"Calculate a correlation of two functions over a set of random examples. The two-sided Kolmogorov-Smirnov test compares one-dimensional distributions, not multidimensional functions.",,2013-10-12 20:15:29.000
186001,57372,21947.0,2,,CC BY-SA 3.0,083d1d96-46d4-4db7-966f-c3a2987662e3,"The answer is **absolutely not**. There is no ""in the eye of the beholder"", there is no argument, the answer is **no, your data is not significant at the $p=0.05$ level**.  (Ok, there is one way out, but its a very narrow path.)

The key problem is this phrase: ""We **came across** some data..."". 

This suggests that you looked at several other statistical hypothesis, and rejected them because they did not reach your significance level. You found one hypothesis that (barely) met your standard, and you are wondering whether it is significant. Unless your $p$ value accounts for such multiple hypothesis testing, it is overly optimistic.  Given that you are just three decimal points away from your threshold, considering even *one* additional hypothesis would surely push $p$ over the line.  

There is a name for this sort of statistical malfeasance: [data dredging][1].  I'm ambivalent about reporting it in the paper as an interesting hypothesis; does it have some physical reason you expect it to hold?

**There is, however, one way out.** Perhaps you decided *a priori* to perform just this *one* test on just this *one* data set. You wrote that down in your lab notebook, in front of someone so that you could prove it later. Then you did your test. 

If you did this, then your result is valid at the $p=0.05$ level, and you can back it up to skeptics like me. Otherwise, sorry, it is not a statistically significant result.


  [1]: https://en.wikipedia.org/wiki/Data_dredging",,2013-10-12 20:45:05.873
186003,57236,,25,,,05fbbf93-72aa-4c57-a648-fdcbf5598db8,,http://twitter.com/#!/StackStats/status/389138447711354880,2013-10-12 21:20:10.563
186004,57358,594.0,5,,CC BY-SA 3.0,8750ffda-8d60-4e3f-9baa-f816cfb1712e,"Consider the following short example of transformations.

Let the joint density of X and Y be given by the unit square, i.e.

$$f_{X,Y}(x,y) = \begin{cases} 1\ 0<x<1\ \text{ and }\ 0<y<1 \\ \text{elsewhere} \end{cases}$$

Then the Cumulative Distribution Function of $Z=X+Y$ is given by:

$$
F_Z = \begin{cases}0\ \text{ for }\ z<0 \\ \int_0^{z} \int_0^{z-x} dy\,dx\ \text{ for }\ 0\leq z <1 \\1-\int_{z-1}^1 \int_{z-x}^1 dy\,dx\ \text{for}\ 1\leq z<2 \\1\ \text{ for }\ 2\leq{z} \end{cases}
$$

I understand why we have to partition our CDF, what I am having trouble figuring out is why for the interval $[1,2)$ that specific form. What is the intuition here? Thanks.",added 10 characters in body,2013-10-12 22:31:39.933
186005,57358,594.0,5,,CC BY-SA 3.0,63a4a681-21bb-4d65-9b59-5b23b24d9b21,"Consider the following short example of transformations.

Let the joint density of X and Y be given by the unit square, i.e.

$$f_{X,Y}(x,y) = \begin{cases} 1\ 0<x<1\ \text{ and }\ 0<y<1 \\ \text{elsewhere} \end{cases}$$

Then the Cumulative Distribution Function of $Z=X+Y$ is given by:

$$
F_Z = \begin{cases}\begin{array}{ll} 0\ & \text{ for }\ z<0 \\ \int_0^{z} \int_0^{z-x} dy\,dx\ & \text{ for }\ 0\leq z <1 \\1-\int_{z-1}^1 \int_{z-x}^1 dy\,dx\ & \text{ for }\ 1\leq z<2 \\1\ & \text{ for }\ 2\leq{z} \end{array}\end{cases}
$$

I understand why we have to partition our CDF, what I am having trouble figuring out is why for the interval $[1,2)$ that specific form. What is the intuition here? Thanks.",formatting,2013-10-12 22:39:40.437
186007,56684,594.0,6,,CC BY-SA 3.0,a81169c8-3d86-4d53-a948-3475fdd8200e,<hypothesis-testing><normal-distribution><bernoulli-distribution>,edited tags,2013-10-12 22:59:22.727
186009,57358,594.0,5,,CC BY-SA 3.0,b98cd876-2088-4b96-944d-af30ce185fae,"Consider the following short example of transformations.

Let the joint density of X and Y be given by the unit square, i.e.

$$f_{X,Y}(x,y) = \begin{cases} 1\ \quad 0<x<1\ \text{ and }\ 0<y<1 \\ 0 \quad \text{elsewhere} \end{cases}$$

Then the Cumulative Distribution Function of $Z=X+Y$ is given by:

$$
F_Z = \begin{cases}\begin{array}{ll} 0\ & \text{ for }\ z<0 \\  \int_0^{z} \int_0^{z-x} dy\,dx\ & \text{ for }\ 0\leq z <1 \\1-\int_{z-1}^1 \int_{z-x}^1 dy\,dx\ & \text{ for }\ 1\leq z<2 \\1\ & \text{ for }\ 2\leq{z} \end{array}\end{cases}
$$

I understand why we have to partition our CDF, what I am having trouble figuring out is why for the interval $[1,2)$ that specific form. What is the intuition here? Thanks.",formatting,2013-10-12 23:05:55.717
186011,57368,594.0,5,,CC BY-SA 3.0,56b24923-d6b1-431a-9f0a-0c730453fdcc,"Suppose that $X$, $Y$ and $Z$ are $\text{i.i.d.} \sim \text{Uniform}(0,1)$. Let $t > 0$ be a ﬁxed constant.

(i) Compute $P(X/Y \leq t)$  
(ii) Compute $ P(XY \leq t)$  
(iii) Compute $ P(XY/Z \leq t)$   

I found the solution for (i) part undermining the different values of $t$.  
$ P(X/Y \leq t) = \int_0^1\int_0^{yt}dx dy  = t/2 $ ; when $t\leq 1$

I am finding hard time to lake limits for different values of $t$ i.e when $t\leq 1 $ and $t>1$

",formatting,2013-10-12 23:14:21.973
186012,57367,594.0,6,,CC BY-SA 3.0,f2ea2cf0-2a8e-4c65-9306-797a174eaed6,<hypothesis-testing><paired-data>,edited tags,2013-10-12 23:17:27.737
186013,57367,594.0,6,,CC BY-SA 3.0,cbfa9d53-1499-4e46-84b2-362858ecba21,<hypothesis-testing>,edited tags,2013-10-13 00:13:24.583
186014,57237,,25,,,e5f76412-bdfc-4413-9f5d-252228f36dd6,,http://twitter.com/#!/StackStats/status/389183745468952576,2013-10-13 00:20:10.317
186036,57378,5001.0,2,,CC BY-SA 3.0,0bcada00-63ca-43a9-b7f8-5f8e34dde4cc,"I am trying to interpret one of the p-values in a one variable linear regression.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is `y = 0.514x + 0.00087` and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?",,2013-10-13 08:09:51.043
186016,57354,594.0,5,,CC BY-SA 3.0,6b9c8c48-15d5-4e7e-a4c5-fe6ebeba05ce,"I have a problem when computing the mutual information between two variables.
Let's consider the following table:

                 k=1       k=2       k=3    
    c = 1         10        20         5    
    c = 2          5         3        20

I want to calculate the mutual information between the class ($c$) and the cluster ($k$).

E.g., for $\text{MI}(c=1, k=1)$, I will calculate the $P(c,k)$ as follows:

                 k=1            k~=1    
    c=1        10/63           25/63    
    c~=1        5/63           23/63

and I can calculate the MI based on the following:

$\text{MI}(c,k) = \sum_{c\in\{0,1\}} \sum_{k\in\{0,1\}} p(c,k)log_2 \frac{p(c,k)}{p(c)p(k)}$ 

In this case: I am getting the same MI for $(c=1, k=1)$ and $(c=2, k=1)$. What is the reason for this? How can I calculate the MI for each $(c,k)$ based on the first table?

",added 34 characters in body,2013-10-13 01:56:26.553
186017,57251,,25,,,fee0bf89-cd47-4af0-bb23-7fd45d290003,,http://twitter.com/#!/StackStats/status/389229042077286400,2013-10-13 03:20:09.907
186020,57373,17459.0,3,,CC BY-SA 3.0,e2e784d5-b1b7-4edd-94fe-6ee0a142b610,<covariance><density-function><covariance-matrix>,,2013-10-13 03:46:30.167
186018,57373,17459.0,2,,CC BY-SA 3.0,e2e784d5-b1b7-4edd-94fe-6ee0a142b610,"I am stuck by a problem and wonder if anyone can give me some suggestions?
x1, x2, x3 all follow uniform [0,1] distribution, and subject to constraint that x1+x2+x3<=1.
what's the joint distribution for p(x1,x2,x3) ?  and what's the variance-covariance matrix for it ?

I get the joint distribution by geometrical way, that the pdf should be 1/6.
However, I can't calculate the variance-covariance matrix for it. I wonder how to get it ?

Thank you very much.
",,2013-10-13 03:46:30.167
186019,57373,17459.0,1,,CC BY-SA 3.0,e2e784d5-b1b7-4edd-94fe-6ee0a142b610,what's the pdf and covariance for this distribution?,,2013-10-13 03:46:30.167
186021,57373,594.0,5,,CC BY-SA 3.0,872a0d51-6505-4bb6-aa26-01c3aa9a2928,"I am stuck on a problem and wonder if anyone can give me some suggestions.

$X_1, X_2, X_3$ all follow a $\text{Uniform}[0,1]$ distribution and are subject to the constraint $X_1+X_2+X_3\leq 1$.

What's the joint distribution for $(X_1, X_2, X_3)$, that is, what's $p(X_1, X_2, X_3)$, and what's the variance-covariance matrix for it?

I get the joint distribution by geometrical way, that the pdf should be $1/6$.

However, I can't calculate the variance-covariance matrix for it. I wonder how to get it?
",formatting,2013-10-13 04:17:33.090
186026,57374,17123.0,2,,CC BY-SA 3.0,99ca1f82-0607-4eff-aa38-aa1af3f190e7,"Suppose there is a population, with goods and bads. The bad rate of the population(=bads/(bads+goods)) is of course unknown. Now, I have a sample of N from the population and I know the bad rate of this sample as b. The question is can I calculate the confidence interval based on N and b ONLY? In other words, can I calculate the confidence interval x such that with, say, 95% confidence the population bad rate falls in [range1,range2], where range1 will be b-x and range2 will be b+x.

Thank you.",,2013-10-13 04:36:36.017
186024,57374,17123.0,3,,CC BY-SA 3.0,99ca1f82-0607-4eff-aa38-aa1af3f190e7,<confidence-interval><estimation>,,2013-10-13 04:36:36.017
186025,57374,17123.0,1,,CC BY-SA 3.0,99ca1f82-0607-4eff-aa38-aa1af3f190e7,How to estimate the confidence interval using sample average and sample size ONLY?,,2013-10-13 04:36:36.017
186029,57359,,25,,,547ab796-736a-4d15-aae3-53ee81672086,,http://twitter.com/#!/StackStats/status/389274345891434496,2013-10-13 06:20:11.157
186030,57374,594.0,5,,CC BY-SA 3.0,b6f7b306-b4c0-4c80-a9b5-da020de0a758,"Suppose there is a population, with goods and bads. The bad rate of the population(=bads/(bads+goods)) is of course unknown. 

Now, I have a sample of $N$ from the population and I know the bad rate of this sample as $b$. The question is can I calculate the confidence interval based on $N$ and $b$ ONLY? In other words, can I calculate the confidence interval $x$ such that with, say, 95% confidence the population bad rate falls in $[\text{range}_1,\text{range}_2]$, where $\text{range}_1$ will be $b-x$ and $\text{range}_2$ will be $b+x$.

",added 46 characters in body,2013-10-13 06:26:53.203
186031,57375,,2,anon,CC BY-SA 3.0,7e1773e8-6d58-496a-9401-44bac6c0d313,"Exactly as Glen_b said. Under random sampling the confidence interval for a binomial proportion can be easily calculated with, e.g., using the normal approximation. The formula can be found from Wikipedia, among other sources (http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).

As an example, the 95% confidence interval for a sample of 1000 with the proportion of bad of 0.5, the confidence interval would be from 0.5-sqrt((1/1000)*0.5*0.5)*1.96 to 0.5+sqrt((1/1000)*0.5*0.5)*1.96. In other words, in this case the 95% confidence interval would be 0.469-0.530.



",,2013-10-13 06:29:18.963
186032,57376,19455.0,2,,CC BY-SA 3.0,e90faa02-1c8b-4e11-922c-51408132aa10,"I would suggest a different approach. Instead of sweeping across all possible classifiers,
stop and think about your problem. How does your feature space look like? For the case of binary classification, are there two large clusters with some boundary, or is your feature space ""segmented"" and contains many clusters? 

In the former case, an SVM would be a good choice to separate the two clusters (with the right choice of kernel), in the latter a decision tree which splits the feature space into areas would probably be a better choice. Another issue is interpretability, do you need some sort of report or methodology for classification, or simply a prediction result? Decision tree can provide you with a methodology you can follow, enabling you to debug and check if you are overfitting. From my personal experience, understanding your dataset is at least as important as the choice of algorithm.

",,2013-10-13 07:01:11.173
186034,57377,12756.0,1,,CC BY-SA 3.0,294c3ee3-bbd6-4ff3-880e-f19958acb9b4,Usage of Linear optimization model,,2013-10-13 07:04:53.763
186035,57377,12756.0,2,,CC BY-SA 3.0,294c3ee3-bbd6-4ff3-880e-f19958acb9b4,"Gonna need some support.

Young Energy operates a power plant. The power plant is a coal-fired boiler that produces steam which in turn drives a generator. The company can buy different types of coal, and then mix them to meet the demands placed on it which is fired in the boiler. The table shows the characteristics of the different types of coal are:

![enter image description here][1]


The requirement to be burned in the pan is: 

 - BTU/lb: 11900,   
 - content of the ashes max 12,2% and 
 - max moisture 9,4%.

  [1]: https://i.stack.imgur.com/Jg23l.png

> How should I implement a linear optimization model in this context?",,2013-10-13 07:04:53.763
186033,57377,12756.0,3,,CC BY-SA 3.0,294c3ee3-bbd6-4ff3-880e-f19958acb9b4,<self-study><optimization><model>,,2013-10-13 07:04:53.763
186038,57378,5001.0,1,,CC BY-SA 3.0,0bcada00-63ca-43a9-b7f8-5f8e34dde4cc,Interpretation of the p-value of the y-intercept coefficient in a linear regression,,2013-10-13 08:09:51.043
186037,57378,5001.0,3,,CC BY-SA 3.0,0bcada00-63ca-43a9-b7f8-5f8e34dde4cc,<regression><p-value>,,2013-10-13 08:09:51.043
186041,57379,22646.0,3,,CC BY-SA 3.0,d3df0456-f538-4b70-a9b8-965b946b6be4,<regression>,,2013-10-13 08:15:40.013
186039,57379,22646.0,2,,CC BY-SA 3.0,d3df0456-f538-4b70-a9b8-965b946b6be4,"Hi I am new to R and statistis and usd to linear models. Can you please explain the output? I used it to make a growth curve.
thanks


Formula: length ~ a * (1 - exp(-c * est_age))

Parameters:
   Estimate Std. Error t value Pr(>|t|)    
a 1.097e+03  1.026e+01 106.966  < 2e-16 ***
c 1.539e-01  1.982e-02   7.765 2.33e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 41.74 on 38 degrees of freedom

Number of iterations to convergence: 6 
Achieved convergence tolerance: 7.32e-07 ",,2013-10-13 08:15:40.013
186040,57379,22646.0,1,,CC BY-SA 3.0,d3df0456-f538-4b70-a9b8-965b946b6be4,standard error of the residuals for a non-linear model,,2013-10-13 08:15:40.013
186042,57378,5001.0,5,,CC BY-SA 3.0,9d0a3ba1-4b8c-4faa-b597-089e0d9c276c,"I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if and what way faults are found in it.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is `y = 0.514x + 0.00087` and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?

Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as 

> ""at least as extreme as 0.00087 in the same direction, that is,
> positive"".",added 185 characters in body,2013-10-13 08:16:04.243
186043,57378,5001.0,5,,CC BY-SA 3.0,249f4689-fe24-4cb9-9336-d45132e5a3b4,"I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if faults are found within it.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is `y = 0.514x + 0.00087` and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?

Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as 

> ""at least as extreme as 0.00087 in the same direction, that is,
> positive"".",added 3 characters in body,2013-10-13 08:21:39.643
186045,57379,594.0,5,,CC BY-SA 3.0,482e4d28-861d-4c72-a10e-a4ea65bb33df,"Hi I am new to R and statistics and used to linear models. Can you please explain the output? I used it to make a growth curve.


    Formula: length ~ a * (1 - exp(-c * est_age))
    
    Parameters:
       Estimate Std. Error t value Pr(>|t|)    
    a 1.097e+03  1.026e+01 106.966  < 2e-16 ***
    c 1.539e-01  1.982e-02   7.765 2.33e-09 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

    Residual standard error: 41.74 on 38 degrees of freedom
    Number of iterations to convergence: 6 
    Achieved convergence tolerance: 7.32e-07 

",added 44 characters in body,2013-10-13 09:23:36.447
186046,57379,594.0,6,,CC BY-SA 3.0,482e4d28-861d-4c72-a10e-a4ea65bb33df,<nonlinear-regression>,added 44 characters in body,2013-10-13 09:23:36.447
186047,57185,,5,user10619,CC BY-SA 3.0,837be1ea-06e8-4188-973d-5176ca508e5d,There is some confusion with respect to the measurement error. What is the definition in statistics and definition in psychometry ?  The statistics does not seem to recognize the measurement error popularly called construct bias in psychometry.,"changed the background  to be specific, deleted confusing part in the main question",2013-10-13 09:32:59.457
186048,57185,,6,user10619,CC BY-SA 3.0,837be1ea-06e8-4188-973d-5176ca508e5d,<mathematical-statistics><bias><measurement-error>,"changed the background  to be specific, deleted confusing part in the main question",2013-10-13 09:32:59.457
186049,57333,22359.0,5,,CC BY-SA 3.0,61df977b-84aa-4e6d-afb9-79d343c6fa98,"Try [this][1] paper. Your answer might be at chapter 3.2, figures 2 and 3. 

Long story short: The same performance can be obtained for different pairs of C and kernel parameters. You shouldn't try to manually tune a SVM.

**Edit:** Some details:

I usually tune C (the cost parameter) when I have largely imbalanced classes. That is, one class have 10% and the other 90%. Some SVM libraries (esp. libSVM which I use) lets you specify a cost for each class. According to [libsvm][1] paper, $\frac{c_1}{c_2} = \frac{n_2}{n_1}$ where $n_2>n_1$ , $n_i$ is the volume of the i'th class. If you let $c_2 = 1$ then $c_1 = n_2/n_1$ . There is also a ""global"" C, that is multiplied with the specific $c_i$ values.

When the learning algorithm computes the error for the current SVM parameters, it multiplies each wrongly classified instance with this cost. If the cost is the same for both classes, the lesser class errors will get diluted and your final model will tend not to predict very well (or not at all) the weakly represented class.

Gamma acts as the $\sigma$ for a Gaussian kernel $G(x) = exp(-x^2/2\sigma^2)$. Note from the equation of RBF : $K(x,y)=exp(-\gamma||x-y||^2)$  that $\gamma$ is more or less proportional to $1/\sigma^2$. Basically $\gamma$ controls the width of the kernel.

The intuition behind this is that a large kernel will tend to produce a smoother border between classes and a narrower kernel a more intricate border. In extreme, the former will tend to give higher bias (it learns only the general aspect of the data) and the latter will tend to overfit (it learns all the details, including the outliers and errors in the data). None of these extremes are welcome in applications. A midpoint is desired, but this midpoint cannot be computed analytically and depends on the actual data.

This is why, the metaparameters are usually searched through cross validation. Please keep  in mind that you must optimize for BOTH parameters in the same time.

Hope it helps!

p.s. I am not an expert in SVM so I can't give you the intuition on how exactly the values for global cost parameter C actually influences the results or the convergence speed of SVM.

  [1]: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf",Added more details and some insights from my experience,2013-10-13 09:41:06.863
186051,57380,22648.0,2,,CC BY-SA 3.0,03eb5e40-d770-475c-8399-6eddf3575e67,"I'm trying to predict a response variable in linear regression that should be always positive (cost per click). It's a monetary amount. In adwords, you pay google for clicks on your ads, and a negative number would mean that google pays you when people clicked :P

The predictors are all continuous values. The Rsquared and RMSE are decent when compared to other models, even out-of-sample:

RMSE  Rsquared 

1.4141477 0.8207303

I cannot rescale the predictions, because it's money, so even a small rescaling factor could change costs significantly.

As far as I understand, for the regression model there's nothing special about zero and negative numbers, so it finds the best regression hyperplane no matter whether the output is partly negative.

This is a very first attempt, using all variables I have. So there's room for refinement.

Is there any way to tell the model that the output cannot be negative?",,2013-10-13 09:41:29.343
186052,57380,22648.0,1,,CC BY-SA 3.0,03eb5e40-d770-475c-8399-6eddf3575e67,Negative values in predictions for an always-positive response variable in linear regression,,2013-10-13 09:41:29.343
186050,57380,22648.0,3,,CC BY-SA 3.0,03eb5e40-d770-475c-8399-6eddf3575e67,<regression><predictive-models>,,2013-10-13 09:41:29.343
186053,57379,,24,,CC BY-SA 3.0,5656219a-1981-480c-8721-c5bc33aa4d8a,,Proposed by 26338 approved by -1 edit id of 5604,2013-10-13 09:53:05.250
186055,57379,,6,,CC BY-SA 3.0,5656219a-1981-480c-8721-c5bc33aa4d8a,<standard-error><nonlinear-regression>,added standard error tag for clarification,2013-10-13 09:53:05.250
186054,57379,,4,user88,CC BY-SA 3.0,7a589384-e816-404b-b754-5c2207ff5ae6,Standard error of the residuals for a non-linear model,added standard error tag for clarification,2013-10-13 09:53:05.250
186067,57384,306.0,2,,CC BY-SA 3.0,ecc88eb5-300b-4bee-8d65-720dd1a9327a,"Assuming the coefficients to be normally distributed with the mean of 0 and an estimated standard error which you have not mentioned, the p value tells the quantile of how far the calculated value is from the mean. In the given case if you think that the value is significant at 99.73% level, even then the coefficient is different from 0. If the confidence level that you want is higher than this, then you fail to reject the hypothesis that the coefficient is different from 0.",,2013-10-13 12:29:25.737
186068,57385,22651.0,1,,CC BY-SA 3.0,785e6034-8181-463a-aeae-f3b6db6a4b97,Testing linearity,,2013-10-13 13:09:09.757
186056,57347,6162.0,5,,CC BY-SA 3.0,0cf1739b-4e5d-4cb6-bcf0-1884bfdf0f63,"There is a natural exact confidence interval for the grandmean in the balanced random one-way ANOVA model $$(y_{ij} \mid \mu_i) \sim_{\text{iid}} {\cal N}(\mu_i, \sigma^2_w), \quad j=1,\ldots,J, 
\qquad 
\mu_i \sim_{\text{iid}} {\cal N}(\mu, \sigma^2_b), \quad i=1,\ldots,I.$$
Indeed, it is easy to check that the distribution of the observed means $\bar{y}_{i\bullet}$ is $\bar{y}_{i\bullet} \sim_{\text{iid}} {\cal N}(\mu, \tau^2)$ with $\tau^2=\sigma^2_b+\frac{\sigma^2_w}{J}$, 
and it is well known that the between sum of squares $SS_b$ has distribution $$SS_b \sim J\tau^2\chi^2_{I-1}$$ and is independent of the overall observed mean $$\bar y_{\bullet\bullet} \sim {\cal N}(\mu, \frac{\tau^2}{I})$$. 
Thus $$\frac{\bar y_{\bullet\bullet}  - \mu}{\frac{1}{\sqrt{I}}\sqrt{\frac{SS_b}{J(I-1)}}}$$ has a Student $t$ distribution with $I-1$ degrees of freedom, wherefrom it is easy to get an exact confidence interval about $\mu$.

**Note that this confidence interval is nothing but the classical interval for a Gaussian mean by considering only the group means $\bar{y}_{i\bullet}$ as the observations**. 
Thus the simple approach you mention:

> The simple approach is to first compute the mean of each experiment:
> 38.0, 49.3, and 31.7, and then compute the mean, and its 95% confidence interval, of those three values. Using this method, the
> grand mean is 39.7 with the 95% confidence interval ranging from 17.4
> to 61.9.

is right. And your intuition about the ignored variation:

> The problem with that approach is that it totally ignores the
> variation among triplicates. I wonder if there isn't a good way to
> account for that variation.

is wrong. I also mention the correctness of such a simplification in [http://stats.stackexchange.com/a/72578/8402][1]


  [1]: http://stats.stackexchange.com/a/72578/8402",added 2 characters in body,2013-10-13 10:06:57.330
186057,57377,15827.0,5,,CC BY-SA 3.0,2d99c0a5-59dc-4dcd-a667-8f2816d343bc,"Young Energy operates a power plant. The power plant is a coal-fired boiler that produces steam which in turn drives a generator. The company can buy different types of coal, and then mix them to meet the demands placed on it which is fired in the boiler. The table shows the characteristics of the different types of coal are:

![enter image description here][1]


The requirement to be burned in the pan is: 

 - BTU/lb: 11900,   
 - content of the ashes max 12,2% and 
 - max moisture 9,4%.

  [1]: https://i.stack.imgur.com/Jg23l.png

> How should I implement a linear optimization model in this context?",removed unnecessary wording,2013-10-13 10:17:12.280
186058,57381,8074.0,2,,CC BY-SA 3.0,11563e57-1ec8-44b5-8624-dd75b24b9edf,"I think there are a few options for showing this type of data:

The first option would be to conduct an ""Empirical Orthogonal Functions Analysis"" (EOF) (also referred to as ""Principal Component Analysis"" (PCA) in non-climate circles). For your case, this should be conducted on a correlation matrix of your data locations. For example, your data matrix `dat` would be your spatial locations in the column dimension, and the measured parameter in the rows; So, your data matrix will consist of time series for each location. The `prcomp()` function will allow you to obtain the principle components, or dominant modes of correlation, relating to this field:

    res <- prcomp(dat, retx = TRUE, center = TRUE, scale = TRUE) # center and scale should be ""TRUE"" for an analysis of dominant correlation modes)
    #res$x and res$rotation will contain the PC modes in the temporal and spatial dimension, respectively.

The second option would be to create maps that show correlation relative to an individual location of interest:

    C <- cor(dat)
    #C[,n] would be the correlation values between the nth location (e.g. dat[,n]) and all other locations. 

Hope that helps. 
",,2013-10-13 10:19:34.250
186060,57374,0.0,10,,,00410162-1954-4cde-98d7-a60da85911a2,"{""OriginalQuestionIds"":[4756],""Voters"":[{""Id"":805,""DisplayName"":""Glen_b""},{""Id"":601,""DisplayName"":""John""},{""Id"":21054,""DisplayName"":""COOLSerdash""},{""Id"":22047,""DisplayName"":""Nick Cox""},{""Id"":17230,""DisplayName"":""Scortchi""}]}",101,2013-10-13 11:46:42.203
186063,57382,,2,user10619,CC BY-SA 3.0,987eb6d7-1ade-42d4-b384-d507b299197c,Gross sampling error (MSE) appears to be a composite of two errors sampling and measurement error. How do we assess measurement error ? can we find out net sampling error ? ,,2013-10-13 11:56:43.490
186061,57382,,1,user10619,CC BY-SA 3.0,987eb6d7-1ade-42d4-b384-d507b299197c,does sampling error include measurement error?,,2013-10-13 11:56:43.490
186062,57382,,3,user10619,CC BY-SA 3.0,987eb6d7-1ade-42d4-b384-d507b299197c,<sampling><standard-error><measurement-error>,,2013-10-13 11:56:43.490
186064,57378,5001.0,5,,CC BY-SA 3.0,f64aff0b-4368-4d42-be62-c83e954ed432,"I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if faults are found within it.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is 

`y = 0.514x + 0.00087` 

and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?

Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as 

> ""at least as extreme as 0.00087 in the same direction, that is,
> positive"".",added 8 characters in body,2013-10-13 12:10:42.713
186065,57383,,2,anon,CC BY-SA 3.0,944e979c-a2d2-4836-80bc-a758a9199821,"This depends on what you mean by a genomic location. For each cytoband this would be rather straight forward to do. Roughly:

1) Get the cytoband locations for all genes. These are stored in the organism specific packages, e.g., org.Dm.eg.db, and are named as 'MAP' . You might need the chiptype specific annotation package to map between the probe identifiers and the genes first. 

2) Once you have the cytoband annotations for the genes, you can then test each cytoband separately with the functionality offered by, e.g., the topGO package. There is a section with the heading 'Predefined list of interesting genes' in the vignette of the topGO package that shortly shows how to do this is a similar case.

For the smoothing approach you have thought of, it might be worth correcting the counts with the actual number of genes in any predefined window, taking into account that not all genes might be present on the chip. The exact gene locations are available in the organism specific annotation package (the same as above). Some difficulties might arise, since certain locations probably have a gene in both strands, so you just need to decide how to count them.

The cytoband based approach is available in, e.g., Chipster software (see the manual entry at http://chipster.csc.fi/manual/stat-hyperG-cytoband.html), and the source code for the analysis is available at https://github.com/chipster/chipster/blob/master/src/main/modules/microarray/R-2.12/stat-hyperG-cytoband.R, which might in some details, if you decide to use the cytobands.",,2013-10-13 12:11:47.693
186070,57385,22651.0,2,,CC BY-SA 3.0,785e6034-8181-463a-aeae-f3b6db6a4b97,"i have data which i need to test if they exihibit a linear relationship so that i can be able to use to make prediction of a response.Kindly assist in how to handle this problem.this is the data:task is to show that there is linear relationship between bricks used and wastes generated.

Trials    	 1	 2	 3	 4	 5	 6	 7	 8
No. of Bricks (x)	1400	1800	2100	2400	2700	3000	3500	3800
Wastage, % (y)	10.31	12.26	13.32	15.65	15.12	18.93	20.72	19.04
Thanks.
Lawrence",,2013-10-13 13:09:09.757
186069,57385,22651.0,3,,CC BY-SA 3.0,785e6034-8181-463a-aeae-f3b6db6a4b97,<model>,,2013-10-13 13:09:09.757
186071,57385,503.0,5,,CC BY-SA 3.0,2cdf7a9d-d9ae-4c84-8982-97f162202ba9,"I have two variables and I need to test if they exhibit a linear relationship so that I will be able to predict a response. Kindly assist in how to handle this problem. This is the data:task is to show that there is linear relationship between bricks used and wastes generated.

    Trials    	      1       2	      3	      4	      5	      6	      7	       8
    No. Bricks (x)	1400	1800	2100	2400	2700	3000	3500	3800
    Wastage, % (y)	10.31	12.26	13.32	15.65	15.12	18.93	20.72	19.04

","fixed spelling, grammar and formatting, removed signature",2013-10-13 13:15:53.497
186072,57386,503.0,2,,CC BY-SA 3.0,00bed328-e374-47d5-ae01-a9b2e32b0b01,"My first point would be that you do not need to have a linear relationship in order to predict a response.

Second, if you are trying to predict a response outside the range of the data (i.e. to less than 1400 or more than 3800 bricks) be very cautious.

To your question: The first thing I would do is make a graph. In `R` this could be done as follows:

    x <- c(1400, 1800, 2100, 2400, 2700, 3000, 3500, 3800)
    y <- c(10.31, 12.26, 13.32, 15.65, 15.12, 18.93, 20.72, 19.04)
    plot(x,y)
    lines(lowess(x,y))

The last line adds a loess curve to the data. The relationship appears to be linear at the lower levels, but then flatten at higher levels of bricks. 

I would not rely on any statistical test of linearity. With only 8 points, the deviation from linearity would have to be extreme for it to be significant and a much smaller deviation might be important. ",,2013-10-13 13:23:42.913
186073,57387,12522.0,2,,CC BY-SA 3.0,e0a5ea15-110a-4837-84dd-c73e83b52dd2,"A possible formulation of this model is as follows:

The purpose of the optimization problem is to obtain the % of each coal type to mix in order to minimize the cost of the mix without violating any operational constraint.

$i = $ index for coal type (1 = A, 2 = B, 3 = B, 4 = D)

$x_{i} =$ % of coal type $i$ to be included in the mix

$c_{i} =$ cost per pound of coal of type $i$ 

$b_{i} =$ BTUs per pound of coal of type $i$

$a_{i} =$ % of ashes of coal of type $i$

$m_{i} =$ % of moisture of coal of type $i$

Objective Function: Minimize the cost of a pound of the mix

Min $Z = \sum_{i=1}^{4} c_{i} \cdot x_{i}$

Subject to the following contraints:

- $BTU/lb$ of the mix must be equal to 11,900:
  
  $\sum_{i=1}^4 b_{i} \cdot x_{i} = 11900$

- Content of ashes of the mix must be less than 12.2%:

  $\sum_{i=1}^4 a_{i} \cdot x{i} \leqslant 12.2\%$

- The percent of moisture of the mix must be less than 9.4%:

  $\sum_{i=1}^4 m_{i} \cdot x{i} \leqslant 9.4\%$

- The percent of each coal in the mix must add up to 100%:

  $\sum_{i=1}^4 x{i} = 100\%$

- Non-negativity constraint:

  $x_{i} \geqslant 0, \forall i$

You can implement the model in R using the Rglpk package or using the Excel Solver Add-in in MS Excel.",,2013-10-13 13:26:01.097
186076,57388,22653.0,3,,CC BY-SA 3.0,05212a22-889b-4f40-8eff-fcced24eb259,<meta-analysis>,,2013-10-13 13:49:31.680
186075,57388,22653.0,1,,CC BY-SA 3.0,05212a22-889b-4f40-8eff-fcced24eb259,Meta Analysis: Pooling samples or determine an average effect size,,2013-10-13 13:49:31.680
186074,57388,22653.0,2,,CC BY-SA 3.0,05212a22-889b-4f40-8eff-fcced24eb259,"I am new to meta analysis and how I understood the terminology is that there are actually two ways of performing a meta analysis. Let's consider 5 clinical studies with fixed effects. Fixed effects in terms of the same medical treatment as well as demographic details of the participants. One way of analysing these data would be to pool all 5 studies together to obtain a very large study to increase the power to detect the effect of the medical treatment. The other would be to try to detect the effect in each analysis separately and then determine the average effect across the studies. As I understood meta analysis, both seem to be reasonable techniques. However, can anyone tell me pro's and con's for both techniques? When should I use which method? I would assume the results to be pretty similar anyhow or is that wrong to assume?",,2013-10-13 13:49:31.680
186077,57389,166.0,2,,CC BY-SA 3.0,facc6fac-f0ed-4d70-a08b-a75b727d474a,"Your interpretation is almost right.

A correct interpretation requires the following components:

 1. The interpretation of p-values is in reference to either **a**) [frequentist] the probability of obtaining a value if you ran the same experiment (and statistic) many times over that you would obtain values that were as extreme or more extreme (the 'extreme' language implies a two-tailed test) if the true population value were 0 (or, in some tests that the difference is 0), i.e. if the null hypothesis were true; or **b**) the probability of obtaining that estimate of the parameter (e.g. intercept; using this statistical approach), or a more extreme value, if the population value for that parameter is 0.  Your definition correctly uses the frequentist form.
 2. As you can see from point 1 above, the interpretation of p-values in a regression equation are not dependent on assumptions that the rest of the model is correct... just that the same approach was used.  However, it does assume that those parameters are estimated.  **So, your definition lacks** in that you say 'first coefficient is 0.514'.  All you need to assert is that the first parameter is estimated... and that overall the model used has the same parameters estimated.  The values obtained for the estimate are immaterial to the definition of the p-value for any given parameter.
 3. That we are referring to the value of y when all xs are at the value of zero.  You correctly imply this point.

You should also note that your example, in using the frequentist form, is not free from your wants and subjective beliefs.  Specifically, the p-value is tied to the design of the experiment you ran. You acknowledge this when you mention using the same number of sampling pairs.

In regards to your second question, the typical p-value reported for a regression equation is implicitly two-tailed.  So, it refers to the absolute value of the parameters obtained.  You didn't provide the Excel function you uses to calaculate the p-value, but I'd check there to see if Excel is calculating one-tailed (in the same direction) or two-tailed (extreme or more extreme) p-values.",,2013-10-13 13:57:06.653
186078,57378,5001.0,5,,CC BY-SA 3.0,abb10b42-6233-449b-b741-19847b23706f,"I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if faults are found within it.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is 

`y = 0.514x + 0.00087` 

and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?

Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as 

> ""at least as extreme as 0.00087 in the same direction, that is,
> positive"".

Edit: The Excel funcion is `Tools > Data Analysis > Regression` in Office 2003 with service pack 2.",added 103 characters in body,2013-10-13 14:06:47.563
186081,57390,21985.0,1,,CC BY-SA 3.0,d29f0bee-c706-4ccd-bc0a-807c66c66f50,"Auto regressive process, maximum likelihood estimator",,2013-10-13 14:45:19.743
186080,57390,21985.0,2,,CC BY-SA 3.0,d29f0bee-c706-4ccd-bc0a-807c66c66f50,"A first-order autoregressive process, $X_0,\dots,X_n$, is given through the following conditional distributions:
$X_i | X_{i-1},\dots,X_0 \sim \mathcal{N}(\alpha X_{i-1},1)$,
for $i = 1,2,\dots,n$ and $X_0 \sim \mathcal{N}(0,1)$.

I know that the log-likelihood function $\ell{(\alpha)}$ is of the form:
$\ell(\alpha) = - \frac{1}{2} \sum (x_i - \alpha x_{i-1})^2 + c$. But I don't know how to show that.

I found for $\hat{\alpha}_{ML}$ the following solution: $\hat{\alpha}_{ML} = \frac{s}{t}, \mathrm{where} \; s = \sum x_1 x_{i-1} \mathrm{and} \; t = \sum (x_{i-1})^2$. Is this right?

Then I have to show that this is the global maximum. If I take the second derivative I get a constant. Is this the sign that I got the global maximum, because the first derivative is linear wrt to $\alpha$? Right?

",,2013-10-13 14:45:19.743
186079,57390,21985.0,3,,CC BY-SA 3.0,d29f0bee-c706-4ccd-bc0a-807c66c66f50,<self-study><maximum-likelihood><autoregressive>,,2013-10-13 14:45:19.743
186082,57368,21840.0,6,,CC BY-SA 3.0,ae55474d-0293-4e54-8c5d-634aad7a544e,<self-study><density-function><joint-distribution>,edited tags,2013-10-13 15:01:11.003
186083,57391,2420.0,2,,CC BY-SA 3.0,a2a7b8f0-41ab-4ee7-a584-bb5a6dcbeef9,"I'm a software developer and I'll like to learn about neural networks. At this point I've find a problem which I'll like to solve at some point. It is about electrical load forecasting. I'm looking for similar problems and it will be great if I can find some similar examples with solutions. At this point I'm having troubles in finding the right model for the RNN, and more exactly I'm struggling with the input layer. As the output I need the forecast values for each hour.

Any reference to books, links resources or advices are welcome and very appreciated. 

This is the problem that I'll like to solve:

A very small factory, use a number of equipments to produce bread. Some of them are electrical equipments which means that they consume electrical power.  Knowing which equipments will run on the next day, an electricity consumptions forecast can be computed.  

The equipment named E.V. is a special case of equipment. The human operator completes it's values in an empirically manner in order to have a good forecast for the next day. Those values can be positive or negative. 


    +----------------------------------------------------+  
    |equipment name|power| 1h| 2h| 3h| 4h| 5h| 6h| 7h| 8h|  
    +----------------------------------------------------+  
    |Equipment 1   |  2MW| - | - | on| on| on| - | - | - |
    |Equipment 2   |  5MW| - | - | - | on| on| on| - | - |
    |Equipment 3   |  1MW| on| on| on| on| on| on| on| on|
    |E.V.          |     | .1|-.1|-.1| .1|-.2| .1| .1|-.1|
    +--------------+-------------------------------------+
    |total/(forecast)    |1.1| .9|2.9|8.1|7.8|6.1|1.1| .9|
    +--------------+-------------------------------------+
    |real consumption    | .9| .9|2.7|8.2|7.9|3.1|0.8| .7|  
    +--------------+-------------------------------------+

The problem is that the machines are not running at their maximal power, so it will be great if a more exactly forecast can be build.",,2013-10-13 15:03:39.740
186085,57391,2420.0,3,,CC BY-SA 3.0,a2a7b8f0-41ab-4ee7-a584-bb5a6dcbeef9,<regression><machine-learning><forecasting><neural-networks>,,2013-10-13 15:03:39.740
186084,57391,2420.0,1,,CC BY-SA 3.0,a2a7b8f0-41ab-4ee7-a584-bb5a6dcbeef9,How to find the input layer and the architecture for a Neural Network,,2013-10-13 15:03:39.740
186086,57392,5203.0,2,,CC BY-SA 3.0,5c99a260-8e91-4080-929f-7858de2bf2ef,"Yes, assuming by ""gross sampling error"" you mean mean-squared error or the $\epsilon$ term in a model like $Y=AX + \epsilon$

The error component of a model includes *all* sources of variability that are not explicitly included in the model. This includes sampling errors (uncertainty due to measuring only a subset of the population), measurement errors (uncertainty due to imprecisions in each measurement), and other things, like error attributable to a misspecified model (e.g., missing predictors/interactions). 

Keep in mind that these are actually types of errors. For example, there may be measurement error associated with each variable in the model, and that error might be a combination of systematic error (essentially, a bias; e.g., someone forgot that the scale reports the weight of the container + its contents) and random error.  Given that, there isn't an automatic, all-purpose way of identifying the various error contributions. 

One way to examine measurement errors is through **calibration**. For example, you could put a weight on the scale and compare the scale's reading to the known mass of the weight. In many cases, the phenomena causing measurement error are reasonably well understood and have a specific structure (e.g., [shot noise][1]), which allows them to be incorporated into the model. Some large-scale physics experiments take this to incredible extremes to compare an apparatus's expected performance to the real data. Surveys are sometimes **benchmarked** by comparing data collected during the survey to larger data sets. For example, you might ask participants for demographic information (e.g., age, gender, income). These values are then compared to known population values (e.g., from a census or tax records), which might tell you how representative your respondents are of the general population. 

Sampling error is much harder to measure directly. You might expect sampling error to shrink as the number of samples approaches the size of the population, whereas a systematic measurement error would remain approximately the same, regardless of sample size. 

  [1]: http://en.wikipedia.org/wiki/Shot_noise",,2013-10-13 15:15:38.567
186087,57388,,25,,,7d664d4c-5795-45eb-8b4e-e46127803609,,http://twitter.com/#!/StackStats/status/389410239004823552,2013-10-13 15:20:10.623
186088,57381,8074.0,5,,CC BY-SA 3.0,2a35e65a-a0bd-4fc6-b8a9-51d0c9446cb0,"I think there are a few options for showing this type of data:

The first option would be to conduct an ""Empirical Orthogonal Functions Analysis"" (EOF) (also referred to as ""Principal Component Analysis"" (PCA) in non-climate circles). For your case, this should be conducted on a correlation matrix of your data locations. For example, your data matrix `dat` would be your spatial locations in the column dimension, and the measured parameter in the rows; So, your data matrix will consist of time series for each location. The `prcomp()` function will allow you to obtain the principal components, or dominant modes of correlation, relating to this field:

    res <- prcomp(dat, retx = TRUE, center = TRUE, scale = TRUE) # center and scale should be ""TRUE"" for an analysis of dominant correlation modes)
    #res$x and res$rotation will contain the PC modes in the temporal and spatial dimension, respectively.

The second option would be to create maps that show correlation relative to an individual location of interest:

    C <- cor(dat)
    #C[,n] would be the correlation values between the nth location (e.g. dat[,n]) and all other locations. 

Hope that helps. 
",corrected spelling,2013-10-13 16:04:24.417
186089,57393,14850.0,2,,CC BY-SA 3.0,0620371e-2325-4828-b420-9728b9c35820,"It appears that you're using <code>nls</code> to fit a logistic model.

By typing 

    ?summary.nls

you can read about the output.

Estimates and standard errors are estimated by the Gauss-Newton algorithm (if the <code>nls</code> defaults are used)

The P-values are the results of a one sided test of whether the parameters are zero or not. 

You can check the exact calculations used to create the output shown by typing:

    stats:::summary.nls",,2013-10-13 16:17:54.617
186090,57378,5001.0,5,,CC BY-SA 3.0,2671864b-2426-41f9-a7ae-35f58d368e46,"I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if faults are found within it.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is 

`y = 0.514x + 0.00087` 

and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?

Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as 

> ""at least as extreme as 0.00087 in the same direction, that is,
> positive"".

Edit: The Excel funcion is `Tools > Data Analysis > Regression` in Office 2003 with service pack 2. Excel regression p-values on coefficients are 2 sided.",added 55 characters in body,2013-10-13 16:30:43.817
186093,57394,16046.0,2,,CC BY-SA 3.0,e7e70389-f554-4886-b5ae-65780f3c30a0,"I am reading an [article][1] which is trying to justify the need for causal inference in their inferential framework. The thought experiment is as follows:

> Suppose a statistician is asked to design a model for a simple time
> series $X_1,X_2,X_3,...$ and she decides to use a Bayesian method.
> Assume she collects a first observation $X_1 = x_1$. She computes the
> posterior probability density function (pdf) over the parameters
> $\theta$ of the model given the data using Bayes’ rule: $$p(\theta|X_1
= x_1) = \int\frac{p(X_1 = x_1|\theta)p(\theta)}{p(X_1 = x_1|\theta')p(\theta')}, $$
> 
> where $p(X_1 = x_1|θ)$ is the likelihood of $x_1$ given $\theta$ and
> p($\theta$) is the prior pdf of $\theta$. She can use the model to
> predict the next observation by drawing a sample $x_2$ from the
> predictive ￼pdf: $$p(X_2 = x_2|X_1 = x_1) = \int p(X_2 = x_2|X_1 =
 x_1,\theta)p(\theta|X_1 = x_1)d\theta,$$
> 
> where $p(X_2 = x_2|X1 = x1,\theta)$ is the likelihood of $x_2$ given
> $x_1$ and $\theta$. Note that $x_2$ is not drawn from $p(X_2 = x_2|X_1
> = x_1, \theta)$. She understands that the nature of $x_2$ is very different from $x_1$: while $x_1$ is informative and does change the
> belief state of the Bayesian model, $x_2$ is non-informative and thus
> is a reflection of the model’s belief state. Hence, she would never
> use $x_2$ to further condition the Bayesian model. Mathematically, she
> seems to imply that: 
> $$p(\theta|X_1 =x_1,X_2 =x_2)=p(\theta|X_1 =x_1)$$
However I hardly believe that what this poor statistician should imply is:
$$p(\theta|X_1 =x_1,\text{do}(X_2 =x_2))=p(\theta|X_1 =x_1)$$

Am I right about this?
  [1]: http://www.aaai.org/Papers/JAIR/Vol38/JAIR-3812.pdf",,2013-10-13 16:53:32.577
186096,57378,5001.0,5,,CC BY-SA 3.0,78375e55-c472-4aaa-bad6-78b3471b1092,"I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if faults are found within it.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is 

`y = 0.514x + 0.00087` 

and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?

Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as 

> ""at least as extreme as 0.00087 in the same direction, that is,
> positive"".

**Edit**: The Excel funcion is `Tools > Data Analysis > Regression` in Office 2003 with service pack 2. Excel regression p-values on coefficients are 2 sided.

**Edit**: Differentiation from this question [here][1].  The most up voted answer discusses the p-value of a hypothesis.  I am not interested in that.  I am interested in the p-value of a coefficient that is not the coefficient of an independent variable. 


  [1]: http://stats.stackexchange.com/questions/31/what-is-the-meaning-of-p-values-and-t-values-in-statistical-tests",added 377 characters in body,2013-10-13 16:57:56.673
186097,57394,16046.0,5,,CC BY-SA 3.0,84dc775e-bf28-4c92-b58d-1bae19fffce8,"I am reading an [article][1] which is trying to justify the need for causal inference in their inferential framework. The thought experiment is as follows:

> Suppose a statistician is asked to design a model for a simple time
> series $X_1,X_2,X_3,...$ and she decides to use a Bayesian method.
> Assume she collects a first observation $X_1 = x_1$. She computes the
> posterior probability density function (pdf) over the parameters
> $\theta$ of the model given the data using Bayes’ rule: $$p(\theta|X_1
= x_1) = \int\frac{p(X_1 = x_1|\theta)p(\theta)}{p(X_1 = x_1|\theta')p(\theta')}, $$
> 
> where $p(X_1 = x_1|θ)$ is the likelihood of $x_1$ given $\theta$ and
> p($\theta$) is the prior pdf of $\theta$. She can use the model to
> predict the next observation by drawing a sample $x_2$ from the
> predictive ￼pdf: $$p(X_2 = x_2|X_1 = x_1) = \int p(X_2 = x_2|X_1 =
 x_1,\theta)p(\theta|X_1 = x_1)d\theta,$$
> 
> where $p(X_2 = x_2|X1 = x1,\theta)$ is the likelihood of $x_2$ given
> $x_1$ and $\theta$. Note that $x_2$ is not drawn from $p(X_2 = x_2|X_1
> = x_1, \theta)$. She understands that the nature of $x_2$ is very different from $x_1$: while $x_1$ is informative and does change the
> belief state of the Bayesian model, $x_2$ is non-informative and thus
> is a reflection of the model’s belief state. Hence, she would never
> use $x_2$ to further condition the Bayesian model. Mathematically, she
> seems to imply that: 
> $$p(\theta|X_1 =x_1,X_2 =x_2)=p(\theta|X_1 =x_1)$$

However I hardly believe that what this poor statistician should imply is:
$$p(\theta|X_1 =x_1,\text{do}(X_2 =x_2))=p(\theta|X_1 =x_1)$$

Am I right about this?
  [1]: http://www.aaai.org/Papers/JAIR/Vol38/JAIR-3812.pdf",added 2 characters in body,2013-10-13 16:59:51.637
186098,57395,668.0,2,,CC BY-SA 3.0,9114b16b-0fb1-4360-87d0-d5e7dd2fb31f,"**Such a distribution does not exist.**

To see why not, let $0 \lt t \lt 1/2$ and notice that $X_2\gt 1-t$ entails $X_1\le t$ and $X_3\gt 1-t$ also implies $X_1\le t$, for otherwise in either situation the sum of all the $X_i$ would exceed $1.$  The latter two events are disjoint, because we cannot simultaneously have $X_2\gt 1-t \gt 1/2$ and $X_3\gt 1-t\gt 1/2.$  Consequently the chance that $X_1\le t$ is no less than the sum of the chances that $X_2\ge 1-t$ and $X_3\ge 1-t$, each of which equals $t$ by the uniform distribution assumptions.  This shows that $t \ge t+t,$ which for $t\gt 0$ obviously is false.

This contradiction forces us to give up at least one of the assumptions: if indeed $X_1+X_2+X_3\le 1$, then the only other assumptions used in this argument are that each $X_i$ has a Uniform$[0,1]$ distribution.  Therefore *at least one of the $X_i$ cannot have a Uniform$[0,1]$ distribution,* QED.",,2013-10-13 17:06:36.577
186109,57396,15827.0,5,,CC BY-SA 3.0,5b5be56c-e160-4bd5-a1b4-03226ab23dfa,"I have a large population of size $n$ from an unknown continuous random variable $X$, and I do not know the underlying distribution of $X$. Given a constant number $c$, I want to determine the minimum sample size I need to estimate the probability $P(X \le c)$ given a confidence level, $p_c$, and confidence interval, $I_c$  (I am not sure if we need them! ). How can I find the minimum sample size to estimate this probability? Thanks for your help.",more use of TeX ,2013-10-13 18:10:21.797
186112,57398,22658.0,2,,CC BY-SA 3.0,7200c158-80c3-4841-ba53-c6fa7ab5a8d8,"There are 2 competing statistical models.  Model #1 (null hypothesis, McNemar): probability correct to incorrect = probability of incorrect to correct = 0.5 or equivalent b=c.  Model #2: probability correct to incorrect < probability of incorrect to correct or equivalent b > c.  For model #2 we use maximum likelihood method and logistic regression to determine model parameters representing model 2.  Statistical methods look different because each method reflects a different model.  
",,2013-10-13 18:26:48.740
186115,57399,6813.0,3,,CC BY-SA 3.0,9fb279df-c7cf-483c-a537-7a1203d9c279,<probability><binomial-distribution><model>,,2013-10-13 18:37:21.500
186099,57378,5001.0,5,,CC BY-SA 3.0,45bc7efb-6cb5-4ae4-9a86-7cc437114f2d,"I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if faults are found within it.

From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is 

`y = 0.514x + 0.00087` 

and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.

Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:

> Under the assumption that the true value of the y-intercept is zero
> and the first coefficient is 0.514, random sampling of the same number
> of (x,y) pairs, specifically 90, would result in a least squares best
> fit line with a y-intercept at least as extreme as 0.00087, with a
> probability of 0.0027.

If not, then what would be the correct interpretation?

Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as 

> ""at least as extreme as 0.00087 in the same direction, that is,
> positive"".

**Edit**: The Excel funcion is `Tools > Data Analysis > Regression` in Office 2003 with service pack 2. Excel regression p-values on coefficients are 2 sided.

**Edit**: Regarding differentiation from this question [here][1]: The most up voted answer there discusses the p-value of a hypothesis, which seems ill defined or at least not specific.  I am not interested in that.  I am interested in the p-value of a coefficient that is not the coefficient of an independent variable. I am being very specific.


  [1]: http://stats.stackexchange.com/questions/31/what-is-the-meaning-of-p-values-and-t-values-in-statistical-tests",added 15 characters in body,2013-10-13 17:07:28.077
186102,57396,22656.0,2,,CC BY-SA 3.0,bd064baa-526d-4455-a7f3-db5d754efb34,"I have a large population of size n, but I do not know the underlying distribution of the population. I want to determine the minimum sample size I need to estimated the CDF given a confidence level and confidence interval (I am not sure if we need them! ). How can I find the minimum sample size? Thanks for your help.",,2013-10-13 17:09:46.403
186101,57396,22656.0,1,,CC BY-SA 3.0,bd064baa-526d-4455-a7f3-db5d754efb34,Minimum Sample Size Required to Estimate CDF of a Given Population (Given a Confidence Level & Confidence Interval),,2013-10-13 17:09:46.403
186100,57396,22656.0,3,,CC BY-SA 3.0,bd064baa-526d-4455-a7f3-db5d754efb34,<probability><distributions><mathematical-statistics>,,2013-10-13 17:09:46.403
186103,57394,16046.0,5,,CC BY-SA 3.0,248ea3c2-28f5-46e0-b8e6-c08ddc47c9bd,"I am reading an [article][1] which is trying to justify the need for causal inference in their inferential framework. The thought experiment is as follows:

> Suppose a statistician is asked to design a model for a simple time
> series $X_1,X_2,X_3,...$ and she decides to use a Bayesian method.
> Assume she collects a first observation $X_1 = x_1$. She computes the
> posterior probability density function (pdf) over the parameters
> $\theta$ of the model given the data using Bayes’ rule: $$p(\theta|X_1
= x_1) = \int\frac{p(X_1 = x_1|\theta)p(\theta)}{p(X_1 = x_1|\theta')p(\theta')}, $$
> 
> where $p(X_1 = x_1|θ)$ is the likelihood of $x_1$ given $\theta$ and
> p($\theta$) is the prior pdf of $\theta$. She can use the model to
> predict the next observation by drawing a sample $x_2$ from the
> predictive ￼pdf: $$p(X_2 = x_2|X_1 = x_1) = \int p(X_2 = x_2|X_1 =
 x_1,\theta)p(\theta|X_1 = x_1)d\theta,$$
> 
> where $p(X_2 = x_2|X1 = x1,\theta)$ is the likelihood of $x_2$ given
> $x_1$ and $\theta$. Note that $x_2$ is not drawn from $p(X_2 = x_2|X_1
> = x_1, \theta)$. She understands that the nature of $x_2$ is very different from $x_1$: while $x_1$ is informative and does change the
> belief state of the Bayesian model, $x_2$ is non-informative and thus
> is a reflection of the model’s belief state. Hence, she would never
> use $x_2$ to further condition the Bayesian model. Mathematically, she
> seems to imply that: 
> $$p(\theta|X_1 =x_1,X_2 =x_2)=p(\theta|X_1 =x_1)$$

However I hardly believe that what this poor statistician should imply is:
$$p(\theta|X_1 =x_1,\text{do}(X_2 =x_2))=p(\theta|X_1 =x_1)$$
Where ""do(or set)"" here comes from [Pearl][2]'s framework of causality which can be found [here][3] and [here][4].
Now am I right about this?


  [1]: http://www.aaai.org/Papers/JAIR/Vol38/JAIR-3812.pdf
  [2]: http://bayes.cs.ucla.edu/jp_home.html
  [3]: ftp://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf
  [4]: http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=dp_ob_title_bk",added 325 characters in body,2013-10-13 17:15:58.790
186104,57397,20473.0,2,,CC BY-SA 3.0,1cb3c237-05c4-4a2c-9402-57003ff3dfee,"I assume that you are using the OLS estimator on this linear regression model. You can use the _**inequality constrained least-squares estimator**_, which will be the solution to a minimization problem under inequality constraints. Using standard matrix notation (vectors are column vectors) the minimization problem is stated as

$$\min_{\beta} (\mathbf y-\mathbf X\beta)'(\mathbf y-\mathbf X\beta) \\s.t.-\mathbf Z\beta \le \mathbf 0 $$

...where $\mathbf y$ is $n \times 1$ , $\mathbf X$ is $n\times k$, $\beta$ is $k\times 1$ and $\mathbf Z$ is the $m \times k$ matrix containing the out-of-sample regressor series of length $m$ that are used for prediction. We have $m$ linear inequality constraints (and the objective function is convex, so the first order conditions are sufficient for a minimum).

The Lagrangean of this problem is

$$L =  (\mathbf y-\mathbf X\beta)'(\mathbf y-\mathbf X\beta) -\lambda'\mathbf Z\beta = \mathbf y'\mathbf y-\mathbf y'\mathbf X\beta -  \beta'\mathbf X'\mathbf y+ \beta'\mathbf X'\mathbf X\beta-\lambda'\mathbf Z\beta$$

$$= \mathbf y'\mathbf y -  2\beta'\mathbf X'\mathbf y+ \beta'\mathbf X'\mathbf X\beta-\lambda'\mathbf Z\beta $$

where $\lambda$ is a $m \times 1$ column vector of non-negative Karush -Kuhn -Tucker multipliers. The first order conditions are (you may want to review rules for matrix and vector differentiation)

$$\frac {\partial L}{\partial \beta}= \mathbb 0\Rightarrow  -  2\mathbf X'\mathbf y +2\mathbf X'\mathbf X\beta - \mathbf Z'\lambda  $$

$$\Rightarrow \hat \beta_R = \left(\mathbf X'\mathbf X\right)^{-1}\mathbf X'\mathbf y + \frac 12\left(\mathbf X'\mathbf X\right)^{-1}\mathbf Z'\lambda = \hat \beta_{OLS}+ \left(\mathbf X'\mathbf X\right)^{-1}\mathbf Z'\xi \qquad [1]$$

...where $\xi = \frac 12 \lambda$, for convenience, and $\hat \beta_{OLS}$ is the estimator we would obtain from ordinary least squares estimation.

The method is fully elaborated in [Liew (1976)][1].



  [1]: http://www.jstor.org/stable/2285614",,2013-10-13 17:17:58.327
186105,57393,14850.0,5,,CC BY-SA 3.0,5ab41cf5-b791-4eb3-be5a-341b42604931,"It appears that you're using <code>nls</code> to fit a logistic model.

By typing 

    ?summary.nls

you can read about the output.

Estimates and standard errors are estimated by the Gauss-Newton algorithm (if the <code>nls</code> defaults are used)

The P-values are the results of a two sided test of whether the parameters are zero or not. 

You can check the exact calculations used to create the output shown by typing:

    stats:::summary.nls",edited body,2013-10-13 17:22:26.553
186106,57396,22656.0,5,,CC BY-SA 3.0,e4b867a3-e4c4-4963-a887-773954408d84,"I have a large population of size n from an unknown continuous random variable X, and I do not know the underlying distribution of X.  I want to determine the minimum sample size I need to estimate the  *CDF* of X given a confidence level, $p_c$, and confidence interval, $I_c$  (I am not sure if we need them! ). More precisely, given a constant number *c*, I want to estimate the probability $P(X<=c)$. How can I find the minimum sample size to estimate this probability? Thanks for your help.",added 176 characters in body,2013-10-13 17:26:00.743
186108,57396,22656.0,4,,CC BY-SA 3.0,e5419645-6087-4a41-a426-810f1016097c,Minimum Sample Size Required to Estimate the Probability $P(X<=c)$ for a Constant c (Given a Confidence Level & Confidence Interval),added 176 characters in body,2013-10-13 17:31:26.440
186107,57396,22656.0,5,,CC BY-SA 3.0,e5419645-6087-4a41-a426-810f1016097c,"I have a large population of size *n* from an unknown continuous random variable X, and I do not know the underlying distribution of X. Given a constant number *c*, I want to determine the minimum sample size I need to estimate the probability $P(X<=c)$ given a confidence level, $p_c$, and confidence interval, $I_c$  (I am not sure if we need them! ). How can I find the minimum sample size to estimate this probability? Thanks for your help.",added 176 characters in body,2013-10-13 17:31:26.440
186110,57396,15827.0,4,,CC BY-SA 3.0,5b5be56c-e160-4bd5-a1b4-03226ab23dfa,Minimum Sample Size Required to Estimate the Probability $P(X \le c)$ for a Constant $c$ (Given a Confidence Level & Confidence Interval),more use of TeX ,2013-10-13 18:10:21.797
186113,57399,6813.0,2,,CC BY-SA 3.0,9fb279df-c7cf-483c-a537-7a1203d9c279,"Recently, I was wondering about calculating the probability of a given individual in a given population ""knowing"" (let's say, present in individual's friend set) at least one person with a given trait A and at least one person with another given trait, B; where it is possible that one person in the given individual's friend set can possess both traits.

For example, using genetic traits, in a given population, how could one calculate the probability that a given individual in a given population ""knows"" at least one person with grey eye colour and at least one person who is greater than 200cm tall; where, naturally, it is possible that any individual can possess grey eye colour *and* be greater than 200cm tall.

I have developed a sort of model, but it may not be correctly specified; it is as follows:

**Assumptions and Qualifications:**

 - First of all, for simplicity, let's assume that we define ""knowing"" as mutually connected friends on a social network.
 - Secondly, the frequencies of genetic traits are (a) going to be determined by ethnicity of a given population as well as environmental factors (nutrition, healthcare) and (b) unlikely to be independently distributed across a given individual's friend set (e.g. family members will have greater genetic similarity); however, for this problem, let's adopt a simple model where both of the above conditions are violated.
 - Thirdly, assume that the individual's friend set provides a microcosmic representation of society; this facilitates a frequentist generation of probabilities from the instance rate in the population.
 - Finally, instances of genetic traits in the population are fabricated, but are used to generate probabilities for the examples.

**Model Formulation:**

I have reasoned that a binomial random distribution can be applied to the probability of each genetic trait, where a ""success"" is defined as an individual in the friend set possessing that genetic trait.

Thus for trait A, we have:

$$P(A=k) = \binom{N}{k}p_{a}^{k}(1-p_{a})^{N-k}$$

and, for trait B:

$$P(B=k) = \binom{N}{k}p_{b}^{k}(1-p_{b})^{N-k}$$

where $N$ is number of friends in the friend set, $k$ is number of people containing the genetic trait and $p_{x}$ is the probability associated with possessing genetic trait $X$.

Because the scenario is concerned with the probability of *at least one* person containing trait A and trait B, it is easier to find the complement of no people in a friend set containing the trait; for both traits:

$$P(A >= 1) = 1 - P(A = 0)$$

and 

$$P(B >= 1) = 1 - P(B = 0)$$

Furthermore, we know that probability of an intersection of events is given by:

$$\mathbb{P}(A \cap B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)\mathbb{P}(A)$$

However, because we are assuming independence between genetic traits, $\mathbb{P}(A)$ and $\mathbb{P}(B)$ are independent, thus:

$$\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$$

Combining the information above, we get the following model for the above scenario:

$$\mathbb{P}(A >=1, B >=1) = \left(1-\binom{N}{k}p_{a}^{k}(1-p_{a})^{N-k}\right)\left(1-\binom{N}{k}p_{b}^{k}(1-p_{b})^{N-k}\right)$$

So, for the original example above, assuming a friend set of size $N = 300$, the instance of trait A in the population is $\frac{1}{800}$ and the instance of trait B in the population is $\frac{1}{5000}$; according to the model, we get the final probability:

$$\mathbb{P}(A >=1, B >=1)$$

$$ = \left(1-\binom{300}{0}\left(\frac{1}{800}\right)^{0}\left(\frac{799}{800}\right)^{300}\right)\left(1-\binom{300}{0}\left(\frac{1}{5000}\right)^{0}\left(\frac{4999}{5000}\right)^{300}\right)$$

$$\approx 0.018 = 1.8\%$$

Does this model seem reasonable given the assumptions?

Assuming this model is *not* correctly specified, maybe somebody could provide a more accurate representation.",,2013-10-13 18:37:21.500
186116,57400,7155.0,2,,CC BY-SA 3.0,d82bfde8-16f1-49b4-b727-3ee57af1bc65,"Define neural network to be $f$, time-series to be $x$, lag order to be $n$ and forecast horizon to be $h$.

$ f(x_{t-1}, x_{t-2},..,x_{t-n}) = [x_t, x_{t+1},..,x_{t+h}]$

Assume you have the following time series,

    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

You define $n=2$, $h=1$.

Your inputs for that time-series are circulant matrix like.

x = 

    [[ 1, 0],
     [ 2, 1],
     [ 3, 2],
     [ 4, 3],
     [ 5, 4],
     [ 6, 5],
     [ 7, 6],
     [ 8, 7]]

Your outputs are

y = 

    [2, 3, 4, 5, 6, 7, 8, 9]

So the length of your input layer is given by $n$, the length of your output layer is given by $h$, where your first input neuron is $x_{t-1}$ and your last input in $x_{t-n}$. Same goes for the forecast horizon.

Instead of having multiple outputs for the forecast horizon, you can use a forecast horizon of 1 then recurse on the predictions to obtain any forecast horizon you want.

For classic parametric stationary time series models the limit of the recursive behaviour of the system is well-studied.

Your problem is a little more involved though. You have inputs and outputs of the system and you want the predict outputs to follow some reference trajectory.

One solution is to use Narma-L2, which approximates the system by linear feedback using two neural networks. Define control inputs to be $c$ and production outputs to be $p$. Define reference production outputs to be $r$

You train two neural networks of the forms $g(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n}) = c_{t}$ and $k(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n}) = p_{t}$.

The prediction for control inputs is then $c_t = \frac{r_t - k(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n})}{g(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n})}$

Also, neural networks are a PITA. There's plenty of good nonparametric regression models that are easier to train, like Gaussian Process Regression for instance.

See: [Neural Network NARMA Control of a Gyroscopic
Inverted Pendulum][1]


  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.4988&rep=rep1&type=pdf",,2013-10-13 18:42:03.260
186117,57400,7155.0,5,,CC BY-SA 3.0,fccab00c-9736-4b45-b698-3d47295079a1,"Define neural network to be $f$, time-series to be $x$, lag order to be $n$ and forecast horizon to be $h$.

$ f(x_{t-1}, x_{t-2},..,x_{t-n}) = [x_t, x_{t+1},..,x_{t+h}]$

Assume you have the following time series,

    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

You define $n=2$, $h=1$.

Your inputs for that time-series are circulant matrix like.

x = 

    [[ 1, 0],
     [ 2, 1],
     [ 3, 2],
     [ 4, 3],
     [ 5, 4],
     [ 6, 5],
     [ 7, 6],
     [ 8, 7]]

Your outputs are

y = 

    [2, 3, 4, 5, 6, 7, 8, 9]

So the length of your input layer is given by $n$, the length of your output layer is given by $h$, where your first input neuron is $x_{t-1}$ and your last input in $x_{t-n}$. Same goes for the forecast horizon.

Instead of having multiple outputs for the forecast horizon, you can use a forecast horizon of 1 then recurse on the predictions to obtain any forecast horizon you want.

For classic parametric stationary time series models the limit of the recursive behaviour of the system is well-studied.

Your problem is a little more involved though. You have inputs and outputs of the system and you want the predict outputs to follow some reference trajectory.

One solution is to use Narma-L2, which approximates the system by linear feedback using two neural networks. Define control inputs to be $c$ and production outputs to be $p$. Define reference production outputs to be $r$

You train two neural networks of the forms $g(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n}) = c_{t}$ and $k(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n}) = p_{t}$.

The prediction for control inputs is then $c_t = \frac{r - k(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n})}{g(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n})}$

Also, neural networks are a PITA. There's plenty of good nonparametric regression models that are easier to train, like Gaussian Process Regression for instance.

See: [Neural Network NARMA Control of a Gyroscopic
Inverted Pendulum][1]


  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.4988&rep=rep1&type=pdf",deleted 2 characters in body,2013-10-13 18:48:04.373
186120,57401,15377.0,3,,CC BY-SA 3.0,fb0e59dc-b1c0-4cef-bf5e-b1c54818d119,<random-variable>,,2013-10-13 19:40:09.530
186119,57401,15377.0,1,,CC BY-SA 3.0,fb0e59dc-b1c0-4cef-bf5e-b1c54818d119,A question on continuous random variable,,2013-10-13 19:40:09.530
186118,57401,15377.0,2,,CC BY-SA 3.0,fb0e59dc-b1c0-4cef-bf5e-b1c54818d119,"Let say, I have 2 continuous random variables X1 & X2. Both have same location parameters. Other parameters may be same or may not.

Now say, the q1-th quantile of X1 is less than the q1-th quantile of x2. But the q2-th quantile of x1 is more than the q2th quantile of x2.

My question is, is that possible? Is there any example of x1 & x2 which have that property?

I will be really grateful if someone can give me some pointer.

Best regards,",,2013-10-13 19:40:09.530
186121,57167,21952.0,5,,CC BY-SA 3.0,1c8b7396-c86d-48f3-8bc1-1da43e0f1665,"I have the following relationships

logY ~ logX1 + logX2 + logX3 + logX4 + logX5 

and

X1 ~ Z1 + Z2 + Z3 + Z4 + Z5

X2 ~ Z1 + Z2 + Z3 + Z4 + Z5

X3 ~ Z1 + Z2 + Z3 + Z4 + Z5



where Y and Z1, Z2, Z3, Z4, Z5 are endogenous (Say while the Z's play a role in determining Y, the values of Z's are fixed depending upont he values of Y - Kind of like advertising expense has an impact on sales revenue but at the same time managers determine the advertisement expense on the expected sales revenue). So all the variable are changing simultaneously. Can anyone help me on how I can estimate this relationship? I also have instruments for each of the Z's (lagged variables have been treated as instruments and I have the previous year data for the problem as well. Thank you for all your help and suggestions.
",added 20 characters in body,2013-10-13 19:43:04.407
186122,57393,14850.0,5,,CC BY-SA 3.0,57789e06-2c0a-4cac-ac4c-67bde15ff2dd,"It appears that you're using <code>nls</code>.

By typing 

    ?summary.nls

you can read about the output.

Estimates and standard errors are estimated by the Gauss-Newton algorithm (if the <code>nls</code> defaults are used)

The P-values are the results of a two sided test of whether the parameters are zero or not. 

You can check the exact calculations used to create the output shown by typing:

    stats:::summary.nls",deleted 24 characters in body,2013-10-13 20:02:59.907
186123,57402,7229.0,2,,CC BY-SA 3.0,0ade767c-13eb-4c64-9951-623b65a1a1b3,"I don't see clearly behind the lines but it seems to me that there are too much data points. 

Since you want to show the regional homogeneity and not exactly stations, I'd suggest you firstly to group them spatially. For example, overlay by a ""fishnet"" and compute average measured value in every cell (at every time moment). If you place these average values in the cell centers this way you rasterize the data (or you can compute also mean latitude and longitude in every cell if you don't want overlaying lines). Or to average inside administrative units, whatever. Then for these new averaged ""stations"" you can calculate correlations and plot a map with smaller number of lines.

![enter image description here][1]

This can also remove those random single high-correlation lines going through all area.


  [1]: https://i.stack.imgur.com/EnuPm.png",,2013-10-13 20:29:43.190
186124,57403,20062.0,2,,CC BY-SA 3.0,f8fe78fc-d97c-41c0-b5f8-2daefa063682,"**Mantel's test widely occur in biological studies** in which is used to
examine the correlation between spatial distribution of animals (position in space) with f.e. their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (*PNAS, Animal Behaviour, Molecular Ecology...*)

Recently I fabricated some patterns which may occur in nature, but Mantel's test **seems to be quite useless.**

**Example data:** *(compressed as possible)*

    r.gen<-seq(-100,100,5)
    r.val<-sample(r.gen, 289, replace=TRUE)
    z10<-rep(0, times=10)
    z11<-rep(0, times=11)
    r5<-c(5,15,25,15,5)
    r71<-c(5,20,40,50,40,20,5)
    r72<-c(15,40,60,75,60,40,15)
    r73<-c(25,50,75,100,75,50,25)
    rbPal <- colorRampPalette(c(""blue"",""red""))
    my.data<-data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
    c1 = c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
    z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
    seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
    seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
    seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
    seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
    replace=TRUE))
    
    # adding colors
    my.data$Col1 <- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
    my.data$Col2 <- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
    my.data$Col3 <- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
    my.data$Col4 <- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
    my.data$Col5 <- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]

Creating matrix of geographical distances (for Moran's I is inversed):

    point.dists <- dist(cbind(my.data$x,my.data$y))
    point.dists.inv <- 1/point.dists
    point.dists.inv<-as.matrix(point.dists.inv)
    diag(point.dists.inv) <- 0

Plot creation:

    X11(width=12, height=6)
    par(mfrow=c(2,5))
    par(mar=c(1,1,1,1))


    library(ape)
    for (i in 3:7) {
    my.res<-mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
    plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
    ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
    text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
    2), ""\n p.value ="", round(my.res$p, 3)))
    my.res<-Moran.I(my.data[ ,i], point.dists.inv)
    text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
    3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
    round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
    }
    
    par(mar=c(5,4,4,2)+0.1)
    
    for (i in 3:7) {
    plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
    ylab=""behavioural distance"")
    }

***Imaginary situation:*** Suppose you have orchard (17 x 17 trees) and on each tree a crow is sitting. Levels of ""noise"" for each crow are given and you are searching for pattern in distribution.

***There are 5 possibilities:***

 1. **""Birds of a feather flock together.""** The more similar crows are, the
    smaller is geographical distance between them **(single cluster)**.
    
 2. **""Birds of a feather flock together.""** Also the more similar crows
    are, the smaller is geographical distance between them **(multiple
    clusters)** but, one cluster of noisy crows has no knowledge about the
    existence of second cluster (oterwise they would form one big
    cluster).
    
 3. **""Monotonical trend occur""**. 
    
 4. **""Opposites attract each other""**. Similar crows can not withstand each
    other.
    
 5. **""Random pattern""** - the level of noise has no significant effect on
    spatial distribution.

Under each case are given plot of points from which the Mantel test computes correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).

![enter image description here][1]


  [1]: https://i.stack.imgur.com/TWQqa.png

**Why scientists do not use Moran's I instead? Is there some hidden reason I do not see?
And if there is such reason,how can I know (how different the hypotheses must be constructed) to appropriately use Mantel's or Moran's I test? A real life example will be helpful.**


P.S. if you google each tests, in these examples are both used on the exact same data
and exact the same hypothesis...**not very helpful**

Mantel test: http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm

Moran's I: http://www.ats.ucla.edu/stat/r/faq/morans_i.htm


",,2013-10-13 20:35:21.350
186126,57403,20062.0,3,,CC BY-SA 3.0,f8fe78fc-d97c-41c0-b5f8-2daefa063682,<r><correlation><spatial>,,2013-10-13 20:35:21.350
186125,57403,20062.0,1,,CC BY-SA 3.0,f8fe78fc-d97c-41c0-b5f8-2daefa063682,What is the point in using Mantel's test instead of Moran's I?,,2013-10-13 20:35:21.350
186129,57404,19547.0,3,,CC BY-SA 3.0,3ed8d6ba-09d0-4f22-968d-42ba1d8902f5,<hypothesis-testing>,,2013-10-13 20:50:28.997
186127,57404,19547.0,2,,CC BY-SA 3.0,3ed8d6ba-09d0-4f22-968d-42ba1d8902f5,I was wondering if it is possible to apply the method of hypothesis testing to real life. For example if someone can use it for decision making. I have always used this method for homework problems but maybe we can use this method as an aid in decision making. Therefore we could somehow know for example the probability of rejecting wrongly an alternative decision. What do you think on that? And it would be nice if someone can give an example if he thinks that this can be done.,,2013-10-13 20:50:28.997
186128,57404,19547.0,1,,CC BY-SA 3.0,3ed8d6ba-09d0-4f22-968d-42ba1d8902f5,Hypothesis Testing applied to real life,,2013-10-13 20:50:28.997
186132,57405,19750.0,3,,CC BY-SA 3.0,3c4db1a9-6e40-4fdf-a292-1a88b29339b1,<bayesian><model-selection>,,2013-10-13 21:00:08.203
186131,57405,19750.0,1,,CC BY-SA 3.0,3c4db1a9-6e40-4fdf-a292-1a88b29339b1,Bayesian variable selection,,2013-10-13 21:00:08.203
186130,57405,19750.0,2,,CC BY-SA 3.0,3c4db1a9-6e40-4fdf-a292-1a88b29339b1,"Chapter 13 of Kevin Murphy's book [Machine Learning: A Probabilistic Perspective][1] discusses Sparse Linear Models. After a short introduction on the benefits of sparse models, he introduces the following problem:

![enter image description here][2]


  [1]: http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020
  [2]: https://i.stack.imgur.com/AciWF.png

How does he derive equation 13.1 above? i.e. why does it take that form, and what is $f$ supposed to represent here?",,2013-10-13 21:00:08.203
186133,57404,668.0,10,,,c9900018-0e1d-4a42-a042-045c302a0e6a,"{""OriginalQuestionIds"":[6966],""Voters"":[{""Id"":919,""DisplayName"":""whuber""}]}",101,2013-10-13 21:02:14.880
186134,57273,,25,,,6991772e-6c6f-48e9-b50b-1b3e185fa270,,http://twitter.com/#!/StackStats/status/389501157363957760,2013-10-13 21:21:27.260
186135,57406,22507.0,2,,CC BY-SA 3.0,bff2b055-8c6a-4749-be10-5b1cb2ef9037,"Machine learning often deals with optimization of a function which has many local minimas. Feedforward neural networks with hidden units is a good example. Whether these functions are discrete or continuous, there is no method which achieves a global minimum and stops. It is easy to prove that there is no general algorithm to find a global minimum of a continuous function even if it is one-dimensional and smooth (has infinitely many derivatives).  In practice, all algorithms for learning neural networks stuck into a local minimum.  It is easy to check this: create a random neural network, make a big set of its responses to random inputs, then try to learn another neural network with the same architecture to copy the responses.  While the perfect solution exists, neither backpropagation not any other learning algorithm will be able to discover it, starting from a random set of weights.

Some learning methods, like simulated annealing or genetic algorithms, explore many local minimas.  For continuous functions there are methods like gradient descent, which  find the closest local minimum.  They are much faster, thats why they are widely used in practice.  But given enough time, the former group of methods outperforms the later in terms of training set error.  But with reasonable time constraints, for real world problems, the latter group is usually better.

For some models, like logistic regression, there is one local minimum, the function is convex, the minimization converges to the minimum, but the models themselves are simplistic.

Thats the bitter truth.

Note also that proof of convergence and proof of convergence to the best solution are two different things.  K-means algorithm is an example of this. 

Finally, for some models we don't know how to learn at all. For example, if the output is an arbitrary computable function of inputs, we don't know good algorithms which, in reasonable time, find a Turing or equivalent machine implementing this function.  For instance, if f(1)=2, f(2)=3, f(3)=5, f(4)=7, ..., f(10)=29 (prime numbers), we don't know any learning algorithm which would be able to predict that f(11)=31, unless it already knows the concept of prime numbers.",,2013-10-13 21:24:53.867
186136,57324,5045.0,5,,CC BY-SA 3.0,3ba4788f-0466-485f-bd5f-20395f02ea74,"Take a look at the tooth brushing example at the very start of Chapter 14 of Andrew Vickers' book [What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics][1]. It starts on page 57 or you can use the table of contents button in the bottom *left* corner to find it. 

Here's an excerpt:

> [I]f you do nothing else, please try to remember the following
> sentence: “the $p$-value is the probability that the data would be at
> least as extreme as those observed, if the null hypothesis were true.”
> Though I’d prefer that you also understood it—about which, teeth
> brushing.
> 
> I have three young children. In the evening, before we get to bedtime
> stories (bedtime stories being a nice way to end the day), we have to
> persuade them all to bathe, use the toilet, clean their teeth, change
> into pajamas, get their clothes ready for the next day and then
> actually get into bed (the persuading part being a nice way to go
> crazy). My five-year-old can often be found sitting on his bed, fully
> dressed, claiming to have clean teeth. The give-away is the bone dry
> toothbrush: he says that he has brushed his teeth, I tell him that he
> couldn’t have.
> 
> My reasoning here goes like this: the toothbrush is dry; it is
> unlikely that the toothbrush would be dry if my son had cleaned his
> teeth; therefore he hasn’t cleaned his teeth. Or using
> statistician-speak: here are the data (a dry toothbrush); here is a
> hypothesis (my son has cleaned his teeth); the data would be unusual
> if the hypothesis were true, therefore we should reject the
> hypothesis. 
> 
> [...]
> 
> So here is what to parrot when we run into each other at a bar and I
> still haven’t managed to work out any new party tricks: “The $p$-value
> is the probability that the data would be at least as extreme as those
> observed, if the null hypothesis were true.” When I recover from
> shock, you can explain it to me in terms of a toothbrush (“The
> probability of the toothbrush being dry if you’ve just cleaned your
> teeth”).

The other thing I really like about this example is that it also explains that failing to reject the null does not mean the null is necessarily true. Vickers writes that his son has now worked out the trick and has taken to running his toothbrush under the tap for a second or two before heading to bed. Just because the toothbrush is wet (and the data is consistent with the null hypothesis), it does not mean that his son has cleaned his teeth. 

  [1]: http://www.pearsonhighered.com/vickers/",added 2237 characters in body,2013-10-13 21:27:49.040
186137,57406,22507.0,5,,CC BY-SA 3.0,0a2c7dfd-79e9-4f4c-ab25-e4d472e27e8c,"Machine learning often deals with optimization of a function which has many local minimas. Feedforward neural networks with hidden units is a good example. Whether these functions are discrete or continuous, there is no method which achieves a global minimum and stops. It is easy to prove that there is no general algorithm to find a global minimum of a continuous function even if it is one-dimensional and smooth (has infinitely many derivatives).  In practice, all algorithms for learning neural networks stuck into a local minimum.  It is easy to check this: create a random neural network, make a big set of its responses to random inputs, then try to learn another neural network with the same architecture to copy the responses.  While the perfect solution exists, neither backpropagation not any other learning algorithm will be able to discover it, starting from a random set of weights.

Some learning methods, like simulated annealing or genetic algorithms, explore many local minimas.  For continuous functions there are methods like gradient descent, which  find the closest local minimum.  They are much faster, thats why they are widely used in practice.  But given enough time, the former group of methods outperforms the later in terms of training set error.  But with reasonable time constraints, for real world problems, the latter group is usually better.

For some models, like logistic regression, there is one local minimum, the function is convex, the minimization converges to the minimum, but the models themselves are simplistic.

Thats the bitter truth.

Note also that proof of convergence and proof of convergence to the best solution are two different things.  K-means algorithm is an example of this. 

Finally, for some models we don't know how to learn at all. For example, if the output is an arbitrary computable function of inputs, we don't know good algorithms which, in reasonable time, find a Turing or equivalent machine implementing this function.  For instance, if f(1)=2, f(2)=3, f(3)=5, f(4)=7, ..., f(10)=29 (ten first primes), we don't know any learning algorithm which would be able to predict, in reasonable time, that f(11)=31, unless it already knows the concept of prime numbers.",added 3 characters in body,2013-10-13 21:30:45.657
186138,57407,594.0,2,,CC BY-SA 3.0,0103da2a-a50d-48cc-8404-8a799f0b6086,"Since you appear to doubt the example offered, I have included a diagram. As Michael Mayer said, two normal distributions centered around 0, one with larger variance, is sufficient.

In the diagram, we compare the 0.1 and the 0.9 quantiles for $\sigma=1$ (blue) and $\sigma=0.8$ (dark orange)

![normcdfs sigma=1, sigma=0.8][1]

Michael Mayer's example fulfills the requirements of your question with $q_1=0.1$, $q_2=0.9$ and $X_1$ being the one with larger variance.

  [1]: https://i.stack.imgur.com/pT43v.png
",,2013-10-13 22:09:36.687
186139,57408,7155.0,2,,CC BY-SA 3.0,513c9664-a49a-4184-8b6d-f0a6752b6dca,"The divisor is just a normalizing constant, so we can ignore it for the moment. If we plug in $f(\gamma)$ it simplifies to $p(D|\gamma)p(\gamma)$, by Bayes' rule is equal to $p(\gamma|D)p(D)$. Now since $p(D)$ isn't a function of $\gamma$ it falls into the normalizing constant. Thus it simplifies to $p(\gamma|D)$.

This expansion seems pointless until you realize that the $\gamma$ that $p(\gamma|D)$ is at its maximum is the same $\gamma$ that $f(\gamma)$ is at its minimum. So we can study $f(\gamma)$ by itself.",,2013-10-13 22:17:17.043
186214,57428,20470.0,5,,CC BY-SA 3.0,db7a9836-5766-421a-9970-ebc244a2e876,"I am trying to implement the ID3 algorithm on a data set . However , all attributes are continuous and can have values between 1-10. I found that we have to specify the bin intervals for discretization but couldn't understand how to do this exactly . 

Can some one explain on how to do this? The [data set][1] I am using is Breast Cancer Data from Wisconsin hospitals.


  [1]: http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data",grammar / rephrasal,2013-10-14 09:23:33.433
186213,57428,,24,,CC BY-SA 3.0,db7a9836-5766-421a-9970-ebc244a2e876,,"Proposed by 28740 approved by 805, 88 edit id of 5605",2013-10-14 09:23:33.433
186140,57380,594.0,5,,CC BY-SA 3.0,0c5da6a6-6c47-4e94-a493-13a739ff0d52,"I'm trying to predict a response variable in linear regression that should be always positive (cost per click). It's a monetary amount. In adwords, you pay google for clicks on your ads, and a negative number would mean that google pays you when people clicked :P

The predictors are all continuous values. The Rsquared and RMSE are decent when compared to other models, even out-of-sample:

      RMSE        Rsquared 
    1.4141477     0.8207303

I cannot rescale the predictions, because it's money, so even a small rescaling factor could change costs significantly.

As far as I understand, for the regression model there's nothing special about zero and negative numbers, so it finds the best regression hyperplane no matter whether the output is partly negative.

This is a very first attempt, using all variables I have. So there's room for refinement.

Is there any way to tell the model that the output cannot be negative?
",added 20 characters in body,2013-10-13 22:19:36.263
186142,57409,10060.0,2,,CC BY-SA 3.0,71e5fb88-481e-4227-8145-9e7b060c66dd,"If the domain all have a base of 100, a stacked bar chart may be suitable.

![enter image description here][1]

  [1]: https://i.stack.imgur.com/BaEoD.png",,2013-10-13 22:36:21.843
186143,57399,6813.0,5,,CC BY-SA 3.0,2c159a88-eb82-488f-ba4f-b8b95497442b,"Recently, I was wondering about calculating the probability of a given individual in a given population ""knowing"" (let's say, present in individual's friend set) at least one person with a given trait A and at least one person with another given trait, B; where it is possible that any number of people in the given individual's friend set can possess both traits.

For example, using genetic traits, in a given population, how could one calculate the probability that a given individual in a given population ""knows"" at least one person with grey eye colour and at least one person who is greater than 200cm tall; where, naturally, it is possible that any number of people in the friend set can possess grey eye colour *and* be greater than 200cm tall.

I have developed a sort of model, but it may not be correctly specified; it is as follows:

**Assumptions and Qualifications:**

 - First of all, for simplicity, let's assume that we define ""knowing"" as mutually connected friends on an online social network.
 - Secondly, the frequencies of genetic traits are (a) going to be determined by ethnicity of a given population as well as environmental factors (nutrition, healthcare) and (b) unlikely to be independently distributed across a given individual's friend set (e.g. family members will have greater genetic similarity); however, for this problem, let's adopt a simple model where both of the above conditions are violated.
 - Thirdly, assume that the individual's friend set provides a microcosmic representation of society; this facilitates a frequentist generation of probabilities from the instance rate in the population.
 - Finally, instances of genetic traits in the population are fabricated, but are used to generate probabilities for the examples.

**Model Formulation:**

I have reasoned that a binomial random distribution can be applied to the probability of each genetic trait, where a ""success"" is defined as an individual in the friend set possessing that genetic trait.

Thus for trait A, we have:

$$P(A=k) = \binom{N}{k}p_{a}^{k}(1-p_{a})^{N-k}$$

and, for trait B:

$$P(B=k) = \binom{N}{k}p_{b}^{k}(1-p_{b})^{N-k}$$

where $N$ is number of friends in the friend set, $k$ is number of people containing the genetic trait and $p_{x}$ is the probability associated with possessing genetic trait $X$.

Because the scenario is concerned with the probability of *at least one* person possessing trait A and *at least one* person possessing trait B, it is easier to find the complement of no people in a friend set containing the trait; for both traits:

$$P(A >= 1) = 1 - P(A = 0)$$

and 

$$P(B >= 1) = 1 - P(B = 0)$$

Furthermore, we know that probability of an intersection of events is given by:

$$\mathbb{P}(A \cap B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)\mathbb{P}(A)$$

However, because we are assuming independence between genetic traits, $\mathbb{P}(A)$ and $\mathbb{P}(B)$ are independent, thus:

$$\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$$

Combining the information above, we get the following model for the above scenario:

$$\mathbb{P}(A >=1, B >=1) = \left(1-\binom{N}{k}p_{a}^{k}(1-p_{a})^{N-k}\right)\left(1-\binom{N}{k}p_{b}^{k}(1-p_{b})^{N-k}\right)$$

So, for the original example above, assuming a friend set of size $N = 300$, the instance of trait A in the population is $\frac{1}{800}$ and the instance of trait B in the population is $\frac{1}{5000}$; according to the model, we get the final probability:

$$\mathbb{P}(A >=1, B >=1)$$

$$ = \left(1-\binom{300}{0}\left(\frac{1}{800}\right)^{0}\left(\frac{799}{800}\right)^{300}\right)\left(1-\binom{300}{0}\left(\frac{1}{5000}\right)^{0}\left(\frac{4999}{5000}\right)^{300}\right)$$

$$\approx 0.018 = 1.8\%$$

Does this model seem reasonable given the assumptions?

Assuming this model is *not* correctly specified, maybe somebody could provide a more accurate representation.",added 75 characters in body,2013-10-13 22:42:12.770
186145,57410,22662.0,2,,CC BY-SA 3.0,904f8820-3e23-4cf7-b5ac-70e3915743d9,"The joint entropy is the amount of information we get when we observe X and Y at the same time, but what would happen if we don't observe them at the same time.

For example, when i toss a coin, if i got tails i will only observe the variable X, but if i got heads i will only observe the variable Y. How could i find the entropy?",,2013-10-13 22:51:56.027
186146,57410,22662.0,1,,CC BY-SA 3.0,904f8820-3e23-4cf7-b5ac-70e3915743d9,Joint entropy of two random variables,,2013-10-13 22:51:56.027
186144,57410,22662.0,3,,CC BY-SA 3.0,904f8820-3e23-4cf7-b5ac-70e3915743d9,<entropy>,,2013-10-13 22:51:56.027
186147,57195,22547.0,5,,CC BY-SA 3.0,3bc0266b-79cf-4130-8bd0-1e2718e27c6b,"I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). 

I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. 

## Data set ##

To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.

![enter image description here][1]

Straight away you see one problem: there are lots of sites that have similar identifiers or are very close, but without more information we have no reason to join data sets (FWIW, I identify them using both the USAF and WBAN codes). Looking deeper in to the data, I saw that they have similar coordinates, but not identical, and the elevations of the sites differ. So, I have to treat them as separate stations, because I don't know better, and the data inevitably contains pairs of stations that are very close to each other.

## Preliminary Analysis ##

I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. 

![correlation between daily data during each calendar month][2]

I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.

### Problems ###
Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. 

I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
![enter image description here][3]

The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.

I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.

### Questions ###

I'm learning my way into this field and R at the same time, and would appreciate suggestions on:

 1. What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.
 2. Are there more appropriate methods to show the correlation between multiple data sets separated in space?
 3. ... in particular, methods that are easy to show results from visually?
 4. Are any of these implemented in R?
 5. Do any of these approaches lend themselves to automation?


  [1]: https://i.stack.imgur.com/aZm4N.jpg
  [2]: https://i.stack.imgur.com/X4YZI.jpg
  [3]: https://i.stack.imgur.com/NWzm2.jpg","Added more information, clarified question",2013-10-14 00:19:48.887
186148,57411,20473.0,2,,CC BY-SA 3.0,b4d04a36-7744-4806-9795-04e1a324e4d1,"Entropy (joint entropy included), _is a property of the distribution_ that a random variable follows. The available sample (and hence the timing of observation) plays no role in it.  

Copying for Cover & Thomas, the joint entropy $H(X,Y)$ of two discrete random variables $X, Y,$ with joint distribution $p(x,y)$, is defined as

$$H(X,Y) = - \sum_{S_X}\sum_{S_Y}p(x,y)\log p(x,y) $$

Examine the expression: the sums are taken over _all possible values_ of $X$ and $Y$, i.e. over all the values that belong to the support of each r.v. ($S_X$ and $S_Y$ respectively), irrespective of whether some of these values may not materialize or be observed in a sample. What we actually observe, or when, plays no role, in calculating entropy, and joint entropy in particular.  

Turning to your specific example: The side of a coin itself can not be modeled as a random variable. A random variable maps _events_ into real numbers. The side of a coin is not an event. _Observing_ one of the two sides is an event.  _Not observing_ a side, is an event. So let's define a random variable $X$ by ""$X$ takes the value $1$ if heads is observed, $0$ otherwise"". And define $Y$ by ""$Y$ takes the value $1$ if tails is observed, $0$ otherwise"". Assume the coin is fair. The joint distribution of these two random variables is then described by 
$$\begin{align}
P(X=1,Y=1) &= 0 \\
P(X=1,Y=0) &= 0.5 \\
P(X=0,Y=1) &= 0.5 \\
P(X=0,Y=0) &= 0
\end{align}$$

As usual, we consider the distribution at non-zero values, so 

$$H(X,Y) = - 0.5\log(0.5) - 0.5\log(0.5) $$

and using base-2 for the logarithm we get

$$H(X,Y) = - 0.5(-1) - 0.5(-1) = 1 $$

Finally, you can easily find that the entropy of $X$ (and likewise for $Y$) is 
$$H(X) = - \sum_{S_X}p(x)\log p(x) = - 0.5(-1) - 0.5(-1) = 1 $$

So in this case $H(X,Y) = H(X) = H(Y)$. But the general expression for the decomposition of joint entropy is 

$$H(X,Y) = H(X) + H(Y\mid X) = H(Y) + H(X\mid Y)$$ 

where $H(Y\mid X)$ and  $H(X\mid Y)$ are conditional entropies. Then we conclude that $H(Y\mid X) = H(X\mid Y) = 0$ in this case. The intuition is straightforward: given $X$ what has happened to $Y$ is certain (and likewise in reverse), so conditional entropy is zero.




",,2013-10-14 00:19:58.667
186149,57276,,25,,,48a1e4f1-9329-47c8-b939-6ed34c52ce28,,http://twitter.com/#!/StackStats/status/389546456044302336,2013-10-14 00:21:27.263
186150,57195,22547.0,5,,CC BY-SA 3.0,eb0d5c7a-9887-4322-a0cf-386ff41cf4d4,"I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). 

I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. 

## Data set ##

To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.

![enter image description here][1]

Straight away you see one problem: there are lots of sites that have similar identifiers or are very close. FWIW, I identify them using both the USAF and WBAN codes. Looking deeper in to the metadata I saw that they have different coordinates and elevations, and data stop at one site then start at another. So, because I don't know any better, I have to treat them as separate stations. This means the data contains pairs of stations that are very close to each other.

## Preliminary Analysis ##

I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. 

![correlation between daily data during each calendar month][2]

I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.

### Problems ###
Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. 

I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
![enter image description here][3]

The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.

I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.

### Questions ###

I'm learning my way into this field and R at the same time, and would appreciate suggestions on:

 1. What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.
 2. Are there more appropriate methods to show the correlation between multiple data sets separated in space?
 3. ... in particular, methods that are easy to show results from visually?
 4. Are any of these implemented in R?
 5. Do any of these approaches lend themselves to automation?


  [1]: https://i.stack.imgur.com/aZm4N.jpg
  [2]: https://i.stack.imgur.com/X4YZI.jpg
  [3]: https://i.stack.imgur.com/NWzm2.jpg",Fixed an annoying paragraph,2013-10-14 00:27:35.263
186153,57412,22665.0,3,,CC BY-SA 3.0,b5ffea2e-53f7-43f9-beca-9f08924391fa,<bayesian><prediction>,,2013-10-14 00:46:15.680
186152,57412,22665.0,1,,CC BY-SA 3.0,b5ffea2e-53f7-43f9-beca-9f08924391fa,Understanding Bayesian Predictive Distributions,,2013-10-14 00:46:15.680
186151,57412,22665.0,2,,CC BY-SA 3.0,b5ffea2e-53f7-43f9-beca-9f08924391fa,"I'm taking an Intro to Bayes course and I'm having some difficulty understanding predictive distributions. I understand why they are useful and I'm familiar with the definition, but there are some things I don't quite understand. 


**1) How to get the right predictive distribution for a vector of new observations**

Suppose that we have built a sampling model $p(y_i | \theta)$ for the data and a prior $p(\theta)$. Assume that the observations $y_i$ are conditionally independent given $\theta$. 

We have observed some data $\mathcal{D} = \{y_1, y_2, \, ... \, , y_k\}$, and we update our prior $p(\theta)$ to the posterior $p(\theta | \mathcal{D})$. 

If we wanted to predict a vector of new observations $\mathcal{N} = \{\tilde{y}_1, \tilde{y}_2, \, ... \, , \tilde{y}_n\}$, I think we should try to get the posterior predictive using this formula
$$
p(\mathcal{N} | \mathcal{D}) = \int p(\theta | \mathcal{D}) p ( \mathcal{N} | \theta) \, \mathrm{d} \theta = \int p(\theta | \mathcal{D}) \prod_{i=1}^n p(\tilde{y}_i | \theta) \, \mathrm{d} \theta,
$$
which is not equal to
$$
\prod_{i=1}^n  \int p(\theta | \mathcal{D}) p(\tilde{y}_i | \theta) \, \mathrm{d} \theta,
$$
so the predicted observations are not independent, right?

Say that $\theta | \mathcal{D} \sim$ Beta($a,b$) and $p(y_i | \theta) \sim$ Binomial($n, \theta$) for a fixed $n$. In this case, if I wanted to simulate 6 new $\tilde{y}$, if I understand this correctly, it would be wrong to simulate 6 draws independently from the Beta-Binomial distribution that corresponds to the posterior predictive for a single observation. Is this correct? I don't know how to interpret that the observations are not independent marginally, and I'm not sure I understand this correctly.

**Simulating from posterior predictives**

Many times when we simulate data from the posterior predictive we follow this scheme:

For $b$ from 1 to $B$:

1) Sample $\theta^{(b)}$ from $p(\theta | \mathcal{D})$.

2) Then simulate new data $\mathcal{N}^{(b)}$ from $p(\mathcal{N} | \theta^{(b)})$. 

I don't quite know how to prove this scheme works, although it looks intuitive. Also, does this have a name? I tried to look up a justification and I tried different names, but I had no luck.

**Thanks!**


",,2013-10-14 00:46:15.680
186156,57413,22666.0,3,,CC BY-SA 3.0,b5fd25fe-d85b-482f-932a-91fcebcb248c,<panel-data>,,2013-10-14 01:03:58.377
186154,57413,22666.0,1,,CC BY-SA 3.0,b5fd25fe-d85b-482f-932a-91fcebcb248c,time persistence in panel data,,2013-10-14 01:03:58.377
186155,57413,22666.0,2,,CC BY-SA 3.0,b5fd25fe-d85b-482f-932a-91fcebcb248c,"I am using dynamic model with panel quarter data using Stata. And my sample contain  16 nations from 2000 to 2010.
Is there an approximated number of observations in the panel data to be considered as a time persistent process? ",,2013-10-14 01:03:58.377
186159,57414,22667.0,3,,CC BY-SA 3.0,a03803f5-2902-4799-8446-b97c5187dad1,<statistical-significance><sampling><python>,,2013-10-14 01:17:13.023
186158,57414,22667.0,1,,CC BY-SA 3.0,a03803f5-2902-4799-8446-b97c5187dad1,"I have a discrete distribution and want to know to what extent other samples differ from it, what is the right test?",,2013-10-14 01:17:13.023
186157,57414,22667.0,2,,CC BY-SA 3.0,a03803f5-2902-4799-8446-b97c5187dad1,"This is kind of a basic stats question, but I want to make sure I am doing this right.

I have a distribution of objects. Specifically: 
    `array([   6072.,  112673.,  126874., 44366., 5384., 14697., 20323., 68197., 98024.,39483.,     103990., 18556., 32930., 23551., 6897.])`


I then have a lot of samples like [1,4,0,0,0,0...] (same length) and I'd like to know how far the samples are from the distribution above. Correlation doesn't really do it. 
[4,0,0,0...] should be as far away as [32,0,0,0,..] but with lower significance ",,2013-10-14 01:17:13.023
186160,57415,22507.0,2,,CC BY-SA 3.0,c841ba15-c3ce-4c61-ae92-c6472cf5bc86,"If you want to do logistic regression, a simple approach is:

- for each continuous feature with missing data, replace all missing values by the average or median value for this feature, and create one more boolean feature which indicates whether the data is missing or not
- for each unordered categorical feature with missing data, put all missing values into a new category",,2013-10-14 02:23:17.417
186168,57418,19043.0,2,,CC BY-SA 3.0,1c750ec5-1aa1-4e8f-9aff-861c64148e74,"I am comparing measurements on a test group relative to a control group in three different environmental conditions. I am interested in both differences between environmental conditions and differences between test and control groups. I ran a two-way ANOVA with an interaction term and looked at pairwise comparisons when terms were significant. 

When the interaction term was significant the Tukey HSD function in R automatically outputs all comparisons. Comparisons between test and test groups on different environmental conditions, comparisons between test and control groups on different environmental conditions and so forth. Needless to say, this resulted in a large number of tests to correct for.

My adviser thinks that I should only do three tests to compare test group to control group on each environmental condition (and then only adjust for three tests). I think that because I am interested in differences between environmental conditions in this study, I should run most tests. If I wasn't interested in differences in environmental conditions it should be a nested ANOVA, right? You can see from graphs that the interaction term come from differences between test and control groups on two ecological sites, but it doesn't seem valid to just only run comparisons between groups you 'suspect' will be different. The ones I'm not sure I care about are differences test and control groups on two different environmental conditions. 

**Is it valid to only run comparisons between groups you are interested in to reduce the number of tests you have to adjust the Tukey HSD p-value for or should you run comparisons on all combinations of groups.**

Thanks for help in advance.",,2013-10-14 04:02:43.010
186167,57418,19043.0,1,,CC BY-SA 3.0,1c750ec5-1aa1-4e8f-9aff-861c64148e74,Do only certain pairwise comparisons after significant interaction in two-way ANOVA,,2013-10-14 04:02:43.010
186169,57418,19043.0,3,,CC BY-SA 3.0,1c750ec5-1aa1-4e8f-9aff-861c64148e74,<anova><multiple-comparisons><tukey-hsd-test>,,2013-10-14 04:02:43.010
186170,57418,19043.0,6,,CC BY-SA 3.0,82e1a7cf-3d64-44db-a76b-385b33fa8e00,<r><anova><multiple-comparisons><post-hoc><tukey-hsd-test>,edited tags,2013-10-14 04:09:01.183
186173,57419,22668.0,3,,CC BY-SA 3.0,e8e95338-ea3a-4e90-9c3b-3c0097e82f6c,<regression><panel-data><traminer>,,2013-10-14 04:45:22.343
186172,57419,22668.0,1,,CC BY-SA 3.0,e8e95338-ea3a-4e90-9c3b-3c0097e82f6c,Weightining using TraMineR,,2013-10-14 04:45:22.343
186171,57419,22668.0,2,,CC BY-SA 3.0,e8e95338-ea3a-4e90-9c3b-3c0097e82f6c,"I have read some posts on weightining. However, I am still unclear on the sort of weights I need to use. I am using data from the Longitudinal Survey of Australian Youth (LSAY). This survey provides longitudinal weights for each survey wave (i.e. correction for sampling error and attrition). Because I have weight variables for each period (10) and I can only specify one of them in TraMiner, I am not sure which I should use. I have read that whether weights for the first or last wave should be used. I have not read any reasons why.

Can anyone provide me some guidance on this issue?

Regards,",,2013-10-14 04:45:22.343
186174,57403,20062.0,5,,CC BY-SA 3.0,44667f13-691a-4752-9016-f28d60d1696e,"**Mantel's test widely occur in biological studies** in which is used to
examine the correlation between spatial distribution of animals (position in space) with for example their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (*PNAS, Animal Behaviour, Molecular Ecology...*)

Recently I fabricated some patterns which may occur in nature, but Mantel's test **seems to be quite useless.**

**Example data:** *(compressed as possible)*

    r.gen<-seq(-100,100,5)
    r.val<-sample(r.gen, 289, replace=TRUE)
    z10<-rep(0, times=10)
    z11<-rep(0, times=11)
    r5<-c(5,15,25,15,5)
    r71<-c(5,20,40,50,40,20,5)
    r72<-c(15,40,60,75,60,40,15)
    r73<-c(25,50,75,100,75,50,25)
    rbPal <- colorRampPalette(c(""blue"",""red""))
    my.data<-data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
    c1 = c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
    z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
    seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
    seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
    seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
    seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
    replace=TRUE))
    
    # adding colors
    my.data$Col1 <- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
    my.data$Col2 <- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
    my.data$Col3 <- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
    my.data$Col4 <- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
    my.data$Col5 <- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]

Creating matrix of geographical distances (for Moran's I is inversed):

    point.dists <- dist(cbind(my.data$x,my.data$y))
    point.dists.inv <- 1/point.dists
    point.dists.inv<-as.matrix(point.dists.inv)
    diag(point.dists.inv) <- 0

Plot creation:

    X11(width=12, height=6)
    par(mfrow=c(2,5))
    par(mar=c(1,1,1,1))


    library(ape)
    for (i in 3:7) {
    my.res<-mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
    plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
    ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
    text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
    2), ""\n p.value ="", round(my.res$p, 3)))
    my.res<-Moran.I(my.data[ ,i], point.dists.inv)
    text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
    3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
    round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
    }
    
    par(mar=c(5,4,4,2)+0.1)
    
    for (i in 3:7) {
    plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
    ylab=""behavioural distance"")
    }

***Imaginary situation:*** Suppose you have orchard (17 x 17 trees) and on each tree a crow is sitting. Levels of ""noise"" for each crow are given and you are searching for pattern in distribution.

***There are 5 possibilities:***

 1. **""Birds of a feather flock together.""** The more similar crows are, the
    smaller is geographical distance between them **(single cluster)**.
    
 2. **""Birds of a feather flock together.""** Also the more similar crows
    are, the smaller is geographical distance between them **(multiple
    clusters)** but, one cluster of noisy crows has no knowledge about the
    existence of second cluster (oterwise they would form one big
    cluster).
    
 3. **""Monotonical trend occur""**. 
    
 4. **""Opposites attract each other""**. Similar crows can not withstand each
    other.
    
 5. **""Random pattern""** - the level of noise has no significant effect on
    spatial distribution.

Under each case are given plot of points from which the Mantel test computes correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).

![enter image description here][1]


  [1]: https://i.stack.imgur.com/TWQqa.png

**Why scientists do not use Moran's I instead? Is there some hidden reason I do not see?
And if there is such reason,how can I know (how different the hypotheses must be constructed) to appropriately use Mantel's or Moran's I test? A real life example will be helpful.**


P.S. if you google each tests, in these examples are both used on the exact same data
and exact the same hypothesis...**not very helpful**

Mantel test: http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm

Moran's I: http://www.ats.ucla.edu/stat/r/faq/morans_i.htm


",added 7 characters in body,2013-10-14 04:50:14.720
186175,57420,306.0,2,,CC BY-SA 3.0,de36620c-5ba4-4198-95a5-a9fbb686d8ca,"First things first. There needs to be greater information given as this does not have a universally correct answer. Different types of distributions have to be looked at with different types of procedures. 

But just to show that yes this is possible, we assume that each of the variables that you have mentioned are normally distributed but the parameters of the normal distributions are different from each other for any given pair.

Now we take n samples of these variables, each sample containing one instance of each of the variables. We group together each of the variables separately and then calculate the correlation coefficients for each pair of the groups. If we cannot reject the hypothesis of these correlation coefficients being zero, we hypothesize that the variables are independent of each other. So we have a set of variables which are independent from each other, but they have different probability distributions.",,2013-10-14 05:05:30.750
186176,57419,594.0,5,,CC BY-SA 3.0,ee55be4e-5e4e-422d-9bd6-3fe8890f6109,"I have read some posts on weighting. However, I am still unclear on the sort of weights I need to use. I am using data from the Longitudinal Survey of Australian Youth (LSAY). This survey provides longitudinal weights for each survey wave (i.e. correction for sampling error and attrition). Because I have weight variables for each period (10) and I can only specify one of them in TraMiner, I am not sure which I should use. I have read that whether weights for the first or last wave should be used. I have not read any reasons why.

Can anyone provide me some guidance on this issue?
",deleted 12 characters in body,2013-10-14 05:42:42.343
186178,57421,22669.0,1,,CC BY-SA 3.0,e4deac19-e258-40f6-aba2-15f23c743082,"ARIMA (0,1,1) or (0,1,0) - or something else?",,2013-10-14 05:55:50.823
186179,57421,22669.0,3,,CC BY-SA 3.0,e4deac19-e258-40f6-aba2-15f23c743082,<time-series><modeling><arima>,,2013-10-14 05:55:50.823
186177,57421,22669.0,2,,CC BY-SA 3.0,e4deac19-e258-40f6-aba2-15f23c743082,"I've just started learning time series so please excuse me if it's painfully obvious; I haven't managed to find the answer elsewhere.

I have a data series showing a pretty obvious trend although it's quite noisy. I can take pretty much any division of the data and run classical tests to show a highly significant difference in means.

I decided to have a look at time series analysis to see if it could help describe the trend. An ARIMA(0,1,1) model comes out with AIC,BIC=34.3,37.3 (Stata), whilst an ARIMA(0,1,0) model comes out with AIC,BIC=55.1,58.1 - so I understand I'm supposed to prefer the (0,1,1) model.

However, the coefficient for the MA(1) is displaying as -0.9999997 (and not showing any p-values). If I try the same in SPSS I get an MA(1) coefficient of 1.000 (I assume SPSS uses opposite signs) with a p-value of 0.990 - does this mean it suggests I drop the term?

My understanding is that the effect of a MA(1) coefficient of -1 is basically to remove the old error term and convert the whole series to a linear trend. Does this mean ARIMA is totally unsuitable for my needs? On the plus side it gives me a sensible value for the trend. If I use the (0,1,0) model then I still get a reasonable value for the trend but it's not significant any more.

Thanks for your help!",,2013-10-14 05:55:50.823
186218,57428,,5,user88,CC BY-SA 3.0,83a273f9-8dd7-4fab-aa03-d24c7a7844f0,"I am trying to implement the ID3 algorithm on a data set. However, all attributes are continuous and can have values between 1-10. I found that we have to specify the bin intervals for discretization but couldn't understand how to do this exactly. 

Can some one explain on how to do this? The [data set][1] I am using is Breast Cancer Data from Wisconsin hospitals.


  [1]: http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data",deleted 3 characters in body; edited title,2013-10-14 09:25:05.803
186220,57382,,4,user88,CC BY-SA 3.0,a35668d6-5197-48bc-8b7c-1f1348fbe40b,Does sampling error include measurement error?,edited title,2013-10-14 09:25:44.370
186180,57422,9007.0,2,,CC BY-SA 3.0,d8d17543-b723-4ab9-b12a-496a57957098,"TraMineR, and more generally sequence analysis, treats each sequence as a whole. Ideally, weights should therefore correct for selection bias of the sample of sequences you consider, and longitudinal weights should be used.
The choice of the weights depends on which sequences you retain for your analysis:

1. If you use only complete sequences until the last wave, then the choice should be the longitudinal weights associated to the last wave, which accounts attrition until the last wave).

2. If you chose to retain all sequences complete until say the 6th wave and admit missing states for the last four waves, then you should chose the weights associated to wave 6.

3. If you retain all sequences, even for those for which you have valid data for the first wave only, then you should use weights of the first wave.

4. In case you select only sequences with no more than say 4 missing states, the choice may be more ambiguous. If most of the four missing states occur in the last four positions, then you could adopt the solution 2 as a good approximation.

Hope this helps.  
",,2013-10-14 05:56:44.773
186181,57423,5637.0,2,,CC BY-SA 3.0,377b0648-7361-496e-839f-2e712619c7e0,"It is difficult to give the right answer without looking at the data. Here are some points that may help you in your modelling. 

The coefficient of MA(1) very close to 1 indicates the sign of overdifferencing.  This means unit root in Moving averages.

My suggestion would be: Check the original series is stationary (visually) or check the presence of unit root. If you observe deterministic trend (eg: linear), add the trend part with time series model. If the original series is stationary build the time series without differencing. ",,2013-10-14 06:12:22.867
186182,57391,2420.0,5,,CC BY-SA 3.0,e2987fb5-fcde-4d72-bec8-fd332af0c4ba,"I'm a software developer and I'll like to learn about neural networks. At this point I've find a problem which I'll like to solve at some point. It is about electrical load forecasting. I'm looking for similar problems and it will be great if I can find some similar examples with solutions. At this point I'm having troubles in finding the right model for the RNN, and more exactly I'm struggling with the input layer. As the output I need the forecast values for each hour.

Any reference to books, links resources or advices are welcome and very appreciated. 

This is the problem that I'll like to solve:

A very small factory, use a number of equipments to produce bread. Some of them are electrical equipments which means that they consume electrical power.  Knowing which equipments will run on the next day, an electricity consumptions forecast can be computed.  

The equipment named E.V. is a special case of equipment. The human operator completes it's values in an empirically manner in order to have a good forecast for the next day. Those values can be positive or negative. 


    +----------------------------------------------------+  
    |equipment name|power| 1h| 2h| 3h| 4h| 5h| 6h| 7h| 8h|  
    +----------------------------------------------------+  
    |Equipment 1   |  2MW| - | - | on| on| on| - | - | - |
    |Equipment 2   |  5MW| - | - | - | on| on| on| - | - |
    |Equipment 3   |  1MW| on| on| on| on| on| on| on| on|
    |E.V.          |     | .1|-.1|-.1| .1|-.2| .1| .1|-.1|
    +--------------+-------------------------------------+
    |total/(forecast)    |1.1| .9|2.9|8.1|7.8|6.1|1.1| .9|
    +--------------+-------------------------------------+
    |real consumption    | .9| .9|2.7|8.2|7.9|3.1|0.8| .7|  
    +--------------+-------------------------------------+

The problem is that the machines are not running at their maximal power, so it will be great if a more exactly forecast can be build. I have data from 2 years back for every day. Also, do you think that date is a good candidate for the input layer?",date related quesion,2013-10-14 06:16:41.540
186183,57278,,25,,,9e94a38e-f2a7-4bf9-82df-2b2d8269b230,,http://twitter.com/#!/StackStats/status/389637058559750144,2013-10-14 06:21:28.587
186184,57391,2420.0,5,,CC BY-SA 3.0,6acfb5c0-357c-4606-b929-7b277e355f9a,"I'm a software developer and I'll like to learn about neural networks. At this point I've find a problem which I'll like to solve at some point. It is about electrical load forecasting. I'm looking for similar problems and it will be great if I can find some similar examples with solutions. At this point I'm having troubles in finding the right model for the RNN, and more exactly I'm struggling with the input layer. As the output I need the forecast values for each hour.

Any reference to books, links resources or advices are welcome and very appreciated. 

This is the problem that I'll like to solve:

A very small factory, use a number of equipments to produce bread. Some of them are electrical equipments which means that they consume electrical power.  Knowing which equipments will run on the next day, an electricity consumptions forecast can be computed.  

The equipment named E.V. is a special case of equipment. The human operator completes it's values in an empirically manner in order to have a good forecast for the next day. Those values can be positive or negative. 


    +----------------------------------------------------+  
    |equipment name|power| 1h| 2h| 3h| 4h| 5h| 6h| 7h| 8h|  
    +----------------------------------------------------+  
    |Equipment 1   |  2MW| - | - | on| on| on| - | - | - |
    |Equipment 2   |  5MW| - | - | - | on| on| on| - | - |
    |Equipment 3   |  1MW| on| on| on| on| on| on| on| on|
    |E.V.          |     | .1|-.1|-.1| .1|-.2| .1| .1|-.1|
    +--------------+-------------------------------------+
    |total/(forecast)    |1.1|0.9|2.9|8.1|7.8|6.1|1.1|0.9|
    +--------------+-------------------------------------+
    |real consumption    |0.9|0.9|2.7|8.2|7.9|3.1|0.8|0.7|  
    +--------------+-------------------------------------+

The problem is that the machines are not running at their maximal power, so it will be great if a more exactly forecast can be build. I have data from 2 years back for every day. Also, do you think that date is a good candidate for the input layer?",edited body,2013-10-14 06:32:43.717
186185,57403,20062.0,5,,CC BY-SA 3.0,5e6dfa0f-fb61-4e74-86fc-8cf4b9f8d1ca,"**Mantel's test widely occur in biological studies** in which is used to
examine the correlation between spatial distribution of animals (position in space) with for example their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (*PNAS, Animal Behaviour, Molecular Ecology...*)

Recently I fabricated some patterns which may occur in nature, but Mantel's test **seems to be quite useless.**

**Example data:** *(compressed as possible)*

    r.gen<-seq(-100,100,5)
    r.val<-sample(r.gen, 289, replace=TRUE)
    z10<-rep(0, times=10)
    z11<-rep(0, times=11)
    r5<-c(5,15,25,15,5)
    r71<-c(5,20,40,50,40,20,5)
    r72<-c(15,40,60,75,60,40,15)
    r73<-c(25,50,75,100,75,50,25)
    rbPal <- colorRampPalette(c(""blue"",""red""))
    my.data<-data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
    c1 = c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
    z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
    seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
    seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
    seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
    seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
    replace=TRUE))
    
    # adding colors
    my.data$Col1 <- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
    my.data$Col2 <- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
    my.data$Col3 <- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
    my.data$Col4 <- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
    my.data$Col5 <- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]

Creating matrix of geographical distances (for Moran's I is inversed):

    point.dists <- dist(cbind(my.data$x,my.data$y))
    point.dists.inv <- 1/point.dists
    point.dists.inv<-as.matrix(point.dists.inv)
    diag(point.dists.inv) <- 0

Plot creation:

    X11(width=12, height=6)
    par(mfrow=c(2,5))
    par(mar=c(1,1,1,1))


    library(ape)
    for (i in 3:7) {
    my.res<-mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
    plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
    ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
    text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
    2), ""\n p.value ="", round(my.res$p, 3)))
    my.res<-Moran.I(my.data[ ,i], point.dists.inv)
    text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
    3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
    round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
    }
    
    par(mar=c(5,4,4,2)+0.1)
    
    for (i in 3:7) {
    plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
    ylab=""behavioural distance"")
    }

***Imaginary situation:*** Suppose you have orchard (17 x 17 trees) and on each tree a crow is sitting. Levels of ""noise"" for each crow are given and you are searching for pattern in distribution.

***There are 5 possibilities:***

 1. **""Birds of a feather flock together.""** The more similar crows are, the
    smaller is geographical distance between them **(single cluster)**.
    
 2. **""Birds of a feather flock together.""** Also the more similar crows
    are, the smaller is geographical distance between them **(multiple
    clusters)** but, one cluster of noisy crows has no knowledge about the
    existence of second cluster (otherwise they would fuse into a one big
    cluster).
    
 3. **""Monotonical trend occur""**. 
    
 4. **""Opposites attract each other""**. Similar crows can not withstand each
    other.
    
 5. **""Random pattern""** - the level of noise has no significant effect on
    spatial distribution.

Under each case are given plot of points from which the Mantel test computes correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).

![enter image description here][1]


  [1]: https://i.stack.imgur.com/TWQqa.png

**Why scientists do not use Moran's I instead? Is there some hidden reason I do not see?
And if there is such reason,how can I know (how different the hypotheses must be constructed) to appropriately use Mantel's or Moran's I test? A real life example will be helpful.**


P.S. if you google each tests, in these examples are both used on the exact same data
and exact the same hypothesis...**not very helpful**

Mantel test: http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm

Moran's I: http://www.ats.ucla.edu/stat/r/faq/morans_i.htm


",added 8 characters in body,2013-10-14 06:56:42.453
186186,57420,306.0,5,,CC BY-SA 3.0,7ba1fcea-62cb-4447-b98e-500632d6aeef,"First things first. There needs to be greater information given as this does not have a universally correct answer. Different types of distributions have to be looked at with different types of procedures. 

But just to show that yes this is possible, we assume that each of the variables that you have mentioned are normally distributed but the parameters of the normal distributions are different from each other for any given pair.

Now we take n samples each of these variables. Then calculate the correlation coefficients for each pair of the variables. If we cannot reject the hypothesis of these correlation coefficients being zero, we hypothesize that the variables are independent of each other. So we have a set of variables which are independent from each other, but they have different probability distributions.",deleted 109 characters in body,2013-10-14 07:13:02.163
186187,57413,1406.0,4,,CC BY-SA 3.0,51b68dd9-d950-4e19-b0aa-279f9c39c540,Time persistence in panel data,edited title,2013-10-14 07:21:24.520
186188,57424,1406.0,2,,CC BY-SA 3.0,7e6a7696-5fd7-41ff-aa97-c4831b878996,"Time series issues, such as unit roots, etc in panel data can be accounted for when there is enough time series dimension for single unit regression estimation. This means at least 30 observations. If you have less, you can only use ideas from time series regressions, such as doing regression on growth rates instead of levels, etc. 

In fact J. Wooldridge in his book ""Econometric Analysis of Cross Section and Panel Data"" recommends to treat all the time series issues as a question of covariance matrix of the unit error term. Translated to Stata parlance, use cluster-robust standard errors for your analysis and you should be ok, with the usual caveat that there are no magical fixes in modelling, i.e. if your model is not sound, no fancy estimation method is going to help you.",,2013-10-14 07:37:48.140
186216,57431,12683.0,5,,CC BY-SA 3.0,a72964dc-3626-4a94-a8a2-d1f335bebd3b,"Statistics doesn't give a special meaning to 'measurement' in the way it does to 'estimate'. (As @Glen said, we 'estimate parameters'.) So it's going to depend on your area of application and on what $O$ and $\theta$ represent.

If the variance $\sigma^2$ describes the measurement error of some instrument or procedure, and $\theta$ is some property considered rather inherent to the thing being measured, it's natural to talk about 'measuring $\theta$', and about the $O$s as 'measurements of $\theta$'. E.g. the $O$s are several measurements of the length $\theta$ of a steel shaft.

If the variance $\sigma^2$ describes the variability of different individuals, and $\theta$ is some feature of the population considered rather contingent, it's not so natural to talk about 'measuring $\theta$'. E.g. the $O$s are single measurements of the lengths of each steel shaft from a batch, rather than measurements of the average length $\theta$ of a shaft in the batch .

In any case 'measuring an observation' is oddly worded; 'making an observation' is usual. ",deleted 1 characters in body,2013-10-14 09:23:44.217
186189,57394,1406.0,5,,CC BY-SA 3.0,e83e26ec-eecc-480b-a967-923e9194bd62,"I am reading an [article][1] which is trying to justify the need for causal inference in their inferential framework. The thought experiment is as follows:

> Suppose a statistician is asked to design a model for a simple time
> series $X_1,X_2,X_3,...$ and she decides to use a Bayesian method.
> Assume she collects a first observation $X_1 = x_1$. She computes the
> posterior probability density function (pdf) over the parameters
> $\theta$ of the model given the data using Bayes’ rule: $$p(\theta|X_1
= x_1) = \int\frac{p(X_1 = x_1|\theta)p(\theta)}{p(X_1 = x_1|\theta')p(\theta')}, $$
> 
> where $p(X_1 = x_1|θ)$ is the likelihood of $x_1$ given $\theta$ and
> p($\theta$) is the prior pdf of $\theta$. She can use the model to
> predict the next observation by drawing a sample $x_2$ from the
> predictive ￼pdf: $$p(X_2 = x_2|X_1 = x_1) = \int p(X_2 = x_2|X_1 =
 x_1,\theta)p(\theta|X_1 = x_1)d\theta,$$
> 
> where $p(X_2 = x_2|X_1 = x_1,\theta)$ is the likelihood of $x_2$ given
> $x_1$ and $\theta$. Note that $x_2$ is not drawn from $p(X_2 = x_2|X_1
> = x_1, \theta)$. She understands that the nature of $x_2$ is very different from $x_1$: while $x_1$ is informative and does change the
> belief state of the Bayesian model, $x_2$ is non-informative and thus
> is a reflection of the model’s belief state. Hence, she would never
> use $x_2$ to further condition the Bayesian model. Mathematically, she
> seems to imply that: 
> $$p(\theta|X_1 =x_1,X_2 =x_2)=p(\theta|X_1 =x_1)$$

However I hardly believe that what this poor statistician should imply is:
$$p(\theta|X_1 =x_1,\text{do}(X_2 =x_2))=p(\theta|X_1 =x_1)$$
Where ""do(or set)"" here comes from [Pearl][2]'s framework of causality which can be found [here][3] and [here][4].
Now am I right about this?


  [1]: http://www.aaai.org/Papers/JAIR/Vol38/JAIR-3812.pdf
  [2]: http://bayes.cs.ucla.edu/jp_home.html
  [3]: ftp://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf
  [4]: http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=dp_ob_title_bk",added 2 characters in body,2013-10-14 07:40:23.930
186192,57425,11117.0,3,,CC BY-SA 3.0,1a93c317-2f29-4436-926e-084ec8b0242e,<probability><modeling>,,2013-10-14 07:44:40.060
186190,57425,11117.0,2,,CC BY-SA 3.0,1a93c317-2f29-4436-926e-084ec8b0242e,"Consider that $\theta$ is an hidden parameter and one has an observation such that $O$:
$$
O \sim N(\theta,\sigma^2).
$$
My question concerning vocabulary: 

do we measure $\theta$ and it gives us $O$? (so we measure the true value)

or 

do we measure $O$ ? (so we measure the observation)

I am looking for strong sources.",,2013-10-14 07:44:40.060
186191,57425,11117.0,1,,CC BY-SA 3.0,1a93c317-2f29-4436-926e-084ec8b0242e,Vocabulary: do we measure actual values or observations?,,2013-10-14 07:44:40.060
186195,57426,20470.0,3,,CC BY-SA 3.0,e8f487bd-71b5-48c1-8765-0c4484ff0625,<time-series><machine-learning><predictive-models><markov-chain>,,2013-10-14 07:49:16.613
186194,57426,20470.0,1,,CC BY-SA 3.0,e8f487bd-71b5-48c1-8765-0c4484ff0625,Hidden Markov Model for Predicting Event Occurrence,,2013-10-14 07:49:16.613
186193,57426,20470.0,2,,CC BY-SA 3.0,e8f487bd-71b5-48c1-8765-0c4484ff0625,"I have a set of `108,000` observations (taken over the course of 100 days) and approximately `2000` event occurrences throughout the whole observation time-span. The data looks something like the figure below where the red columns represent event times, also indicated as  `tE`s:

![enter image description here][1]

As shown with red rectangles in the figure, I have dissected {`tE` to `tE-5`} for each event, effectively treating these as ""pre-event windows"". 

**HMM Training:** I plan to [train][2] a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested by Rabiner on Pg. 273 of this [paper][3]. Then, hopefully I will have an HMM that captures the sequence patterns which lead to an event.

**HMM Prediction:** Then I plan to use this HMM to [predict][4] `P(Observations|HMM)` on a new day, where `Observations` will be a sliding window vector, which I will update to contain the observations between the current time `t` and `t-5` as the day goes on. 

I expect to see `P(Observations|HMM)` increase for `Observations` that resemble the ""pre-event windows"". This would in effect allow me to predict the events before they happen.

**Question**: *Does the plan below sound like a plausible implementation of a Hidden Markov Model?*


  


  [1]: https://i.stack.imgur.com/QkIn0.png
  [2]: http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
  [3]: http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf
  [4]: http://en.wikipedia.org/wiki/Forward_algorithm",,2013-10-14 07:49:16.613
186196,57425,11117.0,5,,CC BY-SA 3.0,d0f192e9-7cde-4e01-9f28-57737f56e217,"Consider that $\theta$ is an hidden parameter and one has an observation such that $O$:
$$
O \sim N(\theta,\sigma^2).
$$
My question concerning vocabulary: 

do we measure $\theta$ and it gives us $O$? (so we measure the true value)

or 

do we measure $O$ ? (so we measure the observation)

I am looking for unquestionable sources.",added 8 characters in body,2013-10-14 07:54:35.030
186197,57427,20473.0,2,,CC BY-SA 3.0,4c8804a1-b223-4df3-b431-66cc93e17195,"Using the **chain rule**, the joint density here can be decomposed as (denoting $\mathbf X$ the collection of the $n+1$ random variables)

$$f_{\mathbf X}(x_n,x_{n-1},...,x_0) = f(x_n\mid x_{n-1},...,x_0)\cdot f(x_{n-1}\mid x_{n-2},...,x_0)\cdot f(x_{n-2}\mid x_{n-3},...,x_0) \cdot...\cdot f(x_0)$$



$$=\left(\prod_{i=1}^{n}\frac {1}{\sqrt{2\pi}}\exp\left\{-\frac {(x_i-\alpha x_{i-1})^2}{2}\right\}\right)\frac {1}{\sqrt{2\pi}}\exp\left\{-\frac {x_0^2}{2}\right\}$$

Viewed as a likelihood function of $\alpha$, and taking its natural logarithm, we have

$$\ln L(\alpha \mid \mathbf X) = -\frac 12\sum_{i=1}^n (x_i-\alpha x_{i-1})^2 +c$$

...where in $c$ is also included the density of $x_0$ (but $x_0$ affects estimation of $\alpha$ through its presence in the conditional density related to $X_1$).

Then 

$$\frac {\partial \ln L(\alpha \mid \mathbf X)}{\partial \alpha} = \frac {\partial }{\partial \alpha} \left(-\frac 12\sum_{i=1}^n (x_i-\alpha x_{i-1})^2\right)$$

$$=-\frac 12\frac {\partial }{\partial \alpha} \left(\sum_{i=1}^n (x_i^2-2\alpha x_ix_{i-1}+\alpha^2x_{i-1}^2)\right)  $$

$$=-\frac 12\frac {\partial }{\partial \alpha} \left(\sum_{i=1}^n x_i^2-2\alpha \sum_{i=1}^nx_ix_{i-1}+\alpha^2\sum_{i=1}^nx_{i-1}^2)\right) $$

$$=\sum_{i=1}^n x_ix_{i-1} -\alpha\sum_{i=1}^nx_{i-1}^2$$

Setting 

$$\frac {\partial \ln L(\alpha \mid \mathbf X)}{\partial \alpha} =0\Rightarrow \hat \alpha_{ML} = \frac {\sum_{i=1}^n x_ix_{i-1}}{\sum_{i=1}^nx_{i-1}^2}$$

while $$\frac {\partial^2 \ln L(\alpha \mid \mathbf X)}{\partial \alpha^2} = -\sum_{i=1}^nx_{i-1}^2 <0$$

which guarantees a global and unique maximum, since it is negative irrespective of $\alpha$.


",,2013-10-14 08:16:58.337
186198,57287,21624.0,5,,CC BY-SA 3.0,555137da-1248-44f8-b27a-0c20a1c5f421,"I am using bootstrap for my simulation.

The number of the population is flexible for each case, and the sample size is decided by a certain percentage. For example, I have a 10,000 population, and I decide to use 10% for each iteration of bootstrap, so the sample size is 1,000.

In practice, I found it is hard to decide how many times to run the bootstrap is enough. With less simulation, the results appear insufficiant, while with a large number of simulation they are purely redundant.

May I know if there is a method that can help me to decide the number of iterations to run?",added 7 characters in body,2013-10-14 08:16:58.780
186217,57426,,4,user88,CC BY-SA 3.0,262b3cb7-c2aa-421d-bb15-ad2eab34a1bd,Hidden Markov model for predicting event occurrence,edited title,2013-10-14 09:24:06.377
186219,57428,,4,user88,CC BY-SA 3.0,83a273f9-8dd7-4fab-aa03-d24c7a7844f0,How to discretise continuous attributes while implementing the ID3 algorithm?,deleted 3 characters in body; edited title,2013-10-14 09:25:05.803
186235,57435,22677.0,3,,CC BY-SA 3.0,91dad556-62a6-4f5e-8e17-f1484f4ffe6f,<econometrics><nonlinear>,,2013-10-14 13:34:50.787
186199,57426,20470.0,5,,CC BY-SA 3.0,18fd3bbb-aaf5-45aa-8c0a-634fbd2dfab0,"I have a set of `108,000` observations (taken over the course of 100 days) and approximately `2000` event occurrences throughout the whole observation time-span. The data looks something like the figure below where the red columns represent event times, also indicated as  $t_E$'s:

![enter image description here][1]

As shown with red rectangles in the figure, I have dissected {$t_E$ to $t_{E-5}$} for each event, effectively treating these as ""pre-event windows"". 

**HMM Training:** I plan to [train][2] a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested by Rabiner on Pg. 273 of this [paper][3]. Then, hopefully I will have an HMM that captures the sequence patterns which lead to an event.

**HMM Prediction:** Then I plan to use this HMM to [predict][4] $P(Observations|HMM)$ on a new day, where $Observations$ will be a sliding window vector, which I will update to contain the observations between the current time $t$ and $t-5$ as the day goes on. 

I expect to see $P(Observations|HMM)$ increase for `Observations` that resemble the ""pre-event windows"". This would in effect allow me to predict the events before they happen.

**Question**: *Does the plan below sound like a plausible implementation of a Hidden Markov Model?*


  


  [1]: https://i.stack.imgur.com/QkIn0.png
  [2]: http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
  [3]: http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf
  [4]: http://en.wikipedia.org/wiki/Forward_algorithm",added 6 characters in body,2013-10-14 08:18:53.413
186200,57426,20470.0,5,,CC BY-SA 3.0,79e07fc6-349d-4511-8c49-e0b64d399d6c,"I have a set of `108,000` observations (taken over the course of 100 days) and approximately `2000` event occurrences throughout the whole observation time-span. The data looks something like the figure below where the red columns highlight event times, i.e. $t_E$'s:

![enter image description here][1]

As shown with red rectangles in the figure, I have dissected {$t_E$ to $t_{E-5}$} for each event, effectively treating these as ""pre-event windows"". 

**HMM Training:** I plan to [train][2] a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested by Rabiner on Pg. 273 of this [paper][3]. Then, hopefully I will have an HMM that captures the sequence patterns which lead to an event.

**HMM Prediction:** Then I plan to use this HMM to [predict][4] $P(Observations|HMM)$ on a new day, where $Observations$ will be a sliding window vector, which I will update to contain the observations between the current time $t$ and $t-5$ as the day goes on. 

I expect to see $P(Observations|HMM)$ increase for $Observations$ that resemble the ""pre-event windows"". This would in effect allow me to predict the events before they happen.

**Question**: *Does this sound like a plausible implementation of a Hidden Markov Model?*


  


  [1]: https://i.stack.imgur.com/QkIn0.png
  [2]: http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
  [3]: http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf
  [4]: http://en.wikipedia.org/wiki/Forward_algorithm",deleted 14 characters in body,2013-10-14 08:24:37.727
186203,57428,22629.0,1,,CC BY-SA 3.0,350589a0-ccf5-4589-bf50-737effc43f8b,How to discrete the continuous attributes for implementing ID3 Algorithm,,2013-10-14 08:33:17.943
186202,57428,22629.0,3,,CC BY-SA 3.0,350589a0-ccf5-4589-bf50-737effc43f8b,<data-mining><discrete-data><threshold><cart>,,2013-10-14 08:33:17.943
186201,57428,22629.0,2,,CC BY-SA 3.0,350589a0-ccf5-4589-bf50-737effc43f8b,"I am trying to implement the ID3 algorithm for a data set . However , all the attributes are continuous and can have values between 1-10. I have searched on this and found that we have to find the threshold for discretization but couldn't understand on how to do this exactly . Can some one explain on how to do this .? The dataset I am using is Breast Cancer Data from Wisconsin hospitals(http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)",,2013-10-14 08:33:17.943
186206,57429,20740.0,3,,CC BY-SA 3.0,464c6a0b-e9e9-4e8c-b199-a0127f74d019,<regression><multiple-regression><interaction><linear-model>,,2013-10-14 08:35:58.630
186205,57429,20740.0,2,,CC BY-SA 3.0,464c6a0b-e9e9-4e8c-b199-a0127f74d019,"

I am looking to do a linear regression on two independent variables that will be present in varying proportions.

For example trying to do a linear regression on $Y$ which is payment behavior (payback rate) of customers based on the the quality (let's say Gini coefficient) of the new and existing customer credit scores ($X_1$ and $X_2$, respectively) adjusted for the proportion of new and existing customers in the sample.

Existing customers will be present in proportion $p$ and new customers in proportion $1-p = q$.

In general more new customers, $q$, has a negative effect.  Better scoring ($X_1, X_2$) and more existing customers p have a positive effect.

What is a good way to model this? 

Would something like the following be a good solution trying to use $p$ and $q$ as some sort of interaction effect?  

$Y = X_1+X_2+\frac{X_1}{q}+X_2 p$

Would it be better to include p and q as variables themselves as well?
",,2013-10-14 08:35:58.630
186204,57429,20740.0,1,,CC BY-SA 3.0,464c6a0b-e9e9-4e8c-b199-a0127f74d019,Linear regression with independent variables with varying proportions,,2013-10-14 08:35:58.630
186207,57381,8074.0,5,,CC BY-SA 3.0,dfc399a8-a6a3-4ab5-abc4-23efce60346b,"I think there are a few options for showing this type of data:

The first option would be to conduct an ""Empirical Orthogonal Functions Analysis"" (EOF) (also referred to as ""Principal Component Analysis"" (PCA) in non-climate circles). For your case, this should be conducted on a correlation matrix of your data locations. For example, your data matrix `dat` would be your spatial locations in the column dimension, and the measured parameter in the rows; So, your data matrix will consist of time series for each location. The `prcomp()` function will allow you to obtain the principal components, or dominant modes of correlation, relating to this field:

    res <- prcomp(dat, retx = TRUE, center = TRUE, scale = TRUE) # center and scale should be ""TRUE"" for an analysis of dominant correlation modes)
    #res$x and res$rotation will contain the PC modes in the temporal and spatial dimension, respectively.

The second option would be to create maps that show correlation relative to an individual location of interest:

    C <- cor(dat)
    #C[,n] would be the correlation values between the nth location (e.g. dat[,n]) and all other locations. 

###EDIT: additional example

While the following example doesn't use gappy data, you could apply the same analysis to a data field following interpolation with DINEOF (see previous link for R script). The example uses monthly anomaly sea level pressure data from the following data set (http://www.esrl.noaa.gov/psd/gcos_wgsp/Gridded/data.hadslp2.html):
    
    #dropbox link:
    #https://dl.dropboxusercontent.com/u/52403158/slp_sm_grd.csv
    #https://dl.dropboxusercontent.com/u/52403158/slp_sm.csv
    
    slp <- read.csv(""slp_sm.csv"", row.names=1)
    grd <- read.csv(""slp_sm_grd.csv"", row.names=1)
    
    slp <- as.matrix(slp)
    time <- as.POSIXct(rownames(slp))
    
    ###EOF SLP anom
    
    #centered and scaled data
    slp.sc <- scale(slp, center=TRUE, scale=TRUE)
    
    #correlation matrix
    COR <- cov(slp.sc) 
    
    # Decompose matrix using eigen() to derive PC loadings
    E <- eigen(COR)
    E$vectors # loadings
    E$values # lambda values
    expl.var <- E$values / sum(E$values) # explained variance
    cum.expl.var <- cumsum(expl.var) # cumulative explained variance
    plot(cum.expl.var)
    
    # Project data on loadings to derive new coordinates (principal components)
    A <- slp.sc %*% E$vectors
    
###Create maps of EOF modes

    
    ###Map the leading EOF mode
    
    #make interpolation
    require(akima)
    require(maps)
    
    eof.num <- 1
    F1 <- interp(x=grd$lon, y=grd$lat, z=E$vectors[,eof.num]) # interpolated spatial EOF mode
    
    
    png(paste0(""EOF_mode"", eof.num, "".png""), width=7, height=6, units=""in"", res=400)
    #x11(width=7, height=5)
     
    par(ps=10) #settings before layout
    layout(matrix(c(1,2), nrow=2, ncol=1, byrow=TRUE), heights=c(4,2), widths=7)
    #layout.show(2) # run to see layout; comment out to prevent plotting during .pdf
    par(cex=1) # layout has the tendency change par()$cex, so this step is important for control
     
    par(mar=c(4,4,1,1)) # I usually set my margins before each plot
    pal <- colorRampPalette(c(""blue"", ""cyan"", ""yellow"", ""red""))
    image(F1, col=pal(100))
    map(""world"", add=TRUE, lwd=2)
    contour(F1, add=TRUE, col=""white"")
    box()
    
    par(mar=c(4,4,1,1)) # I usually set my margins before each plot
    plot(time, A[,eof.num], t=""l"", lwd=1, ylab="""", xlab="""")
    mtext(paste0(""EOF "", eof.num, "" [expl.var = "", round(expl.var[eof.num]*100), ""%]""), side=3, line=1) 
    
    dev.off() # closes device
    
![enter image description here][1]

###Create correlation map

    ##Correlation to target location
    loc <- c(-90, 0)
    target <- which(grd$lon==loc[1] & grd$lat==loc[2])
    
    F1 <- interp(x=grd$lon, y=grd$lat, z=COR[,target]) # interpolated spatial EOF mode
    
    
    png(paste0(""Correlation_map"", ""_lon"", loc[1], ""_lat"", loc[2], "".png""), width=7, height=5, units=""in"", res=400)
    #x11(width=7, height=5)
     
    par(ps=10) #settings before layout
    layout(matrix(c(1,2), nrow=2, ncol=1, byrow=TRUE), heights=c(4,1), widths=7)
    #layout.show(2) # run to see layout; comment out to prevent plotting during .pdf
    par(cex=1) # layout has the tendency change par()$cex, so this step is important for control
     
    par(mar=c(4,4,1,1)) # I usually set my margins before each plot
    pal <- colorRampPalette(c(""blue"", ""cyan"", ""yellow"", ""red"", ""yellow"", ""cyan"", ""blue""))
    #pal <- colorRampPalette(c(""purple4"", ""white"", ""blue""))
    ncolors <- 100
    breaks <- seq(-1,1,,ncolors+1)
    image(F1, col=pal(ncolors), breaks=breaks)
    map(""world"", add=TRUE, lwd=2)
    contour(F1, add=TRUE, col=""white"")
    box()
    
    par(mar=c(4,4,0,1)) # I usually set my margins before each plot
    levs <- breaks[-1] - diff(breaks)/2
    image(x=levs, y=1, z=as.matrix(levs), col=pal(ncolors), breaks=breaks, ylab="""", xlab="""", yaxt=""n"")
    mtext(""Correlation [R]"", side=1, line=2.5)
    box()
    
    dev.off() # closes device
    

![enter image description here][2]




 


  [1]: https://i.stack.imgur.com/oo2C0.png
  [2]: https://i.stack.imgur.com/fK7St.png",Additional example provided,2013-10-14 08:44:05.013
186208,57430,16474.0,2,,CC BY-SA 3.0,b1fa5d6c-a568-498c-8bae-a37f7381e63e,"I would stick with $p$ only, as $q$ does not add any information on top of $p$. I would add interaction terms between $X_1$ and $p$ and $X_2$ and $p$ and then include the main effects of both $X_1$, $X_2$ and $p$. So:

$Y =\beta_0 + \underbrace{\beta_1 X_1 + \beta_2 X_2 + \beta_3 p}_{\textrm{main effects}} + \underbrace{\beta_4 X_1 p + \beta_5 X_2 p}_{\textrm{interactions}} + \varepsilon$",,2013-10-14 08:46:34.843
186209,57381,8074.0,5,,CC BY-SA 3.0,5ed1c6a3-d400-4698-8f47-bacd44a26022,"I think there are a few options for showing this type of data:

The first option would be to conduct an ""Empirical Orthogonal Functions Analysis"" (EOF) (also referred to as ""Principal Component Analysis"" (PCA) in non-climate circles). For your case, this should be conducted on a correlation matrix of your data locations. For example, your data matrix `dat` would be your spatial locations in the column dimension, and the measured parameter in the rows; So, your data matrix will consist of time series for each location. The `prcomp()` function will allow you to obtain the principal components, or dominant modes of correlation, relating to this field:

    res <- prcomp(dat, retx = TRUE, center = TRUE, scale = TRUE) # center and scale should be ""TRUE"" for an analysis of dominant correlation modes)
    #res$x and res$rotation will contain the PC modes in the temporal and spatial dimension, respectively.

The second option would be to create maps that show correlation relative to an individual location of interest:

    C <- cor(dat)
    #C[,n] would be the correlation values between the nth location (e.g. dat[,n]) and all other locations. 

###EDIT: additional example

While the following example doesn't use gappy data, you could apply the same analysis to a data field following interpolation with DINEOF (http://menugget.blogspot.de/2012/10/dineof-data-interpolating-empirical.html). The example below uses a subset of monthly anomaly sea level pressure data from the following data set (http://www.esrl.noaa.gov/psd/gcos_wgsp/Gridded/data.hadslp2.html):
    
    #dropbox link:
    #https://dl.dropboxusercontent.com/u/52403158/slp_sm_grd.csv
    #https://dl.dropboxusercontent.com/u/52403158/slp_sm.csv
    
    slp <- read.csv(""slp_sm.csv"", row.names=1)
    grd <- read.csv(""slp_sm_grd.csv"", row.names=1)
    
    slp <- as.matrix(slp)
    time <- as.POSIXct(rownames(slp))
    
    ###EOF SLP anom
    
    #centered and scaled data
    slp.sc <- scale(slp, center=TRUE, scale=TRUE)
    
    #correlation matrix
    COR <- cov(slp.sc) 
    
    # Decompose matrix using eigen() to derive PC loadings
    E <- eigen(COR)
    E$vectors # loadings
    E$values # lambda values
    expl.var <- E$values / sum(E$values) # explained variance
    cum.expl.var <- cumsum(expl.var) # cumulative explained variance
    plot(cum.expl.var)
    
    # Project data on loadings to derive new coordinates (principal components)
    A <- slp.sc %*% E$vectors
    
###Create maps of EOF modes

    
    ###Map the leading EOF mode
    
    #make interpolation
    require(akima)
    require(maps)
    
    eof.num <- 1
    F1 <- interp(x=grd$lon, y=grd$lat, z=E$vectors[,eof.num]) # interpolated spatial EOF mode
    
    
    png(paste0(""EOF_mode"", eof.num, "".png""), width=7, height=6, units=""in"", res=400)
    #x11(width=7, height=5)
     
    par(ps=10) #settings before layout
    layout(matrix(c(1,2), nrow=2, ncol=1, byrow=TRUE), heights=c(4,2), widths=7)
    #layout.show(2) # run to see layout; comment out to prevent plotting during .pdf
    par(cex=1) # layout has the tendency change par()$cex, so this step is important for control
     
    par(mar=c(4,4,1,1)) # I usually set my margins before each plot
    pal <- colorRampPalette(c(""blue"", ""cyan"", ""yellow"", ""red""))
    image(F1, col=pal(100))
    map(""world"", add=TRUE, lwd=2)
    contour(F1, add=TRUE, col=""white"")
    box()
    
    par(mar=c(4,4,1,1)) # I usually set my margins before each plot
    plot(time, A[,eof.num], t=""l"", lwd=1, ylab="""", xlab="""")
    mtext(paste0(""EOF "", eof.num, "" [expl.var = "", round(expl.var[eof.num]*100), ""%]""), side=3, line=1) 
    
    dev.off() # closes device
    
![enter image description here][1]

###Create correlation map

    ##Correlation to target location
    loc <- c(-90, 0)
    target <- which(grd$lon==loc[1] & grd$lat==loc[2])
    
    F1 <- interp(x=grd$lon, y=grd$lat, z=COR[,target]) # interpolated spatial EOF mode
    
    
    png(paste0(""Correlation_map"", ""_lon"", loc[1], ""_lat"", loc[2], "".png""), width=7, height=5, units=""in"", res=400)
    #x11(width=7, height=5)
     
    par(ps=10) #settings before layout
    layout(matrix(c(1,2), nrow=2, ncol=1, byrow=TRUE), heights=c(4,1), widths=7)
    #layout.show(2) # run to see layout; comment out to prevent plotting during .pdf
    par(cex=1) # layout has the tendency change par()$cex, so this step is important for control
     
    par(mar=c(4,4,1,1)) # I usually set my margins before each plot
    pal <- colorRampPalette(c(""blue"", ""cyan"", ""yellow"", ""red"", ""yellow"", ""cyan"", ""blue""))
    #pal <- colorRampPalette(c(""purple4"", ""white"", ""blue""))
    ncolors <- 100
    breaks <- seq(-1,1,,ncolors+1)
    image(F1, col=pal(ncolors), breaks=breaks)
    map(""world"", add=TRUE, lwd=2)
    contour(F1, add=TRUE, col=""white"")
    box()
    
    par(mar=c(4,4,0,1)) # I usually set my margins before each plot
    levs <- breaks[-1] - diff(breaks)/2
    image(x=levs, y=1, z=as.matrix(levs), col=pal(ncolors), breaks=breaks, ylab="""", xlab="""", yaxt=""n"")
    mtext(""Correlation [R]"", side=1, line=2.5)
    box()
    
    dev.off() # closes device
    

![enter image description here][2]




 


  [1]: https://i.stack.imgur.com/oo2C0.png
  [2]: https://i.stack.imgur.com/fK7St.png",added 64 characters in body,2013-10-14 08:51:04.077
186210,57431,12683.0,2,,CC BY-SA 3.0,a3f5abf1-75f0-4699-81e3-9214f3aee652,"Statistics doesn't give a special meaning to 'measurement' in the way it does to 'estimate'. (As @Glen said, we 'estimate parameters'.) So it's going to depend on your area of application and on what $O$ and $\theta$ represent.

If the variance $\sigma^2$ describes the measurement error of some instrument or procedure, and $\theta$ is some property considered rather inherent to the thing being measured, it's natural to talk about 'measuring $\theta$', and about the $O$s as 'measurements of $\theta$'. E.g. the $O$s are several measurements of the length $\theta$ of a steel shaft.

If the variance $\sigma^2$ describes the variability of different individuals, and $\theta$ is some feature of the population considered rather contingent, it's not so natural to talk about 'measuring $\theta$'. E.g. the $O$s are several measurements of the lengths of each steel shaft from a batch, rather than measurements of the average length $\theta$ of a shaft in the batch .

In any case 'measuring an observation' is oddly worded; 'making an observation' is usual. ",,2013-10-14 09:18:20.040
186211,57403,20062.0,5,,CC BY-SA 3.0,dffd28bc-e1d2-4308-aefc-0fe9ea06d46c,"**Mantel's test widely occur in biological studies** in which is used to
examine the correlation between spatial distribution of animals (position in space) with for example their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (*PNAS, Animal Behaviour, Molecular Ecology...*)

Recently I fabricated some patterns which may occur in nature, but Mantel's test **seems to be quite useless.**


----------


***Imaginary situation:*** Suppose you have orchard (17 x 17 trees) and on each tree a crow is sitting. Levels of ""noise"" for each crow are given and you are searching for pattern in distribution.

***There are 5 possibilities:***

 1. **""Birds of a feather flock together.""** The more similar crows are, the
    smaller is geographical distance between them **(single cluster)**.
    
 2. **""Birds of a feather flock together.""** Also the more similar crows
    are, the smaller is geographical distance between them **(multiple
    clusters)** but, one cluster of noisy crows has no knowledge about the
    existence of second cluster (otherwise they would fuse into a one big
    cluster).
    
 3. **""Monotonical trend occur""**. 
    
 4. **""Opposites attract each other""**. Similar crows can not withstand each
    other.
    
 5. **""Random pattern""** - the level of noise has no significant effect on
    spatial distribution.

Under each case are given plot of points from which the Mantel test computes correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).

![enter image description here][1]


  [1]: https://i.stack.imgur.com/TWQqa.png

**Why scientists do not use Moran's I instead? Is there some hidden reason I do not see?
And if there is such reason,how can I know (how different the hypotheses must be constructed) to appropriately use Mantel's or Moran's I test? A real life example will be helpful.**


----------


**Example data:** *(compressed as possible)*

    r.gen<-seq(-100,100,5)
    r.val<-sample(r.gen, 289, replace=TRUE)
    z10<-rep(0, times=10)
    z11<-rep(0, times=11)
    r5<-c(5,15,25,15,5)
    r71<-c(5,20,40,50,40,20,5)
    r72<-c(15,40,60,75,60,40,15)
    r73<-c(25,50,75,100,75,50,25)
    rbPal <- colorRampPalette(c(""blue"",""red""))
    my.data<-data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
    c1 = c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
    z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
    seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
    seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
    seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
    seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
    replace=TRUE))
    
    # adding colors
    my.data$Col1 <- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
    my.data$Col2 <- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
    my.data$Col3 <- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
    my.data$Col4 <- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
    my.data$Col5 <- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]

Creating matrix of geographical distances (for Moran's I is inversed):

    point.dists <- dist(cbind(my.data$x,my.data$y))
    point.dists.inv <- 1/point.dists
    point.dists.inv<-as.matrix(point.dists.inv)
    diag(point.dists.inv) <- 0

Plot creation:

    X11(width=12, height=6)
    par(mfrow=c(2,5))
    par(mar=c(1,1,1,1))

    library(ape)
    for (i in 3:7) {
    my.res<-mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
    plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
    ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
    text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
    2), ""\n p.value ="", round(my.res$p, 3)))
    my.res<-Moran.I(my.data[ ,i], point.dists.inv)
    text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
    3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
    round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
    }
    
    par(mar=c(5,4,4,2)+0.1)
    
    for (i in 3:7) {
    plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
    ylab=""behavioural distance"")
    }

P.S. if you google each tests, in these examples are both used on the exact same data
and exact the same hypothesis...**not very helpful**

Mantel test: http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm

Moran's I: http://www.ats.ucla.edu/stat/r/faq/morans_i.htm",deleted 6 characters in body,2013-10-14 09:18:28.737
186221,57432,12683.0,2,,CC BY-SA 3.0,ab4205ad-e5bf-4fa6-a338-767f22e3bf63,"In regression analysis each response $Y_i$ is modelled conditional on the observed predictor value $x_i$; as (with a normal distribution of errors) $Y_i\sim\mathcal{N}(\beta_0+\beta_1 x,\sigma^2)$ where $\beta_0$ and $\beta_1$ are the intercept & slope coefficients respectively, and $\sigma^2$ is the common error variance. Just as if the $x_i$s had been set by an experimenter rather than themselves sampled. The marginal distribution of the $Y_i$s is not necessarily thought about at all.",,2013-10-14 11:06:43.030
186222,57432,12683.0,5,,CC BY-SA 3.0,1716269b-37ec-4ab7-a62c-5813ac6ee135,"In regression analysis each response $Y_i$ is modelled conditional on the observed predictor value $x_i$; as (with a normal distribution of errors) $Y_i\sim\mathcal{N}(\beta_0+\beta_1 x_i,\sigma^2)$ where $\beta_0$ and $\beta_1$ are the intercept & slope coefficients respectively, and $\sigma^2$ is the common error variance. Just as if the $x_i$s had been set by an experimenter rather than themselves sampled. The marginal distribution of the $Y_i$s is not necessarily thought about at all.",added 2 characters in body,2013-10-14 11:25:59.807
186224,57421,22669.0,5,,CC BY-SA 3.0,76777c45-044d-4adc-989c-bc08337d5dd5,"I've just started learning time series so please excuse me if it's painfully obvious; I haven't managed to find the answer elsewhere.

I have a data series showing a pretty obvious trend although it's quite noisy. I can take pretty much any division of the data and run classical tests to show a highly significant difference in means.

I decided to have a look at time series analysis to see if it could help describe the trend. An ARIMA(0,1,1) model comes out with AIC,BIC=34.3,37.3 (Stata), whilst an ARIMA(0,1,0) model comes out with AIC,BIC=55.1,58.1 - so I understand I'm supposed to prefer the (0,1,1) model.

However, the coefficient for the MA(1) is displaying as -0.9999997 (and not showing any p-values). If I try the same in SPSS I get an MA(1) coefficient of 1.000 (I assume SPSS uses opposite signs) with a p-value of 0.990 - does this mean it suggests I drop the term?

My understanding is that the effect of a MA(1) coefficient of -1 is basically to remove the old error term and convert the whole series to a linear trend. Does this mean ARIMA is totally unsuitable for my needs? On the plus side it gives me a sensible value for the trend. If I use the (0,1,0) model then I still get a reasonable value for the trend but it's not significant any more.

Thanks for your help!

EDIT:
Thanks for looking in. The trend looks like a fairly linear decrease; the data points seen to fairly noisily rattle around above and below a trend line. The ARIMA (0,1,1) model produces something that's not far off a straight line decrease which seems sensible - the (0,1,1) produces what is essentially a lagged version of the data, translated down by one monthof trend.
The data aren't stationary (due to the trend) - though the first differences seem to be. I don't think the (0,1,1) is a bad model - I'm just a little confused by the p-value seeming to suggest I should drop the MA term - or wondering if it means I should bin ARIMA entirely!",added 658 characters in body,2013-10-14 11:40:14.750
186225,57297,,25,,,7fbf0b61-0b7a-4a51-b47b-caf471e3048f,,http://twitter.com/#!/StackStats/status/389727656163704832,2013-10-14 12:21:28.737
186226,57426,20470.0,5,,CC BY-SA 3.0,7edd3968-28cc-4aba-be6d-5fbae8c73ce4,"I have a set of `108,000` observations (taken over the course of 100 days) and approximately `2000` event occurrences throughout the whole observation time-span. The data looks like the figure below where there are 3 observation levels and the red columns highlight event times, i.e. $t_E$'s:

![enter image description here][1]

As shown with red rectangles in the figure, I have dissected {$t_E$ to $t_{E-5}$} for each event, effectively treating these as ""pre-event windows"". 

**HMM Training:** I plan to [train][2] a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested on Pg. 273 of Rabiner's [paper][3]. Hopefully, this will allow me to train an HMM that captures the sequence patterns which lead to an event.

**HMM Prediction:** Then I plan to use this HMM to [predict][4] $P(Observations|HMM)$ on a new day, where $Observations$ will be a sliding window vector, which I will update to contain the observations between the current time $t$ and $t-5$ as the day goes on. 

I expect to see $P(Observations|HMM)$ increase for $Observations$ that resemble the ""pre-event windows"". This would in effect allow me to predict the events before they happen.

**Question**: *Does this sound like a plausible implementation of a Hidden Markov Model?*


  


  [1]: https://i.stack.imgur.com/QkIn0.png
  [2]: http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
  [3]: http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf
  [4]: http://en.wikipedia.org/wiki/Forward_algorithm",added 30 characters in body,2013-10-14 12:46:02.177
186227,57433,,2,user12555,CC BY-SA 3.0,a35c990b-1530-4ea3-99a9-0f6822f464b0,"I have experience of deploying random forests in a SQL Server environment via `User Defined Function`. The trick is to convert the `IF-THEN ELSE` rules that you get from each tree into a `CASE-WHEN END` or any other `Conditional Processing` construct (admittedly I've used JMP Pro's Bootstrap Forest implementation - 500k lines of SQL code).

There is absolutely no reason why this cannot be achived using the `rattle` `R` package. Have a look at `randomForest2Rules` & `printRandomForests` functions in that package. Both take `random forest` object as input and visit each tree in the forest and output a set of `IF-THEN ELSE` rules. Taking this as a starting point it should not be difficult converting this logic into your desired language. 

The above, also makes it important to decide the smallest no. of trees you need in the forest to make predictions at a desired level of accuracy (hint: plot(rf.object) shows you at what point the forest predictions do not improve despite adding more trees.) in order to keep the no. of lines to represent the forest down.",,2013-10-14 13:08:34.127
186230,57434,1945.0,3,,CC BY-SA 3.0,983812e4-e1f1-431c-92a1-455d31585f13,<regression><machine-learning>,,2013-10-14 13:13:22.673
186229,57434,1945.0,1,,CC BY-SA 3.0,983812e4-e1f1-431c-92a1-455d31585f13,How does RVM achieve sparsity?,,2013-10-14 13:13:22.673
186228,57434,1945.0,2,,CC BY-SA 3.0,983812e4-e1f1-431c-92a1-455d31585f13,"I have read several textbook descriptions on RVM and none of them provide an adequate (plain English) explanation of how RVM achieves sparsity.

I am left feeling like the authors left out a paragraph of text that would have connected the dots and instead decided to replace (rather than supplement) it with mathematical derivations.

Could someone please explain the basic idea as to how RVM works in relation to learning sparse regression models?",,2013-10-14 13:13:22.673
186231,57319,1693.0,33,,,9496dc85-ae19-4aa4-b3d1-8a038ebbea8a,,821,2013-10-14 13:17:00.657
186232,57433,,5,user12555,CC BY-SA 3.0,a00853fa-17b9-4568-800e-a1decd2956d8,"I have experience of deploying random forests in a SQL Server environment via `User Defined Function`. The trick is to convert the `IF-THEN ELSE` rules that you get from each tree into a `CASE-WHEN END` or any other `Conditional Processing` construct (admittedly I've used JMP Pro's Bootstrap Forest implementation - 500k lines of SQL code).

There is absolutely no reason why this cannot be achived using the `rattle` `R` package. Have a look at `randomForest2Rules` & `printRandomForests` functions in that package. Both take `random forest` object as input and visit each tree in the forest and output a set of `IF-THEN ELSE` rules. Taking this as a starting point it should not be difficult converting this logic into your desired language in an automated way, since the output from the above mentioned function is structured text. 

The above, also makes it important to decide the smallest no. of trees you need in the forest to make predictions at a desired level of accuracy (hint: plot(rf.object) shows you at what point the forest predictions do not improve despite adding more trees.) in order to keep the no. of lines to represent the forest down.",added 91 characters in body,2013-10-14 13:25:40.947
186234,57435,22677.0,2,,CC BY-SA 3.0,91dad556-62a6-4f5e-8e17-f1484f4ffe6f,"I ask this question out of curiosity

earlier today when i was trying to test for heteroscedasticity in `R`, i accidentally mistook `white.test` of `tseries` package for `white.test` of `bstat` package. 

i found out later that the former tests for **Neglected Non Linearity** while the latter tests for **Heteroscedasticity**

now this is something new, i hadn't heard about the ***""neglected""*** part before, can someone please enlighten me about the ***""neglected""***?

",,2013-10-14 13:34:50.787
186233,57435,22677.0,1,,CC BY-SA 3.0,91dad556-62a6-4f5e-8e17-f1484f4ffe6f,"Can someone enlighten me on what is ""Neglected Nonlinearity""?",,2013-10-14 13:34:50.787
186236,57436,7949.0,2,,CC BY-SA 3.0,22360406-35cc-489f-a885-6b279f0c5be0,"There is no obvious relationship between $R^2$ and reversal of the sign of a regression coefficient. Assume you have data for which the true model is for example
$$
y_i = 0+5x_i -z_z + \epsilon_i
$$
with $\epsilon_i \sim N(0, sd_\text{error}^2)$. I show the zero to make explicit that the intercept of the true model is zero, this is just a simplification.

When x and y are highly correlated and centered about zero then the coefficient of z when regressing over just z will be positive instead of negative. Note that the true model coefficients do not change with $sd_\text{error}$ but you can make $R^2$ vary between zero and one by changing the magnitude of the residual error. Look for example at the following R-code:

    require(MASS)
    sd.error <- 1
    x.and.z <- mvrnorm(1000, c(0,0) , matrix(c(1, 0.9,0.9,1),nrow=2)) # set correlation to 0.9
    x <- x.and.y[, 1]
    z <- x.and.y[, 2]
    y <- 5*x - z + rnorm(1000, 0, sd.error) # true model
    modell1 <- lm(y~x+z)
    modell2 <- lm(y~z)
    print(summary(modell1)) # coefficient of z should be negative
    print(summary(modell2)) # coefficient of z should be positive   

and play a bit with sd.error. Look for example at $sd_\text{error}=50$.

Note that with a very large sd.error the coefficient estimation will become more unstable and the reversal might not show up every time. But that's a limitation of the sample size. 

A short summary would be that the variance of the error does not affect the expectations and thus reversal. Therefore neither does $R^2$.",,2013-10-14 13:45:26.510
186237,57365,12683.0,5,,CC BY-SA 3.0,06cf0d71-63ad-44cb-b46a-d861e6de8a9d,"We have a response variable $Y$ and predictor $X$, and we draw $n$ samples $(Y_1,X_1), \ldots, (Y_n, X_n)$ from the population of interest to do a regression analysis. Under the assumptions of a simple linear regression model, my question is a conceptual one: how do we really think about the response on the $i$th unit, $Y_i$? Do we say it's drawn from the level or subpopulation of individuals with $ X = x_i $, or from the aggregate population over all the values of $X$? Moreover, while we assume that the response $Y$ in every subpopulation defined by $X$ is normal with equal variances, how do we think about the aggregate population from which $Y_i$ is drawn?  ",fixed typo,2013-10-14 14:09:18.437
186238,57436,7949.0,5,,CC BY-SA 3.0,1e8b0301-099c-495f-863c-9c56e18e8d06,"There is no obvious relationship between $R^2$ and reversal of the sign of a regression coefficient. Assume you have data for which the true model is for example
$$
y_i = 0+5x_i -z_z + \epsilon_i
$$
with $\epsilon_i \sim N(0, sd_\text{error}^2)$. I show the zero to make explicit that the intercept of the true model is zero, this is just a simplification.

When x and y are highly correlated and centered about zero then the coefficient of z when regressing over just z will be positive instead of negative. Note that the true model coefficients do not change with $sd_\text{error}$ but you can make $R^2$ vary between zero and one by changing the magnitude of the residual error. Look for example at the following R-code:

    require(MASS)
    sd.error <- 1
    x.and.z <- mvrnorm(1000, c(0,0) , matrix(c(1, 0.9,0.9,1),nrow=2)) # set correlation to 0.9
    x <- x.and.z[, 1]
    z <- x.and.z[, 2]
    y <- 5*x - z + rnorm(1000, 0, sd.error) # true model
    modell1 <- lm(y~x+z)
    modell2 <- lm(y~z)
    print(summary(modell1)) # coefficient of z should be negative
    print(summary(modell2)) # coefficient of z should be positive   

and play a bit with sd.error. Look for example at $sd_\text{error}=50$.

Note that with a very large sd.error the coefficient estimation will become more unstable and the reversal might not show up every time. But that's a limitation of the sample size. 

A short summary would be that the variance of the error does not affect the expectations and thus reversal. Therefore neither does $R^2$.",edited body,2013-10-14 14:17:13.783
186241,57437,10147.0,3,,CC BY-SA 3.0,87609a67-2cdd-49fe-b148-5b698893ab70,<regression><categorical-data><generalized-linear-model>,,2013-10-14 14:23:05.487
186240,57437,10147.0,1,,CC BY-SA 3.0,87609a67-2cdd-49fe-b148-5b698893ab70,Ordered Response Variable,,2013-10-14 14:23:05.487
186239,57437,10147.0,2,,CC BY-SA 3.0,87609a67-2cdd-49fe-b148-5b698893ab70,"For regression with ordered response variable, there are different methods, for example, discriminant analysis, probit or logit model. I am wondering what are teh different focuces of the different methods and  which one is more often used.
Thanks.",,2013-10-14 14:23:05.487
186244,57438,13385.0,3,,CC BY-SA 3.0,dcf068ac-18ea-4455-a7a6-be4ddc3f2ecb,<regression>,,2013-10-14 14:28:38.417
186242,57438,13385.0,1,,CC BY-SA 3.0,dcf068ac-18ea-4455-a7a6-be4ddc3f2ecb,Finding parameters to maximize expected utility of random variable,,2013-10-14 14:28:38.417
186243,57438,13385.0,2,,CC BY-SA 3.0,dcf068ac-18ea-4455-a7a6-be4ddc3f2ecb,"I'm trying to analyze some data consisting of five randomized parameters and a utility function which indirectly depends on the parameters, by experimentation.  That is to say, the parameters of the experiment are chosen randomly, and successes and failures are counted up.  I want to find parameters for which the expected utility of successes and failures is highest.

From my days in calculus, I can see that an algorithm could consist of:

 1. Regression to a (hopefully analytically tractable) surface
 2. Finding a maximum
 3. Finding the pre-image of my maximum (if I use any of the C libraries I've seen, which seem to focus on the maximum value, not its pre-image)

But I'm not sure about the ""fiddly bits"" like:

 - The distribution of points (I don't have any data yet)
 - Any substantive idea of the shape of the surface, though I am expecting diminishing marginal utility, so it should be non-linear and have a bump.
 - Numerical stability

This seems like it should be straight-forward, in terms of applied decision theory.  So, is my plan sensible?  Any pointers to literature, algorithms, C or Haskell libraries?",,2013-10-14 14:28:38.417
186263,57444,22262.0,1,,CC BY-SA 3.0,b60ff2ee-04b5-4d89-ac6d-e8f94c271e61,Using quantile regression to predict probability of surpassing threshold,,2013-10-14 15:45:47.440
186338,57466,22690.0,2,,CC BY-SA 3.0,12f10fd2-c133-4190-b061-d656b66ce430,"I'm trying to understand the effects of adding non-conditionally independent features to a naive Bayes classifier. Let's say I have the features vector $X = [x_1,x_2,x_3,x_4]$ and that for each value of $x_3$ I get the same value for $x_4$:

For all $i \in \{samples\}$, $x_{3}^{i} = x_{4}^{i}$

I could say that the conditionally independent assumption of $x_n$ given the class $Y = y_k$ does not hold anymore since the value of $x_{3}^{i}$ foresee $x_{4}^{i}$, and that naive Bayes classifier may not produce the expected results. I'm not really sure about that explanation and I would appreciate your point of view about it.",,2013-10-14 21:58:00.910
186245,57426,20470.0,5,,CC BY-SA 3.0,6c318c0d-15b2-46bd-98aa-b23e9dbc815a,"**Question**: *Is the set-up below look like a sensible implementation of a Hidden Markov model?*

I have a set of `108,000` observations (taken over the course of 100 days) and approximately `2000` event occurrences throughout the whole observation time-span. The data looks like the figure below where there are 3 observation levels and the red columns highlight event times, i.e. $t_E$'s:

![enter image description here][1]

As shown with red rectangles in the figure, I have dissected {$t_E$ to $t_{E-5}$} for each event, effectively treating these as ""pre-event windows"". 

**HMM Training:** I plan to [train][2] a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested on Pg. 273 of Rabiner's [paper][3]. Hopefully, this will allow me to train an HMM that captures the sequence patterns which lead to an event.

**HMM Prediction:** Then I plan to use this HMM to [predict][4] $P(Observations|HMM)$ on a new day, where $Observations$ will be a sliding window vector, which I will update to contain the observations between the current time $t$ and $t-5$ as the day goes on. 

I expect to see $P(Observations|HMM)$ increase for $Observations$ that resemble the ""pre-event windows"". This would in effect allow me to predict the events before they happen.



  


  [1]: https://i.stack.imgur.com/QkIn0.png
  [2]: http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
  [3]: http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf
  [4]: http://en.wikipedia.org/wiki/Forward_algorithm",added 13 characters in body,2013-10-14 14:37:27.677
186246,57439,20831.0,2,,CC BY-SA 3.0,41ebf3db-4483-4538-8eeb-0f6cc762e905,"As you say, the data is not stationary, we can find the stationary transformed data by differencing, and checked by the unit root test (e.g Augmented Dickey-Fuller test, Elliott-Rothenberg-Stock test, KPSS test, Phillips-Perron test, Schmidt-Phillips test, Zivot-Andrews test...) We can talk about ARMA model only after confirming the stationarity.

It is a classical way to identify the ARMA(p, q) by the ACF plot and PACF plot. ARMA(0,1) and ARMA(0,0) can be told here. Another method to identify p, q is about the EACF, but it is not widely used for univariate time series.

Empirical studies show that AIC usually tends to overfitting. The advantage of using AIC is for automatic algorithm to find the best model, but it is not usually recommended in traditional time series textbook.",,2013-10-14 14:41:48.740
186248,57403,20062.0,5,,CC BY-SA 3.0,c81ab607-7372-4c3f-b984-d563b7758365,"**Mantel's test widely occur in biological studies** in which is used to
examine the correlation between spatial distribution of animals (position in space) with for example their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (*PNAS, Animal Behaviour, Molecular Ecology...*)

Recently I fabricated some patterns which may occur in nature, but Mantel's test **seems to be quite useless.**


----------


***Imaginary situation:*** Suppose you have orchard (17 x 17 trees) and on each tree a crow is sitting. Levels of ""noise"" for each crow are given and you are searching for pattern in distribution.

***There are 5 possibilities:***

 1. **""Birds of a feather flock together.""** The more similar crows are, the
    smaller is geographical distance between them **(single cluster)**.
    
 2. **""Birds of a feather flock together.""** Also the more similar crows
    are, the smaller is geographical distance between them **(multiple
    clusters)** but, one cluster of noisy crows has no knowledge about the
    existence of second cluster (otherwise they would fuse into a one big
    cluster).
    
 3. **""Monotonical trend occur""**. 
    
 4. **""Opposites attract each other""**. Similar crows can not withstand each
    other.
    
 5. **""Random pattern""** - the level of noise has no significant effect on
    spatial distribution.

Under each case is given plot of points from which the Mantel test computes correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).

![enter image description here][1]


  [1]: https://i.stack.imgur.com/TWQqa.png

**Why scientists do not use Moran's I instead? Is there some hidden reason I do not see?
And if there is such reason,how can I know (how different the hypotheses must be constructed) to appropriately use Mantel's or Moran's I test? A real life example will be helpful.**


----------


**Example data:** *(compressed as possible)*

    r.gen<-seq(-100,100,5)
    r.val<-sample(r.gen, 289, replace=TRUE)
    z10<-rep(0, times=10)
    z11<-rep(0, times=11)
    r5<-c(5,15,25,15,5)
    r71<-c(5,20,40,50,40,20,5)
    r72<-c(15,40,60,75,60,40,15)
    r73<-c(25,50,75,100,75,50,25)
    rbPal <- colorRampPalette(c(""blue"",""red""))
    my.data<-data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
    c1 = c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
    z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
    seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
    seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
    seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
    seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
    replace=TRUE))
    
    # adding colors
    my.data$Col1 <- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
    my.data$Col2 <- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
    my.data$Col3 <- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
    my.data$Col4 <- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
    my.data$Col5 <- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]

Creating matrix of geographical distances (for Moran's I is inversed):

    point.dists <- dist(cbind(my.data$x,my.data$y))
    point.dists.inv <- 1/point.dists
    point.dists.inv<-as.matrix(point.dists.inv)
    diag(point.dists.inv) <- 0

Plot creation:

    X11(width=12, height=6)
    par(mfrow=c(2,5))
    par(mar=c(1,1,1,1))

    library(ape)
    for (i in 3:7) {
    my.res<-mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
    plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
    ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
    text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
    2), ""\n p.value ="", round(my.res$p, 3)))
    my.res<-Moran.I(my.data[ ,i], point.dists.inv)
    text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
    3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
    round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
    }
    
    par(mar=c(5,4,4,2)+0.1)
    
    for (i in 3:7) {
    plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
    ylab=""behavioural distance"")
    }

P.S. if you google each tests, in these examples are both used on the exact same data
and exact the same hypothesis...**not very helpful**

Mantel test: http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm

Moran's I: http://www.ats.ucla.edu/stat/r/faq/morans_i.htm",deleted 1 characters in body,2013-10-14 14:52:45.480
186249,57440,2666.0,2,,CC BY-SA 3.0,f6003478-3f0a-49fd-84f7-4ef3f6cfd2d3,"I don't think that discriminant analysis will be very efficient because it does not use the ordering.  There are 4 commonly used families for ordinal response that are based on direct probability modeling: logistic, probit, log-log (Cox model) and complementary log-log.  These are implemented in the R `rms` package `orm` function, which also handles continuous $Y$.  Graphical methods can be used to choose from among the 4.  Proportional odds is the easiest to interpret.",,2013-10-14 14:56:24.637
186250,57237,22570.0,5,,CC BY-SA 3.0,e340690a-b502-4f76-bddb-311a253cff53,"I'm trying to generate sets of causally connected random variables and started off doing this with a monte carlo approach.

The baseline is a 2-dimensional measured histogram from which I draw random values.

In my concrete examples these variables are acceleration $\bf{a}$ and velocity $\bf{v}$ - so obviously
$v_{i+1} = v_{i} + a_i * dt$
has to hold.

My current naive approach is:

I start with a some $v_0$.
Then I generate a random $a_0$ according to the measured probability of $\bf{a}$ for the value of $v_0$. Using this $a_0$ I can calculate $v_1$ and the whole procedure starts over again.

So when I check the generated accelerations $\bf{a}$ in bins of $\bf{v}$ everything's fine.
But I obviously this does not at all respect the marginal distribution of $\bf{v}$.

I'm kind of familiar with basic monte carlo methods, though lacking some theoretical background as you might guess.
I'd be fine if the two variables where *just* connected by some correlation matrix, but the causal connection between the two gives me headaches.

I didn't manage to find an example for this kind of problem somewhere - I might be googl'ing the wrong terms.
I'd be satisfied if somebody could point me to some literature/example or promising method to get a hold on this.

(Or tell me that's is not really possible given my inputs - that's what I'm guessing occasionally...)


**EDIT:**

The actual aim of this whole procedure:
I have a set of measurements $\bf{a}$ and $\bf{v}$, represented in a two-dimensional histogram $N(a,v)$. Given this input I'd like to generate sets of random $\bf{a_r}$ and $\bf{v_r}$ that reproduce the measured distribution.",added 283 characters in body,2013-10-14 15:03:07.133
186251,57438,13385.0,5,,CC BY-SA 3.0,d6d0f1cc-0a92-48ab-80d0-705713ce14c7,"I'm trying to analyze some data consisting of five randomized parameters and a utility function which indirectly depends on the parameters, by experimentation.  That is to say, the parameters of the experiment are chosen randomly, and successes and failures are counted up.  I want to find parameters for which the expected utility of successes and failures is highest.

From my days in calculus, I can see that an algorithm could consist of:

 1. Regression to a (hopefully analytically tractable) surface
 2. Finding a maximum
 3. Finding the pre-image of my maximum (if I use any of the C libraries I've seen, which seem to focus on the maximum value, not its pre-image)

But I'm not sure about the ""fiddly bits"" like:

 - The distribution of points (I don't have any data yet)
 - Any substantive idea of the shape of the surface, though I am expecting diminishing marginal utility, so it should be non-linear and have a bump.
 - Numerical stability

This seems like it should be straight-forward, in terms of applied decision theory.  So, is my plan sensible?  Any pointers to literature, algorithms, C or Haskell libraries?

Addition in response to comment:

I'm trying to find the ""best"" parameters in terms of student performance.  The 5-tuple represents:

  1. $b$: The ""base"" waiting time before seeing a problem again.
  2. $p_1$: A constant factor if the student says the problem was easy.
  3. $p_2$: A constant factor if the student says it was hard.
  4. $p_3$: A constant factor if the student says it was ""normal"".
  5. $p_4$: A constant factor if the student got it wrong.

The waiting time for the next viewing is computed by multiplying all of the responses the student has issued, and the base waiting time, and computing $e^{b \prod p_{i,j}}$.  So, for example, a wrong answer makes the waiting time much shorter.  An 'easy' report makes it quite a bit longer.

Now, if the student gets the next viewing wrong, we want to count it as a failure.  If the student gets it right (regardless of the difficulty the student reports), we count it as a success.

I want to maximize the utility function $\frac{|\text{success}|}{|\text{total}|}$ by varying the 5-tuple.  I guess $\frac{|\text{success}|}{|\text{failure}|}$ would serve the same purpose.",response to comment,2013-10-14 15:16:08.880
186271,57444,22262.0,5,,CC BY-SA 3.0,07b0186d-57eb-4941-b4dc-32f2b7cc555b,"Consider a continuous response $Y \sim N(0,\sigma^2)$ and design matrix vector $i$;  $\mathbf{X}$. Suppose that I am interested in estimating the probability that $Y \leq 0.1$ given observing $\mathbf{X}$. 

I want to use quantile regression to do this - **can I confirm that this is a legitimate methodology**?

We have quantiles $\tau \in [0,1]$ and after estimating our quantile regression for each $\tau$ we have our quantile estimates $\mathbf{q} := \{\hat{Q}(\tau) : \tau \in \{0.01,0.02,...,0.99\}\}$. I want to select the $\tau$ such that $\hat{Q}(\tau) \approx 0.1$. When I find such a $\hat{Q}(\tau)$ it seems to then follow naturally that $P(Y \leq 0.1) = \tau$. The reason is that my model has estimated the $\tau$-th quantile to be $0.1$, which is point on the x-axis in $Y$'s pdf that I need to find.

In practice this may not work since an estimated quantile can be lower for higher $\tau$ under some $\mathbf{X}$.
 
Not looking for logistic regression with a discretized response as a solution.
",added 78 characters in body,2013-10-14 16:37:14.707
186252,57426,20470.0,5,,CC BY-SA 3.0,944c2c04-734d-428e-abbd-b7c221e35074,"**Question**: *Is the set-up below a sensible implementation of a Hidden Markov model?*

I have a set of `108,000` observations (taken over the course of 100 days) and approximately `2000` event occurrences throughout the whole observation time-span. The data looks like the figure below where there are 3 observation levels and the red columns highlight event times, i.e. $t_E$'s:

![enter image description here][1]

As shown with red rectangles in the figure, I have dissected {$t_E$ to $t_{E-5}$} for each event, effectively treating these as ""pre-event windows"". 

**HMM Training:** I plan to [train][2] a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested on Pg. 273 of Rabiner's [paper][3]. Hopefully, this will allow me to train an HMM that captures the sequence patterns which lead to an event.

**HMM Prediction:** Then I plan to use this HMM to [predict][4] $P(Observations|HMM)$ on a new day, where $Observations$ will be a sliding window vector, which I will update to contain the observations between the current time $t$ and $t-5$ as the day goes on. 

I expect to see $P(Observations|HMM)$ increase for $Observations$ that resemble the ""pre-event windows"". This would in effect allow me to predict the events before they happen.



  


  [1]: https://i.stack.imgur.com/QkIn0.png
  [2]: http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
  [3]: http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf
  [4]: http://en.wikipedia.org/wiki/Forward_algorithm",deleted 10 characters in body,2013-10-14 15:16:34.037
186253,57319,,25,,,da5a9977-52ba-4e15-84a7-725c14cd88d8,,http://twitter.com/#!/StackStats/status/389772954042896384,2013-10-14 15:21:28.577
186254,57441,22678.0,2,,CC BY-SA 3.0,fb956893-b1ab-46c3-82de-e2ab487cc1f3,"I would disagree on your first point. The $L_2$ regularized model is
$$
\parallel Y-K\beta \parallel_2^2 + \lambda \beta^T R \beta
$$
where K is the known kernel matrix and $R$ is the regularization matrix.
$K=R$ is only a good choice, when the gaussian kernel is used.
For more information please see
*A. Smola, B. Schölkopf, On a Kernel-based Method for Pattern Recognition,
Regression, Approximation, and Operator Inversion, 1997*

@author, the discussion about ""good kernels"" is rather popular.
See this post for example:
http://stats.stackexchange.com/questions/48506/what-function-could-be-a-kernel

However, there are ways to compute an optimized kernel based on your regularization idea.
You should find some approaches presented at NIPS.

",,2013-10-14 15:23:57.277
186255,57319,1693.0,5,,CC BY-SA 3.0,144b9427-a108-4d03-a359-8c24f6a65123,"I am modeling an outcome for hospital patients, 'RA' (whether readmitted).  My predictor of interest is 'HHS' (whether referred to Home Health Services such as from a visiting nurse).  Those referred readmit at a 15.2% rate; others, 9.2%, but the former are needier, sicker patients.  Conventional thinking is that if we controlled for severity of illness this difference would not only be washed out but would reverse itself.  In other words, holding constant the severity of illness, having HHS should mean a lower RA rate.

With HHS as the sole predictor, B in a logistic regression = 0.6 (N ~ 25k).  B is reduced to 0.2 with a group of covariates controlled, each accounting for some aspect of severity of illness, but B doesn't fall below zero.

HHS alone explains only about 1% of the variance in RA; with the other predictors, this becomes 4%.* Perhaps this is the problem--that these covariates are not explaining enough variance to ""succeed"" in reversing the sign of the coefficient of interest.  If this is true, is there a way to estimate how high their explained variance needs to be for such a reversal to show up?


----------

*Using either of 2 pseudo-RSQ formulas; Cox & Snell's or Menard's [-2LL0 - (-2LL1)] / [-2LL0.]",see comment 10/14/13 11:27 am,2013-10-14 15:24:08.373
186256,57414,22667.0,5,,CC BY-SA 3.0,f792b475-1e2c-4fbe-a530-94b5b2083319,"This is kind of a basic stats question, but I want to make sure I am doing this right.

I have a distribution of objects. Specifically: 
    `array([   6072.,  112673.,  126874., 44366., 5384., 14697., 20323., 68197., 98024.,39483.,     103990., 18556., 32930., 23551., 6897.])`


I then have a lot of samples like [1,4,0,0,0,0...] (same length) and I'd like to know how far the samples are from the distribution above. Correlation doesn't really do it. 
[32,0,0,0,..] should be further away than [4,0,0,0...].",deleted 25 characters in body,2013-10-14 15:28:49.173
186259,57442,22677.0,2,,CC BY-SA 3.0,fe815939-6dd1-40a1-8abb-0a5e3993091e,"I'm really sorry with this foolish question. 

I just wanted to know how to do ***Heteroscedasticity Test on a Univariate Model***?

+ ex: an univariate autoregressive model
+ ex: an univariate ARCH/GARCH model

if it possible, how does one do that on `R`




",,2013-10-14 15:34:17.913
186258,57442,22677.0,3,,CC BY-SA 3.0,fe815939-6dd1-40a1-8abb-0a5e3993091e,<heteroscedasticity><autoregressive><univariate>,,2013-10-14 15:34:17.913
186257,57442,22677.0,1,,CC BY-SA 3.0,fe815939-6dd1-40a1-8abb-0a5e3993091e,How to do Univariate Heteroscedasticity Test,,2013-10-14 15:34:17.913
186260,57411,20473.0,5,,CC BY-SA 3.0,6d981043-2a0b-4202-a989-df7844e3ca83,"Entropy (joint entropy included), _is a property of the distribution_ that a random variable follows. The available sample (and hence the timing of observation) plays no role in it.  

Copying for Cover & Thomas, the joint entropy $H(X,Y)$ of two discrete random variables $X, Y,$ with joint distribution $p(x,y)$, is defined as

$$H(X,Y) = - \sum_{S_X}\sum_{S_Y}p(x,y)\log p(x,y) $$

Examine the expression: the sums are taken over _all possible values_ of $X$ and $Y$, i.e. over all the values that belong to the support of each r.v. ($S_X$ and $S_Y$ respectively), irrespective of whether some of these values may not materialize or be observed in a sample. What we actually observe, or when, plays no role, in calculating entropy, and joint entropy in particular.  

Turning to your specific example: The side of a coin itself can not be modeled as a random variable. A random variable maps _events_ into real numbers. The side of a coin is not an event. _Observing_ one of the two sides is an event.  _Not observing_ a side, is an event. So let's define a random variable $X$ by ""$X$ takes the value $1$ if heads is observed, $0$ otherwise"". And define $Y$ by ""$Y$ takes the value $1$ if tails is observed, $0$ otherwise"". Assume the coin is fair. The joint distribution of these two random variables is then described by 
$$\begin{align}
P(X=1,Y=1) &= 0 \\
P(X=1,Y=0) &= 0.5 \\
P(X=0,Y=1) &= 0.5 \\
P(X=0,Y=0) &= 0
\end{align}$$

Note that the numerical mapping we chose (the zero/one values) does not play, as numbers go, any decisive part in the probabilities assigned -we could have chosen a 5/6 mapping for $X$ and a 56/89 mapping for $Y$ (or whatever) -the allocation of probabilities in the joint distribution would have been the same (it is the underlying structure of _events_ that is the critical factor).    
Next, as always, we consider the distribution at non-zero values, so 

$$H(X,Y) = - 0.5\log(0.5) - 0.5\log(0.5) $$

and using base-2 for the logarithm we get

$$H(X,Y) = - 0.5(-1) - 0.5(-1) = 1 $$

Finally, you can easily find that the entropy of $X$ (and likewise for $Y$) is 
$$H(X) = - \sum_{S_X}p(x)\log p(x) = - 0.5(-1) - 0.5(-1) = 1 $$

So in this case $H(X,Y) = H(X) = H(Y)$. But the general expression for the decomposition of joint entropy is 

$$H(X,Y) = H(X) + H(Y\mid X) = H(Y) + H(X\mid Y)$$ 

where $H(Y\mid X)$ and  $H(X\mid Y)$ are conditional entropies. Then we conclude that $H(Y\mid X) = H(X\mid Y) = 0$ in this case. The intuition is straightforward: given $X$ what has happened to $Y$ is certain (and likewise in reverse), so conditional entropy is zero.




",added 397 characters in body,2013-10-14 15:37:05.653
186261,57443,2149.0,2,,CC BY-SA 3.0,b547c9c5-28e8-4446-8580-23ce17815d02,"This question was answered in 1988 http://www.unc.edu/~jbhill/tsay.pdf by R.Tsay and implemented in AUTOBOX in 1990. As of this date (today) no other forecasting/time series package has implemented his elegant and creative solution. Simply adjust your series for time trend changes, level shift changes , seasonal pulses and pulses AND the correct ARIMA structure. Verify that the model parameters are constant over time and then search for change points in error variance as he recommends.",,2013-10-14 15:43:16.087
186264,57444,22262.0,3,,CC BY-SA 3.0,b60ff2ee-04b5-4d89-ac6d-e8f94c271e61,<probability><conditional-probability><quantiles><quantile-regression><conditioning>,,2013-10-14 15:45:47.440
186262,57444,22262.0,2,,CC BY-SA 3.0,b60ff2ee-04b5-4d89-ac6d-e8f94c271e61,"Consider a continuous response $Y(i) \sim N(0,\sigma^2)$ and design matrix vector $i$;  $\mathbf{X}(i)$. Suppose that I am interested in estimating the probability that $Y(i+1) \leq 0.1$ given observing $\mathbf{X}(i+1)$. 

I want to use quantile regression to do this - **can I confirm that this is a legitimate methodology**?

We have quantiles $\tau \in [0,1]$ and after estimating our quantile regression for each $\tau$ we have our quantile estimates $\mathbf{q} := \{\hat{Q}(\tau) : \tau \in \{0.01,0.02,...,0.99\}\}$. I want to select the $\tau$ such that $\hat{Q}(\tau) \approx 0.1$. When I find such a $\hat{Q}(\tau)$ it seems to then follow naturally that $P(Y(i+1) \leq 0.1) = \tau$. The reason is that my model has estimated the $\tau$-th quantile to be $0.1$, which is point on the x-axis in $Y$'s pdf that I need to find.

In practice this may not work since an estimated quantile can be lower for higher $\tau$ under some $\mathbf{X}(i+1)$.

As a side question that I think deserves integration into this question; **are there other methods that could be used to to solve this type of probability estimation problem** (aside from discretizing the response for a classification model input, using an ensemble classifier and taking the ratio of votes in favour of $Y(i+1) <= 0.1$, or arbitrary decision boundaries in conditional expectation models).

 

",,2013-10-14 15:45:47.440
186265,57445,22143.0,2,,CC BY-SA 3.0,f63ecaec-7186-43fd-9e47-8e412f96918f,"In Relevance vector machines ([RVM]( http://en.wikipedia.org/wiki/Relevance_vector_machine)) we have a prior on the weight vector $\mathbf{w}$ (which is $N+1$ dimensional, where $N$ is the number of examples) as shown in equation (5) of ([1]):
$$p(\mathbf{w}|\alpha) = \Pi_{i=0}^{N}\mathcal{N}(w_i|0,\alpha_i^{-1}),$$
where $\mathbf{\alpha}$ is the $N+1$ dimensional vector of hyperparameters.

This prior is supposed to ensure that the weight vector $\mathbf{w}$ (which represents the number of ""support vectors"" which are active) is ""sparse"" if we can integrate out all the nuisance parameters ($\alpha$). See paragraph preceding Section 2.2 in ([1]).

*Potential points of confusion:*
 
- the notation $\mathbf{w}$ is different from the $d$-dimensional linear model representation. Here, while comparing RVM with SVM, only think of the dual SVM formulation with the $N+1$ dimensional parameter $\mathbf{w}$.
- ""Sparse"" for (dual) SVMs means the number of support vectors is small. Do not confuse with number of non-zero coefficients in (the d-dimensional) linear models.


  [1]:http://jmlr.org/papers/volume1/tipping01a/tipping01a.pdf",,2013-10-14 15:49:20.960
186266,57437,15827.0,5,,CC BY-SA 3.0,f7639700-4688-40cc-ba3f-ad0f0440ce1f,"For regression with ordered response variable, there are different methods, for example, discriminant analysis, probit or logit model. I am wondering what are the different focuses of the different methods and  which one is more often used.
",deleted 7 characters in body,2013-10-14 16:01:02.053
186267,57446,4320.0,2,,CC BY-SA 3.0,2cde2e82-2e8c-40ba-a66b-eccd36ace0e5,"One problem with the approach you've described is you will need to define what kind of increase in $P(O)$ is meaningful, which may be difficult as $P(O)$ will always be very small in general. It may be better to train two HMMs, say HMM1 for observation sequences where the event of interest occurs and HMM2 for observation sequences where the event **doesn't** occur. Then given an observation sequence $O$ you can predict the event will occur if $P(O|HMM1) > P(O|HMM2)$.

***Disclaimer**: What follows is based on my own personal experience, so take it for what it is.* One of the nice things about HMMs is they allow you to deal with variable length sequences and variable order effects (thanks to the hidden states). Sometimes this is necessary (like in lots of NLP applications). However, it seems like you have a priori assumed that only the last 5 observations are relevant for predicting the event of interest. If this assumption is realistic then you may have significantly more luck using traditional techniques (logistic regression, naive bayes, SVM, etc) and simply using the last 5 observations as features/independent variables. Typically these types of models will be easier to train and (in my experience) produce better results.",,2013-10-14 16:03:34.203
186268,57444,22262.0,5,,CC BY-SA 3.0,917e042b-7e08-40ca-963d-5a74ab7f0069,"Consider a continuous response $Y(i) \sim N(0,\sigma^2)$ and design matrix vector $i$;  $\mathbf{X}(i)$. Suppose that I am interested in estimating the probability that $Y(i+1) \leq 0.1$ given observing $\mathbf{X}(i+1)$. 

I want to use quantile regression to do this - **can I confirm that this is a legitimate methodology**?

We have quantiles $\tau \in [0,1]$ and after estimating our quantile regression for each $\tau$ we have our quantile estimates $\mathbf{q} := \{\hat{Q}(\tau) : \tau \in \{0.01,0.02,...,0.99\}\}$. I want to select the $\tau$ such that $\hat{Q}(\tau) \approx 0.1$. When I find such a $\hat{Q}(\tau)$ it seems to then follow naturally that $P(Y(i+1) \leq 0.1) = \tau$. The reason is that my model has estimated the $\tau$-th quantile to be $0.1$, which is point on the x-axis in $Y$'s pdf that I need to find.

In practice this may not work since an estimated quantile can be lower for higher $\tau$ under some $\mathbf{X}(i+1)$.
 

",deleted 411 characters in body,2013-10-14 16:04:01.923
186269,57442,15827.0,5,,CC BY-SA 3.0,f3657eb3-2ec2-4f41-bb6a-c51e510469d0,"
I just wanted to know how to do ***Heteroscedasticity Test on a Univariate Model***?

+ ex: an univariate autoregressive model
+ ex: an univariate ARCH/GARCH model

If it is possible, how does one do that in `R`? 




","""I'm really sorry with this foolish question."" No need to apologise. If it's worth asking, ask! ",2013-10-14 16:05:14.313
186270,57444,22262.0,5,,CC BY-SA 3.0,7ce63764-1e05-42e1-9f22-0e6b27f8ebeb,"Consider a continuous response $Y \sim N(0,\sigma^2)$ and design matrix vector $i$;  $\mathbf{X}$. Suppose that I am interested in estimating the probability that $Y \leq 0.1$ given observing $\mathbf{X}$. 

I want to use quantile regression to do this - **can I confirm that this is a legitimate methodology**?

We have quantiles $\tau \in [0,1]$ and after estimating our quantile regression for each $\tau$ we have our quantile estimates $\mathbf{q} := \{\hat{Q}(\tau) : \tau \in \{0.01,0.02,...,0.99\}\}$. I want to select the $\tau$ such that $\hat{Q}(\tau) \approx 0.1$. When I find such a $\hat{Q}(\tau)$ it seems to then follow naturally that $P(Y \leq 0.1) = \tau$. The reason is that my model has estimated the $\tau$-th quantile to be $0.1$, which is point on the x-axis in $Y$'s pdf that I need to find.

In practice this may not work since an estimated quantile can be lower for higher $\tau$ under some $\mathbf{X}$.
 

",deleted 26 characters in body,2013-10-14 16:11:31.397
186272,57447,2666.0,2,,CC BY-SA 3.0,08a44fc4-13ab-4603-8dea-6386df1a3834,"It doesn't appear that $Y$ is binary.  Ordinal regression is a good choice here.  With any of the ordinal models (proportional odds, proportional hazards, probit, etc.) you can compute the probability that $Y \geq y$ for all $y$.  That probability will change at the unique values of $y$.  The R `rms` package `orm` function implements this efficiently and has a function generator for exceedance probabilities.  If you were extremely fortunate and really have Gaussian residuals you can use the maximum likelihood estimator of the exceedance probabilities, which is a simple function of $\hat{\mu}$ and $\hat{\sigma}$.",,2013-10-14 16:42:40.467
186343,57467,16469.0,1,,CC BY-SA 3.0,c8d779be-ca7c-4724-a9e8-d9741c65c5e6,How to test (and accept) that a coefficient in a linear regression model equals zero,,2013-10-14 21:58:30.230
186273,57443,2149.0,5,,CC BY-SA 3.0,7b81da42-3129-4abd-833e-4a2b04491995,"This question was answered in 1988 http://www.unc.edu/~jbhill/tsay.pdf by R.Tsay and implemented in AUTOBOX in 1990. As of this date (today) no other forecasting/time series package has implemented his elegant and creative solution. Simply adjust your series for time trend changes, level shift changes , seasonal pulses and pulses AND the correct ARIMA structure. Verify that the model parameters are constant over time and then search for change points in error variance as he recommends.


Edited to respond to Nick ..

Nick As you may know ARCH/GARCH concerns itself with developing an ARIMA model for the squared residuals. The problem is if you have unusual(one-time) anomilies these are dealt with by inorporating pulse indicator series, yielding a zero residual for each identified point. Squaring these residiuals leads to a distribution that has long tails and is not amenable to ARIMA. When I programmed and implented ARCH/GARCH so that I could jump on the ""next new thing"" I found that it was fundamentally inconsistent with Intervention Detection schemes. Essentially ARCH/GARCH provides a possible solution for a ""change in variance"" that may well be more easily handled by Intervention Detection (violations in the expected value). Thus at this point in time my prefernces (Occam's Razor) for the most simplest solution/transformation/drug/remedy causes me to keep the soution as simple as possible but not too simple. The current release of AUTOBOX treats variance heterogeneity by identifying anomalies,parameter changes and determninistic variance changes ... If all this fails the user can square the residuals and build an arima model to construct his/her own ARCH/GARCH model . Here I stand, I can do no other !",added 1188 characters in body,2013-10-14 16:44:48.263
186274,57421,22669.0,5,,CC BY-SA 3.0,6d07cfeb-3566-4844-8418-05cceb2b3d98,"I've just started learning time series so please excuse me if it's painfully obvious; I haven't managed to find the answer elsewhere.

I have a data series showing a pretty obvious trend although it's quite noisy. I can take pretty much any division of the data and run classical tests to show a highly significant difference in means.

I decided to have a look at time series analysis to see if it could help describe the trend. An ARIMA(0,1,1) model comes out with AIC,BIC=34.3,37.3 (Stata), whilst an ARIMA(0,1,0) model comes out with AIC,BIC=55.1,58.1 - so I understand I'm supposed to prefer the (0,1,1) model.

However, the coefficient for the MA(1) is displaying as -0.9999997 (and not showing any p-values). If I try the same in SPSS I get an MA(1) coefficient of 1.000 (I assume SPSS uses opposite signs) with a p-value of 0.990 - does this mean it suggests I drop the term?

My understanding is that the effect of a MA(1) coefficient of -1 is basically to remove the old error term and convert the whole series to a linear trend. Does this mean ARIMA is totally unsuitable for my needs? On the plus side it gives me a sensible value for the trend. If I use the (0,1,0) model then I still get a reasonable value for the trend but it's not significant any more.

Thanks for your help!

EDIT:
Thanks for looking in. The trend looks like a fairly linear decrease; the data points seen to fairly noisily rattle around above and below a trend line. The ARIMA (0,1,1) model produces something that's not far off a straight line decrease which seems sensible - the (0,1,1) produces what is essentially a lagged version of the data, translated down by one month of trend.
The data aren't stationary (due to the trend) - though the first differences seem to be. I don't think the (0,1,1) is a bad model - I'm just a little confused by the p-value seeming to suggest I should drop the MA term - or wondering if it means I should bin ARIMA entirely!

EDIT2
@vinux - thanks for the suggestion; that makes a lot of sense (and seems to be what the -1 MA term is trying to create?).
I've uploaded as many graphs as I could think of as people had requested.

![tsline y - graph of raw values][1]
![tsline D.y - graph of differences][2]
![ac y - autocorrelations of y][3]
![pac y - partial autocorrelations of y][4]
![ac D.y - autocorrelations of first differences][5]
![pac D.y - partial autocorrelations of first differences][6]


  [1]: https://i.stack.imgur.com/OR2sj.png
  [2]: https://i.stack.imgur.com/xlzl2.png
  [3]: https://i.stack.imgur.com/8lQ57.png
  [4]: https://i.stack.imgur.com/IlqiP.png
  [5]: https://i.stack.imgur.com/gUa6p.png
  [6]: https://i.stack.imgur.com/KE5Sa.png",added graphs,2013-10-14 16:53:09.697
186275,57443,2149.0,5,,CC BY-SA 3.0,f9f1a92e-2d9f-446a-bed2-40a9d31110ef,"This question was answered in 1988 http://www.unc.edu/~jbhill/tsay.pdf by R.Tsay and implemented in AUTOBOX in 1990. As of this date (today) no other forecasting/time series package has implemented his elegant and creative solution. Simply adjust your series for time trend changes, level shift changes , seasonal pulses and pulses AND the correct ARIMA structure. Verify that the model parameters are constant over time and then search for change points in error variance as he recommends.


Edited to respond to Nick ..

Nick As you may know ARCH/GARCH concerns itself with developing an ARIMA model for the squared residuals. The problem is if you have unusual(one-time) anomilies these are dealt with by inorporating pulse indicator series, yielding a zero residual for each identified point. Squaring these residiuals leads to a distribution that has long tails and is not amenable to ARIMA. When I programmed and implented ARCH/GARCH so that I could jump on the ""next new thing"" I found that it was fundamentally inconsistent with Intervention Detection schemes. Essentially ARCH/GARCH provides a possible solution for a ""change in variance"" that may well be more easily handled by Intervention Detection (violations in the expected value). Thus at this point in time my prefernces (Occam's Razor) for the most simplest solution/transformation/drug/remedy causes me to keep the soution as simple as possible but not too simple. The current release of AUTOBOX treats variance heterogeneity by identifying anomalies,parameter changes and determninistic variance changes and no need for power transformations via Box-Cox ... If all this fails the user can square the residuals and build an arima model to construct his/her own ARCH/GARCH model . Here I stand, I can do no other !",added 50 characters in body,2013-10-14 17:04:26.240
186276,57448,16644.0,2,,CC BY-SA 3.0,db327fdd-ad54-4cfe-b297-940579fe33ed,"The Dvoretzky-Kiefer-Wolfowitz inequality can be used here. The required sample size $b$ (I'm using $b$ to distinguish it from $n$ because you already set your population size as $n$ in the problem statement) is determined by $$b \geq \left( {1 \over 2 \epsilon^2 } \right) \mathrm{ln} \left( {2 \over \alpha} \right),$$ where $\epsilon$ is how close you want your empirical cdf to be and $1-\alpha$ is the confidence level. 

So, for example, if you want to estimate $F(c)$ within $\epsilon = 0.01$ with 95% confidence, the formula gives a sample size of $$b \geq 18444.4,$$ or $b = 18445.$ 

This will cover any and all $c,$ so it is possible you can do much better. Perhaps one of the commenters will fill in the details on a more efficient solution for a single value of $c.$ ",,2013-10-14 17:26:35.837
186306,57396,22656.0,5,,CC BY-SA 3.0,780c3d10-d6d5-494d-bad5-284780aa2870,"I have a large population of size $n$ from an unknown continuous random variable $X$, and I do not know the underlying distribution of $X$. Given a constant number $c$, I want to determine the minimum sample size I need to estimate the probability $P(X \le c)$ given a confidence level, $p_c$, and confidence interval, $I_c$  (I am not sure if we need them! ). How can I find the minimum sample size to estimate this probability? 

I have found the following discussion in [Wikipedia][1] which is independent of the number of population. I am not sure if it is a good way to determine sample size! 
![enter image description here][2]





I have also found some methods to determine sample size for data to be analyzed by nonparametric tests.you don't have to make any assumption about the distribution of the values. That is why it is called nonparametric. Now I am confused if these nonparametric methods can be used to solve my problem or the method I found in Wikipedia is the correct way to solve my problem, or there exists a better solution.

Thanks for your help. 


  [1]: http://en.wikipedia.org/wiki/Sample_size_determination#Estimating_proportions_and_means
  [2]: https://i.stack.imgur.com/t09Kn.jpg",added 1 characters in body,2013-10-14 20:06:33.677
186342,57467,16469.0,3,,CC BY-SA 3.0,c8d779be-ca7c-4724-a9e8-d9741c65c5e6,<hypothesis-testing><statistical-significance>,,2013-10-14 21:58:30.230
186277,57443,15827.0,5,,CC BY-SA 3.0,b1524954-3a02-4e66-bc96-c3e5fa0dd9a3,"This question was answered in 1988 http://www.unc.edu/~jbhill/tsay.pdf by R.Tsay and implemented in AUTOBOX in 1990. As of this date (today) no other forecasting/time series package has implemented his elegant and creative solution. Simply adjust your series for time trend changes, level shift changes, seasonal pulses and pulses AND the correct ARIMA structure. Verify that the model parameters are constant over time and then search for change points in error variance as he recommends.


Edited to respond to Nick ..

As you may know ARCH/GARCH concerns itself with developing an ARIMA model for the squared residuals. The problem is if you have unusual (one-time) anomalies these are dealt with by incorporating pulse indicator series, yielding a zero residual for each identified point. Squaring these residuals leads to a distribution that has long tails and is not amenable to ARIMA. When I programmed and implemented ARCH/GARCH so that I could jump on the ""next new thing"" I found that it was fundamentally inconsistent with Intervention Detection schemes. Essentially ARCH/GARCH provides a possible solution for a ""change in variance"" that may well be more easily handled by Intervention Detection (violations in the expected value). Thus at this point in time my preferences (Occam's Razor) for the simplest solution/transformation/drug/remedy causes me to keep the solution as simple as possible but not too simple. The current release of AUTOBOX treats variance heterogeneity by identifying anomalies, parameter changes and deterministic variance changes and no need for power transformations via Box-Cox... If all this fails the user can square the residuals and build an arima model to construct his/her own ARCH/GARCH model. Here I stand, I can do no other!",small fixes,2013-10-14 17:29:22.363
186280,57449,21884.0,3,,CC BY-SA 3.0,1ccb25bf-ad69-4d50-93c4-1bba353d9255,<covariance><matrix>,,2013-10-14 17:30:14.367
186279,57449,21884.0,1,,CC BY-SA 3.0,1ccb25bf-ad69-4d50-93c4-1bba353d9255,Covariance matrix equality,,2013-10-14 17:30:14.367
186278,57449,21884.0,2,,CC BY-SA 3.0,1ccb25bf-ad69-4d50-93c4-1bba353d9255,"The (unbiased) sample covariance matrix

$$\mathbf{S}=\dfrac{1}{n-1}\sum_{j=1}^{n}(\mathbf{X}_{j}-\bar{\mathbf{X}})(\mathbf{X}_{j}-\bar{\mathbf{X}})^{T}$$
can be rewritten as

$$\mathbf{S}=\dfrac{1}{n-1}\mathbf{X}^{T}\mathbf{X}-\dfrac{1}{n(n-1)}\mathbf{X}^{T}\mathbf{1}\mathbf{1}^{T}\mathbf{X}$$

where $$\mathbf{1}=\left(\begin{array}{c}
1\\
\vdots\\
1
\end{array}\right)_{(n\times1)}.$$

One (tedious) way of proving this is to expand out the left hand side and the right hand side of the equality, and showing that the entries of the matrices match. I've done this successfully.

My question: is there a neater / more concise way to prove such an equality?",,2013-10-14 17:30:14.367
186283,57450,19265.0,3,,CC BY-SA 3.0,88e41e68-9a8d-43ea-a315-3dcba00b969c,<svm><libsvm><loss-functions>,,2013-10-14 18:06:27.340
186281,57450,19265.0,1,,CC BY-SA 3.0,88e41e68-9a8d-43ea-a315-3dcba00b969c,What is the loss function for C - Support Vector Classification?,,2013-10-14 18:06:27.340
186282,57450,19265.0,2,,CC BY-SA 3.0,88e41e68-9a8d-43ea-a315-3dcba00b969c,"In article [LIBSVM: A Library for Support Vector Machines][1] there is written, than C-SVC uses loss function:

$$ \frac{1}{2}w^Tw+C\sum\limits_{i=1}^l\xi_i$$

OK, I know, what is $w^Tw$. 

But what is $\xi_i$?  I know, that it is somehow connected with misclassifications, but is it calculated exactly?


P.S. I don't use any non-linear kernels.


  [1]: http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf",,2013-10-14 18:06:27.340
186286,57451,21840.0,3,,CC BY-SA 3.0,891f8bfe-7da9-41a3-a3e3-dda017d0e5df,<self-study><multivariate-analysis><density-function><cumulative-distribution-function>,,2013-10-14 18:11:53.353
186285,57451,21840.0,1,,CC BY-SA 3.0,891f8bfe-7da9-41a3-a3e3-dda017d0e5df,Probability of having real roots,,2013-10-14 18:11:53.353
186284,57451,21840.0,2,,CC BY-SA 3.0,891f8bfe-7da9-41a3-a3e3-dda017d0e5df,"Let $U,V,W$ are independent random variables with uniform(0,1) distribution. I am trying to find the probability that $Ux^{2}+Vx+W$ has real roots, that is, $P(V^{2}-4UW> 0)$
I have solved this question using double integral but how to do this using triple integral.
My Approach:
I started with cdf:
$P(V^{2}-4UW >0) =P(V^{2} > 4UW) = P(V>2\sqrt{UW})$ = $\int\int_{2\sqrt{UW}}^1 P(V>2\sqrt{UW}) du dw$
=$\int\int\int_{2\sqrt{UW}}^1 vdu dw dv$

I am finding hard time to get the limits of integral over the region in 3 dimensions.

Thanks!",,2013-10-14 18:11:53.353
186289,57452,22627.0,3,,CC BY-SA 3.0,c2368592-87e7-423b-a53e-c2e61888382e,<mathematical-statistics><extreme-value>,,2013-10-14 18:13:32.393
186288,57452,22627.0,1,,CC BY-SA 3.0,c2368592-87e7-423b-a53e-c2e61888382e,"Expected maximum given population size, mean, and variance",,2013-10-14 18:13:32.393
186287,57452,22627.0,2,,CC BY-SA 3.0,c2368592-87e7-423b-a53e-c2e61888382e,"How would one estimate the maximum given population size, a few moments, and perhaps some additional assumption on the distribution?

Something like ""I'm going to do $N_s≫1$ measurements out of population of size $N_p≫N_s$; will record mean $μ_s$, standard deviation $σ_s$, and maximal value in the sample hs; I am willing to assume binomial (or Poisson, etc) distribution; what is the expected maximal value hp of the entire population?""

Related question: does one need to make the assumptions on the nature of the population distribution, or the sample statistics would be enough to estimate hp?
",,2013-10-14 18:13:32.393
186290,57412,,25,,,2f8277b9-8db0-4204-b6fd-98d9da5d7fdf,,http://twitter.com/#!/StackStats/status/389818253478227968,2013-10-14 18:21:28.833
186291,56768,0.0,34,,,eb0a2164-7ff0-48dc-9547-825eb8ee35a6,,806,2013-10-14 18:25:40.383
186292,57453,22682.0,3,,CC BY-SA 3.0,0227bfcb-8f78-452f-b9d1-f86d3872233e,<r><random-forest><cart><max-margin>,,2013-10-14 18:27:13.640
186294,57453,22682.0,1,,CC BY-SA 3.0,0227bfcb-8f78-452f-b9d1-f86d3872233e,Is there a way to remove individual trees from a forest in the randomForest package in R?,,2013-10-14 18:27:13.640
186293,57453,22682.0,2,,CC BY-SA 3.0,0227bfcb-8f78-452f-b9d1-f86d3872233e,"I am trying to implement the ideas in this paper: http://www.sciencedirect.com/science/article/pii/S0925231212003396. 

This requires me to be able to remove individual trees from the forest and reclassify my training data for each removal. I've been using the randomForest package in R and had a comb through the manual but couldn't find any way of running the forest with a subset of trees, or even with an individual tree. There is a getTree function but that only gives a matrix of the node structure of the tree.

Is there any way to do this, either in randomForest (preferably) or via another package?",,2013-10-14 18:27:13.640
186327,57463,22687.0,1,,CC BY-SA 3.0,61c15056-2b23-4203-b6e1-23875b097154,How does one generate the table mapping t-test values to p values?,,2013-10-14 21:14:24.377
186341,57467,16469.0,2,,CC BY-SA 3.0,c8d779be-ca7c-4724-a9e8-d9741c65c5e6,"I understand that in a linear regression model like:


$y_i = b_0 + b_1 * x_i  + \epsilon_i$

I can have a null-hypothesis:

$H_0: b_1 = 0$ and $H_1: b_1 \neq 0$. 

And then I can reject $H_0$ or fail to reject $H_0$. But what if I want to accept that $b_1 = 0$?",,2013-10-14 21:58:30.230
186295,57421,22669.0,5,,CC BY-SA 3.0,5e25e120-d328-48c7-ab4e-5564cd5eebf4,"I've just started learning time series so please excuse me if it's painfully obvious; I haven't managed to find the answer elsewhere.

I have a data series showing a pretty obvious trend although it's quite noisy. I can take pretty much any division of the data and run classical tests to show a highly significant difference in means.

I decided to have a look at time series analysis to see if it could help describe the trend. An ARIMA(0,1,1) model comes out with AIC,BIC=34.3,37.3 (Stata), whilst an ARIMA(0,1,0) model comes out with AIC,BIC=55.1,58.1 - so I understand I'm supposed to prefer the (0,1,1) model.

However, the coefficient for the MA(1) is displaying as -0.9999997 (and not showing any p-values). If I try the same in SPSS I get an MA(1) coefficient of 1.000 (I assume SPSS uses opposite signs) with a p-value of 0.990 - does this mean it suggests I drop the term?

My understanding is that the effect of a MA(1) coefficient of -1 is basically to remove the old error term and convert the whole series to a linear trend. Does this mean ARIMA is totally unsuitable for my needs? On the plus side it gives me a sensible value for the trend. If I use the (0,1,0) model then I still get a reasonable value for the trend but it's not significant any more.

Thanks for your help!

EDIT:
Thanks for looking in. The trend looks like a fairly linear decrease; the data points seen to fairly noisily rattle around above and below a trend line. The ARIMA (0,1,1) model produces something that's not far off a straight line decrease which seems sensible - the (0,1,1) produces what is essentially a lagged version of the data, translated down by one month of trend.
The data aren't stationary (due to the trend) - though the first differences seem to be. I don't think the (0,1,1) is a bad model - I'm just a little confused by the p-value seeming to suggest I should drop the MA term - or wondering if it means I should bin ARIMA entirely!

EDIT2
@vinux - thanks for the suggestion; that makes a lot of sense (and seems to be what the -1 MA term is trying to create?).
I've uploaded as many graphs as I could think of as people had requested.

![tsline y - graph of raw values][1]
![tsline D.y - graph of differences][2]
![ac y - autocorrelations of y][3]
![pac y - partial autocorrelations of y][4]
![ac D.y - autocorrelations of first differences][5]
![pac D.y - partial autocorrelations of first differences][6]

I've also put the monthly data up in CSV format at [pastebin][7]


  [1]: https://i.stack.imgur.com/OR2sj.png
  [2]: https://i.stack.imgur.com/xlzl2.png
  [3]: https://i.stack.imgur.com/8lQ57.png
  [4]: https://i.stack.imgur.com/IlqiP.png
  [5]: https://i.stack.imgur.com/gUa6p.png
  [6]: https://i.stack.imgur.com/KE5Sa.png
  [7]: http://pastebin.com/7ih4mhfB ""Pastebin""",added 116 characters in body,2013-10-14 18:46:44.143
186296,57451,21840.0,5,,CC BY-SA 3.0,0b157093-6dd9-4e5b-a6e9-007dbccde960,"Let $U,V,W$ are independent random variables with uniform(0,1) distribution. I am trying to find the probability that $Ux^{2}+Vx+W$ has real roots, that is, $P(V^{2}-4UW> 0)$
I have solved this question using double integral but how to do this using triple integral.
My Approach:
I started with cdf:
$P(V^{2}-4UW >0) =P(V^{2} > 4UW) = P(V>2\sqrt{UW})$ = $\int\int_{2\sqrt{uw}}^1 P(V>2\sqrt{UW}) dU dW$
=$\int\int\int_{2\sqrt{uw}}^1 vdU dW dV$

I am finding hard time to get the limits of integral over the region in 3 dimensions.

Thanks!",fixed question,2013-10-14 18:47:38.117
186297,57454,22507.0,2,,CC BY-SA 3.0,6de349ea-b006-4f3d-92a1-df4a30c869a3,"One idea is, instead of creating one forest with N trees, create N ""forests"" of 1 tree each by calling <code>randomForest()</code> N times. Then you could manipulate them as you wish.",,2013-10-14 18:48:24.640
186298,57451,21840.0,5,,CC BY-SA 3.0,7490454d-4e8d-46b6-9142-01dbdaf77de5,"Let $U,V,W$ are independent random variables with uniform(0,1) distribution. I am trying to find the probability that $Ux^{2}+Vx+W$ has real roots, that is, $P(V^{2}-4UW> 0)$
I have solved this question using double integral but how to do this using triple integral.
My Approach:
I started with cdf:
$P(V^{2}-4UW >0) =P(V^{2} > 4UW) = P(V>2\sqrt{UW})$ = $\int\int_{2\sqrt{uw}}^1 P(V>2\sqrt{UW}) dU dW$
=$\int\int\int_{2\sqrt{uw}}^1 vdU dW dV$

I am finding hard time to get the limits of integral over the region in 3 dimensions.

Using double integral:
$P(V^{2}-4UW >0) =P(V^{2} > 4UW) = P(-2ln V <-ln 4 - ln U - ln W) = P(X <-ln 4 +Y)$
where $X=-2 ln V, Y = - ln U - ln W $
$X$ has exp(1) and $Y$ has gamma(2,1) distribution.
$P(X <-ln 4 +Y) = \int_{ln4}^\infty P(X < -ln4 +Y) f_Y(y) dy $
$$=\int_{ln4}^\infty\int_0^{-ln4+y} \frac{1}{2} e^{-\frac{x}{2}}ye^{-y} dxdy $$
Solving this I got 0.2545.

Thanks!",Posted alternate solution ,2013-10-14 19:12:55.470
186299,57453,22682.0,5,,CC BY-SA 3.0,c38813a0-5fb5-4bc3-b841-ce498a563d12,"I am trying to implement the ideas in this paper: http://www.sciencedirect.com/science/article/pii/S0925231212003396. 

This requires me to be able to remove individual trees from the forest and reclassify my training data for each removal. I've been using the randomForest package in R and had a comb through the manual but couldn't find any way of running the forest with a subset of trees, or even with an individual tree. There is a getTree function but that only gives a matrix of the node structure of the tree.

Is there any way to do this, either in randomForest (preferably) or via another random forest implementation (e.g. scikit-learn)?",added 41 characters in body,2013-10-14 19:13:29.730
186301,57455,14748.0,1,,CC BY-SA 3.0,4c0ab385-bff0-4982-a74b-ee6f80265401,Predictive algorithm validation,,2013-10-14 19:13:34.737
186302,57455,14748.0,3,,CC BY-SA 3.0,4c0ab385-bff0-4982-a74b-ee6f80265401,<machine-learning><predictive-models>,,2013-10-14 19:13:34.737
186300,57455,14748.0,2,,CC BY-SA 3.0,4c0ab385-bff0-4982-a74b-ee6f80265401,"In putting a binary 1/0 predictive algorithm into production, what are the consequences where only the positive (1) predictions are checked, meaning only true or false positives are detected, and then fed back into training the model? Will that bias the algorithm in any way so that it progressively gets worse and worse because it never sees true or false negatives?",,2013-10-14 19:13:34.737
186304,57396,22656.0,5,,CC BY-SA 3.0,84413c7f-4cbd-4eed-8d54-a697f1521ae5,"I have a large population of size $n$ from an unknown continuous random variable $X$, and I do not know the underlying distribution of $X$. Given a constant number $c$, I want to determine the minimum sample size I need to estimate the probability $P(X \le c)$ given a confidence level, $p_c$, and confidence interval, $I_c$  (I am not sure if we need them! ). How can I find the minimum sample size to estimate this probability? 

I have found the following discussion in [Wikipedia][1] which is independent of the number of population. I am not sure if it is a good way to determine sample size! 
![enter image description here][2]





I have also found some methods to determine sample size for data to be analyzed by nonparametric tests.you don't have to make any assumption about the distribution of the values. That is why it is called nonparametric. Now I am confused if these nonparametric methods can be used to solve my problem or the method I found in Wikipedia is the correct way to solve my problem, or there exist a better solution.

Thanks for your help. 


  [1]: http://en.wikipedia.org/wiki/Sample_size_determination#Estimating_proportions_and_means
  [2]: https://i.stack.imgur.com/t09Kn.jpg",added 740 characters in body,2013-10-14 19:53:31.000
186305,57456,22143.0,2,,CC BY-SA 3.0,7f86091b-54f8-443e-8072-63faa27b434a,"$\xi_i$ are the slack variables. They are typically nonzero when the 2-class data is non-separable. We are trying the minimize the slack as much as possible (by minimizing their sum, since they are non-negative) along with maximizing the margin ($w^Tw$) term.

*Exact calculation*: Well, if the convex program has been solved to optimality without any optimization error, then yes, they are calculated exactly.",,2013-10-14 20:04:52.890
186346,57468,22693.0,2,,CC BY-SA 3.0,57b8f504-34bd-42a3-9fb0-34dc60348f08,"I have performed a log transformation on my skewed data, however on my DV it went from positive skew to negative skew after the (log) transformation, further data was missing from my DV after the transformation? Please help",,2013-10-14 22:01:18.457
186345,57468,22693.0,3,,CC BY-SA 3.0,57b8f504-34bd-42a3-9fb0-34dc60348f08,<data-transformation>,,2013-10-14 22:01:18.457
186344,57468,22693.0,1,,CC BY-SA 3.0,57b8f504-34bd-42a3-9fb0-34dc60348f08,Trouble transforming data,,2013-10-14 22:01:18.457
186307,57457,6162.0,2,,CC BY-SA 3.0,808109a5-5906-412d-a9d6-abf0d6d420b1,"Here is a solution without multiple integrals calculation (because I don't like multiple integrals). Actually it only uses three elementary simple integrals. 
$$
P(V^{2}-4UW \leq  0) = E\bigl[P(V^{2}-4UW \leq 0 \mid U,W)\bigr] = E\bigl[f(U,W)\bigr]$$ where $f(u,w)=P(V^{2}-4uw \leq 0)= \min\bigl\{1, 2\sqrt{uw}\bigr\}$.
$$
E\bigl[f(U,W)\bigr] = E[g(W)]
$$
where 
$$\begin{align}
g(w) & = E\bigl[\min\bigl\{1, 2\sqrt{Uw}\bigr\}\bigr] 
= 1 \times \Pr(2\sqrt{Uw}>1) + E\bigl[2\sqrt{Uw} \mathbf{1}_{2\sqrt{Uw}\leq 1}\bigr] \\
& = \Pr(U>\frac{1}{4w}) + 2\sqrt{w}E\bigl[\sqrt{U} \mathbf{1}_{U \leq \frac{1}{4w}}\bigr]  \\
& = \max\bigl\{0, 1 - \frac{1}{4w}\bigr\} +  2\sqrt{w} \times \frac{2}{3} \times \min\bigl\{1, \frac{1}{{(4w)}^{\frac{3}{2}}}\bigr\} \\ 
& =\begin{cases} 
 0 + \frac{4}{3}\sqrt{w}  & \text{if } w \leq \frac{1}{4} \\
1 - \frac{1}{4w} + \frac{1}{6w}  & \text{if } w > \frac{1}{4}
\end{cases}, \end{align}$$
and we get 
$$ E[g(W)] = \frac{1}{9} + \frac{3}{4} - \frac{1}{12} \log 4 = \frac{31}{36}-\frac{\log 2}{6},$$
and finally 
$$P(V^{2}-4UW >  0) = \frac{5}{36} + \frac{\log 2}{6} \approx 0.2544134.$$



",,2013-10-14 20:07:35.763
186310,57458,22684.0,3,,CC BY-SA 3.0,3adcd59c-1bd4-482c-85b7-8a6ee8ba83cf,<data-mining>,,2013-10-14 20:08:14.273
186308,57458,22684.0,2,,CC BY-SA 3.0,3adcd59c-1bd4-482c-85b7-8a6ee8ba83cf," 0 down vote favorite
	

I came across a online data mining course project

http://www.kdnuggets.com/data_mining_course/assignments/final-project.html

The data is of samples with 7000 features as genes. Each gene is associated with a value. Some of the values are negative. The data looks like in this way:

SNO ""U48730_at"" ""U58516_at"" ""U73738_at"" ""X06956_at"" ""X16699_at"" ""X83863_at""

X1 "" 27"" "" 161"" "" 0"" "" 34"" "" 2"" "" 116""
X2 "" 27"" "" 265"" "" 0"" "" 98"" "" 2"" "" 123""
X3 "" 24"" "" 126"" "" 0"" "" 21"" "" 0"" "" 142""
X4 "" 27"" "" 163"" "" -1"" "" 16"" "" -1"" "" 134""
X5 "" 41"" "" 138"" "" 1"" "" 29"" "" 1"" "" 153""
X6 "" 55"" "" 107"" "" -1"" "" 17"" "" 0"" "" 152""
X7 "" 27"" "" 99"" "" 0"" "" 57"" "" 1"" "" 139""
X8 "" 2"" "" 137"" "" -1"" "" 19"" "" -3"" "" 213""
X9 "" -5"" "" 161"" "" -3"" "" 23"" "" 2"" "" 193""
X10 "" 0"" "" 110"" "" -3"" "" 7"" "" -1"" "" 208""
X11 "" -7"" "" 67"" "" 1"" "" 2"" "" -2"" "" 149""
X12 "" 4"" "" 93"" "" 3"" "" 37"" "" 2"" "" 266""
X13 "" 2"" "" 75"" "" 3"" "" 30"" "" 6"" "" 205""

The professor advise the students to first do 'data cleaning'. The original sentence is Threshold both train and test data to a minimum value of 20, maximum of 16,000.

I first thought that it is to search over each gene and if there is a value out of the bounds, then just discard this gene as a feature. However, it seems for every gene, there must be a sample with the value out of bound.

What should I do by ""threshold this data""? Is that like if the value is below 20, then set it 20 or if the value is above 16000, then just set it as 16000?

In fact, I did the last operation in R by 

    data[data<20] <- 20

and it turns out that the speed of the command is very slow. (79*7070 samples)

Thanks in advance!
",,2013-10-14 20:08:14.273
186309,57458,22684.0,1,,CC BY-SA 3.0,3adcd59c-1bd4-482c-85b7-8a6ee8ba83cf,Questions about thresholding the data,,2013-10-14 20:08:14.273
186311,57459,22143.0,2,,CC BY-SA 3.0,cbfd51d1-65e9-4e2e-b071-265e35f9dc2b,"> What should I do by ""threshold this data""? Is that like if the value is below 20, then set it 20 or if the value is above 16000, then just set it as 16000?

Yes.",,2013-10-14 20:17:26.320
186312,57460,14799.0,2,,CC BY-SA 3.0,b5cf480f-d515-4c9c-bee8-3afc1ec22aff,"This is for OLS regression. Consider a geometric representation of three variables -- two predictors, $X_1$ and $X_2$, and a dependent variable, $Y$. Each variable is represented by a vector from the origin. The length of the vector equals the standard deviation of the corresponding variable. The cosine of the angle between any two vectors equals the correlation of the corresponding two variables. I will take all the standard deviations to be 1.

![enter image description here][1]

The picture shows the plane determined by the $X_1$ and $X_2$ when they correlate positively with one another. $Y$ is a vector coming out of the screen; the dashed line is its projection into the predictor space and is the regression estimate of $Y$, $\hat{Y}$. The length of the dashed line equals the multiple correlation, $R$, of $Y$ with $X_1$ and $X_2$.

If the projection is in any of the colored sectors then both predictors correlate positively with $Y$. The signs of the regression coefficients $\beta_1$ and $\beta_2$ are immediately apparent visually, because $\hat{Y}$ is the vector sum of $\beta_1 X_1$ and $\beta_2 X_2$. If the projection is in the yellow sector then both $\beta_1$ and $\beta_2$ are positive, but if the projection is in either the red or the blue sector then we have what appears to be suppression; that is, the sign of one of the regression weights is opposite to the sign of the corresponding simple correlation with $Y$. In the picture, $\beta_1$ is positive and $\beta_2$ is negative.

Since the length of the projection can vary between 0 and 1 no matter where it is in the predictor space, there is no minimum $R^2$ for suppression.


  [1]: https://i.stack.imgur.com/io9v4.png",,2013-10-14 20:20:50.680
186315,57461,22685.0,2,,CC BY-SA 3.0,3d6be59c-08ee-4d27-b223-2cc28dcb8dd4,"On [pg. 378][1] they claim two probability distributions are e(k) close if the distance between them is at most e(k).  

What is significance of two distributions X and Y being ""close to"" or ""far from"" each other? Why would anybody care, especially in cryptography ?

  [1]: http://download.springer.com/static/pdf/305/chp%253A10.1007%252F978-3-642-22792-9_21.pdf?auth66=1381954902_7a8eccbd8188fded3878a75ad24f8c83&ext=.pdf",,2013-10-14 20:32:28.583
186313,57461,22685.0,1,,CC BY-SA 3.0,3d6be59c-08ee-4d27-b223-2cc28dcb8dd4,What is the point of measuring statistical distance?,,2013-10-14 20:32:28.583
186314,57461,22685.0,3,,CC BY-SA 3.0,3d6be59c-08ee-4d27-b223-2cc28dcb8dd4,<distance>,,2013-10-14 20:32:28.583
186317,57448,,24,,CC BY-SA 3.0,4514d200-2907-4faf-a33d-ad7a7a9bdf88,,"Proposed by 30815 approved by 7290, 601 edit id of 5606",2013-10-14 20:35:33.910
186316,57448,22143.0,5,,CC BY-SA 3.0,4514d200-2907-4faf-a33d-ad7a7a9bdf88,"The [Dvoretzky-Kiefer-Wolfowitz inequality](http://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality) can be used here. The required sample size $b$ (I'm using $b$ to distinguish it from $n$ because you already set your population size as $n$ in the problem statement) is determined by $$b \geq \left( {1 \over 2 \epsilon^2 } \right) \mathrm{ln} \left( {2 \over \alpha} \right),$$ where $\epsilon$ is how close you want your empirical cdf to be and $1-\alpha$ is the confidence level. 

So, for example, if you want to estimate $F(c)$ within $\epsilon = 0.01$ with 95% confidence, the formula gives a sample size of $$b \geq 18444.4,$$ or $b = 18445.$ 

This will cover any and all $c,$ so it is possible you can do much better. Perhaps one of the commenters will fill in the details on a more efficient solution for a single value of $c.$ ",Added a link to D-K-W inequality.,2013-10-14 20:35:33.910
186319,57458,5237.0,6,,CC BY-SA 3.0,b4c3ac5e-cd11-426f-9546-63c2bd28b344,<data-mining><data-transformation><outliers>,added tags; formatted; removed peripheral comments,2013-10-14 20:47:24.533
186347,57467,16469.0,5,,CC BY-SA 3.0,ab8dc2fe-f835-48aa-8582-41c958cd6470,"I understand that in a linear regression model like:


$y_i = b_0 + b_1 * x_i  + \epsilon_i$

I can have a null and an alternative hypothesis:

$H_0: b_1 = 0$ and $H_1: b_1 \neq 0$. 

And then I can reject $H_0$ or fail to reject $H_0$. But what if I want to accept that $b_1 = 0$?",added 19 characters in body,2013-10-14 22:03:47.687
186318,57458,5237.0,5,,CC BY-SA 3.0,b4c3ac5e-cd11-426f-9546-63c2bd28b344,"I came across a [data mining course project](http://www.kdnuggets.com/data_mining_course/assignments/final-project.html) online.

The data is of samples with 7000 features as genes. Each gene is associated with a value. Some of the values are negative. The data looks like in this way:

    SNO ""U48730_at"" ""U58516_at"" ""U73738_at"" ""X06956_at"" ""X16699_at"" ""X83863_at""
    
    X1 "" 27"" "" 161"" "" 0"" "" 34"" "" 2"" "" 116""
    X2 "" 27"" "" 265"" "" 0"" "" 98"" "" 2"" "" 123""
    X3 "" 24"" "" 126"" "" 0"" "" 21"" "" 0"" "" 142""
    X4 "" 27"" "" 163"" "" -1"" "" 16"" "" -1"" "" 134""
    X5 "" 41"" "" 138"" "" 1"" "" 29"" "" 1"" "" 153""
    X6 "" 55"" "" 107"" "" -1"" "" 17"" "" 0"" "" 152""
    X7 "" 27"" "" 99"" "" 0"" "" 57"" "" 1"" "" 139""
    X8 "" 2"" "" 137"" "" -1"" "" 19"" "" -3"" "" 213""
    X9 "" -5"" "" 161"" "" -3"" "" 23"" "" 2"" "" 193""
    X10 "" 0"" "" 110"" "" -3"" "" 7"" "" -1"" "" 208""
    X11 "" -7"" "" 67"" "" 1"" "" 2"" "" -2"" "" 149""
    X12 "" 4"" "" 93"" "" 3"" "" 37"" "" 2"" "" 266""
    X13 "" 2"" "" 75"" "" 3"" "" 30"" "" 6"" "" 205""

The professor advise the students to first do 'data cleaning'. The original sentence is Threshold both train and test data to a minimum value of 20, maximum of 16,000.

I first thought that it is to search over each gene and if there is a value out of the bounds, then just discard this gene as a feature. However, it seems for every gene, there must be a sample with the value out of bound.

What should I do by ""threshold this data""? Is that like if the value is below 20, then set it 20 or if the value is above 16000, then just set it as 16000?

In fact, I did the last operation in R by 

    data[data<20] <- 20

and it turns out that the speed of the command is very slow. (79*7070 samples)
",added tags; formatted; removed peripheral comments,2013-10-14 20:47:24.533
186321,57461,674.0,5,,CC BY-SA 3.0,30f8528b-ec84-4928-b491-04cf40c8b1d3,"On [pg. 378][1] of ""Cryptography with Tamperable and Leaky
Memory"", Kalai et al. claim two probability distributions are $e(k)$ close if the distance between them is at most $e(k)$.  

What is significance of two distributions X and Y being ""close to"" or ""far from"" each other? Why would anybody care, especially in cryptography?

  [1]: http://download.springer.com/static/pdf/305/chp%253A10.1007%252F978-3-642-22792-9_21.pdf?auth66=1381954902_7a8eccbd8188fded3878a75ad24f8c83&ext=.pdf",added 65 characters in body,2013-10-14 20:57:16.717
186322,57452,22627.0,5,,CC BY-SA 3.0,2e150419-f34e-494b-9ef3-456837404f4f,"How would one estimate the maximum given population size, a few moments, and perhaps some additional assumption on the distribution?

Something like ""I'm going to do $N_s≫1$ measurements out of population of size $N_p≫N_s$; will record mean $μ_s$, standard deviation $σ_s$, and maximal value in the sample $X_s$; I am willing to assume binomial (or Poisson, etc) distribution; what is the expected maximal value $X_p$ of the entire population?""

Related question: does one need to make the assumptions on the nature of the population distribution, or the sample statistics would be enough to estimate $X_p$?

Edit: the background I just added in the comments may not be clear enough. So here it is:

The end purpose it to print a set of shapes (wires, gates, etc) on a VLSI circuit that matches the designed shapes (a.k.a. targets) as well as possible. The measure of fitness of the manufactured set of shapes is the MAXIMAL difference from the target, rather than the $\sigma$ along the $~10^9$ location. The reason for evaluating the maximum difference is clear: a single short circuit is bad enough to bring down the entire chip, and then it wouldn't matter how close you were to the target in the remaining 99.999999% of the chip's location. 

The problem is that it's very costly to measure the printed shape in too many locations: you literally need to look though an electron microscope at the half-manufactured chip (that's going to get trashed after the destructive measurements), adjust for metrology errors, etc. Therefore more than $10^4$ measurements is hardly ever being done. The result of those measurement is the maximal target difference $X_s$ of the SAMPLE, as well as any other sample statistics you may wish for.

And now one needs to estimate the maximal difference $X_p$ for the entire population... And now one wishes that he paid more attention in the statistics class back in college...",added 1281 characters in body,2013-10-14 21:00:15.407
186323,57319,1693.0,5,,CC BY-SA 3.0,799b3975-d4b3-4a57-bbd3-45f0970c940b,"I am modeling an outcome for hospital patients, 'RA' (whether readmitted).  My predictor of interest is 'HHS' (whether referred to Home Health Services such as from a visiting nurse).  Those referred readmit at a 15.2% rate; others, 9.2%, but the former are needier, sicker patients.  Conventional thinking is that if we controlled for severity of illness this difference would not only be washed out but would reverse itself.  In other words, holding constant the severity of illness, having HHS should mean a lower RA rate.

With HHS as the sole predictor, B in a **logistic** regression = 0.6 (N ~ 25k).  B is reduced to 0.2 with a group of covariates controlled, each accounting for some aspect of severity of illness, but B doesn't fall below zero.

HHS alone explains only about 1% of the variance in RA; with the other predictors, this becomes 4%.* Perhaps this is the problem--that these covariates are not explaining enough variance to ""succeed"" in reversing the sign of the coefficient of interest.  If this is true, is there a way to estimate how high **their** explained variance needs to be for such a reversal to show up?


----------

*Using either of 2 pseudo-RSQ formulas; Cox & Snell's or Menard's [-2LL0 - (-2LL1)] / [-2LL0.]",emphasized things that seem to be getting missed,2013-10-14 21:00:27.517
186324,57462,22143.0,2,,CC BY-SA 3.0,bf401e13-0cfd-4398-8bac-afa630655a96,"*Try 1*:

If $X \sim U[a,b]$ (uniform, either discrete or continuous), then the MLE estimator for b (which is $\max_{x \in [a,b]} X$) is essentially $\max_{i=1,...,N_s}x_i$.

I chose uniform distribution because it is the worst case distribution in terms of entropy. This is in line with the MaxEnt (maximum entropy) principle. I also assumed a linear order in the values of the random variable.

We can make the following claim about the estimator $\max_{i=1,...,N_s}x_i$ to *its* mean using Hoeffdings inequality (without assuming that $X \sim U[a,b]$). Assuming $x_i$ are i.i.d from some distribution with bounded support $[a,b]$, we have
\begin{align*}
\mathbb{P}_{x_1,...,x_{N_s}}\left(|\max_{i=1,...,N_s}x_i - \mathbb{E}[\max_{i=1,...,N_s}x_i]| \geq \epsilon\right) \leq 2\exp\left(\frac{-2\epsilon^2}{N_s(b-a)}\right)
\end{align*}
Here we do not need to know $b$ exactly, any rough or crude upper bound will suffice. The above concentration is only saying that the estimator is close to the expected value of the estimator which is not the same as being close to the unknown $\max_{x \in [a,b] X} = b$.


*Additional comment*: I would make the measurements uniformly at random over the plane/chip so that hopefully no region with high $X$ values is missed. This observation is independent of the above.",,2013-10-14 21:10:03.567
186325,57451,,5,,CC BY-SA 3.0,2f6b0898-ce1f-4eff-97e6-e8eb77b1939f,"Let $U,V,W$ are independent random variables with $\mathrm{Uniform}(0,1)$ distribution. I am trying to find the probability that $Ux^{2}+Vx+W$ has real roots, that is, $P(V^{2}-4UW> 0)$
I have solved this question using double integral but how to do this using triple integral.
My Approach:
I started with cdf:
$P(V^{2}-4UW >0) =P(V^{2} > 4UW) = P(V>2\sqrt{UW})$ = $\int\int_{2\sqrt{uw}}^1 P(V>2\sqrt{UW}) dU dW$
=$\int\int\int_{2\sqrt{uw}}^1 vdU dW dV$

I am finding hard time to get the limits of integral over the region in 3 dimensions.

Using double integral:
$P(V^{2}-4UW >0) =P(V^{2} > 4UW) = P(-2\ln V <-\ln 4 - \ln U - \ln W) = P(X <-\ln 4 +Y)$
where $X=-2 \ln V, Y = - \ln U -\ln W $
$X$ has $\exp(1)$ and $Y$ has $\mathrm{gamma}(2,1)$ distribution.
$P(X <-\ln 4 +Y) = \int_{\ln4}^\infty P(X < -\ln 4 +Y) f_Y(y) dy $
$$=\int_{\ln 4}^\infty\int_0^{-\ln 4+y} \frac{1}{2} e^{-\frac{x}{2}}ye^{-y} dxdy $$
Solving this I got $0.2545$.

Thanks!",added 42 characters in body,2013-10-14 21:11:37.503
186328,57463,22687.0,3,,CC BY-SA 3.0,61c15056-2b23-4203-b6e1-23875b097154,<hypothesis-testing><t-test>,,2013-10-14 21:14:24.377
186326,57463,22687.0,2,,CC BY-SA 3.0,61c15056-2b23-4203-b6e1-23875b097154,"In the dark ages, we would map the results of a Student's t-test to a null hypothesis probability *p* by looking up *T* and degrees of freedom in a table to get an approximate result. 

What is the mathematical algorithm that generates that table? *ie*, how can I write a function to generate a precise *p* given an arbitrary *T* and *df*?

The reason I ask is that I'm writing a piece of embedded software that continually monitors hundreds of populations with hundreds of samples each, and raises an alert if successive snapshots of a given population come to differ significantly. Currently it uses a crude *z*-score comparison, but it would be nice to use a more valid test. ",,2013-10-14 21:14:24.377
186329,57452,668.0,6,,CC BY-SA 3.0,dab5fe07-d5c4-4682-beff-318a5b36ca8f,<estimation><sampling><spatial><extreme-value>,edited tags,2013-10-14 21:18:11.250
186330,57464,22507.0,2,,CC BY-SA 3.0,3577bed7-1dd0-4108-9f1d-f360f3b72378,The algorithm which never received 0's will be grossly biased and predict amost exclusively 1's. ,,2013-10-14 21:19:18.890
186331,57461,,25,,,1c1522b0-ff57-48d2-a5a9-db20fb291343,,http://twitter.com/#!/StackStats/status/389863932598689792,2013-10-14 21:22:59.600
186332,57465,22143.0,2,,CC BY-SA 3.0,0d143c84-dcd7-4ca1-ae6c-63b6cb33c9fc,"I am thinking of the following two points:

 - You are observing the true labels and their associated predictors, a.k.a the pair $y_i,x_i$ only when the algorithm is predicting a label of $1$. The algorithm is updated regardless of whether it made an error or not. This means that there is no feedback on mistakes (like in online learning). We get new data irrespective of our prediction performance.


 - The question we need to ask is then: *Does the algorithm's output influence the data source?* If the algorithm is not influencing the source, then this aspect where we 'conditionally observe new data' will not bias the algorithm by itself (everything else held constant).",,2013-10-14 21:24:50.677
186333,57464,668.0,33,,,2dda2333-42d3-4dce-87db-f82d7f00e7f6,,822,2013-10-14 21:26:23.200
186334,57407,594.0,5,,CC BY-SA 3.0,4548fda7-26e2-4787-ab8d-f628e5069c28,"Since you appear to doubt the example offered, I have included a diagram. As Michael Mayer said, two normal distributions centered around 0, one with larger variance, is sufficient.

In the diagram, we compare the 0.1 and the 0.9 quantiles for $\sigma=1$ (blue) and $\sigma=0.8$ (dark orange)

![normcdfs sigma=1, sigma=0.8][1]

Michael Mayer's example fulfills the requirements of your question with $q_1=0.1$, $q_2=0.9$ and $X_1$ being the one with larger variance.

---

Edit:

For the case where $q_1$ and $q_2$ must both be on the same side of whatever the measure of location is, let's take two symmetric distributions.

Let $X_1$ be $\sim \text{N}(0,1^2)$ and let $X_2$ be an equal mixture of a $\text{N}(-0.8,0.1^2)$ and a  $\text{N}(0.8,0.1^2)$, and let $q_1 = 0.6$ and $q_2 = 0.9$:

![normal 0,1 vs symmetric mixture of normals with small s.d.][2]

This example fulfills the new requirements of your question with $q_1=0.6$, $q_2=0.9$ and $X_1$ being the one with only a single normal component (shown in blue above).

  [1]: https://i.stack.imgur.com/pT43v.png
  [2]: https://i.stack.imgur.com/2dHFj.png",added 613 characters in body,2013-10-14 21:47:55.657
186335,57462,22143.0,5,,CC BY-SA 3.0,be99e4b7-a614-49c0-bcdb-20e91dbd0300,"*Try 1*:

If $X \sim U[a,b]$ (uniform, either discrete or continuous), then the MLE estimator for b (which is $\max_{x \in [a,b]} X$) is essentially $\max_{i=1,...,N_s}x_i$.

I chose uniform distribution because it is the worst case distribution in terms of entropy. This is in line with the MaxEnt (maximum entropy) principle. I also assumed a linear order in the values of the random variable.

We can make the following claim about the estimator $\max_{i=1,...,N_s}x_i$ to *its* mean using Hoeffdings inequality (without assuming that $X \sim U[a,b]$). Assuming $x_i$ are i.i.d from some distribution with bounded support $[a,b]$, we have
\begin{align*}
\mathbb{P}_{x_1,...,x_{N_s}}\left(|\max_{i=1,...,N_s}x_i - \mathbb{E}[\max_{i=1,...,N_s}x_i]| \geq \epsilon\right) \leq 2\exp\left(\frac{-2\epsilon^2}{N_s(b-a)}\right)
\end{align*}
Here we do not need to know $b$ exactly, any rough or crude upper bound will suffice. The above concentration is only saying that the estimator is close to the expected value of the estimator which is not the same as being close to the unknown $\max_{x \in [a,b]}X = b$.


*Additional comment*: I would make the measurements uniformly at random over the plane/chip so that hopefully no region with high $X$ values is missed. This observation is independent of the above.",lowercase X to uppercase X,2013-10-14 21:47:57.387
186336,57401,594.0,5,,CC BY-SA 3.0,4bc20b9d-f358-4cae-85d9-fd2330ce198f,"Let say, I have 2 continuous random variables X1 & X2. Both have same location parameters. Other parameters may be same or may not.

Now say, the q1-th quantile of X1 is less than the q1-th quantile of x2. But the q2-th quantile of x1 is more than the q2th quantile of x2.

My question is, is that possible? Is there any example of x1 & x2 which have that property?

I will be really grateful if someone can give me some pointer.

---

Edit: At this point, I realize the question I asked for was not correctly specified. 

I'm particularly interested in the case where the two quantiles being considered are on the same side of the location parameter. 
",Edited in modification of question,2013-10-14 21:50:33.490
186337,57407,594.0,5,,CC BY-SA 3.0,7c29850d-de7d-4942-b4c9-16f85e4e8aaa,"Since you appear to doubt the example offered, I have included a diagram. As Michael Mayer said, two normal distributions centered around 0, one with larger variance, is sufficient.

In the diagram, we compare the 0.1 and the 0.9 quantiles for $\sigma=1$ (blue) and $\sigma=0.8$ (dark orange)

![normcdfs sigma=1, sigma=0.8][1]

Michael Mayer's example fulfills the requirements of your question with $q_1=0.1$, $q_2=0.9$ and $X_1$ being the one with larger variance.

---

Edit:

For the case where $q_1$ and $q_2$ must both be on the same side of whatever the measure of location is, let's take two symmetric distributions, which share the same mean and median.

Let $X_1$ be $\sim \text{N}(0,1^2)$ and let $X_2$ be an equal mixture of a $\text{N}(-0.8,0.1^2)$ and a  $\text{N}(0.8,0.1^2)$, and let $q_1 = 0.6$ and $q_2 = 0.9$:

![normal 0,1 vs symmetric mixture of normals with small s.d.][2]

This example fulfills the new requirements of your question with $q_1=0.6$, $q_2=0.9$ and $X_1$ being the one with only a single normal component (shown in blue above).

Further, you should note that 'location parameter' isn't sufficiently specified. I could parameterize normal distributions by their 5th percentile and their standard deviation, and call the parameter based on the 5th percentile the location parameter (it's just a shift of the mean by 1.645. and still a perfectly valid location parameter). Then Michael's original example suffices even under the new conditions!

(If that contradicts your intention, your intention needs to be specific enough to exclude it.)

  [1]: https://i.stack.imgur.com/pT43v.png
  [2]: https://i.stack.imgur.com/2dHFj.png",added 613 characters in body,2013-10-14 21:56:22.453
186340,57466,22690.0,3,,CC BY-SA 3.0,12f10fd2-c133-4190-b061-d656b66ce430,<classification><naive-bayes><non-independent>,,2013-10-14 21:58:00.910
186339,57466,22690.0,1,,CC BY-SA 3.0,12f10fd2-c133-4190-b061-d656b66ce430,Naive Bayes with invalid independence assumption,,2013-10-14 21:58:00.910
186349,57469,22507.0,2,,CC BY-SA 3.0,166e842b-c687-4ccc-8081-e2243825f323,"You cannot. ""Accept that $b_1=0$"" is the same as ""reject that $b_1\ne 0$"".  But on what basis you could do this?  No matter how many observations you have, you cannot distinguish between 0 and sufficiently small value of $b_1$.  You can only accept that $|b_1|<\epsilon$  (the smaller $\epsilon$ the more observations you need).",,2013-10-14 22:07:54.383
186352,57468,155.0,4,,CC BY-SA 3.0,25195edb-211a-41e2-a906-7cea371476ed,Whether to log transform variable when untransformed variable has positive skew and transformed has negative skew?,deleted 11 characters in body; edited tags; edited title,2013-10-14 22:08:55.297
186351,57468,155.0,6,,CC BY-SA 3.0,25195edb-211a-41e2-a906-7cea371476ed,<normal-distribution><data-transformation>,deleted 11 characters in body; edited tags; edited title,2013-10-14 22:08:55.297
186350,57468,155.0,5,,CC BY-SA 3.0,25195edb-211a-41e2-a906-7cea371476ed,"I have performed a log transformation on my skewed data, however on my DV it went from positive skew to negative skew after the (log) transformation, further data was missing from my DV after the transformation. Please help",deleted 11 characters in body; edited tags; edited title,2013-10-14 22:08:55.297
186353,57470,22143.0,2,,CC BY-SA 3.0,521a17b0-41bd-4c4a-aa6a-e27917f68ab9,"*Try 2*:

This is a heuristic and I don't know of any statistical guarantees. The procedure is as follows:

 - construct the empirical distribution function. If it looks exponential, convert the values to log scale to see a power-law tail.
 - Fit a curve on this modified histogram. That is, do a 1-D regression. Hopefully the curve mimics the tail of a well-behaved distribution.
 - Pick the point where the line intersects the x-axis in the interval $[\max_{i=1,...,N_s}x_i,\infty)$.

This is another estimator of the max value of the support of the *population*.",,2013-10-14 22:17:38.120
186354,57471,155.0,2,,CC BY-SA 3.0,636a075a-7045-445b-bdf0-fee02ec436e7,"### Additional missing data after log transformation
If you have additional missing data after log transformation, it is likely that you have data that is less than or equal to zero. (i.e., log(0), log(-1), etc. is not defined). So if you want to use a log transformation on data with negative numbers, you need to add a constant to the raw variable so that the minimum of the resulting variable is greater than zero. So your transformation should be

   $$\log(x + c)$$

where $x$ is your untransformed variable and $c = 1 - \textrm{min}(x)$.

### Transformation flips the skewness
There is plenty of discussion on this site about when and whether transformations are useful. You might also like this [discussion of issues surrounding transformations](http://pareonline.net/getvn.asp?v=8&n=6). In general, if a log transformation is flipping the direction of your skewness, then there is a good chance that you did not have very much skewness to begin with. To test whether the transformation makes a substantive difference with the context of multiple regression, examine your correlations, R-squares, and standardised betas before and after transformation, and see what changes you observed. In many cases you will see that it makes little difference.

Another point, is that the assumption pertains to the residuals of a multiple regression and not the dependent variable itself.

If you really care about optimising the transformation to make the variable approximate a normal distribution, then you can use the [box-cox transformation](http://en.wikipedia.org/wiki/Power_transform). Or a simpler approach is just to try a range of transformations. A common set of transformations from greater to less change is:

    -1/x^2
    -1/x
    log(x)
    sqrt(x)

So if `log(x)` is transforming too much, you could try `sqrt(x)`.

",,2013-10-14 22:29:32.767
186355,57468,155.0,6,,CC BY-SA 3.0,36cdac63-432c-4aae-ab87-6ef7390f5283,<normal-distribution><data-transformation><missing-data>,edited title,2013-10-14 22:29:51.690
186356,57468,155.0,4,,CC BY-SA 3.0,36cdac63-432c-4aae-ab87-6ef7390f5283,Whether to log transform variable when untransformed variable has positive skew and transformed has negative skew with additional missing data?,edited title,2013-10-14 22:29:51.690
186357,57471,155.0,5,,CC BY-SA 3.0,8b3e371b-3130-4219-acaa-68fb302ddd0f,"### Additional missing data after log transformation
If you have additional missing data after log transformation, it is likely that you have data that is less than or equal to zero. (i.e., log(0), log(-1), etc. is not defined). So if you want to use a log transformation on data with negative numbers, you need to add a constant to the raw variable so that the minimum of the resulting variable is greater than zero. So your transformation could be

   $$\log(x + c)$$

where $x$ is your untransformed variable and $c = 1 - \textrm{min}(x)$.

### Transformation flips the skewness
There is plenty of discussion on this site about when and whether transformations are useful. You might also like this [discussion of issues surrounding transformations](http://pareonline.net/getvn.asp?v=8&n=6). In general, if a log transformation is flipping the direction of your skewness, then there is a good chance that you did not have very much skewness to begin with. To test whether the transformation makes a substantive difference with the context of multiple regression, examine your correlations, R-squares, and standardised betas before and after transformation, and see what changes you observed. In many cases you will see that it makes little difference.

Another point, is that the assumption pertains to the residuals of a multiple regression and not the dependent variable itself.

If you really care about optimising the transformation to make the variable approximate a normal distribution, then you can use the [box-cox transformation](http://en.wikipedia.org/wiki/Power_transform). Or a simpler approach is just to try a range of transformations. A common set of transformations from greater to less change is:

    -1/x^2
    -1/x
    log(x)
    sqrt(x)

So if `log(x)` is transforming too much, you could try `sqrt(x)`.

",deleted 1 characters in body,2013-10-14 22:34:52.727
186360,57472,21991.0,3,,CC BY-SA 3.0,13705cbe-734f-49e8-8367-005ffc64f799,<regression><probability><mathematical-statistics><expectation-maximization>,,2013-10-14 22:37:36.997
186359,57472,21991.0,1,,CC BY-SA 3.0,13705cbe-734f-49e8-8367-005ffc64f799,Numerical example to understand Expectation-Maximization,,2013-10-14 22:37:36.997
186358,57472,21991.0,2,,CC BY-SA 3.0,13705cbe-734f-49e8-8367-005ffc64f799,"I am trying to get a good grasp on the EM algorithm, to be able to implement and use it. I spent a full day reading the theory and a paper where EM is used to track an aircraft using the position information coming from a radar. Honestly, I dont think I fully understand the underlying idea. Can someone point me to a numerical example showing a few iterations (3-4) of the EM for a simpler problem (like estimating the parameters of a Gaussian distribution or a sequence of a sinusoidal series or fitting a line). 

Even if someone can point me to a piece of code (with synthetic data), I can try and step through the code. 

Thanks a lot. 
 

 ",,2013-10-14 22:37:36.997
186470,57498,16474.0,5,,CC BY-SA 3.0,f948319d-b751-4408-ab0c-565465769ac6,"As statisticians, we come across many distributions under the banners ""discrete"",""continuous"" and ""univariate"",""multivariate"".But can anyone, offer me a good reason behind the existence and motivation for so many distributions. How do we get them? and what can a layman understand from it?

What is the logic behind the existence of distributions?",deleted 18 characters in body,2013-10-15 08:01:56.230
186361,57407,594.0,5,,CC BY-SA 3.0,0ae68851-626f-47fb-b116-4176f95c786f,"Since you appear to doubt the example offered, I have included a diagram. As Michael Mayer said, two normal distributions centered around 0, one with larger variance, is sufficient.

In the diagram, we compare the 0.1 and the 0.9 quantiles for $\sigma=1$ (blue) and $\sigma=0.8$ (dark orange)

![normcdfs sigma=1, sigma=0.8][1]

Michael Mayer's example fulfills the requirements of your question with $q_1=0.1$, $q_2=0.9$ and $X_1$ being the one with larger variance.

---

Edit:

For the case where $q_1$ and $q_2$ must both be on the same side of whatever the measure of location is, let's take two symmetric distributions, which share the same mean and median.

Let $X_1$ be $\sim \text{N}(0,1^2)$ and let $X_2$ be an equal mixture of a $\text{N}(-0.8,0.1^2)$ and a  $\text{N}(0.8,0.1^2)$, and let $q_1 = 0.6$ and $q_2 = 0.9$:

![normal 0,1 vs symmetric mixture of normals with small s.d.][2]

This example fulfills the new requirements of your question with $q_1=0.6$, $q_2=0.9$ and $X_1$ being the one with only a single normal component (shown in blue above).

Further, you should note that 'location parameter' isn't sufficiently specified. I could parameterize normal distributions by their 5th percentile and their standard deviation, and call the parameter based on the 5th percentile the location parameter (it's just a shift of the mean by 1.645\sigma. and can work as a perfectly valid location parameter). Then Michael's original example suffices even under the new conditions. If that contradicts your intention, your intention needs to be stated specifically enough to exclude it.

  [1]: https://i.stack.imgur.com/pT43v.png
  [2]: https://i.stack.imgur.com/2dHFj.png",added 18 characters in body,2013-10-14 22:45:40.010
186362,57471,15827.0,5,,CC BY-SA 3.0,2bb59698-3f9c-4bfa-91ca-1ca5c8970d17,"### Additional missing data after log transformation
If you have additional missing data after log transformation, it is likely that you have data that is less than or equal to zero. (i.e., log(0), log(-1), etc. is not defined). So if you want to use a log transformation on data with negative numbers, you need to add a constant to the raw variable so that the minimum of the resulting variable is greater than zero. So your transformation could be

   $$\log(x + c)$$

where $x$ is your untransformed variable and $c = 1 - \textrm{min}(x)$.

### Transformation flips the skewness
There is plenty of discussion on this site about when and whether transformations are useful. You might also like this [discussion of issues surrounding transformations](http://pareonline.net/getvn.asp?v=8&n=6). In general, if a log transformation is flipping the direction of your skewness, then there is a good chance that you did not have very much skewness to begin with. To test whether the transformation makes a substantive difference with the context of multiple regression, examine your correlations, R-squares, and standardised betas before and after transformation, and see what changes you observed. In many cases you will see that it makes little difference.

Another point, is that the assumption pertains to the residuals of a multiple regression and not the dependent variable itself.

If you really care about optimising the transformation to make the variable approximate a normal distribution, then you can use the [Box-Cox transformation](http://en.wikipedia.org/wiki/Power_transform). Or a simpler approach is just to try a range of transformations. A common set of transformations from greater to less change is:

    -1/x^2
    -1/x
    log(x)
    sqrt(x)

So if `log(x)` is transforming too much, you could try `sqrt(x)`.

","Box, Cox proper names ",2013-10-14 22:48:09.820
186363,57467,16469.0,6,,CC BY-SA 3.0,fa327cb6-3f3e-4483-8587-c59b514bf44b,<hypothesis-testing><statistical-significance><equivalence>,edited tags,2013-10-14 22:50:41.347
186366,57473,5643.0,3,,CC BY-SA 3.0,7be0dfba-53df-4825-b890-17af59f59aaa,<standard-error><group-differences><sample-mean>,,2013-10-14 23:00:43.557
186365,57473,5643.0,1,,CC BY-SA 3.0,7be0dfba-53df-4825-b890-17af59f59aaa,Intuition for the difference of sample means,,2013-10-14 23:00:43.557
186364,57473,5643.0,2,,CC BY-SA 3.0,7be0dfba-53df-4825-b890-17af59f59aaa,"I read in Wilcox, 2003 p. 247 that the standard error of the difference between two sample means is (assuming the normality and homoskedasticity assumptions):

$\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$

Rather than simply adding the two distributions standard errors as in:

$\frac{\sigma_1^2}{\sqrt{n_1}} + \frac{\sigma_2^2}{\sqrt{n_2}}$

What is the intuition behind squaring the sum of the two variances divided into their respective sample size?",,2013-10-14 23:00:43.557
186370,57474,22163.0,3,,CC BY-SA 3.0,49318068-1b21-415f-8ca4-bfc985f90017,<regression><logistic><spss><generalized-linear-model>,,2013-10-14 23:12:42.747
186369,57474,22163.0,2,,CC BY-SA 3.0,49318068-1b21-415f-8ca4-bfc985f90017,"I want to do a logistic regression in SPSS. However, since I analyse unemployment spells the subjects are sometimes repeated (violating the independence assumption of the regression). One way of removing the within subject variation is by applying a Genlin model with the repeated subject subcommand (in essence a GEE model). Thus, I tried out a Genlin model with binomal probability and the logit link, comparing it to a standard logistic regression. I used the exact same variables in the two procedures. 

However, the results that was delivered from the Genlin procedure was inverted relative to that of the logistic regression. For instance: Exp(B) for women (of the independent variable sex/gender) was just above 2.0 in logistic regression while being at 0.49 in Genlin. The same happened with every independent variable.

 - Any suggestions to why the results of the Genlin procedure is
   inverted?  
 - Is there any way to get Genlin results in accordance to the logistic regression?",,2013-10-14 23:12:42.747
186368,57474,22163.0,1,,CC BY-SA 3.0,49318068-1b21-415f-8ca4-bfc985f90017,Inverse SPSS results: Logistic regression command vs. Genlin?,,2013-10-14 23:12:42.747
186371,57475,503.0,2,,CC BY-SA 3.0,93fa2ee1-d5ef-4acd-8ec1-e4a5a18418e6,"Look into equivalence testing.  See [this search][1] for lots of threads. Also see [Esteban & Nowacki][2]


  [1]: http://stats.stackexchange.com/search?q=equivalence%20testing
  [2]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3019319/",,2013-10-14 23:15:43.797
186372,57476,503.0,2,,CC BY-SA 3.0,3ab9d683-82ef-4a69-a3ca-f9b555418e55,"You don't square the sum of the variances, you take the square root of thQe sum of the variances. You do this for the same reason that the standard deviation is the square root of the variance: It make the units the same as the original ones, rather than squared units. 

Although we often lose site of it while doing statistics, the square of a measure involve squaring the *measure* as well as the number of units. For example, the square of 2 meters is not 4 meters, it is 4 meters squared, more commonly called 4 square meters. The same thing happens with other units that we aren't used to thinking of in this way: e.g. if you are measuring IQ, the square of an IQ is not an IQ of 10,000; it is a squared IQ of 10,000. 

You divide by the sample size as a scaling technique. Variances (tend to) go up with sample size; you divide by n to deal with that. ",,2013-10-14 23:21:40.057
186373,57476,15827.0,5,,CC BY-SA 3.0,6288582f-90b1-4736-ace3-d63c29d70dca,"You don't square the sum of the variances, you take the square root of the sum of the variances. You do this for the same reason that the standard deviation is the square root of the variance: It make the units the same as the original ones, rather than squared units. 

Although we often lose sight of it while doing statistics, the square of a measure involve squaring the *measure* as well as the number of units. For example, the square of 2 meters is not 4 meters, it is 4 meters squared, more commonly called 4 square meters. The same thing happens with other units that we aren't used to thinking of in this way: e.g. if you are measuring IQ, the square of an IQ is not an IQ of 10,000; it is a squared IQ of 10,000. 

You divide by the sample size as a scaling technique. Variances (tend to) go up with sample size; you divide by $n$ to deal with that. ",small fixes,2013-10-14 23:23:03.383
186374,57477,594.0,2,,CC BY-SA 3.0,78b3e376-3e9a-4f3a-9ee2-94dd21d38cd4,"For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$

You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. 

This is not the case for independent variables.

http://en.wikipedia.org/wiki/Variance#Basic_properties
",,2013-10-14 23:40:17.447
186375,57477,594.0,5,,CC BY-SA 3.0,3ee355a1-e8e0-4d5f-8979-e9a468c0efe5,"You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. 

This is not the case for independent variables.

For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$

Further, 

$\text{Var}(\bar X) = \text{Var}(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}\text{Var}(\sum_iX_i)= \frac{1}{n^2}\sum_i\text{Var}(X_i)= \frac{1}{n^2}\cdot n\cdot\sigma^2_1= \sigma^2_1/n$

(if the $X_i$ are independent of each other).

http://en.wikipedia.org/wiki/Variance#Basic_properties

(If the algebra is unconvincing, or you don't believe the basic properties of variance that I used, you might consider simulation, which should soon make it clear.)",added 198 characters in body,2013-10-14 23:46:12.817
186378,57478,7860.0,3,,CC BY-SA 3.0,6ce06302-530c-47d7-b95f-30367712e271,<python><density-function><error><kde>,,2013-10-15 00:04:46.003
186377,57478,7860.0,1,,CC BY-SA 3.0,6ce06302-530c-47d7-b95f-30367712e271,Adding errors to Gaussian kernel density estimator,,2013-10-15 00:04:46.003
186376,57478,7860.0,2,,CC BY-SA 3.0,6ce06302-530c-47d7-b95f-30367712e271,"I'm using the [scipy.stats.gaussian_kde][1] function to generate a `KDE` from a set of $N$ points in a 2D space: $A = \{(x_1,y_1), (x_2,y_2), (x_3,y_2), ..., (x_N,y_N)\}$

Each one of these points has a given error attached to it. So for example, the point $(x_1,y_1)$ has errors $(e_{x_1},e_{y_1})$ and so on. I can assume the errors are normally distributed in both axis.

The `python` function that I use to generate the `KDE` has no way to integrate these errors into the calculations and I wonder how I would even do such a thing if I did it manually.

Ie: what is the statistically correct way to generate a `KDE` accounting for errors in the data used?


  [1]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html",,2013-10-15 00:04:46.003
186379,57473,5643.0,5,,CC BY-SA 3.0,2b79a748-b9aa-41a9-80d5-8abfe4c490ec,"I read in Wilcox, 2003 p. 247 that the standard error of the difference between two sample means is (assuming the normality and homoskedasticity assumptions):

$\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$

Rather than simply adding the two samples standard errors as in:

$\frac{\sigma_1^2}{\sqrt{n_1}} + \frac{\sigma_2^2}{\sqrt{n_2}}$

What is the intuition behind squaring the sum of the two variances divided into their respective sample size?","Rephrased, question is about sample mean errors, not the underlying distributions",2013-10-15 00:20:57.200
186380,57468,,25,,,54dbb17a-d61b-472e-9152-fa82fbd069a0,,http://twitter.com/#!/StackStats/status/389909221011386368,2013-10-15 00:22:57.193
186383,57479,64247.0,2,abe3,CC BY-SA 3.0,faa20063-e539-41da-84c8-90ad3c0a7df3,"I am looking over slides for a big data class. The slides suggest doing a pairwise plot of data (if not too many variables) to evaluate the quality of output from k-means clustering -- with each data point color-coded by its cluster. The slides say: 

> If the (colored) clusters look separated in at least some of the plots. They won’t be very separated in all of the plots.

How would this tell you if a pairwise plot is effective? You would want the colors to be mixed up in the plots to make sure that you have genuine multi-dimensional clusters and not just groups of data points that are very similar on one variable?",,2013-10-15 00:37:03.313
186382,57479,64247.0,3,abe3,CC BY-SA 3.0,faa20063-e539-41da-84c8-90ad3c0a7df3,<machine-learning>,,2013-10-15 00:37:03.313
186381,57479,64247.0,1,abe3,CC BY-SA 3.0,faa20063-e539-41da-84c8-90ad3c0a7df3,How would you use pair-wise plots to test the effectiveness of k-means clustering?,,2013-10-15 00:37:03.313
186384,57480,2121.0,2,,CC BY-SA 3.0,d6786cef-171e-4805-8863-c37895c71e51,"I think if each dataset is already weighted to your satisfaction, then you have a couple of different options. Which one is the right one may vary based on your objectives and the particulars of your existing data collection and weighting. 

 - Union all of the datasets, along with their pre-calculated weights, and that's it.

> This would be the right choice if each dataset was weighted towards a proper total count and didn't over-state the importance of any individual record relative to another dataset. If one dataset was weighted to reflect Total US Population, and another dataset was weighted in place to its own total count of respondents, then this would not be the right choice.

 - Calculate a weight for each dataset to multiply by each record's existing weight

> This would be the right choice if each of your datasets are of equal importance regardless of their size. Example below...

 - Union all of the raw data and re-calculate the weights on the new, entire dataset

> This would be the right choice if the reasons for non-response are similar across your different surveys - it results in the simplest data for you to work with, and it's the least likely to produce extreme weights.

Example for #2: each dataset is weighted to equal importance, with this ""dataset weight"" being multiplied by whatever weight has already been calculated within the dataset. 

    > Survey 1: 100 people   weight:  2
    > Survey 2: 200 people   weight:  1
    > Survey 3: 300 people   weight:  2/3
    > Survey 4: 150 people   weight:  4/3
    > Survey 5: 250 people   weight:  4/5",,2013-10-15 01:07:38.337
186387,57481,14548.0,3,,CC BY-SA 3.0,9ff8dbd6-f8e4-4b1f-a4ff-81f0618288d4,<regression><confidence-interval>,,2013-10-15 01:08:53.047
186386,57481,14548.0,2,,CC BY-SA 3.0,9ff8dbd6-f8e4-4b1f-a4ff-81f0618288d4,"Having performed linear regression, I can find the confidence interval for the response conditioned on a particular x value. However, I am interested in a C.I for the *mean* response for a set of N new observations. That is, I need to combine the N prediction intervals intervals. 

The closest post I could find was http://stats.stackexchange.com/questions/8755/calculating-the-mean-using-regression-data, but it only handles the univariate case.

I tried deriving the standard error of the mean response below, but I'm not sure if this correct.

$\begin{align}
var(\hat{\bar{y}}) &= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_1 \ldots x_n \right) \\
&= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_i \right), \quad \text{where the } \hat{y_i}|x_i \text{ are independent} \\
&= \frac{1}{n^2}  \sum_i var(\hat{y}_i|x_i) \\
\end{align}$ 

where $var(\hat{y}_i|x_i) = \sqrt{\sigma^2 x_i^T (X^TX)^{-1}x_i}$ for $x_i$ in the training data and $var(\hat{y}_i|x^*_i) = \sqrt{\sigma^2 (1+ x_i^{*T} (X^TX)^{-1}x^*_i)}$ for $x_i$ in the test data.

Am I right track here? Also, is there an R implementation somewhere, or should I do it from scratch?

Thanks,

A.",,2013-10-15 01:08:53.047
186385,57481,14548.0,1,,CC BY-SA 3.0,9ff8dbd6-f8e4-4b1f-a4ff-81f0618288d4,Combining prediction confidence intervals in a regression,,2013-10-15 01:08:53.047
186390,57482,22695.0,3,,CC BY-SA 3.0,2524c28b-0a79-43cd-b8dd-a276358e8fc3,<bayesian><poisson-distribution><conditional-probability><prior><posterior>,,2013-10-15 01:09:29.977
186389,57482,22695.0,1,,CC BY-SA 3.0,2524c28b-0a79-43cd-b8dd-a276358e8fc3,Finding the full conditonal distribution when there are multiple distributions involved,,2013-10-15 01:09:29.977
186388,57482,22695.0,2,,CC BY-SA 3.0,2524c28b-0a79-43cd-b8dd-a276358e8fc3,"6 neighboring countries have the following disease instances: $y = (y_1, y_2,...,y_n)$ with a population of $x = (x_1, x_2,...,x_n)$.

The following model and prior distributions are considered:

$y_i|\theta_i,p_i  \sim poisson(\theta_i x_i)$

$\theta_i | \alpha, \beta \sim gamma(\alpha, \beta)$

$\alpha \sim gamma(1,1)$

$\beta \sim gamma(10,1)$

a) Find the full conditional rate $p(\theta_i | \theta_{-i}, \alpha, \beta, x, y)$

b) Find the posterior distribution.

Attempt: 

a) For finding the conditional rate with two variables, I would use Bayes' theory. I am not sure if this applies with multiple distributions.

$$p(\theta_i | \theta_{-i}, \alpha, \beta, x, y) = \frac{P(\theta_i \bigcap \theta_{-i} \bigcap \alpha \bigcap \beta \bigcap x \bigcap y)}{P( \theta_{-i}, \alpha, \beta, x, y)}$$

$$ = \frac{P(\theta_{-i}, \alpha, \beta, x, y | \theta_i)P(\theta_i)}{\sum_{i=1}^6 P(\theta_{-i}, \alpha, \beta, x, y | \theta_i)P(\theta_i)}$$

b) The posterior probability is the (prior)x(likelihood). So this would be $$poision(\theta_i x_i) x L(theta_i x_i)$$

I'm not sure how to do the pdf of a poisson variable as it is variable. The likelihood function is $L(\theta_i y_i) = \frac{\theta_i^{\sum_{i=1}^n y_i} e^{-n \theta_i}}{y_1!,y_2!,..,y_n!}$",,2013-10-15 01:09:29.977
186393,57483,22659.0,3,,CC BY-SA 3.0,68a5c8ea-ab92-4955-b766-da8fe9bc7aee,<machine-learning><gaussian-process>,,2013-10-15 01:19:13.100
186392,57483,22659.0,1,,CC BY-SA 3.0,68a5c8ea-ab92-4955-b766-da8fe9bc7aee,Scikit-learn's Gaussian Processes: how to include multiple hyperparameters in kernel/cov function?,,2013-10-15 01:19:13.100
186391,57483,22659.0,2,,CC BY-SA 3.0,68a5c8ea-ab92-4955-b766-da8fe9bc7aee,"I'm using the scikits-learn implementation of gaussian processes. A simple thing to do is to combine multiple kernels as a linear combination to describe your timeseries properly. So I'd like to include both the squared exponential kernel and the periodic kernel. Linear combinations of valid kernels produce valid kernels, and same goes for multiplying valid kernels (given by Rasmussen and Wiliams).

Unfortunately I haven't figured out how to give the theta parameters properly to the model. For example, if we have

$$
k_{gauss}(x,x') = \exp{(\theta (x-x')^2)}
$$

then it is alright (this is how the squared-exponential kernel is defined in scikits-learn). But if I wanted

$$
k_{gauss}(x,x') = \theta_0 \exp{(\theta_1 (x-x')^2)}
$$

then it is impossible, it seems. The $\mathbf{\theta}$ thing is supposed to be an array, in case you have multiple dimensions/features (even though scikits-learn doesn't support multidimensional GPs, someone developed it and it will be merged soon). So there is one row with the columns being the parameter in such-and-such dimension. But you cannot have more rows, otherwise it screams at you.

So question: has anyone actually been able to use kernels that use more than one hyperparameter? If so, what am I doing wrong? And if it is indeed not possible with the current code in scikits, does anyone have some tips on how to extend it so that it can? This is a really important feature that I need. Thanks.",,2013-10-15 01:19:13.100
186394,57482,594.0,5,,CC BY-SA 3.0,f58526e1-91e2-4f64-8482-cdcd0b0b61ff,"6 neighboring countries have the following disease instances: $y = (y_1, y_2,...,y_n)$ with a population of $x = (x_1, x_2,...,x_n)$.

The following model and prior distributions are considered:

$y_i|\theta_i,p_i  \sim \text{Poisson}(\theta_i x_i)$

$\theta_i | \alpha, \beta \sim \text{gamma}(\alpha, \beta)$

$\alpha \sim \text{gamma}(1,1)$

$\beta \sim \text{gamma}(10,1)$

a) Find the full conditional rate $p(\theta_i | \theta_{-i}, \alpha, \beta, x, y)$

b) Find the posterior distribution.

Attempt: 

a) For finding the conditional rate with two variables, I would use Bayes' theory. I am not sure if this applies with multiple distributions.

$$p(\theta_i | \theta_{-i}, \alpha, \beta, x, y) = \frac{P(\theta_i \bigcap \theta_{-i} \bigcap \alpha \bigcap \beta \bigcap x \bigcap y)}{P( \theta_{-i}, \alpha, \beta, x, y)}$$

$$ = \frac{P(\theta_{-i}, \alpha, \beta, x, y | \theta_i)P(\theta_i)}{\sum_{i=1}^6 P(\theta_{-i}, \alpha, \beta, x, y | \theta_i)P(\theta_i)}$$

b) The posterior probability is the (prior)x(likelihood). So this would be $$\text{Poisson}(\theta_i x_i) \times L(\theta_i x_i)$$

I'm not sure how to do the pdf of a Poisson variable as it is variable. The likelihood function is $L(\theta_i y_i) = \frac{\theta_i^{\sum_{i=1}^n y_i} e^{-n \theta_i}}{y_1!,y_2!,..,y_n!}$",formatting,2013-10-15 01:21:50.723
186395,57473,594.0,5,,CC BY-SA 3.0,0bc4ab9c-c6f5-4737-9603-0f5fa9867269,"I read in Wilcox, 2003 p. 247 that the standard error of the difference between two sample means is (assuming the normality and homoskedasticity assumptions):

$\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$

Rather than simply adding the two sample standard errors as in:

$\frac{\sigma_1}{\sqrt{n_1}} + \frac{\sigma_2}{\sqrt{n_2}}$

What is the intuition behind squaring the sum of the two variances divided into their respective sample size?",deleted 5 characters in body,2013-10-15 01:23:16.260
186396,57407,594.0,5,,CC BY-SA 3.0,62d6b7f5-f1fe-4859-a46c-c0af7e5c3043,"Since you appear to doubt the example offered, I have included a diagram. As Michael Mayer said, two normal distributions centered around 0, one with larger variance, is sufficient.

In the diagram, we compare the 0.1 and the 0.9 quantiles for $\sigma=1$ (blue) and $\sigma=0.8$ (dark orange)

![normcdfs sigma=1, sigma=0.8][1]

Michael Mayer's example fulfills the requirements of your question with $q_1=0.1$, $q_2=0.9$ and $X_1$ being the one with larger variance.

---

Edit:

For the case where $q_1$ and $q_2$ must both be on the same side of whatever the measure of location is, let's take two symmetric distributions, which share the same mean and median.

Let $X_1$ be $\sim \text{N}(0,1^2)$ and let $X_2$ be an equal mixture of a $\text{N}(-0.8,0.1^2)$ and a  $\text{N}(0.8,0.1^2)$, and let $q_1 = 0.6$ and $q_2 = 0.9$:

![normal 0,1 vs symmetric mixture of normals with small s.d.][2]

This example fulfills the new requirements of your question with $q_1=0.6$, $q_2=0.9$ and $X_1$ being the one with only a single normal component (shown in blue above).

Further, you should note that 'location parameter' isn't sufficiently specified. I could parameterize normal distributions by their 5th percentile and their standard deviation, and call the parameter based on the 5th percentile the location parameter (it's just a shift of the mean by $1.645\sigma$. and can work as a perfectly valid location parameter). Then Michael's original example suffices even under the new conditions. If that contradicts your intention, your intention needs to be stated specifically enough to exclude it.

  [1]: https://i.stack.imgur.com/pT43v.png
  [2]: https://i.stack.imgur.com/2dHFj.png",added 2 characters in body,2013-10-15 01:24:42.953
186397,57473,5643.0,5,,CC BY-SA 3.0,ecc2a928-5bd1-42cb-b396-fd330d53b2e4,"I read in Wilcox, 2003 p. 247 that the standard error of the difference between two sample means is (assuming the normality and homoskedasticity assumptions):

$\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$

Rather than simply adding the two sample standard errors as in:

$\frac{\sigma_1}{\sqrt{n_1}} + \frac{\sigma_2}{\sqrt{n_2}}$

What is the intuition behind taking the square of the sum of the two variances divided into their respective sample size, rather than the sum of the standard errors?",added 12 characters in body,2013-10-15 01:42:01.237
186398,57481,14548.0,5,,CC BY-SA 3.0,41881012-ce7a-46db-a81c-d7ad9962b1d3,"Having performed linear regression, I can find the confidence interval for the response conditioned on a particular x value. However, I am interested in a C.I for the *mean* response for a set of N new observations. That is, I need to combine the N prediction intervals intervals. 

The closest post I could find was http://stats.stackexchange.com/questions/8755/calculating-the-mean-using-regression-data, but it only handles the univariate case.

I tried deriving the standard error of the mean response below, but I'm not sure if this correct.

$\begin{align}
var(\hat{\bar{y}}) &= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_1 \ldots x_n \right) \\
&= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_i \right), \quad \text{where the } \hat{y_i}|x_i \text{ are independent} \\
&= \frac{1}{n^2}  \sum_i var(\hat{y}_i|x_i) \\
\end{align}$ 

where $var(\hat{y}_i|x_i) = \sqrt{\sigma^2 x_i^T (X^TX)^{-1}x_i}$ for $x_i$ in the training data and $var(\hat{y}_i|x^*_i) = \sqrt{\sigma^2 (1+ x_i^{*T} (X^TX)^{-1}x^*_i)}$ for $x^*_i$ in the test data.

Am I right track here? Also, is there an R implementation somewhere, or should I do it from scratch?

Thanks,

A.",added 2 characters in body,2013-10-15 01:51:40.657
186400,57484,22698.0,1,,CC BY-SA 3.0,9c09643e-5a5a-453c-92c1-eb7ab9d19d61,Bound for the correlation of three random variables,,2013-10-15 01:55:03.623
186401,57484,22698.0,3,,CC BY-SA 3.0,9c09643e-5a5a-453c-92c1-eb7ab9d19d61,<correlation><random-variable>,,2013-10-15 01:55:03.623
186399,57484,22698.0,2,,CC BY-SA 3.0,9c09643e-5a5a-453c-92c1-eb7ab9d19d61,There are three random variables with the same correlation pho. What’s the tightest bound you can give for pho?,,2013-10-15 01:55:03.623
186402,57485,7155.0,2,,CC BY-SA 3.0,7738bdad-8e94-4a84-a6b4-06e11160436e,"On scikit-learn==0.14.1.

$\theta_0$ can be a vector. The following code works for me.

    import numpy as np
    from sklearn.gaussian_process import GaussianProcess
    from sklearn.datasets import make_regression
    X, y = make_regression()
    bad_theta = np.abs(np.random.normal(0,1,100))
    model = GaussianProcess(theta0=bad_theta)
    model.fit(X,y)

You can pass any kernel you want as the parameter corr. The following is the radial basis function that sklearn uses for Gaussian processes.

    def squared_exponential(theta, d):
        """"""
        Squared exponential correlation model (Radial Basis Function).
        (Infinitely differentiable stochastic process, very smooth)::

                                                n
            theta, dx --> r(theta, dx) = exp(  sum  - theta_i * (dx_i)^2 )
                                            i = 1

        Parameters
        ----------
        theta : array_like
            An array with shape 1 (isotropic) or n (anisotropic) giving the
            autocorrelation parameter(s).

        dx : array_like
            An array with shape (n_eval, n_features) giving the componentwise
            distances between locations x and x' at which the correlation model
            should be evaluated.

        Returns
        -------
        r : array_like
            An array with shape (n_eval, ) containing the values of the
            autocorrelation model.
        """"""

        theta = np.asarray(theta, dtype=np.float)
        d = np.asarray(d, dtype=np.float)

        if d.ndim > 1:
            n_features = d.shape[1]
        else:
            n_features = 1

        if theta.size == 1:
            return np.exp(-theta[0] * np.sum(d ** 2, axis=1))
        elif theta.size != n_features:
            raise ValueError(""Length of theta must be 1 or %s"" % n_features)
        else:
            return np.exp(-np.sum(theta.reshape(1, n_features) * d ** 2, axis=1))

It looks like you're doing something pretty interesting, btw.
",,2013-10-15 01:56:23.430
186404,57486,22677.0,1,,CC BY-SA 3.0,e8c88b1b-732d-4f19-8efb-19c08704ee6b,How does one determine what ARL0 should be used on CPM package to test for Structural Change,,2013-10-15 02:10:01.050
186405,57486,22677.0,2,,CC BY-SA 3.0,e8c88b1b-732d-4f19-8efb-19c08704ee6b,"i'm trying to find multiple break point by using `processStream` from `CPM` package on `R` 
can someone enlighten me on what is ***ARL0*** how does one determine what ***ARL0*** should be used

+ `processStream(ret.fin.chn,""Kolmogorov-Smirnov"",ARL0=500,lambda=NA)`
+ `$changePoints`
+  `[1]   59   75  250  286  443  448  663 1037 1042 1261 1576 1842 1853 2013 2035 2621 2633`
+ `$detectionTimes`
+ `[1]   73   89  285  334  447  503  670 1040 1145 1428 1639 1951 1874 2030 2078 2632 2644`

while 

+ `processStream(ret.fin.chn,""Kolmogorov-Smirnov"",ARL0=2000,lambda=NA)`
+ `$changePoints'
+ `[1]   59   75  663 1037 1261 1559 1842 2013 2035 2621 2633`
+ `$detectionTimes`
+ `[1]   75   90  691 1041 1480 1688 2026 2032 2266 2633 2646`

and

+ `processStream(ret.fin.chn,""Kolmogorov-Smirnov"",ARL0=3000,lambda=NA)`
+ `$changePoints`
+ `[1]   59   75  663 1037 1261 1559 1842 2013 2149`
+ `$detectionTimes`
+ `[1]   75   92  692 1041 1490 1690 2026 2032 2284`

so its seems that different ARL0 will gives fewer break point detection, its that a good thing? 

*note: the times series `ret.fin.chn` contains 2749 rows*

below are excerpts from `R` help

>ARL0

>Determines the ARL_0 which the CPM should have, which corresponds to the average number of observations before a false positive occurs, assuming that the sequence does not undergo a chang. Because the thresholds of the CPM are computationally expensive to estimate, the package contains pre-computed values of the thresholds corresponding to several common values of the ARL_0. This means that only certain values for the ARL_0 are allowed. Specifically, the ARL_0 must have one of the following values: 370, 500, 600, 700, ..., 1000, 2000, 3000, ..., 10000, 20000, ..., 50000*
",,2013-10-15 02:10:01.050
186403,57486,22677.0,3,,CC BY-SA 3.0,e8c88b1b-732d-4f19-8efb-19c08704ee6b,<r><code><structural-change>,,2013-10-15 02:10:01.050
186406,57481,14548.0,5,,CC BY-SA 3.0,7f56abee-c49a-46f0-81b7-944bb8c252d0,"Having performed linear regression, I can find the confidence interval for the response conditioned on a particular x value. However, I am interested in a C.I for the *mean* response for a set of N new observations. That is, I need to combine the N prediction intervals intervals. 

The closest post I could find was http://stats.stackexchange.com/questions/8755/calculating-the-mean-using-regression-data, but it only handles the univariate case.

I tried deriving the standard error of the mean response below, but I'm not sure if this correct.

$\begin{align}
var(\hat{\bar{y}}) &= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_1 \ldots x_n \right) \\
&= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_i \right), \quad \text{where the } \hat{y_i}|x_i \text{ are independent} \\
&= \frac{1}{n^2}  \sum_i var(\hat{y}_i|x_i) \\
\end{align}$ 

where $var(\hat{y}_i|x_i) = \sqrt{\sigma^2 x_i^T (X^TX)^{-1}x_i}$ for $x_i$ in the training data and $var(\hat{y}_i|x^*_i) = \sqrt{\sigma^2 (1+ x_i^{*T} (X^TX)^{-1}x^*_i)}$ for $x^*_i$ in the test data.

Am I on the right track here? Also, is there an R implementation somewhere, or should I do it from scratch?

Thanks,

A.",fixed grammar,2013-10-15 02:15:34.537
186409,57487,13037.0,1,,CC BY-SA 3.0,735b28ff-8b3c-4a08-bedf-85e50bf60955,Weighted Least Squares Estimate,,2013-10-15 02:16:00.033
186408,57487,13037.0,2,,CC BY-SA 3.0,735b28ff-8b3c-4a08-bedf-85e50bf60955,"Here is a problem from a practice test. Suppose that $$X_i = \mu + \epsilon_i,\quad i=1,\ldots,n\quad \epsilon_i\sim N(0,\sigma^2_1)$$ $$Y_i = \mu + \delta_i,\quad i=1,\ldots,m\quad \delta_i\sim N(0,\sigma^2_2)$$ All $\epsilon_i$'s and $\delta_i$'s are independent. The paramters $\mu, \sigma_1^2, $ and $\sigma_2^2$ are unknown. Let $\theta=m/n$, $\rho=\sigma_2^2/\sigma_1^2$. Suppose $\rho$ is known. Show that the least squares (weighted) estimator of $\mu$ is $$ \hat{\mu} = \dfrac{\rho\bar{X} + \theta\bar{Y}}{\rho+\theta}$$

MY ATTEMPT:

I can't figure out how to use the fact that $\rho$ is known. I tried $$\hat{\mu} = \text{argmin}\left\{\sum_{i=1}^n (X_i-\mu)^2 + \sum_{i=1}^m (Y_i-\mu)^2\right\}$$ and arrived that the weighted averaged $$ \hat{\mu} = \dfrac{n\bar{X} + m\bar{Y}}{n+m}$$ But again this does not use the fact that we know what the ratio $\sigma_2^2/\sigma_1^2$ is. Any ideas?",,2013-10-15 02:16:00.033
186407,57487,13037.0,3,,CC BY-SA 3.0,735b28ff-8b3c-4a08-bedf-85e50bf60955,<self-study><least-squares>,,2013-10-15 02:16:00.033
186410,57477,594.0,5,,CC BY-SA 3.0,e14e6a4e-c63b-44cd-81cf-6443b465e117,"You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. 

This is not the case for independent variables.

For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$

Further, 

$\text{Var}(\bar X) = \text{Var}(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}\text{Var}(\sum_iX_i)= \frac{1}{n^2}\sum_i\text{Var}(X_i)= \frac{1}{n^2}\cdot n\cdot\sigma^2_1= \sigma^2_1/n$

(if the $X_i$ are independent of each other).

http://en.wikipedia.org/wiki/Variance#Basic_properties

In summary: the correct term has $\sigma^2/n$ terms because we're looking at averages and that's the variance of an average of independent random variables. It has a $+$ because the two samples are independent, so their variances add. It has a square root because we want the standard deviation of the distribution of the difference in sample means (the standard error of the difference in means). That's *every single part of the formula explained*.

The reason why we don't just add standard errors is *standard errors don't add*. The *variances* add, though, so we can use that to work out the standard errors.

(If the algebra is unconvincing, or you don't believe the basic properties of variance that I used, you might consider simulation, which should soon make it clear.)",added 620 characters in body,2013-10-15 02:21:47.723
186411,57477,594.0,5,,CC BY-SA 3.0,7d6e02ab-fed7-4363-9ece-55f262151ce4,"You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. 

This is not the case for independent variables.

For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$

Further, 

$\text{Var}(\bar X) = \text{Var}(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}\text{Var}(\sum_iX_i)= \frac{1}{n^2}\sum_i\text{Var}(X_i)= \frac{1}{n^2}\cdot n\cdot\sigma^2_1= \sigma^2_1/n$

(if the $X_i$ are independent of each other).

http://en.wikipedia.org/wiki/Variance#Basic_properties

In summary: the correct term:

![annotated se formula][1]

1) has $\sigma^2/n$ terms because we're looking at averages and that's the variance of an average of independent random variables. 

2) It has a $+$ because the two samples are independent, so their variances add. 

3) It has a square root because we want the standard deviation of the distribution of the difference in sample means (the standard error of the difference in means). 

That's *every single part of the formula explained*.

The reason why we don't just add standard errors is *standard errors don't add* - the standard error of the difference in means is NOT the sum of the standard errors of the means. The variances *do* add, though, so we can use that to work out the standard errors.

(If the algebra is unconvincing, or you don't believe the basic properties of variance that I used, you might consider simulation, which should soon make it clear.)


  [1]: https://i.stack.imgur.com/TUqrV.png",added 98 characters in body,2013-10-15 02:41:42.597
186412,57488,17730.0,2,,CC BY-SA 3.0,c2d5107a-1748-47d5-aa9d-7f0d0410c4ee,"I want to express the joint probability of $\Phi_A$ and $\Phi_B$: $p(\Phi_A, \Phi_B)$  conditioned that $\Phi_A$ and $\Phi_B$ are both greater than some value C.  How would I express this mathematically?  I guess my intuition says:

$p(\Phi_A, \Phi_B | \Phi>C)$

Is this correct? Is there a better way to express this?",,2013-10-15 02:45:34.410
186414,57488,17730.0,3,,CC BY-SA 3.0,c2d5107a-1748-47d5-aa9d-7f0d0410c4ee,<conditional-probability><joint-distribution>,,2013-10-15 02:45:34.410
186413,57488,17730.0,1,,CC BY-SA 3.0,c2d5107a-1748-47d5-aa9d-7f0d0410c4ee,How to express joint conditional probability with multiple conditions,,2013-10-15 02:45:34.410
186471,57429,20740.0,5,,CC BY-SA 3.0,24e0b951-6a32-4a75-91ba-8862a89af12f,"
I am looking to do a linear regression on two independent variables that will be present in varying proportions.

For example trying to do a linear regression on $Y$ which is payment behavior (payback rate) of customers based on the the quality (let's say Gini coefficient) of the new and existing customer credit scores ($X_1$ and $X_2$, respectively) adjusted for the proportion of new and existing customers in the sample.

Existing customers will be present in proportion $p$ and new customers in proportion $1-p = q$.

$Y$, payback rate is the percentage of total customers who pay back.  It could be expressed as the weighted average $Y = Y_1q + Y_2p$ where $Y_i$ is the payback rate of new/existing customers.

In general more new customers, $q$, has a negative effect.  Better scoring ($X_1, X_2$) and more existing customers p have a positive effect.

What is a good way to model this? 

Would something like the following be a good solution trying to use $p$ and $q$ as some sort of interaction effect?  

$Y = X_1+X_2+\frac{X_1}{q}+X_2 p$

Would it be better to include p and q as variables themselves as well?",clarifying Y variable,2013-10-15 08:16:02.787
186415,57477,594.0,5,,CC BY-SA 3.0,a580f93c-7fdd-4473-8da7-54a5eb729789,"You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. 

This is not the case for independent variables.

For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$

Further, 

$\text{Var}(\bar X) = \text{Var}(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}\text{Var}(\sum_iX_i)= \frac{1}{n^2}\sum_i\text{Var}(X_i)= \frac{1}{n^2}\cdot n\cdot\sigma^2_1= \sigma^2_1/n$

(if the $X_i$ are independent of each other).

http://en.wikipedia.org/wiki/Variance#Basic_properties

In summary: the correct term:

![annotated se formula][1]

$\color{red}{(1)}$ has $\sigma^2/n$ terms because we're looking at averages and that's the variance of an average of independent random variables;

$\color{red}{(2)}$ has a $+$ because the two samples are independent, so their variances add; and

$\color{red}{(3)}$ has a square root because we want the standard deviation of the distribution of the difference in sample means (the standard error of the difference in means). 

That's *every single part of the formula explained*.

The reason why we don't just add standard errors is *standard errors don't add* - the standard error of the difference in means is NOT the sum of the standard errors of the means. The variances *do* add, though, so we can use that to work out the standard errors.

(If the algebra is unconvincing, or you don't believe the basic properties of variance that I used, you might consider simulation, which should soon make it clear.)


  [1]: https://i.stack.imgur.com/TUqrV.png",added 98 characters in body,2013-10-15 02:46:54.563
186416,57477,594.0,5,,CC BY-SA 3.0,2f1883d2-2fd1-4d53-8447-0846c67be5d2,"You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. 

This is not the case for independent variables.

For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$

Further, 

$\text{Var}(\bar X) = \text{Var}(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}\text{Var}(\sum_iX_i)= \frac{1}{n^2}\sum_i\text{Var}(X_i)= \frac{1}{n^2}\cdot n\cdot\sigma^2_1= \sigma^2_1/n$

(if the $X_i$ are independent of each other).

http://en.wikipedia.org/wiki/Variance#Basic_properties

In summary: the correct term:

![annotated se formula][1]

$\color{red}{(1)}$ has $\sigma^2/n$ terms because we're looking at averages and that's the variance of an average of independent random variables;

$\color{red}{(2)}$ has a $+$ because the two samples are independent, so their variances add; and

$\color{red}{(3)}$ has a square root because we want the standard deviation of the distribution of the difference in sample means (the standard error of the difference in means). The part under the bar of the square root is the variance of the difference (the square of the standard error). Taking square roots of squared standard errors gives us standard errors. 

That's *every single part of the formula explained*.

The reason why we don't just add standard errors is *standard errors don't add* - the standard error of the difference in means is NOT the sum of the standard errors of the sample means for independent samples. The variances *do* add, though, so we can use that to work out the standard errors.

(If the algebra is unconvincing, or you don't believe the basic properties of variance that I used, you might consider simulation, which should soon make it clear.)


  [1]: https://i.stack.imgur.com/TUqrV.png",added 98 characters in body,2013-10-15 02:52:05.747
186417,57489,4656.0,2,,CC BY-SA 3.0,6086c157-09ec-4319-bdb9-192fccb6652e,"The common correlation $\rho$ can have value $+1$ but not $-1$. If $\rho_{X,Y}= \rho_{X,Z}=-1$, then $\rho_{Y,Z}$ cannot equal $-1$ but is in fact $+1$.
The smallest value of the common correlation of three random variables
is $-\frac{1}{2}$. More generally,
the minimum common correlation of $n$ random variables is $-\frac{1}{n-1}$
when, regarded as vectors, they are at the vertices of a simplex (of dimension $n-1$)
in $n$-dimensional space.  

Start with $n$ uncorrelated zero-mean unit-variance
random variables $X_i$ and set
$Y_i = X_i - \frac{1}{n}\sum X_i$. Then, $E[Y_i]=0$,
$$\operatorname{var}(Y_i) = \left(\frac{n-1}{n}\right)^2 + (n-1)\left(\frac{1}{n}\right)^2
= \frac{n-1}{n}$$ 
and
$$\operatorname{cov}(Y_i,Y_j) = -2\left(\frac{n-1}{n}\right)\left(\frac{1}{n}\right) +
(n-2)\left(\frac{1}{n}\right)^2 = -\frac{1}{n}$$
giving 
$$\rho_{Y_i,Y_j} 
= \frac{\operatorname{cov}(Y_i,Y_j)}{\sqrt{\operatorname{var}(Y_i)\operatorname{var}(Y_j)}}
=\frac{-1/n}{(n-1)/n} 
= -\frac{1}{n-1}.$$",,2013-10-15 02:58:22.840
186420,57484,155.0,4,,CC BY-SA 3.0,7f7256b0-76bf-4f68-9601-89d7a19c3d22,Bound for the correlation of three random variables with the same correlation?,made consistent with the comments,2013-10-15 03:16:25.790
186419,57484,155.0,5,,CC BY-SA 3.0,7f7256b0-76bf-4f68-9601-89d7a19c3d22,"There are three random variables, $x,y,z$. The three correlations between the three variables are the same. I.e.,

$$\rho=\textrm{cor}(x,y)=\textrm{cor}(x,z)=\textrm{cor}(y,z)$$

What is the tightest bound you can give for $\rho$?",made consistent with the comments,2013-10-15 03:16:25.790
186421,57484,594.0,5,,CC BY-SA 3.0,71c200b6-6e63-4b17-ab1e-7a999f52cb05,"There are three random variables, $x,y,z$. The three correlations between the three variables are the same. That is,

$$\rho=\textrm{cor}(x,y)=\textrm{cor}(x,z)=\textrm{cor}(y,z)$$

What is the tightest bound you can give for $\rho$?",added 3 characters in body; edited title,2013-10-15 03:18:13.273
186422,57484,594.0,4,,CC BY-SA 3.0,71c200b6-6e63-4b17-ab1e-7a999f52cb05,Bound for the correlation of three random variables,added 3 characters in body; edited title,2013-10-15 03:18:13.273
186423,55209,594.0,33,,,e201a40e-9dd3-417c-991b-702b74bb6242,,823,2013-10-15 03:20:06.157
186424,57249,594.0,33,,,46b59258-5de9-4ce0-bbbd-87187711d00b,,824,2013-10-15 03:21:42.803
186425,57490,22677.0,2,,CC BY-SA 3.0,c1253c55-e6fb-40b4-ae47-d3c323a760ab,"@Dail if you're more inclined to the applied rather than the theoretical behind detection of structural break, you might want try `http://cran.r-project.org/web/packages/cpm/index.html` this is the link for `CPM` package of `R`, where you can use `processStream` to find multiple break point in your time series. 
",,2013-10-15 03:22:22.863
186426,55209,,25,,,8fd16737-01b9-4541-8167-202142b760bf,,http://twitter.com/#!/StackStats/status/389954516902494208,2013-10-15 03:22:56.557
186472,57507,22706.0,2,,CC BY-SA 3.0,58a9887b-bd08-4567-9144-642d46e51d93,"A penalized regression provides biased estimates and I thought that SE and confidence intervals in the frequentist ridge and lasso regressions are not useful. I would assume that the same problems exists in an Bayesian approach  but Kyung, Gill, Ghaosh and Casella (2010) say that the Bayesian formulation produces valid standard errors. Does it mean that a 95% credibility intervals includes with 95% probability the true biased estimate and if yes, is this a useful information?",,2013-10-15 08:17:27.980
186474,57507,22706.0,3,,CC BY-SA 3.0,58a9887b-bd08-4567-9144-642d46e51d93,<bayesian><lasso><credible-interval>,,2013-10-15 08:17:27.980
186473,57507,22706.0,1,,CC BY-SA 3.0,58a9887b-bd08-4567-9144-642d46e51d93,How do I interpret the credibility interval in a Bayesian Regularized Regression?,,2013-10-15 08:17:27.980
186428,57477,594.0,5,,CC BY-SA 3.0,dd36fcab-d87d-440e-9886-9f37c277dc48,"You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. 

This is not the case for independent variables.

For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$

Further, 

$\text{Var}(\bar X) = \text{Var}(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}\text{Var}(\sum_iX_i)= \frac{1}{n^2}\sum_i\text{Var}(X_i)= \frac{1}{n^2}\cdot n\cdot\sigma^2_1= \sigma^2_1/n$

(if the $X_i$ are independent of each other).

http://en.wikipedia.org/wiki/Variance#Basic_properties

In summary: the correct term:

![annotated se formula][1]

$\color{red}{(1)}$ has $\sigma^2/n$ terms because we're looking at averages and that's the variance of an average of independent random variables;

$\color{red}{(2)}$ has a $+$ because the two samples are independent, so their variances add; and

$\color{red}{(3)}$ has a square root because we want the standard deviation of the distribution of the difference in sample means (the standard error of the difference in means). The part under the bar of the square root is the variance of the difference (the square of the standard error). Taking square roots of squared standard errors gives us standard errors. 

That's *every single part of the formula explained*.

The reason why we don't just add standard errors is *standard errors don't add* - the standard error of the difference in means is NOT the sum of the standard errors of the sample means for independent samples - the sum will always be too large. The variances *do* add, though, so we can use that to work out the standard errors.

(If the algebra is unconvincing, or you don't believe the basic properties of variance that I used, you might consider simulation, which should soon make it clear.)


  [1]: https://i.stack.imgur.com/TUqrV.png",added 35 characters in body,2013-10-15 04:39:23.473
186429,57491,668.0,2,,CC BY-SA 3.0,3a9a0504-70ba-438c-a7a3-671289724b38,"###Overview###

Any correlation matrix is the covariance matrix of the standardized random variables, whence--like all correlation matrices--it must be positive semi-definite.  Equivalently, its eigenvalues are non-negative.  This imposes a simple condition on $\rho$: it must not be any less than $-1/2$ (and of course cannot exceed $1$).  Conversely, any such $\rho$ actually corresponds to the correlation matrix of some trivariate distribution, proving these bounds are the tightest possible.

-----

###Derivation of the conditions on $\rho$###

Consider the $n$ by $n$ correlation matrix with all off-diagonal values equal to $\rho.$ (The question concerns the case $n=3,$ but this generalization is no more difficult to analyze.)  Let's call it $\mathbb{C}(\rho, n).$  By definition, $\lambda$ is an eigenvalue of provided there exists a nonzero vector $\mathbf{x}_\lambda$ such that

$$\mathbb{C}(\rho,n) \mathbf{x}_\lambda = \lambda \mathbf{x}_\lambda.$$

These eigenvalues are easy to find in the present case, because 

1. Letting $\mathbf{1} = (1, 1, \ldots, 1)'$, compute that

    $$\mathbb{C}(\rho,n)\mathbf{1} = (1+(n-1)\rho)\mathbf{1}.$$

2. Letting $\mathbf{y}_j = (-1, 0, \ldots, 0, 1, 0, \ldots, 0)$ with a $1$ only in the $j^\text{th}$ place (for $j = 2, 3, \ldots, n$), compute that

    $$\mathbb{C}(\rho,n)\mathbf{y}_j = (1-\rho)\mathbf{y}_j.$$

Because the $n$ eigenvectors found so far span the full $n$ dimensional space (proof: an easy row reduction shows the absolute value of their determinant equals $n$, which is nonzero), they constitute a basis of *all* the eigenvectors.  We have therefore found all the eigenvalues and determined they are either $1+(n-1)\rho$ or $1-\rho$ (the latter with multiplicity $n-1$).  In addition to the well-known inequality $-1 \le \rho \le 1$ satisfied by all correlations, non-negativity of the first eigenvalue further implies

$$\rho \ge -\frac{1}{n-1}$$

while the non-negativity of the second eigenvalue imposes no new conditions.

----

###Proof of sufficiency of the conditions###

The implications work in both directions: provided $-1/(n-1)\le \rho \le 1,$ the matrix $\mathbb{C}(\rho, n)$ is nonnegative-definite and therefore is a valid correlation matrix.  It is, for instance, the correlation matrix for a multinormal distribution.  Specifically, writing

$$\Sigma(\rho, n) = (1 + (n-1)\rho)\mathbb{I}_n - \frac{\rho}{(1-\rho)(1+(n-1)\rho)}\mathbf{1}\mathbf{1}'$$

for the inverse of $\mathbb{C}(\rho, n)$ when $-1/(n-1) \lt \rho \lt 1,$ let the vector of random variables $(X_1, X_2, \ldots, X_n)$ have distribution function

$$f_{\rho, n}(\mathbf{x}) = \frac{\exp\left(-\frac{1}{2}\mathbf{x}\Sigma(\rho, n)\mathbf{x}'\right)}{(2\pi)^{n/2}\left((1-\rho)^{n-1}(1+(n-1)\rho)\right)^{1/2}}$$

where $\mathbf{x} = (x_1, x_2, \ldots, x_n)$: the correlation matrix for these $n$ random variables is $\mathbb{C}(\rho, n).$

The special cases $\rho = -1/(n-1)$ and $\rho = 1$ can also be realized by *degenerate* distributions; I won't go into the details except to point out that in the former case the distribution can be considered supported on the hyperplane $\mathbf{x}.\mathbf{1}=0$, where it is a sum of identically distributed mean-$0$ Normal distribution, while in the latter case (perfect positive correlation) it is supported on the line generated by $\mathbf{1}'$, where it has a mean-$0$ Normal distribution.

-----

###More about non-degeneracy###

A review of this analysis makes it clear that the correlation matrix $\mathbb{C}(-1/(n-1), n)$ has a rank of $n-1$ and $\mathbb{C}(1, n)$ has a rank of $1$ (because only one eigenvector has a nonzero eigenvalue).  For $n\ge 2$, this makes the correlation matrix degenerate in either case.  Otherwise, the existence of its inverse $\Sigma(\rho, n)$ proves it is nondegenerate.

",,2013-10-15 04:52:22.687
186432,57492,20130.0,2,,CC BY-SA 3.0,bddab7b4-a694-4187-a25a-79d9500b647b,"There's a question for [textbooks](http://stats.stackexchange.com/questions/4612/good-econometrics-textbooks) here, but I would like to ask similar question about handbooks: What econometrics handbooks would you recommend?

One option is Elsevier's 6 volumes of _Handbook of Econometrics_ series edited by Griliches and Instriligator. However, perhaps you would recommend other handbooks, more concise or otherwise?

I suggest two possible formats: one is a reference-card format with minimum explanations, and the other one is a more extended format with proofs and more detailed exposition.",,2013-10-15 04:59:24.173
186431,57492,20130.0,1,,CC BY-SA 3.0,bddab7b4-a694-4187-a25a-79d9500b647b,What are good econometrics handbooks?,,2013-10-15 04:59:24.173
186430,57492,20130.0,3,,CC BY-SA 3.0,bddab7b4-a694-4187-a25a-79d9500b647b,<references><econometrics><big-list>,,2013-10-15 04:59:24.173
186433,57492,668.0,16,,,6646ce27-17a8-4f10-9747-9bb92a410f93,,,2013-10-15 05:02:38.393
186434,57492,668.0,10,,,c4100e75-afee-4758-a145-e7aba0b4655a,"{""Voters"":[{""Id"":919,""DisplayName"":""whuber""}]}",105,2013-10-15 05:02:49.377
186435,57473,155.0,4,,CC BY-SA 3.0,53fb2689-fa60-459f-b1b9-b1261babc13c,Intuition for the standard error of the difference of sample means,edited title,2013-10-15 05:11:39.080
186436,57493,3183.0,2,,CC BY-SA 3.0,7c9cabbc-776b-456b-b912-869a04ef1d54,"It sounds like your question has two parts: the underlying idea and a concrete example.  I'll start with the underlying idea, then link to an example at the bottom.

------

EM is useful in Catch-22 situations where it seems like you need to know $A$ before you can calculate $B$ and you need to know $B$ before you can calculate $A$.

The most common case people deal with is probably mixture distributions, which can be tricky to deal with. For our example, let's look at a simple Gaussian mixture model:

> You have two different univariate Gaussian distributions with different means and unit variance.  

>You have a bunch of data points, but you're not sure which points came from which distribution, and you're also not sure about the means of the two distributions.

And now you're stuck:

* If you knew which distribution each point came from, then you could estimate the two distributions' means using the sample means from the relevant points.  But you don't actually know which points to assign to which distribution, so this won't work.

* If you knew the true means, you could figure out which data points came from which Gaussian.  For example, if a data point had a very high value, it probably came from the distribution with the higher mean.  But you don't know what the means are, so this won't work either.

But if you don't know either of these things, it's hard to know where to start.

***What EM lets you do is alternate between these two tractable steps instead of tackling the whole process at once.***

First, you'll update the probability that each data point came from Gaussian 1 versus Gaussian 2 using the procedure from the first bullet point.  Then you'll update your estimates for the distributions' means using the procedure from the second bullet point. Each time you do these updates, you're improving a lower bound on the model's likelihood.

That's already pretty cool: even though the two suggestions in the bullet points above didn't seem like they'd work individually, you can still use them together to improve the model.  The ***real*** magic of EM is that, after enough iterations, the lower bound will be so high that there won't be any space between it and the local maximum.  As a result, and you've locally optimized the likelihood.

So you haven't just *improved* the model, you've found the *best* possible model one can find with incremental updates.

------

[This](http://en.wikipedia.org/wiki/File:Em_old_faithful.gif) page from Wikipedia shows a slightly more complicated example (two-dimensional Gaussians and unknown covariance), but the basic idea is the same.  It also includes well-commented `R` code for implementing the example.

In the code, the ""Expectation"" step (E-step) corresponds to my second bullet point: figuring out which Gaussian gets responsibility for each data point, given the current parameters for each Gaussian.  The ""Maximization"" step (M-step) updates the means and covariances, given these assignments.

As you can see in the animation, these updates quickly allow the algorithm to go from a set of terrible estimates to a set of very good ones: there really do seem to be two clouds of points centered on the two Gaussian distributions.",,2013-10-15 05:25:55.930
186437,57494,155.0,2,,CC BY-SA 3.0,6387df5d-b4c1-4b53-89fb-034f3f30dd09,"The standard error of the mean for $n$ independent observations is $\frac{\sigma}{\sqrt{n}}$ where $\sigma$ is the standard deviation.

So if we have two independent samples we have the standard errors for the means of group 1 and group 2.

$$\sigma_{\mu_1}=\frac{\sigma_1}{\sqrt{n_1}}$$
$$\sigma_{\mu_2}=\frac{\sigma_2}{\sqrt{n_2}}$$

If we square these values we get the variance of the mean:

$$\sigma^2_{\mu_1}=\frac{\sigma^2_1}{n_1}$$
$$\sigma^2_{\mu_2}=\frac{\sigma^2_2}{n_2}$$

The variance of the sum or difference of two independent random variables 
is the sum of the two variances. Rhus,

$$\sigma^2_{\mu_1 - \mu_2} =\sigma^2_{\mu_1} + \sigma^2_{\mu_2} =  \frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2} $$ 

So if we want the standard error of the difference we take the square root of the variance:

$$\sigma_{\mu_1 - \mu_2} =\sqrt{\sigma^2_{\mu_1} + \sigma^2_{\mu_2}} =  \sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}} $$ 


",,2013-10-15 05:35:05.007
186438,57495,11440.0,2,,CC BY-SA 3.0,c3ec3129-9bec-42cb-9670-fa7335773365,"Well, it is your choice which notation to use, but you certainly can just use logical operators:

$p(\Phi_A, \Phi_B \; |\; \Phi_A>C \,\cap \Phi_B > C)$

Your current notation is not clear as $\Phi$ is not defined and not obvious what it means.",,2013-10-15 05:35:17.670
186439,57212,22190.0,5,,CC BY-SA 3.0,7c83219f-3170-4c4c-b894-0189093e9b8b,"I am new to statistics and was asked to develop a statistical model, which I had started, they ask me to carry out concordance and discordance now, however I don't know anything about these terms except that the concordance is the probability that a pair of individuals will both have a certain characteristic, given that one of the pair has the characteristic and the opposite for discordance.  
Still I don't know why I have to find them and what would be the appropriate value of both for a decent model.  
",deleted 3 characters in body,2013-10-15 05:42:02.543
186440,57494,155.0,5,,CC BY-SA 3.0,789499c8-878e-4dea-bc29-bdcdec678d48,"### Algebraic intuition
The standard error of the mean for $n$ independent observations is $\frac{\sigma}{\sqrt{n}}$ where $\sigma$ is the standard deviation.

So if we have two independent samples we have the standard errors for the means of group 1 and group 2.

$$\sigma_{\mu_1}=\frac{\sigma_1}{\sqrt{n_1}}$$
$$\sigma_{\mu_2}=\frac{\sigma_2}{\sqrt{n_2}}$$

If we square these values we get the variance of the mean:

$$\sigma^2_{\mu_1}=\frac{\sigma^2_1}{n_1}$$
$$\sigma^2_{\mu_2}=\frac{\sigma^2_2}{n_2}$$

The variance of the sum or difference of two independent random variables 
is the sum of the two variances. Thus,

$$\sigma^2_{\mu_1 - \mu_2} =\sigma^2_{\mu_1} + \sigma^2_{\mu_2} =  \frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2} $$ 

So if we want the standard error of the difference we take the square root of the variance:

$$\sigma_{\mu_1 - \mu_2} =\sqrt{\sigma^2_{\mu_1} + \sigma^2_{\mu_2}} =  \sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}} $$ 

So imagine this is intuitive if the component steps are intuitive. In particular it helps if you find intuitive the idea that the variance of the sum of independent variables is the sum of the variances of the component variables.

### Fuzzy Intuition
In terms of more general intuition, if $n_1 = n_2$ and $\sigma=\sigma_1=\sigma_2$ then the standard error of the difference between means will be $\sqrt{2}\sigma_\mu\approx 1.4\times \sigma_\mu$. It makes sense that this value of approximately 1.4 is greater than 1 (i.e., the variance of a variable after adding a constant; i.e., equivalent to one sample t-test) and less than 2 (i.e., the standard deviation of the sum of two perfectly correlated variables (with equal variance) and the standard error implied by the formula you mention: $\frac{\sigma_1}{\sqrt{n_1}} + \frac{\sigma_2}{\sqrt{n_2}}$).",added 790 characters in body,2013-10-15 05:50:56.207
186443,57496,12314.0,1,,CC BY-SA 3.0,02a56597-a917-40a1-bfe8-276fce6f92ec,Forecasting the population of a village; 1 day to a year ahead,,2013-10-15 06:05:55.207
186441,57496,12314.0,2,,CC BY-SA 3.0,02a56597-a917-40a1-bfe8-276fce6f92ec,"Suppose that I have daily data on the population of a small village, given by $Y(t)$, as well as daily data on various factors that are relevant to the size of the population in the future, given by vector $X(t)$. These explanatory variables include untransformed variables as well as features engineered to be informative over long horizons (e.g. one of the variables captures the number of deaths over the last 30 days). I have collected this data for 10 years. 

My objective is to forecast $Y(t)$ ahead by 1,2,3,...,365 days. I expect long-run forecasts to be different to short-run forecasts. If a holiday season is coming up I might expect a downwards spike in a few months time (people visiting the city), but if someone is on their deathbed then I will expect a downwards spike in a few days.

Since the population is sufficiently small that $\Delta Y(t+k)$ is typically in $\{-2,-1,0,1,2\}$ for the forecasting horizon under question, I will use a multiple categorical response variable classification model that will assign probabilities to the various class labels being observed.

My question centers on the specific considerations I need to make when constructing forecasts of the change from $Y(t)$ to $Y(t+k)$ where $k$ is large (e.g. 100 days). 

Basically there will be the most hideous autocorrelation structure in $\Delta Y(t+k)$ over these time scales. If someone dies on day $2$, they are also dead on day $3, 4, ..., k$, meaning a string of $k$ or so $\Delta Y(t+k)$ will contain this same information. 

These queries result:

- What are some ways of dealing with this immense autocorrelation structure in my response. Is it even a problem?
- Are there alternative methodologies to the ones I've proposed for forecasting these horizons (aside from typical machine learning methods such as random forests which I'm already working with).
- Any other handy advice.








",,2013-10-15 06:05:55.207
186445,57474,22163.0,4,,CC BY-SA 3.0,850cd926-bedb-49f6-b5fe-5e7113c274e8,Inverted SPSS results: Logistic regression command vs. Genlin?,added 4 characters in body,2013-10-15 06:12:48.370
186444,57474,22163.0,5,,CC BY-SA 3.0,850cd926-bedb-49f6-b5fe-5e7113c274e8,"I want to do a logistic regression in SPSS. However, since I analyse unemployment spells the subjects are sometimes repeated (violating the independence assumption of the regression). One way of removing the within subject variation is by applying a Genlin model with the repeated subject subcommand (in essence a GEE model). Thus, I tried out a Genlin model with binomal probability and the logit link, comparing it to a standard logistic regression. I used the exact same variables in the two procedures. 

However, the results that was delivered from the Genlin procedure was inverted relative to that of the logistic regression. For instance: Exp(B) for women (of the independent variable sex/gender) was just above 2.0 in logistic regression while being at 0.49 in Genlin. The same happened with every independent variable.

 - Any suggestions to why the results of the Genlin procedure is
   inverted?  
 - Is there any way to get the Genlin results in accordance to the logistic regression?",added 4 characters in body,2013-10-15 06:12:48.370
186447,57496,12314.0,6,,CC BY-SA 3.0,2fea95e8-7ac4-436f-9cd8-fa3a90b5965b,<time-series><predictive-models><forecasting><autocorrelation><ordinal-data>,edited tags; edited title,2013-10-15 06:16:06.313
186446,57496,12314.0,4,,CC BY-SA 3.0,2fea95e8-7ac4-436f-9cd8-fa3a90b5965b,Forecasting time-series ahead by multiple time horizons,edited tags; edited title,2013-10-15 06:16:06.313
186448,57484,,25,,,83cb3cf9-0674-4239-9da8-cc186207a1fb,,http://twitter.com/#!/StackStats/status/389999817193426944,2013-10-15 06:22:56.990
186450,57497,22703.0,2,,CC BY-SA 3.0,8e440619-e0db-46c8-83fd-221f7648ffcc,"Any statistical distribution is described in terms of shape, scale and location parameters. But what do these parameters mean, geometrically, statistically and for a layman with minimum statistical knowledge?

I have explored wikipedia and still, this doubt continues to exist.

Please help. This understanding is quite important to me.",,2013-10-15 06:36:38.530
186451,57497,22703.0,1,,CC BY-SA 3.0,8e440619-e0db-46c8-83fd-221f7648ffcc,Parameters of a Statistical Distribution,,2013-10-15 06:36:38.530
186449,57497,22703.0,3,,CC BY-SA 3.0,8e440619-e0db-46c8-83fd-221f7648ffcc,<distributions>,,2013-10-15 06:36:38.530
186452,57493,3183.0,5,,CC BY-SA 3.0,650487e7-d01c-447e-8cd9-8a4e429d8f75,"It sounds like your question has two parts: the underlying idea and a concrete example.  I'll start with the underlying idea, then link to an example at the bottom.

------

EM is useful in Catch-22 situations where it seems like you need to know $A$ before you can calculate $B$ and you need to know $B$ before you can calculate $A$.

The most common case people deal with is probably mixture distributions, which can be tricky to deal with. For our example, let's look at a simple Gaussian mixture model:

> You have two different univariate Gaussian distributions with different means and unit variance.  

>You have a bunch of data points, but you're not sure which points came from which distribution, and you're also not sure about the means of the two distributions.

And now you're stuck:
* If you knew the true means, you could figure out which data points came from which Gaussian.  For example, if a data point had a very high value, it probably came from the distribution with the higher mean.  But you don't know what the means are, so this won't work.

* If you knew which distribution each point came from, then you could estimate the two distributions' means using the sample means of the relevant points.  But you don't actually know which points to assign to which distribution, so this won't work either.

So neither approach seems like it works: you'd need to know the answer before you can find the answer, and you're stuck.

***What EM lets you do is alternate between these two tractable steps instead of tackling the whole process at once.***

First, you'll update the probability that each data point came from Gaussian 1 versus Gaussian 2 using the procedure from the first bullet point.  Then you'll update your estimates for the distributions' means using the procedure from the second bullet point. Each time you do these updates, you're improving a lower bound on the model's likelihood.

That's already pretty cool: even though the two suggestions in the bullet points above didn't seem like they'd work individually, you can still use them together to improve the model.  The ***real*** magic of EM is that, after enough iterations, the lower bound will be so high that there won't be any space between it and the local maximum.  As a result, and you've locally optimized the likelihood.

So you haven't just *improved* the model, you've found the *best* possible model one can find with incremental updates.

------

[This](http://en.wikipedia.org/wiki/File:Em_old_faithful.gif) page from Wikipedia shows a slightly more complicated example (two-dimensional Gaussians and unknown covariance), but the basic idea is the same.  It also includes well-commented `R` code for implementing the example.

In the code, the ""Expectation"" step (E-step) corresponds to my first bullet point: figuring out which Gaussian gets responsibility for each data point, given the current parameters for each Gaussian.  The ""Maximization"" step (M-step) updates the means and covariances, given these assignments, as in my second bullet point.

As you can see in the animation, these updates quickly allow the algorithm to go from a set of terrible estimates to a set of very good ones: there really do seem to be two clouds of points centered on the two Gaussian distributions.",swap bullet points so E step is first & everything is consistent,2013-10-15 06:37:59.930
186455,57498,22703.0,3,,CC BY-SA 3.0,2eb6ddc4-7257-480d-bcdb-3c8bfb807e85,<distributions>,,2013-10-15 06:42:30.540
186453,57498,22703.0,2,,CC BY-SA 3.0,2eb6ddc4-7257-480d-bcdb-3c8bfb807e85,"As statisticians, we come across many distributions under the banners ""discrete"",""continuous"" and ""univariate"",""multivariate"".But can anyone, offer me a good reason behind the existence and motivation for so many distributions. How do we get them? and what can a layman understand from it?

What is the logic behind the existence of distributions?

Please help...",,2013-10-15 06:42:30.540
186454,57498,22703.0,1,,CC BY-SA 3.0,2eb6ddc4-7257-480d-bcdb-3c8bfb807e85,Motivation for statistical distributions,,2013-10-15 06:42:30.540
186456,57493,3183.0,5,,CC BY-SA 3.0,fbbee6c7-5fba-4b20-a9a0-8474398d01d2,"It sounds like your question has two parts: the underlying idea and a concrete example.  I'll start with the underlying idea, then link to an example at the bottom.

------

EM is useful in Catch-22 situations where it seems like you need to know $A$ before you can calculate $B$ and you need to know $B$ before you can calculate $A$.

The most common case people deal with is probably mixture distributions. For our example, let's look at a simple Gaussian mixture model:

> You have two different univariate Gaussian distributions with different means and unit variance.  

>You have a bunch of data points, but you're not sure which points came from which distribution, and you're also not sure about the means of the two distributions.

And now you're stuck:

* If you knew the true means, you could figure out which data points came from which Gaussian.  For example, if a data point had a very high value, it probably came from the distribution with the higher mean.  But you don't know what the means are, so this won't work.

* If you knew which distribution each point came from, then you could estimate the two distributions' means using the sample means of the relevant points.  But you don't actually know which points to assign to which distribution, so this won't work either.

So neither approach seems like it works: you'd need to know the answer before you can find the answer, and you're stuck.

***What EM lets you do is alternate between these two tractable steps instead of tackling the whole process at once.***

First, you'll update the probability that each data point came from Gaussian 1 versus Gaussian 2 using the procedure from the first bullet point.  Then you'll update your estimates for the distributions' means using the procedure from the second bullet point. Each time you do these updates, you're improving a lower bound on the model's likelihood.

That's already pretty cool: even though the two suggestions in the bullet points above didn't seem like they'd work individually, you can still use them together to improve the model.  The ***real*** magic of EM is that, after enough iterations, the lower bound will be so high that there won't be any space between it and the local maximum.  As a result, and you've locally optimized the likelihood.

So you haven't just *improved* the model, you've found the *best* possible model one can find with incremental updates.

------

[This](http://en.wikipedia.org/wiki/File:Em_old_faithful.gif) page from Wikipedia shows a slightly more complicated example (two-dimensional Gaussians and unknown covariance), but the basic idea is the same.  It also includes well-commented `R` code for implementing the example.

In the code, the ""Expectation"" step (E-step) corresponds to my first bullet point: figuring out which Gaussian gets responsibility for each data point, given the current parameters for each Gaussian.  The ""Maximization"" step (M-step) updates the means and covariances, given these assignments, as in my second bullet point.

As you can see in the animation, these updates quickly allow the algorithm to go from a set of terrible estimates to a set of very good ones: there really do seem to be two clouds of points centered on the two Gaussian distributions that EM finds.",minor,2013-10-15 06:43:12.093
186457,57493,3183.0,5,,CC BY-SA 3.0,1a66876b-f5f0-4bc0-b933-3c7dc4ebd148,"It sounds like your question has two parts: the underlying idea and a concrete example.  I'll start with the underlying idea, then link to an example at the bottom.

------

EM is useful in Catch-22 situations where it seems like you need to know $A$ before you can calculate $B$ and you need to know $B$ before you can calculate $A$.

The most common case people deal with is probably mixture distributions. For our example, let's look at a simple Gaussian mixture model:

> You have two different univariate Gaussian distributions with different means and unit variance.  

>You have a bunch of data points, but you're not sure which points came from which distribution, and you're also not sure about the means of the two distributions.

And now you're stuck:

* If you knew the true means, you could figure out which data points came from which Gaussian.  For example, if a data point had a very high value, it probably came from the distribution with the higher mean.  But you don't know what the means are, so this won't work.

* If you knew which distribution each point came from, then you could estimate the two distributions' means using the sample means of the relevant points.  But you don't actually know which points to assign to which distribution, so this won't work either.

So neither approach seems like it works: you'd need to know the answer before you can find the answer, and you're stuck.

***What EM lets you do is alternate between these two tractable steps instead of tackling the whole process at once.***

You'll need to start with a guess about the two means (although your guess doesn't necessarily have to be very accurate, you do need to start somewhere).

If your guess about the means was accurate, then you'd have enough information to carry out the step in my first bullet point above, and you could (probabilistically) assign each data point to one of the two Gaussians.  Even though we know our guess is wrong, let's try this anyway. And then, given each point's assigned distributions, you could get new estimates for the means using the second bullet point. It turns out that, each time you do loop through these two steps, you're improving a lower bound on the model's likelihood.

That's already pretty cool: even though the two suggestions in the bullet points above didn't seem like they'd work individually, you can still use them together to improve the model.  The ***real*** magic of EM is that, after enough iterations, the lower bound will be so high that there won't be any space between it and the local maximum.  As a result, and you've locally optimized the likelihood.

So you haven't just *improved* the model, you've found the *best* possible model one can find with incremental updates.

------

[This](http://en.wikipedia.org/wiki/File:Em_old_faithful.gif) page from Wikipedia shows a slightly more complicated example (two-dimensional Gaussians and unknown covariance), but the basic idea is the same.  It also includes well-commented `R` code for implementing the example.

In the code, the ""Expectation"" step (E-step) corresponds to my first bullet point: figuring out which Gaussian gets responsibility for each data point, given the current parameters for each Gaussian.  The ""Maximization"" step (M-step) updates the means and covariances, given these assignments, as in my second bullet point.

As you can see in the animation, these updates quickly allow the algorithm to go from a set of terrible estimates to a set of very good ones: there really do seem to be two clouds of points centered on the two Gaussian distributions that EM finds.",minor clarification,2013-10-15 06:58:22.890
186458,57499,22703.0,2,,CC BY-SA 3.0,558e7911-fca3-44f5-8eee-07ef37164a66,"Well, I would suggest you to go through a book on R by Maria L Rizzo. One of the chapters contain the use of EM algorithm with a numerical example. I remember going through the code for better understanding. 

Also, try to view it from a clustering point of view in the beginning. Work out by hand, a clustering problem where 10 observations are taken from two different normal densities. This should help.Take help from R :)",,2013-10-15 07:03:26.383
186461,57500,22703.0,3,,CC BY-SA 3.0,db493b5e-b2c6-4b5b-8ea0-974233d502f1,<regression><mathematical-statistics><modeling>,,2013-10-15 07:09:16.460
186460,57500,22703.0,1,,CC BY-SA 3.0,db493b5e-b2c6-4b5b-8ea0-974233d502f1,Regression Methods,,2013-10-15 07:09:16.460
186459,57500,22703.0,2,,CC BY-SA 3.0,db493b5e-b2c6-4b5b-8ea0-974233d502f1,"
What is the fundamental difference between 1) Linear Regression 2) Non linear regression 3) Parametric Regression 4) Non Parametric Regression?
When do we go for each of the types? How do we know what to choose? What kind of data is of demand here? What are the assumptions unique to each?
At times, if you go through papers you get to see a combination of the names above.

Please help",,2013-10-15 07:09:16.460
186475,57497,15827.0,5,,CC BY-SA 3.0,828167bb-bd9d-4c05-8118-42b036de8963,"Any statistical distribution is described in terms of shape, scale and location parameters. But what do these parameters mean, geometrically, statistically and for a layman with minimum statistical knowledge?

I have explored wikipedia and still, this doubt continues to exist.

",personal comments unnecessary ,2013-10-15 08:18:12.107
186478,57508,3993.0,2,,CC BY-SA 3.0,db70dac2-603d-4dc2-b845-8d57649c7408,"***TL, DR summary:***

Is there any theoretical or empirical basis to support the following statement being true as a general rule of thumb?

""When estimating a mixed model, typically the estimated variances/standard deviations of random effects associated with 'higher-order' terms (e.g., random effects of two-way, three-way, and beyond interaction terms) turn out to be *smaller* than the estimated variances/standard deviations of random effects associated with 'lower-order' terms (e.g., the residual variance, variances associated with simple effects of grouping factors).""

The source of this claim is me. ;)

****

Okay, now for the longer version...

Typically when I sit down to start analyzing a new dataset which I know will call for a mixed model, one of the first models that I fit (after the statistical foreplay of looking through the observations in the dataset, plotting various things, cross-tabulating different factors, etc.) is one that is pretty close to the ""maximal"" random effects specification, where every random effect that is in-principle possible to estimate from the data, is estimated.

Naturally, it is not uncommon that this nearly-maximal model will have some computational problems (convergence errors, or wacky variance/covariance estimates, or etc.) and that I have to trim back this model to find one that my data can more easily support. Fine.

In these situations, the method I have come to prefer for trimming random terms is not to rely on significance tests or likelihood ratios, but rather to just identify the random effects that seem to have the smallest standard deviations (which can admittedly be a little tricky when predictors are on very different scales, but I try to take account of this in my appraisal) and remove these terms first, sequentially in an iterative process. The idea being that I want to alter the predictions of the model as little as possible while still reducing the complexity of the model. 

One pattern that I seem to have noticed after a pretty good amount of time spent doing this is that following this method very often leads me to trim random effects associated with higher-order terms (as defined above) of the model first. This is not always true, and occasionally some of the higher-order terms explain a lot of variance, but this doesn't seem to be the general pattern. In sharp contrast, I usually find that lower-order random terms -- particularly those associated with simple effects of the grouping factors -- explain a pretty good amount of variance and are fairly essential to the model. At the extreme, the residual term commonly accounts for close to the most variance, although of course removing this term wouldn't be sensible.

This entirely informal observation leads me to form the hypothesis that I stated at the beginning of this question.

If it is true, then it constitutes a useful piece of advice that might be passed down to people who are less experience with this kind of model selection process. But before I begin doing so, I want to check with other, more experienced users of mixed models about their reactions to this observation. Does it seem more or less true to you? Is it roughly consistent with your experience fitting many different mixed models to many different datasets? Do you know of any sensible, theoretical reasons why we might actually *expect* this to be true in a lot of cases? Or does it just seem like bullshit?

One possible answer here is that it is not true even in my own case, and I have simply deceived myself. Certainly a possibility that I am open to.

Another possibility is that it might be true in my own case, but that this could simply be a kind of coincidence having to do with the kinds of datasets that I tend to work with routinely (which, FYI, are datasets in psychological / social sciences, a slight majority being experimental in origin, but also a fair proportion of non-experimental stuff). If this is the case then there is probably no good reason for expecting my observations to hold in general in other fields that handle very different kinds of data. Still, if there is a coherent non-coincidental reason for why this might be expected to be true, even if only for these particular kinds of datasets, I would love to hear it.

And of course another possibility is that others *have* noticed similar patterns in their own data, and that it represents some kind of general rule of thumb that people find useful to keep in mind as they fit mixed models to various different data. If this is the case then it seems like there must be some compelling statistical-theoretical reason for why this pattern arises. But I really don't know what that reason would look like.

I welcome anyone's thoughts and opinions about this. Note that as far as I'm concerned, totally legitimate responses to this question might be as simple as comments like ""Yeah I have noticed something similar in the data I've worked with, but I have no idea why it should be true"" or conversely ""I have noticed nothing remotely like this in the data I've worked with."" Of course I also welcome longer and more involved discussions...",,2013-10-15 08:21:54.933
186477,57508,3993.0,1,,CC BY-SA 3.0,db70dac2-603d-4dc2-b845-8d57649c7408,Relative variances of higher-order vs. lower-order random terms in mixed models,,2013-10-15 08:21:54.933
186462,57501,20470.0,2,,CC BY-SA 3.0,4172a0ce-6ab2-4563-9a26-ea59addbb64e,"This is a recipe to learn EM with a practical 'Coin-Toss' example:

**1-** Read this short [EM tutorial paper][1] by Do and Batzoglou.

**2-** You may have question marks in your head, have a look at the explanations on this maths stack exchange [page][2] (which clarified the example on the paper for me).

**3-** Look at/run this code that I wrote in Python that simulates the solution to the coni-toss problem in the EM tutorial paper of item 1:

***Warning :*** The code may be messy/suboptimal, but it does the job.

    import numpy as np
    import math
    
    #### E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* #### 
    
    def get_mn_log_likelihood(obs,probs):
        """""" Return the (log)likelihood of obs, given the probs""""""
        # Multinomial Distribution Log PMF
        # ln (pdf)      =             multinomial coeff            *   product of probabilities
        # ln[f(x|n, p)] = [ln(n!) - (ln(x1!)+ln(x2!)+...+ln(xk!))] + [x1*ln(p1)+x2*ln(p2)+...+xk*ln(pk)]     
    
        multinomial_coeff_denom= 0
        prod_probs = 0
        for x in range(0,len(obs)): # loop through state counts in each observation
            multinomial_coeff_denom = multinomial_coeff_denom + math.log(math.factorial(obs[x]))
            prod_probs = prod_probs + obs[x]*math.log(probs[x])
    
    multinomial_coeff = math.log(math.factorial(sum(obs))) -  multinomial_coeff_denom
    likelihood = multinomial_coeff + prod_probs
    return likelihood
    
    # 1st:  Coin B, {HTTTHHTHTH}, 5H,5T
    # 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T
    # 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T
    # 4th:  Coin B, {HTHTTTHHTT}, 4H,6T
    # 5th:  Coin A, {THHHTHHHTH}, 7H,3T
    # so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45
    
    # represent the experiments
    head_counts = np.array([5,9,8,4,7])
    tail_counts = 10-head_counts
    experiments = zip(head_counts,tail_counts)
    
    # initialise the pA(heads) and pB(heads)
    pA_heads = np.zeros(100); pA_heads[0] = 0.60
    pB_heads = np.zeros(100); pB_heads[0] = 0.50
    
    # E-M begins!
    delta = 0.001  
    j = 0 # iteration counter
    improvement = float('inf')
    while (improvement>delta):
        expectation_A = np.zeros((5,2), dtype=float) 
        expectation_B = np.zeros((5,2), dtype=float)
        for i in range(0,len(experiments)):
            e = experiments[i] # i'th experiment
            ll_A = get_mn_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) # loglikelihood of e given coin A
            ll_B = get_mn_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) # loglikelihood of e given coin B
    
            weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of A proportional to likelihood of A 
            weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of B proportional to likelihood of B                            
    
            expectation_A[i] = np.dot(weightA, e) 
            expectation_B[i] = np.dot(weightB, e)
    
        pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); 
        pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); 
    
        improvement = max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - np.array([pA_heads[j],pB_heads[j]]) ))
        j = j+1


  [1]: http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf
  [2]: http://math.stackexchange.com/questions/25111/how-does-expectation-maximization-work",,2013-10-15 07:21:33.433
186463,57501,20470.0,5,,CC BY-SA 3.0,14a084c7-7ff5-43b0-918e-1dc5169d1673,"This is a recipe to learn EM with a practical 'Coin-Toss' example:

**1-** Read this short [EM tutorial paper][1] by Do and Batzoglou.

**2-** You may have question marks in your head, have a look at the explanations on this maths stack exchange [page][2] (which clarified the example on the paper for me).

**3-** Look at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:

***Warning :*** The code may be messy/suboptimal, but it does the job.

    import numpy as np
    import math
    
    #### E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* #### 
    
    def get_mn_log_likelihood(obs,probs):
        """""" Return the (log)likelihood of obs, given the probs""""""
        # Multinomial Distribution Log PMF
        # ln (pdf)      =             multinomial coeff            *   product of probabilities
        # ln[f(x|n, p)] = [ln(n!) - (ln(x1!)+ln(x2!)+...+ln(xk!))] + [x1*ln(p1)+x2*ln(p2)+...+xk*ln(pk)]     
    
        multinomial_coeff_denom= 0
        prod_probs = 0
        for x in range(0,len(obs)): # loop through state counts in each observation
            multinomial_coeff_denom = multinomial_coeff_denom + math.log(math.factorial(obs[x]))
            prod_probs = prod_probs + obs[x]*math.log(probs[x])
    
    multinomial_coeff = math.log(math.factorial(sum(obs))) -  multinomial_coeff_denom
    likelihood = multinomial_coeff + prod_probs
    return likelihood
    
    # 1st:  Coin B, {HTTTHHTHTH}, 5H,5T
    # 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T
    # 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T
    # 4th:  Coin B, {HTHTTTHHTT}, 4H,6T
    # 5th:  Coin A, {THHHTHHHTH}, 7H,3T
    # so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45
    
    # represent the experiments
    head_counts = np.array([5,9,8,4,7])
    tail_counts = 10-head_counts
    experiments = zip(head_counts,tail_counts)
    
    # initialise the pA(heads) and pB(heads)
    pA_heads = np.zeros(100); pA_heads[0] = 0.60
    pB_heads = np.zeros(100); pB_heads[0] = 0.50
    
    # E-M begins!
    delta = 0.001  
    j = 0 # iteration counter
    improvement = float('inf')
    while (improvement>delta):
        expectation_A = np.zeros((5,2), dtype=float) 
        expectation_B = np.zeros((5,2), dtype=float)
        for i in range(0,len(experiments)):
            e = experiments[i] # i'th experiment
            ll_A = get_mn_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) # loglikelihood of e given coin A
            ll_B = get_mn_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) # loglikelihood of e given coin B
    
            weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of A proportional to likelihood of A 
            weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of B proportional to likelihood of B                            
    
            expectation_A[i] = np.dot(weightA, e) 
            expectation_B[i] = np.dot(weightB, e)
    
        pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); 
        pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); 
    
        improvement = max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - np.array([pA_heads[j],pB_heads[j]]) ))
        j = j+1


  [1]: http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf
  [2]: http://math.stackexchange.com/questions/25111/how-does-expectation-maximization-work",edited body,2013-10-15 07:27:05.690
186464,57502,22704.0,2,,CC BY-SA 3.0,e7896a51-07f0-4ea1-9b17-ccb021b9c595,"Take a look at a post in Healthy Algorithm:
http://healthyalgorithms.com/2011/11/23/causal-modeling-in-python-bayesian-networks-in-pymc/

also in PyMC's totorial:
http://pymc-devs.github.io/pymc/tutorial.html

Maybe you would try the following code clip (assuming you have imported pymc as mc):

    A = mc.Normal('A', mu_A, tau_A)
    B = mc.Normal('B', mu_B, tau_B)
    p_C = mc.Lambda('p_C', lambda A=A, B=B: <<dependency spec goes here>>, doc='Pr[C|AB]')
    C = mc.Bernoulli('C', p_C)",,2013-10-15 07:29:18.930
186465,57503,5637.0,2,,CC BY-SA 3.0,dee819ef-2704-4837-a2ff-5b8c33a9c5cc,"The names of the parameters are suggestive. Location, scale, and shape parameters are associated with central tendency, dispersion and skewness respectively. For eg: If you change location parameters, mostly it change only the central tendency measures.


Try this online tool. [Distributions][1]

See how the distribution changes for different values of parameters. You could try this with generalized extreme value distribution.

Not all standard distributions have all three parameters. Some distributions have only one or two of the parameters (eg: gamma distribution-shape and scale parameters) 


  [1]: http://socr.ucla.edu/htmls/SOCR_Distributions.html",,2013-10-15 07:33:28.403
186466,57501,20470.0,5,,CC BY-SA 3.0,6b73b0a7-9218-4d3b-8333-e23230351bb4,"This is a recipe to learn EM with a practical and (in my opinion) very intuitive 'Coin-Toss' example:

**1-** Read this short [EM tutorial paper][1] by Do and Batzoglou. This is the schema where the coin toss example is explained:

![enter image description here][2]

**2-** You may have question marks in your head, have a look at the explanations on this maths stack exchange [page][3] (which clarified the example on the paper for me).

**3-** Look at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:

**P.S** The code may be suboptimal, but it does the job.

    import numpy as np
    import math
    
    #### E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* #### 
    
    def get_mn_log_likelihood(obs,probs):
        """""" Return the (log)likelihood of obs, given the probs""""""
        # Multinomial Distribution Log PMF
        # ln (pdf)      =             multinomial coeff            *   product of probabilities
        # ln[f(x|n, p)] = [ln(n!) - (ln(x1!)+ln(x2!)+...+ln(xk!))] + [x1*ln(p1)+x2*ln(p2)+...+xk*ln(pk)]     
    
        multinomial_coeff_denom= 0
        prod_probs = 0
        for x in range(0,len(obs)): # loop through state counts in each observation
            multinomial_coeff_denom = multinomial_coeff_denom + math.log(math.factorial(obs[x]))
            prod_probs = prod_probs + obs[x]*math.log(probs[x])
    
    multinomial_coeff = math.log(math.factorial(sum(obs))) -  multinomial_coeff_denom
    likelihood = multinomial_coeff + prod_probs
    return likelihood
    
    # 1st:  Coin B, {HTTTHHTHTH}, 5H,5T
    # 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T
    # 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T
    # 4th:  Coin B, {HTHTTTHHTT}, 4H,6T
    # 5th:  Coin A, {THHHTHHHTH}, 7H,3T
    # so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45
    
    # represent the experiments
    head_counts = np.array([5,9,8,4,7])
    tail_counts = 10-head_counts
    experiments = zip(head_counts,tail_counts)
    
    # initialise the pA(heads) and pB(heads)
    pA_heads = np.zeros(100); pA_heads[0] = 0.60
    pB_heads = np.zeros(100); pB_heads[0] = 0.50
    
    # E-M begins!
    delta = 0.001  
    j = 0 # iteration counter
    improvement = float('inf')
    while (improvement>delta):
        expectation_A = np.zeros((5,2), dtype=float) 
        expectation_B = np.zeros((5,2), dtype=float)
        for i in range(0,len(experiments)):
            e = experiments[i] # i'th experiment
            ll_A = get_mn_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) # loglikelihood of e given coin A
            ll_B = get_mn_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) # loglikelihood of e given coin B
    
            weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of A proportional to likelihood of A 
            weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) # corresponding weight of B proportional to likelihood of B                            
    
            expectation_A[i] = np.dot(weightA, e) 
            expectation_B[i] = np.dot(weightB, e)
    
        pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); 
        pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); 
    
        improvement = max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - np.array([pA_heads[j],pB_heads[j]]) ))
        j = j+1


  [1]: http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf
  [2]: https://i.stack.imgur.com/mj0nb.gif
  [3]: http://math.stackexchange.com/questions/25111/how-does-expectation-maximization-work",added 128 characters in body,2013-10-15 07:35:33.550
186467,57504,22678.0,2,,CC BY-SA 3.0,58cc8075-3c1b-41fc-93a7-c150bc326371,"Basically, it depends of the function type, you are trying to model from data:

* Linear. $f(x)=a_1x_2+a_2x_2+ \cdots$

* Nonlinear: $f(x)=x_1*x_2/x_4+exp(-x_2/x_2)$

* Parameteric: actually, the both from top, but where you have physical/application meaning for the parameters $x_i$. e.g. splines, where the parameters of interest represent the path of a trajectory. 

* Non-Parametric: Linear model for nonlinear problems. Same as splines, but the bases are kernels. This is good, when you have a nonlinear/complex model but would like to do some kind of model selection (which abstract $x_i$ is the most important for your data e.g.). See Kernel (ridge) regression for details on this.

",,2013-10-15 07:41:50.867
186468,57505,22678.0,2,,CC BY-SA 3.0,db2f0890-4be7-445f-bfaa-ee5038e3b14c,"You will need a robust loss function in the kernel estimation model. 
However, this topic may become quite advances very fast. :) 
For a good start, I would suggest the one class SVM from sklearn. 
http://scikit-learn.org/stable/modules/svm.html#density-estimation-novelty-detection
",,2013-10-15 07:46:47.790
186469,57506,5671.0,2,,CC BY-SA 3.0,451e1840-c3e4-4432-b9f9-c64af518c673,"Actually the simplest approach would be **Association Rule Mining**, aka Frequent Itemset Mining (FIM). ""Clustering"" is an attempt to uncover structure, but not so much to make recommendations. It's explorative, not predictive; the clusters will most often be something rather obvious to the domain expert.

FIM will learn rules of the form that students, which have taken class A and B, have also taken class C with x% probability, i.e.

$$ {A,B} \rightarrow {C} \text{ with confidence }x\%$$

You *really* need to go through some introductory course. APRIORI is discussed everywhere, and is an obbvious fit here. In particular as you don't have quantities to predict (you don't have users that take class A 5 times and class B 2 times and thus are likely to buy -2 times class C...) Depending on your data, FPGrowth or Eclat algorithms may be more performat though.",,2013-10-15 07:47:16.070
186476,57508,3993.0,3,,CC BY-SA 3.0,db70dac2-603d-4dc2-b845-8d57649c7408,<variance><multilevel-analysis><random-effects-model><mixed-model>,,2013-10-15 08:21:54.933
186479,57403,20062.0,5,,CC BY-SA 3.0,9cac6f26-dc33-4ea3-8231-94c3d00fb31d,"**Mantel's test widely occur in biological studies** in which is used to
examine the correlation between spatial distribution of animals (position in space) with for example their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (*PNAS, Animal Behaviour, Molecular Ecology...*)

Recently I fabricated some patterns which may occur in nature, but Mantel's test **seems to be quite useless.** *(see p-values tests under each plot)*


----------


***Imaginary situation:*** Suppose you have orchard (17 x 17 trees) and on each tree a crow is sitting. Levels of ""noise"" for each crow are given and you are searching for pattern in distribution.

***There are 5 possibilities:***

 1. **""Birds of a feather flock together.""** The more similar crows are, the
    smaller is geographical distance between them **(single cluster)**.
    
 2. **""Birds of a feather flock together.""** Also the more similar crows
    are, the smaller is geographical distance between them **(multiple
    clusters)** but, one cluster of noisy crows has no knowledge about the
    existence of second cluster (otherwise they would fuse into a one big
    cluster).
    
 3. **""Monotonical trend occur""**. 
    
 4. **""Opposites attract each other""**. Similar crows can not withstand each
    other.
    
 5. **""Random pattern""** - the level of noise has no significant effect on
    spatial distribution.

Under each case is given plot of points from which the Mantel test computes correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).

![enter image description here][1]


  [1]: https://i.stack.imgur.com/TWQqa.png

**Why scientists do not use Moran's I instead? Is there some hidden reason I do not see?
And if there is such reason,how can I know (how different the hypotheses must be constructed) to appropriately use Mantel's or Moran's I test? A real life example will be helpful.**


----------


**Example data:** *(compressed as possible)*

    r.gen<-seq(-100,100,5)
    r.val<-sample(r.gen, 289, replace=TRUE)
    z10<-rep(0, times=10)
    z11<-rep(0, times=11)
    r5<-c(5,15,25,15,5)
    r71<-c(5,20,40,50,40,20,5)
    r72<-c(15,40,60,75,60,40,15)
    r73<-c(25,50,75,100,75,50,25)
    rbPal <- colorRampPalette(c(""blue"",""red""))
    my.data<-data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
    c1 = c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
    z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
    seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
    seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
    seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
    seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
    replace=TRUE))
    
    # adding colors
    my.data$Col1 <- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
    my.data$Col2 <- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
    my.data$Col3 <- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
    my.data$Col4 <- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
    my.data$Col5 <- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]

Creating matrix of geographical distances (for Moran's I is inversed):

    point.dists <- dist(cbind(my.data$x,my.data$y))
    point.dists.inv <- 1/point.dists
    point.dists.inv<-as.matrix(point.dists.inv)
    diag(point.dists.inv) <- 0

Plot creation:

    X11(width=12, height=6)
    par(mfrow=c(2,5))
    par(mar=c(1,1,1,1))

    library(ape)
    for (i in 3:7) {
    my.res<-mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
    plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
    ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
    text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
    2), ""\n p.value ="", round(my.res$p, 3)))
    my.res<-Moran.I(my.data[ ,i], point.dists.inv)
    text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
    3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
    round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
    }
    
    par(mar=c(5,4,4,2)+0.1)
    
    for (i in 3:7) {
    plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
    ylab=""behavioural distance"")
    }

P.S. if you google each tests, in these examples are both used on the exact same data
and exact the same hypothesis...**not very helpful**

Mantel test: http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm

Moran's I: http://www.ats.ucla.edu/stat/r/faq/morans_i.htm",added 39 characters in body,2013-10-15 08:23:13.427
186480,57509,16474.0,2,,CC BY-SA 3.0,54f66405-b401-4c26-8511-b8ea573e7d12,"In many cases a distribution can be described as a result of some idealized experiment. For example if we flip a fair coin $n$ times the number of heads will follow a binomial distribution with parameters $n$ and .5. These idealized experiments are often used as models; they are used as simplified representation of how the data came to be. There are obviously many such models, and as a consequence many distributions. If you want the logic behind all distributions, then that will require a book of many volumes, e.g.:

N. L. Johnson, S. Kotz and N. Balakrishnan (2000). _Continuous Multivariate Distributions_, Vol. 1 (second edition), New York: Wiley & Sons.

 N. L. Johnson, S. Kotz and N. Balakrishnan (1997). _Discrete Multivariate Distributions_. New York: John Wiley & Sons.

 N. L. Johnson, S. Kotz and N. Balakrishnan (1995). _Continuous Univariate Distributions_, Vol. 2 (second edition), New York: John Wiley & Sons.

 N. L. Johnson, S. Kotz and N. Balakrishnan (1994). _Continuous Univariate Distributions_, Vol. 1 (second edition), New York: John Wiley & Sons.

 N. L. Johnson, A. W. Kemp and S. Kotz (1992). _Univariate Discrete Distributions_ (second edition), New York: John Wiley & Sons.

A shorter list of distributions that is more suitable/affordable for owning yourself is:

Forbes, C., Evans, M., Hastings, N., & Peacock, B. (2011). Statistical distributions. Wiley",,2013-10-15 08:28:49.833
186481,57492,20130.0,5,,CC BY-SA 3.0,04914208-f376-4f68-b180-47443cbc6682,"There's a question for [textbooks](http://stats.stackexchange.com/questions/4612/good-econometrics-textbooks) here, but I would like to ask similar question about handbooks: What econometrics handbooks would you recommend?

Assumed audience is researchers and graduate-level students. It needs to include the material of Greene's _Econometric Analysis_ and Wooldridge's _Econometric Analysis of Cross Section and Panel Data_ in a denser form (without discussions and proofs), as well as mainstream techniques not mentioned in these two books.

One option is Elsevier's 6 volumes of _Handbook of Econometrics_ series edited by Griliches and Instriligator. However, perhaps you would recommend other handbooks, more concise or otherwise?

I suggest two possible formats: one is a reference-card format with minimum explanations, and the other one is a more extended format with proofs and more detailed exposition.",added 322 characters in body,2013-10-15 08:37:16.480
186482,57510,19557.0,2,,CC BY-SA 3.0,2cd67842-111b-4450-b010-57955347945e,"ID3 is an [algorithm](http://www.cise.ufl.edu/~ddd/cap6635/Fall-97/Short-papers/2.htm) for building a decision tree classifier based on maximizing information gain at each level of splitting across all available attributes. It's a precursor to the [C4.5 algorithm](http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/dtrees/c4.5/tutorial.html).

The ID3 algorithm won't predict a continuous value (you typically use regression for that) but it can use continuous values as input and should segment those appropriately to classify the various instances in your training set. Without looking too closely at the data or knowing what tool you're using, I'd suggest using simple bins for the value you're attempting to predict (""high"" or ""low""). 

It depends on the distribution of values across the range 1-10 how many bins you use. The goal is to get a relatively even distribution of training samples across the bins. If you have values concentrated in one particular bin (either ""high"" or ""low""), you'll have problems with the precision and recall of your resulting model. In highly skewed data sets, always guessing the dominant class will seem the optimal result to the model. 

Most machine learning tools like [Weka](http://www.cs.waikato.ac.nz/ml/weka/) allow you to reclassify data into bins. It's also fairly simple to do this with a Python script or even a simple formula in Excel (=IF(::cell of interest:: > 5,""high"",""low"")) since this dataset is so small.

However you decide to accomplish binning, make sure you look at the data first to make sure those bins make sense.",,2013-10-15 08:47:11.623
186485,57511,8819.0,1,,CC BY-SA 3.0,00523226-1afc-440c-a428-077b71005636,Integration with respect to Multivariate normal distribution,,2013-10-15 08:47:23.500
186484,57511,8819.0,3,,CC BY-SA 3.0,00523226-1afc-440c-a428-077b71005636,<normal-distribution><multivariate-analysis><numerical-integration>,,2013-10-15 08:47:23.500
186483,57511,8819.0,2,,CC BY-SA 3.0,00523226-1afc-440c-a428-077b71005636,"I am working on the numerical integration of an integral of the following functional form:

$$ \int\limits_{R^{G}} F(x_{1},x_{2},\text{…}x_{G})d\text{Φ}_{\text{Σ}}(x_{1},x_{2},\text{…},x_{G}) $$

Here  
$$ \text{Φ}_{\text{Σ}}(x_{1},x_{2},\text{…},x_{G}) $$ 

is the G-dimensional multivariate normal distribution with correlation matrix $\Sigma$ and F is some function of the constituent marginals. 

What I am essentially doing is calculating the expectation of a function over a correlated multivariate normal distribution. Practically, G is expected to be equal to or less than 4 and most often just 2 or 3.

Can some one know-how share any of the fundamental references that tackles the issue.My research yielded some information, and it appears that Gaussian quadrature is one of the preferred ways to approach the problem. I am referring to the book **Applied Computational
Economics and Finance** by Miranda and Fackler for addressing the implementation aspects of the algorithm. 

But, I wanted to get some help from the expert community here on if I am on the right track. 

Sorry if it is a repeat, however I searched on the site, and was not able to find a question that matches with what I had.",,2013-10-15 08:47:23.500
186486,57511,,25,,,98ff56cb-661f-4da7-9f71-50572a9a8643,,http://twitter.com/#!/StackStats/status/390045115139784704,2013-10-15 09:22:56.877
186487,57496,12314.0,5,,CC BY-SA 3.0,56107172-1e2b-409a-8a30-1b988fa357cd,"Suppose that I have daily data on the population of a small village, given by $Y(t)$, as well as daily data on various factors that are relevant to the size of the population in the future, given by vector $X(t)$. These explanatory variables include untransformed variables as well as features engineered to be informative over long horizons (e.g. one of the variables captures the number of deaths over the last 30 days). I have collected this data for 8 years. 

My objective is to forecast $Y(t)$ ahead by 1,2,3,...,365 days. I expect long-run forecasts to be different to short-run forecasts. If a holiday season is coming up I might expect a downwards spike in a few months time (people visiting the city), but if someone is on their deathbed then I will expect a downwards spike in a few days.

Since the population is sufficiently small that $\Delta Y(t+k)$ is typically in $\{-2,-1,0,1,2\}$ for the forecasting horizon under question, I will use a multiple categorical response variable classification model that will assign probabilities to the various class labels being observed.

My question centers on the specific considerations I need to make when constructing forecasts of the change from $Y(t)$ to $Y(t+k)$ where $k$ is large (e.g. 100 days). 

Basically there will be the most hideous autocorrelation structure in $\Delta Y(t+k)$ over these time scales. If someone dies on day $2$, they are also dead on day $3, 4, ..., k$, meaning a string of $k$ or so $\Delta Y(t+k)$ will contain this same information. 

These queries result:

- What are some ways of dealing with this immense autocorrelation structure in my response. Is it even a problem?
- Are there alternative methodologies to the ones I've proposed for forecasting these horizons (aside from typical machine learning methods such as random forests which I'm already working with).
- Any other handy advice.








",deleted 1 characters in body,2013-10-15 09:23:12.893
186488,57512,22707.0,2,,CC BY-SA 3.0,b7ed8621-0a12-4c24-a4a6-d5cae062c956,"The output of TANH is already between -1 and 1. So, if you normalise the inpu, be sure to normalise for the hidden activation functions. In theory it is not required to normalise, because tanh(1000) is mathematically different from tanh(10000). But in practice these are the same, so you should indeed normalise the input in most applications.",,2013-10-15 09:31:11.227
186490,57513,10594.0,2,,CC BY-SA 3.0,96a35508-4fe5-4957-98d5-5291599c2ef8,"I am conducting a glm. The explanatory variable is categorical with three levels (control, treat1, treat2). The response variable is 0 or 1.
The response rate for each treatment level is ploted as the figure below (from left to right: control, treat1, treat2):

![enter image description here][1]

There seems to be a big treatment effect between treat1 vs. control and treat2 vs. control. I applied glm:

`fit <- glm(response ~ treatment, family = binomial, data = dat)`

    Coefficients:
    Estimate Std. Error z value Pr(>|z|)        
    (Intercept)   -21.57    6536.57  -0.003    0.997
    treat1       23.76    6536.57   0.004    0.997
    treat2      43.13    9364.95   0.005    0.996

The z-test shows that neither treat1 nor treat2 is significant compared to the reference level control.

However, the analysis of deviance confirmed that the treatment factor as a whole is highly significant:

    drop1(M2, test=""Chisq"")
    response ~ treatment
                Df   Deviance    AIC    LRT  Pr(>Chi)    
     <none>          13.003    19.003                     
     treatment   2   77.936    79.936 64.932 7.946e-15 ***

How shall I interpret such strange result? Why the individual z-test does not give me any significant result, while according to the plot there is obvious an effect between treat1 and control, and between treat2 and control?

Thanks
 



  [1]: https://i.stack.imgur.com/Abi8S.png",,2013-10-15 09:49:58.953
186493,57513,10594.0,5,,CC BY-SA 3.0,a205df0c-ff86-4268-aaa7-7c1e43aa77b3,"I am conducting a glm. The explanatory variable is categorical with three levels (control, treat1, treat2). The response variable is 0 or 1.
The response rate for each treatment level is ploted as the figure below (from left to right: control, treat1, treat2):

![enter image description here][1]

There seems to be a big treatment effect between treat1 vs. control and treat2 vs. control. I applied glm:

`fit <- glm(response ~ treatment, family = binomial, data = dat)`

    Coefficients:
                 Estimate Std. Error z value Pr(>|z|)        
    (Intercept)   -21.57    6536.57  -0.003    0.997
    treat1        23.76    6536.57   0.004    0.997
    treat2        43.13    9364.95   0.005    0.996

The z-test shows that neither treat1 nor treat2 is significant compared to the reference level control.

However, the analysis of deviance confirmed that the treatment factor as a whole is highly significant:

    drop1(M2, test=""Chisq"")

    response ~ treatment
                Df   Deviance    AIC    LRT  Pr(>Chi)    
     <none>          13.003    19.003                     
     treatment   2   77.936    79.936 64.932 7.946e-15 ***

How shall I interpret such strange result? Why the individual z-test does not give me any significant result, while according to the plot there is obvious an effect between treat1 and control, and between treat2 and control?

Thanks
 



  [1]: https://i.stack.imgur.com/Abi8S.png",added 18 characters in body,2013-10-15 09:57:39.077
186496,57514,14874.0,3,,CC BY-SA 3.0,ee3b9636-f7b1-4cff-a940-8cfb978839b9,<gamma-distribution><bivariate><nonlinear>,,2013-10-15 10:00:24.350
186495,57514,14874.0,1,,CC BY-SA 3.0,ee3b9636-f7b1-4cff-a940-8cfb978839b9,Constructing a bivariate distribution from two gamma-distributed random variables with nonlinear dependence?,,2013-10-15 10:00:24.350
186494,57514,14874.0,2,,CC BY-SA 3.0,ee3b9636-f7b1-4cff-a940-8cfb978839b9,"I've got 2 gamma-distributed random variables $(X,Y)$ with arbitrary scale and shape parameters. Further, $Y$ should be a non-linear function of $X$, lets say $Y=\sqrt{X}$. What I am interested in is the joint probability $F_{X,Y}(\cdot)$. 

All suggestions or general comments are welcome.

Thank you in advance

",,2013-10-15 10:00:24.350
186497,57403,20062.0,5,,CC BY-SA 3.0,bf03f4b5-03a4-4ba2-8e69-2386289dcdc5,"**Mantel's test widely occur in biological studies** in which is used to
examine the correlation between spatial distribution of animals (position in space) with for example their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (*PNAS, Animal Behaviour, Molecular Ecology...*)

Recently I fabricated some patterns which may occur in nature, but Mantel's test **seems to be quite useless.** *(see p-values under each plot)*


----------


***Imaginary situation:*** Suppose you have orchard (17 x 17 trees) and on each tree a crow is sitting. Levels of ""noise"" for each crow are given and you are searching for pattern in distribution.

***There are 5 possibilities:***

 1. **""Birds of a feather flock together.""** The more similar crows are, the
    smaller is geographical distance between them **(single cluster)**.
    
 2. **""Birds of a feather flock together.""** Also the more similar crows
    are, the smaller is geographical distance between them **(multiple
    clusters)** but, one cluster of noisy crows has no knowledge about the
    existence of second cluster (otherwise they would fuse into a one big
    cluster).
    
 3. **""Monotonical trend occur""**. 
    
 4. **""Opposites attract each other""**. Similar crows can not withstand each
    other.
    
 5. **""Random pattern""** - the level of noise has no significant effect on
    spatial distribution.

Under each case is given plot of points from which the Mantel test computes correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).

![enter image description here][1]


  [1]: https://i.stack.imgur.com/TWQqa.png

**Why scientists do not use Moran's I instead? Is there some hidden reason I do not see?
And if there is such reason,how can I know (how different the hypotheses must be constructed) to appropriately use Mantel's or Moran's I test? A real life example will be helpful.**


----------


**Example data:** *(compressed as possible)*

    r.gen<-seq(-100,100,5)
    r.val<-sample(r.gen, 289, replace=TRUE)
    z10<-rep(0, times=10)
    z11<-rep(0, times=11)
    r5<-c(5,15,25,15,5)
    r71<-c(5,20,40,50,40,20,5)
    r72<-c(15,40,60,75,60,40,15)
    r73<-c(25,50,75,100,75,50,25)
    rbPal <- colorRampPalette(c(""blue"",""red""))
    my.data<-data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
    c1 = c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
    z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
    z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
    seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
    seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
    seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
    seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
    replace=TRUE))
    
    # adding colors
    my.data$Col1 <- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
    my.data$Col2 <- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
    my.data$Col3 <- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
    my.data$Col4 <- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
    my.data$Col5 <- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]

Creating matrix of geographical distances (for Moran's I is inversed):

    point.dists <- dist(cbind(my.data$x,my.data$y))
    point.dists.inv <- 1/point.dists
    point.dists.inv<-as.matrix(point.dists.inv)
    diag(point.dists.inv) <- 0

Plot creation:

    X11(width=12, height=6)
    par(mfrow=c(2,5))
    par(mar=c(1,1,1,1))

    library(ape)
    for (i in 3:7) {
    my.res<-mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
    plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
    ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
    text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
    2), ""\n p.value ="", round(my.res$p, 3)))
    my.res<-Moran.I(my.data[ ,i], point.dists.inv)
    text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
    3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
    round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
    }
    
    par(mar=c(5,4,4,2)+0.1)
    
    for (i in 3:7) {
    plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
    ylab=""behavioural distance"")
    }

P.S. if you google each tests, in these examples are both used on the exact same data
and exact the same hypothesis...**not very helpful**

Mantel test: http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm

Moran's I: http://www.ats.ucla.edu/stat/r/faq/morans_i.htm",deleted 6 characters in body,2013-10-15 10:06:31.947
186498,57515,22677.0,2,,CC BY-SA 3.0,a1d78672-68e5-4d0e-856e-20f5a7ea7b35,"Nevermind, apparently the $ARL0=\frac1\alpha$ where alpha is ***false alarm probability***

a further reading wold be
>Nonparametric monitoring of data streams for changes in location and scale
GJ Ross, DK Tasoulis, NM Adams - Technometrics, 2011 - Taylor & Francis",,2013-10-15 10:13:54.107
186499,57426,20470.0,5,,CC BY-SA 3.0,d998e12c-3396-4e85-86d8-20c15af34373,"**Question**: *Is the set-up below a sensible implementation of a Hidden Markov model?*

I have a set of `108,000` observations (taken over the course of 100 days) and approximately `2000` event occurrences throughout the whole observation time-span. The data looks like the figure below where there are 3 observation levels and the red columns highlight event times, i.e. $t_E$'s:

![enter image description here][1]

As shown with red rectangles in the figure, I have dissected {$t_E$ to $t_{E-5}$} for each event, effectively treating these as ""pre-event windows"". 

**HMM Training:** I plan to [train][2] a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested on Pg. 273 of Rabiner's [paper][3]. Hopefully, this will allow me to train an HMM that captures the sequence patterns which lead to an event.

**HMM Prediction:** Then I plan to use this HMM to [predict][4] $P(Observations|HMM)$ on a new day, where $Observations$ will be a sliding window vector, which I will update to contain the observations between the current time $t$ and $t-5$ as the day goes on. 

I expect to see $log[P(Observations|HMM)]$ increase for $Observations$ that resemble the ""pre-event windows"". This should in effect allow me to predict the events before they happen.



  


  [1]: https://i.stack.imgur.com/QkIn0.png
  [2]: http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm
  [3]: http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf
  [4]: http://en.wikipedia.org/wiki/Forward_algorithm",added 6 characters in body,2013-10-15 10:32:57.617
186500,57479,0.0,36,,,a9ce8885-4a10-45ea-b0b9-8c8275db45c5,,from http://programmers.stackexchange.com/questions/214426/how-would-you-use-pair-wise-plots-to-test-the-effectiveness-of-k-means-clusterin,2013-10-15 10:39:06.057
186503,57516,21884.0,2,,CC BY-SA 3.0,7ece545a-e6b3-4db3-9afe-5d8e1cd67990,"Is it true that (and if so, how does one prove) the following.
 $$E\left|\hat{Var}_{n}(X)-Var(X)\right|^{2}=O(n^{-1})$$
  where:

• $X$ is a random variable with mean $\mu$ and variance $\sigma^{2}$
 

• $\hat{Var}_{n}(X)$ is the sample variance of $X$ from $n$
  i.i.d. random variables $X_{1},\cdots,X_{n}$ with mean $\mu$
  and variance $\sigma^{2}$.

Many thanks in advance. (Feel free to change my notation).",,2013-10-15 10:40:17.880
