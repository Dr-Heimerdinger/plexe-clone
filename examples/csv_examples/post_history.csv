id,post_id,user_id,post_history_type_id,user_display_name,content_license,revision_guid,text,comment,creation_date
22973,8478,2269.0,1,,CC BY-SA 2.5,72878e3c-2f75-4067-9f01-7f9c0d9b72f0,Kullback–Leibler vs Kolmogorov-Smirnov distance,,2011-04-07 11:39:13.870
22972,8478,2269.0,2,,CC BY-SA 2.5,72878e3c-2f75-4067-9f01-7f9c0d9b72f0,"I can see the a lot of formal differences between the two measures, but since both  are used to measure the distance between distributions, I wondered whether there was typical situations where one should be use instead of the other, and what are the rationale to do so.

Thanks,

Greg",,2011-04-07 11:39:13.870
22974,8478,2269.0,3,,CC BY-SA 2.5,72878e3c-2f75-4067-9f01-7f9c0d9b72f0,<distributions><distance-functions><kolmogorov-smirnov-test><kullback-leibler>,,2011-04-07 11:39:13.870
22976,8478,,25,,,e195f56d-477d-41f0-9816-c4a34499863b,,http://twitter.com/#!/StackStats/status/55965302416080898,2011-04-07 12:09:02.477
22980,8478,,5,,CC BY-SA 2.5,d61c994e-64dc-41bd-87bd-1c0b980db58f,"I can see that there are a lot of formal differences between Kullback–Leibler vs Kolmogorov-Smirnov distance measures.
However, both are used to measure the distance between distributions.

* Is there a typical situation where one should be used instead of the other? 
* What is the rationale to do so?",added 7 characters in body,2011-04-07 14:26:13.410
23069,8504,115.0,2,,CC BY-SA 3.0,0d7850f8-8b08-420b-b653-928deda42374,"The KL-divergence is typically used in information-theoretic settings, or even Bayesian settings, to measure the information change between distributions before and after applying some inference, for example. It's not a distance in the typical (metric) sense, because of lack of symmetry and triangle inequality, and so it's used in places where the directionality is meaningful. 

The KS-distance is typically used in the context of a non-parametric test. In fact, I've rarely seen it used as a generic ""distance between distributions"", where the $\ell_1$ distance, the Jensen-Shannon distance, and other distances are more common. ",,2011-04-08 04:07:17.223
26420,9524,2872.0,1,,CC BY-SA 3.0,b70c32e3-f269-4e7e-8fdb-8fb77aa0fa6d,"Any good movies with maths, probabilities etc?",,2011-05-07 11:13:51.243
26419,9524,2872.0,2,,CC BY-SA 3.0,b70c32e3-f269-4e7e-8fdb-8fb77aa0fa6d,"Can you suggest some good movies which involve math, probabilities etc? One example is [21][1]. I would also be interested in movies that involve algorithms (e.g. text decryption). In general ""geeky"" movies with famous scientific theories but no science fiction or documentaries. Thanks in advance!


  [1]: http://en.wikipedia.org/wiki/21_%282008_film%29",,2011-05-07 11:13:51.243
26421,9524,2872.0,3,,CC BY-SA 3.0,b70c32e3-f269-4e7e-8fdb-8fb77aa0fa6d,<probability>,,2011-05-07 11:13:51.243
26426,9524,,25,,,78ac57da-6346-4ccb-9f31-ed7f30c5100a,,http://twitter.com/#!/StackStats/status/66837557815676928,2011-05-07 12:11:30.140
26427,9524,,16,,,101c07d2-4565-4357-abbe-0b1d27f2aced,,,2011-05-07 12:32:39.210
26429,9524,,4,,CC BY-SA 3.0,db37784d-623d-4729-9123-68d777ee8d93,Are there any good movies involving mathematics or probability?,edited tags; edited title,2011-05-07 12:34:34.433
26428,9524,,6,,CC BY-SA 3.0,db37784d-623d-4729-9123-68d777ee8d93,<probability><references>,edited tags; edited title,2011-05-07 12:34:34.433
26440,9529,192.0,16,,,0a4c4ecc-5f60-49ec-b06b-60958c546b3b,,,2011-05-07 14:19:42.887
26439,9529,192.0,2,,CC BY-SA 3.0,1b2c43cf-4004-4f05-892c-6916f9394665,"[Pi][1]


  [1]: http://www.imdb.com/title/tt0138704/",,2011-05-07 14:19:42.887
61444,20582,2584.0,3,,CC BY-SA 3.0,03bfa18d-c4cb-4112-97f9-7ee24f6fb8a1,<normal-distribution><pca><independence><independent-component-analysis>,,2012-02-16 00:21:27.737
61443,20582,2584.0,1,,CC BY-SA 3.0,03bfa18d-c4cb-4112-97f9-7ee24f6fb8a1,When will PCA be equivalent to ICA?,,2012-02-16 00:21:27.737
61445,20582,2584.0,2,,CC BY-SA 3.0,03bfa18d-c4cb-4112-97f9-7ee24f6fb8a1,"$X = AS$ where $A$ is my mixing matrix and each column of $S$ represents my sources.  $X$ is the data I observe.

If the columns of $S$ are independent and Gaussian, will the components of PCA be extremely similar to that of ICA?  Is this the only requirement for the two methods to coincide?",,2012-02-16 00:21:27.737
61449,20582,2584.0,5,,CC BY-SA 3.0,ccc7bb76-8dcd-48bd-bb38-ca16e847b0ab,"$X = AS$ where $A$ is my mixing matrix and each column of $S$ represents my sources.  $X$ is the data I observe.

If the columns of $S$ are independent and Gaussian, will the components of PCA be extremely similar to that of ICA?  Is this the only requirement for the two methods to coincide?

Can someone provide an example of this being true when the $cov(X)$ isn't diagonal?  ",added 89 characters in body,2012-02-16 00:34:11.820
263255,80672,30778.0,1,,CC BY-SA 3.0,3b3425e3-81f8-4d8f-966c-2ab6e0af755b,"Which notation and why: $\text{P}()$, $\Pr()$, or $\mathbb{P}()$",,2014-07-18 17:34:55.273
263256,80672,30778.0,3,,CC BY-SA 3.0,3b3425e3-81f8-4d8f-966c-2ab6e0af755b,<notation>,,2014-07-18 17:34:55.273
263254,80672,30778.0,2,,CC BY-SA 3.0,3b3425e3-81f8-4d8f-966c-2ab6e0af755b,"Are these merely stylistic conventions, or are there substantive differences in the meanings of these notations?

Are there other notations meaning ""*the probability of*"" that should be considered in this question?",,2014-07-18 17:34:55.273
263266,80677,20473.0,2,,CC BY-SA 3.0,b6ff43d5-dac9-4dce-99d2-5619889e3579,"Stylistic conventions, mainly, but with some underlying rationale.

$\mathbb{P}()$ and $\Pr()$ can be seen as two ways to ""free up"" the letter $\text{P}$ for other use -it _is_ used to denote other things than ""probability"", for example in research with complicated and extensive notation where one starts to exhaust available letters.  

$\mathbb{P}()$ requires special fonts, which is a disadvantage. $\Pr()$ may be useful when the author would want the reader to think of probability in abstract and general terms, using the second lower-capital letter ""$r$"" to disassociate the symbol as a whole from the usual way we write up functions.  

For example, some problems are solved when one remembers that the cumulative distribution function of a random variable can be written and treated as a probability of an ""inequality-event"", and apply the basic probability rules rather than functional analysis.

In some cases, one may also see $\text {Prob}()$, again, usually in the beginning of an argument that will end up in a specific formulation of how this probability is functionally determined.

The _italics_ version $P()$ is also used, and also in lower-case form, $p()$, -this last version is especially used when discussing discrete random variables (where the _probability mass function_ _is_ a probability).  



$\pi(\;,\;)$ is used for conditional (""transition"") probabilities in Markov Theory.",,2014-07-18 18:15:52.797
1074071,171590,0.0,50,,,e50e6f75-083c-468c-978e-52ad408066ea,,,2020-04-28 02:01:55.213
1075351,307384,171258.0,1,,CC BY-SA 4.0,2574d816-cc78-483c-8947-bdaa87fe06e9,Is CPCV similar to bootstrap?,,2020-04-30 14:06:48.817
1075350,307384,171258.0,2,,CC BY-SA 4.0,2574d816-cc78-483c-8947-bdaa87fe06e9,"I am writing to ask if the Combinatorial (Purged) Cross-Validation"" method of Marcos Lopez de Prado's ""Advances in Financial Machine Learning"" book is similar to the idea of bootstrap. If not, what is the key difference? It seems like bootstrap is based on permutation (resampling with replacement) and C(P)CV is based on combinations (resampling without replacement)? At the same time, I suppose the key goal between both methods is to generate as many artificial samples from the same dataset? ",,2020-04-30 14:06:48.817
263267,80677,30778.0,5,,CC BY-SA 3.0,6f71a157-8d49-47d1-9a73-2773a2afd35f,"Stylistic conventions, mainly, but with some underlying rationale.

$\mathbb{P}()$ and $\Pr()$ can be seen as two ways to ""free up"" the letter $\text{P}$ for other use—it _is_ used to denote other things than ""probability"", for example in research with complicated and extensive notation where one starts to exhaust available letters.  

$\mathbb{P}()$ requires special fonts, which is a disadvantage. $\Pr()$ may be useful when the author would want the reader to think of probability in abstract and general terms, using the second lower-capital letter ""$r$"" to disassociate the symbol as a whole from the usual way we write up functions.  

For example, some problems are solved when one remembers that the cumulative distribution function of a random variable can be written and treated as a probability of an ""inequality-event"", and apply the basic probability rules rather than functional analysis.

In some cases, one may also see $\text {Prob}()$, again, usually in the beginning of an argument that will end up in a specific formulation of how this probability is functionally determined.

The _italics_ version $P()$ is also used, and also in lower-case form, $p()$, -this last version is especially used when discussing discrete random variables (where the _probability mass function_ _is_ a probability).  



$\pi(\;,\;)$ is used for conditional (""transition"") probabilities in Markov Theory.",deleted 1 character in body,2014-07-18 18:19:02.053
263269,80672,30778.0,4,,CC BY-SA 3.0,46b2dcc3-3a5a-4ff3-ac12-5f22d0e7fd2e,"Which notation and why: $\text{P}()$, $\Pr()$, $\text{Prob}()$, or $\mathbb{P}()$",Updated based on an answer,2014-07-18 18:20:47.403
263379,80672,,25,,,2c39c3ef-9c81-4b47-9761-00f9287df1d2,,http://twitter.com/#!/StackStats/status/490332168099749888,2014-07-19 03:07:53.513
335661,102770,47667.0,3,,CC BY-SA 3.0,c6b9edd9-a297-470a-bb6a-8cfd7b8c342b,<machine-learning><python><prediction><scikit-learn>,,2015-03-17 22:08:24.090
335660,102770,47667.0,1,,CC BY-SA 3.0,c6b9edd9-a297-470a-bb6a-8cfd7b8c342b,How do i get prediction accuracy when testing unknown data on a saved model in Scikit-Learn?,,2015-03-17 22:08:24.090
335659,102770,47667.0,2,,CC BY-SA 3.0,c6b9edd9-a297-470a-bb6a-8cfd7b8c342b,"i have a model i have trained for binary classification, i now want to use it to predict unknown class elements.

         from sklearn.externals import joblib
         model = joblib.load('../model/randomForestModel.pkl')
         test_data = df_test.values # df_test is a dataframe with my test data
         output = model.predict(test_data[:,1:]) # this outputs the prediction either 1 or 0

I know how to get confusion_matrix, accuracy_score, classification_report given the training dataset, but in the case i do not have the train data.
i would like to get something akin to this from weka:
 

           inst#     actual  predicted error prediction
               1        1:?        1:0       0.757 


Is it possible in Scikit-learn? if so, how do i do it? 
 ",,2015-03-17 22:08:24.090
393989,120255,58559.0,2,,CC BY-SA 3.0,110ca7d0-7784-43ee-a524-c28a46d5c69f,"you can only get the prediction accuracy on any dataset, if you have the true classes / targets. so in a real world application of your model to a completely unknown testset without classes, you cannot get the prediction error. the prediction error is derived from the true classes.

but you can use your trainingset and split it into test / validation parts, to get prediction accuracy.",,2015-09-01 09:35:33.257
394356,120364,56490.0,3,,CC BY-SA 3.0,bfad4d65-0725-4f24-aa9c-f77d83da18a7,<machine-learning><neural-networks><optimization><pattern-recognition><gradient-descent>,,2015-09-02 09:21:11.367
394358,120364,56490.0,1,,CC BY-SA 3.0,bfad4d65-0725-4f24-aa9c-f77d83da18a7,"Using gradient information in minimizing error function, in Bishop's Pattern Recognition",,2015-09-02 09:21:11.367
394357,120364,56490.0,2,,CC BY-SA 3.0,bfad4d65-0725-4f24-aa9c-f77d83da18a7,"In Bishop's book *Pattern Recognition*, there appears the following paragraph on page 239, where I inlined the equation he refers to

> In the quadratic approximation to the error function, 
$$E(w) = E(w^\star) + (w − w^\star)^Tb + \frac 1 2 (w − w^\star)^TH(w − w^\star)$$,
with $b$ the gradient and $H$ the Hessian, the error
surface is specified by the quantities b and H, which contain a total of W(W +
3)/2 independent elements (because the matrix H is symmetric), where W is the
dimensionality of w (i.e., the total number of adaptive parameters in the network).
The location of the minimum of this quadratic approximation therefore depends on
O(W^2) parameters, and we should not expect to be able to locate the minimum until
we have gathered O(W^2) independent pieces of information. If we do not make
use of gradient information, we would expect to have to perform O(W^2) function evaluations, each of which would require O(W) steps. Thus, the computational effort needed to find the minimum using such an approach would be O(W^3). 

> Now compare this with an algorithm that makes use of the gradient information. Because each evaluation of ∇E brings W items of information, we might hope to find the minimum of the function in O(W) gradient evaluations. As we shall see, by using error backpropagation, each such evaluation takes only O(W) steps and so the minimum can now be found in O(W^2) steps. For this reason, the use of gradient information forms the basis of practical algorithms for training neural networks.

I can't see the intuition he's referring to. What is he referring to as information, and how is he estimating the number of steps to the minimum?",,2015-09-02 09:21:11.367
394398,120364,56490.0,5,,CC BY-SA 3.0,6faf18ce-a3b8-4ec3-82d7-69df8b199e2c,"In Bishop's book *Pattern Recognition*, there appears the following paragraph on page 239, where I inlined the equation he refers to

> In the quadratic approximation to the error function, 
$$E(w) \approx E(\hat w) + (w − \hat w)^Tb + \frac 1 2 (w − \hat w)^TH(w − \hat w)$$,
with $b$ the gradient and $H$ the Hessian, the error
surface is specified by the quantities b and H, which contain a total of W(W +
3)/2 independent elements (because the matrix H is symmetric), where W is the
dimensionality of w (i.e., the total number of adaptive parameters in the network).
The location of the minimum of this quadratic approximation therefore depends on
O(W^2) parameters, and we should not expect to be able to locate the minimum until
we have gathered O(W^2) independent pieces of information. If we do not make
use of gradient information, we would expect to have to perform O(W^2) function evaluations, each of which would require O(W) steps. Thus, the computational effort needed to find the minimum using such an approach would be O(W^3). 

> Now compare this with an algorithm that makes use of the gradient information. Because each evaluation of ∇E brings W items of information, we might hope to find the minimum of the function in O(W) gradient evaluations. As we shall see, by using error backpropagation, each such evaluation takes only O(W) steps and so the minimum can now be found in O(W^2) steps. For this reason, the use of gradient information forms the basis of practical algorithms for training neural networks.

I can't see the intuition he's referring to. What is he referring to as information, and how is he estimating the number of steps to the minimum?",deleted 3 characters in body,2015-09-02 12:30:44.787
1075352,307384,171258.0,3,,CC BY-SA 4.0,2574d816-cc78-483c-8947-bdaa87fe06e9,<cross-validation><bootstrap>,,2020-04-30 14:06:48.817
1075781,133686,0.0,50,,,8f71d039-40a5-4f87-9676-cfaaff868130,,,2020-05-01 13:05:15.870
394423,120364,,5,,CC BY-SA 3.0,71df54ad-77c2-4a34-ab52-1616cf76aa7f,"In Bishop's book *Pattern Recognition*, there appears the following paragraph on page 239, where I included the equation he refers to

> In the quadratic approximation to the error function,  $$\displaystyle E(\mathbf{w}) \simeq E(\mathbf{\hat w}) + (\mathbf{w} − \mathbf{\hat w})^{\textrm T}\mathbf{b} + \frac 1 2 (\mathbf w − \mathbf{\hat w})^{\textrm T}\mathbf{H}(\mathbf{w} − \mathbf{\hat w})$$
With $\mathbf{b}$ the gradient and $\mathbf{H}$ the Hessian, the error surface is specified by the quantities $\mathbf{b}$ and $\mathbf{H}$, which contain a total of $W(W + 3)/2$ independent elements (because the matrix $\mathbf{H}$ is symmetric), where $W$ is the dimensionality of $\mathbf{w}$ (i.e., the total number of adaptive parameters in the network). The location of the minimum of this quadratic approximation therefore depends on $O(W^2)$ parameters, and we should not expect to be able to locate the minimum until we have gathered $O(W^2)$ independent pieces of information. If we do not make use of gradient information, we would expect to have to perform $O(W^2)$ function evaluations, each of which would require $O(W)$ steps. Thus, the computational effort needed to find the minimum using such an approach would be $O(W^3)$. 
> 
Now compare this with an algorithm that makes use of the gradient information. Because each evaluation of $\nabla E$ brings $W$ items of information, we might hope to find the minimum of the function in $O(W)$ gradient evaluations. As we shall see, by using error backpropagation, each such evaluation takes only $O(W)$ steps and so the minimum can now be found in $O(W^2)$ steps. For this reason, the use of gradient information forms the basis of practical algorithms for training neural networks.

I can't see the intuition he's referring to. What is he referring to as information, and how is he estimating the number of steps to the minimum?",improved formatting and fixed grammar,2015-09-02 13:35:33.620
394424,120364,,24,,CC BY-SA 3.0,71df54ad-77c2-4a34-ab52-1616cf76aa7f,,"Proposed by 49647 approved by 7290, 67799 edit id of 13479",2015-09-02 13:35:33.620
409081,80672,30778.0,5,,CC BY-SA 3.0,e502750f-4551-4bf0-9bf8-afb006b95ded,"Are these merely stylistic conventions (whether italicized or non-italicized), or are there substantive differences in the meanings of these notations?

Are there other notations meaning ""*the probability of*"" that should be considered in this question?",added 39 characters in body,2015-10-14 17:45:16.690
440052,133686,57794.0,1,,CC BY-SA 3.0,de1e3047-bdd4-4062-919e-2aa4fd5c99ef,How to calculate influence of variables at ROW LEVEL?,,2016-01-13 09:48:47.270
440053,133686,57794.0,3,,CC BY-SA 3.0,de1e3047-bdd4-4062-919e-2aa4fd5c99ef,<modeling><scoring><importance>,,2016-01-13 09:48:47.270
440054,133686,57794.0,2,,CC BY-SA 3.0,de1e3047-bdd4-4062-919e-2aa4fd5c99ef,"There are several algorithms which give relative Importance of variables at OVERALL Model level.
But, the most influencing variable might not be the reason why a particular row might get higher or lower scores.
 eg: different applications get different reasons for Credit decline though scored by the same Credit Risk model. This happens because one variable's value drives the overall score for that particular application down, although this variable might not be the one which is the most significant across all rows at a model level.

I know how to calculate the top influencing variables in a Logistic / Liner Regression Model for each ROW (by rank ordering the product of coefficient & variable value for each variable).

But how do we calculate relative importance of variables at each ROW level using other algorithms like Neural network, Random Forrest, etc.",,2016-01-13 09:48:47.270
440081,133686,,5,,CC BY-SA 3.0,7939bcc6-0dd7-4c95-8eec-5bfeec69d408,"There are several algorithms which give relative importance of variables at OVERALL Model level.
But the most influencing variable might not be the reason why a particular row might get higher or lower scores.
 eg: different applications get different reasons for Credit decline though scored by the same Credit Risk model. This happens because one variable's value drives the overall score for that particular application down, although this variable might not be the one which is the most significant across all rows at a model level.

I know how to calculate the top influencing variables in a Logistic / Linear Regression Model for each ROW (by rank ordering the product of coefficient & variable value for each variable).

But how do we calculate relative importance of variables at each ROW level using other algorithms like Neural network, Random Forest, etc.",deleted 1 character in body,2016-01-13 11:03:52.143
574524,171590,47569.0,3,,CC BY-SA 3.0,4d627dd1-4c34-4316-b09c-3e218cd12c9a,<svm><feature-selection><random-forest><model-selection>,,2016-12-13 03:29:32.103
574523,171590,47569.0,1,,CC BY-SA 3.0,4d627dd1-4c34-4316-b09c-3e218cd12c9a,How to choose the best algorithm for measuring attribute importance/relevance?,,2016-12-13 03:29:32.103
574522,171590,47569.0,2,,CC BY-SA 3.0,4d627dd1-4c34-4316-b09c-3e218cd12c9a,"Let's say we want to conclude that attributes A, B, C and D are the most relevant attributes to maximize the precision of predicting Y and rank them by their importance.  
  
Now let's say SVM and Random Forest both seem to be good fits to model the data and Random Forest provides better performance (higher precision). Does that mean Random Forest would also be a better choice to rank the attributes according to their relevance to D?  
  
In a more general sense, can we say the best algorithm for maximizing precision of predicting Y is also the best algorithm to rank the relevance of attributes to Y?",,2016-12-13 03:29:32.103
574800,171590,47569.0,5,,CC BY-SA 3.0,a5468008-e1af-4c96-9d3b-959cb7945672,"Let's say we want to conclude that attributes A, B, C and D are the most relevant attributes to maximize the precision of predicting Y, and then rank those attributes based on their importance/relevance.  
  
Now let's say SVM and Random Forest both seem to be good fits to model the data and Random Forest provides better performance (higher precision). Does that mean Random Forest would also be a better choice to rank the attributes according to their relevance to D?  
  
In a more general sense, can we say the best algorithm for maximizing precision of predicting Y is also the best algorithm to rank the relevance of attributes to Y?",added 34 characters in body,2016-12-13 16:16:24.120
574911,171590,47569.0,5,,CC BY-SA 3.0,7a0d7e0b-a5da-4c85-8dde-df18982f5b49,"Let's say we want to conclude that attributes `A`, `B`, `C` and `D` are the most relevant attributes to maximize the precision of predicting `Y`, and then rank those attributes based on their importance/relevance.  
  
Now let's say *SVM* and *Random Forest* both seem to be good fits to model the data and Random Forest provides better performance (higher precision). Does that mean Random Forest would also be a better choice to rank the attributes according to their relevance to `D`?  
  
In a more general sense, can we say the best algorithm for maximizing precision of predicting `Y` is also the best algorithm to rank the relevance of attributes to `Y`?",added 20 characters in body,2016-12-13 19:04:59.423
576048,172027,85141.0,2,,CC BY-SA 3.0,a88b10d6-6f09-40b6-8648-9b53af7a9278,"Is it better to split your dataset into train and test before vectorizing?

Or is it better to do it in reverse and vectorize inputs then perform train test split?

For example I'm trying to use some categorical inputs such as types of animals, should I vectorize them into one hot then split into train/test or split the data set randomly then vectorize into one hot?",,2016-12-15 23:47:26.190
576050,172027,85141.0,3,,CC BY-SA 3.0,a88b10d6-6f09-40b6-8648-9b53af7a9278,<neural-networks>,,2016-12-15 23:47:26.190
576049,172027,85141.0,1,,CC BY-SA 3.0,a88b10d6-6f09-40b6-8648-9b53af7a9278,Vectorization of data before splitting in to test and train with Neural Network?,,2016-12-15 23:47:26.190
576055,172031,9483.0,2,,CC BY-SA 3.0,c32b6d4e-b538-445b-87c3-a6d11cdc1c78,">  should I vectorize them into one hot then split into train/test or split the data set randomly then vectorize into one hot?

That makes no difference. If you split first, make sure your vectorize the same way.",,2016-12-16 00:01:43.807
614464,182063,109482.0,2,,CC BY-SA 3.0,4a48ee19-810a-4154-8f46-49e9ae9cd654,"I have a classification problem with $K$ labels. I represent the correct label $y$ of an observation $x$ as a vector $y$ in $R^K$, with entries $y_{k'} = \delta_{kk'}$ if $x$ belongs to class $k$.

Given an observation $x$, I predict its label with a vector $f(x) \in R^K$, where the components $f_k(x)$ satisfy $f_k(x) \in (0,1)$ and $\sum_k f_k(x) = 1$. A larger value of some $f_k(x)$ means that $x$ is more likely to belong to class $k$.

We want to learn the best function $f$ by choosing an appropriate loss function $\ell$. I know that a common choice is the cross-entropy: $\ell(x,y) = -\sum_k y_k \log f_k(x)$. Is squared loss $\ell(x,y) = \frac{1}{2}||y - f(x)||_2^2$ ever used? If so, does it tend to produce classifiers with noticeably distinct performance profiles, compared to cross-entropy?

A [comment](https://stats.stackexchange.com/questions/243090/square-loss-for-classification-via-regression) on a related question warns against the use of squared loss.",,2017-03-11 00:16:56.000
614463,182063,109482.0,3,,CC BY-SA 3.0,4a48ee19-810a-4154-8f46-49e9ae9cd654,<machine-learning><classification><least-squares><loss-functions><cross-entropy>,,2017-03-11 00:16:56.000
614462,182063,109482.0,1,,CC BY-SA 3.0,4a48ee19-810a-4154-8f46-49e9ae9cd654,Squared Loss for Multilabel Classification,,2017-03-11 00:16:56.000
614596,182063,,25,,,72cb34b9-2b27-4bbf-9ee4-451885c99f4e,,http://twitter.com/StackStats/status/840562442765778945,2017-03-11 13:57:58.027
614949,182191,7483.0,2,,CC BY-SA 3.0,a1d95afc-5186-4714-85aa-b2b3c47f1fa4,"This scheme is called the [Brier loss](https://en.wikipedia.org/wiki/Brier_score). It is a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule), and hence only the optimal classifier is correct, etc. It corresponds, of course, to the $L_2$ distance between the predictive label distribution and the true label distribution (which is a point mass).

Deep learning types these days strongly prefer the cross-entropy loss, which corresponds to the KL divergence $KL( y \| \hat y)$. This will penalize giving very low probabilities to the correct class very harshly, perhaps encouraging a flattening out of predicted probabilities relative to the Brier loss.

There's some relatively unenlightening discussion of the difference in [this question](http://stats.stackexchange.com/q/166958/9964), and in [an /r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/34f23i/question_cross_entropy_vs_euclidean_distance_for/) heavy on opinions and low on supporting evidence. The latter suggests that maybe cross entropy gives stronger gradient signals in a typical deep learning problem, though. [Here](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/) is a blog post which briefly mentions the latter issue.",,2017-03-12 13:49:18.100
615732,182191,7483.0,5,,CC BY-SA 3.0,524adf39-e8d2-4ade-a4e1-6a0a293036ef,"This scheme is called the [Brier loss](https://en.wikipedia.org/wiki/Brier_score). It is a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule), and hence only the optimal classifier is correct, etc. It corresponds, of course, to the $L_2$ distance between the predictive label distribution and the true label distribution (which is a point mass).

Deep learning types these days strongly prefer the cross-entropy loss, which corresponds to the KL divergence $KL( y \| \hat y)$. This will penalize giving very low probabilities to the correct class very harshly, perhaps encouraging a flattening out of predicted probabilities relative to the Brier loss.

Consider a $K$-way classification problem, where your estimate of the probability of the $i$th class is $\hat p_i$.
Let $y$ be the correct label for a given instance $x$, and $B = (\hat p_y(x) - 1)^2$ the Brier loss. Then
$$\nabla B = 2 (\hat p_y(x) - 1) \nabla \hat p_y(x),$$
whereas if $C(x, y, w) = - \log \hat p_y(x)$ is the cross-entropy loss, then
$$\nabla C = - \frac{1}{\hat p_y(x)} \nabla \hat p_y(x).$$
Plotting these:
[![enter image description here][1]][1]

We can thus see that the cross-entropy really emphasizes wrong values, whereas Brier loss scales just linearly with the probability estimate.

<!--
In a typical deep learning setup, we determine $\hat p(x)$ by first taking some transformation $z = f(x)$, then learn $K$ vectors $w_i$, with
$$
\hat p_i(x) = \frac{w_i^\top z}{\sum_{k=1}^K w_k^\top z}
$$
so that
\begin{align}
\frac{\partial \hat p_i(x)}{\partial w_{ia}}
&= \frac{z_a}{\sum_{k=1}^K w_k^\top z}
 - \frac{w_i^\top z \, z_a}{\left( \sum_{k=1}^K w_k^\top z \right)^2}
 = \left( 1 - \hat p_i(x) \right) \frac{z_a}{\sum_{k=1}^K w_k^\top z}
\\
\frac{\partial \hat p_i(x)}{\partial z_a}
&= \frac{w_{ia}}{\sum_{k=1}^K w_k^\top z}
 - \frac{w_i^\top z \, \sum_{k=1}^K w_{ia}}{\left( \sum_{k=1}^K w_k^\top z \right)^2}
 = \left( 1 - \hat p_i(x) \right) \frac{w_{ia}}{\sum_{k=1}^K w_k^\top z}
.\end{align}
-->

  [1]: https://i.stack.imgur.com/fBCay.png",added 713 characters in body,2017-03-13 22:16:41.453
647385,182191,7483.0,5,,CC BY-SA 3.0,670fb3cd-444b-4e06-9810-4a06a71a6e05,"This scheme is called the [Brier loss](https://en.wikipedia.org/wiki/Brier_score). It is a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule), and hence only the optimal classifier is correct, etc. It corresponds, of course, to the $L_2$ distance between the predictive label distribution and the true label distribution (which is a point mass).

Deep learning types these days strongly prefer the cross-entropy loss, which corresponds to the KL divergence $KL( y \| \hat y)$. This will penalize giving very low probabilities to the correct class very harshly, perhaps encouraging a flattening out of predicted probabilities relative to the Brier loss.

Consider a $K$-way classification problem, where your estimate of the probability of the $i$th class is $\hat p_i$.
Let $y$ be the correct label for a given instance $x$, and $B = (\hat p_y(x) - 1)^2$ the Brier loss. Then
$$\nabla B = 2 (\hat p_y(x) - 1) \nabla \hat p_y(x),$$
whereas if $C(x, y, w) = - \log \hat p_y(x)$ is the cross-entropy loss, then
$$\nabla C = - \frac{1}{\hat p_y(x)} \nabla \hat p_y(x).$$
Plotting these:
[![enter image description here][1]][1]

We can thus see that the cross-entropy really emphasizes wrong values, whereas Brier loss scales just linearly with the probability estimate.

<!--
In a typical deep learning setup, we determine $\hat p(x)$ by first taking some transformation $z = f(x)$, then learn $K$ vectors $w_i$, with
$$
\hat p_i(x) = \frac{w_i^\top z}{\sum_{k=1}^K w_k^\top z}
$$
so that
\begin{align}
\frac{\partial \hat p_i(x)}{\partial w_{ia}}
&= \frac{z_a}{\sum_{k=1}^K w_k^\top z}
 - \frac{w_i^\top z \, z_a}{\left( \sum_{k=1}^K w_k^\top z \right)^2}
 = \left( 1 - \hat p_i(x) \right) \frac{z_a}{\sum_{k=1}^K w_k^\top z}
\\
\frac{\partial \hat p_i(x)}{\partial z_a}
&= \frac{w_{ia}}{\sum_{k=1}^K w_k^\top z}
 - \frac{w_i^\top z \, \sum_{k=1}^K w_{ia}}{\left( \sum_{k=1}^K w_k^\top z \right)^2}
 = \left( 1 - \hat p_i(x) \right) \frac{w_{ia}}{\sum_{k=1}^K w_k^\top z}
.\end{align}
-->

Another interesting property: suppose that there are three categories, with the first one being correct. Cross-entropy would value the predictions $(.8, .2, 0)$ and $(.8, .1, .1)$ equally, whereas Brier loss would prefer the second one. I don't know if that's of huge practical importance, but only caring about the true category seems like a reasonable criterion to me, and that leads to cross-entropy being the only proper scoring rule.

  [1]: https://i.stack.imgur.com/fBCay.png",added 442 characters in body,2017-04-24 22:05:55.187
719168,80672,,6,,CC BY-SA 3.0,2f008801-644f-40b0-a58f-b78647d1d0df,<probability><notation>,edited tags,2017-10-18 07:47:19.060
728346,133686,9081.0,6,,CC BY-SA 3.0,fe657bc9-90d6-4d09-a9be-4c6c67ed1ede,<regression><modeling><importance>,edited tags,2017-11-06 13:26:36.810
737966,212670,14355.0,3,,CC BY-SA 3.0,5a482cf6-a12f-4425-8abf-fe79e69c5dd6,<distributions><sampling>,,2017-11-28 11:54:19.820
737965,212670,14355.0,1,,CC BY-SA 3.0,5a482cf6-a12f-4425-8abf-fe79e69c5dd6,Sampling from Skew Normal Distribution,,2017-11-28 11:54:19.820
1080970,309011,232778.0,2,,CC BY-SA 4.0,e1f85394-f5ba-4724-a9bc-cb7ce4d74766,"Is there someone who worked on french text generation using GPT 2 model? Is there a pre-trained GPT 2 model on french text? Can we do finetuning on the Standard GPT 2 and obtain good results on french text? Another question : Adding POS Tagging on GPT can improve the generation or not?

Thanks,",,2020-05-12 21:42:39.827
1080969,309011,232778.0,1,,CC BY-SA 4.0,e1f85394-f5ba-4724-a9bc-cb7ce4d74766,GPT 2 Model on french text,,2020-05-12 21:42:39.827
1080968,309011,232778.0,3,,CC BY-SA 4.0,e1f85394-f5ba-4724-a9bc-cb7ce4d74766,<natural-language>,,2020-05-12 21:42:39.827
1084587,310030,235658.0,1,,CC BY-SA 4.0,153e0954-6551-480e-b12a-133485de69bc,Using pos_weight to improve recall in a multi-class multi-label problem,,2020-05-20 21:23:36.233
1084589,310030,235658.0,3,,CC BY-SA 4.0,153e0954-6551-480e-b12a-133485de69bc,<neural-networks><classification><precision-recall><multi-class><multilabel>,,2020-05-20 21:23:36.233
737964,212670,14355.0,2,,CC BY-SA 3.0,5a482cf6-a12f-4425-8abf-fe79e69c5dd6,"I want to draw samples from a [skew normal distribution][1] as part of a matlab project of mine. I already implemented the CDF and PDF of the distribution, but sampling from it still bothers me.  
Sadly, the [description of this process from the documentation of an R package][2] is riddled with dead links, so I did some reading on the process.

One way of sampling from the distribution would be `inverse transform sampling`, which uses a uniform random variable $U\sim Unif(0,1) $ and involves solving

$F(F^{-1}(u)) = u$

with $F(x)$ being the CDF of the distribution we want to sample from. Since I don't know how to find the inverse of $F(x)$ myself, I did some searching, finding this question asked several times but not answered.

Edit: **another method** to sample from the distribution would be [rejection sampling][3], however for that I need to find a distribution that

 1. I can draw samples from
 2. Has a pdf ""which is at least as high at every point as the distribution we want to sample from, so that the former completely encloses the latter."" (from the rejection sampling wiki article)

I've plotted the skew normal distribution with $\xi=1,\omega=1.5,\alpha=4$ and its truncated version (truncated to [0,2.5] here). 



[![unrestricted and truncated skew-normal distribution][4]][4]

In my application of this, I will always truncate the distribution to a certain interval, so I'd need to find a distribution that 'contains' the SN pdf for (hopefully) all parameters.

Any ideas how I could go about sampling from such truncated skew-normal distributions?


  [1]: https://en.wikipedia.org/wiki/Skew_normal_distribution
  [2]: http://azzalini.stat.unipd.it/SN/faq-r.html
  [3]: https://en.wikipedia.org/wiki/Rejection_sampling
  [4]: https://i.stack.imgur.com/asXAR.png",,2017-11-28 11:54:19.820
738534,212670,0.0,36,,,58873abf-74cf-47dd-b79d-a227a449d8ec,,from https://math.stackexchange.com/questions/2541051/sampling-from-skew-normal-distribution,2017-11-29 16:41:34.650
738543,212670,,6,,CC BY-SA 3.0,5f4cdb44-88f4-4db7-9963-03b20cf6427f,<distributions><normal-distribution><sampling>,edited tags,2017-11-29 17:02:04.883
738587,212847,5179.0,2,,CC BY-SA 3.0,c50b338a-e9ca-4158-84db-c6fbf87f9b27,"[![enter image description here][1]][1]

The [most direct way of simulating a random variable][2] from a distribution with cdf $F$ is to first simulate a Uniform variate $U\sim\mathcal{U}(0,1)$ and second return the inverse cdf transform $F^{-1}(U)$. When the inverse $F^{-1}$ is not available in closed form, a [numerical inversion][3] can be used. Numerical inversion may however be costly, especially in the tails.

One can also use [accept-reject algorithms][2] when the density $f$ is available and dominated by another density $g$, i.e., that there exists a constant $M$ such that$$f(x)<M g(x)$$. In the case of the skew-Normal distribution, the density is$$f(x)=2\varphi(x)\Phi(\alpha x)$$when $\varphi$ and $\Phi$ are the pdf and cdf of the standard Normal distribution, respectively. (Adding a location and a scale parameter does not modify the algorithm, since the outcome simply needs to be rescaled and translated.)

This density seems ideally suited for accept-reject since$$2\varphi(x)\Phi(\alpha x)< 2\varphi(x)$$as $\Phi$ is a cdf. This inequality implies that a first option to run accept-reject is to pick the Normal pdf for $g$ and $M=2$. This works out, as shown by the following picture when $\alpha=-3$:

[![enter image description here][4]][4]

and leads to an algorithm of the kind

    T=1e3 #number of simulations
    x=NULL
    while (length(x)<T){
       y=rnorm(2*T)
       x=c(x,y[runif(2*T)<pnorm(alpha*y)])}
    x=x[1:T]

which returns a reasonable fit of the pdf by the histogram:

[![enter image description here][5]][5]


  [1]: https://i.stack.imgur.com/suc5r.jpg
  [2]: http://www.nrbook.com/devroye/
  [3]: https://link.springer.com/chapter/10.1007/978-3-662-05946-3_7
  [4]: https://i.stack.imgur.com/Gzerq.jpg
  [5]: https://i.stack.imgur.com/7yn7c.jpg",,2017-11-29 18:27:29.380
738588,212670,5179.0,6,,CC BY-SA 3.0,35b1ea71-dd7f-43b1-b632-719568b5656b,<distributions><normal-distribution><simulation><random-generation><accept-reject>,edited tags,2017-11-29 18:28:08.130
738652,212847,5179.0,5,,CC BY-SA 3.0,c2d7bd71-b4c1-4257-894e-481088d4201d,"[![enter image description here][1]][1]

The [most direct way of simulating a random variable][2] from a distribution with cdf $F$ is to first simulate a Uniform variate $U\sim\mathcal{U}(0,1)$ and second return the inverse cdf transform $F^{-1}(U)$. When the inverse $F^{-1}$ is not available in closed form, a [numerical inversion][3] can be used. Numerical inversion may however be costly, especially in the tails.

One can also use [accept-reject algorithms][2] when the density $f$ is available and dominated by another density $g$, i.e., that there exists a constant $M$ such that$$f(x)<M g(x)$$. In the case of the skew-Normal distribution, the density is$$f(x)=2\varphi(x)\Phi(\alpha x)$$when $\varphi$ and $\Phi$ are the pdf and cdf of the standard Normal distribution, respectively. (Adding a location and a scale parameter does not modify the algorithm, since the outcome simply needs to be rescaled and translated.)

This density seems ideally suited for accept-reject since$$2\varphi(x)\Phi(\alpha x)< 2\varphi(x)$$as $\Phi$ is a cdf. This inequality implies that a first option to run accept-reject is to pick the Normal pdf for $g$ and $M=2$. This works out, as shown by the following picture when $\alpha=-3$:

[![enter image description here][4]][4]

and leads to an algorithm of the kind

    T=1e3 #number of simulations
    x=NULL
    while (length(x)<T){
       y=rnorm(2*T)
       x=c(x,y[runif(2*T)<pnorm(alpha*y)])}
    x=x[1:T]

which returns a reasonable fit of the pdf by the histogram:

[![enter image description here][5]][5]

There are however [transforms of standard distributions][6] that result in skew Normal variates: If $X_1,X_2$ are iid $\mathcal{N}(0,1)$, then

 1. $$\dfrac{\alpha|X_1|+X_2}{\sqrt{1+\alpha^2}}$$
 2. $$\dfrac{1+\alpha}{\sqrt{2(1+\alpha^2)}}\max\{X_1,X_2\}+\dfrac{1-\alpha}{\sqrt{2(1+\alpha^2)}}\min\{X_1,X_2\}$$

are skew Normal variates with parameter $\alpha$.

  [1]: https://i.stack.imgur.com/suc5r.jpg
  [2]: http://www.nrbook.com/devroye/
  [3]: https://link.springer.com/chapter/10.1007/978-3-662-05946-3_7
  [4]: https://i.stack.imgur.com/Gzerq.jpg
  [5]: https://i.stack.imgur.com/7yn7c.jpg
  [6]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.588.8&rep=rep1&type=pdf",added 499 characters in body,2017-11-29 20:49:44.807
738690,212847,5179.0,5,,CC BY-SA 3.0,e11f11f5-816e-4f9f-bef4-61aa09353d44,"[![enter image description here][1]][1]

The [most direct way of simulating a random variable][2] from a distribution with cdf $F$ is to first simulate a Uniform variate $U\sim\mathcal{U}(0,1)$ and second return the inverse cdf transform $F^{-1}(U)$. When the inverse $F^{-1}$ is not available in closed form, a [numerical inversion][3] can be used. Numerical inversion may however be costly, especially in the tails.

One can also use [accept-reject algorithms][2] when the density $f$ is available and dominated by another density $g$, i.e., that there exists a constant $M$ such that$$f(x)<M g(x)$$. In the case of the skew-Normal distribution, the density is$$f(x)=2\varphi(x)\Phi(\alpha x)$$when $\varphi$ and $\Phi$ are the pdf and cdf of the standard Normal distribution, respectively. (Adding a location and a scale parameter does not modify the algorithm, since the outcome simply needs to be rescaled and translated.)

This density seems ideally suited for accept-reject since$$2\varphi(x)\Phi(\alpha x)< 2\varphi(x)$$as $\Phi$ is a cdf. This inequality implies that a first option to run accept-reject is to pick the Normal pdf for $g$ and $M=2$. This works out, as shown by the following picture when $\alpha=-3$:

[![enter image description here][4]][4]

and leads to an algorithm of the kind

    T=1e3 #number of simulations
    x=NULL
    while (length(x)<T){
       y=rnorm(2*T)
       x=c(x,y[runif(2*T)<pnorm(alpha*y)])}
    x=x[1:T]

which returns a reasonable fit of the pdf by the histogram:

[![enter image description here][5]][5]

There are however [transforms of standard distributions][6] that result in skew Normal variates: If $X_1,X_2$ are iid $\mathcal{N}(0,1)$, then

 1. $$\dfrac{\alpha|X_1|+X_2}{\sqrt{1+\alpha^2}}$$
 2. $$\dfrac{1+\alpha}{\sqrt{2(1+\alpha^2)}}\max\{X_1,X_2\}+\dfrac{1-\alpha}{\sqrt{2(1+\alpha^2)}}\min\{X_1,X_2\}$$

are skew Normal variates with parameter $\alpha$. (Both representations are identical when considering that $(X_1+X_2,X_1-X_2)/\sqrt{2}$ is an iid $\mathcal{N}(0,1)$ pair.)

  [1]: https://i.stack.imgur.com/suc5r.jpg
  [2]: http://www.nrbook.com/devroye/
  [3]: https://link.springer.com/chapter/10.1007/978-3-662-05946-3_7
  [4]: https://i.stack.imgur.com/Gzerq.jpg
  [5]: https://i.stack.imgur.com/7yn7c.jpg
  [6]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.588.8&rep=rep1&type=pdf",added 123 characters in body,2017-11-29 21:53:19.490
738800,212847,5179.0,5,,CC BY-SA 3.0,2ddbdee6-3bd4-4694-8064-36c77fbe4151,"The [most direct way of simulating a random variable][2] from a distribution with cdf $F$ is to first simulate a Uniform variate $U\sim\mathcal{U}(0,1)$ and second return the inverse cdf transform $F^{-1}(U)$. When the inverse $F^{-1}$ is not available in closed form, a [numerical inversion][3] can be used. Numerical inversion may however be costly, especially in the tails.

One can also use [accept-reject algorithms][2] when the density $f$ is available and dominated by another density $g$, i.e., that there exists a constant $M$ such that$$f(x)<M g(x)$$. In the case of the skew-Normal distribution, the density is$$f(x)=2\varphi(x)\Phi(\alpha x)$$when $\varphi$ and $\Phi$ are the pdf and cdf of the standard Normal distribution, respectively. (Adding a location and a scale parameter does not modify the algorithm, since the outcome simply needs to be rescaled and translated.)

This density seems ideally suited for accept-reject since$$2\varphi(x)\Phi(\alpha x)< 2\varphi(x)$$as $\Phi$ is a cdf. This inequality implies that a first option to run accept-reject is to pick the Normal pdf for $g$ and $M=2$. This works out, as shown by the following picture when $\alpha=-3$:

[![enter image description here][4]][4]

and leads to an algorithm of the kind

    T=1e3 #number of simulations
    x=NULL
    while (length(x)<T){
       y=rnorm(2*T)
       x=c(x,y[runif(2*T)<pnorm(alpha*y)])}
    x=x[1:T]

which returns a reasonable fit of the pdf by the histogram:

[![enter image description here][5]][5]

There are however [transforms of standard distributions][6] that result in skew Normal variates: If $X_1,X_2$ are iid $\mathcal{N}(0,1)$, then

 1. $$\dfrac{\alpha|X_1|+X_2}{\sqrt{1+\alpha^2}}$$
 2. $$\dfrac{1+\alpha}{\sqrt{2(1+\alpha^2)}}\max\{X_1,X_2\}+\dfrac{1-\alpha}{\sqrt{2(1+\alpha^2)}}\min\{X_1,X_2\}$$

are skew Normal variates with parameter $\alpha$. (Both representations are identical when considering that $(X_1+X_2,X_1-X_2)/\sqrt{2}$ is an iid $\mathcal{N}(0,1)$ pair.)

  [1]: https://i.stack.imgur.com/suc5r.jpg
  [2]: http://www.nrbook.com/devroye/
  [3]: https://link.springer.com/chapter/10.1007/978-3-662-05946-3_7
  [4]: https://i.stack.imgur.com/Gzerq.jpg
  [5]: https://i.stack.imgur.com/7yn7c.jpg
  [6]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.588.8&rep=rep1&type=pdf",deleted 43 characters in body,2017-11-30 08:41:44.340
757251,217992,6708.0,2,,CC BY-SA 3.0,d3909e93-84ec-4a9e-bd88-992a7fae8073,"I would need to estimate a time varying autocorrelation of a variable.
I've tried to search for a package in R but I wasn't able to find it.
Do you have any suggestion? (it would be fine also in Matlab or Stata).
Thank you for your support in advance.
",,2018-01-22 12:38:46.880
757253,217992,6708.0,3,,CC BY-SA 3.0,d3909e93-84ec-4a9e-bd88-992a7fae8073,<autocorrelation><time-varying-covariate>,,2018-01-22 12:38:46.880
757252,217992,6708.0,1,,CC BY-SA 3.0,d3909e93-84ec-4a9e-bd88-992a7fae8073,Time varying (auto)correlation estimation,,2018-01-22 12:38:46.880
794347,228594,136479.0,3,,CC BY-SA 3.0,ce3fbdd6-8dd6-4762-a40e-21e9f2242394,<machine-learning><neural-networks><keras>,,2018-04-23 04:10:23.457
794346,228594,136479.0,1,,CC BY-SA 3.0,ce3fbdd6-8dd6-4762-a40e-21e9f2242394,Self Learning Neural network,,2018-04-23 04:10:23.457
794345,228594,136479.0,2,,CC BY-SA 3.0,ce3fbdd6-8dd6-4762-a40e-21e9f2242394,"Is it possible for a neural network to learn based on its own predictions? So what I really want to do is to have my neural network model to learn from its own predictions but I'm not sure what the implications would be. On my own intuition, I think that it might lead the model into a state where it doesn't work well with new input since what it knows too much only about itself. I would want any advice for the matter. 

Also I thought of having the model learn from its own predictions that pass a certain uncertainty. But I also have a hard time determining uncertainty for a neural network model and Im using keras. ",,2018-04-23 04:10:23.457
794352,228594,136479.0,5,,CC BY-SA 3.0,5e31fb2f-551d-4439-85aa-18233da101d6,"Is it possible for a neural network to learn based on its own predictions? So what I really want to do is to have my neural network model to learn from its own predictions but I'm not sure what the implications would be. On my own intuition, I think that it might lead the model into a state where it doesn't work well with new input since what it knows too much only about itself. I would want any advice for the matter. 

Also I thought of having the model learn from its own predictions that pass a certain uncertainty. But I also have a hard time determining uncertainty for a neural network model and Im using keras. 


[![A workflow of what Im trying to ][1]][1]


  [1]: https://i.stack.imgur.com/aMl2q.png",added 97 characters in body,2018-04-23 04:43:06.797
825068,217992,9081.0,5,,CC BY-SA 4.0,93b85945-18ad-43a1-8452-7a983274682d,"I would need to estimate a time varying autocorrelation of a variable.
I've tried to search for a package in R but I wasn't able to find it.
Do you have any suggestion? (it would be fine also in Matlab or Stata).

",deleted 38 characters in body,2018-07-09 09:13:35.087
830976,212670,14355.0,6,,CC BY-SA 4.0,df7f630f-7549-4eca-9ceb-7194062a9e1b,<distributions><simulation><random-generation><skew-normal-distribution><accept-reject>,added skew-normal tag to make it easier to find this question,2018-07-24 15:34:31.500
839357,133686,9081.0,6,,CC BY-SA 4.0,a77d2b6e-7b42-4d50-80d2-ee1d4edfe6dc,<regression><neural-networks><modeling><random-forest><importance>,edited tags,2018-08-16 12:51:27.470
852814,133686,0.0,50,,,b7932c80-21e0-4d33-a901-3b6672d6d443,,,2018-09-21 03:02:23.070
864778,133686,0.0,50,,,41df3a29-f4da-4871-bb0d-619da493f5a2,,,2018-10-21 15:00:21.823
878233,133686,0.0,50,,,ca831ecb-8cc6-4c3c-a039-d39e66bdc506,,,2018-11-25 18:01:50.507
889369,217992,9081.0,5,,CC BY-SA 4.0,b37aff78-32fc-4257-b688-ad7f58ba5d8a,"I would need to estimate a time varying autocorrelation of a variable. Do you have any references or examples? 
I've tried to search for a package in `R` but I wasn't able to find it.
Do you have any suggestion? (it would be fine also in Matlab or Stata).

",added 43 characters in body; edited tags,2018-12-24 13:06:33.410
889370,217992,9081.0,6,,CC BY-SA 4.0,b37aff78-32fc-4257-b688-ad7f58ba5d8a,<time-series><references><autocorrelation><time-varying-covariate>,added 43 characters in body; edited tags,2018-12-24 13:06:33.410
889526,217992,,25,,,6be7b41c-1f8a-4eaa-ae13-c927dd262d3f,,https://twitter.com/StackStats/status/1077489239951097856,2018-12-25 09:00:28.183
889705,133686,0.0,50,,,9b0bbca6-06fb-4bac-b4c5-5786adbe5aee,,,2018-12-26 05:01:25.277
917124,120364,0.0,50,,,d751f8b7-2cbf-41ba-8b75-47709960966a,,,2019-03-06 03:02:10.363
921577,171590,0.0,50,,,2becaa29-59a8-421a-8939-7eb46fd24004,,,2019-03-15 21:01:45.377
921593,171590,9081.0,6,,CC BY-SA 4.0,3106c9d3-08ab-41cf-8993-f0b6519b5f4f,<svm><feature-selection><random-forest><model-selection><importance>,edited tags,2019-03-15 22:25:21.640
930611,266712,196216.0,2,,CC BY-SA 4.0,4a19b758-84fc-4840-be78-a5d459f51ad8,"`ARMA(p,q)` is generally denoted as a special case of `ARIMA(p,d,q)`, when `d = 0`. However, `ARIMA(p,d,q)` is actually `ARMA(p+d,q)` so an ARIMA is actually an ARMA model, right? Then, how come ARIMA model is generalized version of ARMA models? ",,2019-04-07 15:47:15.563
930612,266712,196216.0,1,,CC BY-SA 4.0,4a19b758-84fc-4840-be78-a5d459f51ad8,ARMA vs ARIMA Models,,2019-04-07 15:47:15.563
930613,266712,196216.0,3,,CC BY-SA 4.0,4a19b758-84fc-4840-be78-a5d459f51ad8,<time-series><arima>,,2019-04-07 15:47:15.563
934458,171590,0.0,50,,,8d3af5c2-56f8-4ca4-8236-d26050d9f8cf,,,2019-04-16 00:03:40.710
939441,133686,0.0,50,,,b2f32f97-c39d-4b77-b6b7-6bc1b4104861,,,2019-04-28 07:01:50.743
964695,120364,0.0,50,,,fcfa9123-cd41-4f1f-91e6-24bb1af7c8d7,,,2019-07-06 19:00:35.123
982822,171590,0.0,50,,,9b81d004-3276-41f2-a028-e22f92b776bd,,,2019-08-26 06:01:10.127
984947,133686,0.0,50,,,5052668c-cb9a-4c51-94d1-ee10776782c7,,,2019-09-01 02:02:10.290
1026636,171590,0.0,50,,,b032610b-5c55-4af2-b40e-345db3b2982e,,,2019-12-25 14:01:35.197
1028525,133686,0.0,50,,,b6c15476-cf09-4a06-a837-1b41f7521c30,,,2020-01-01 00:01:23.227
1084588,310030,235658.0,2,,CC BY-SA 4.0,153e0954-6551-480e-b12a-133485de69bc,"I have a multi-label classification problem, and so I’ve been using the Pytorch's [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss). I’d like to optimize my model for a higher F2 score, and so want to bias it to have greater recall (with decent precision too of course). Are there any guidelines for setting the values of `pos_weight` to be able to do this?

",,2020-05-20 21:23:36.233
1111709,316695,20742.0,2,,CC BY-SA 4.0,ec10c343-65a9-4858-a22a-78633d538306,"Suppose I have a softmax distribution produced by a classifier. There are four labels, and so the sum of the softmax probabilities over the four labels will be 1.0.

**I am looking for a measurement of how ""certain"" or ""confident"" the classifier's prediction is.**

For example, suppose for two data instances, the softmax distributions are:

```python
outcome1 = [0.25, 0.25, 0.20, 0.30]
outcome2 = [0.02, 0.94, 0.02, 0.02]
```

It's clear that the classifier is more confident in `outcome2` since there is a large probability mass (0.94) on one of the labels. On the other hand, the classifier is less confident in `outcome1` since the probabilities are fairly equal.

So I'm looking for a way to quantify this degree of ""certainty"" in a classifier's prediction.

One thing I was thinking of was computing the Shannon entropy of each outcome:

```python
from scipy.stats import entropy
print(entropy(outcome1, base=2)) # 1.9854752972273344
print(entropy(outcome2, base=2)) # 0.4225426691977457
```

Can I say that the classifier is ` 1.98 / 0.42 = 4.7` times more confident in `outcome2` versus `outcome1`?

Any help would be appreciated.",,2020-07-15 20:03:03.220
1111710,316695,20742.0,3,,CC BY-SA 4.0,ec10c343-65a9-4858-a22a-78633d538306,<machine-learning><classification>,,2020-07-15 20:03:03.220
1111708,316695,20742.0,1,,CC BY-SA 4.0,ec10c343-65a9-4858-a22a-78633d538306,Measuring the confidence of a softmax classification outcome,,2020-07-15 20:03:03.220
1118830,318704,242632.0,3,,CC BY-SA 4.0,c0dc612b-c9fe-4667-883e-38decab8d613,<p-value><sphericity>,,2020-08-01 05:36:59.207
1118829,318704,242632.0,1,,CC BY-SA 4.0,c0dc612b-c9fe-4667-883e-38decab8d613,Why is the Treatments P-value halved after a Geisser-Greenhouse correction for sphericity,,2020-08-01 05:36:59.207
1118828,318704,242632.0,2,,CC BY-SA 4.0,c0dc612b-c9fe-4667-883e-38decab8d613,"I have noticed that when Graphpad Prism does a sphericity correction using the Geisser-Greenhouse correction, it halves the P value for this source of variation (typically Treatments). For example, in the example I am working on, it calculates an F value of 20.25, with DF(num) = 1.391, and DF(den) = 2.783. The P value for this F-ratio should be 0.046 (which you will get if you use FDIST(20.25, 1.391, 2.783) in Excel. However, Prism gives a P value of 0.023 (half of 0.046). On the other hand, for the Individual or Between Rows source, there is no correction for sphericity, and the F-ratio is 0.25, with DF(num) = 2, and DF(den) = 4. It gives a P-value of 0.7901 for this F-ratio, and as you can verify, it is the 'full"" P-value. Why does it halve it when there is a Geisser-Greenhouse correction for sphericity? I redid the analysis with no correction for sphericity; it now gives an F-ratio of 20.25, with DF(num) = 2, and DF(den) = 4. And the P-value is the ""full"" value of 0.0081.

Excel gives the one-tail P-value when the formula FDIST(x, df_num, df_den) is used. Using the above values, it gives a P-value of 0.046 for FDIST(20.25, 1.391, 2.783). To get the two-tail value, we need to double this P-value, not halve it. That is why I am confused.

Any insights? Thank you.",,2020-08-01 05:36:59.207
1129210,171590,0.0,50,,,db988213-70d7-4b93-ae3c-674433bc5525,,,2020-08-28 07:04:55.597
1130162,133686,0.0,50,,,e0001ea0-fca6-44d6-9bae-5060f379d1d6,,,2020-08-30 18:03:25.387
1154442,328393,250224.0,2,,CC BY-SA 4.0,fb4754e8-82d0-4659-a1eb-fcfb493b6f3b,"Based on this [article][1], it is apparent that MICE works with the following logic:

 1. Fill missing values in every column apart from the column in question with either random or the mean of the given values
 2. Use these feature sets as dependent values to regress given values in the column in question (dropping missing values)
 3. Use the model trained on given values to predict missing values in the column in question
 4. Cycle through the other columns using these given values and the predictions a set number of iterations (minimum of however many columns with missing data plus a certain factor)

It seems that this method for imputing missing values requires at least 2 variables with missing values, as the iteration is done with predictions for each column with missing values to introduce error into the imputation process.

Say you only have one feature with missing values (ie 10 features; 1 feature is missing 10% of rows; 9 features are 100% filled). Will MICE work in this scenario, or will it be reduced to simple/singular imputation? In other words, does MICE require datasets with more than one feature to impute missing values on?


  [1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/",,2020-11-02 18:48:18.500
1154443,328393,250224.0,1,,CC BY-SA 4.0,fb4754e8-82d0-4659-a1eb-fcfb493b6f3b,Will Multiple Imputation (MICE) work on dataset with missing data on only one feature?,,2020-11-02 18:48:18.500
1154444,328393,250224.0,3,,CC BY-SA 4.0,fb4754e8-82d0-4659-a1eb-fcfb493b6f3b,<data-imputation><multiple-imputation><mice>,,2020-11-02 18:48:18.500
1165918,331384,14484.0,1,,CC BY-SA 4.0,4be09b4c-f8f7-4395-9b78-b867d42a96d4,Fader/Hardie BG/NBD model. Interpretation of $a$ and $b$ of the beta distribution drop out process,,2020-12-03 20:28:13.933
1165917,331384,14484.0,3,,CC BY-SA 4.0,4be09b4c-f8f7-4395-9b78-b867d42a96d4,<beta-distribution><customer-lifetime-value>,,2020-12-03 20:28:13.933
1165916,331384,14484.0,2,,CC BY-SA 4.0,4be09b4c-f8f7-4395-9b78-b867d42a96d4,"[On R's CLVTools package documentation][1], there's a sentence referring to the pareto/NBD model

I'm working with the BG/NBD model not the pareto/NBD model. I'd like to understand if I can interpret the parameters of this model in a similar way to the documentation on the pareto/NBD model.

From that link above on the Pareto/NBD model:

> For the standardPareto/NBD model, we get 4 parameters , ,  and
> . where ,  represent the shape and scale parameterof the gamma
> distribution that determines the purchase rate and ,  of the
> attrition rate across individualcustomers. / can be interpreted as
> the mean purchase and / as the mean attrition rate

The BG/NBD model I'm working with also returns 4 parameters:

 - r: shape parameter of the Gamma distribution of the purchase process.
 - alpha: scale parameter of the Gamma distribution of the purchase
   process. 
 - a: shape parameter of the Beta distribution of the dropout
   process. 
 - b: shape parameter of the Beta distribution of the dropout
   process.

The first two of these, , , are the same as for the Pareto/NBD model. So I'm assuming that I can interpret those two in the same way? ""/ can be interpreted as the mean purchase (rate)""?

My question is about the remaining two parameters of the BG/NBD model, a and b. Can I calculate the mean attrition rate in a similar manner by dividing one over the other?



  [1]: https://cran.r-project.org/web/packages/CLVTools/vignettes/CLVTools.pdf",,2020-12-03 20:28:13.933
1174136,333564,,2,user306613,CC BY-SA 4.0,c7d6c233-1e23-48ab-992e-06c68074b904,"[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/sH9up.png
My notes tell me that option b is correct but I think option d is correct. is there any idea why (b) is correct?",,2020-12-27 20:34:34.003
1168831,332131,216426.0,2,,CC BY-SA 4.0,aee2a980-b9a3-4df2-81df-206e2c30e5c3,"I'm running an ordinal logistic regression model with 5 IV and a DV with 4 levels. I'm using the function polr from the MASS package. My data consists of 46 variables and all of the IV's are continuous.

After fitting the model, I run the summary command but I get the following warning: 
In sqrt(diag(vc)) : NaNs produced

R is only able to calculate SE's for 2 IV's. One of these IV's is significant with a p-value of roughly 0,008. 

Now, I have two questions:

1) What could be the reason for having negative values in the covariance matrix and NaNs being produced? Could it be because the number of observations is too low?
2) Considering that my model produces NaNs, does that mean that the calculated p-values for my other two IV's are wrong or can I trust them?

Thanks!",,2020-12-11 12:37:25.730
1168830,332131,216426.0,3,,CC BY-SA 4.0,aee2a980-b9a3-4df2-81df-206e2c30e5c3,<p-value><ordered-logit>,,2020-12-11 12:37:25.730
1168832,332131,216426.0,1,,CC BY-SA 4.0,aee2a980-b9a3-4df2-81df-206e2c30e5c3,Ordinal logistic regression - NaNs produced in R,,2020-12-11 12:37:25.730
1169067,332131,216426.0,5,,CC BY-SA 4.0,4db24ea3-1ae7-4f1c-9ec4-dbcbbaac469c,"I'm running an ordinal logistic regression model with 5 IV and a DV with 4 levels. I'm using the function polr from the MASS package. My data consists of 46 observations and all of the IV's are continuous.

After fitting the model, I run the summary command but I get the following warning: 
In sqrt(diag(vc)) : NaNs produced

R is only able to calculate SE's for 2 IV's. One of these IV's is significant with a p-value of roughly 0,008. 

Now, I have two questions:

1) What could be the reason for having negative values in the covariance matrix and NaNs being produced? Could it be because the number of observations is too low?
2) Considering that my model produces NaNs, does that mean that the calculated p-values for my other two IV's are wrong or can I trust them?

Thanks!",added 3 characters in body,2020-12-12 09:21:49.863
1171282,332808,254239.0,3,,CC BY-SA 4.0,e78b8963-6b66-4352-939f-c919fee709f8,<regression><arima><linear>,,2020-12-18 08:52:10.763
1171284,332808,254239.0,1,,CC BY-SA 4.0,e78b8963-6b66-4352-939f-c919fee709f8,Dominance analysis in linear regressions with ARIMA errors,,2020-12-18 08:52:10.763
1171283,332808,254239.0,2,,CC BY-SA 4.0,e78b8963-6b66-4352-939f-c919fee709f8,"I have a question regarding dominance analysis in linear regressions with ARIMA errors. I am currently working with stress models for the banking industry. In certain cases, we are using dynamic regressions for these purposes using macroeconomic data, such as exogenous variables. We want to assess the relative importance of the macroeconomic variable within the fitted equation. In any case, the macroeconomic variable should have a higher relative importance than the ARIMA errors.

I have been doing some research online, but I could not find anything related to dynamic regressions. On the contrary, for linear regressions I have found a variety of methods: standardized coefficients, Shapley regressions and dispersion importance (Johnson and Lebreton - 2004). I have found a R package ""relaimpo"" that implements some of these methods. 

Are you aware of any methodology developed so far regarding this matter?",,2020-12-18 08:52:10.763
1171387,332808,35995.0,6,,CC BY-SA 4.0,088d4e89-1cfc-4ba7-af9e-e37afab296a7,<regression><arima><importance><dynamic-regression>,edited tags,2020-12-18 14:30:56.083
1173061,333279,219155.0,2,,CC BY-SA 4.0,c3061058-0f57-4b33-b6a5-c8bfe80d93a5,"These might be dumb questions but I am having trouble to wrap my head around of a particular problem. I have a sparse count matrix $G $ that I want to optimize which is $N \times p$. Also, I have correlation matrix $C_{ij}$ which is $N \times N$ that I want matrix $G$ to be optimized for. So, without any constraints I have the following loss function:

$$ loss = \sum_{i}{\sum_{j} (corr(G_i, G_j) - C_{ij}})^{2} $$

where $G_i$ is a vector $1 \times p$ and  $i = j = 1,...,N$.

So, my first question is **how can I solve/implement this particular problem**? 

My second question is related to my lack of knowledge regarding defining optimization problems :)

First of all, as I mentioned above $G$ is a count matrix and it is very sparse and I also want to keep the distribution of each $G_i$ while optimizing, not just randomly change it as it is described above because the initial values are important. So, my question is that **is there a way for me to add these information into the problem design**?

",,2020-12-23 19:04:16.020
1173059,333279,219155.0,1,,CC BY-SA 4.0,c3061058-0f57-4b33-b6a5-c8bfe80d93a5,MSE of correlations,,2020-12-23 19:04:16.020
1173060,333279,219155.0,3,,CC BY-SA 4.0,c3061058-0f57-4b33-b6a5-c8bfe80d93a5,<optimization><loss-functions><constrained-optimization>,,2020-12-23 19:04:16.020
1173070,333279,,5,,CC BY-SA 4.0,8861ffaf-b822-41d9-bd50-6a0144f00b06,"These might be dumb questions but I am having trouble to wrap my head around of a particular problem. I have a sparse count matrix $G $ that I want to optimize which is $N \times p$. Also, I have correlation matrix $C_{ij}$ which is $N \times N$ that I want matrix $G$ to be optimized for. So, without any constraints I have the following loss function:

$$ \text{loss} = \sum_i\sum_j (\operatorname{corr}(G_i, G_j) - C_{ij})^2 $$

where $G_i$ is a vector $1 \times p$ and  $i = j = 1,\ldots,N$.

So, my first question is **how can I solve/implement this particular problem**? 

My second question is related to my lack of knowledge regarding defining optimization problems :)

First of all, as I mentioned above $G$ is a count matrix and it is very sparse and I also want to keep the distribution of each $G_i$ while optimizing, not just randomly change it as it is described above because the initial values are important. So, my question is that **is there a way for me to add these information into the problem design**?

",added 17 characters in body,2020-12-23 19:27:24.750
1173129,333299,254734.0,3,,CC BY-SA 4.0,5530afc1-cc4d-47c5-bcfa-9c3edfd6ad0a,<mixed-model><regression-coefficients>,,2020-12-23 23:47:53.303
1173130,333299,254734.0,1,,CC BY-SA 4.0,5530afc1-cc4d-47c5-bcfa-9c3edfd6ad0a,Comparing coefficient across groups in 3 way linear mixed model,,2020-12-23 23:47:53.303
1173131,333299,254734.0,2,,CC BY-SA 4.0,5530afc1-cc4d-47c5-bcfa-9c3edfd6ad0a,"I wonder how to compare coefficients across group in linear mixed model.



Dependent variable (DV, continuous variable) is changed by time variable (T, continuous, fixed effect).
And hypothesis is that 2 moderator variables (MV1 and MV2, categorical by 0 or 1, independent variable, fixed effects) affect the association of DV with T. 
There is one covariate as continuous variable
Study participants (S) are considered as random effect.

    m <-lmer(DV ~ MV1*MV2*T + (1|S) + CV, REML=T, data=data)

And result was

    Fixed effects:
                                           Estimate Std. Error         df t value Pr(>|t|)    
    (Intercept)                            18.55909    4.74223  300.00698   3.914 0.000113 ***
    MV1                                   -5.92450    1.32025  318.51178  -4.487 1.01e-05 ***
    MV2                                   -0.65531    1.39709  324.53991  -0.469 0.639349    
    T                                      0.14929    0.01166 1518.32489  12.804  < 2e-16 ***
    CV                                    -0.03221    0.06305  298.48239  -0.511 0.609864    
    MV1:MV2                                3.24588    1.89431  321.91658   1.713 0.087584 .  
    MV1:T                                 -0.09325    0.01333 1517.30915  -6.997 3.91e-12 ***
    MV2:T                                  0.09113    0.01780 1517.49347   5.120 3.44e-07 ***
    MV1:MV2:T                             -0.05137    0.02134 1518.24445  -2.407 0.016204 *

I found that interaction term (MV1 X MV2 X T) was significant.
When i divide whole population in 4 groups, like 
MV1 = 0 and MV2 = 0
MV1 = 1 and MV2 = 0
MV1 = 0 and MV2 = 1
MV1 = 1 and MV2 = 1

How can i statistically compare coefficients of T on DV across 4 groups?
I wonder how MV1 moderates the association of MV2 X T with DV
     and how MV2 moderates the association of MV1 X T with DV. 

And i would like to know hot to express the group difference of coefficient in publish.
In previous articles, they used Wald test or Cohen's d. 

",,2020-12-23 23:47:53.303
1173170,333299,254734.0,5,,CC BY-SA 4.0,795f9b57-0afb-4962-a794-41334af1eaa6,"I wonder how to compare coefficients across group in linear mixed model.



Dependent variable (DV, continuous variable) is changed by time variable (T, continuous, fixed effect).
And hypothesis is that 2 moderator variables (MV1 and MV2, categorical by 0 or 1, independent variable, fixed effects) affect the association of DV with T. 
There is one covariate as continuous variable
Study participants (S) are considered as random effect.

    m <-lmer(DV ~ MV1*MV2*T + (1|S) + CV, REML=T, data=data)

And result was

    Fixed effects:
                                           Estimate Std. Error         df t value Pr(>|t|)    
    (Intercept)                            18.55909    4.74223  300.00698   3.914 0.000113 ***
    MV1                                   -5.92450    1.32025  318.51178  -4.487 1.01e-05 ***
    MV2                                   -0.65531    1.39709  324.53991  -0.469 0.639349    
    T                                      0.14929    0.01166 1518.32489  12.804  < 2e-16 ***
    CV                                    -0.03221    0.06305  298.48239  -0.511 0.609864    
    MV1:MV2                                3.24588    1.89431  321.91658   1.713 0.087584 .  
    MV1:T                                 -0.09325    0.01333 1517.30915  -6.997 3.91e-12 ***
    MV2:T                                  0.09113    0.01780 1517.49347   5.120 3.44e-07 ***
    MV1:MV2:T                             -0.05137    0.02134 1518.24445  -2.407 0.016204 *

I found that interaction term (MV1 X MV2 X T) was significant.
When i divide whole population in 4 groups, like   
MV1 = 0 and MV2 = 0  
MV1 = 1 and MV2 = 0  
MV1 = 0 and MV2 = 1  
MV1 = 1 and MV2 = 1  
  
How can i statistically compare coefficients of T on DV across 4 groups?  
I wonder how MV1 moderates the association of MV2 X T with DV  
           and how MV2 moderates the association of MV1 X T with DV. 

And i would like to know hot to express the group difference of coefficient in publish.
In previous articles, they used Wald test or Cohen's d. 

",added 22 characters in body,2020-12-24 04:49:28.727
1173588,333299,254734.0,6,,CC BY-SA 4.0,ae735282-6bc7-4123-b65d-55377be20711,<mixed-model><regression-coefficients><cohens-d><wald-test>,edited tags,2020-12-25 22:21:52.177
1173594,333419,66768.0,1,,CC BY-SA 4.0,7e8b0540-4d57-4d96-9bb4-f3b4c0e94778,How to set up a DL classification model so that it selects from an ever changing menu,,2020-12-25 22:58:48.770
1173592,333419,66768.0,2,,CC BY-SA 4.0,7e8b0540-4d57-4d96-9bb4-f3b4c0e94778,"So, I have a deep learning time series forecasting model. At every timestep, there are 10 available classes, menu choice 0-9 (e.g. 10 different TV channels). I have a softmax output layer and CE loss function.

My problem is that I also have approx. 10.000 entities (let's say that these are programs on the 10 tv channels) represented by embeddings. Some entities are popular (the model should learn that these entities have a high probability of being selected) and some are less popular (low probability of being selected). At every timestep, there is one entity at each class, totally 10 entities available. The same entities may jump from one class to another and could even appear simultaneously at different classes. (E.g. a popular program is shown on channel 2 and a less popular program on channel 3, so the model should predict a high probability for category 2 and a low probability for category 3. At the next time step, the entities could swap and so should the classification probabilities.)

At every timestep trained (or predicted), I can insert the 10 embeddings at the bottom of the network. E.g. if each embedding has 16 dimensions, I will insert 160 values for each timestep. But I don't think that my model is able to understand where one embedding starts and another stops, much less to understand that the first 16 values represents the first category in my softmax layer, the next 16 values, represent the second category in my softmax layer, etc.

At first, this seemed basic to me, but I'm struggling with finding the right approach. I've been looking into one shot learning and siamese networks etc. but I haven't yet found the one simple, ingenious recipe that I hope exists for this kind of a problem. All good ideas appreciated!
",,2020-12-25 22:58:48.770
1173593,333419,66768.0,3,,CC BY-SA 4.0,7e8b0540-4d57-4d96-9bb4-f3b4c0e94778,<machine-learning><time-series><neural-networks><classification><siamese>,,2020-12-25 22:58:48.770
1173684,333419,66768.0,5,,CC BY-SA 4.0,3a63d5d9-941f-45c7-97f9-d9149c122b32,"So, I have a deep learning time series forecasting model. At every timestep, there are 10 available classes, menu choice 0-9 (e.g. 10 different TV channels). I have a softmax output layer and CE loss function.

My problem is that I also have approx. 10.000 entities (let's say that these are programs on the 10 tv channels) represented by embeddings. Some entities are popular (the model should learn that these entities have a high probability of being selected) and some are less popular (low probability of being selected). At every timestep, there is one entity at each class, totally 10 entities available. The same entities may jump from one class to another and could even appear simultaneously at different classes. (E.g. a popular program is shown on channel 2 and a less popular program on channel 3, so the model should predict a high probability for category 2 and a low probability for category 3. At the next time step, the entities could swap and so should the classification probabilities.)

At every timestep trained (or predicted), I can insert the 10 embeddings at the bottom of the network. E.g. if each embedding has 16 dimensions, I will insert 160 values for each timestep. But I don't think that my model is able to understand where one embedding starts and another stops, much less to understand that the first 16 values represents the first category in my softmax layer, the next 16 values, represent the second category in my softmax layer, etc.

At first, this seemed basic to me, but I'm struggling with finding the right approach. I've been looking into one shot learning and siamese networks etc. but I haven't yet found the one simple, ingenious recipe that I hope exists for this kind of a problem. All good ideas appreciated!

----

The following was added after tchainzzz's comment about meta-learning:

Thanks for an interesting read - a few known and a lot of exotic models. I think, however, it helped me see meta learning (including the approaches that I mentioned) are not the right path for me. It seems that the main aim for meta-learning is to classify an entity (e.g. a picture) based on a limted training set (very few similar pictures). My situation is a bit different in both aspects: I'm not trying to classify the entities. (I already have the label for each picture.) And i don't have a limited training set.

Let me try to make my case a bit cleaner an more precise: Let's say we have 10,000 pet pictures and 10,000 kids. (We have labels and we can make random embeddings for each picture and for each kid.) Each kid is presented with 10 pet pictures at a time, and they always have to pick one picture that they like best. During inference, we're going to give probabilities on which picture (from 0 to 9) that they will pick.

* So, why not use few shot learning? Because in our case, we can train each kid and each picture hundreds of times.

* What not use siamese/metric learning? I don't think it will work here, because it sasumes that there is one ideal pet that each kid would select, and we need to figure out which picture is more like that ideal pet. But we don't have an ideal pet for each kid.

* What not use some kind of ranking solution? (We could probably create a system, like elo chess ranking. Every time a kid selects a picture, that picture would get a higher ranking, particularily for that kid, and more so if the competing picture already have a high ranking.) Because it's not a neural network classification architechture. I can add such a ranking as a feature, but the queston is how to create an NN model so that it ""understands"" that the classification should happen from a menu of 10 available dishes.

Actually, when I mentioned siamese network, I probably didn't use the term correctly. My intention was not to refer to the metric learning aspect of siamese netowork, but the weights sharing aspect: I was thinking that a possible solution to my problem was that I cold insert the embedding for the kid next to the embedding of picture i (from 0 to 9) into 10 siamese twin networks sharing the same weights. Each twin would have one output mapped to 10 classes in softmax layer. I have tried this quickly, without much luck. But so far, this is my best idea and i'm continuing to work in this direction.",added 2543 characters in body,2020-12-26 11:57:34.563
1173688,333419,66768.0,5,,CC BY-SA 4.0,9639c845-30df-4103-a302-689f9a929d17,"So, I have a deep learning time series forecasting model. At every timestep, there are 10 available classes, menu choice 0-9 (e.g. 10 different TV channels). I have a softmax output layer and CE loss function.

My problem is that I also have approx. 10.000 entities (let's say that these are programs on the 10 tv channels) represented by embeddings. Some entities are popular (the model should learn that these entities have a high probability of being selected) and some are less popular (low probability of being selected). At every timestep, there is one entity at each class, totally 10 entities available. The same entities may jump from one class to another and could even appear simultaneously at different classes. (E.g. a popular program is shown on channel 2 and a less popular program on channel 3, so the model should predict a high probability for category 2 and a low probability for category 3. At the next time step, the entities could swap and so should the classification probabilities.)

At every timestep trained (or predicted), I can insert the 10 embeddings at the bottom of the network. E.g. if each embedding has 16 dimensions, I will insert 160 values for each timestep. But I don't think that my model is able to understand where one embedding starts and another stops, much less to understand that the first 16 values represents the first category in my softmax layer, the next 16 values, represent the second category in my softmax layer, etc.

At first, this seemed basic to me, but I'm struggling with finding the right approach. I've been looking into one shot learning and siamese networks etc. but I haven't yet found the one simple, ingenious recipe that I hope exists for this kind of a problem. All good ideas appreciated!

----

The following was added after tchainzzz's comment about meta-learning:

Thanks for an interesting read - a few known and a lot of exotic models. I think, however, it helped me see that meta learning (including the approaches that I mentioned) is not the right path for me. It seems that the main aim for meta-learning is to classify an entity (e.g. a picture) based on a limited training set (few similar pictures). My situation is a bit different in both aspects: I'm not trying to classify the entities. (I already have the label for each picture.) And i don't have a limited training set.

Let me try to make my case a bit cleaner an more precise: Let's say we have 10,000 pet pictures and 10,000 kids. (We have labels and we can make random embeddings for each picture and for each kid.) Each kid is presented with 10 pet pictures at a time, and they always have to pick one picture that they like best. During inference, we're going to give probabilities on which picture (from 0 to 9) that they will pick.

* So, why not use few shot learning? Because in our case, we can train each kid and each picture hundreds of times.

* What not use siamese/metric learning? I don't think it will work here, because it assumes that there is one ideal pet that each kid would select, and we need to figure out which picture is more like that ideal pet. But we don't have an ideal pet for each kid.

* Why not use some kind of ranking solution? (We could probably create a system, like elo chess ranking. Every time a kid selects a picture, that picture would get a higher ranking, particularly for that kid, and more so if the competing picture already have a high ranking.) Because that's not a neural network classification architecture. I can add such a ranking as a feature, but the question is how to create an NN model so that it ""understands"" that the classification should happen from a menu of 10 available dishes.

Actually, when I mentioned siamese network, I probably didn't use the term correctly. My intention was not to refer to the metric learning aspect of siamese networks, but the weights sharing aspect: I was thinking that a possible solution to my problem was that I could insert the embedding for the kid next to the embedding of picture i (from 0 to 9) into 10 siamese twin networks (0 to 9) sharing the same weights. Each twin would have one output mapped to 10 classes in softmax layer. I have tried this quickly, without much luck. But so far, this is my best idea and i'm continuing to work in this direction.",added 2543 characters in body,2020-12-26 12:02:42.663
1173735,333419,66768.0,5,,CC BY-SA 4.0,dd0dd61b-3c84-4959-baf0-2c693dfa0c65,"*The question is edited for clarity after tchainzzz's comments about meta-learning.*

Let's say we have 10,000 pet pictures and 10,000 kids. Each kid is presented with 10 randomly picked pet pictures at a time. Each time, they have to pick the one picture that they like best. Our goal, during inference, is to predict probabilities on which picture (from 0 to 9) the (same) kids will pick. My struggle is how to construct a NN to make this classification.

Paths I've been thinking about or tried:

* I have created embeddings for kids and pictures using a (Netflix competition winner style factorization method.) The embeddings are pretty good, I think: Visualizing the picture embeddings in a projector, similar pets are grouped together.

* The first thing I tried was to concatenate the embeddings of the 10 pictures and feed this together with the embedding of the kid. The output layer a softmax and CE loss. But it doesn't work - I guess it's to difficult for the model to ""understand"" where one picture embedding starts and another stops, and to relate each of the embeddings to the 10 categories in the output layer.

* tchainzzz pointed me in the direction of meta-learning, including few-shot learning (before I had clarified my case). But these methods are mainly intended for classifying the entities (is the pet a dog or a cat?) and they are intended for limited training sets. In our case, we're not classifying the pictures (we already know which ones are cats and which ones are dogs) and we have ample training data.

* Why not use metric learning with siamese networks? I don't think it will work here, because this method assumes that there is one ideal pet that each kid would select, and we just need to figure out which picture is more like that ideal pet. But we don't have an ideal pet for each kid, only the previously performed selections.

* Why not use some kind of ranking solution? (We could probably create a system, like elo chess ranking. Every time a kid selects a picture, that picture would get a higher ranking, particularly for that kid, and more so if the competing picture already have a high ranking.) Because that's not a neural network classification architecture. I can add such a ranking as a feature, but the question is how to create an NN model so that it ""understands"" that the classification should happen from a menu of 10 available dishes.

* There is, however, elements from 'siamese networks' that I've been thinking about. Not the metric part of siamese architechture, but the 'shared weights' part: A possible solution to my problem could be that I insert the embedding for the kid next to the embedding of one picture (i from 0 to 9) into 10 siamese twin networks (i from 0 to 9) sharing the same weights. Each twin would have one output mapped to 10 classes in a softmax layer. (The softmax layer is on the outside of the siamese part of the network.) I have tried this quickly, without much luck. But so far, this is my best idea and i'm continuing to work in this direction.

Any further advise or ideas would be welcome!",deleted 1219 characters in body,2020-12-26 15:45:42.470
1173774,333419,66768.0,5,,CC BY-SA 4.0,9f40298f-a84f-417a-aa0a-2c79b5347877,"*The question is edited for clarity after tchainzzz's comments about meta-learning.*

Let's say we have 10,000 pet pictures and 10,000 kids. Each kid is presented with 10 randomly picked pet pictures at a time. Each time, they have to pick the one picture that they like best. Our goal, during inference, is to predict probabilities on which picture (from 0 to 9) the (same) kids will pick. My struggle is how to construct a NN to make this classification.

Paths I've been thinking about or tried:

* I have created embeddings for kids and pictures using a (Netflix competition winner style) factorization method. The embeddings are pretty good: Visualizing the picture embeddings in a projector, similar pets are grouped together.

* The first thing I tried was to concatenate the embeddings of the 10 pictures and feed this together with the embedding of the kid. The output layer a softmax and CE loss. But it doesn't work - I guess it's to difficult for the model to ""understand"" where one picture embedding starts and another stops, and to relate each of the embeddings to the 10 categories in the output layer.

* tchainzzz pointed me in the direction of meta-learning, including few-shot learning (before I had clarified my case). But these methods are mainly intended for classifying the entities (is the pet a dog or a cat?) and they are intended for limited training sets. In our case, we're not classifying the pictures (we already know which ones are cats and which ones are dogs) and we have ample training data.

* Why not use metric learning with siamese networks? I don't think it will work here, because this method assumes that there is one ideal pet that each kid would select, and we just need to figure out which picture is more like that ideal pet. But we don't have an ideal pet for each kid, only the previously performed selections.

* Why not use some kind of ranking solution? (We could probably create a system, like elo chess ranking. Every time a kid selects a picture, that picture would get a higher ranking, particularly for that kid, and more so if the competing picture already have a high ranking.) Because that's not a neural network classification architecture. I can add such a ranking as a feature, but the question is how to create an NN model so that it ""understands"" that the classification should happen from a menu of 10 available dishes.

* There is, however, elements from 'siamese networks' that I've been thinking about. Not the metric part of siamese architechture, but the 'shared weights' part: A possible solution to my problem could be that I insert the embedding for the kid next to the embedding of one picture (i from 0 to 9) into 10 siamese twin networks (i from 0 to 9) sharing the same weights. Each twin would have one output mapped to 10 classes in a softmax layer. (The softmax layer is on the outside of the siamese part of the network.) I have tried this quickly, without much luck. But so far, this is my best idea and i'm continuing to work in this direction.

Any further advise or ideas would be welcome!",deleted 9 characters in body,2020-12-26 18:07:39.877
1173814,333419,,25,,,ddc55b11-b4f3-4b9b-803e-abe9f101d91e,,https://twitter.com/StackStats/status/1342938251573792770,2020-12-26 21:00:04.387
1174030,333536,254958.0,1,,CC BY-SA 4.0,0c30dc9f-7d7d-4775-8585-078a50d177d9,Forecasting in R without auto.arima(),,2020-12-27 14:53:36.730
1174031,333536,254958.0,3,,CC BY-SA 4.0,0c30dc9f-7d7d-4775-8585-078a50d177d9,<r><forecasting><arima>,,2020-12-27 14:53:36.730
1174029,333536,254958.0,2,,CC BY-SA 4.0,0c30dc9f-7d7d-4775-8585-078a50d177d9,"I am trying to forecast data regarding vehicle registrations year-wise using auto.arima() in R. However, one of my variables (which is data for 3-wheeler registrations) gives me the same forecast: I used auto.arima for it too but the process generated was an ARIMA(0,0,0) process due to which the forecast values were the same throughout. For another similar variable, I got an ARIMA(0,1,0) process and my point forecasts were the same. So my question is, is there any other method of forecasting single variables without using the auto.arima() function? Your responses and help will be much appreciated, thanking you all in anticipation!! [![enter image description here][1]][1] [![enter image description here][2]][2]


  [1]: https://i.stack.imgur.com/Jll7m.png
  [2]: https://i.stack.imgur.com/Ss2GX.png",,2020-12-27 14:53:36.730
1174042,333537,170017.0,1,,CC BY-SA 4.0,75e46002-651e-4350-bb66-4cd1bbd872f9,Probability that Secret Santa arrangement will result in perfect pairings for couples,,2020-12-27 15:33:22.287
1174041,333537,170017.0,2,,CC BY-SA 4.0,75e46002-651e-4350-bb66-4cd1bbd872f9,"4 couples, 8 people total, participate in a Secret Santa gift exchange. Call the people A, B, C, D, E, F, G, H. Assume A+B are a married couple.  Likewise, assume C+D, E+F, and G+H are all couples. All 8 people put their names on a piece of paper and then the people randomly draw names from the bowl.  If a person draws their own name, they hold onto the paper with their name while drawing a second piece of paper, then they replace the piece of paper with their own name. This ensures that nobody draws their own name until the last person. When the last person draws, if the name in the bowl is their own, then that person switches names with the second last person to draw.  This is guaranteed to be valid because the second to last person did not choose the last person.  Thus, after switching the last person will have a valid name and the second to last person will have the last person's name.  
a) Assume they draw in this order: A, B, C, D, E, F, G, H.  What is the probability that everyone draws their own partner's name?  
b) Assume they draw in a random order.  What is the probability that everyone draws their own partner's name?",,2020-12-27 15:33:22.287
1174043,333537,170017.0,3,,CC BY-SA 4.0,75e46002-651e-4350-bb66-4cd1bbd872f9,<combinatorics><permutation>,,2020-12-27 15:33:22.287
1174135,333564,,3,user306613,CC BY-SA 4.0,c7d6c233-1e23-48ab-992e-06c68074b904,<machine-learning><reinforcement-learning><supervised-learning><pattern-recognition><semi-supervised-learning>,,2020-12-27 20:34:34.003
1174137,333564,,1,user306613,CC BY-SA 4.0,c7d6c233-1e23-48ab-992e-06c68074b904,Q-function in Q-Learning,,2020-12-27 20:34:34.003
1174258,333564,,5,user306613,CC BY-SA 4.0,1a2e6561-6ddc-40cd-8847-ec944518445e,"I ran into solved old-exam question as follows:
[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/sH9up.png
My notes tell me that option b is correct but I think option d is correct. is there any idea why (b) is correct?",added 49 characters in body,2020-12-28 09:23:56.373
1174282,333601,254990.0,1,,CC BY-SA 4.0,a2a76f60-8aee-41d8-ad59-58ac570b3b30,Regresor sobreestima valores bajos y subestima valores altos,,2020-12-28 12:33:27.493
1174283,333601,254990.0,2,,CC BY-SA 4.0,a2a76f60-8aee-41d8-ad59-58ac570b3b30,"Tengo un regresor (random forest)  para el cual ya preprocesé los datos y ajusté hiperparámetros. Para ver como se está como comportando, hice un gráfico de valores predichos vs valores reales. Acá, me di cuenta de que el regresor estaba consistentenmente sobreestimando valores bajos y subestimando valores altos:
[![enter image description here][1]][1]

¿Hay alguna forma de ajustar este sesgo de mi modelo? Intenté agregandole un sesgo de forma manual y las métricas efectivamente mejoraron un poco, pero no creo que sea la forma más correcta.

Saludos!


  [1]: https://i.stack.imgur.com/2fJxW.png",,2020-12-28 12:33:27.493
1174281,333601,254990.0,3,,CC BY-SA 4.0,a2a76f60-8aee-41d8-ad59-58ac570b3b30,<regression><machine-learning><random-forest><bias>,,2020-12-28 12:33:27.493
1174313,333601,,10,,,fb09cb04-486a-4e8e-9721-6e5ee50dd29c,"{""Voters"":[{""Id"":919,""DisplayName"":""whuber""}]}",102,2020-12-28 13:57:51.123
1174318,333613,211479.0,3,,CC BY-SA 4.0,c2db8241-7e87-4d9a-ab67-c9582024af26,<machine-learning><reinforcement-learning><q-learning>,,2020-12-28 14:07:41.880
1174317,333613,211479.0,1,,CC BY-SA 4.0,c2db8241-7e87-4d9a-ab67-c9582024af26,Why does the PyTorch tutorial on DQN define state as a difference?,,2020-12-28 14:07:41.880
1174316,333613,211479.0,2,,CC BY-SA 4.0,c2db8241-7e87-4d9a-ab67-c9582024af26,"

I'm a master's student in EECS working my way towards understanding how DQN [0] works.

I'm working towards solving the CartPole-v0 task in as few iterations as possible.

First of all I implemented a basic Q-learning algorithm which took forever to converge, then I added decaying learning rate and experimentation proportion which made a whole lot of difference. I'm not interpreting state from the image yet, I just take an observation and discretize it to simplify things - one complication at a time.

I'm now trying to add experience replay but keep the Q-matrix approach - I will substitute it with a function-approximation ANN later. I'm wondering why the seventh code snippet in the PyTorch tutorial on DQN, the second under section ""training loop"" [1] is representing state as a difference between two screens. Right now I'm not doing this, and my replay memory implementation is not helping learning - quite the opposite. Of course, a good RL algorithm is independent from the representation of state and the error is probably somewhere else - especially since my Q-learning algorithm does learn if I avoid using replay memory - but this tickled my curiosity. I skimmed over the DQN paper but did not find any references to such a representation. I'll admit that I haven't read it in detail yet because I don't want to get more confused, so I might have missed it.

Is there a specific reason for this kind of representation? Does it only make sense in the context of translating an image to state?

Thanks in advance!

[0] https://doi.org/10.1038%2Fnature14236

[1] https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
",,2020-12-28 14:07:41.880
1174372,333630,188602.0,2,,CC BY-SA 4.0,e531a3a4-5106-4393-947b-1db963236d7f,"Suppose we have $k_1$ exogenous variables $X_1$, $k_2$ endogenous variables $X_2$ and $k_2$ instruments $Z$ and the model $Y=X_1\beta_1+X_2\beta_2+e$. Let the 2SLS estimates be $(\hat\beta_1,\hat\beta_2). $If we directly run a regression with the instruments instead $Y=X_1\alpha_1+Z\alpha_2+e$, then the claim is that $\hat\alpha_1=\hat\beta_1$ i.e. the estimated coefficient on the exogenous variables are the same.


I'm having trouble formally proving the result as well as understanding the intuition. I tried applying the Frisch–Waugh–Lovell theorem so that $\hat\alpha_1=(X_1'M_ZX_1)'(X_1'M_ZY)$ where $M_Z=(I-Z(Z'Z)^{-1}Z')$ and then tried showing that $\hat\beta_1$ will be equal to that. However, applying the FWL theorem in the 2SLS context, $\hat\beta_1=(X_1'M_{\hat{X_2}}X_1)'(X_1'M_{\hat{X_2}}Y)$. This would require $M_{\hat{X_2}}=M_Z$. However, I can't see why the two should be equal. Furthermore, that would imply that the residuals from regressing $X_1$ on $Z$ and $X_1$ on $\hat{X}_2$ would be the same which doesn't seem to make sense either since the predicted value $\hat{X}_2$ uses both $X_1$ and $Z$. What am I missing here?

",,2020-12-28 17:54:38.210
1174371,333630,188602.0,1,,CC BY-SA 4.0,e531a3a4-5106-4393-947b-1db963236d7f,Slope Coefficient on Exogenous Variables in 2SLS and Directly Regressing the instrument,,2020-12-28 17:54:38.210
1174373,333630,188602.0,3,,CC BY-SA 4.0,e531a3a4-5106-4393-947b-1db963236d7f,<econometrics><instrumental-variables>,,2020-12-28 17:54:38.210
1174387,333634,218101.0,2,,CC BY-SA 4.0,9ec67663-547d-496f-932b-90533f07a1c0,"I am working on a dataset with 20 independent variables and 41188 instances. The task is a binary classification where the target variable has 36548 number of 0's and 4640 of 1's. I have used logistic regression model with 10 folds of cross validation. Since the target variable is unbalanced, I decided to resample data. I made the model 3 times: first without resampling data, then resampling data once with under-sampling technique and once with SMOTE technique. Following are the reports gained:

 **- Without resampling:**
  [![enter image description here][1]][1]

------------------------------------------

 **- With Under-sampling technique**
[![enter image description here][2]][2]

------------------------------------------

 **- With SMOTE technique:**
[![enter image description here][3]][3]

Usually with unbalanced data, *accuracy* alone is not sufficient to evaluate the performance of the classifier and thus *precision, recall* and *ROC* values should be taken into accounts as well. The first model made without any resampling techniques, delivers higher accuracy, precision and recall values than the the others while its ROC value is slightly less than the model with SMOTE resampling technique. My question is which model is more trustable? and why the accuracy, precision and recall values were decreased after resampling the data? 


  [1]: https://i.stack.imgur.com/iZOVQ.png
  [2]: https://i.stack.imgur.com/juad7.png
  [3]: https://i.stack.imgur.com/KEcJO.png",,2020-12-28 19:00:25.707
1174386,333634,218101.0,3,,CC BY-SA 4.0,9ec67663-547d-496f-932b-90533f07a1c0,<logistic>,,2020-12-28 19:00:25.707
1174388,333634,218101.0,1,,CC BY-SA 4.0,9ec67663-547d-496f-932b-90533f07a1c0,A question about a logistic regression classifier performance (with and without resampling),,2020-12-28 19:00:25.707
1174390,333634,199619.0,6,,CC BY-SA 4.0,b1e7ed14-865a-4016-a32c-c2bd9fd6a6bd,<logistic><classification><unbalanced-classes><scoring-rules><smote>,edited tags,2020-12-28 19:09:24.640
1174391,333279,219155.0,33,,,71d1c9cb-3ee5-4b8c-974c-9e718b701a1f,,8935,2020-12-28 19:14:09.397
1174412,333640,66768.0,2,,CC BY-SA 4.0,ebbeb19f-eedf-4374-a861-54338fe129bf,"I have N events (i from 1 to N), each with an estimated probability of success, p(i).

If all my events were independent I'd be able to calculate the probability of at least one success as (1 - product of (1 - p(i))). But some events are not independent, such as {3,4,5}, {4,5,6} and {5,6,7}. 

Within each of these groups, I know all conditional probabilities, such as p(3|4) =70%, p(3|5) = 30%, etc. 

Now to the big question: **Does it exist a formula to calculate the overall probability of at least one success?**",,2020-12-28 20:00:35.957
1174411,333640,66768.0,3,,CC BY-SA 4.0,ebbeb19f-eedf-4374-a861-54338fe129bf,<probability><conditional-probability>,,2020-12-28 20:00:35.957
1174410,333640,66768.0,1,,CC BY-SA 4.0,ebbeb19f-eedf-4374-a861-54338fe129bf,Probability of at least one success in a long string of connected events,,2020-12-28 20:00:35.957
1174413,333640,66768.0,5,,CC BY-SA 4.0,1b293f72-fd04-4db0-b4a2-6d162d87ba93,"I have N events (i from 1 to N), each with an estimated probability of success, p(i).

If all my events were independent I'd be able to calculate the probability of at least one success as (1 - product of (1 - p(i))). But some events are not independent, such as {3,4,5}, {4,5,6} and {5,6,7}. 

Within each of these groups, I know all conditional probabilities, such as p(3|4) =70%, p(3|5) = 30%, etc. 

Now to the big question: **Does it exist a formula to calculate the overall probability of at least one success?** (A ""closed"" formula would be nice. But efficiency in calculation is more important!)",added 85 characters in body,2020-12-28 20:06:42.540
1174418,333419,66768.0,33,,,346ca559-ce9e-4358-8659-b6bcb40f498c,,8936,2020-12-28 20:08:43.450
1174420,333640,66768.0,6,,CC BY-SA 4.0,e5020085-6e5e-47fc-bd10-1d528dee4e58,<probability><bayesian><conditional-probability>,edited tags,2020-12-28 20:14:49.180
1174434,333634,218101.0,5,,CC BY-SA 4.0,0eaf4aa9-2a50-412f-af8c-67355f197976,"I am working on a dataset with 20 independent variables and 41188 instances. The task is a binary classification where the target variable has 36548 number of *no*'s and 4640 of *yes*'s. I have used logistic regression model with 10 folds of cross validation. Since the target variable is unbalanced, I decided to resample data. I made the model 3 times: first without resampling data, then resampling data once with under-sampling technique and once with SMOTE technique. Following are the reports gained:

 **- Without resampling:**
  [![enter image description here][1]][1]

------------------------------------------

 **- With Under-sampling technique**
[![enter image description here][2]][2]

------------------------------------------

 **- With SMOTE technique:**
[![enter image description here][3]][3]

Usually with unbalanced data, *accuracy* alone is not sufficient to evaluate the performance of the classifier and thus *precision, recall* and *ROC* values should be taken into accounts as well. The first model made without any resampling techniques, delivers higher accuracy, precision and recall values than the the others while its ROC value is slightly less than the model with SMOTE resampling technique. My question is which model is more trustable? and why the accuracy, precision and recall values were decreased after resampling the data? 


  [1]: https://i.stack.imgur.com/iZOVQ.png
  [2]: https://i.stack.imgur.com/juad7.png
  [3]: https://i.stack.imgur.com/KEcJO.png",added 3 characters in body,2020-12-28 21:41:13.117
1174439,333634,218101.0,5,,CC BY-SA 4.0,cf2be40d-9496-4fc9-828e-5e79304bc577,"I am working on a dataset with 20 independent variables and 41188 instances. The task is a binary classification where the target variable has 36548 number of *no*'s and 4640 of *yes*'s. I have used logistic regression model with 10 folds of cross validation. Since the target variable is unbalanced, I decided to resample data. I made the model 3 times: first without resampling data, then resampling data once with under-sampling technique and once with SMOTE technique. Following are the reports gained:

 **- Without resampling:**
  [![enter image description here][1]][1]

------------------------------------------

 **- With Under-sampling technique**
[![enter image description here][2]][2]

------------------------------------------

 **- With SMOTE technique:**
[![enter image description here][3]][3]

Usually with unbalanced data, *accuracy* alone is not sufficient to evaluate the performance of the classifier and thus *precision, recall* and *ROC* values should be taken into accounts as well. The first model made without any resampling techniques, delivers weighted average higher accuracy, precision and recall values than the the others while its ROC value is slightly less than the model with SMOTE resampling technique. Moreover, in the first model the recall value of class *yes* (0.423) is much lower than the recall value of class *no* (0.973).

My question is which model is more trustable? and why the accuracy, precision and recall values were decreased after resampling the data? 


  [1]: https://i.stack.imgur.com/iZOVQ.png
  [2]: https://i.stack.imgur.com/juad7.png
  [3]: https://i.stack.imgur.com/KEcJO.png",added 148 characters in body,2020-12-28 21:59:01.287
1174445,333647,129041.0,2,,CC BY-SA 4.0,24a759dd-8d72-439b-a4f7-af78dff66e12,"Consider the two series in the chart below: $walkA$ and $walkB$. 

They are based on the same steps, although the steps come in a different order. 

Indeed, $stepsA$ and $stepsB$ have identical sample mean $\hat \mu=0.5$ and sample std dev $\hat \sigma=3$. 

However, from a visual inspection, I would intuitively say that $walkA$ is more likely to be a random walk than $walkB$. 

Is it there a mathematical method to make such statements?

(Apologies for the imprecise language, I am exactly looking for a more formal approach to such questions)

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/kXgJk.png",,2020-12-28 22:08:08.933
1174443,333647,129041.0,1,,CC BY-SA 4.0,24a759dd-8d72-439b-a4f7-af78dff66e12,Which one is more likely to be random walk?,,2020-12-28 22:08:08.933
1174444,333647,129041.0,3,,CC BY-SA 4.0,24a759dd-8d72-439b-a4f7-af78dff66e12,<time-series><random-walk>,,2020-12-28 22:08:08.933
1174446,333647,129041.0,5,,CC BY-SA 4.0,e939eac7-4138-49e2-9391-dd041694d97e,"Consider the two series in the chart below: $walkA$ and $walkB$. 

They are based on the same steps, although the steps come in a different order. 

Indeed, $stepsA$ and $stepsB$ have identical sample mean $\hat \mu=0.5$ and sample std dev $\hat \sigma=3$. 

However, from a visual inspection, I would intuitively say that $walkA$ is more likely to be a random walk than $walkB$. 

Is it there a mathematical method to make such claims?

[![enter image description here][1]][1]


  [1]: https://i.stack.imgur.com/kXgJk.png",deleted 109 characters in body,2020-12-28 22:14:26.090
1174448,333419,66768.0,5,,CC BY-SA 4.0,83f37a42-62c9-4cce-a5f4-ed61adb86120,"*The question is edited for clarity after tchainzzz's comments about meta-learning.*

Let's say we have 10,000 pet pictures and 10,000 kids. Each kid is presented with 10 randomly picked pet pictures at a time. Each time, they have to pick the one picture that they like best. Our goal, during inference, is to predict probabilities on which picture (from 0 to 9) the (same) kids will pick. My struggle is how to construct a NN to make this classification.

Paths I've been thinking about or tried:

* I have created embeddings for kids and pictures using a (Netflix competition winner style) factorization method. The embeddings are pretty good: Visualizing the picture embeddings in a projector, similar pets are grouped together.

* The first thing I tried was to concatenate the embeddings of the 10 pictures and feed this together with the embedding of the kid. The output layer a softmax and CE loss. But it doesn't work - I guess it's to difficult for the model to ""understand"" where one picture embedding starts and another stops, and to relate each of the embeddings to the 10 categories in the output layer.

* tchainzzz pointed me in the direction of meta-learning, including few-shot learning (before I had clarified my case). But these methods are mainly intended for classifying the entities (is the pet a dog or a cat?) and they are intended for limited training sets. In our case, we're not classifying the pictures (we already know which ones are cats and which ones are dogs) and we have ample training data.

* Why not use metric learning with siamese networks? I don't think it will work here, because this method assumes that there is one ideal pet that each kid would select, and we just need to figure out which picture is more like that ideal pet. But we don't have an ideal pet for each kid, only the previously performed selections.

* Why not use some kind of ranking solution? (We could probably create a system, like elo chess ranking. Every time a kid selects a picture, that picture would get a higher ranking, particularly for that kid, and more so if the competing picture already have a high ranking.) Because that's not a neural network classification architecture. I can add such a ranking as a feature, but the question is how to create an NN model so that it ""understands"" that the classification should happen from a menu of 10 available dishes.

* There is, however, elements from 'siamese networks' that I've been thinking about. Not the metric part of siamese architechture, but the 'shared weights' part: A possible solution to my problem could be that I insert the embedding for the kid next to the embedding of one picture (i from 0 to 9) into 10 siamese twin networks (i from 0 to 9) sharing the same weights. Each twin would have one output mapped to 10 classes in a softmax layer. (The softmax layer is on the outside of the siamese part of the network.) I have tried this quickly, without much luck. But so far, this is my best idea and i'm continuing to work in this direction.

Any further advice or ideas would be welcome!",edited body,2020-12-28 22:20:16.487
1174455,333649,37792.0,3,,CC BY-SA 4.0,50ec426f-c201-4684-abf4-30e54efb091f,<r><time-series><sampling><autoregressive>,,2020-12-28 22:37:31.573
1174456,333649,37792.0,2,,CC BY-SA 4.0,50ec426f-c201-4684-abf4-30e54efb091f,"This is probably a very straight forward question but I want to verify how I should sample from an AR(1) process in R using just the `rnorm()` function in R (or any similar function in another language). Say I want to sample 100 samples from the following AR(1) model:

$$
X_t = \rho X_{t-1} + \epsilon_t, \qquad \epsilon_t \sim N(0, \sigma^2_\epsilon).
$$

Then the joint distribution of the vector $(X_1,\dots,X_{100})$ is multivariate Gaussian with zero mean and covariance matrix $\Sigma$, where 

$$
BB^T :=\Sigma = \frac{\sigma^2_\epsilon}{1-\rho^2}
\begin{bmatrix}
1 & \rho & \rho^2 & \rho^3 & \rho^4&\cdots\\
\rho & 1 &\rho & \rho^2 &  \rho^3 &\cdots\\
\rho^2 &\rho & 1 &\rho &    \rho^2&\cdots\\
\rho^3 &\rho^2 &\rho & 1 &    \rho&\cdots\\
\vdots &\vdots &\vdots& \vdots &\ddots&    \vdots\\
\end{bmatrix}
$$
So I should be able to sample in R using the following:
```
X = matrix(rnorm(100), 1, 100) %*% B
```
Is this approach correct?",,2020-12-28 22:37:31.573
1174457,333649,37792.0,1,,CC BY-SA 4.0,50ec426f-c201-4684-abf4-30e54efb091f,Sampling from an AR(1) process using normal samples,,2020-12-28 22:37:31.573
1174460,333649,37792.0,5,,CC BY-SA 4.0,1cc46dda-f246-4e89-94e1-92cc60f3590a,"This is probably a very straight forward question but I want to verify how I should sample from an AR(1) process in R using just the `rnorm()` function in R (or any similar function in another language). Say I want to sample 100 samples from the following AR(1) model:

$$
X_t = \rho X_{t-1} + \epsilon_t, \qquad \epsilon_t \sim N(0, \sigma^2_\epsilon).
$$

Then the joint distribution of the vector $(X_1,\dots,X_{100})$ is multivariate Gaussian with zero mean and covariance matrix $\Sigma$, where 

$$
BB^T :=\Sigma = \frac{\sigma^2_\epsilon}{1-\rho^2}
\begin{bmatrix}
1 & \rho & \rho^2 & \rho^3 & \rho^4&\cdots\\
\rho & 1 &\rho & \rho^2 &  \rho^3 &\cdots\\
\rho^2 &\rho & 1 &\rho &    \rho^2&\cdots\\
\rho^3 &\rho^2 &\rho & 1 &    \rho&\cdots\\
\vdots &\vdots &\vdots& \vdots &\ddots&    \vdots\\
\end{bmatrix}
$$
So I should be able to sample in R using the following:
```
X = matrix(rnorm(100, 0, sigmaeps), 1, 100) %*% B
```
Is this approach correct?",added 13 characters in body,2020-12-28 22:53:29.510
1174473,333653,254365.0,2,,CC BY-SA 4.0,20d2eb1c-2537-44eb-8d6b-4ff6b3bc0f61,"I tried to solve the exercise and it seems meaningful, but the problem is that in the book the result is $25/72$, while mine is $1/7776$. I don't know if maybe I didn't understand the problem, but by the text I understood that there are two dices rolled three times, and I have to calculate the probability to have 7 as result. Now, I could use the binomial probability density function, but I didn't know very well how to use the data I have (if someone can also explain me the resolution with this structure I'll thank him). By logic, throwing three times two dices, the total number of possibilities is 46'656, because I have $36^3$. Now, to have 7 as result and a dice must have at least 1 as value, the only possibility that I have to reach 7 is 11/11/12 (the first two dices are 1, the 3th and 4th 1, the 5th 1 and the 7th 2) and all the combination of this sequence:
11/11/12
11/11/21
11/21/11
11/12/11
21/11/11
12/11/11

Calculating the probability, I have $3* (1/36 * 1/36 * 2/36)$, and the probability of having 7 as result is $6/7776$

I don't know where I'm wrong.",,2020-12-28 23:51:40.400
1174474,333653,254365.0,1,,CC BY-SA 4.0,20d2eb1c-2537-44eb-8d6b-4ff6b3bc0f61,"What is the probability that in 3 rolls of a pair of six-sided dice, exactly one total of 7 is rolled?",,2020-12-28 23:51:40.400
1174475,333653,254365.0,3,,CC BY-SA 4.0,20d2eb1c-2537-44eb-8d6b-4ff6b3bc0f61,<probability><self-study><dice>,,2020-12-28 23:51:40.400
1174477,333655,255099.0,2,,CC BY-SA 4.0,fcbdbfca-d434-45a5-807d-d7c3b58f76a6,"There's a really cool method called convergent cross-mapping [(see here)](https://link.springer.com/chapter/10.1007/978-3-319-58895-7_27) that's used to see if two time-series are causally linked within a dynamic system. It seems really powerful and like it might have a wide range of applications. 

I'm wondering if it's sufficient to just ""plug in"" two variables you're interested in and go, or if you need to control for potential omitted variables like you do in multivariate regression studies. 

Thanks!",,2020-12-28 23:56:56.197
1174478,333655,255099.0,1,,CC BY-SA 4.0,fcbdbfca-d434-45a5-807d-d7c3b58f76a6,Does convergent cross-mapping require you to control for other variables?,,2020-12-28 23:56:56.197
1174479,333655,255099.0,3,,CC BY-SA 4.0,fcbdbfca-d434-45a5-807d-d7c3b58f76a6,<time-series><causality><granger-causality><controlling-for-a-variable>,,2020-12-28 23:56:56.197
1174483,333656,237981.0,3,,CC BY-SA 4.0,23567b5a-c634-4c59-b3a9-6a330cc9d1e9,<normal-distribution><conditional-expectation><multivariate-distribution>,,2020-12-29 00:09:46.923
1174484,333656,237981.0,2,,CC BY-SA 4.0,23567b5a-c634-4c59-b3a9-6a330cc9d1e9,"I've been reading over this [Multivariate Gaussian conditional proof][1], trying to make sense of *how* the mean and variance of a gaussian conditional was derived. I've come to accept that unless I allocate a dozen or so hours to refreshing my linear algebra knowledge, it's out of my reach for the time being. 

that being said, I'm looking for a conceptual explanation for that these equations represent: 

$$\mu_{1|2} = \mu_1 + \Sigma_{1,2} * \Sigma^{-1}_{2,2}(x_2 - \mu_2)$$ 

I read the first as ""Take $\mu1$ and augment it by some factor, which is the covariance scaled by the precision (measure of how closely $X_2$ is clustered about $\mu_2$, maybe?) and projected onto the distance of the specific $x_2$ from $mu_2$."" 

$$\Sigma_{1|2} = \Sigma_{1,1} - \Sigma_{1,2} * \Sigma^{-1}_{2,2} * \Sigma_{1,2}$$ 

I read the second as, ""take the variance about $\mu_1$ and subtract some factor, which is covariance squared scaled by the precision about $x_2$."" 


In either case, the precision $\Sigma^{-1}_{2,2}$ seems to be playing a really important role.      

A few questions: 
- Am I right to treat precision as a measure of how closely observations are clustered about the expectation? 
- Why is the covariance squared in the latter equation? (Is there a geometric interpretation?) So far, I've been treating $\Sigma_{1,2} * \Sigma^{-1}_{2,2}$ as a ratio, (a/b), and so this ratio acts to scale the (second) $\Sigma_{1,2}$, essentially accounting for/damping the effect of the covariance; I don't know if this is valid.     
- Anything else you'd like to add/clarify? 

  [1]: https://statproofbook.github.io/P/mvn-cond",,2020-12-29 00:09:46.923
1174485,333656,237981.0,1,,CC BY-SA 4.0,23567b5a-c634-4c59-b3a9-6a330cc9d1e9,Interpretation of multivariate conditional gaussian function form?,,2020-12-29 00:09:46.923
