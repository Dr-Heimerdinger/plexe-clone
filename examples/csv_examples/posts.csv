id,owner_user_id,post_type_id,accepted_answer_id,parent_id,owner_display_name,title,tags,content_license,body,creation_date
333656,237981.0,1,,,,Interpretation of multivariate conditional gaussian function form?,<normal-distribution><conditional-expectation><intuition><multivariate-distribution>,CC BY-SA 4.0,"<p>I've been reading over this <a href=""https://statproofbook.github.io/P/mvn-cond"" rel=""nofollow noreferrer"">Multivariate Gaussian conditional proof</a>, trying to make sense of <em>how</em> the mean and variance of a gaussian conditional was derived. I've come to accept that unless I allocate a dozen or so hours to refreshing my linear algebra knowledge, it's out of my reach for the time being.</p>
<p>that being said, I'm looking for a conceptual explanation for that these equations represent:</p>
<p><span class=""math-container"">$$\mu_{1|2} = \mu_1 + \Sigma_{1,2} * \Sigma^{-1}_{2,2}(x_2 - \mu_2)$$</span></p>
<p>I read the first as &quot;Take <span class=""math-container"">$\mu1$</span> and augment it by some factor, which is the covariance scaled by the precision (measure of how closely <span class=""math-container"">$X_2$</span> is clustered about <span class=""math-container"">$\mu_2$</span>, maybe?) and projected onto the distance of the specific <span class=""math-container"">$x_2$</span> from <span class=""math-container"">$mu_2$</span>.&quot;</p>
<p><span class=""math-container"">$$\Sigma_{1|2} = \Sigma_{1,1} - \Sigma_{1,2} * \Sigma^{-1}_{2,2} * \Sigma_{1,2}$$</span></p>
<p>I read the second as, &quot;take the variance about <span class=""math-container"">$\mu_1$</span> and subtract some factor, which is covariance squared scaled by the precision about <span class=""math-container"">$x_2$</span>.&quot;</p>
<p>In either case, the precision <span class=""math-container"">$\Sigma^{-1}_{2,2}$</span> seems to be playing a really important role.</p>
<p>A few questions:</p>
<ul>
<li>Am I right to treat precision as a measure of how closely observations are clustered about the expectation?</li>
<li>Why is the covariance squared in the latter equation? (Is there a geometric interpretation?) So far, I've been treating <span class=""math-container"">$\Sigma_{1,2} * \Sigma^{-1}_{2,2}$</span> as a ratio, (a/b), and so this ratio acts to scale the (second) <span class=""math-container"">$\Sigma_{1,2}$</span>, essentially accounting for/damping the effect of the covariance; I don't know if this is valid.</li>
<li>Anything else you'd like to add/clarify?</li>
</ul>
",2020-12-29 00:09:46.923
333657,255100.0,2,,333653.0,,,,CC BY-SA 4.0,"<p>To get the result you can start by calculating the probability of obtaining a sum of 7 when rolling two dices:
P(7)= 6/36 = 1/6</p>
<p>And then as you say its a binomial:
So it's combinatory of 1 in 3 times the probability of success to the power of 1 times probability of failure to the power of 2
So: (3!/1!*(3-1)!) * (1/6)^1 * (5/6)^2</p>
<p>Hope it makes everything clear</p>
",2020-12-29 00:14:28.920
333658,128628.0,2,,333649.0,,,,CC BY-SA 4.0,"<p>Your general approach is fine, but obviously you will need to find the matrix <span class=""math-container"">$B$</span> from your stated form for <span class=""math-container"">$\Sigma$</span>.  (Be careful here --- unless you use the <em>symmetric</em> square root as <span class=""math-container"">$B$</span> you will need to amend your code to <em>pre</em>-multiply by <span class=""math-container"">$B$</span> instead of post-multiplying.)</p>
<p>In any case, if you want to save yourself some time you can use the <code>rGARMA</code> function in the <a href=""https://cran.r-project.org/package=ts.extend"" rel=""nofollow noreferrer""><code>ts.extend</code> package</a> where this stuff is already programmed.  This function will produce random vectors from any stationary Gaussian ARMA model, and it can also generate from the conditional distribution if some of the time-series values are given.  Here is an example:</p>
<pre><code>#Load the package
library(ts.extend)

#Produce n = 16 time-series vectors from AR(1) process with length m = 30
set.seed(1)
m        &lt;- 30
AR       &lt;- 0.7
MEAN     &lt;- 4
ERRORVAR &lt;- 2
SERIES   &lt;- rGARMA(n = 16, m = m, ar = AR, mean = MEAN, errorvar = ERRORVAR)

#Plot the series
plot(SERIES)
</code></pre>
<p><a href=""https://i.stack.imgur.com/XnIgM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XnIgM.jpg"" alt=""enter image description here"" /></a></p>
",2020-12-29 00:29:12.573
333659,128628.0,2,,333647.0,,,,CC BY-SA 4.0,"<p>The increments in a random walk are <em>exchangeable</em>, so it would be reasonable to use a permutation test to test the underlying hypothesis of exchangeability here.  The blue &quot;walk&quot; will certainly fail that test, which will alert you to the fact that it is not a random walk.</p>
",2020-12-29 00:33:09.677
333660,150694.0,1,333665.0,,,Does the assumption of autocorrelation apply to linear regression with a categorical predictor?,<r><regression><anova><categorical-data><assumptions>,CC BY-SA 4.0,"<p>[Edit:] <strong>My question concerns the use of a <code>categorical predictor</code> in linear regression specifically.</strong></p>
<p>Some of the assumptions for linear regression include: normality, homoscedasticity, and autocorrelation (of residuals), among others.</p>
<p>Normally, these all relate to the residuals of the model, but in a <a href=""https://stats.stackexchange.com/a/502177/195567"">previous answer</a>, it was suggested that when using a <code>categorical predictor</code>, assumptions should be checked on the raw data instead, <strong>but also that all the same assumptions endure</strong>. That being said, a regression with a <code>categorical</code> predictor is very similar to an ANOVA, and to my knowledge, ANOVA does not have the assumption of autocorrelation of residuals.</p>
<p>Which brings the question: <strong>Does the assumption of autocorrelation apply to linear regression with a categorical predictor? If yes, then how do you test it in <code>R</code> on the raw data (and not from the model)?</strong></p>
<h2>Edit</h2>
<p>I began my question by saying the &quot;main&quot; assumptions but perhaps I should have said, &quot;the main assumptions of interest to check at the analysis stage (once data is already collected and respecting independence of observations) with a categorical variable (group) as predictor.&quot; I had read online (e.g., <a href=""http://r-statistics.co/Assumptions-of-Linear-Regression.html"" rel=""nofollow noreferrer"">http://r-statistics.co/Assumptions-of-Linear-Regression.html</a>) that there are 10 assumptions. I will name them below for the sake of completeness (though I think the list is missing independence and linearity proper):</p>
<ol>
<li>The regression model is linear in parameters [re: related to Linearity]</li>
<li>The mean of residuals is zero</li>
<li>Homoscedasticity of residuals or equal variance [re: Equal Variance]</li>
<li>No autocorrelation of residuals</li>
<li>The X variables and residuals are uncorrelated</li>
<li>The number of observations must be greater than the number of Xs</li>
<li>The variability in X values is positive</li>
<li>The regression model is correctly specified</li>
<li>No perfect multicollinearity</li>
<li>Normality of residuals [re: Normality]</li>
</ol>
<p>Some of these assumptions either have been already verified in my case or do not apply. For example, multicollinearity does not apply to my situation as I have a single predictor, and by definition when comparing two levels of a categorical predictor the relationship will be linear (<a href=""https://www.researchgate.net/post/Check-linearity-between-the-dependent-and-dummy-coded-variables"" rel=""nofollow noreferrer"">example 1</a>, <a href=""https://stats.stackexchange.com/a/430985/195567"">example 2</a>).</p>
<p><strong>That being said, as the title of my question points out, my question relates exclusively to the autocorrelation of residuals assumption for the case of a <code>categorical predictor</code>.</strong> The source linked before mentions &quot;This is applicable especially for time series data&quot;, but it doesn't say that it doesn't apply to non-time-series data, so that's why I wasn't sure (also: in relation to a categorical predictor).</p>
",2020-12-29 00:57:47.943
333661,154203.0,2,,333655.0,,,,CC BY-SA 4.0,"<p><strong>TLDR:</strong> They test for causality in Granger sense. It is not causality in the interventional meaning as defined in Pearl et al. (2016). If you seek for Granger causality - simply plug in any two variables. If you wish to perform causal inference - things are never so simple.</p>
<hr />
<p>This is indeed very cool method and interesting question. However, as many other authors, Tsonis et al. (2018) call causality in Granger sense &quot;just causality&quot;, which is in my opinion very misleading attitude. There are many definitions of causality and Granger causality is not one of them. It is something different.</p>
<p>To show an example how it is misleading let me first cite Tsonis et al. (2018):</p>
<p>&quot;<em>if past sea surface temperatures can be estimated from time series of sardine abundance, temperature had a measurable and recoverable influence on the population dynamics of sardines</em>&quot;</p>
<p>Ok. But what if we used something different than the temperature measures around the particular sea? What if we measured the number of sunburns got by the population of people sunbathing by the sea? Let me paraphrase their sentence:</p>
<p>&quot;<em>if past numbers of sunburns can be estimated from time series of sardine abundance, sunburns had a measurable and recoverable influence on the population dynamics of sardines</em>&quot;</p>
<p>Now, this looks bad. But it is how causality in Granger sense works. It seeks for predictors, and has purely observational meaning. If we observe unnaturally high number of sunburns this summer, will we observe changes in sardines population in the future? Very likely. But if we ban suntan cream from use, will population of sardines change? Highly unlikely.</p>
<hr />
<p>It is important to note, that the variables Tsonis et al. (2018) say that they influence something, are arguably &quot;very exogenous&quot;. It is extremely unlikely to find variables that influence temperature in short term, it is much more unlikely to find variables that influence cosmic radiation.</p>
<p>Such situation helps causal inference (and make this article even more misleading, because it makes sense) but it it is an external knowledge (assumption about DGP), which can not be derived directly from the data. And causal inference always requires such external knowledge.</p>
<hr />
<p>Pearl, J., Glymour, M. and Jewell, N.P., 2016. <em>Causal inference in statistics: A primer</em>. John Wiley &amp; Sons.</p>
<p>Tsonis, Anastasios A., Ethan R. Deyle, Hao Ye, and George Sugihara. <em>Convergent cross mapping: theory and an example</em>. In Advances in nonlinear geosciences, pp. 587-600. Springer, Cham, 2018.</p>
",2020-12-29 01:07:29.643
333662,255105.0,1,,,,Does it make sense to use variance to describe non normal distributions?,<normal-distribution><variance>,CC BY-SA 4.0,"<p>I would like to know if it makes any sense to use the variance or standard deviation to describe non gaussian distributions</p>
",2020-12-29 01:12:27.663
333663,255099.0,1,,,,"Comparing time series data with multiple pairs of time series, or difference-in-difference with continuous treatment conditions?",<time-series><multivariate-analysis><causality><difference-in-difference><granger-causality>,CC BY-SA 4.0,"<p>My dataset contains time-series for two variables (<span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span>) from 2017 to 2020, for each of many different countries. Each country has its own time series for each variable (X_usa, X_india, Y_usa, Y_india, etc). My goal is to determine the general relationship between <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span>.</p>
<p>The difficulty is that I don't know a good method to account for both country-effects, and time-effects. I could do a standard regression analysis, where each observation is a country's mean (over all times) values of <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span>. I could also do a Granger Causality Test, using just two time-series, where each date shows the mean of all the countries' <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> values.</p>
<p>The goal is to integrate information from BOTH dimensions. Standard regression, of course, fails to account for temporal effects present in time-series data (see, for instance, the start of <a href=""https://www.youtube.com/watch?v=uhONGgfx8Do&amp;t=120s"" rel=""nofollow noreferrer"">this talk</a>). Meanwhile, taking the mean across countries would erase &quot;spikes&quot; or temporal patterns present in just some of the countries in the dataset.</p>
<p>Is there a way to extend statistical tests for time-series (like Granger Causality) to multiple pairs of time-series data?</p>
<p>Or, similarly, is there a way to use difference-in-difference (i.e., countries are treated at different points in time), but with a continuous set of measurements over time for each country, rather than just before/after, and with a continuous treatment variable? (I could, if needed, transform the &quot;treatment,&quot; which is the amount of <span class=""math-container"">$X$</span>, into a single variable for each country, rather than a time-series).</p>
",2020-12-29 01:19:44.063
333664,35053.0,1,,,,How to determine time series similarity by magnitude and pattern?,<time-series><similarities><hierarchical-clustering><dynamic-time-warping>,CC BY-SA 4.0,"<h2>Context</h2>
<p>I know that Dynamic Time Warping (DTW) can be used to assign a dissimilarity score between two time series. Based on the distance matrix of DTW scores, I can cluster data with say, Hierarchical Clustering (HC). But in my observation, the similarly clustered series are similar in pattern, not necessarily in magnitude.</p>
<p>So, my question is: how can I determine time series similarity by both magnitude and pattern.</p>
<h2>Example</h2>
<p>I used the gaminder data in <code>R</code> to determine similarities between countries in terms of 3 variables: life expectancy, GDP, and population using DTW. Then I clustered them using HC.</p>
<p>Following shows the resulting dendrogram (I cut the tree in 8 clusters):</p>
<p><a href=""https://i.stack.imgur.com/7kTjT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7kTjT.png"" alt=""enter image description here"" /></a></p>
<p>As an example, note here:</p>
<ol>
<li>Yemen and Oman are most similar in a cluster, followed by Syria, Jordan and Westbank &amp; Gaza</li>
<li>Bahrain and Mongolia are most similar in another cluster, followed by Phillipines</li>
</ol>
<p>The following shows a scatter plot with <code>year</code> on x-axis and <code>lifeExp</code>ectancy on y-axis. The number on each facet represents a cluster. Notice that in cluster 7 Yemen and Oman are similar by pattern, BUT other countries in the cluster seem to be similar to Oman in terms of life expectancy (i.e. magnitude). The same is seen in cluster 2 and 4. Others have less number of members.</p>
<p>How can I find similarity by magnitude and pattern?</p>
<p><a href=""https://i.stack.imgur.com/y45on.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y45on.png"" alt=""enter image description here"" /></a></p>
<h3>R Code to reproduce plot</h3>
<pre><code>suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(dtwclust))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(gapminder))
suppressPackageStartupMessages(library(ggrepel))

scale_this &lt;- function(x){
  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)
}


df &lt;- gapminder %&gt;% 
  filter(continent == &quot;Asia&quot;) %&gt;% # countries in Asia only
  group_by(country) %&gt;%           # scaling the vars for each country
  mutate(lifeExp = scale_this(lifeExp),
         pop = scale_this(pop),
         gdpPercap = scale_this(gdpPercap)
  ) %&gt;%
  ungroup()


df_matrix &lt;- df %&gt;% 
  split(.$country, drop = T) %&gt;% 
  purrr::map(~ .x[, c(&quot;lifeExp&quot;,    # names of all vars
                       &quot;pop&quot;,
                       &quot;gdpPercap&quot;
                      )]) %&gt;% 
  purrr::map(as.matrix) 


clusters_gp &lt;- df_matrix %&gt;% 
  tsclust(., 
          k = 8L,                 # assuming clusters
          distance = &quot;dtw_basic&quot;, # this is dtw score
          seed = 390,             # to reproduce results
          type=&quot;hierarchical&quot;,    # type of clustering
          control = hierarchical_control(method = &quot;ward.D&quot;)) 


Gps &lt;- as.data.frame(cutree(clusters_gp, k = 8)) # num of clusters
colnames(Gps) &lt;- &quot;Gp&quot;
Gps$country &lt;- row.names(Gps)
row.names(Gps) &lt;- NULL


## Getting the clustering info into the original data
gapminder_Asia &lt;- gapminder %&gt;% 
  filter(continent == &quot;Asia&quot;) %&gt;% 
  left_join(x=., y=Gps, by = &quot;country&quot;)


### Select 15 countries at random to label on plot
set.seed(123)

selected_countries &lt;- gapminder_Asia %&gt;% 
  group_by(Gp) %&gt;% 
  select(country) %&gt;% 
  unique() %&gt;% 
  sample_n(size = 4, replace = TRUE) %&gt;% 
  ungroup() %&gt;% 
  pull(country) %&gt;% 
  unique()


sc_data &lt;- gapminder_Asia %&gt;% 
  filter(year == 1982,
         country %in% selected_countries)



### lifeExp
ggplot(data = gapminder_Asia,
       aes(x = year, y = lifeExp)) +
  geom_line(aes(group=country), color = &quot;grey&quot;) +
  facet_wrap(~ Gp) +
  geom_line(data = gapminder_Asia %&gt;% 
              filter(country %in% selected_countries),
            aes(group=country)) +
  geom_text_repel(data = sc_data,
            aes(label = country),
            box.padding = 1,
            nudge_x = .15,
            nudge_y = .5,
            arrow = arrow(length = unit(0.015, &quot;npc&quot;)),
            hjust = 0
            ) +
  theme_bw()
</code></pre>
",2020-12-29 01:43:07.750
333665,24790.0,2,,333660.0,,,,CC BY-SA 4.0,"<p>This statement is plain wrong: <em>&quot;I believe the three main assumptions for linear regression are: normality, homoscedasticity, and autocorrelation.&quot;</em>.  When it comes to statistics, it's dangerous to believe things without doing your due dilligence and checking them out.</p>
<p>If you check any statistical book on linear regression modelling, you will see that there are 4 major assumptions underlying this type of modelling, which can be conveniently summarized via the word LINE:</p>
<pre><code>L: Linearity
I: Independence
N: Normality
E: Equal Variance 
</code></pre>
<p>The Equal Variance assumption is what you referred to as homoscedasticity.</p>
<p>In practice, the most important assumption is Linearity, followed by Independence, followed by Equal Variance, followed by Normality.</p>
<p>The independence assumption means that the response values Y are assumed to be independent from each other. In other words, knowing one value of Y tells you nothing about any of the other values of Y.</p>
<p>Usually, linear regression is applied to data collected from a cross-sectional sample. As an example, this sample can be obtained by randomly sampling a population of subjects and recording the value of the response variable Y and the predictor variables X1, ..., Xp for each of these subjects at the same time point.</p>
<p>However, it is possible to use linear regression to a response value Y collected over time. An example of such response variable will be annual sea surface temperature at a particular ocean location.  The values of this variable will be collected year after year for that same site and will likely be autocorrelated (e.g., a high value of sea surface temperature this year was likely preceded by a high value the previous year).  If you formulate a linear regression model which will relate annual sea surface temperature (Y) to year (X), then you would have to check its residuals for the presence of autocorrelation.  If autocorrelation is present, then you might want to consider a generalized least squares fit for your model rather than an ordinary least squares fit.</p>
<p>Thus, worrying about autocorrelation of the model residuals only makes sense if your response values were collected over time.  If your response values were collected at regular time intervals (e.g., every year), then you could use graphical methods such as ACF or PACF plots to detect the presence of autocorrelation among the model residuals.  If the data were collected at irregular time intervals, the graphical methods you use would have to reflect that irregularity.</p>
<p>Perhaps after reading this answer you can revise your question and make it more specific.</p>
",2020-12-29 03:13:54.927
333666,253814.0,2,,333662.0,,,,CC BY-SA 4.0,"<p>Yes, the variance can still be a useful descriptor for non-Gaussian distributions, but it will also depend on your goal. For example, a Gaussian distribution is uniquely characterized by its mean and variance (i.e. if we know the mean and variance, then we know everything about the distribution), and this is true for some others, but certainly not all distributions.</p>
",2020-12-29 03:33:07.250
333667,102906.0,1,,,,Can one uniformly generate complex numbers of absolute value less than a given constant $R \neq 1$?,<random-variable><random-generation><uniform-distribution><circular-statistics><complex-numbers>,CC BY-SA 4.0,"<p>Can one uniformly generate complex numbers of absolute value less than a given constant R?</p>
<p>This would appear to be equivalent to picking points <span class=""math-container"">$(x,y)$</span> uniformly in a disk of radius R, where <span class=""math-container"">$x$</span> is the real component of the complex number, and <span class=""math-container"">$y$</span>, its complex component.</p>
<p>This question appears to differ from numerous others that have been asked, in that <span class=""math-container"">$R$</span> is not assumed to equal 1 (that is, the unit disc).</p>
<p>(Note: I am a Mathematica user <a href=""https://mathematica.stackexchange.com/users/29989/paul-b-slater"">MathematicaQuestions</a>, not an R [no pun, intended] user.)</p>
",2020-12-29 03:39:28.570
333668,255117.0,1,,,,the ratio of validation set and test set should be equal?,<cross-validation><data-mining>,CC BY-SA 4.0,"<p>I always heard that the common ratio of the train:validation:test is 70:15:15 or 80:10:10 or 60:20:20, sounds like the validation set and test set should be equal size. Assuming that I wanna  use 5 fold cross validation, so the train:val:test may be equal to 68:16:16? as 84/16 = 5.</p>
",2020-12-29 03:46:06.280
333669,204258.0,1,,,,Incidence rate ratio after negative binomial results in R,<generalized-linear-model><negative-binomial-distribution><incidence-rate-ratio><error-message>,CC BY-SA 4.0,"<p>I am trying to calculate the incidence rate ratio of my negative binomial model. After running my negative binomial regression model, I use <code>exp(Mymodel)</code> in order to get the incidence rate ratio but R gives me this message: non-numeric argument to mathematical function.</p>
<p>Is the code wrong?</p>
",2020-12-29 04:16:19.040
333670,16159.0,2,,333662.0,,,,CC BY-SA 4.0,"<p>You can compute many statistics for many distributions. They have a used.</p>
<p>Some of the most common ones include:</p>
<ul>
<li>Quantiles</li>
<li>moments (including mean and variance)</li>
<li>Extreme values</li>
</ul>
<p>If I rearrange this list it is still useful:</p>
<ul>
<li>measures of central tendency.</li>
<li>measures of tendency of variation or spread</li>
<li>Measures of other geometric tendencies.</li>
<li>Ordered measures vs moments</li>
</ul>
<p>You can see how under the heading of the measures of Tendency of variation, also called measures of tendency of spread, the variance could be quite useful. In fact, being a moment, it is not necessarily derived or contingent on a distribution being normally distributed.</p>
",2020-12-29 04:17:43.583
333677,144189.0,1,,,,Monte Carlo / Gambler's Fallacy in fair coin toss: why bet on Tails after 26 consecutive Heads,<mathematical-statistics>,CC BY-SA 4.0,"<p>given a <strong>fair</strong> coin that tossed Heads <a href=""https://en.wikipedia.org/wiki/Gambler%27s_fallacy#Monte_Carlo_Casino"" rel=""nofollow noreferrer"">26 consecutive times</a>, we need to decide whether gamble on Tails and provide a logical statistical explanation.</p>
<p>Since this is a fair coin and the probability of each event is 1/2:</p>
<ul>
<li>after 26 consecutive Heads, the probability of Tails remains equal to the probability of Heads (1/2).</li>
<li>the probability of every sequence appearing is equally very low 7.45e^-9: HHHHHHHHHHHHHHHHHHHHHHHH<strong>H</strong> has the exact same probability as HHHHHHHHHHHHHHHHHHHHHHHH<strong>T</strong> which has the exact same probability as HTHTHTHTHTHTHTHTHTHTHTHTHT</li>
</ul>
<p>Nevertheless, my biased human intuition tells me to bet on tails. What is the correct theoretical foundation? ..or is this just a common misconception of chance?</p>
<p>Appendix:</p>
<ul>
<li>simulating 10,000 tosses I observed maximum of 12 consecutive Heads</li>
</ul>
<p>Thank you</p>
",2020-12-29 09:45:25.860
333671,102906.0,2,,333667.0,,,,CC BY-SA 4.0,"<p>The MathWorld entry <a href=""https://mathworld.wolfram.com/DiskPointPicking.html"" rel=""nofollow noreferrer"">DiskPointPicking</a> asserts (without an explicit proof) that to generate uniformly distributed  points (<span class=""math-container"">$x,y)$</span> in the unit disk, one should employ
<span class=""math-container"">\begin{equation}
x=\sqrt{r} \cos{\theta},\hspace{.2in} y=\sqrt{r} \sin{\theta}.
\end{equation}</span>
where <span class=""math-container"">$r \in [0,1]$</span>, and <span class=""math-container"">$\theta \in [0, 2 \pi]$</span> are uniformly distributed
variables.</p>
<p>So for points (<span class=""math-container"">$x,y)$</span> in a disk of radius <span class=""math-container"">$R$</span>, it appears then that
one should employ
<span class=""math-container"">\begin{equation}
x=\sqrt{\tilde r} \cos{\theta},\hspace{.2in} y=\sqrt{\tilde r} \sin{\theta}.
\end{equation}</span>
where <span class=""math-container"">$\tilde r \in [0,R^2]$</span>, and <span class=""math-container"">$\theta \in [0, 2 \pi]$</span> are uniformly distributed
variables.</p>
<hr />
<hr />
<p>Upon further reflection/application, I'm somewhat confused here by my original answer (trying to apply the MathWorld argument).</p>
<p>Say,
the maximum absolute value
<span class=""math-container"">$R$</span> of the complex numbers <span class=""math-container"">$x+ I y$</span>, I want to generate
is <span class=""math-container"">$\frac{1}{2}$</span>. Now (assuming <span class=""math-container"">$x=y$</span>, which seems permissible), we have the relation,
<span class=""math-container"">\begin{equation}
\sqrt{(\frac{1}{\sqrt{8}})^2+(\frac{1}{\sqrt{8}})^2}=\sqrt{\frac{1}{4}}= \frac{1}{2}.
\end{equation}</span></p>
<p>So, it seems that I want to choose <span class=""math-container"">$\tilde{r}$</span>--before taking its square root--from <span class=""math-container"">$[0,\frac{1}{8}]$</span>, not <span class=""math-container"">$[0,R^2 =\frac{1}{4}]$</span>.</p>
<p>???</p>
",2020-12-29 04:53:06.557
333672,255103.0,1,,,,Testing uniform distribution of a discrete function derived from a random number generator,<random-variable><uniform-distribution><kolmogorov-smirnov-test>,CC BY-SA 4.0,"<p>(Forewarning: I'm a programmer, not a statistician, so I apologize in advance for any misuse of terminology!)</p>
<p>I'm testing a known random number generator that implements the <a href=""https://www.pcg-random.org/"" rel=""nofollow noreferrer"">PCG</a> algorithm. This RNG outputs integers in the range <span class=""math-container"">$[0, 2^{64})$</span>. I built a function on top of it <span class=""math-container"">$F(n)$</span> that allows me to generate a random integer in the range <span class=""math-container"">$[0, n)$</span> by calculating <span class=""math-container"">$X \bmod n$</span> where <span class=""math-container"">$X$</span> is an output of the PCG RNG. This is subject to <a href=""https://stackoverflow.com/a/10984975/2012737"">modulo bias</a>, so I implemented the solution to that in my function as well.</p>
<p>Now I want to make sure that the numbers produced by <span class=""math-container"">$F$</span> are uniformly distributed. I generate 1,000,000 samples for each of several values of <span class=""math-container"">$n$</span>. I then attempt to use the Kolmogorov-Smirnov test, but I have the following questions:</p>
<ol>
<li><p>For small values of <span class=""math-container"">$n$</span>, like 10, the stepping of the function interferes with the test (which, as we know, isn't designed to work with discrete distributions). I tried to work around the problem by using a chi-squared test, but my binning method seemed to have more of an impact on the test than the actual distribution. So I came up with the &quot;brilliant&quot; solution of adding to each sample a fraction of the step value, <span class=""math-container"">$i/N$</span> where <span class=""math-container"">$i$</span> is the iteration and <span class=""math-container"">$N$</span> is the number of samples (in my case 1,000,000). In other words, I am testing <span class=""math-container"">$F'(n, i) = F(n) + \frac{i}{N}$</span> instead.</p>
<p>This makes the Kolmogorov-Smirnov test appear to work for even the small values of <span class=""math-container"">$n$</span> I mentioned, i.e., if I run my test suite 100 times I get roughly 1 failure at <span class=""math-container"">$\alpha = 0.01$</span>. And if I mess with the values produced by the RNG, for example by changing <span class=""math-container"">$X \bmod n$</span> to <span class=""math-container"">$(X \land {10101010101010101010101010101010}_2) \bmod n$</span>, the p-value immediately diminishes to 0.</p>
<p>That said, I haven't found any academic research that suggests adding a fraction of the step value to each sample is a reasonable way to convert a discrete distribution to a seemingly continuous one. Is this a valid approach or am I actually testing something I don't want to, like the uniformity of the step itself?</p>
</li>
<li><p>If I pick an <span class=""math-container"">$n$</span> that I suspect will be subject to a lot of modulo bias, for example <span class=""math-container"">$\frac{2}{3}({2^{64}-1})$</span>, I do get reliable rejection of the null hypothesis if I omit my bias correction code (yay!). But I wonder if there's a more analytic way to choose <span class=""math-container"">$n$</span> values to test? I just chose a few that I thought would be common use cases or interact with the code in question in interesting ways.</p>
</li>
</ol>
<p>Thanks for any insight you can provide!</p>
",2020-12-29 06:26:56.413
333673,179252.0,1,,,,R fitted() function applied to rbart (Bayesian Additive Regression Trees w/ random effects) object in the dbarts package,<machine-learning><bayesian><nonparametric-bayes>,CC BY-SA 4.0,"<p>I'm having a hard time figuring out what the output of <code>fitted()</code> applied to a <code>rbart</code> object means. Specifically, I fit my data using the <code>rbart_vi()</code> function in the R package <code>dbarts</code>, and I'm trying to understand what <code>fitted(rbartFit)</code> returns. The documentation says it returns the &quot;posterior mean of a predicted quantity&quot;. Can someone please explain what this means and how it is calculated? I don't have much background in either Bayesian stats or machine learning so am spending a lot of time figuring out but with very little progress :(
Thanks!!</p>
",2020-12-29 06:37:52.807
333674,18403.0,2,,333630.0,,,,CC BY-SA 4.0,"<p>The OLS regression of <span class=""math-container"">$Y$</span> on <span class=""math-container"">$X$</span> decomposes dependent variable <span class=""math-container"">$Y$</span> into orthogonal components
<span class=""math-container"">$$
Y = \hat{Y} + e
$$</span>
in <span class=""math-container"">$\mathbb{R}^n$</span>, where <span class=""math-container"">$\hat{Y} = X \hat{\beta}$</span> is the fitted value and <span class=""math-container"">$e$</span> the residuals.</p>
<p>It's a basic fact of linear algebra that this orthogonal decomposition remains <em>the same</em>, for all regressor matrix <span class=""math-container"">$X$</span> with the same column space. The OLS <span class=""math-container"">$\hat{\beta}$</span> may change, but <span class=""math-container"">$\hat{Y}$</span> remains the same.</p>
<p>This can be extended to a subset of regressors.</p>
<p>Suppose <span class=""math-container"">$X = [X_1 \; Z]$</span> and you replace <span class=""math-container"">$Z$</span> by <span class=""math-container"">$\tilde{Z}$</span> so that the regressors in
<span class=""math-container"">$\tilde{Z}$</span> has the same span as those in <span class=""math-container"">$Z$</span>, i.e. <span class=""math-container"">$Z = \tilde{Z} W$</span> for some invertible <span class=""math-container"">$W$</span>. Then
<span class=""math-container"">\begin{align*}
Y &amp;= X_1 \hat{\beta}_1 + Z \hat{\beta}_2 + e \\
  &amp;= X_1 \hat{\beta}_1 + \tilde{Z} W \hat{\beta}_2 + e.
\end{align*}</span>
Uniqueness of the orthogonal decomposition therefore tells you that regressing
<span class=""math-container"">$Y$</span> on <span class=""math-container"">$[X_1 \; \tilde{Z}]$</span> gives OLS estimate
<span class=""math-container"">$$
[\hat{\beta}_1 \; W \hat{\beta}_2].
$$</span>
Notice that the coefficients for <span class=""math-container"">$X_1$</span> from the two regressions remains the same---<span class=""math-container"">$\hat{\beta}_1$</span>.</p>
<p>The 2SLS example is a special case, where <span class=""math-container"">$Z$</span> consists of instruments and
<span class=""math-container"">$\tilde{Z} = \hat{X}_2$</span> are the fitted values from the first stage regression of <span class=""math-container"">$X_2$</span> on <span class=""math-container"">$Z$</span>. Note that <span class=""math-container"">$\tilde{Z}$</span> and <span class=""math-container"">$Z$</span> has the same column space if the model is exactly identified---in your notation, <span class=""math-container"">$k_2$</span> instruments for <span class=""math-container"">$k_2$</span> endogenous variables.</p>
<p>The statement is no longer true when model is over-identified---when the column space of <span class=""math-container"">$\tilde{Z}$</span> is a proper subspace of that of <span class=""math-container"">$Z$</span>.</p>
",2020-12-29 06:50:27.867
333675,24715.0,2,,333673.0,,,,CC BY-SA 4.0,"<p>I don’t have experience with this package, so for the software specific details you’d need to check the documentation or asks it’s developers, however I can help you with the “posterior mean of a predicted quantity”.</p>
<p>Unlike non-Bayesian models, in Bayesian setting we usually find not the point estimates, but their distributions. For example, in frequentist setting you would be calculating mean of a sample to estimate the mean of the population. In Bayesian setting, you would end up with a <em>distribution</em> of the possible values for the mean. Since in some cases you need a point estimate, you can use a summary statistic to reduce this <em>posterior distribution</em> to single point, for example mean, median, or mode of the posterior distribution. So the Bayesian estimator finds the distribution of the predictions, so called <em>posterior predictive distribution</em>, and the quote means that the <code>fitted</code> function returns mean of this distribution.</p>
<p>Of course, the model would make predictions for each sample, so this would be mean of the distribution predicted per each sample.</p>
<p>If this is still not clear, I’d highly recommend referring to one of the many great Bayesian textbooks before proceeding further, as otherwise it might be tough for you to understand what’s going on.</p>
",2020-12-29 07:52:53.240
333676,229148.0,2,,182063.0,,,,CC BY-SA 4.0,"<p>Thank @djs for the great answer. Agreeing majority of it, but maybe not the last part. (Had to post another answer due to lack of reputation to comment directly.)</p>
<blockquote>
<p>Another interesting property: suppose that there are three categories, with the first one being correct. Cross-entropy would value the predictions <span class=""math-container"">$(.8, .2, 0)$</span> and <span class=""math-container"">$(.8, .1, .1)$</span> equally, whereas Brier loss would prefer the second one. I don't know if that's of huge practical importance, but only caring about the true category seems like a reasonable criterion to me, and that leads to cross-entropy being the only proper scoring rule.</p>
</blockquote>
<p>IMO, caring about false categories is actually a valuable feature. In <a href=""https://en.wikipedia.org/wiki/Knowledge_distillation"" rel=""nofollow noreferrer"">knowledge distillation</a>, utilizing predictions of false categories (so-called &quot;dark knowledge&quot;) is one of the underlying principles.</p>
<p>For three categories (dog, cat, car), suppose the true label is <strong>&quot;dog&quot;</strong>, a prediction <span class=""math-container"">$(.8, .2, 0)$</span> is obviously better than a prediction <span class=""math-container"">$(.8, .1, .1)$</span> , because <strong>&quot;car&quot;</strong> is no where near <strong>&quot;dog&quot;</strong> and <span class=""math-container"">$0.1$</span> prediction for <strong>&quot;car&quot;</strong> is hardly reasonable.</p>
<p>Nonetheless, this doesn't make MSE a better loss function for classification. Cross entropy is still preferred.</p>
",2020-12-29 08:51:11.887
8478,2269.0,1,8504.0,,,Kullback–Leibler vs Kolmogorov-Smirnov distance,<distributions><distance-functions><kolmogorov-smirnov-test><kullback-leibler>,CC BY-SA 2.5,"<p>I can see that there are a lot of formal differences between Kullback–Leibler vs Kolmogorov-Smirnov distance measures.
However, both are used to measure the distance between distributions.</p>

<ul>
<li>Is there a typical situation where one should be used instead of the other? </li>
<li>What is the rationale to do so?</li>
</ul>
",2011-04-07 11:39:13.870
333678,255041.0,1,333841.0,,,Is there an issue using an imbalanced covariate (not dependent) in logistic regression?,<regression><logistic><unbalanced-classes><clinical-trials>,CC BY-SA 4.0,"<p>I am investigating data from a randomised control trial, where treatment allocation is done on a 2:1 ratio (2 patients on the experimental treatment for every 1 patient on placebo). 400 on experimental treatment and 200 on placebo, for example.</p>
<p>I am conducting some exploratory analysis to investigate if there are any significant interactions between the treatment term (binary covariate) and other selected covariates, when using death (binary dependent variable) as the outcome.</p>
<p>As mentioned, the trial has 2:1 randomisation, so there is an imbalance in the number of patients on the experimental drug and placebo.</p>
<p>My plan is to build a logistic regression model with death as the dependent variable. The model will include a treatment term and other relevant covariates (selected using AIC) and also any relevent interactions involving the treatment term.</p>
<p><strong>My question surrounds the imbalance in the treatment allocation</strong> (<em>Note: not the imbalance in the dependent variable death</em>): Does the fact that the treatment arms are unequal (400 experimental, 200 placebo) have any impact on the conclusions I can draw from the logistic model? I have been led to believe that the treatment imbalance leads to differing variance in each treatment arm (np(1-p)) - is this really a problem? If so, can it be solved?</p>
<p>I have considered using upsampling to balance the treatment arms. Is there anything such as weighted logistic regression or conditional logistic regression that would be suitable?</p>
<p>I realise there has been much discussion surrounding unbalanced datasets, however the discussion is usually focused on imbalance in the dependent variable: <a href=""https://stats.stackexchange.com/questions/357466/are-unbalanced-datasets-problematic-and-how-does-oversampling-purport-to-he"">Are unbalanced datasets problematic, and (how) does oversampling (purport to) help?</a></p>
",2020-12-29 10:08:25.933
333679,255041.0,2,,333677.0,,,,CC BY-SA 4.0,"<p>The key points here are that we are told (1) that the coin is fair and (2) that each flip is completely random and independent from the previous flip. If these two conditions are met, then the probability of the next flip being tails is 1/2. If the conditions are NOT met, then the probability is unlikely to be 1/2.</p>
<p>If we didn't know if the coin was fair, and we observed 26 heads in a row, suspiciousness should be fully aroused. This would be very strong evidence that the coin is not fair. Betting on heads would be the thing to do. (In your example, we assume the coin is fair so this does not apply here)</p>
<p>As Xi'an commented, the fact that each toss is independent means that the &quot;history&quot; is complete irrelevant. If the flips are not independent, then the history could give us information about the next flip. (Again this does not apply to your question because we are assuming independent flips.)</p>
",2020-12-29 10:36:34.770
333680,255139.0,1,,,,Nested fixed effects in the lme4 package,<mixed-model><lme4-nlme><random-effects-model><fixed-effects-model><nested-models>,CC BY-SA 4.0,"<p>I have browsed the internet for an answer to my question but have failed in my attempts. My experimental design includes nested fixed effects and I do not know how to specify this in my model in the lme4 package or if it is even possible.</p>
<p>For my experiment I have collected seeds from Festuca rubra growing on Iceland. Across Iceland a band of vulcanic activity streaches, that heats the soil making these areas at lot warmer than the rest of the island. Seeds were collected from 5 heated localities and 5 non-heated, making it 10 different populations, 5 heated and 5 non-heated. However, the localities are very different making it hard to argue that the populations are replicates. I want to know if the if the variation is larger between populations growing on the same soil-type or between populations growing on different soil-types in regards to survival, biomass productions etc.</p>
<p>Below is a sketch of my experimental design</p>
<p><a href=""https://i.stack.imgur.com/0A8dl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0A8dl.png"" alt=""enter image description here"" /></a></p>
<p>My random effect is block nested in treatment 1, my fixed effects are treatment 1 and population nested within treatment 2, is it possible to express this in a model?</p>
<p>(survival~treatment1+?treatment2/population?+(1|treatment2/block)</p>
",2020-12-29 10:45:48.530
333682,35995.0,2,,333681.0,,,,CC BY-SA 4.0,"<p>The quantile regression estimators for different quantiles are asymptotically distributed as a multivariate normal random vector with a certain mean vector and a certain covariance matrix, the expressions of which are available in the literature; see <a href=""https://methods.sagepub.com/book/quantile-regression"" rel=""nofollow noreferrer"">Hao &amp; Naiman (2007)</a> Chapter 4 (short and to the point), <a href=""https://www.wiley.com/en-us/Quantile+Regression%3A+Theory+and+Applications-p-9781119975281"" rel=""nofollow noreferrer"">Davino et al. (2013)</a> Chapter 3 or <a href=""https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1"" rel=""nofollow noreferrer"">Koenker (2005)</a> Chapters 3.3 and 4, among other. Hence, testing the hypothesis that quantile regression slopes are equal for two or more different quantiles is a relatively straightforward task. As noted in <a href=""https://methods.sagepub.com/book/quantile-regression"" rel=""nofollow noreferrer"">Hao &amp; Naiman (2007)</a> Chapter 4, the covariance matrix can be tedious to calculate analytically; simple bootstrap of the original data (not of regression residuals) is an easier alternative; <a href=""https://methods.sagepub.com/book/quantile-regression"" rel=""nofollow noreferrer"">Hao &amp; Naiman (2007)</a> Chapter 4 give a detailed account of it.</p>
<p><strong>References</strong></p>
<ul>
<li>Davino, C., Furno, M., &amp; Vistocco, D. (2013). <a href=""https://www.wiley.com/en-us/Quantile+Regression%3A+Theory+and+Applications-p-9781119975281"" rel=""nofollow noreferrer""><em>Quantile Regression: Theory and Applications</em></a> (Vol. 988). John Wiley &amp; Sons.</li>
<li>Hao, L. &amp; Naiman, D. Q. (2007). <a href=""https://methods.sagepub.com/book/quantile-regression"" rel=""nofollow noreferrer""><em>Quantile Regression</em></a> (No. 149). SAGE Publications.</li>
<li>Koenker, R. (2005). <a href=""https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1"" rel=""nofollow noreferrer""><em>Quantile Regression</em></a>. Cambridge University Press.</li>
</ul>
",2020-12-29 10:54:59.577
333681,35995.0,1,,,,Testing equality of quantile regression slopes at different quantiles,<hypothesis-testing><quantile-regression><multivariate-regression>,CC BY-SA 4.0,"<p>How do I test if the quantile regression slopes are equal for different quantiles?</p>
<p>E.g. I run a quantile regression at 5% quantile, 50% quantile (median) and 95% quantile and obtain the slope estimates <span class=""math-container"">$(\hat\beta^{QR}_{0.05},\hat\beta^{QR}_{0.50},\hat\beta^{QR}_{0.95})$</span>. It is highly unlikely all three numbers will be exactly the same even if the true slopes are equal. So how do I test <span class=""math-container"">$H_0\colon \beta^{QR}_{0.05}=\beta^{QR}_{0.50}=\beta^{QR}_{0.95}$</span>?</p>
",2020-12-29 10:54:59.577
333683,64455.0,1,,,,Belief propagation on Polytree,<bayesian-network><belief-propagation>,CC BY-SA 4.0,"<p>I'm working through exercises on Belief Propagation and the Junction Tree Algorithm and I'm stuck with the following problem. Consider the distribution P(A,B,C,D,E,F,G,H)=P(A)P(B)P(C)P(F) P(D|A,B)P(E|B,C)P(G|D,F)P(H|E,F), i.e.</p>
<p><a href=""https://i.stack.imgur.com/3OTHE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3OTHE.png"" alt=""enter image description here"" /></a></p>
<p>The questions asked are:</p>
<ul>
<li>Is it possible to use belief propagation on this graph (without forming a Junction Tree) to compute P(B|G,H)?</li>
<li>What about P(B|F,G)</li>
<li>Now assume only F has been observed. How would you compute P(B|F)?</li>
</ul>
<p>Now, AFAIK this is a directed polytree (Nodes may have multiple parents, but there is at most a single path between any two nodes). Thus Belief Propagation should allow exact inference in this tree, which is why I would answer &quot;Yes&quot; to both 1) and 2). Is this correct? It seems like I'm missing something here. Why would conditioning change something about whether Belief Propagation can be used here?</p>
<p>Regarding 3, we note that B is cond. indep. of F given no other nodes, so P(B) = P(B|F), is this correct?</p>
",2020-12-29 11:10:45.593
333734,158099.0,2,,333710.0,,,,CC BY-SA 4.0,"<blockquote>
<p>After skimming some Bayesian Statistics textbooks, I came to the
conclusion that the standard Bayesian approach would be to take
:<span class=""math-container"">$$[_()]≥2/3$$</span> Still, I am not sure about this.</p>
</blockquote>
<p>Correct. We want the probability of the next toss being heads greater than or equal to <span class=""math-container"">$2/3$</span>. This means
<span class=""math-container"">$$P(H|\mathcal D)=\int P(H|\theta)\pi(\theta|\mathcal D)d\theta=\int \theta \pi(\theta|\mathcal D)d\theta = E[\theta|\mathcal D]$$</span>
which is the posterior expectation. <span class=""math-container"">$\mathcal D$</span> represents the data.</p>
",2020-12-29 21:25:12.480
333684,255143.0,1,,,,Error with Fuzzy Tree model in Matlab,<matlab><fuzzy>,CC BY-SA 4.0,"<p>I create a fuzzy tree (aggregated structure) in Matlab and I have an error which I hope to be supported.
My fuzzy tree has 3 fuzzy models (fis1, fis2, fis3). <strong>fis1</strong> is the tipper1 and it has 2 inputs: service and food. <strong>fis2</strong> is tipper2 and it has 2 input: service and food. <strong>Outputs of fis1 and fis2 are Inputs of fis3</strong>. However, when I run it, the error is <strong>“Error using FuzzyInferenceSystem/addRule (line 1148). Invalid input membership function name in rule description.”</strong>
I think maybe error comes because I don’t create membership function for inputs of fis3. But I have already created membership function for outputs of fis1 and fis2, so if I create membership function for fis3, the inputs of fis3 will have 2 times of membership function. Could you support me with this issue? Thank you.</p>
<p><a href=""https://i.stack.imgur.com/IBnFN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IBnFN.png"" alt=""Aggregated fuzzy tree "" /></a></p>
<pre><code>% Fuzzy model 1
fis1 = mamfis('Name',&quot;tipper1&quot;);
fis1 = addInput(fis1,[0 10],'Name',&quot;service&quot;);
fis1 = addMF(fis1,&quot;service&quot;,&quot;gaussmf&quot;,[1.5 0],'Name',&quot;poor&quot;);
fis1 = addMF(fis1,&quot;service&quot;,&quot;gaussmf&quot;,[1.5 5],'Name',&quot;good&quot;);
fis1 = addMF(fis1,&quot;service&quot;,&quot;gaussmf&quot;,[1.5 10],'Name',&quot;excellent&quot;);
fis1 = addInput(fis1,[0 10],'Name',&quot;food&quot;);
fis1 = addMF(fis1,&quot;food&quot;,&quot;trapmf&quot;,[-2 0 1 3],'Name',&quot;rancid&quot;);
fis1 = addMF(fis1,&quot;food&quot;,&quot;trapmf&quot;,[7 9 10 12],'Name',&quot;delicious&quot;);
fis1 = addOutput(fis1,[0 30],'Name',&quot;tip1&quot;);
fis1 = addMF(fis1,&quot;tip1&quot;,&quot;trimf&quot;,[0 5 10],'Name',&quot;cheap&quot;);
fis1 = addMF(fis1,&quot;tip1&quot;,&quot;trimf&quot;,[10 15 20],'Name',&quot;average&quot;);
fis1 = addMF(fis1,&quot;tip1&quot;,&quot;trimf&quot;,[20 25 30],'Name',&quot;generous&quot;);
rulefis1a = &quot;service==poor | food==rancid =&gt; tip1=cheap&quot;;
rulefis1b = &quot;service==good =&gt; tip1=average&quot;;
rulefis1c = &quot;service==excellent | food==delicious =&gt; tip1=generous&quot;;
rulefis1d = [rulefis1a rulefis1b rulefis1c];
fis1b = addRule(fis1,rulefis1d);
 
%Fuzzy model 2
fis2 = mamfis('Name',&quot;tipper2&quot;);
fis2 = addInput(fis2,[0 10],'Name',&quot;service&quot;);
fis2 = addMF(fis2,&quot;service&quot;,&quot;gaussmf&quot;,[1.5 0],'Name',&quot;poor&quot;);
fis2 = addMF(fis2,&quot;service&quot;,&quot;gaussmf&quot;,[1.5 5],'Name',&quot;good&quot;);
fis2 = addMF(fis2,&quot;service&quot;,&quot;gaussmf&quot;,[1.5 10],'Name',&quot;excellent&quot;);
fis2 = addInput(fis2,[0 10],'Name',&quot;food&quot;);
fis2 = addMF(fis2,&quot;food&quot;,&quot;trapmf&quot;,[-2 0 1 3],'Name',&quot;rancid&quot;);
fis2 = addMF(fis2,&quot;food&quot;,&quot;trapmf&quot;,[7 9 10 12],'Name',&quot;delicious&quot;);
fis2 = addOutput(fis2,[0 30],'Name',&quot;tip2&quot;);
fis2 = addMF(fis2,&quot;tip2&quot;,&quot;trimf&quot;,[0 5 10],'Name',&quot;cheap&quot;);
fis2 = addMF(fis2,&quot;tip2&quot;,&quot;trimf&quot;,[10 15 20],'Name',&quot;average&quot;);
fis2 = addMF(fis2,&quot;tip2&quot;,&quot;trimf&quot;,[20 25 30],'Name',&quot;generous&quot;);
rulefis2a = &quot;service==poor | food==rancid =&gt; tip2=cheap&quot;;
rulefis2b = &quot;service==good =&gt; tip2=average&quot;;
rulefis2c = &quot;service==excellent | food==delicious =&gt; tip2=generous&quot;;
rulefis2d = [rulefis2a rulefis2b rulefis2c];
fis2b = addRule(fis2,rulefis2d);

%Fuzzy model 3
fis3 = mamfis('Name','fis3','NumInputs',2,'NumOutputs',1);
fis3.Outputs(1).Name = &quot;final_tip&quot;;
con1 = [&quot;tipper1/tip1&quot; &quot;fis3/input1&quot;];
con2 = [&quot;tipper2/tip2&quot; &quot;fis3/input2&quot;];
fis3 = addMF(fis3,&quot;final_tip&quot;,&quot;trapmf&quot;,[-2 0 1 3],'Name',&quot;rancid&quot;);
fis3 = addMF(fis3,&quot;final_tip&quot;,&quot;trapmf&quot;,[7 9 10 12],'Name',&quot;delicious&quot;);
rulefis3a = &quot;input1==cheap | input2==average =&gt; final_tip=rancid&quot;;
rulefis3b = &quot;input1==average =&gt; final_tip=rancid&quot;;
rulefis3c = &quot;input1==generous | input2==generous =&gt; final_tip=delicious&quot;;
rulefis3d = [rulefis3a rulefis3b rulefis3c];
fis3b = addRule(fis3,rulefis3d);
 
aggTree = fistree([fis1b fis2b fis3b],[con1;con2]);
output = evalfis(aggTree,[0.2 0.25 0.3 0.2]);
</code></pre>
",2020-12-29 11:20:20.850
333685,255147.0,1,,,,Use of ignore_index on CrossEntropyLoss() for text models,<machine-learning><natural-language><recurrent-neural-network><cross-entropy><language-models>,CC BY-SA 4.0,"<p>I have been using PyTorch's <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss"" rel=""nofollow noreferrer"">CrossEntropyLoss()</a> on a Language Autoencoder. I noticed that most people use ignore_index for ignoring the pad token in loss calculation eg <a href=""https://github.com/shentianxiao/text-autoencoders/blob/a096cbe0710ca4ef7b1d123006394a4401a98108/model.py#L98"" rel=""nofollow noreferrer"">this</a>.</p>
<p>From what I understand whenever the Label value is 0 (Corresponding to padding) it will not add to the loss irrespective of what the predicted value is. I have experienced experimentally is that it starts producing random values after the seq(where there should be ideally padding) when using ignore index, which makes sense as it doesnt add to loss. It outputs pad token only when not using it. So why include it, does it help training or make the model more robust?</p>
",2020-12-29 11:43:46.747
333686,35995.0,2,,333536.0,,,,CC BY-SA 4.0,"<p>What's wrong with a constant forecast? Would you take a nonconstant one even if its accuracy were lower? You responded, no, and I concur. Perhaps your data generating process (DGP) is best approximated by ARIMA(0,1,0) or ARIMA(0,0,0) among the class of models considered, ARIMA(p,d,q). If the DGP is a random walk, i.e. ARIMA(0,1,0), or white noise, i.e. ARIMA(0,0,0), previous observations will not be constant, yet an optimal (under symmetric loss) point forecast will be a constant.</p>
<p>If you were worried about possibly obtaining negative forecasts, you could refer to the thread <a href=""https://stats.stackexchange.com/questions/80859/how-to-achieve-strictly-positive-forecasts"">&quot;How to achieve strictly positive forecasts?&quot;</a>. The core idea there is to set <code>lambda=0</code> when fitting a model with <code>auto.arima</code>: <code>fit &lt;- auto.arima(x, lambda=0); forecast(fit)</code>. But your series values and their forecasts appear quite far away from zero, so this is probably not a problem.</p>
<p>If you wanted integer forecasts, Weiss <a href=""https://onlinelibrary.wiley.com/doi/10.1002/9781118445112.stat08133"" rel=""nofollow noreferrer"">&quot;Integer‐Valued Autoregressive Moving‐Average (INARMA) Models&quot;</a> (2018) could be of interest. But again, this is probably more relevant when the numbers are small, which is not the case in your example.</p>
",2020-12-29 12:12:46.880
333687,2958.0,2,,333668.0,,,,CC BY-SA 4.0,"<p>There isn't any hard requirement here, but some considerations that may lead to equally sized sets in the end.</p>
<ul>
<li><p>The largest difference is between training vs. validation/test/optimization/verification:</p>
<ul>
<li>for training, as a rule of thumb the ratio training sample size : model complexity/variates/degrees of freedom counts. In other words, we can adapt to smaller training sample sizes (to some extent).</li>
<li>In contrast, the precision of any kind of testing (whether the results will be used for optimization or as final estimate of generalization error) depends on the <em>absolute</em> number of tested cases.</li>
</ul>
<p>As a side note: any general recommendation of a split ratio without looking at the acutal sample size (and expected model complexity) therefore doesn't make too much sense.</p>
</li>
<li><p>The &quot;absolute number&quot; consideration thus applies to both validation (aka optimization) and test (aka verification) sets, and this may be a first reason why it can make sense to have them both the same size.</p>
</li>
<li><p>One may argue that optimization requires more precise performance estimates since we typically compare multiple (many) models. Which would boil down to larger validation/optimization set than test/verification set.</p>
</li>
<li><p>OTOH, few people will be willing to accept a low precision final estimate of generalization error.</p>
</li>
<li><p>And the optimization &quot;wrongly&quot; picking only an almost-optimal rather than <em>the</em> optimal model won't hurt much in practice: if the chosen model does not have sufficient performance, we'll see that in the test/verification and typically need to reconsider our modeling in a more thorough manner. If test/verification (or rather, an actual validation) shows that the optimized model is fit for purpose, we don't care whether it is only almost optimal. It is sufficient, and that's all we really need.<br />
So we may get away with somewhat lower precision in the optimization.</p>
</li>
<li><p>Last but not least, if you do a cross validation for any of the two testing steps, its sample size will be the whole data set available at that stage since the test results are pooled across the folds. From a precision point of view, that's the best you can get wrt. the given size of your data set.</p>
</li>
</ul>
<hr />
<p>In my experience and with the data I see (I'm typically in small to very small sample size situations), the important point is to make sure there are sufficient cases to derive meaningful interpretations from the test results. After that, I'm fine with somewhat handwavy pragmatic decisions.</p>
<p>Here's a paper we wrote about these thoughts:<br />
<a href=""http://dx.doi.org/10.1016/j.aca.2012.11.007"" rel=""nofollow noreferrer"">Beleites, C. <em>et al.</em>: Sample size planning for classification models. Anal Chim Acta, 2013, 760, 25-33.
DOI: 10.1016/j.aca.2012.11.007</a><br />
<a href=""http://arxiv.org/abs/1211.1323"" rel=""nofollow noreferrer"">accepted manuscript on arXiv: 1211.1323</a></p>
",2020-12-29 12:46:40.853
333688,255140.0,1,,,,"The book Probability Theory by Klenke, Exercise 1.2.1, I wonder if it’s an error?",<probability><mathematical-statistics><measure-theory>,CC BY-SA 4.0,"<p>The book <em>Probability Theory</em> by Klenke has for Exercise 1.2.1 the following:</p>
<blockquote>
<p>Let <span class=""math-container"">$$A =\{(a,b]\cap \mathbb Q : \ a,b\in\mathbb R, \ a\le b\}.$$</span> Define <span class=""math-container"">$$\mu: A \to [0,\infty)$$</span> by <span class=""math-container"">$$ \mu((a,b]\cap \mathbb Q) =b−a.$$</span> Show that <span class=""math-container"">$A$</span> is a semiring and <span class=""math-container"">$\mu$</span> is a content on <span class=""math-container"">$A$</span> that is lower and upper semicontinuous but is not <span class=""math-container"">$\sigma$</span>-additive.</p>
</blockquote>
<p>I come to the conclusion that <span class=""math-container"">$\mu$</span> is <span class=""math-container"">$\sigma$</span>-additive.  Mostly because all my attempts to think of an example of failure to be <span class=""math-container"">$\sigma$</span>-additive bear no fruit.</p>
<p>Can someone help me prove it one way or the other?</p>
",2020-12-29 13:17:37.847
333689,86926.0,1,,,,Interpretation of p values for each level of a covariate?,<anova><survival><p-value><interpretation><cox-model>,CC BY-SA 4.0,"<p>In the output from a cox regression I get several p values from one variate - one for each level of the variate bar one level. I want to know the effect of the variate on the outcome (adjusted by other variates) and therefore one p value would suffice. (I can't seem to use anova because I'm using the R survival function survSplit - although that would be good). I am therefore stuck with trying to interpret the p values for individual levels of the variate.</p>
<p>My question is - what exactly do they mean ? - and when do I know that the variate has a significant effect on the outcome ? Do all the p values for each level of the variate have to be significant to conclude that the variate has a significant effect ?</p>
<p>The p values seem to be calculated with reference to one of the levels of the variate (presumably treated as a reference level) - but is that useful ? As an example the outcome could be death with genetic-time groups given by survSplit and the variate of interest could be an ordinal variable e.g. the nodal status of a patient.</p>
<p>In regression I normally think of whole variates, not different levels, as having a regression coefficient. Does that mean that here each level of a variate has its own regression coefficient which is independent of all levels apart from the reference level ? and if it is significant it can be interpreted as having an association with the outcome/time (=hazard) that is relative to the reference level's association with the outcome/time ? (note that this doesn't seem very useful if I don't know what the reference level's association with outcome/time actually is !?)</p>
<p>Unfortunately anova.rms and base anova will not work directly (at least with my data) on one R survival coxph(survSplit) object, presumably because the groups:strata(time_group) parameter which has two levels always gives one of these levels (for each time) with a line of NAs and se(coef) of zero: as in the following results from model</p>
<pre><code>coxph(Surv(tstart, time, BCSSsplit) ~ groupsplit:strata(time_group) + Nodal_status_CATEGOR_CATEGOR, data = BCSSsplitdata)                                                            
                                                 se(coef)      z        p
Nodal_status_CATEGOR_CATEGORa                   2.959e-01  2.071   0.0384
Nodal_status_CATEGOR_CATEGORb                   2.698e-01  5.643 1.67e-08
Nodal_status_CATEGOR_CATEGORc                   2.964e+03 -0.005   0.9959
groupsplitHer2+:strata(time_group)time_group=1  2.774e-01 -4.112 3.92e-05
groupsplitTNBC:strata(time_group)time_group=1   0.000e+00     NA       NA
groupsplitHer2+:strata(time_group)time_group=2  3.930e-01  2.563   0.0104
groupsplitTNBC:strata(time_group)time_group=2   0.000e+00     NA       NA
</code></pre>
<p>The error given is <code>Error in .rowNamesDF&lt;-(x, value = value) : invalid 'row.names' length</code></p>
<p>anova will however compare two such objects - so I'll have to settle for that unless someone knows a method whereby anova will handle such a coxph object. (The time split (or other adjustment) is necessary following Schoenfeld testing for proportional hazards.)</p>
",2020-12-29 13:29:38.540
333735,255189.0,1,,,,Confidence intervals for fitting parameters,<regression><confidence-interval><fitting><uncertainty>,CC BY-SA 4.0,"<p>I have a problem with how confidence intervals are defined in Bayesian linear regression, as it seems to lead to an nonsensical result.</p>
<p>Say that I have data set <span class=""math-container"">$(x_i,y_i)$</span> where <span class=""math-container"">$y_i \sim \mathcal{N}(\beta_0 + x_i\beta_1,\,\sigma^{2}) = \mathcal{N}(X\beta,\,\sigma^{2})$</span>, where the <span class=""math-container"">$i$</span>-th column of <span class=""math-container"">$X$</span> is <span class=""math-container"">$[1, x_i]$</span> and <span class=""math-container"">$\beta$</span> is the vector of fitting parameters. Let's suppose for now that <span class=""math-container"">$\sigma$</span> is known.</p>
<p>The likelihood function for <span class=""math-container"">$\beta$</span> is then: <span class=""math-container"">$$L(\beta) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}}$$</span></p>
<p>The maximum-likelihood values of <span class=""math-container"">$\beta$</span> are given as <span class=""math-container"">$\beta =(X^{T}X)^{-1}X^{T}y$</span>, while the variance is given by <span class=""math-container"">$Var(\beta) = \sigma^2 (X^{T}X)^{-1}$</span> (see e.g. <a href=""https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares"" rel=""nofollow noreferrer"">here</a>).</p>
<p>In this case the previous general result simplifies to: <span class=""math-container"">$$Var(\beta)=\sigma ^{2}\frac{1}{n\sum {(x_{i}-{\bar {x}})^{2}}}\begin{pmatrix}\sum x_{i}^{2}&amp;-\sum x_{i}\\-\sum x_{i}&amp;n\end{pmatrix}$$</span>
where <span class=""math-container"">$n$</span> is the number of data-points in the set.</p>
<p>Moreover, if <span class=""math-container"">$\sum x_i=0$</span> the off-diagonal terms drop off and we have <span class=""math-container"">$Var(\beta_0) = \dfrac{\sigma^2}{n}$</span> and <span class=""math-container"">$Var(\beta_1) = \dfrac{\sigma^2}{\sum x_i^2}$</span>.</p>
<p>While at first glance this seems ok (it gives the <span class=""math-container"">$\sim \frac{1}{\sqrt{N}}$</span> for the uncertainty) it can lead to some strange conclusions.</p>
<p>As en experimentalist I would never trust an intercept with an uncertainty  much lower than the data from which it was determined. Also, if I have a data-set with a sufficient number of points (to capture all of it's features) I don't see how adding more equally &quot;bad&quot; points should help.
As an illustration for these two points have a look at the following graph, where <span class=""math-container"">$y_i \sim \mathcal{N}(3 + 2x_i,\,\sigma=0.4)$</span>:</p>
<p><a href=""https://i.stack.imgur.com/QrfRY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QrfRY.png"" alt=""a graph"" /></a></p>
<p>Lastly we can imagine a paradoxical situation. Imagine that I perform a (noisy) measurement (e.g. measure the mains voltage) <span class=""math-container"">$n$</span> times, and assign an <span class=""math-container"">$x$</span> value to each of the measurements (such that <span class=""math-container"">$\sum x =0$</span>!). These are, by design, uncorrelated, and by doing a &quot;linear fit&quot; I can determine actual voltage with an uncertainty by a factor of <span class=""math-container"">$\frac{1}{\sqrt{n}}$</span> lower than what my instrument is capable of (which is given simply by the standard deviation of the data).</p>
<p>What I'm missing here???</p>
<p>The actual problem that I'm trying to solve is a bit more nuanced than this, but the gist is the same: how to properly determine the error bars for some fitting parameters, when different datasets are of different size but of the same &quot;quality&quot;.</p>
<p>I'm not a mathematician so the language used might seem a bit &quot;off&quot;.</p>
<p>Thanks in advance!</p>
",2020-12-29 21:44:10.457
172031,9483.0,2,,172027.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>should I vectorize them into one hot then split into train/test or split the data set randomly then vectorize into one hot?</p>
</blockquote>

<p>That makes no difference. If you split first, make sure your vectorize the same way.</p>
",2016-12-16 00:01:43.807
333690,213075.0,1,,,,Am I understanding critical value and p-value correctly?,<machine-learning><hypothesis-testing><mathematical-statistics>,CC BY-SA 4.0,"<p>Assume I created a program that plays tic-tac-toe, and measured its preformance over 100 game (1 point for win, 0.5 for draw, 0 otherwise). Afterwards, I made some improvements and a friend made the following hypothesis.</p>
<blockquote>
<p>The new version will score on average 0.3 points more per game.</p>
</blockquote>
<p>I need to explain how I would test this hypothesis using p-value and critical value. Here are my attempts:</p>
<ol>
<li><p>Critical value - playing several games and calculating the average points, if the average turns out to be greater than the cut-off (i.e. critical value) then we can accept the hypothesis.</p>
</li>
<li><p>p-value - We play several games and calculate the average number of points. We then calculate the p-value -using 1% as a threshold for example- if the p-value is below the threshold, we reject the null hypothesis.</p>
</li>
</ol>
<p>Any mistakes or improvements I can make? Thanks</p>
",2020-12-29 13:34:34.663
333691,226509.0,1,,,,Not rejecting null-hypothesis and inflation of type I error,<hypothesis-testing><statistical-significance><anova>,CC BY-SA 4.0,"<h2>Context</h2>
<p>In statistics, it is quite common to perform statistical tests in the explicit goal of <em>not-rejecting</em> the null hypothesis. For example, prior to conducting an ANOVA, one will check for homogeneity of variances (F-test, Bartlett Test).</p>
<p>I have the feeling that doing so is quite dishonest since each of these tests always have unknown type II error and this is similar to saying : &quot;My assumption is that I am a unicorn, prove me wrong&quot; while the burden of proof should be on claiming that &quot;variances are similar&quot; and not the other way around.</p>
<h2>Problem</h2>
<p>If one wants to test that Group A is different than Group B and Group C and thus follows this general routine :</p>
<ol>
<li>Test for homogeneity of variances and do not reject H0 (type II error unknown)</li>
<li>ANOVA test is statistically significant ( .05 level)</li>
</ol>
<p>Considering that unknown type II error, can we really say that the Type I error is only 5 % ?</p>
",2020-12-29 13:44:25.503
333693,35995.0,5,,,,,,CC BY-SA 4.0,"<p>Decision theory is the science of making optimal decisions in the face of uncertainty. Statistical decision theory is concerned with the making of decisions when in the presence of statistical knowledge (data) which sheds light on some of the uncertainties involved in the decision problem.<span class=""math-container"">$^1$</span></p>
<p>Alternatively, a general theory for the processing and use of statistical observations. In a broader interpretation of the term, statistical decision theory is the theory of choosing an optimal non-deterministic behaviour in incompletely known situations.<span class=""math-container"">$^2$</span></p>
<p><span class=""math-container"">$^1$</span> Berger <a href=""https://link.springer.com/chapter/10.1007/978-1-349-20181-5_26"" rel=""nofollow noreferrer"">&quot;Statistical Decision Theory&quot;</a> in Eatwell, Milgate &amp; Newman (Eds.) &quot;Game Theory&quot; (1989).<br />
<span class=""math-container"">$^2$</span> <a href=""https://encyclopediaofmath.org/wiki/Statistical_decision_theory"" rel=""nofollow noreferrer"">Encyclopaedia of Mathematics</a>.</p>
",2020-12-29 14:05:25.367
333692,35995.0,4,,,,,,CC BY-SA 4.0,Decision theory is the science of making optimal decisions in the face of uncertainty. Statistical decision theory is concerned with the making of decisions when in the presence of statistical knowledge (data) which sheds light on some of the uncertainties involved in the decision problem.,2020-12-29 14:05:25.367
333694,169239.0,1,,,,How to treat data for Bayesian VARs,<time-series><bayesian><vector-autoregression><kalman-filter><time-varying-covariate>,CC BY-SA 4.0,"<p>I am working on a project where I need to implement a time varying structural VAR. From my understanding this uses the Kalman filter and is basically a Bayesian tool.</p>
<p>From frequentist time series analysis I know that you would usually make your data stationary and test for cointegration. And then after building a model I would test for serial correlation and normality in the residuals. Is any or all of that necessary for Bayesian analysis?</p>
<p>I am relatively new to any time series analysis, let alone the Bayesian stuff. So if any other information is required to answer this question please let me know.</p>
",2020-12-29 14:05:25.400
333696,35995.0,5,,,,,,CC BY-SA 4.0,"<p>In information theory, the cross-entropy between two probability distributions <span class=""math-container"">$p$</span> and <span class=""math-container"">$q$</span> over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution <span class=""math-container"">$q$</span>, rather than the true distribution <span class=""math-container"">$p$</span>.</p>
<p>The cross-entropy of the distribution <span class=""math-container"">$q$</span> relative to a distribution <span class=""math-container"">$p$</span> over a given set is defined as follows:
<span class=""math-container"">$$
H(p,q)=-\mathbb{E}_p(\log q)
$$</span>
where <span class=""math-container"">$\mathbb{E}_p(\cdot)$</span> is the expected value operator with respect to the distribution <span class=""math-container"">$p$</span>.</p>
<p>Source: <a href=""https://en.wikipedia.org/wiki/Cross_entropy"" rel=""nofollow noreferrer"">Wikipedia</a>.<br />
Excerpt source: Brownlee <a href=""https://machinelearningmastery.com/cross-entropy-for-machine-learning/#:%7E:text=Cross%2Dentropy%20is%20a%20measure%20of%20the%20difference%20between%20two,encode%20and%20transmit%20an%20event."" rel=""nofollow noreferrer"">&quot;A Gentle Introduction to Cross-Entropy for Machine Learning&quot;</a> (2019).</p>
",2020-12-29 14:17:20.190
333695,35995.0,4,,,,,,CC BY-SA 4.0,A measure of the difference between two probability distributions for a given random variable or set of events.,2020-12-29 14:17:20.190
333697,19001.0,2,,333613.0,,,,CC BY-SA 4.0,"<p>The problem is that an image doesn't &quot;represent&quot; state -- it doesn't have information about the motion of objects in cartpole. If you don't make motion part of your state, then you don't have an MDP anymore -- it's not markov. So basically, whatever way you choose to represent &quot;state&quot;, you have to make sure you end up with an MDP.</p>
<p>Presumably, taking the difference of two consecutive frames is enough to provide velocity information and make it an MDP. Stacking N frames (possibly N&gt;2) is another common way to do this.</p>
",2020-12-29 14:54:19.233
333698,136436.0,2,,333601.0,,,,CC BY-SA 4.0,"<p>I would be interested to hear more information about what your inputs are, and what you mean by &quot;added a bias by summing&quot;. If you really just added some simple term (without cheating by looking at actuals), and your metric improved, then there should be a way to improve the model, maybe by adding complexity.</p>
<p>However, the behavior you see might just be unavoidable due to natural noise in the output variable. Imagine you are predicting height from age and gender. Then there is going to be some natural variation in height above what can be described by those simple inputs. So all the super tall people are going to be underestimated, and all the super short people are going to be overestimated.</p>
<p>In other words, you can't correct for this because real value is not an input the model can see. Try plotting the residual error against each of your inputs on the x-axis one at a time. If you see similar weak points or biases in these plots, then there could be an opportunity to fix things.</p>
",2020-12-29 14:55:54.640
333699,251402.0,1,,,,Better to use wald test or likelihood ratio test to make pairwise comparisons after omnibus test in this scenario,<regression><logistic><post-hoc><genetics>,CC BY-SA 4.0,"<p>I am testing the association between a gene and a binary disease. The gene has many different &quot;versions&quot;. These versions are called alleles.I am also including covariates for sex, age, etc.</p>
<p>Right now I am doing a logistic regression-based omnibus test for the gene like this (pseudocode):</p>
<pre><code>full&lt;- &quot;disease ~ sex + age + allele1 + allele2 + allele3&quot; 
null&lt;- &quot;disease ~ sex + age&quot; 
anova(null, full, test='Chisq')
</code></pre>
<p>I think I realize I could follow omnibus with the wald test to determine how significant each allele is, but I am wondering if this is best done with the LR test, which would allow me to account for the covariates on each allele comparisons, like this:</p>
<pre><code>full1&lt;- &quot;disease ~ sex + age + allele1&quot; 
null&lt;- &quot;disease ~ sex + age&quot; 
anova(null, full1, test='Chisq')

full2&lt;- &quot;disease ~ sex + age + allele2&quot; 
null&lt;- &quot;disease ~ sex + age&quot;
anova(null, full2, test='Chisq')
</code></pre>
<p>, etc.</p>
<p>My gut feeling is that the LR approach would be better for by allele comparisons, because of the inclusion of covariates. Is this the case?</p>
",2020-12-29 15:05:43.180
333700,241161.0,1,,,,Use L1 logistic regression for feature selection but a nonlinear classifier for the training. Is that feature selection reliable?,<classification><generalized-linear-model><feature-selection>,CC BY-SA 4.0,"<p>I am wondering the following question. Probably it is a non-sense one but hope not too much..</p>
<p>Assume I have a binary classification model to build and I use a linear classifier like Logistic regression with L1 penalty (so the decision boundary is still linear) for feature selection.</p>
<p>Then I go through the training phase and I test several algorithms (linear and nonlinear classifier) for comparison.
If I see that the best performance is given by a nonlinear classifier, does my feature selection using a linear one make sense? It is likely that if I use a nonlinear classifier for feature selection I get a different subset.</p>
<p>So how to deal with this?</p>
<p>Thanks. Luigi</p>
",2020-12-29 15:16:20.843
333701,255159.0,1,,,,I need help finding mean and standard deviation,<self-study><normal-distribution><mean><standard-deviation>,CC BY-SA 4.0,"<p>Given that 80% of the values are less than 140 and 75% is less than 135. Distribution is a normal distribution. Find the mean and standard deviation.</p>
<p>Anyone can help to find and explaining a solution? Thanks in advance.</p>
",2020-12-29 15:21:29.540
333702,136436.0,2,,333700.0,,,,CC BY-SA 4.0,"<p>Not a nonsense question at all! &quot;Is it likely that if I use a nonlinear classifier for feature selection I get a different subset.&quot; Yes, that is quite possible, however, I think this is still a perfectly reasonable approach. Presumably you are doing this to cut down on time, so that you can quickly train many models on a smaller feat set. If your use case is not highly sensitive to getting the absolute highest possible performance and you are okay with a pretty good model, you are done.</p>
<p>If not computationally/time limited and you want to squeeze out more accuracy, I would do the exact process you describe, but then at the very end plug all the features back into your final non-linear model and do some kind of stepwise feature selection, either using tree-based feat importance if you are using trees, or a model agnostic version like permutation importance. You might still be left with a similar feat set, or you might pick up some extra features that help the non-linear version, but not L1.</p>
",2020-12-29 15:30:30.110
333703,158099.0,2,,333701.0,,,,CC BY-SA 4.0,"<p>Assuming <span class=""math-container"">$\mu, \sigma$</span> for mean and deviation respectively, you have two equations of the following form: <span class=""math-container"">$P(X\leq x)=p$</span>. When standardised, it becomes</p>
<p><span class=""math-container"">$$P(X\leq x)=P\left(\frac{X-\mu}{\sigma}\leq \frac{x-\mu}{\sigma}\right)=P\left(Z\leq \frac{x-\mu}{\sigma}\right)=\Phi\left(\frac{x-\mu}{\sigma}\right)$$</span></p>
<p>where <span class=""math-container"">$\Phi$</span> represents the CDF of the standard normal distribution. You can look up its value <a href=""https://en.wikipedia.org/wiki/Standard_normal_table"" rel=""nofollow noreferrer"">here</a>, i.e. <span class=""math-container"">$\Phi(v_1)=0.8$</span> and <span class=""math-container"">$\Phi(v_2)=0.75$</span> and solve for <span class=""math-container"">$\mu,\sigma$</span> since you've a linear system of equations with two unknowns.</p>
",2020-12-29 15:34:37.610
333704,136436.0,2,,228594.0,,,,CC BY-SA 4.0,"<p>Idk if this is what you are looking for, but this looks a little bit like semisupervised learning, which I'd check out. You start with a few labeled observations, but then you can feed the most certain (unlabeled) predictions back in as if they were actuals in order to supplement the small amount of data you do have.  <a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Semi-supervised_learning</a></p>
",2020-12-29 15:41:11.173
333705,20286.0,2,,333699.0,,,,CC BY-SA 4.0,"<p>Your one-at-a-time approach for evaluating the alleles via likelihood-ratio (LR) tests runs a risk of <a href=""https://stats.stackexchange.com/q/113766/28500"">omitted-variable bias</a>. If any of your omitted alleles is associated with outcome, then your estimated coefficient for the included allele will tend to have a bias that will affect your ability to interpret the LR tests appropriately.</p>
<p>The coefficients returned by commands (in R) like <code>summary(full)</code> for a model fit by maximum likelihood (e.g., logistic regression) represent Wald tests for each predictor coefficient based on the full model with covariates. The variances for the coefficient estimates are based on the full variance-covariance matrix calculated at the full model solution. So there's no problem doing Wald tests in a way that incorporates information about the covariates.</p>
<p>Be sure that your coding of the alleles doesn't get you into trouble, however. If those are the only 3 possible alleles you might end up with a linear dependence among the predictors. Also, you aren't allowing for <em>interactions</em> among alleles, so you might be missing something important.</p>
",2020-12-29 15:57:24.600
333706,82120.0,1,,,,Interpreting viterbi decoding and posterior probabilities,<r><hidden-markov-model>,CC BY-SA 4.0,"<p>I am wondering why the last state of the predicted probabilities does not match the viterbi decoding sequence.</p>
<p>Consider this MWE:</p>
<pre><code>library(HMM)
trans_probs &lt;- c(0.25 , 0.5  , 0.025, 0.2  , 0.025, 0.25 , 0.15 , 0.075, 0.5  ,
                 0.025, 0.05 , 0.025, 0.05 , 0.85 , 0.025, 0.025, 0.075, 0.15 ,
                 0.125, 0.625, 0.05 , 0.075, 0.475, 0.025, 0.375)
transProbs &lt;- matrix(data=trans_probs, nrow=5, ncol=5, byrow=TRUE)


emission_probs &lt;- c(0.6  , 0.175, 0.175, 0.05 , 0.05 , 0.6  , 0.175, 0.175, 0.05 ,
                    0.175, 0.6  , 0.175, 0.05 , 0.175, 0.175, 0.6  , 0.6  , 0.05 ,
                    0.175, 0.175)
emissionProbs &lt;- matrix(data=emission_probs, nrow=5, ncol=4, byrow=TRUE)

observation &lt;- c('0', '2', '1', '0', '2', '1', '3')

d_init &lt;- c(0.2, 0.2, 0.2, 0.2, 0.2)

hmm &lt;- initHMM(States = c('0', '1', '2', '3', '4'), # List of states
               Symbols = c('0', '1', '2', '3'), # List of potential observations
               startProbs = d_init, # Initial probabilities
               transProbs = transProbs, # Transition matrix
               emissionProbs = emissionProbs) # Emission matrix
</code></pre>
<pre><code>viterbi(hmm, observation)
# &quot;4&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot;
</code></pre>
<pre><code>posterior(hmm, observation)
      index
states          1          2          3          4          5          6          7
     0 0.29617487 0.11420953 0.04867024 0.19014791 0.12761431 0.05713045 0.01679575
     1 0.02431431 0.17779530 0.16953596 0.01685586 0.12137835 0.28633013 0.07110010
     2 0.01700985 0.61010230 0.02643983 0.01053570 0.51826439 0.20811925 0.07377565
     3 0.02427890 0.04835995 0.72334261 0.04223564 0.04093185 0.43001382 0.61623302
     4 0.63822207 0.04953292 0.03201137 0.74022489 0.19181110 0.01840636 0.22209549
</code></pre>
<p>Notice the last observation, 7, departs from the pattern of the highest posterior probability matching the viterbi decoding selection.</p>
",2020-12-29 16:03:53.783
333707,255242.0,1,,,J. Ziegler,Pairwise comparison of numeric fixed effect of linear mixed model,<r><lme4-nlme><lsmeans>,CC BY-SA 4.0,"<p>Using the <em>sleepstudy</em> data from the lme4 package I want to do pairwise comparison using the emmeans package.</p>
<p>This is the model:</p>
<pre><code>library(lme4)
lmm &lt;- lmer(Reaction ~ Days + (1 + Days | Subject), sleepstudy)
</code></pre>
<p>Now when I want to do pairwise comparison like this, I only get NAs, no pairwise comparisons:</p>
<pre><code>library(emmeans)
lmm.pw &lt;- emmeans(lmm, &quot;Days&quot;)
pairs(lmm.pw)
</code></pre>
<p>According to the <a href=""https://cran.r-project.org/web/packages/emmeans/vignettes/FAQs.html"" rel=""nofollow noreferrer"">emmeans FAQs</a>, this is due to &quot;Days&quot; being numeric.
However, when I try to fit the model with &quot;Days&quot; as factor it gives me the following error:</p>
<pre><code>lmm &lt;- lmer(Reaction ~ as.factor(Days) + (1 + as.factor(Days) | Subject), sleepstudy)
</code></pre>
<blockquote>
<p>number of observations (=180) &lt;= number of random effects (=180) for term (0 + fDays | Subject); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable</p>
</blockquote>
<p>This was already solved on <a href=""https://stats.stackexchange.com/questions/193678/number-of-random-effects-is-not-correct-in-lmer-model"">stackexchange</a> but I don't really understand why I should be able to fit the model when &quot;Days&quot; is numeric, but not when it is a factor?</p>
<p>Likewise, why do I get different <code>summary()</code> outputs for these two simpler models, when the only difference is that &quot;Days&quot; is either a factor or numeric?</p>
<pre><code>lmm1 &lt;- lmer(Reaction ~ Days + (1|Subject), sleepstudy)
lmm1f &lt;- lmer(Reaction ~ as.factor(Days) + (1|Subject), sleepstudy)
</code></pre>
<p>I am able to get pairwise comparisons from the lmm1f-model. But the fixed effects table that I get by running <code>summary(lmm1f)</code> seems to treat every level of <code>as.factor(Days)</code> as a separate fixed effect (<strong>EDIT Dec 30 2020: see <a href=""https://stats.stackexchange.com/questions/169543/output-of-fixed-effects-summary-in-lmertest-in-r-and-post-hoc-tests"">here</a> for an explanation of the summary output</strong>), which makes me wonder if the model truly does what I want it to do.</p>
<p>Pretty sure I'm overlooking/not understanding some basic stuff. Any help is appreciated, thanks!</p>
<hr />
<p>I'm using R version 4.0.2 in RStudio Version 1.3.1073 on macOS 11.1.</p>
<p>Packages: lme4 (1.1-23), emmeans (1.5.0)</p>
",2020-12-29 16:11:18.890
333708,157245.0,2,,332808.0,,,,CC BY-SA 4.0,"<p>@Anderson Arroyo</p>
<p>A model like this is possible to estimate but, to my understanding, is not implemented in R/on CRAN as a package currently.</p>
<p>For instance, you could implement a model similar to the one you are interested in here using Stata and the SSC module <code>domme</code> (see <a href=""https://jasemjournal.com/wp-content/uploads/2020/07/Luchman-et-al-2020-Vol4Issue2.pdf"" rel=""nofollow noreferrer"">Luchman, Xue, and Kaplan, 2020</a> for a discussion of the approach as well as this <a href=""https://github.com/jluchman/domme"" rel=""nofollow noreferrer"">github</a> page).</p>
<p>Below there is an example from an ARMA model with a first order moving average, first order autoregression and an exogenous variable.  The approach is a dominance analysis adapted to focus on parameter estimation as opposed to independent variables and estimates a McFadden's pseudo-R^2.</p>
<pre><code>. webuse friedman2, clear

. arima consump m2 if tin(, 1981q4), ar(1) ma(1)

(setting optimization to BHHH)
Iteration 0:   log likelihood = -344.67575  
Iteration 1:   log likelihood = -341.57248  
Iteration 2:   log likelihood = -340.67391  
Iteration 3:   log likelihood = -340.57229  
Iteration 4:   log likelihood =  -340.5608  
(switching optimization to BFGS)
Iteration 5:   log likelihood =  -340.5515  
Iteration 6:   log likelihood = -340.51272  
Iteration 7:   log likelihood = -340.50949  
Iteration 8:   log likelihood =  -340.5079  
Iteration 9:   log likelihood = -340.50775  
Iteration 10:  log likelihood = -340.50774  

ARIMA regression

Sample:  1959q1 - 1981q4                        Number of obs     =         92
                                                Wald chi2(3)      =    4394.80
Log likelihood = -340.5077                      Prob &gt; chi2       =     0.0000

------------------------------------------------------------------------------
             |                 OPG
     consump |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
consump      |
          m2 |   1.122029   .0363563    30.86   0.000     1.050772    1.193286
       _cons |  -36.09872   56.56703    -0.64   0.523    -146.9681    74.77062
-------------+----------------------------------------------------------------
ARMA         |
          ar |
         L1. |   .9348486   .0411323    22.73   0.000     .8542308    1.015467
             |
          ma |
         L1. |   .3090592   .0885883     3.49   0.000     .1354293    .4826891
-------------+----------------------------------------------------------------
      /sigma |   9.655308   .5635157    17.13   0.000     8.550837    10.75978
------------------------------------------------------------------------------
Note: The test of the variance against zero is one sided, and the two-sided
      confidence interval is truncated at zero.

. matrix list e(b)

e(b)[1,5]
       consump:    consump:       ARMA:       ARMA:      sigma:
                                     L.          L.            
            m2       _cons          ar          ma       _cons
y1   1.1220286  -36.098721   .93484865   .30905921   9.6553076

. domme (consump = m2) (ARMA = L.ar L.ma) if tin(, 1981q4), reg(arima consump m2) ropt(ar(1) ma(1)) fitstat(e(),
&gt;  mcf)

Total of 7 models/regressions

General dominance statistics: ARIMA regression
Number of obs             =                      92
Overall Fit Statistic     =                  0.5125

            |      Dominance      Standardized      Ranking
            |      Stat.          Domin. Stat.
------------+------------------------------------------------------------------------
consump     |
 m2         |         0.2181      0.4256            2 
ARMA        |
 L.ar       |         0.2449      0.4778            1 
 L.ma       |         0.0495      0.0965            3 
-------------------------------------------------------------------------------------
Conditional dominance statistics
-------------------------------------------------------------------------------------

             #param_ests:  #param_ests:  #param_ests:
                       1             2             3
consump:m2        0.3482        0.2265        0.0797
 ARMA:L.ar        0.3905        0.2533        0.0910
 ARMA:L.ma        0.0856        0.0578        0.0050
-------------------------------------------------------------------------------------
Complete dominance designation
-------------------------------------------------------------------------------------

                  dominated?:  dominated?:  dominated?:
                          m2         L_ar         L_ma
  dominates?:m2            0           -1            1
dominates?:L_ar            1            0            1
dominates?:L_ma           -1           -1            0
-------------------------------------------------------------------------------------

Strongest dominance designations

ARMA:L.ar completely dominates consump:m2
consump:m2 completely dominates ARMA:L.ma
ARMA:L.ar completely dominates ARMA:L.ma
</code></pre>
<p>I have been working on a port of this module to <a href=""https://github.com/jluchman/domir"" rel=""nofollow noreferrer"">R</a> but it is not yet ready to accommodate an AR(I)MA model.  Hope to add such capabilities in the future and, eventually, release on CRAN.</p>
",2020-12-29 16:12:50.780
333709,24790.0,2,,333680.0,,,,CC BY-SA 4.0,"<p>Your study design is not at all clear - can you provide more details to help potential responders understand what is really going on?</p>
<p>What is clear is that you selected 5 locations where the soil is heated and 5 locations where the soil is non-heated. But what exactly are you measuring at each location, how and how often? For example, if you gather seeds of Festuca rubra at each location, how many seeds are you gathering per location and what are you measuring for these seeds? In particular, are you aggregating the thing(s) you are measuring across seeds and reporting a single aggregated value per location?</p>
<p>What is the blocking factor shown into your diagram and how does it come into play into your study? Is the blocking factor something like year? What is the difference between the blocking factors a through d and the blocking factors e through h?</p>
<p>You also don't mention much about the snow treatment - was snow physically removed at all locations with heated soil as a part of your study or did it simply melt by itself? If snow melts by itself resulting in &quot;snow removal&quot;, the heated location and the removed snow are equivalent - you can't have a heated location without having melted snow. In that case, I can't see how your Treatment 1 and Treatment 2 would not refer to the exact same thing?</p>
",2020-12-29 16:32:07.117
333710,254952.0,1,,,,"Determining the minimum number of tosses, for heads to be twice more likely than tails in the next toss",<probability><bayesian><mathematical-statistics><point-estimation>,CC BY-SA 4.0,"<p>I would like some help with the following statistical problem.</p>
<p>We have a coin with probability <span class=""math-container"">$\theta$</span> for heads, with prior for <span class=""math-container"">$\theta$</span> being a Beta(a,a) distribution (a is a known parameter).
After tossing the coin n times,  we get n-1 times heads.</p>
<p>Assuming that we toss the coin again, we would like to find the minimum n for which  heads is at least two times more likely than tails.</p>
<p>I tried to address the problem in the following way:</p>
<p>We have that <span class=""math-container"">$\pi(\theta)=Beta(a,a)$</span> and <span class=""math-container"">$L(\theta;x)=f(x;\theta)=Bin(n,x)$</span> <br />
Therefore we get:
<span class=""math-container"">$\pi(\theta|x) \propto Beta(a,a) \times L(\theta;x) \implies \pi(\theta|x) =Beta(a+x,a+n-x)$</span></p>
<p>Obviously, the posterior of <span class=""math-container"">$\theta$</span> is a function of n and x</p>
<p>But for x=n-1 we get <span class=""math-container"">$\pi_n(\theta|x) \propto Beta(a+n-1,a+1)$</span>, that is the posterior of <span class=""math-container"">$\theta$</span> depends only on n.</p>
<p>The n+1 toss is a Bernoulli trial with  parameter <span class=""math-container"">$\theta$</span> following the beta distribution described by <span class=""math-container"">$\pi_n(\theta|x)$</span>, i.e. <span class=""math-container"">$Beta(a+n-1,a+1)$</span>.</p>
<p>In order for  heads to be at least two times more likely than tails, we must have
<span class=""math-container"">$\theta \geq 2/3$</span>.</p>
<p>I am not quite sure, how to continue with the problem after this point.</p>
<p>After skimming some Bayesian  Statistics textbooks, I came to the conclusion that the standard Bayesian approach  would be to take :<span class=""math-container"">$E[\pi_n(\theta)] \geq 2/3$</span>
Still, I am not sure about this.</p>
<p>I have, also, thought of <span class=""math-container"">$\frac{\int_{2/3}^{1}\pi_n(\theta)}{\int_{0}^{2/3}\pi_n(\theta)} &gt; 1$</span></p>
<p>So, summing up, I am not sure how to continue after having calculated the posterior distribution <span class=""math-container"">$\pi_n(\theta|x)$</span> of <span class=""math-container"">$\theta$</span>.</p>
<p>Any help would be greatly appreciated.</p>
",2020-12-29 16:32:38.627
333711,22.0,2,,318704.0,,,,CC BY-SA 4.0,"<p>The Geisser-Greenhouse correction leads to a change in the numbers of degrees of freedom and results (almost always) in fractional degrees of freedom. GraphPad Prism computes the P-value correctly accounting for the fractional degrees of freedom.</p>
<p>R gives the same results as Prism:</p>
<pre><code>pf(20.25, 1.391, 2.783, lower.tail = FALSE)
[1] 0.02296042
</code></pre>
<p>If you truncate both df values to integers, R gives the same result as Excel:</p>
<pre><code>pf(20.25, 1, 2, lower.tail = FALSE)
[1] 0.04600191
</code></pre>
<p>Excel computes P from F incorrectly when the df values are fractional. If you enter a fractional df, it simply removes the fraction so truncates to an integer. So the following Excel formula all give the same result (FDIST is the older form; F.DIST.RT is the newer form). The result, 0.0460, is correct for the first and third lines below (with integer df values), but is wrong for the second and fourth lines (with the fractional df values).</p>
<pre><code>=F.DIST.RT(20.25, 1, 2)
=F.DIST.RT(20.25, 1.391, 2.783)
=FDIST(20.25, 1, 2)
=FDIST(20.25, 1.391, 2.783)
</code></pre>
<p>The fact that the incorrect P-value is about twice the correct one is just a coincidence with this example, it seems, and has nothing to do with one- vs two-tails of the distributions.</p>
<p>Bottom line: GraphPad Prism seems to be doing the calculations correctly (as does R) and Excel does not. I have submitted a bug report to Microsoft.</p>
",2020-12-29 16:36:38.807
333712,255171.0,1,333733.0,,,P-value does not change by transformation/standardization,<r><regression><p-value><data-transformation>,CC BY-SA 4.0,"<p>In terms of regression model, I have read a statement that transforming/standardizing variables does not change its p-value as long as we keep the same model.</p>
<p>I recently transformed a variable (took the log and scaled it in R) and when I checked if there was an association between the transformed variable and the other variable by using Wilcoxon test, I found that the p-value remained as same as before I standardized the variable.</p>
<p>Could you please explain about the theory behind this?
Does transforming/standardizing variables always not change its p-value? Or, it only applies with certain condition?</p>
<p>Thank you so much for your time and explanation in advance!</p>
",2020-12-29 16:50:43.433
333743,16043.0,2,,310030.0,,,,CC BY-SA 4.0,"<p>While adjusting <code>pos_weight</code> does change the model, and may therefore change the association between predicted probabilities and labels, it doesn't have any direct influence on the F2 score itself. Indeed, for a <em>multi-class</em> and <em>multi-label</em> problem, it's not even clear which class or label would be a &quot;positive.&quot;</p>
<p>Instead, F2 score depends on the precision and recall of the model's predictions. The precision and recall are, in turn, binarizations of the model's predicted probabilities. You can change the F2 score directly by changing the choice of threshold.</p>
",2020-12-29 22:20:33.973
333713,120720.0,2,,333601.0,,,,CC BY-SA 4.0,"<p>This curve looks less bad when you switch the axes.</p>
<p><img src=""https://i.stack.imgur.com/1DmXq.png"" alt=""axes switched"" /></p>
<p>For each given predicted value (currently on the y-axis), you see more or less as many true values above and below (right and left on the x-axis).</p>
<p>So the model is not so much biased.</p>
<hr />
<p>Related is this example of linear regression from this <a href=""https://stats.stackexchange.com/a/386006/164061"">question/answer</a></p>
<p><a href=""https://i.stack.imgur.com/W96oO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W96oO.png"" alt=""example"" /></a></p>
<p>You can fit X versus Y or Y versus X.</p>
<p>Due to <a href=""https://en.m.wikipedia.org/wiki/Regression_dilution"" rel=""nofollow noreferrer"">regression dilution</a> you get that the regression lines are a bit flat (less steep). The one curve represents E(X|Y) and the other curve represents E(Y|X). If you switch their roles then the curves will be biased for high/low values.</p>
<p>This switching of roles is what also happens in your image. Your curve gives an estimate of the expected true value conditional on the regressor. The true values (on the horizontal/x-axis) seem to be more or less evenly distributed around the predicted mean (you have to compare left/right for this and not up/down).</p>
",2020-12-29 17:04:41.817
333714,227871.0,1,,,,Is there any requirements on the number of indicator variables? and Reliability test?,<structural-equation-modeling>,CC BY-SA 4.0,"<p>Hello I am learning to perform structural equation modeling analysis, attached is the proposed model. 1. Am allowed to only one indicator variables (observed variable, exogenous) (a1) for variable A. I found most of the models I read from the literature all have over 2 indicator variables(observed) for each latent variable, some of them even did reliability analysis.
2. Could variables E and F be category data?</p>
<p>As I am still learning this analysis, please forgive me if I used incorrect nouns. Thanks.
<a href=""https://i.stack.imgur.com/Hw1tz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hw1tz.png"" alt=""enter image description here"" /></a></p>
",2020-12-29 17:17:19.903
333715,66768.0,2,,333640.0,,,,CC BY-SA 4.0,"<p>Think I found the solution: We'll use the <strong>chain rule of bayesian networks</strong> to calculate the probability of every event being unsuccessful. The answer is 1 minus that number.</p>
<p>Is there a better answer out there?</p>
",2020-12-29 17:25:45.783
333716,6107.0,2,,80672.0,,,,CC BY-SA 4.0,"<p>This makes me think of <a href=""http://probability.ca/MT/BOOK.pdf"" rel=""nofollow noreferrer"">Meyn and Tweedie's book</a>. They use <span class=""math-container"">$P$</span> to denote the transition kernel for a Markov chain, and <span class=""math-container"">$\mathsf{P}$</span> for the law of the entire chain on <span class=""math-container"">$\mathsf{X}^{\infty}$</span>. This answer is specific to Markov chains, but the distinction is obviously important.</p>
<p>The difference between <span class=""math-container"">$P$</span> and <span class=""math-container"">$\mathbb{P}$</span> (and <span class=""math-container"">$E$</span> and <span class=""math-container"">$\mathbb{E}$</span>) from book to book, is just for aesthetic appeal, in my opinion. I can't generalize where I see <span class=""math-container"">$\Pr()$</span> or <span class=""math-container"">$\text{Prob}()$</span>, really.</p>
",2020-12-29 17:28:18.517
333717,141531.0,1,333874.0,,,Bias corrrection for MLE when dealing with normally distributed small samples,<maximum-likelihood><bias><unbiased-estimator>,CC BY-SA 4.0,"<p>When estimating the standard-deviation for samples of normally distributed data, it is sometimes necessary to account for bias in whatever estimator one chooses -- which is usually related to the number of observations in the sample.</p>
<p>For example if we take the corrected-sample standard deviation,
<span class=""math-container"">$$s = \sqrt{\frac{1}{n-1} \sum_{i}^{N} \left(x_{i} - \bar{x} \right)^{2}}$$</span>
we account for sample-size bias with
<span class=""math-container"">$$C_{4}(N) = \sqrt{\frac{2}{N-1}}\frac{\Gamma(N/2)}{\Gamma((N-1)/2)}$$</span>
with an unbiased estimator for the standard deviation being
<span class=""math-container"">$$\sigma_{\rm{est}} = s / c_{4}(n).$$</span></p>
<p>For <a href=""http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/BetterThanMAD.pdf"" rel=""nofollow noreferrer"">other estimators</a> such as <span class=""math-container"">$\rm{MAD}$</span>, <span class=""math-container"">$S_{n}$</span>, and <span class=""math-container"">$Q_{n}$</span> there are also bias correction factors which also scale with the sample size.</p>
<p>Is there a similar correction factor one can apply when using maximum-likelihood methods? In the plot below I have simulated normally distributed, <span class=""math-container"">$ \mathcal{N}(0, 1)$</span>, for different sample sizes and estimated the standard deviation with <span class=""math-container"">$s$</span>, <span class=""math-container"">$s/c_{4}$</span>, and with a maximum-likelihood evaluation. We can see the bias behaviour:
<a href=""https://i.stack.imgur.com/i45ja.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i45ja.png"" alt=""enter image description here"" /></a></p>
",2020-12-29 17:28:52.320
333718,947.0,2,,333601.0,,,,CC BY-SA 4.0,"<p>Your bias is really through a fairly narrow range. It is from 0-7K; and &gt;32K.  At &gt;32K it looks like you have relatively few observations.  So, those seem to all be outliers that may be difficult to fit.</p>
<p>I would pay more attention to the 0-7K range where you have a lot more observations.  Maybe there is a simple way to add a predictor variable for the 0-7K range.  This predictor could be structured as a simple constant or an interaction variable with some of the other predictors that would change the coefficient associated with that other predictor.</p>
<p>If the above works, and it probably should; you could then try something similar with the &gt;32K range.</p>
",2020-12-29 17:31:22.153
333719,12544.0,2,,333714.0,,,,CC BY-SA 4.0,"<p>Yes, it's 'allowed', but why bother.  In the days of the original LISREL language you had to do this. But with modern programs (and even Lisrel) you can just put a1 in, in the place of A.</p>
<p>E and F can be binary.</p>
",2020-12-29 17:46:03.093
333720,244871.0,1,333764.0,,,t-test or z-test: assume normality or s≈σ?,<t-test><standard-deviation><normality-assumption><z-test>,CC BY-SA 4.0,"<p>Let's say I have a large sample (<span class=""math-container"">$n&gt;30$</span>) with mean <span class=""math-container"">$\bar{X}$</span> and standard deviation <span class=""math-container"">$S$</span>. I can calculate only: <span class=""math-container"">$T=\frac{\bar{X}-\mu}{S/\sqrt{n}}$</span>.</p>
<ul>
<li>If I want to use <strong>t-test</strong> for a hypothesis, then I have to assume <strong>normality of the population</strong>.</li>
<li>If I want to use <strong>z-test</strong> for a hypothesis, then I have to assume that <span class=""math-container"">$\boldsymbol{s\approx\sigma}$</span>.</li>
</ul>
<p><strong>What is the less important assumption?</strong> What test should I do? Should I do t-test, z-test or doesn't matter?</p>
",2020-12-29 18:02:12.670
333781,252551.0,1,,,,P-value for precision recall curve significance,<p-value><auc><precision-recall>,CC BY-SA 4.0,"<p>I am computing the precision-recall curve for my ML model with 2 classes. I want to have a p-value that compare the observed area under the precision recall curve (<em>AUPR_obs</em>) and the area of such a curve with random labels (<em>AUPR_rand</em>). I know that the random area under the precision recall would be close to the proportion of the positive class. What method can I use to obtain the p-value comparing <em>AUPR_obs</em> and <em>AUPR_rand</em>? Would DeLong test be a solution?</p>
",2020-12-30 10:59:01.740
333721,61417.0,1,333741.0,,,How do i calculate if the difference between two advertisement campaigns is significant?,<standard-deviation><group-differences><marketing>,CC BY-SA 4.0,"<p>My brother has asked me if I can evaluate if his advertisement campaigns yield significantly different results. It was a couple of years ago I dealt with these kinds of problems, so would appreciate some feedback.</p>
<p>For the sake of simplicity, we'll try to see if there's a difference between his normal campaign and his most successful one.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Normal campaign</th>
<th>Most successful</th>
</tr>
</thead>
<tbody>
<tr>
<td>Leads(n)=106</td>
<td>Leads(n)=9</td>
</tr>
<tr>
<td>Successes=8</td>
<td>Successes=8</td>
</tr>
<tr>
<td>Success ratio(p)=0.075</td>
<td>success ratio=0.44</td>
</tr>
<tr>
<td>Variance =<span class=""math-container"">$p(1-p)=0.075*(1-0.075)/106=0.00066$</span></td>
<td>variance =<span class=""math-container"">$p(1-p)/n=0.44(1-0.44)/9=0.0273$</span></td>
</tr>
</tbody>
</table>
</div>
<p>My notes are a bit unclear, but I think that I need to form a pooled variance by adding both variances together, and then take the square root in order to get the standard deviation. In that case this would yield:</p>
<p><span class=""math-container"">$$0.0273+0.0006=0.0279
0.0279^{(1/2)}=0.167$$</span></p>
<p>We should then (I think) calculate the difference in success ratio and divide this value with the pooled standard deviation:</p>
<p><span class=""math-container"">$$0.44-0.075=0.365
0.365/0.167=2.18$$</span></p>
<p>That is to say we have a z score of 2.18, which means the result is significant on a 95 percent confidence level.</p>
<p>Question 1:</p>
<p>Are these steps valid, or did I make a mistake somewhere?</p>
<p>Question 2:</p>
<p>Is the method valid for these kinds of values (I've read that it's best when <span class=""math-container"">$np\geq 10$</span> for instance) and if not, is there a method that's more accurate?</p>
",2020-12-29 18:32:56.670
333722,255186.0,1,,,Glingol,How to interpret a specific value,<r><generalized-linear-model><interpretation>,CC BY-SA 4.0,"<p>We need to make a prediction on a data set; we will use logistic regression.</p>
<p>We have the following predictor variable: <code>spezialiserung_schule</code> (specialisation school); it is nominal and can take the values &quot;art&quot;, &quot;science&quot; and &quot;trade&quot;.</p>
<p>If we run a simple logistic regression on it and check the results using <code>summary()</code>, we get the results shown below.</p>
<p>We are particularly interested in the estimate and the p-value. We understand that for both values p is quite large and the estimates are not great either.</p>
<p>If it were a simple binary variable I could multiply the estimate by 0 or 1 and then add the intercept and get the log(ratio). How exactly do I evaluate this if I am no longer using binary? What would be the next step that I take afterwards?</p>
<pre class=""lang-r prettyprint-override""><code>glm(formula = job ~ (spezialisierung_schule), 
    family = binomial(&quot;logit&quot;), data = Trainingsdaten)
</code></pre>
<p>Deviance Residuals</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Min</th>
<th>1Q</th>
<th>Mean</th>
<th>2Q</th>
<th>Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1.5786</td>
<td>-1.5392</td>
<td>0.8237</td>
<td>0.8545</td>
<td>1.3018</td>
</tr>
</tbody>
</table>
</div>
<p>Coefficients</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Estimate</th>
<th>Std. Error</th>
<th>z value</th>
<th>Pr(&gt;IzI)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>(Intercept)</td>
<td>0.81944</td>
<td>0.23539</td>
<td>3.481</td>
<td>0.000499</td>
<td>***</td>
</tr>
<tr>
<td>spezialisierung_schuleKunst</td>
<td>-1.10712</td>
<td>0.79921</td>
<td>-1.385</td>
<td>0.165972</td>
<td></td>
</tr>
<tr>
<td>spezialisierung_schuleWissenschaft</td>
<td>0.08728</td>
<td>0.34966</td>
<td>0.250</td>
<td>0.802884</td>
<td></td>
</tr>
</tbody>
</table>
</div>",2020-12-29 18:33:36.343
333723,254875.0,1,,,,"Accuracy, F-Score, and Kappa Together",<accuracy><confusion-matrix><cohens-kappa>,CC BY-SA 4.0,"<p>What can accuracy, f-score, and kappa indicate together for a confusion matrix that each individually can't?</p>
<p>I get F-Score and Accuracy measure to be quite close so I feel it should be sufficient for my analysis to use just Accuracy and Kappa scores. I wonder if adding F-Score to my analysis can make it stronger.</p>
",2020-12-29 18:44:00.187
333724,234784.0,1,,,,When to use robust one-way repeated measure design ANOVA?,<anova><repeated-measures><multilevel-analysis><robust>,CC BY-SA 4.0,"<p>I have a set of 9 different factor levels from my independent variable to be compared against each other. Here are the results of the different assumption tests in R. I'm just going through my methodology in the hope that someone can guide me to the right conclusion.</p>
<ul>
<li>The Shapiro-Wilk test is significant and shows that the my dependent variable is not normally distributed. I read somewhere that the assumption of normality doesn't always have to be met if the size of the dataset is substantial as it will almost always deviate from normality when it's the case.</li>
<li>Mauchly's test for sphericity is significant, therefore the assumption of sphericity has been violated. My Greenhouse-Geiser correction confirmed it, but it's p-value is significant, which indicates that the levels of my independent variable significantly affects the dependent variable.</li>
</ul>
<p><strong>Where do I go from here?</strong></p>
<p>The example I'm basing my methodology on in a textbook goes on to use these methods on the same dataset every time (though that dataset is different than mine)</p>
<ul>
<li>Repeated measures ANOVA</li>
<li>A multilevel approach</li>
<li>Robust test</li>
</ul>
<p>Based on what I know, I think I have to go for a robust method given the assumptions above, but I have no idea if this is the right call.</p>
",2020-12-29 18:48:07.860
333725,61691.0,2,,309011.0,,,,CC BY-SA 4.0,"<blockquote>
<p>Is there someone who worked on french text generation using GPT 2 model?</p>
</blockquote>
<p>I've seen two such projects:</p>
<ul>
<li><a href=""https://github.com/aquadzn/gpt2-french"" rel=""nofollow noreferrer"">https://github.com/aquadzn/gpt2-french</a></li>
<li><a href=""https://medium.com/@timohear/retraining-gpt-2-to-write-love-letters-in-french-92ae938bc452"" rel=""nofollow noreferrer"">https://medium.com/@timohear/retraining-gpt-2-to-write-love-letters-in-french-92ae938bc452</a></li>
</ul>
<blockquote>
<p>Is there a pre-trained GPT 2 model on french text? Can we do finetuning on the Standard GPT 2 and obtain good results on french text?</p>
</blockquote>
<p>I haven't seen any available from scratch. But there are 4 separate (small) GPT2 models obtained by fine-tuning the (English) small GPT-2 model released by OpenAI on 4 different datasets: <a href=""https://github.com/aquadzn/gpt2-french/releases/"" rel=""nofollow noreferrer"">https://github.com/aquadzn/gpt2-french/releases/</a></p>
<p>Edit:
as a followup, several GPT2 model fine-tuned on French data have been contributed to HuggingFace's Models hub:</p>
<ul>
<li><a href=""https://huggingface.co/dbddv01/gpt2-french-small"" rel=""nofollow noreferrer"">gpt2-french-small</a></li>
<li><a href=""https://huggingface.co/antoiloui/belgpt2"" rel=""nofollow noreferrer"">belgpt2</a></li>
<li><a href=""https://huggingface.co/inspectorsolaris/gpt2_french"" rel=""nofollow noreferrer"">gpt2_french</a></li>
<li><a href=""https://huggingface.co/inspectorsolaris/gpt2_french_pre_trained"" rel=""nofollow noreferrer"">gpt2_french_pre_trained</a></li>
</ul>
",2020-12-29 18:56:50.570
333726,146830.0,2,,331384.0,,,,CC BY-SA 4.0,"<p>In BG/NBD, individual attrition is defined by &quot;p&quot;, which follows a beta distribution. The mean of the beta distribution is a/(a+b).  Hope this help.  BG/NBD was created by Fader, Hardie, and Lee.  Please attribute the credit correctly in the future.</p>
",2020-12-29 19:30:16.043
333742,254365.0,1,,,,Probability exercise using geometric distribution—Roulette gambling,<probability><self-study><geometric-distribution>,CC BY-SA 4.0,"<p>A gambler plays roulette at Monte Carlo and continues gambling, wagering the same amount each time on “Red”, until he wins for the first time. If the probability of “Red” is 18 and the gambler has only 38 enough money for 5 trials.</p>
<p>I need to find the probability that he will win before he exhausts his funds (this means before the 6th trial). The book execute the problem with the inverse probability. Because we need to find <span class=""math-container"">$P(X \le 5)$</span>, it is the same as <span class=""math-container"">$1 - P(X \ge 6)$</span>, that is <span class=""math-container"">$1-(1-P)^5$</span>, with result 0.95. I solved not using the inverse probability, because I can deduce the PDF knowing that it is a geometrical distribution with pdf <span class=""math-container"">$f(x) = (20/38)^{x-1}  18/38$</span>. So <span class=""math-container"">$P(X\le 5)$</span> means the summation from <span class=""math-container"">$x = 1$</span> to <span class=""math-container"">$x = 5$</span> of this function result, but it gives me 0.7 ca, and I don't understand why.</p>
",2020-12-29 22:16:58.190
333727,236703.0,1,,,,Selecting the appropriate analysis method for data with multiple observations for one subject,<r><methodology>,CC BY-SA 4.0,"<p>I have a data frame whith head looking like this:</p>
<pre><code>    SUBJECT_ID   NLR1      NLR2      AGE      SEX      BMI      DAYS_OF_THERAPY  SMOKING
       1       2.9731183 3.3028571 24.10411   Female  19.11111        136        FALSE
       2       3.7941176 1.6904762 44.65753   Male    24.75000        40         FALSE
       3       0.4871245 0.7002584 33.30411   Male    20.74755        25         TRUE
       3       0.5225653 0.6234097 32.79178   Male    23.62029        24         TRUE
       3       0.6386364 2.0526316 32.27123   Male    23.30109        24         TRUE
</code></pre>
<p>These are data from various, often distant in time hospitalizations. The key variables are NLR1 and NLR2. The first one was measured at the beginning of hospitalization. The second measurement took place at the end of hospitalization. The time between the two measurements is different for each observation. It also happens that one subject has more than one observation, which could possibly affect the results of the analysis. Variables such as SEX, BMI and SMOKING status may further influence NLR and I would like to include this in the analysis.</p>
<p>I would like to determine whether:</p>
<ul>
<li>Is it possible to predict the NLR2 score using other variables?</li>
<li>Does NLR1 and NLR2 differ significantly in the entire population?</li>
<li>Is it possible to isolate subpopulations that, taking into account
corrections for other variables, differ in how the NLR changes during
hospitalization?</li>
</ul>
<p>What analytical methods and what R packages would be most appropriate to solve this problem?</p>
<p>EDIT: I have have circa 60 different subjects and circa 100 observations.</p>
",2020-12-29 19:41:41.483
333728,75014.0,1,,,,Estimating long-term side effects of Covid vaccines in short time (statistical design of clinical trials),<survival><censoring><clinical-trials>,CC BY-SA 4.0,"<p>I am interested in statistical procedures and study designs used to determine the absence/acceptability of long-term side effects of covid vaccines. Given that these vaccines were created and approved within a short period of time (less than a year), it seems that the studies using conventional survival analysis are of little value. I assume that one relies on the similarity with other, well-tested vaccines.</p>
<p>I have posted earlier a <a href=""https://biology.stackexchange.com/q/97370"">similar question</a> in the biology community, and some suggested that the question has to do more with statistics (and medicine) than with biology.</p>
<p><strong>TL;DR</strong><br />
After a medication/vaccine has been developed and tested in a lab, it has to pass a <a href=""https://en.wikipedia.org/wiki/Clinical_trial"" rel=""nofollow noreferrer"">clinical study</a>, which is essentially a series of statistical studies aiming to demonstrate that <strong>the medication is safe, produces desired effect, and does not have serious side effects.</strong> the data from these studies are tested against the threshold established by FDA (or equivalent authorities in other countries), and only then the medication is allowed for use on general public.</p>
<p>Note that the approval thresholds can vary depending on the nature of medication: e.g., cancer treatments have well-known side effects (think of the chemotherapy), which are tolerated for the sake of saving human lives. On the other had, vaccines used on healthy public should not have any significant side effects. The vaccines against SARS-CoV-2 might fall somewhere in between, due to the urgency of the current health crisis.</p>
<p>Phase II of clinical trial is most known in the popular culture - it is at this phase that the participants are separated into the <em>test</em> and the <em>control</em> groups, administered respectively the real medication and the placebo, which rises variosu moral dilemmas. Phase II is preceeded by a similarly designed Phase I, where the medication is tested on healthy people, to test its toxicity. Phase III is a finer study of the dozage, the side effects, etc., which is the subject of thre question.</p>
<p>Phase IV of the clinical studies begins after the approval and commercialization of the medication. My understanding is that Phase IV is essentially an epidemiological study, designed as a survival analysis for incidence of various adverse health conditions (or death), with the use of medication/vaccination appearing among the covariates. If this covariate is found to be (statistically) significant in terms of the incidence of the adverse side effects, the medicatuon/vaccine may be prohibited for further used/withdrawn from the market.</p>
<p>Contrary to a widespread belief, vaccines are not by themselves &quot;safe&quot; or &quot;dangerous&quot;. Rather their safety is determined on a case-by-case basis, by the amount of the adverse effects they produce. Thus, the most commonly used vaccines, those that might be obligatory in some countries (e.g., 11 vaccines are obligatory in France) have been typically in use for decades, and judged safe on the basis of the wealth of the available data.
However, even a safe vaccine can be judged too risky, if the risk of contracting the disease is very low - e.g., the use of the smallpox vaccine has been discontinued, since this disease is considered eradicated in most of the world, although it still exist in labs and isolated cases are occasionally reported. Another example is the tuberculosis vaccine, which carries a more substantial risk of side effects, and therefore used only on the contact cases or when traveling to risky regions. Among the vaccines that failed Phase IV trials and had to be withdrawn is <a href=""https://en.wikipedia.org/wiki/Dengue_vaccine"" rel=""nofollow noreferrer"">Dengvaxia by Sanofi Pasteur</a>.</p>
<p>A related problem is that of long-term vaccine efficacy - flu vaccine is known to be notoriously inefficient, due to the high variability of the influenza virus, whereas the tetanus vaccine has to be re-administered every few years.</p>
<p>Let us now get back to SARS-CoV-2 (covid, coronavirus). There is no doubt that it is not poisonous and that it is efficient against the virus. However their long-term side effects obviously could not be tested via Phase IV survival analysis methodology. On the one hand, the risk thresholds for these vaccines are lower, due to the worldwide health emergency (which is why they were authorized via <em>emergency authorization</em>, even before being vetted via peer review process as scientific publications). I so believe that the Phase III clinical trials include statistical procedures/safeguards that allow to assure the lack of serious side effects in the foreseeable future. <em>These statistical methods/designs are the point of my question.</em></p>
",2020-12-29 20:04:05.847
333729,225131.0,1,333732.0,,,Information Entropy Problem,<entropy><information-theory>,CC BY-SA 4.0,"<p>I cannot figure out this simple entropy problem and it is driving me crazy!</p>
<p>From McElreath's Statistical Rethinking:</p>
<p>Imagine instead 5 buckets and a pile of 10 individually numbered pebbles. You stand and toss all 10 pebbles such that each pebble is equally likely to land in any of the 5 buckets.</p>
<p>Distribution &amp; Entropy:</p>
<p>A &lt;- (0,0,10,0,0) = 0</p>
<p>B &lt;- (0,1,8,1,0) = 0.6390319</p>
<p>C &lt;- (0,2,6,2,0) = 0.9502705</p>
<p>D &lt;- (1,2,4,2,1) = 1.4708085</p>
<p>E &lt;- (2,2,2,2,2) = 1.6094379</p>
<p>For &quot;B&quot; I did -1[(.1)log2(.1) + (.8)log2(.8) + (.1)log2(.1)] = 0.92</p>
<p>For &quot;E&quot; I did -1[(.2)log2(.2) + (.2)log2(.2) + (.2)log2(.2) + (.2)log2(.2) + (.2)log2(.2)] = 2.32</p>
<p>Where did I go wrong?!?!</p>
",2020-12-29 20:53:07.037
333730,255194.0,1,,,,Conducting a Stratified Random Sample (Statewide Poll),<sampling><stratification>,CC BY-SA 4.0,"<p>I am interested (in theory) in conducting a stratified random sample of <span class=""math-container"">$n$</span> registered voters from a state which has 150 counties.</p>
<p>If I make all 150 counties my strata, must I conduct a random sample (weighted according size relative to the population) from all of the strata? or, may I select a simple random sample of <span class=""math-container"">$k$</span> strata from the 150 and then conduct a random sample weighted according to relative size of the <span class=""math-container"">$k$</span> strata? If so, how does one determine <span class=""math-container"">$k$</span>?</p>
<p>Thank you.</p>
",2020-12-29 20:56:35.617
333731,247248.0,1,333816.0,,,Should a Hausman test be used to decide between fixed vs. random effects?,<panel-data><multilevel-analysis><random-effects-model><fixed-effects-model><hausman>,CC BY-SA 4.0,"<p>I was taught that a Hausman test should be used in multilevel modeling in order to check whether random effects can be used.</p>
<p>However, I have now stumbled over several sources stating that the Hausman test is actually being misunderstood when using it in this way and that it rather tests whether the between and within effects are different. e.g.:</p>
<p>Snijders, T. A., &amp; Bosker, R. J. (2011). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage.</p>
<p>Bell, A., Fairbrother, M., &amp; Jones, K. (2019). Fixed and random effects models: making an informed choice. Quality &amp; Quantity, 53(2), 1051-1074.</p>
<p>The authors are referring to an article from 2004:</p>
<p>Fielding, A. (2004). The role of the Hausman test and whether higher level effects should be treated as random or fixed. Multilevel Model. Newsl. 16(2), 3–9.</p>
<p>There are, though, relatively recent textbooks treating the Hausman test as a tool to decide between fixed and random effects, e.g.:</p>
<p>Wooldridge, J. M. (2010). Econometric analysis of cross section and panel data. MIT press.</p>
<p>Could somebody please help me out of my confusion?</p>
",2020-12-29 21:09:46.480
333732,158099.0,2,,333729.0,,,,CC BY-SA 4.0,"<p>It's because the entropy in this exercise is calculated using natural logarithm. For example, for (B), we will have:</p>
<p><span class=""math-container"">$$\mathcal H_B=-2\times 0.1\times \ln(0.1)-0.8\times \ln(0.8)\approx 0.6390$$</span></p>
",2020-12-29 21:11:17.590
333733,240922.0,2,,333712.0,,,,CC BY-SA 4.0,"<p>First, let's think about why changing the scale of a variable SHOULDN'T impact the p value.</p>
<p>Let's say I'm analyzing the correlation between age and wages among a sample of workers at a specific company using a simple OLS model (Y=B0+B1*X), where Y is wage and X is age.</p>
<p>I want to know the value of B1 and whether it is significantly different from zero (that's what the p value tells me).</p>
<p>Measuring Y as &quot;dollars per hour&quot;  versus &quot;cents per hour&quot; will change the VALUE of the beta coefficient we get, but it clearly shouldn't (and won't!) change the substantive, magnitudes of relationship between these two variables, or out confidence in the result. If we find that an additional year of age is associated with an additional 0.02 dollars per hour of wage, that is precisely the same thing as finding that an additional year of age is associated with an additional 2 cents of wage. So the beta coefficient will be 0.02 in one model and 2 in the second, but the two values obviously MEAN the same thing. Since they are both describing the exact same relationship and are based on the exact same data, they really should have exactly the same p value.</p>
<p>(You could also think about this in terms of a relationship between anything and temperature: it SHOULDN'T matter whether temperature is measured in Celsius or Fahrenheit)</p>
<p>Now, the mathematical reason that the two coefficients DO, in fact, have the exact same p value is that even though the beta coefficients of the &quot;cents&quot; model is 100 times larger, the standard error of that beta coefficient is ALSO 100 times larger (this, in turn is because the standard deviation of Y is 100 times larger). So when you divide the coefficient by the standard error to get the t value you get the same answer in both models, and because you have the same t value, you have the same p value.</p>
",2020-12-29 21:19:53.333
9524,2872.0,1,9529.0,,,Are there any good movies involving mathematics or probability?,<probability><references>,CC BY-SA 3.0,"<p>Can you suggest some good movies which involve math, probabilities etc? One example is <a href=""http://en.wikipedia.org/wiki/21_%282008_film%29"">21</a>. I would also be interested in movies that involve algorithms (e.g. text decryption). In general ""geeky"" movies with famous scientific theories but no science fiction or documentaries. Thanks in advance!</p>
",2011-05-07 11:13:51.243
333736,58540.0,2,,333299.0,,,,CC BY-SA 4.0,"<p>As a first step, I want to note that you can reconstruct the coefficients for each of the groups of interest as follows. One thing you should do is make sure that T==0 is meaningful in your data, otherwise these coefficients will be less clear:</p>
<pre><code>MV1 = 0 and MV2 = 0 - Coefficient of T
MV1 = 1 and MV2 = 0 - Coefficient for MV1 when T==0 and Coefficient of MV1:T when T&gt;=1
MV1 = 0 and MV2 = 1 - Coefficient for MV2 when T==0 and Coefficient of MV2:T when T&gt;=1
MV1 = 1 and MV2 = 1 - Coefficient for MV1:MV2 when T==0 and Coefficient of MV1:MV2:T when T&gt;=1
</code></pre>
<p>I would suggest using something like <code>emmeans</code> for testing specific linear hypotheses regarding these groups beyond what is given in the regression table. The <a href=""https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html"" rel=""nofollow noreferrer"">vignette</a> on interactions will be particularly helpful for you:</p>
<pre><code>emmip(m, MV1 ~ T | MV2) #graph showing how the moderation of MV1 by T varies across levels of MV2
</code></pre>
<p>Another handy package is <code>ggeffects</code>, which will give you marginal effect estimates for the groups and a graph of the interaction effects. For example (see also <a href=""https://strengejacke.github.io/ggeffects/articles/ggeffects.html#two-way-three-way-and-four-way-interactions"" rel=""nofollow noreferrer"">documentation</a>):</p>
<pre><code>mydf &lt;- ggpredict(m, terms = c(&quot;MV1&quot;, &quot;MV2&quot;, &quot;T&quot;))

plot(mydf)
</code></pre>
",2020-12-29 21:53:27.023
333737,33473.0,2,,328393.0,,,,CC BY-SA 4.0,"<p>Multiple imputation certainly works fine with just one incomplete variable/feature. The difference is that with only one variable with missing values there is no need for the procedure to be iterative and no need for the 'chained equations' aspect of mice. The imputation model for the incomplete variable is fitted to those rows with the variable observed, posterior draws of the imputation model parameters are taken, and the multiple imputations are drawn conditional on these and the predictor variables used.</p>
",2020-12-29 21:56:49.240
333738,237141.0,1,333739.0,,,No Activation Function on Output Layer for Binary Classification,<neural-networks><classification><binary-data><metric>,CC BY-SA 4.0,"<p>In this <a href=""https://github.com/rusty1s/pytorch_geometric/blob/22bef0517cbaf51b14428fddb2be4a49dd924609/examples/seal_link_pred.py#L192"" rel=""nofollow noreferrer"">pytorch example</a>, the output layer does not have an activation function even though the neural network is being used for a binary classification task (i.e. ground truth values are either 0 = negative or 1 = positive). After inspecting the output, I can see that there are values such as -13.02 or 4.56, which are obviously not bounded between 0 and 1. Also, after adding a sigmoid activation function in myself, the performance seems to be worse than without the activation function. Thus, I have two questions:</p>
<ol>
<li>Would 0 be the threshold that determines if the predicted output is the negative or positive class? In other words, is any output value <span class=""math-container"">$\hat{y}$</span>, <span class=""math-container"">$ \hat{y}&lt; 0$</span> is negative and <span class=""math-container"">$\hat{y}&gt;= 0 $</span>is positive?</li>
<li>Why does not using an activation function lead to better performance when this is a binary classification problem, not a regression problem? Is it specific to this example?</li>
</ol>
",2020-12-29 21:57:12.027
333739,16043.0,2,,333738.0,,,,CC BY-SA 4.0,"<ol>
<li>Choosing a threshold depends on what trade-off you wish to achieve in terms of classification errors. Choosing 0 or a different number could be appropriate, depending on context.</li>
<li><a href=""https://github.com/rusty1s/pytorch_geometric/blob/22bef0517cbaf51b14428fddb2be4a49dd924609/examples/seal_link_pred.py#L209"" rel=""nofollow noreferrer"">The network is trained using <code>BCEWithLogitsLoss</code></a>, which combines the sigmoid activation and the binary cross-entropy loss into a single call. <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"" rel=""nofollow noreferrer"">This is described in the <code>pytorch</code> documentation.</a> Combining the two operations avoids round-tripping <span class=""math-container"">$\exp$</span> and <span class=""math-container"">$\log$</span>, which can cause severe loss of precision in floating-point arithmetic.</li>
</ol>
",2020-12-29 21:59:14.493
333740,190769.0,1,333744.0,,,Why does LOOCV have a higher bias than a single validation set?,<variance><cross-validation><bias>,CC BY-SA 4.0,"<p>In <em>An Introduction to Statistical Learning</em>, the following statement is made comparing leave-one-out cross validation to using a single validation set:</p>
<blockquote>
<p>LOOCV has a couple of major advantages over the validation set approach. First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain <span class=""math-container"">$n − 1$</span> observations, almost as many as are in the entire data set. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does.</p>
</blockquote>
<p>I would think that LOOCV actually just provides a better estimate than using a single validation set, since it is able to fit using more data, and therefore would have a lower <em>variance</em> rather than a lower <em>bias</em>. Why would the bias be lower?</p>
",2020-12-29 22:05:07.920
333741,57214.0,2,,333721.0,,,,CC BY-SA 4.0,"<p>You propose a test to see it two binomial proportions are the same (null hypothesis) or whether one is larger than the other (one-sided alternative).</p>
<p>Minitab, does such a test, using a normal approximation, as follows:</p>
<pre><code>Test and CI for Two Proportions 

Sample  X    N  Sample p
1       8  106  0.075472
2       8    9  0.888889

Difference = p (1) - p (2)
Estimate for difference:  -0.813417
95% upper bound for difference:  -0.636015
Test for difference = 0 (vs &lt; 0):  
  Z = -7.54  P-Value = 0.000

* NOTE * The normal approximation may be inaccurate for small samples.

Fisher’s exact test: P-Value = 0.000
</code></pre>
<p>The P-value is quite small, indicating that the second method ('campaign') is significantly better at any reasonable level of significance. However, because the sample size for the second method is so small, the normal approximation may not give an <em>exactly</em> correct P-value.</p>
<p>Fisher's exact test is often used when sample sizes are too small for
a normal approximation to give useful results. It also gives a tiny P-value.
You can google for discussions of Fisher's exact test, if you are not
familiar with it.</p>
<p><em>Note:</em> I have one serious reservation about this inference. If your brother chose the one &quot;most successful&quot; campaign out of many, just on the basis of the high number of successes, then I wonder about
the comparison. If we pick the best out of <strong>dozens</strong> of his campaigns (perhaps with widely varying results with low numbers of trials), then a low P-value would not be surprising (or convincing).</p>
",2020-12-29 22:11:04.953
333787,141683.0,1,333795.0,,,Notation for expected value,<expected-value><kernel-smoothing><notation>,CC BY-SA 4.0,"<p>I often find the following notation in ML Papers, with <span class=""math-container"">$k$</span> as kernel:</p>
<p><span class=""math-container"">$E_{x, y \sim p} [ k(x, y ) ]$</span></p>
<p>I am not familiar with it. Is it the expected value with <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span> as a random variable with distribution <span class=""math-container"">$p$</span>?</p>
",2020-12-30 12:24:36.363
333744,199514.0,2,,333740.0,,,,CC BY-SA 4.0,"<p>LOOCV has a lower variance of the <em>fit</em> compared with the validation set approach, but its aim is not the fit but the estimation of the generalisation error. What we'd like to have is an estimate of the generalisation error from a fit based on <span class=""math-container"">$n$</span> observations. A validation set approach where you split the data into two halves will get you an estimate of the generalisation error from a fit based on half of the observations. As the fit will have a larger variance, the generalisation error will on average be estimated too high (variance in the fit translates into bias in the generalisation error, because a fit going very wrong in <em>any</em> direction of the observation you want to predict will yield a high error). LOOCV gives us an estimate of the generalisation error from a fit based on <span class=""math-container"">$n-1$</span> observations. Lower than <span class=""math-container"">$n$</span>, therefore still bias in the generlisation error, but lower, because <span class=""math-container"">$n-1$</span> observations will allow for a more precise fit.</p>
",2020-12-29 22:25:41.423
333745,100031.0,1,,,,Capitalization of Type in Type I error,<hypothesis-testing>,CC BY-SA 4.0,"<p>I am curious whether the &quot;Type&quot; in &quot;Type I&quot; error should be capitalized, or whether it should be rendered as &quot;type I&quot;. <a href=""https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error"" rel=""nofollow noreferrer"">Wikipedia</a> appears to use the lower case, but this seems to be the <a href=""https://en.wikipedia.org/wiki/Talk:Type_I_and_type_II_errors#Capitalization_of_the_terms_type_I_and_type_II_errors"" rel=""nofollow noreferrer"">arbitrary choice of one editor</a>. I did a brief search of Wasserman's <em>All of Statistics</em> and found mixed capitalization (upper case in some tables, lower case in the index). <em>Elements of Statistical Learning</em> seems to use mostly capitalized.</p>
<p>I anticipate this question will be downvoted for not having a right answer. However, perhaps answers could serve as a kind of poll of whether users prefer capitalized Type or not.</p>
",2020-12-29 23:20:41.897
333746,253814.0,2,,333742.0,,,,CC BY-SA 4.0,"<p>You may have made a calculation error -- your approach is correct, and if I compute it in, e.g., R, I find the same result as your book's solution:</p>
<pre><code>sum(sapply(1:5,function(x) (20/38)^(x-1) * (18/38)))
</code></pre>
<p>Output:</p>
<pre><code>0.9596139
</code></pre>
",2020-12-29 23:29:41.520
333747,246708.0,1,333748.0,,,Can you compare loss values of Neural Networks that use different loss functions?,<machine-learning><neural-networks><loss-functions>,CC BY-SA 4.0,"<p>I've been training neural networks for a text classification task. Since I'm new to this, I switched Loss functions when moving onto a different model at some point. Can I measure these models' Test loss against each other. Can I compute Test loss on these models using the same loss function 'retroactively'?.</p>
<p>Edit: Say I had models A and B, I used a different loss function for both of them. I wanted to know if I could use a loss function C, perhaps different to either used by the 2 models, to evaluate them both after training</p>
",2020-12-29 23:43:18.550
333748,199619.0,2,,333747.0,,,,CC BY-SA 4.0,"<p>Yes, and this is what's happening when you tune the regularization hyperparameter in a regularized regression.</p>
<p>In, say, LASSO regression, you optimize your <span class=""math-container"">$\hat{\beta}$</span> parameter with <span class=""math-container"">$\vert\vert y -X\hat{\beta}\vert\vert_2^2 + \lambda\vert\vert\hat{\beta}\vert\vert_1$</span>. This is a different loss function for every value of <span class=""math-container"">$\lambda$</span>. However, when you determine the best value of <span class=""math-container"">$\lambda$</span> for your production model, you evaluate your models on the same square loss function.</p>
<p>The same idea applies to a neural network. In fact, in <a href=""http://yann.lecun.com/exdb/mnist/"" rel=""nofollow noreferrer"">LeCun's list of MNIST performance</a>, all models report accuracy<span class=""math-container"">$^{\dagger}$</span> as their performance metric, yet one uses Brier score as the training loss function while most use crossentropy.</p>
<p><span class=""math-container"">$^{\dagger}$</span><a href=""https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models"">Set aside the issues with threshold-based metrics like accuracy</a>.</p>
",2020-12-30 00:03:56.200
333749,51433.0,2,,212670.0,,,,CC BY-SA 4.0,"<p>So the accepted answer uses rejection sampling and is very slow. The second answer sounds like a nice idea, but I don't follow the logic, and when I implemented it, the histograms did not match the ground truth PDFs.</p>
<p>However, SciPy has implemented this for the univariate case, and their code references the relevant paper to extend this to multivariate settings: <a href=""https://arxiv.org/abs/0911.2093"" rel=""nofollow noreferrer"">Statistical applications of the multivariate skew-normal distribution</a>.</p>
<p>Here is some code in Python:</p>
<pre class=""lang-py prettyprint-override""><code>def sample(shape, cov, size=1):
    dim      = len(shape)
    assert(cov.shape == (dim, dim))
    aCa      = shape @ cov @ shape
    delta    = (1 / np.sqrt(1 + aCa)) * cov @ shape
    cov_star = np.block([[np.ones(1), delta], [delta[:, None], cov]])
    x        = mvn(np.zeros(dim+1), cov_star).rvs(size)
    x0, x1   = x[:, 0], x[:, 1:]
    inds     = x0 &lt;= 0
    x1[inds] = -1 * x1[inds]
    return x1
</code></pre>
<p>which I used to generate these histograms with groundtruth PDFs in white. I have more details on <a href=""http://gregorygundersen.com/blog/2020/12/29/multivariate-skew-normal/"" rel=""nofollow noreferrer"">my blog</a>.</p>
<p><a href=""https://i.stack.imgur.com/2VdI7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2VdI7.png"" alt=""enter image description here"" /></a></p>
",2020-12-30 00:18:47.870
333750,255209.0,1,,,,How is this formula for variance derived?,<probability><variance><binomial-distribution>,CC BY-SA 4.0,"<p>There's a formula for the variance of the traffic flow between A and B, calculated from sample data, quoted in the UK's Traffic Appraisal Manual. No proof is given and part of me really wants to know how this formula is derived. I've spent a good couple of hours trying, but have failed miserably and I can't find anything on the internet either. I would be very grateful for some help.</p>
<p>The form in the Manual is:<span class=""math-container"">$$
Var(Q_{AB})=\frac{Q(Q-q)}{q^2(q-1)}\cdot{}q_{AB}(q-q_{AB})
$$</span></p>
<p>Where <span class=""math-container"">$Q$</span> is the total flow at the survey location, <span class=""math-container"">$q$</span> is the flow which is surveyed (i.e. the sample size), <span class=""math-container"">$q_{AB}$</span> is the <strong>sampled</strong> flow travelling from A to B, and <span class=""math-container"">$Q_{AB}$</span> is the total flow from A to B.</p>
<p>This can be rearranged into the more intuitive form of:<span class=""math-container"">$$
Var(Q_{AB})=\frac{(Q-q)\cdot{}Q\cdot{}p_{AB}\cdot{}(1-p_{AB})}{q-1}
$$</span></p>
<p>Where <span class=""math-container"">$p_{AB}$</span> is the proportion of sampled traffic travelling from A to B.</p>
<p>Even though this looks more logical, I still can't fathom how this comes from the formula for variance of the binomial distribution, which I thought would be
<span class=""math-container"">$$Var(Q_{AB})=\frac{p_{AB}(1-p_{AB})}{q-1}$$</span></p>
<p>Thanks in advance for any help!</p>
<p>For any curious, the section of the manual in question is here:
<a href=""https://i.stack.imgur.com/CHxd3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CHxd3.png"" alt=""Screen capture of clauses D13.2 and D13.3 from DMRB Volume 12 Section 1 Part 1"" /></a></p>
",2020-12-30 00:47:55.660
333751,255208.0,1,,,,"Poisson regression or ANOVA, repeated measures or independent?",<anova><generalized-linear-model><repeated-measures><poisson-regression>,CC BY-SA 4.0,"<p>I have been trying to figure out the best way to approach analysis of my data for a while now and I'm struggling to understand if a Poisson regression is correct and, embarrassingly, I'm not sure if my data are independent observations.</p>
<p>I have two data sets:</p>
<p>Firstly,</p>
<p>I have data from rows sown with 100 seeds each (n=6) with counts of seedlings recorded at different stages of seedling development (germinated, emerged etc, 6 stages). The same seeds/seedling were tracked over time. I want to know which stages have significantly different counts of seeds/seedlings.
Are these independent observations as they are different outcome variables (germinated, emerged etc)? Note: each time point can only be equal to or less than the preceding time point.
Should this be approached with a GLM with Poisson distribution or can it be a simple one-way ANOVA or a repeated measures ANOVA?</p>
<p>My second data set has data from 10 seeds per pot (n=6 pots) for two species, under 3 different treatments. There are four outcome variables (germinated, ungerminated etc) with the seeds proportioned into one of the four outcome variables (if 80% germinate, for example, then 10% could be ungerminated and 10% dead). This means quite a lot of 0's in some cases.
I have performed a Poisson regression in R to find significant differences between treatments for each outcome variable and it makes sense but I want to check that this is the best approach and meets the correct assumptions but, I'm confused..</p>
",2020-12-30 01:00:13.013
333752,254711.0,2,,333647.0,,,,CC BY-SA 4.0,"<p>A random walk has the property that its derivative should be pure uncorrelated noise. In other words, the autocorrelation of the steps for any lag should be 0.</p>
<p>We can permute the order of your steps a couple of times to obtain a distribution of autocorrelations for lag 1:</p>
<p><a href=""https://i.stack.imgur.com/4TWTc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4TWTc.png"" alt=""enter image description here"" /></a></p>
<p>The lag 1 autocorrelation of your steps A is -0.5606 and for your steps B 0.7000.</p>
<p>I presume the high negative autocorrelation for steps A is because you deliberately tried to make your example appear as 'random' as possible and it therefore zig-zags around more than a truly random walk would? That is, you seem to have constructed something more akin to violet noise?</p>
<p>In any case, this distribution allows us to calculate the probability of finding a lag 1 autocorrelation as extreme as yours. One way to do this is to perhaps take the absolute value of the autocorrelations (as we had no clear hypothesis about the sign) and then to count how many simulated samples are as extreme as the observed ones. In my simulation 6.06% of samples were more extreme than steps A but only 1.16% of samples were more extreme than steps B.</p>
<p><a href=""https://i.stack.imgur.com/0nJz6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0nJz6.png"" alt=""enter image description here"" /></a></p>
",2020-12-30 01:26:17.073
333753,255213.0,1,,,,Bootstrapping OLS coefficient variance different results than Python Statsmodels,<least-squares><standard-deviation><bootstrap><simulation><statsmodels>,CC BY-SA 4.0,"<p>I wanted to test some basic OLS things in python with a sample dataset. One thing I wanted to test was just getting a bootstrap estimate of the variance of the model's coefficients.</p>
<p>Here is the setup code:</p>
<pre class=""lang-py prettyprint-override""><code>    from sklearn.datasets import load_boston
    import pandas as pd
    import numpy as np
    
    X, y, cols = list(load_boston().values())[:3]
    
    data = pd.DataFrame(X)
    
    data.columns=cols
    
    data['label']=y
    
    tv_cut=int(.66*len(data))
    data_train = data.iloc[:tv_cut]
    data_test = data.iloc[tv_cut:]
    
    X_train, y_train = data_train.drop('label', axis=1), data_train.label
    X_test, y_test = data_test.drop('label', axis=1), data_test.label
    
    # standardize
    X_train-=X_train.mean()
    X_test-=X_test.mean()
    X_train/=X_train.std()
    X_test/=X_test.std()
    
    X_train['intercept']=1
    X_test['intercept']=1
    
    def fit_model(X_train, y_train):
        beta = X_train.T.dot(X_train)
        beta=np.linalg.inv(beta).dot(X_train.T).dot(y_train)
    
        return pd.Series(dict(zip(X_train.columns, beta)))
    
    beta = fit_model(X_train, y_train)
    
    # beta
    # CRIM          0.799692
    # ZN            0.324976
    # INDUS         0.176654
    # CHAS          0.210327
    # NOX          -0.921285
    # RM            6.362226
    # AGE          -1.357903
    # DIS          -1.876253
    # RAD           0.194442
    # TAX          -0.973645
    # PTRATIO      -1.422592
    # B             0.667066
    # LSTAT        -0.552090
    # intercept    25.227027
    # dtype: float64
</code></pre>
<p>These parameters ultimately match what both a <code>sklearn</code> model and a <code>statsmodels</code> models give me. I then go and compute what I thought should be the standard deviations of the coefficients</p>
<pre class=""lang-py prettyprint-override""><code>    betas_std=np.diag(y_train.var()*np.linalg.inv(X_train.T.dot(X_train)))**.5
    # CRIM         0.816758
    # ZN           0.711078
    # INDUS        0.745527
    # CHAS         0.489583
    # NOX          1.123602
    # RM           0.746347
    # AGE          0.791013
    # DIS          0.874786
    # RAD          0.535846
    # TAX          0.591263
    # PTRATIO      0.609934
    # B            0.556539
    # LSTAT        0.821042
    # intercept    0.471111
</code></pre>
<p>when compared to the <code>statsmodels</code> results below these figures differ by a constant factor of <code>~2.77</code> and I am not sure why.</p>
<p>Further, I attempt to take the boostrap estimate with what seem like natural parameters of 1k rounds and sampling 50% each time:</p>
<pre class=""lang-py prettyprint-override""><code>    betas = []
    for _ in range(1000):
        X_temp=X_train.sample(frac=.5)
        y_temp=y_train.loc[X_temp.index]
        betas.append(fit_model(X_temp, y_temp))
    
    betas = pd.concat(betas, axis=1).T
    
    betas.std()
    # CRIM         0.369258
    # ZN           0.267626
    # INDUS        0.310513
    # CHAS         0.231419
    # NOX          0.499343
    # RM           0.327914
    # AGE          0.372655
    # DIS          0.356496
    # RAD          0.199193
    # TAX          0.202993
    # PTRATIO      0.231562
    # B            0.280280
    # LSTAT        0.455236
    # intercept    0.186017
</code></pre>
<p>changing the number of rounds and sampling % changes these results. The baseline model which I've assumed is correct is:</p>
<pre class=""lang-py prettyprint-override""><code>    import statsmodels.api as sm
    ols = sm.OLS(y_train, X_train)
    ols_result = ols.fit()
    
    ols_result.bse
    
    # CRIM         0.294650
    # ZN           0.256525
    # INDUS        0.268953
    # CHAS         0.176620
    # NOX          0.405346
    # RM           0.269249
    # AGE          0.285362
    # DIS          0.315584
    # RAD          0.193309
    # TAX          0.213301
    # PTRATIO      0.220037
    # B            0.200775
    # LSTAT        0.296195
    # intercept    0.169956
</code></pre>
<p>What is the cause of the difference between the formulaic calculation of the standard deviation and the <code>statsmodels</code> result? What is/are the cause(s) of the different values from the sampling loop?</p>
",2020-12-30 02:30:51.887
333754,255140.0,2,,333688.0,,,,CC BY-SA 4.0,"<p>Thank you fblundun and thank you Adam k.</p>
<p>Since <span class=""math-container"">$Q$</span> is countable, let <span class=""math-container"">$(x_n)_{n\geq 1}\in (0, 1]\cap Q\in A$</span> for some countable sequence of <span class=""math-container"">$x_1,x_2,\ldots.$</span>  Define <span class=""math-container"">$A_n=(x_n − 1/2^{n+1}, x_n] \cup Q\in A,\forall n$</span>;
define <span class=""math-container"">$B_1= A_1$</span>, <span class=""math-container"">$B_i = A_i \setminus \cup^{i−1}_{j=1}A_j,~i=1,2,\ldots n. $</span> Since <span class=""math-container"">$x_n\in A_n,~\forall n,\cup_i A_i= (0, 1]\cap Q$</span>.</p>
<p><span class=""math-container"">$$\mu(\cup_iB_i)=\mu(\cup_i A_i)=1.$$</span> However, <span class=""math-container"">$$\sum_i \mu(B_i)\leq \sum_i\mu(A_i)= \sum_i1/2^{i+1}=1/2.$$</span> Since
<span class=""math-container"">$\mu(\cup_iB_i)\neq\sum_i\mu(B_i)$</span>, <span class=""math-container"">$\mu$</span> is not <span class=""math-container"">$\sigma$</span>-additive.</p>
",2020-12-30 02:45:29.963
333755,56068.0,1,,,,What is a reasonable number of splits (maximum) for a ensemble classifier?,<machine-learning><matlab><ensemble-learning><adaboost>,CC BY-SA 4.0,"<p>I am trying to use an ensemble classifier (honing in on Matlab fitcensemble).  I've also explored using a single decision tree as well as tree bagging (Matlab fitctree, TreeBagger)</p>
<p>Simple binary (A/B) classification.   My training dataset is imbalanced (~5% B).  Currently playing with 24 features, but that could change.  But I have ~1.7 million of rows. I could have 10x that number of rows if I wanted.</p>
<p>I am trying to decide on parameters related to tree size.  &quot;Min Leaf Size&quot; and &quot;Max Number of Splits&quot;.  I'm exploring the ('OptimizeHyperparameters','all') option which is exploring a variety of settings, but it is showing what I already figured out, which is that having a large number of splits gives better performance.  (It seems to be honing in on the AdaBoostM1 method.)</p>
<p>When my max number of splits is ~1/2 my number of rows I am getting
Sensitivity (for B):  98% (train), 86% (test)
Specificity (for A): 99% (train), 93% (test)</p>
<p>Really I want better than 86% Sensitivity, however my gut tells me having N splits be ~1/2 rows is too high.</p>
<p>The asker in this question states they set their maximum number of splits to 1/16th the number of rows in their dataset:</p>
<p><a href=""https://stats.stackexchange.com/questions/482108/balance-classifier-performance-boosting-ensemble"">Balance classifier performance (boosting ensemble)</a></p>
<p>That sounds like they are calling upon a rule of thumb (which I value), but its just one example.  Can anyone provide guidance/reference for how many splits is reasonable?  Beyond simply trying many options and see what works?</p>
",2020-12-30 02:47:58.937
333756,35107.0,2,,333299.0,,,,CC BY-SA 4.0,"<p>You can do something like this:</p>
<pre><code>library(emmeans)

emt = emtrends(m, ~ MV1  * MV2, var = &quot;T&quot;)

emt    # list the estimated slopes at each factor combination
pairs(emt)   # pairwise comparisons of these slopes
</code></pre>
<p>You can do restricted comparisons via adding, say, <code>by = &quot;MV2&quot;</code>. You may also average these slopes over one of the factors, for example,</p>
<pre><code>emmeans(emt, ~ MV1)
pairs(.Last.value)
</code></pre>
<p>Note: To display the fitted lines (whose slopes are summarized in <code>emt</code>), use</p>
<pre><code>emmip(m, MV1 - T | MV2, cov.reduce = range)
    # or #
emmip(m, MV1 * MV2 ~ T, cov.reduce = range)
</code></pre>
<p>(Depending on whether you want two panels with two lines each, or one panel with all four lines.) The <code>emmip()</code> call shown in the other answer will plot only single points, because predictions for numeric predictors are made by default only at their mean. We specified instead to use the minimum and maximum <code>T</code> values.</p>
",2020-12-30 02:56:40.297
333757,255216.0,1,333761.0,,,How does regularization reduce overfitting for a linear decision boundary (logistic regression)?,<logistic><regularization><overfitting>,CC BY-SA 4.0,"<p>I understand that for higher-order polynomials, reducing the weights of individual features can help to avoid complex functions that are overfit to the training data in a logistic regression classifier.</p>
<p>But I'm not entirely sure how this is the case for <b>non-polynomial features</b> (e.g. there are <i>no</i> <span class=""math-container"">$x_1^2$</span>, <span class=""math-container"">$x_1\cdot x_2$</span>, etc. terms where <span class=""math-container"">$x_1$</span>, <span class=""math-container"">$x_2$</span>, etc. are features inherent to some original dataset). In other words, why is it that when I lower or completely remove the regularization term <span class=""math-container"">$\sum_j\theta_j^2$</span>, my classifier will be more sensitive to outliers?</p>
<p>For instance, decision boundary of an SVM with <span class=""math-container"">$C = 1000$</span> (more regularization):
<a href=""https://i.stack.imgur.com/4GkWk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4GkWk.png"" alt=""enter image description here"" /></a></p>
<p>And an SVM with <span class=""math-container"">$C = 1$</span> (less regularization):
<a href=""https://i.stack.imgur.com/tUedD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tUedD.png"" alt=""enter image description here"" /></a></p>
<p>I think the reason for my confusion is that I can fully imagine a dataset that would be well fit by something like a quadratic function but where the feature space is very large and so it can be easy to overfit the dataset with a complex higher order function. I can't visualize anything of the sort for the <i>linear</i> case.</p>
",2020-12-30 03:05:01.847
333758,128628.0,2,,266712.0,,,,CC BY-SA 4.0,"<p>If you do not impose any restrictions on the coefficients then yes, the general ARMA model is the most general, and it subsumes the ARIMA model.  The general ARMA model includes both the ARIMA model as well as &quot;explosive&quot; cases.  However, it is common to impose the implicit condition that the auto-regressive part of the ARMA model is stationary (autoregressive roots outside the unit circle), and in this latter case, the ARIMA model (with stationary AR part) is the more general, and it subsumes the stationary ARMA model.  The relevant subset relations are shown in the diagram below.</p>
<p>This issue can be rather confusing because time-series books and notes often fail to explicitly differentiate between the general ARMA model (allowing any coefficient values) and the stationary ARMA model (requiring the roots of the AR part to be outside the unit circle).  Many texts regard the &quot;explosive&quot; cases as being of no interest, and they also wish to exclude the ARIMA model from the ARMA form, so they will often implicitly assume they are dealing with the <em>stationary</em> ARMA model without mentioning the constraint explicitly.  This means that you sometimes have to &quot;read between the lines&quot; with these texts and see the way they are treating their models.</p>
<p><a href=""https://i.stack.imgur.com/RLT1s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLT1s.png"" alt=""enter image description here"" /></a></p>
",2020-12-30 03:24:35.667
333759,24790.0,2,,333751.0,,,,CC BY-SA 4.0,"<p>Welcome to this site, Scott!  It's a bit hard from your description to tease out what exactly is going on in your two studies.  You may need to revise your question and provide more specifics.</p>
<p>Your first study seems to involve different time points, although you don't share how many.  Are we talking about 3 time points or 20 time points, say? Also, you mention that counts are recorded at each time point but you don't explain what those counts represent (e.g., are they counts of seeds that germinated?), what entities they are recorded for and, most importantly, whether those entities are kept the same at each time point or they are different from one time point to another? Also, what do you mean by replicate rows?</p>
<p>Your research question for the first study needs to be stated more carefully: <em>&quot;I want to know which time points are significantly different (i.e. significantly less than the preceding time point).&quot;</em>. I suspect what you really mean is that you want to know whether the <strong>expected/average value of your count outcome variable</strong> at time point <span class=""math-container"">$t$</span> is less than that at time point <span class=""math-container"">$t-1$</span>?  (It's clear that the time points themselves are different from each other by virtue of how you chose them.)</p>
<p>If you measure the same entity over time with respect to a variable of interest, you can reasonably expect temporal correlation among the recorded values of that variable for that entity.  As an example, if you track the same seed over time and at each time point you record whether or not that seed germinated, the germination status values (yes or no) will likely be correlated over time.</p>
<p>Your second study needs more clarifications also. How many treatments are we talking about? Etc.</p>
",2020-12-30 03:26:16.177
333760,255219.0,1,,,,How to calculate a var of the sum of two coefficients in linear regression,<variance><estimation><inference>,CC BY-SA 4.0,"<p>Essentially after performing regression on three variables,</p>
<p><span class=""math-container"">$$
y = a_0 + a_1 \cdot x_1 + a_2 \cdot x_2 + a_3 \cdot x_3
$$</span></p>
<p>I want to find variance for <span class=""math-container"">$a_1+a_2$</span> to get CI. Logically, I think I can do</p>
<p><span class=""math-container"">$$\text{Var}(a_1+a_2)=\text{Var}(a_1)+\text{Var}(a_2)+\text{Cov}(a_1,a_2)$$</span></p>
<p>and calculate covariance of two normals because from the model results I'd know mean and variance of <span class=""math-container"">$a_1$</span> and <span class=""math-container"">$a_2$</span>, and they are asymptotically normally distributed.</p>
<ol>
<li>I'm stuck at how to get covariance of two normal RV. Any guidance?</li>
<li>Is there a simple code to calculate this in python or R?</li>
</ol>
",2020-12-30 03:31:12.067
333761,,2,,333757.0,user233429,,,CC BY-SA 4.0,"<p>The idea of regularization is related to the Bayesian idea of shrinkage: you are biasing your model towards a constant fit (hence the penalty on non-zero coefficients). The regularization factor defines the &quot;price&quot; your model must pay to use that coefficient (i.e., make non-zero).</p>
<p>Therefore, if you have noisy data, the noise terms will tend to not give much reduction in RMS or whatever loss metric you have, so the model will think its better off not including them (to save on the &quot;price&quot; of using that coefficient).</p>
<p>That is how regularization reduces overfitting -- in logistic or any other linear model.</p>
",2020-12-30 03:32:16.967
333788,69747.0,2,,333771.0,,,,CC BY-SA 4.0,"<p>I see some evidence of heteroscedasticity, with seemingly larger variability to the right. More importantly, there is a suggestion of nonlinearity (not related to heteroscedasticity) that can be better visualized by overlaying the loess smooth.</p>
<p>But tests for model specification are mostly irrelevant. If the BP test &quot;passed&quot;, it is simply a Type II error, because heteroscedasticity is true in reality when <span class=""math-container"">$Y$</span> and <span class=""math-container"">$X$</span> are related. Rather than test for heteroscedasticity, why not just model it? Then you will have a better idea as to the size of the effect. Use that information, along with subject matter considerations, supplemented with simulation study as needed, to decide whether to ignore it.</p>
",2020-12-30 12:46:22.690
333789,232582.0,2,,333690.0,,,,CC BY-SA 4.0,"<p>There are 3 problems with the explanations:</p>
<ol>
<li><p>For testing, you mention that you calculate &quot;the&quot; average score. You are looking to compare 2 computer programs, so unless you know the actual expectancy for one of them (which you don't in this setting), you'll have to calculate two averages: one for the old program, one for the new and test if the difference is equal to 0.3 (Tip: a more interesting test would be to test if the difference in the means is &gt;0, i.e. a right-tailed test.)</p>
</li>
<li><p>As @BruceET correctly points out in the comment on the OP, we accept (i.e. can't reject) the null Hypothesis <span class=""math-container"">$H_0$</span> if the test statistic is within the critical interval, i.e. lower than the critical value (if we have a symmetric distribution), <em>not</em> greater. If it's greater, we reject the null and accept the alternative.</p>
</li>
<li><p>Lastly, the term &quot;threshold&quot; and &quot;cutoff-off&quot; seem very vague (almost arbitrary) and gives the impression that you consider the critical interval and p-values to be different tests - which they are not. To clarify this, see longer answer below.</p>
</li>
</ol>
<hr />
<p><strong>(Frequentist) Hypothesis testing</strong></p>
<p>Generally speaking, the critical interval and the p-value answer the same question with regard to the test statistic. They are not 2 different 'tests' per se, but rather 2 different ways of calculating the exact same thing.</p>
<p>The idea of hypothesis testing is to setup two hypotheses: the null <span class=""math-container"">$H_0$</span> and the alternative <span class=""math-container"">$H_1$</span>. Then express them in terms of a statistical parameter. [In your case: <span class=""math-container"">$H_0:$</span> new program performs on average 0.3 better than old one. Reformulated in terms of the average score <span class=""math-container"">$\mu$</span>: <span class=""math-container"">$H_0: \mu_{new}-\mu_{old}=0.3$</span>]. Then formulate any random variable that can be calculated from sample data and transform it in such a way that its distribution is known if <span class=""math-container"">$H_0$</span> were true. This new transformed random variable is called the test statistic. [In your case we can use the averages of 100 empirical scores: If we assume that the scores are <span class=""math-container"">$iid$</span> then we can assume <span class=""math-container"">$\bar{X}_{new}\approx \mu_{new}$</span> and <span class=""math-container"">$\bar{X}_{old}\approx \mu_{old}$</span> by the LLN. We can assume <span class=""math-container"">$\bar{X}_{new}\overset{a}{\sim} \mathcal{N}(\mu_{new},\frac{\sigma_{new}^2}{n})$</span> by the CLT. Analogously for <span class=""math-container"">$\bar{X}_{old}$</span>. Our test statistic is <span class=""math-container"">$t=\bar{X}_{new}-\bar{X}_{old} \Leftrightarrow t \overset{a}{\sim} \mathcal{N}(0.3,\frac{s^2}{n})$</span>, where <span class=""math-container"">$s$</span> is the sample standard deviation of the differences.]</p>
<p>Let's say we have a random variable <span class=""math-container"">$t$</span> (a test statistic) and we know its distribution assuming <span class=""math-container"">$H_0$</span> is true. We then set a significance level e.g. <span class=""math-container"">$\alpha=5\%$</span>, and calculate (for a two-tailed test) the 2 quantiles of the distribution of <span class=""math-container"">$t$</span> that satisfy that at least <span class=""math-container"">$1-\alpha=95\%$</span> of probability mass is between them. These two quantiles enclose the critical interval. If <span class=""math-container"">$quantile_1=-quantile_2$</span>, i.e. if the distribution is symmetric, then we say <span class=""math-container"">$|quantile_1|$</span> is the critical value and compare it to <span class=""math-container"">$|t|$</span>. If <span class=""math-container"">$|t|&lt;|quantile_1|$</span> then we can't reject <span class=""math-container"">$H_0$</span>. So the critical interval describes the space that our test statistic <em>has to be in</em> in order for <span class=""math-container"">$H_0$</span> not be rejected.</p>
<p>The p-value is kind of the opposite: it focuses on the area <em>outside</em> of the critical interval. It calculates the probability of <span class=""math-container"">$t$</span> or more extreme values occurring <em>if <span class=""math-container"">$H_0$</span> were true</em>. If this probability is larger than our significance level <span class=""math-container"">$\alpha$</span> then we do not reject.</p>
<p>You can use either of the two: critical interval or p-value, they will lead to the same conclusion since one follows from the other and vice-versa. (A third option would be comparing the random variable to the confidence intervals - still same result.) Note that <span class=""math-container"">$\alpha$</span> is set in advance and used for both. Below a plot to visualize all these values. In the plot, the null is accepted (not rejected).</p>
<p><a href=""https://i.stack.imgur.com/YDKg3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YDKg3.png"" alt=""enter image description here"" /></a></p>
",2020-12-30 12:46:54.460
333762,255115.0,2,,333760.0,,,,CC BY-SA 4.0,"<p>you can use <code>vcov(model)</code> in R to find the covariance matrix.</p>
<pre><code>a = rnorm(100)
b = rnorm(100,1,1)
c = rnorm(100,2,2)
y = rnorm(100,3,1)
m1 = lm(y~a+b+c)
</code></pre>
<p>Assume you have a linear model <span class=""math-container"">$y = \beta_1 \cdot a + \beta_2 \cdot b + \beta_3 \cdot c+\epsilon$</span>  where <span class=""math-container"">$a, b, c$</span> are the regressors, then you can use the above code to fit the model.
Then simply type <code>vcov(m1)</code>, you can get the variance-covariance matrix.</p>
<pre><code>&gt; vcov(m1)
              (Intercept)             a             b             c
(Intercept)  0.0236168925  0.0008928804 -0.0072752173 -0.0048195656
a            0.0008928804  0.0089417637 -0.0007706158 -0.0005058700
b           -0.0072752173 -0.0007706158  0.0084035744  0.0002730054
c           -0.0048195656 -0.0005058700  0.0002730054  0.0022051924
</code></pre>
<p>Then you can use the ordinary formula to get the CI.</p>
<p>btw: <span class=""math-container"">$\text{Var}[X+Y] = \text{Var}[X] + \text{Var}[Y] + 2 \cdot \text{Cov}[X,Y]$</span></p>
",2020-12-30 03:52:21.280
333763,199192.0,2,,333663.0,,,,CC BY-SA 4.0,"<p>I will try and tackle your concerns piecemeal and hopefully you can find a model that suits your needs.</p>
<blockquote>
<p>The difficulty is that I don't know a good method to account for both country-effects, and time-effects.</p>
</blockquote>
<p>Unless I am misinterpreting the way your data is structured, you should be able to implement a model with both <em>unit</em> and <em>time</em> fixed effects. To state this in language specific to your study, your model would be including <em>country</em> and <em>day</em> fixed effects. In your setting, you observe 220 countries over 4 years (<span class=""math-container"">$T\approx 1,460$</span> days), for a near 321,200 country-day (<span class=""math-container"">$N \times T$</span>) observations. Admittedly, this is a lot of dummies to sort through in your output. Standard software packages offer tools to help suppress the unit and time effects. You could also estimate your model using the within-transformation. I am partial to the latter given the large number of country and day effects.</p>
<blockquote>
<p>I could do a standard regression analysis, where each observation is a country's mean (over all times) values of <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span>.</p>
</blockquote>
<p>Perhaps.</p>
<p>Averaging all countries over their days <span class=""math-container"">$T$</span> to obtain a &quot;country mean&quot; and estimating a standard linear model on the averaged data is a &quot;between-effects&quot; model. You could certainty use this estimator, but you're limiting yourself. You can no longer exploit the longitudinal variation.</p>
<blockquote>
<p>I could also do a Granger Causality Test, using just two time-series, where each date shows the mean of all the countries' <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> values.</p>
</blockquote>
<p>You could easily implement a procedure for testing Granger causality in panel datasets. You do not need to take the average over time <span class=""math-container"">$T$</span> to do this.</p>
<blockquote>
<p>The goal is to integrate information from BOTH dimensions. Standard regression, of course, fails to account for temporal effects present in time-series data.</p>
</blockquote>
<p>It is unclear what you mean by &quot;standard regression&quot; in this statement. I have yet to view the video you referenced so I am not sure what someone else has said about this, but I find no valid reason why you cannot estimate a model with time effects.</p>
<blockquote>
<p>Meanwhile, taking the mean across countries would erase &quot;spikes&quot; or temporal patterns present in just some of the countries in the dataset.</p>
</blockquote>
<p>Correct.</p>
<p>It isn't necessary, though. If you want to estimate this model, go for it. Just be mindful of what variation you're trying to exploit.</p>
<blockquote>
<p>Is there a way to extend statistical tests for time-series (like Granger Causality) to multiple pairs of time-series data?</p>
</blockquote>
<p>Sure.</p>
<p>But why can't it work with your dataset as it is currently structured? I assume your dataset is a country-day panel with observations &quot;stacked&quot; on top of each other. For example, you should have separate columns for country and time. By &quot;stacked&quot; I mean the following:</p>
<p><span class=""math-container"">$$
\begin{array}{ccc}
country &amp; time &amp; x &amp; y \\
\hline
 India &amp; 1 &amp; 0 &amp; .47 \\
 India &amp; 2 &amp; 0 &amp; .93 \\
 India &amp; 3 &amp; 0 &amp; .04 \\
 India &amp; 4 &amp; 0 &amp; .47 \\
 India &amp; 5 &amp; 0 &amp; .21 \\
 \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
 India &amp; 1,459 &amp; 0 &amp; .19 \\
 India &amp; 1,460 &amp; 0 &amp; .67 \\
\hline
 USA &amp; 1 &amp; 0 &amp; .12 \\
 USA &amp; 2 &amp; 0 &amp; .51 \\
 USA &amp; 3 &amp; 0 &amp; .09 \\
 USA &amp; 4 &amp; 0 &amp; .75 \\
 USA &amp; 5 &amp; 0 &amp; .13 \\
 \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
 USA &amp; 1,459 &amp; .41 &amp; .95 \\
 USA &amp; 1,460 &amp; .73 &amp; .81 \\
\end{array}
$$</span></p>
<p>The column <span class=""math-container"">$time$</span> denotes your <em>days</em>. To appropriately include time effects, you must instantiate a variable to capture all days a country is observed. It models the common shocks to all entities in <strong>every time period</strong>. It should <em>not</em> represent a &quot;repeating&quot; temporal interval, such as days-per-month (e.g., 1-30) or days-per-year (e.g., 1-365). It should distinguish between day 365 and 366, hence why day 1,460 is available for each country. It might be easier to concatenate year and day together or simply year-month-day (e.g., 2017-01-01). However you do it, you must ensure your time variable is coded in a way that it captures a separate effect for all time periods in your panel. In R, you could use <code>as.factor(time)</code> which 'dummies out' the time effects for you. However, this amounts to the estimation of 1,459 day effects. This could become unwieldy in practice as <span class=""math-container"">$T$</span> grows. I would look into more efficient ways of estimating your fixed effects. I highly recommend the <a href=""https://cran.r-project.org/web/packages/plm/plm.pdf"" rel=""nofollow noreferrer"">plm</a> package in R.</p>
<p>Suppose you aggregated your data up to <em>weekly</em> time units. Including time effects amounts to the inclusion of 207 separate <em>week</em> effects. Now suppose you aggregated your data up to <em>monthly</em> time units. Including time effects amounts to the inclusion of 47 separate month effects. And finally, suppose you aggregated your data up to <em>yearly</em> time units. Including time effects amounts to the inclusion of 3 separate year effects. The purpose of me outlining this is to show you how we include time fixed effects in practice.</p>
<p>I can't recommend an appropriate time unit without knowing more about how, or when, the intervention affects specific countries. You must take into consideration the <em>timing</em> of your treatment. Suppose you're investigating some exposure which impacts a subset of countries in June of 2018, only to end at the conclusion of the third quarter in 2019. In this setting, I wouldn't recommend observing countries in yearly time intervals. Instead, observing outcomes over <em>months</em> or <em>quarters</em> will help capture the precise exposure period.</p>
<blockquote>
<p>Or, similarly, is there a way to use difference-in-difference (i.e., countries are treated at different points in time), but with a continuous set of measurements over time for each country, rather than just before/after, and with a continuous treatment variable?</p>
</blockquote>
<p>Yes.</p>
<p>Difference-in-differences (DiD) is amenable to continuous treatments. I actually addressed a similar question <a href=""https://stats.stackexchange.com/questions/496478/difference-in-differences-model-specification-with-year-quarter-effects-treatme/497561"">here</a>. In particular, I showed how continuous treatments can be used in the classical case and in the more generalized setting where the onset of treatment exposure is staggered across treated entities. I also reference a very popular answer <a href=""https://stats.stackexchange.com/questions/152684/how-do-i-interpret-a-difference-in-differences-model-with-continuous-treatment"">here</a> which should also be of interest to you.</p>
<p>In your setting, you're well outside the realm of the classical DiD approach. Different countries enter into treatment at <em>different times</em>. You <em>must</em> proceed with the generalized DiD estimator. You aren't beholden to me or my suggestions, but if you know the precise onset of treatment for all countries, you could investigate the causal effect of the exposure both ways. Here is the generalized DiD specification, which is also commonly referred to as a two-way fixed effects estimator:</p>
<p><span class=""math-container"">$$
y_{ct} = \alpha_{c} + \lambda_{t} + \delta D_{ct} + \theta X_{ct} + \epsilon_{ct}
$$</span></p>
<p>where <span class=""math-container"">$\alpha_{c}$</span> and <span class=""math-container"">$\lambda_{t}$</span> denote fixed effects for countries <span class=""math-container"">$c$</span> and days <span class=""math-container"">$t$</span> (or weeks, months, quarters), respectively. If you want this to be a DiD estimator, you <em>must</em> include country (unit) <em>and</em> day (time) fixed effects. <span class=""math-container"">$D_{ct}$</span> is a dichotomous treatment variable. It equals 1 for treated countries <em>and</em> only when they enter into a posttreatment period, 0 otherwise. Any country never exposed to treatment is coded 0 for the entire observation period. For example, suppose India is a &quot;control&quot; country. If <span class=""math-container"">$x$</span> represented our dichotomous treatment variable, it would be equal 0 in all 1,460 days. To be clear, <span class=""math-container"">$D_{ct}$</span> is not delineating a precise treatment group. Rather, it is 'turning on' (i.e., shifting from 0 to 1) for countries exposed to the treatment <em>and</em> only during the days when treatment is actually in effect. <span class=""math-container"">$\delta$</span> is your causal estimand.</p>
<p>Your equation is also amendable to a continuous exposure variable. The equation still takes the following form:</p>
<p><span class=""math-container"">$$
y_{ct} = \alpha_{c} + \lambda_{t} + \delta M_{ct} + \theta X_{ct} + \epsilon_{ct}
$$</span></p>
<p>where <span class=""math-container"">$M_{ct}$</span> is a measure of the &quot;bite&quot; or dosage of treatment in the days <em>after</em> treatment commences. To be clear, only the subset of treated countries should experience a 'jump' in intensity. Thus, incorporating <span class=""math-container"">$M_{ct}$</span> means replacing any country-day combination equal to unity with the precise dosage observed in that country. Again, the variable <span class=""math-container"">$M_{ct}$</span> should reflect reality as closely as possible. In the pretreatment epoch, the variable should reflect the <em>absence</em> of intensity. In fact, all 220 countries would be considered &quot;untreated&quot; before the onset of treatment. But once a country enters into their posttreatment epoch, <span class=""math-container"">$M_{ct}$</span> should indicate some measure of intensity. In my fake data frame from earlier, the United States is a treated country. The variable <span class=""math-container"">$x$</span> takes on positive values at some day <span class=""math-container"">$t$</span> in between day 5 and day 1,460. You can think of <span class=""math-container"">$x$</span> as your measure of 'bite' or dosage. I simulated some positive values for <span class=""math-container"">$x$</span> but it could represent almost any dosage of interest: exposure to particulate matter, the proportion of immigrants in a school, or even the saturation of bike patrol officers within a precinct. Again, you must justify to a reader why your continuous treatment is a close approximation of what is going on in the real world.</p>
<blockquote>
<p>A related question is whether country and time fixed effects should kind of &quot;intersect.&quot; My guess is that the time effects are somewhat different in each country.</p>
</blockquote>
<p>This quote was pulled from our discussion in the comments. It appears you want to estimate the following:</p>
<p><span class=""math-container"">$$
y_{ct} = \alpha_{c} + \lambda_{t} + \gamma_{ct} + \delta M_{ct} + \theta X_{ct} + \epsilon_{ct}
$$</span></p>
<p>where <span class=""math-container"">$\gamma_{ct}$</span> denotes a &quot;country-day&quot; effect. In essence, you're saying the shock is likely unique in a particular country <em>and</em> day. For example, the shock in India in the first time period is different than the shock in the United States in the first time period. But that is more country-day effects than you can afford. Your model will balk, claiming you have no residual degrees of freedom. Even if you dropped the country and/or day fixed effects, your model is inestimable. A country-by-day effect would work well in settings where you observe, for example, individuals <span class=""math-container"">$i$</span> nested within your countries <span class=""math-container"">$c$</span>. But you don't have that here!</p>
<p>You can estimate geographic shocks particular to specific <em>regions</em> of the world. For example, you might suspect a shock specific to the subset of countries nested within the Balkan Peninsula. You could create a variable indexing the regions where you suspect shocks and interact those with day (time) dummies. But don't overdue it. You should outline some theoretical basis for doing this. In my opinion, I would estimate such a model as a robustness check. The purpose is to see whether your effects hold even after adjusting for the regional shocks.</p>
",2020-12-30 04:30:42.777
333764,57214.0,2,,333720.0,,,,CC BY-SA 4.0,"<p>The rule is very simple.</p>
<p>Regardless of the size <span class=""math-container"">$n$</span> of the normal sample:</p>
<ul>
<li><p>If population standard deviation <span class=""math-container"">$\sigma$</span> is unknown and estimated by the sample standard deviation <span class=""math-container"">$S = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2},$</span> then use a t test. Critical value and P-value use Student's t distribution with <span class=""math-container"">$n-1$</span> degrees of freedom.</p>
</li>
<li><p>If population standard deviation <span class=""math-container"">$\sigma$</span> is known, use a z test. Critical value and P-value use the standard normal distribution. [To assume <span class=""math-container"">$\sigma \approx S$</span>
would unnecessarily make this an approximate test.]</p>
</li>
</ul>
<p><strong>Addendum per comments:</strong> I understand that there is controversy
about the robustness of t tests for more than <span class=""math-container"">$n=30$</span> non-normal observations. This is not the place for an exhaustive
treatise on the topic, but I will show a case where the t statistic does not have a Student's t distribution because data are not
normal. One can quibble about the consequences for particular
applied situations, but not about the distributional facts.</p>
<p>Consider testing <span class=""math-container"">$H_0: \mu = 1$</span> against <span class=""math-container"">$H_a: \mu &gt; 1$</span> at the 5% level of significance. Later we look at the specific alternative
<span class=""math-container"">$H_a: \mu = 1.5$</span> when discussing power. We have <span class=""math-container"">$n=31$</span> observations.</p>
<p>Suppose the observations are from <span class=""math-container"">$\mathsf{Norm}(\mu=1,\sigma=1).$</span>
Then the statistic <span class=""math-container"">$T =\frac{\bar X - \mu_0}{S/\sqrt{n}}$</span> is
distributed as <span class=""math-container"">$T(\nu = 31-1=30)$</span> under <span class=""math-container"">$H_0.$</span> Rejecting for <span class=""math-container"">$T \ge c = 1.697,$</span> we have a test at the 5% level.</p>
<pre><code>qt(.95, 30)
[1] 1.697261
</code></pre>
<p>If the 31 observations are from <span class=""math-container"">$\mathsf{Exp}(\lambda=1),$</span> then
the significance level of an incorrect t test is nearer 2.3%.
By contrast, if we observe that <span class=""math-container"">$G = \frac{\bar X}{\mu} \sim
\mathsf{Gamma}(\mathrm{shape}=31,\mathrm{rate}=31),$</span> then a
correct test at the 5% level rejects for <span class=""math-container"">$G \ge 1.313.$</span></p>
<pre><code>qgamma(.95, 31, 31)
[1] 1.312597
</code></pre>
<p>These facts are illustrated by the simulation in R below.</p>
<pre><code>#significance levels
set.seed(2020)

par(mfrow=c(1,3))
t.norm = replicate(10^5, t.test(rnorm(31,1,1),mu=1,alt=&quot;g&quot;)$stat)
mean(t.norm &gt;= qt(.95,30))  # prob of false rej
[1] 0.04985   # aprx 0.05   
1 - pt(qt(.95,30), 30)
[1] 0.05

hist(t.norm, prob=T, col=&quot;skyblue2&quot;)
 curve(dt(x,30), add=T)
 abline(v = qt(.95,30), col=&quot;red&quot;, lwd=2)

t.exp = replicate(10^5, t.test(rexp(31,1),mu=1,alt=&quot;g&quot;)$stat)
mean(t.exp &gt;= qt(.95,30))   # prob of false rej
[1] 0.02262

hist(t.exp, br=30, prob=T, col=&quot;wheat&quot;)
 curve(dt(x,31), add=T)
 abline(v = qt(.96,30), col=&quot;red&quot;, lwd=2)

g.exp = replicate(10^5, mean(rexp(31,1))/1)
mean(g.exp &gt;= qgamma(.95,31,31))   # prob of false rej
[1] 0.0486
1 - pgamma(qgamma(.95, 31, 31), 31, 31)
[1] 0.05

hist(g.exp, prob=T, col=&quot;skyblue2&quot;)
 curve(dgamma(x,31,31), add=T)
 abline(v = qgamma(.95,31,31), col=&quot;red&quot;, lwd=2)

par(mfrow=c(1,1))
</code></pre>
<p>In the middle panel below, notice that the density function of
<span class=""math-container"">$\mathsf{T}(30)$</span> is dotted because it does not match the histogram
of the simulated realizations of t statistics from exponential data.
[Also, for exponential data, the null distribution of the correct gamma test is right-skewed, while the null distribution of the incorrect t test is left-skewed.]</p>
<p><a href=""https://i.stack.imgur.com/KDiVD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KDiVD.png"" alt=""enter image description here"" /></a></p>
<p>At this point you might say it's a small price to pay that the t test is a little too conservative for exponential data with
true significance level about 2.3% instead of 5%, but more
serious errors arise in considerations of power.</p>
<p>Now let's look at power against the alternative <span class=""math-container"">$\mu = 1.5.$</span>
For a t test with normal data, the power is found from a non-central t distribution. The non-centrality parameter is 2.784 and the power
is about 85.9%.</p>
<pre><code>ncp = sqrt(31)*(.5)/1; ncp
[1] 2.783882
1 - pt(qt(.95,30), 30, ncp) 
[1] 0.8588956
</code></pre>
<p>The simulated power of an incorrect t test for exponential data is about <span class=""math-container"">$58\%.$</span> The simulated power for the correct test with
exponential data is about <span class=""math-container"">$74.5\%.$</span> [Under the assumption the exponential population mean is <span class=""math-container"">$\mu=1.5,$</span> the statistic
<span class=""math-container"">$\bar X/\mu$</span> has distribution <span class=""math-container"">$\mathsf{Gamma}(31, 31/1.5).]$</span></p>
<pre><code>1 - pgamma(qgamma(.95,31,31), 31, 31/1.5)
[1] 0.7474007
</code></pre>
<p>The following simulation in R illustrates these facts.</p>
<pre><code># Power  
par(mfrow=c(1,3))
t.norm = replicate(10^5, t.test(rnorm(31,1.5,1),mu=1,alt=&quot;g&quot;)$stat)
mean(t.norm &gt;= qt(.95,30))  # power
[1] 0.85951
ncp = sqrt(31)*(.5)/1
1 - pt(qt(.95,30), 30, ncp) 
[1] 0.8588956

hist(t.norm, prob=T, col=&quot;skyblue2&quot;)
 curve(dt(x, 30, ncp), add=T)
 abline(v = qt(.95,30), col=&quot;red&quot;, lwd=2)

t.exp = replicate(10^5, t.test(rexp(31,2/3),mu=1,alt=&quot;g&quot;)$stat)
mean(t.exp &gt;= qt(.95,30))   # power
[1] 0.58397

hist(t.exp, prob=T, col=&quot;wheat&quot;)
 abline(v = qt(.95,30), col=&quot;red&quot;, lwd=2)
 curve(dt(x, 30, ncp), add=T, lty=&quot;dotted&quot;)

g.exp = replicate(10^5, mean(rexp(31,2/3))/1)
mean(g.exp &gt;= qgamma(.95,31,31))   # power
[1] 0.74723
1 - pgamma(qgamma(.95,31,31), 31, 31/1.5)
[1] 0.7474007

hist(g.exp, prob=T, col=&quot;skyblue2&quot;)
 abline(v = qgamma(.95,31,31), col=&quot;red&quot;, lwd=2)
 curve(dgamma(x, 31, 31/1.5), add=T)

par(mfrow=c(1,1))
</code></pre>
<p>The power of the incorrect t test for exponential data cannot
be obtained from a non-central t distribution (dotted density in
the middle panel). So planning the sample size for an experiment
with exponential data would be quite unreliable using power computations based on t distributions.</p>
<p><a href=""https://i.stack.imgur.com/k46yu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k46yu.png"" alt=""enter image description here"" /></a></p>
<p>In this demonstration it has been possible to find exact theoretical significance levels and powers for the the appropriate tests and
reasonable simulated values where the t test is used inappropriately. In my view, this is just the 'tip of the iceberg'
of the problems that can arise from pushing the robustness of the
t test too far, and especially from using <span class=""math-container"">$n=30$</span> as a guide---as
seems to be far too widely recommended.</p>
",2020-12-30 04:34:27.827
333765,75350.0,2,,333720.0,,,,CC BY-SA 4.0,"<blockquote>
<p>If I want to use t-test for a hypothesis, then I have to assume normality of the population.</p>
</blockquote>
<p>Not so.  The population is irrelevant (well...not completely, we really only need to assume finite variance and make vague assumptions about the skew, mainly that the population is not &quot;skewed too much&quot;. See the <a href=""https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem"" rel=""nofollow noreferrer"">Berry-Esseen Theorem</a> for more on how the skew affects the t test through the CLT).  The normality requirement is provided by the Central Limit Theorem.  See this <a href=""https://people.wikimedia.org/%7Ebearloga/notes/comparing-groups/"" rel=""nofollow noreferrer"">excellent</a> blog post for more. Additionally, see this rather flippant <a href=""https://dpananos.github.io/posts/2019/08/blog-post-23/"" rel=""nofollow noreferrer"">blog post</a> I wrote.</p>
<blockquote>
<p>If I want to use z-test for a hypothesis, then I have to assume that <span class=""math-container"">$s\approx\sigma.$</span></p>
</blockquote>
<p>The reason we use the t over the z test is because there is uncertainty introduced by approximating the sample standard deviation.  If we knew <span class=""math-container"">$\sigma$</span> with infinite precision, we would always use a z test.  We never know <span class=""math-container"">$\sigma$</span> with perfect precision so technically we would never use a z test.</p>
<p>That being said, a t distribution becomes almost identical to a standard normal with enough data.  So while we technically can never know <span class=""math-container"">$\sigma$</span> with infinite precision, we can pretend we do anyway and perform a z test with impunity</p>
<blockquote>
<p>What is the less important assumption? What test should I do? Should I do t-test, z-test or doesn't matter?</p>
</blockquote>
<p>I would say that with enough data, error introduced by estimating the standard deviation becomes negligible and the normality requirement has been greatly exaggerated or misunderstood.</p>
",2020-12-30 05:18:07.483
333766,247117.0,1,,,,what is the optimal step size for metropolis-hastings algorithm to have independent state,<normal-distribution><markov-chain-montecarlo><metropolis-hastings><mcmc-acceptance-rate>,CC BY-SA 4.0,"<p>In the PRML chapter 11, The Metropolis-Hasting algorithm,</p>
<p>For a sampler with Gaussian distribution as proposal distribution.</p>
<p>The original distribution is correlated multivariate Gaussian distribution, with standard deviations <span class=""math-container"">$\sigma_{min}$</span> and <span class=""math-container"">$\sigma_{max}$</span></p>
<p>The transition probability is shown as below: <span class=""math-container"">$q(z'|z) = \mathcal{N}(z, \rho^2I)$</span></p>
<p>Because the proposed distribution is symmetric, so  <span class=""math-container"">$q(z'|z) = q(z|z')$</span></p>
<p>Using Metropolis-hastings algorithm, the accept rate is</p>
<p><span class=""math-container"">$A(z'|z)= min \left(1, \frac{\pi(z')q(z|z')}{\pi(z)q(z'|z)} \right)$</span></p>
<p>After cancel out the transition probability, we have</p>
<p><span class=""math-container"">$A(z'|z)= min \left(1, \frac{\pi(z')}{\pi(z)} \right)$</span></p>
<p>It shown that the accept rate depends on the location of two points in the original distribution.</p>
<p>But I don't know how to include <span class=""math-container"">$\sigma_{max}$</span> or <span class=""math-container"">$\sigma_{min}$</span> in this equation.</p>
<p>And according to the article, the number of steps seperating states that are independent is order of <span class=""math-container"">$(\sigma_{max}/\sigma_{min})^2 $</span></p>
<p>How to generate this conclusion?</p>
<p><a href=""https://i.stack.imgur.com/d6ooB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d6ooB.png"" alt=""enter image description here"" /></a></p>
",2020-12-30 05:50:56.797
333767,255226.0,1,,,,Compare two curriculums?,<multivariate-analysis><references>,CC BY-SA 4.0,"<p>A number of different courses at our institution teach the same skill. We're looking at developing an overall curriculum for everyone to use while teaching this skill, and would like to compare it to each of the existing curriculums followed.</p>
<p>My idea is to have a number of different assessors (ideally one from each discipline) go through the proposed curriculum and the existing curriculums. They'll then be asked whether each item in the proposed curriculum is represented in the existing curriculum they are comparing it to, and give a similarity score for each item on a scale (probably 1-5). This will be repeated by each assessor for several (possibly all) the existing curriculums.</p>
<p>While I'm fairly happy I could then compute inter-assessor variability, I'm a little unclear of which statistic would be best to compare how similar the proposed curriculum is to each existing one.</p>
<p>My background is solely in more basic regression and correlation so I'm rather stuck as to what to use. Google and reddit/ stack exchange searches haven't turned up much in the way that could help, possibly because I imagine this is a fairly unique way of looking at this problem.</p>
<p>Could anyone recommend a statistic to use, or even better a paper or textbook chapter that I can use to get an overview?</p>
",2020-12-30 05:58:25.387
333768,234468.0,1,333770.0,,,Understanding definition of P-Value,<hypothesis-testing><p-value>,CC BY-SA 4.0,"<p>I am trying to understand the following definition and wonder if there is a mistake in the notation.</p>
<p>&quot;The p-value is the probability of observing a sample for which the difference between the <span class=""math-container"">$\mu_0$</span> and the sample mean <span class=""math-container"">$\hat{\mu}$</span> (assumption vs. data) is greater than <span class=""math-container"">$| \mu_0 -\hat{\mu} | $</span> in either direction, if the null was true.&quot;</p>
<p>I previously understood <span class=""math-container"">$\hat{\mu}$</span> to be &quot;expectation&quot;</p>
<p>Is expectation and assumption the same thing?
Should the text in brackets read &quot;( data vs assumption )&quot; ?</p>
",2020-12-30 06:07:37.663
333769,196018.0,1,,,,Prediction intervals for location scale model,<probability><prediction><prediction-interval>,CC BY-SA 4.0,"<p>Regarding the example below from Chapter 3 of A.C. Davison's Statistical Models, I'm wondering why <span class=""math-container"">$Q = (Y_+-\bar Y)/S$</span> depends only on <span class=""math-container"">$g$</span>.</p>
<p><a href=""https://i.stack.imgur.com/Y9XwS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y9XwS.png"" alt=""enter image description here"" /></a></p>
",2020-12-30 06:25:01.150
20582,2584.0,1,,,,When will PCA be equivalent to ICA?,<normal-distribution><pca><independence><independent-component-analysis>,CC BY-SA 3.0,"<p>$X = AS$ where $A$ is my mixing matrix and each column of $S$ represents my sources.  $X$ is the data I observe.</p>

<p>If the columns of $S$ are independent and Gaussian, will the components of PCA be extremely similar to that of ICA?  Is this the only requirement for the two methods to coincide?</p>

<p>Can someone provide an example of this being true when the $cov(X)$ isn't diagonal?  </p>
",2012-02-16 00:21:27.737
333770,122894.0,2,,333768.0,,,,CC BY-SA 4.0,"<p><span class=""math-container"">$\mu_0$</span> is a number you put into your null hypothesis to test data against it and <span class=""math-container"">$\hat\mu$</span> is mean calculated from that data.</p>
<p>For example, imagine you have dataset that contains five numbers: <span class=""math-container"">$1, 2, 3, 4, 5$</span> and wish to test if mean (in whole population, this sample comes from) is <span class=""math-container"">$3.14$</span>.</p>
<p>I above setting <span class=""math-container"">$\mu_0=3.14$</span> and <span class=""math-container"">$\hat\mu=3$</span>.</p>
<blockquote>
<p>Is expectation and assumption the same thing?</p>
</blockquote>
<p>Why you chose <span class=""math-container"">$3.14$</span> there? You expected that this is &quot;true mean&quot; (just like I expect my kids to clean a mess in their rooms)? Or maybe you assumed it (based on some expert knowledge)? It doesn't matter as long as <span class=""math-container"">$3.14$</span> is a number you put in null hypothesis. You may also say, this was just a guess, and this is OK too.</p>
<p>Of course, &quot;expectation&quot; can also mean &quot;expected value of random variable&quot; and be denoted by <span class=""math-container"">$E(X)$</span>, but I assume this is not what you meant.</p>
",2020-12-30 07:12:17.940
333771,227699.0,1,,,,Why is a Breusch-Pagan test returning significant heteroskedasticity when the fitted value chart indicates homoskedasticity?,<r><regression><heteroscedasticity><breusch-pagan>,CC BY-SA 4.0,"<p>I was working the results of a regression equation and I wanted to test to see if there was significant heteroskedasticity in the residuals. Checking the results of the graph of fitted values versus residuals using <code>plot()</code> produced a straight line that is almost exactly zero for most of the resulting graph and the data fits rather closely.</p>
<p><a href=""https://i.stack.imgur.com/SAH9x.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SAH9x.jpg"" alt=""enter image description here"" /></a></p>
<p>However, a studentized Breusch-Pagan test of the residuals using the <code>bptest()</code> function indicated significant heteroskedasticity (BP = 8.4085, df = 1, p = 0.0037). I can't figure out why the Breusch-Pagan test is returning significant heteroskedasticity. The regression doesn't look heteroskedastic, the regression line is pretty consistent overall but there are a few outliers in the right half of the graph. There isn't a clearly non-linear pattern like increasing variance in residuals with increasing fitted values like one would expect from heteroskedastic results. <strong>So then why is the Breush-Pagan test telling me that I have significant heteroskedasticity in my regression?</strong></p>
",2020-12-30 07:41:02.097
333772,211.0,1,,,,How to decide if to use weights or not when estimating some $\mu$ of a population that has sub-populations with different $\mu_i$?,<survey><mse><weighted-mean><survey-weights><bias-correction>,CC BY-SA 4.0,"<h2>Setting and Notation</h2>
<p>Let's assume we have a population with (for example) two sub-population. Say, males and females.  In the population they are split 50%-50%. We care about the population level parameter <span class=""math-container"">$\mu$</span>. Furthermore, the male and female sub-population have their own <span class=""math-container"">$\mu_m$</span> and <span class=""math-container"">$\mu_f$</span>, were: <span class=""math-container"">$\mu = \frac{1}{2}\mu_m + \frac{1}{2}\mu_f$</span>.</p>
<p>We have a panel of <span class=""math-container"">$n$</span> people, out of which (let's say) 90% are males and 10% are female (i.e.: <span class=""math-container"">$n_m = 0.9*n$</span> and <span class=""math-container"">$n_f = 0.1*n$</span>). For each person <span class=""math-container"">$i$</span> in the panel we measure <span class=""math-container"">$y_i$</span>. If the panel was fully i.i.d then <span class=""math-container"">$E[y_i] = \mu$</span>. However, since we have a bias panel (for example, from non-response bias), then <span class=""math-container"">$E[y_i] \neq \mu$</span>. However, even for the biased sample, we know that for all <span class=""math-container"">$i$</span> that are males <span class=""math-container"">$E[y_i] = \mu_m$</span> (and similarly for <span class=""math-container"">$i$</span> that are female <span class=""math-container"">$E[y_i] = \mu_f$</span>).</p>
<p>We now have two potential options for estimating <span class=""math-container"">$\mu$</span>:</p>
<ul>
<li><span class=""math-container"">$\bar y$</span>: which will be a biased estimator of <span class=""math-container"">$\mu$</span> (let's say <span class=""math-container"">$E[\bar y] = \mu + B$</span> (B for bias))</li>
<li><span class=""math-container"">$\bar y^* = \bar y_m * 0.5 + \bar y_f * 0.5$</span>: which will be an un-biased estimator of <span class=""math-container"">$\mu$</span>, but will have much larger variance.</li>
</ul>
<h2>Question</h2>
<p><strong>How can we tell which of the two estimators are better to use, <span class=""math-container"">$\bar y^*$</span> or <span class=""math-container"">$\bar y$</span>, given that we don't know the real values of <span class=""math-container"">$\mu$</span>, <span class=""math-container"">$\mu_m$</span>, or <span class=""math-container"">$\mu_f$</span>?</strong></p>
<p>There is obviously a bias-variance trade-off. Can we estimate it somehow?</p>
<p><strong>Are there common practices/references for methods to check these?</strong></p>
<h2>Potential ideas I had</h2>
<p>We can decide to use the estimator that will have the minimal <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" rel=""nofollow noreferrer"">MSE</a>. Of course we do not know what it is for each estimator, so we can decide to estimate it.</p>
<p>For example by saying that:</p>
<ul>
<li><span class=""math-container"">$MSE(\bar y) = var(\bar y) + bias^2(\bar y) = var(\bar y) + (\mu + B)^2$</span></li>
<li><span class=""math-container"">$MSE(\bar y^*) = var(\bar y^*) + bias^2(\bar y^*)$</span></li>
</ul>
<p>We assume that <span class=""math-container"">$bias^2(\bar y^*) = 0$</span>, hence: <span class=""math-container"">$E[\bar y^*] = 0$</span>. Therefore, if we look at: <span class=""math-container"">$\hat B = \bar y - \bar y^*$</span>, we notice that: <span class=""math-container"">$E[\bar y - \bar y^*] = E[\bar y] - E[\bar y^*] = \mu + B - \mu = B$</span>. So <span class=""math-container"">$\hat B$</span> is a consistent estimator (via method of moments) for <span class=""math-container"">$B$</span>. Hence, <span class=""math-container"">$\hat B^2$</span> will also be a consistent (although probably biased) estimator for <span class=""math-container"">$B^2$</span></p>
<p>With this in mind, we can invoke CLT and the <a href=""https://en.wikipedia.org/wiki/Delta_method"" rel=""nofollow noreferrer"">delta method</a>, and build an asymptotic distribution for estimating <span class=""math-container"">$D = MSE(\bar y) - MSE(\bar y^*)$</span>: <span class=""math-container"">$\hat D = var(\bar y) + (\bar y - \bar y^*)^2 - var(\bar y^*)$</span>. The variance: <span class=""math-container"">$Var \left[ var(\bar y) + (\bar y - \bar y^*)^2 - var(\bar y^*) \right]$</span>, will probably need to be estimated via bootstrap. And since <span class=""math-container"">$\hat D$</span> will asymptotically have normal distribution, we can perform an hypothesis test to check if it's larger then 0 (<span class=""math-container"">$H_1$</span>) or not (<span class=""math-container"">$H_0$</span>). This would give us input if to use <span class=""math-container"">$\bar y$</span> or <span class=""math-container"">$\bar y^*$</span>.</p>
<p><strong>Would love to have more ideas / references from others here. Thanks upfront!</strong></p>
",2020-12-30 08:21:48.093
333773,255233.0,1,333812.0,,,Why does chi2-test show me a dependence between randomly generated columns?,<hypothesis-testing><chi-squared-test><scikit-learn><independence>,CC BY-SA 4.0,"<p>I generate two columns of length 343180 with random integer values between 0 and 290 and run sklearn's chi2-test of dependence. One would expect that the null hypothesis (independence) is accepted with a high probability, but actually I get a test score of approx. 15423 and a p-value of 0.</p>
<pre><code>import numpy as np
from sklearn.feature_selection import chi2

X = np.transpose([[np.random.randint(0, 291) for i in range(0, 343180)]])
y = np.asarray([np.random.randint(0, 291) for i in range(0, 343180)])

print(X.shape)
# output: (34318, 1)

print(y.shape)
# output: (34318,)

chi2(X, y)
# output: (array([15423.73497325]), array([0.]))
# which means: p-value = 0.
</code></pre>
<p>Does this has to do with the limits of pseudo random number generation? Or do I misunderstand the concept of a chi2-test? Does the chi2-test, as implemented in sklearn, expect a certain type of distribution of the tested features, and not just an arbitrary discrete distribution?</p>
",2020-12-30 08:50:16.500
333774,119922.0,1,,,,"What does it mean to say that ""the prior over $f$ induces a prior over probabilistic classifications $\pi$""?",<classification><gaussian-process><prior>,CC BY-SA 4.0,"<p>I am currently studying the textbook <em>Gaussian Processes for Machine Learning</em> by Carl Edward Rasmussen and Christopher K. I. Williams. <strong>Chapter 1 Introduction</strong> says the following:</p>
<blockquote>
<p>We now turn to the <em>classification</em> case, and consider the binary (or two-class) classification problem. An example of this is classifying objects detected in astronomical sky surveys into stars or galaxies. Our data has the label <span class=""math-container"">$+1$</span> for stars and <span class=""math-container"">$-1$</span> for galaxies, and our task will be to predict <span class=""math-container"">$\pi(\mathbf{\mathrm{x}})$</span>, the probability that an example with input vector <span class=""math-container"">$\mathbf{\mathrm{x}}$</span> is a star, using as inputs some features that describe each object. Obviously <span class=""math-container"">$\pi(\mathbf{\mathrm{x}})$</span> should lie in the interval <span class=""math-container"">$[0, 1]$</span>. A Gaussian process prior over functions does not restrict the output to lie in this interval, as can be seen from Figure 1.1(a). The approach that we shall adopt is to squash the prior function <span class=""math-container"">$f$</span> pointwise through a response function which restricts the output to lie in <span class=""math-container"">$[0, 1]$</span>. A common choice for this function is the logistic function <span class=""math-container"">$\lambda(z) = (1 + \exp(−z))^{−1}$</span>, illustrated in Figure 1.2(b). Thus the prior over <span class=""math-container"">$f$</span> induces a prior over probabilistic classifications <span class=""math-container"">$\pi$</span>.
<a href=""https://i.stack.imgur.com/yX1Bt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yX1Bt.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/KxqeC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KxqeC.png"" alt=""enter image description here"" /></a></p>
</blockquote>
<p>I'm confused by this part:</p>
<blockquote>
<p>Thus the prior over <span class=""math-container"">$f$</span> induces a prior over probabilistic classifications <span class=""math-container"">$\pi$</span>.</p>
</blockquote>
<p>Wasn't it just said that <span class=""math-container"">$f$</span> is the prior? So what does it mean by &quot;the prior over <span class=""math-container"">$f$</span> induces a prior over probabilistic classifications <span class=""math-container"">$\pi$</span>&quot;?</p>
",2020-12-30 09:08:55.910
333775,252288.0,1,,,,How to calculate the interannual variability and Seasonal Amplitude?,<time-series><statsmodels><geostatistics>,CC BY-SA 4.0,"<p>I have a time series of remote sensing observables. For example, the observed brightness from 2007 to 2019. And we have 2 datapoints per day. How can I calculate the interannual variability of this variable? and the seasonal amplitude? and how to define them? I want to study the relationship between interannual variability and seasonal amplitude of this variable. It's better to do a scatter plot with X-axis as interannual variability and Y-axis as seasonal amplitude. But I have no idea about the definition and the calculation of inter annual variability and seasonal amplitude (and also seasonal variability). I am a beginner, is there anyone who can help me? thanks a lot!</p>
",2020-12-30 09:51:52.790
333790,249015.0,1,333879.0,,,Using FCNN for multi-class semantic segmentation trained on single class labeled image data,<neural-networks><conv-neural-network><multi-class><image-segmentation>,CC BY-SA 4.0,"<p>I am working on project where main task is <code>semantic segmentation</code> of land cover and another objects in Sentinel 2 multi-spectral images. Currently I posses dataset of 50 000 <code>single-labeled</code> images trimmed from Sentinel 2 tiles using OpenStreetMap polygons i.e. every image contains one class e.g. water-body. There is 7 classes in those dataset. Meant for image classification task using convolutional neural networks.</p>
<p><strong>Can I use those single-labeled images for training <code>fully convolutional neural network</code> and then use it for semantic segmentation of (larger) images <code>containing more classes</code> e.g. 7 ?</strong></p>
<p><strong>Or need I to create new dataset of images, each containing those 7 classes ?</strong></p>
<p>Any suggestions or ideas are appreciated !</p>
",2020-12-30 13:00:48.577
333776,128628.0,2,,333772.0,,,,CC BY-SA 4.0,"<p>Your question proposes fixed sample sizes, but I note that it is possible to optimise the sample sizes for stratified sampling using the method discussed in <a href=""https://stats.stackexchange.com/questions/503164/"">this related question</a>.  In order to subsume both your proposed estimators, let's consider the general estimator of the form:</p>
<p><span class=""math-container"">$$\hat{\mu} = \lambda \bar{y}_M + (1-\lambda) \bar{y}_F,$$</span></p>
<p>where we choose the weighting <span class=""math-container"">$0 \leqslant \lambda \leqslant 1$</span>.  The mean and variance of this estimator are:</p>
<p><span class=""math-container"">$$\mathbb{E}(\hat{\mu}) = \lambda \mu_M + (1-\lambda) \mu_F
\quad \quad \quad \quad \quad 
\mathbb{V}(\hat{\mu}) = \lambda^2 \cdot \frac{\sigma_M^2}{n_M} + (1-\lambda)^2 \cdot \frac{\sigma_F^2}{n_F}.$$</span></p>
<p>Consequently, the <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" rel=""nofollow noreferrer"">mean-squared error (MSE)</a> of the estimator is:</p>
<p><span class=""math-container"">$$\begin{align}
\text{MSE}(\hat{\mu}, \mu) 
&amp;= \mathbb{V}(\hat{\mu}) + \text{Bias}(\hat{\mu}, \mu)^2 \\[12pt]
&amp;= \Bigg( \lambda^2 \cdot \frac{\sigma_M^2}{n_M} + (1-\lambda)^2 \cdot \frac{\sigma_F^2}{n_F} \Bigg) + \Bigg( (\lambda-\tfrac{1}{2}) \mu_M + (\tfrac{1}{2}-\lambda) \mu_F \Bigg)^2 \\[6pt]
&amp;= \Bigg( \lambda^2 \cdot \frac{\sigma_M^2}{n_M} + (1-\lambda)^2 \cdot \frac{\sigma_F^2}{n_F} \Bigg) + \Bigg( (\lambda-\tfrac{1}{2}) (\mu_M - \mu_F) \Bigg)^2 \\[6pt]
&amp;= \lambda^2 \cdot \frac{\sigma_M^2}{n_M} + (1-\lambda)^2 \cdot \frac{\sigma_F^2}{n_F} + (\lambda-\tfrac{1}{2})^2 (\mu_M - \mu_F)^2 \\[6pt]
&amp;= \lambda^2 \cdot \frac{\sigma_M^2}{n_M} + (\lambda^2 - 2 \lambda + 1) \cdot \frac{\sigma_F^2}{n_F} + (\lambda^2 - \lambda + \tfrac{1}{4}) (\mu_M - \mu_F)^2 \\[6pt]
&amp;= \lambda^2 \cdot \Bigg( \frac{\sigma_M^2}{n_M} + \frac{\sigma_F^2}{n_F} + (\mu_M - \mu_F)^2\Bigg) - 2 \lambda \cdot \Bigg( \frac{\sigma_F^2}{n_F} + \frac{(\mu_M - \mu_F)^2}{2} \Bigg) \\[6pt]
&amp;\quad \quad + \Bigg( \frac{\sigma_F^2}{n_F} + \Big( \frac{\mu_M - \mu_F}{2} \Big)^2 \Bigg). \\[6pt]
\end{align}$$</span></p>
<p>This function is a convex quadratic form in <span class=""math-container"">$\lambda$</span>, and we can use ordinary calculus methods to minimise it with respect to this parameter.  We have first and second derivatives:</p>
<p><span class=""math-container"">$$\begin{align}
\frac{\partial \text{MSE}}{\partial \lambda} (\hat{\mu}, \mu)
&amp;= 2 \lambda \cdot \Bigg( \frac{\sigma_M^2}{n_M} + \frac{\sigma_F^2}{n_F} + (\mu_M - \mu_F)^2 \Bigg) - 2 \cdot \Bigg( \frac{\sigma_F^2}{n_F} + \frac{(\mu_M - \mu_F)^2}{2} \Bigg), \\[10pt]
\frac{\partial^2 \text{MSE}}{(\partial \lambda)^2} (\hat{\mu}, \mu)
&amp;= 2 \cdot \Bigg( \frac{\sigma_M^2}{n_M} + \frac{\sigma_F^2}{n_F} + (\mu_M - \mu_F)^2\Bigg), \\[10pt]
\end{align}$$</span></p>
<p>which gives the minimising value:</p>
<p><span class=""math-container"">$$\lambda_* = \frac{\sigma_F^2/n_F + (\mu_M - \mu_F)^2/2}{\sigma_M^2/n_M + \sigma_F^2/n_F + (\mu_M - \mu_F)^2}.$$</span></p>
<p>If <span class=""math-container"">$\sigma_M = \sigma_F$</span> then we get the value <span class=""math-container"">$\lambda_* = \tfrac{1}{2}$</span>, which then gives your estimator <span class=""math-container"">$\bar{y}_*$</span>.  This weighted estimator is &quot;optimal&quot; when the standard deviations for the two groups are the same.  Now, obviously we don't know the true parameters, so we don't know the true MSE minimising value <span class=""math-container"">$\lambda_*$</span>.  Of course, one might reasonably take its empirical sample estimator, which is:</p>
<p><span class=""math-container"">$$\hat{\lambda}_* = \frac{s_F^2/n_F + (\bar{y}_M - \bar{y}_F)^2/2}{s_M^2/n_M + s_F^2/n_F + (\bar{y}_M - \bar{y}_F)^2}.$$</span></p>
<p>Now, this method is more general than what you are proposing, since your proposal focuses on only two allowable weightings.  My view is that it is sub-optimal to restrict yourself to these two weightings --- if you are going to allow a choice of weightings in the first place then it is better to allow any weighting you want and then develop an appropriate optimisation procedure.  Developing a procedure that bifurcates between two sub-optimal weightings is likely to give you a poor weighting in most cases (though your estimator might still be okay, since a sub-optimal weighting is not fatal).</p>
",2020-12-30 09:56:15.840
333777,109249.0,1,333782.0,,,Probability of sequence of samples from A being more likely in B,<sampling><binomial-distribution><multinomial-distribution>,CC BY-SA 4.0,"<p>I am trying to solve a problem, whereas I have two discrete and finite probability distributions, <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span>, over the same space. A sequence of observation of size <span class=""math-container"">$k$</span> is sampled from <span class=""math-container"">$A$</span>: <span class=""math-container"">$(o_1, o_2, .., o_k) \text{ where } o_i \sim A$</span></p>
<p>Now, the question is: how can I compute (or approximate) the probability that the sequence of length <span class=""math-container"">$k$</span> (sampled from <span class=""math-container"">$A$</span>) is actually more likely to occur in <span class=""math-container"">$B$</span>?</p>
<p>There is a simple analytical answer, which is for a fixed <span class=""math-container"">$k$</span>, compute all possible combinations, compute the probability of each sequence in both <span class=""math-container"">$B$</span> and <span class=""math-container"">$A$</span>, and then simply divide <span class=""math-container"">$\frac{|\text{sequences more likely in } B|}{|\text{number of sequences}|}$</span>.</p>
<p>However, this is computationally intractable, so I need either a smarter way of doing it, or a good approximations. I thought about elements which are always more likely in <span class=""math-container"">$B$</span> than <span class=""math-container"">$A$</span>, and the working only with those, however I think the error here would be pretty large.</p>
",2020-12-30 10:24:50.077
333778,181884.0,1,333850.0,,,How do I correctly treat nested variables in a regression given multicollinearity of said variables?,<regression><multiple-regression><multicollinearity>,CC BY-SA 4.0,"<p>As per the question, I want to run a regression of variables where those variables are nested within each other and therefore highly correlated. Here is my specific example for context:</p>
<p>I study the effects of <code>Extraversion</code> on various outcomes. Theoretically, <code>Extraversion</code> (a personality trait) is itself made up of two lower-level 'personality aspects', being <code>Assertiveness</code> and <code>Enthusiasm</code>. Despite <code>Extraversion</code> being made up of these two lower-level traits, it is still possible for <code>Extraversion</code> to explain additional variance in an <code>Outcome</code> over and above the individual effects of <code>Assertiveness</code> and <code>Enthusiasm</code>. 20 items (questions on a questionnnaire) are typically used to measured all three constructs (10 for <code>Assertiveness</code>, 10 for <code>Enthusiasm</code>, and the full 20 for <code>Extraversion</code>). The variables are therefore highly correlated (usually &gt; 0.70).</p>
<p>I would like to know how to correctly run a regression to best figure out what the contribution is of each of these three traits, given that they are necessarily highly correlated.</p>
<p>Some made-up data in the form of a correlation matrix to illustrate:</p>
<pre><code>#Correlation matrix.
MyMatrix &lt;- matrix(
  c(1.0, 0.7, 0.8, 0.3,
    0.7, 1.0, 0.6, 0.4,
    0.8, 0.6, 1.0, 0.4,
    0.3, 0.4, 0.4, 1.0), 
  nrow=4, 
  ncol=4)
rownames(MyMatrix) &lt;- colnames(MyMatrix) &lt;- c(&quot;Extraversion&quot;, &quot;Assertiveness&quot;,&quot;Enthusiasm&quot;,&quot;Outcome&quot;)

#Assume means and standard deviations as follows:
MEAN.Extraversion &lt;- 4.00
MEAN.Assertiveness &lt;- 3.90
MEAN.Enthusiasm &lt;- 4.10
MEAN.Outcome &lt;- 5.00
SD.Extraversion &lt;- 1.01
SD.Assertiveness &lt;- 0.95
SD.Enthusiasm &lt;- 0.99
SD.Outcome &lt;- 2.20
s &lt;- c(SD.Extraversion, SD.Assertiveness, SD.Enthusiasm, SD.Outcome)
m &lt;- c(MEAN.Extraversion, MEAN.Assertiveness, MEAN.Enthusiasm, MEAN.Outcome)

#Convert to covariance matrix.
cov.mat &lt;- diag(s) %*% MyMatrix %*% diag(s)
rownames(cov.mat) &lt;- colnames(cov.mat) &lt;- rownames(MyMatrix)
names(m) &lt;- rownames(MyMatrix)

#Run model.
library(lavaan)
m1 &lt;- 'Outcome ~ Extraversion + Assertiveness + Enthusiasm'
fit &lt;- sem(m1, 
           sample.cov=cov.mat,
           sample.nobs=300,
           sample.mean=m,
           meanstructure=TRUE)
summary(fit, standardize=TRUE)
</code></pre>
",2020-12-30 10:26:23.670
333779,243014.0,1,333783.0,,,How to properly calculate statistical INsignificance?,<mathematical-statistics><statistical-significance><numerical-models>,CC BY-SA 4.0,"<p>Just like the author of <a href=""https://stats.stackexchange.com/questions/28905/how-to-quantify-statistical-insignificance"">this post</a> (at a time), I am quite new to statistics. So, I am not sure if I am using the right words here, yet I believe that our questions are pretty different (despite the headlines being worded almost the same).</p>
<p>A (mini-)course on Chemistry took place at my school. It consisted of 4 students only. The authors of the course carried out am exam with only a half of students getting through (that is, two people). They now want to show that a class of 4 people is statistically <strong>insignificant</strong>, that is nothing is furnishing solid evidence that only a half of the students in a bigger class would be able to handle the course.</p>
<p>I now wonder whether there is some mathematical model which would numerically prove this <strong>insignificance</strong>. So, is there such a thing?</p>
",2020-12-30 10:38:58.303
333780,125923.0,2,,333773.0,,,,CC BY-SA 4.0,"<p>Three issues:</p>
<ol>
<li><p><strong>Software engineering:</strong> Avoid 'magic numbers' in the code. Define the constants in the code and re-use them. That makes it easier to change the code to run with different values.</p>
</li>
<li><p><strong>Self-education:</strong> Have you tried changing any of your 'magic numbers'?</p>
</li>
<li><p><strong>Statistics:</strong> <span class=""math-container"">$\chi^2$</span>-test checks for <em>differences</em> in ratios (relative frequencies). It would be quite a surprise if the ratios of so many random numbers would be the same.</p>
</li>
</ol>
<p>So, to give you an answer by addressing all three points:</p>
<pre><code>import numpy as np
from sklearn.feature_selection import chi2

U = 291
L = 0
N = 2 # used to be 343180
X = np.transpose([[np.random.randint(L, U) for i in range(0, N)]])
y = np.asarray([np.random.randint(L, U) for i in range(0, N)])

print(X.shape)
# output: (2, 1)

print(y.shape)
# output: (2,)

chi2(X, y)
# output: (array([7.42424242]), array([0.00643509]))
# which means: p-value &lt; 0.007.
<span class=""math-container"">```</span>
</code></pre>
",2020-12-30 10:40:46.803
333808,69840.0,2,,133686.0,,,,CC BY-SA 4.0,"<p>An answer from game theory would be that you should use the <a href=""https://en.wikipedia.org/wiki/Shapley_value"" rel=""nofollow noreferrer"">Shapley Value</a>. In a nutshell, the Shapley Value tells you, if you have a company which creates M units of profit, how to share those units fairly amongst its employees, where fairness is defined relative to productivity/value add of the employee.</p>
<p>The analogue in machine learning (sticking with binary classification for the sake of an explanation but similar analogues can be drawn in regression) goes as follows. Say that your classifier is predicting <span class=""math-container"">$P(Y=1|\underline{x})=0.83$</span> whereas the base rate, i.e. <span class=""math-container"">$P(Y=1)$</span> in your data is perhaps 0.53. Thus for this example, the probability of Y being equal to 1 is 0.3 higher than the base rate. This is like a company which made 0.3 units of profit, and wants to share that profit up fairly amongst its M employees (where M is the dimensionality of <span class=""math-container"">$\underline{x}$</span>, i.e. the number of features).</p>
<p>In order to calculate these Shapley values, you must figure out what your classifier would be predicting, if it only had access to any subset of the full set of features (these subsets are known as coalitions), of which there are <span class=""math-container"">$2^{M}$</span>, and thus calculating these in practice can be difficult and certainly requires numerical trickery for all but the most simple cases. For the state of the art in this field, see <a href=""https://arxiv.org/abs/2006.01272"" rel=""nofollow noreferrer"">here</a> (disclaimer, I am not an author of this paper but I do work with some of the authors). For an open source package which takes some shortcuts but is easy to use, try <a href=""https://github.com/ModelOriented/treeshap"" rel=""nofollow noreferrer"">TreeShap</a></p>
<p>Note, it is very tempting to interpret Shapley values Causally and this is plain incorrect. For example, if your feature vector <span class=""math-container"">$\underline{x}$</span> contains two features which are highly correlated, where one causally affects the target and the other does not, they will likely have similar Shapley values (there are ways around this using asymmetric Shapley values, when you know the causal relationships, but Shapley values can't help you determine what's causative and what's correlative if you don't already know). The Shapley value of a feature must be interpreted strictly as &quot;the amount by which this example is predicted by our model to be more/less likely than the baseline to be of the positive/negative class, is attributable this much to this feature&quot;</p>
<p>Even then, this can be somewhat disappointing/counterintuitive. Using the above example again, when you have two features which are highly correlated (perhaps one equals the other plus white noise), they will likely have very similar Shapley values. If however, you retrained your model, removing one of these features, the remaining feature would simply double its Shapley value.</p>
<p>The main utility in my opinion, is exactly for the kinds of use case you are talking about. Broadly speaking, you know that low-income low credit score individuals are more likely to be rejected for a loan, but it's nice to know, at the individual level, which feature is being used more. This tells you nothing causative, it doesn't tell you how to improve your chances of not defaulting on a loan, but it can tell you how to improve your chances of getting a loan (i.e. should I work on my credit score or do I need to get a pay rise before the algorithm changes its mind about me?)</p>
",2020-12-30 15:50:14.900
80672,30778.0,1,80677.0,,,"Which notation and why: $\text{P}()$, $\Pr()$, $\text{Prob}()$, or $\mathbb{P}()$",<probability><notation>,CC BY-SA 3.0,"<p>Are these merely stylistic conventions (whether italicized or non-italicized), or are there substantive differences in the meanings of these notations?</p>

<p>Are there other notations meaning ""<em>the probability of</em>"" that should be considered in this question?</p>
",2014-07-18 17:34:55.273
333782,252107.0,2,,333777.0,,,,CC BY-SA 4.0,"<p>There won't be an efficient general-case exact algorithm for this.</p>
<p>Suppose you had an algorithm that finds the exact solution. You could use it to find the probability that a random sequence is more likely to be from <span class=""math-container"">$A$</span> than from <span class=""math-container"">$B$</span>, and also to find the probability that a random sequence is more likely to be from <span class=""math-container"">$B$</span> than from <span class=""math-container"">$A$</span>. Whether those two probabilities sum to 1 tells you whether there exists a sequence that is exactly equally likely to come from either distribution.</p>
<p>Suppose that <span class=""math-container"">$A$</span> is a uniform distribution on the integers <span class=""math-container"">$1, ..., n$</span>. Then every sequence taking values on those integers has probability <span class=""math-container"">$\frac{1}{n^k}$</span> of being drawn from <span class=""math-container"">$A$</span>.</p>
<p>Suppose that <span class=""math-container"">$b_m$</span> is the probability of drawing <span class=""math-container"">$m$</span> from the distribution <span class=""math-container"">$B$</span>.</p>
<p>Then an exact solution would tell us whether there exist <span class=""math-container"">$1 \le m_1 \le ... \le m_k \le n$</span> such that <span class=""math-container"">$\prod_i b_{m_i} = \frac{1}{n^k}$</span>. Or equivalently, whether <span class=""math-container"">$\sum_i -\log b_{m_i} = k \log n$</span>.</p>
<p>This is essentially the <a href=""https://stackoverflow.com/questions/8916539/sum-subset-with-a-fixed-subset-size"">sum-subset problem with fixed size constraint</a>, for which there is no known efficient algorithm (except for small <span class=""math-container"">$k$</span>).</p>
",2020-12-30 11:13:03.650
333783,125923.0,2,,333779.0,,,,CC BY-SA 4.0,"<p>'Significance' in statistics means 'low probability of obtaining results at least as extreme as actually obtained'. You need to make some assumptions before you can proceed. Here is one example:</p>
<p>You can require that in a valid course a certain fraction of students needs to pass, say 90%. You can also fix the so-called ’significance level’ <span class=""math-container"">$\alpha$</span> to some low probability value, say 5%. In addition, you can make the assumption that between repeated courses the number of students that pass is distributed according to the binomial distribution with the above required probability <span class=""math-container"">$p$</span> (90% = 0.9). I.e. you model passing of a student by an flip of an 'unfair' coin.</p>
<p>You can then use the CDF of the binomial distribution to show that, for a class of 4 students, the probability of only two (or less) of them passing is greater than 5%, even though each student has individually a 90% chance of passing.</p>
<p>In Python (this code is adopted from <a href=""https://www.geeksforgeeks.org/python-binomial-distribution/"" rel=""nofollow noreferrer"">GeeksforGeeks</a>):</p>
<pre><code>from scipy.stats import binom 
# setting the values 
# of n and p 
n = 4
p = 0.9
# defining the list of k values 
k_values = list(range(n + 1)) 
# obtaining the mean and variance 
mean, var = binom.stats(n, p) 
# list of CDF values 
dist = [binom.cdf(k, n, p) for k in k_values ] 
# printing the table 
print(&quot;k\tCDF(k)&quot;) 
for i in range(n + 1): 
    print(str(k_values[i]) + &quot;\t&quot; + str(dist[i])) 
# printing mean and variance 
print(&quot;mean = &quot;+str(mean)) 
print(&quot;variance = &quot;+str(var))
</code></pre>
<p>results in:</p>
<pre><code>k   CDF(k)
0   9.999999999999991e-05
1   0.003699999999999998
2   0.05229999999999998
3   0.34389999999999993
4   1.0
mean = 3.6
variance = 0.35999999999999993
</code></pre>
",2020-12-30 11:14:51.033
333784,13244.0,2,,316695.0,,,,CC BY-SA 4.0,"<p>Confidence and certainty are different concepts, though strongly linked. There are two important properties for use cases such as decision making. We look for classifiers that are certain about their outcome (they rely on clues that lead to <em>actually correct</em> outcomes), and confident (among all possible answers, the selected answer is deemed as much more likely to be true, with his own probability estimates).</p>
<p>To give a particular example: neural networks are prone to be overconfident (even when giving a wrong answer, the preferred answer has a much higher score than the rest). They are confident, but not certain (see <a href=""https://arxiv.org/pdf/1706.04599.pdf"" rel=""nofollow noreferrer"">On Calibration of Modern Neural Networks</a> for a description of this issue). I would like to have a classifier that I can rely on, whenever he says is confident about an answer.</p>
<p>Confidence, thus, refers to how &quot;sure&quot; a classifier is about its outcome. In your example, the classifier is much more confident in his second prediction. But that does not mean it is certain. Entropy gives you a measure of how confident the predictions are, but not of how certain. Confidence is about the distributions of the outcome probabilities.</p>
<p>Certainty is more about being calibrated (the classifier estimates reflect the actual chances of something actually happening). Again, refer to the article.</p>
",2020-12-30 11:31:17.937
333785,241161.0,1,,,,Should I balance my dataset for binary classification?,<classification><python><unbalanced-classes><resampling>,CC BY-SA 4.0,"<p>I am running through a binary classification problem.
My target variable is unbalanced the 1s are the 13% of the whole dataset and 0s 87%. Total number of observation 697, number of features 709. Features are time series (financial indeces).</p>
<p>In here <a href=""https://stats.stackexchange.com/questions/321970/imbalanced-data-smote-and-feature-selection"">Imbalanced data, SMOTE and feature selection</a>
I read that feature selection should be applied before using SMOTE but I am kind of unsure if the balancing is really necessary. And also what kind of procedure if SMOTE (oversampling) or RandomUnderSampler (undersampling).</p>
<p>Thanks, Luigi</p>
",2020-12-30 11:36:49.693
333786,243325.0,2,,333785.0,,,,CC BY-SA 4.0,"<p>SMOTE creates artificial data-points, and in my opinion, it should always be the last option to try. The reason being &quot;creating artificial data-points.&quot;</p>
<p>I would follow these steps:</p>
<p>1 - Test some classifiers with the data you have. If the metrics are good enough for your particular goal, you are done.</p>
<p>2 - If the metrics are not good enough, try to engineer new features that capture information regarding the underrepresented class. For instance, once I worked on a time-series classification problem, I realized my under-represented class was quite sensitive to time information. Adding time features such as day of week and hour of day improved my scores a lot, and I did not need to try approaches like SMOTE or under-sampling.</p>
<p>3 - If adding features not enough, try re-weighting loss function, that is, give more weight to examples of underrepresented class when optimizing model parameters (Note that this approach is not possible if you are using a model with a close form solution for your predictions).</p>
<p>4 - If all above fails, then try under or oversampling. Note that this approach always carries the risk of poor out-of-sample results. The reason being you are changing the distribution of your data and making it different than what you will observe out-of-sample</p>
<p>Finally, the best approach is always to get more data, but this is not possible most of the time</p>
",2020-12-30 12:17:41.997
333791,255212.0,2,,172027.0,,,,CC BY-SA 4.0,"<p>It's gonna work on both approach. But applying the fit method of the vectorizer in all dataset  might introduce some data leakage. As principle, your model shouldn't see the test data. So to guarantee that your model will not see test data on the training fase, you should split first, vectorizer after.</p>
<pre class=""lang-py prettyprint-override""><code># Corpus Selection
corpus_sample = data_sample['processed_review']
# Target Selection
y_sample = data_sample['score']
#split
X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(
    corpus_sample, y_sample,
    test_size=0.2,
    stratify=y_sample,
    random_state=42
)
# Apply vectorizer
tfidf = TfidfVectorizer()
tfidf.fit(X_sample_train)
X_sample_train =  tfidf.transform(X_sample_train)
X_sample_test= tfidf.transform(X_sample_test)
</code></pre>
",2020-12-30 13:07:45.847
333792,221954.0,2,,333279.0,,,,CC BY-SA 4.0,"<p>As far as i see, this problem has similarities  with Non-Negative Matrix Factorization (NMF) as data <span class=""math-container"">$G$</span> matrix has non negative entries with many zeros.</p>
<p><span class=""math-container"">$$G_{i,j}= \sum_n W_{i,k} H_{k,j}$$</span></p>
<p>Where <span class=""math-container"">$W$</span> and <span class=""math-container"">$H$</span> are <span class=""math-container"">$n×k$</span> and <span class=""math-container"">$k×n$</span> matrices and <span class=""math-container"">$G$</span> is approximated/represented with <span class=""math-container"">$WH$</span>. The major caveat here <span class=""math-container"">$W$</span> and <span class=""math-container"">$H$</span> have to have only non negative entities too. If your end results is to have a kind of &quot;transformed&quot; non negative correlation matrix C   factorized into two  non negative vector <span class=""math-container"">$W, H$</span> , then NMF might help..</p>
<p>Element of Statistical Learning page 553, section 14.6 represents an optimization setup based on minimising a divergence objective function where <span class=""math-container"">$G_{i,j}$</span> has a Poisson distribution .</p>
",2020-12-30 13:19:48.443
333793,158099.0,2,,333750.0,,,,CC BY-SA 4.0,"<h3>Confusion in the notation</h3>
<p>There is quite confusion in the formulas. For example, we estimate <span class=""math-container"">$Q_a$</span> via <span class=""math-container"">$\frac{q_a}{q}Q$</span>, and treat it as a random variable. Assuming <span class=""math-container"">$Q,q$</span> are known scalars, and <span class=""math-container"">$q_a$</span> is a random variable, the source of randomness inside <span class=""math-container"">$Q_a$</span> comes from <span class=""math-container"">$q_a$</span>, and the following holds true:</p>
<p><span class=""math-container"">$$\operatorname{var}(Q_a)=\frac{Q^2}{q^2}\operatorname{var}(q_a)$$</span></p>
<p>Above formula is a function of <span class=""math-container"">$Q$</span> and <span class=""math-container"">$q$</span>, however, the formula in <em>UK's Traffic Appraisal Manual</em> contains all the terms, i.e. <span class=""math-container"">$Q, q$</span> and <span class=""math-container"">$q_a$</span>. So, the random variable <span class=""math-container"">$q_a$</span> appears in the final formula as well. This is like saying <span class=""math-container"">$\operatorname{var}(X)\propto \alpha X$</span>, which is in general not correct, since the result should've been a scalar instead of a random quantity. If the formula had actually meant <span class=""math-container"">$\operatorname{var}(Q_a|q_a)$</span>, so that we have the right to use <span class=""math-container"">$q_a$</span> in the output expression, then the variance should've been <span class=""math-container"">$0$</span>, since there is no other randomness left in the expression of <span class=""math-container"">$q_a$</span>, i.e. <span class=""math-container"">$\operatorname{var}(X|X)=0.$</span></p>
<p>So, although the intuition is clear, i.e. figuring out a measure of uncertainty in the estimation of the true number of cars having the attribute of interest, the formula doesn't make sense in the analysis.</p>
<h3>An approximate formula</h3>
<p>The situation resembles <a href=""https://en.wikipedia.org/wiki/Hypergeometric_distribution"" rel=""nofollow noreferrer"">hypergeometric distribution</a>, although in that <span class=""math-container"">$Q,q,Q_a$</span> are known. Here, we don't know <span class=""math-container"">$Q_a$</span>, and estimate it using a simple formula, which is also the method of moments estimator for <span class=""math-container"">$Q_a$</span>. Let's say the true number of cars that are of interest is <span class=""math-container"">$R$</span> (since we reserve <span class=""math-container"">$Q_a$</span> for the estimation, i.e. <span class=""math-container"">$\hat R=Q_a$</span>). Then, the variance of <span class=""math-container"">$q_a$</span> is given in the wikipedia page as follows: <span class=""math-container"">$$\operatorname{var}(q_a)=q\frac{R}{Q}\frac{Q-R}{Q}\frac{Q-q}{Q-1}\rightarrow\operatorname{var}(Q_a)=\frac{R(Q-R)(Q-q)}{q(Q-1)}$$</span></p>
<p>But, we don't know <span class=""math-container"">$R$</span>, so we can't quantify this formula. A simple thing to do is just plugging in its estimation, i.e. <span class=""math-container"">$R\approx \hat R=Q_a=q_aQ/q$</span> in the formula:
<span class=""math-container"">$$\operatorname{var}(Q_a)\approx \frac{Q}{(Q-1)}\frac{Q(Q-q)}{q^3}q_a(q-q_a)=\frac{Q(q-1)}{(Q-1)q}\underbrace{\frac{Q(Q-q)}{q^2(q-1)}q_a(q-q_a)}_{\text{Given Formula}}$$</span></p>
<p>This formula is very similar to the one given in the manual, the multiplier in the beginning can actually be assumed close to <span class=""math-container"">$1$</span>, if both <span class=""math-container"">$q$</span> and <span class=""math-container"">$Q$</span> are assumed sufficiently large, since <span class=""math-container"">$\frac{q}{q-1}\approx \frac{Q}{Q-1}$</span>.</p>
<h3>Bayesian way</h3>
<p>As I said, this is an <em>approximation</em> to the variance. In order to correctly quantify the variance, a prior distribution over <span class=""math-container"">$R$</span> should be present. Then, the formula we found above would be <span class=""math-container"">$\operatorname{var}(Q_a|R)$</span>, and we could use Law of Total Variance to find the variance of <span class=""math-container"">$Q_a$</span>:</p>
<p><span class=""math-container"">$$\operatorname{var}(Q_a)=E[\operatorname{var}(Q_a|R)]+\operatorname{var}(E[Q_a|R])$$</span></p>
<p>Here, the first term is a simple expected value <span class=""math-container"">$E[R(Q-R)]\frac{Q-q}{q(Q-1)}$</span>, and the second term is
<span class=""math-container"">$$\operatorname{var}(E[Q_a|R])=\operatorname{var}(E[q_aQ/q|R])=\operatorname{var}(R)$$</span></p>
<p>both depending on the prior distribution of <span class=""math-container"">$R$</span>. I don't think there is one explicitly assumed in the document.</p>
",2020-12-30 13:20:01.970
333794,154739.0,2,,333279.0,,,,CC BY-SA 4.0,"<p>Analysis:</p>
<ol>
<li><a href=""https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#For_a_sample"" rel=""nofollow noreferrer"">Sample correlation</a> is a highly-nonlinear function, so it is unlikely we will be able to solve the optimization problem analytically. So we will need to look for a numerical solution.</li>
<li>The unconstrained problem clearly has multiple extrema, because there are multiple ways to achieve the same correlation value. Further, since you require an integer solution to your matrix coefficients, some extremas may be better than other because they are closer to integer values and introduce less loss when rounded off. It must be noted that exact solutions to optimization problems over integers are often NP-hard, so only an approximate solution may be feasible.</li>
</ol>
<p>So, if you don't want to delve too deep into mathematics and only need an approximate solution, you might try some standard optimization techniques (e.g. <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"" rel=""nofollow noreferrer"">scipy</a>). The strategy would be to optimize the loss function over non-negative floating-point numbers, then round them off to nearest integers. Since there are multiple extrema in place, it would be beneficial to run the algorithm for multiple random initial conditions. Finally, if performance is an issue, you may consider computing <a href=""https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"" rel=""nofollow noreferrer"">Jacobian</a> and/or <a href=""https://en.wikipedia.org/wiki/Hessian_matrix"" rel=""nofollow noreferrer"">Hessian</a> of your loss function analytically, as some methods can perform significantly faster if such information is provided.</p>
<p>Also you mentioned very sparse solutions. Sparsity of a numerical solution is typically controlled by introducing additional <a href=""https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a"" rel=""nofollow noreferrer"">L1 Norm</a> to the loss function, and regulating the desired accurracy/sparsity ratio by changing the prefactor in front of the L1 Norm. In your case, if you require independent sparsity constraints on each row, you may consider adding an independent penalty for each row.</p>
",2020-12-30 13:31:10.080
333795,158099.0,2,,333787.0,,,,CC BY-SA 4.0,"<p>It's not a standard notation, but very common in papers. Especially papers including variational analysis. Although it's not entirely possible to say w/o seeing the context, your interpretation is possibly correct, i.e. the expected value is taken over the joint distribution of <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> wrt the function <span class=""math-container"">$p_{X,Y}(x,y)$</span>.</p>
<p>Normally, if there are no other alternative joint distributions associated with these variables, then the subscript doesn't clarify anything because <span class=""math-container"">$E[k(X,Y)]$</span> would mean the same thing since the expected value is always taken wrt to the joint distribution of the variables inside the parentheses.</p>
<p>It's also a common abuse of notation to denote the random variables with lowercase letters, but typically these are reserved for special values for those RVs, i.e. <span class=""math-container"">$E[k(x,y)]$</span> is typically not the same thing with <span class=""math-container"">$E[k(X,Y)]$</span> in more rigorous texts.</p>
",2020-12-30 13:32:13.183
333796,,1,333805.0,,anon,What is it called to use random error as evidence?,<terminology>,CC BY-SA 4.0,"<p>There are 100 clinical trials. I have 95% confidence and report five of them that are successful. Then, I claim my drug works.</p>
<p>What is this called, when I only publish good results and rely purely on random error? It's <em>publication bias</em>, but there must be a general term.</p>
<p><a href=""https://www.bmj.com/content/352/bmj.i637"" rel=""nofollow noreferrer"">https://www.bmj.com/content/352/bmj.i637</a></p>
<blockquote>
<p>The proportion of clinical trials published within 24 months of study completion ranged from 10.8% (4/37) to 40.3% (31/77) across academic medical centers, whereas results reporting on ClinicalTrials.gov ranged from 1.6% (2/122) to 40.7% (72/177)</p>
</blockquote>
<p>Do they seriously have 95% confidence and report 5% of results?</p>
",2020-12-30 14:10:39.733
333797,211.0,1,,,,Is there a G-test equivalent for continuous variables?,<hypothesis-testing><chi-squared-test><goodness-of-fit><likelihood-ratio><kullback-leibler>,CC BY-SA 4.0,"<p>The <a href=""https://en.wikipedia.org/wiki/G-test#Relation_to_Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">G-test</a> is similar to the chi-square test for goodness of fit. It is proportional to the <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">kl-divergence</a>.</p>
<p>I am wondering if there is a similar test that is applicable to continuous variables. Since there is a <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Definition"" rel=""nofollow noreferrer"">continuous version</a> of kl-divergence, so I imagine there could be something of this sort.</p>
<p>I'm also guessing the <a href=""https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"" rel=""nofollow noreferrer"">Kolmogorov–Smirnov test</a> wouldn't be the answer, since it focuses on goodness of fit based on max distance of the CDF, while kl-divergence is a metric that relies an averaging.</p>
",2020-12-30 14:35:40.450
333798,211463.0,2,,102770.0,,,,CC BY-SA 4.0,"<p>You can't have accuracy scores without the real values. However, you can split your data into test and training data to test the model accuracy. If the model is having good results, it will probably also work well on forecasting future data. When you have access to future data values, you will then be able to compare those values.</p>
",2020-12-30 14:39:50.883
217992,6708.0,1,,,,Time varying (auto)correlation estimation,<time-series><references><autocorrelation><time-varying-covariate>,CC BY-SA 4.0,"<p>I would need to estimate a time varying autocorrelation of a variable. Do you have any references or examples? 
I've tried to search for a package in <code>R</code> but I wasn't able to find it.
Do you have any suggestion? (it would be fine also in Matlab or Stata).</p>
",2018-01-22 12:38:46.880
333799,237420.0,1,,,,"To estimate the total amount of veterans in U.S. with total population given as auxiliary variable, should I use regression or ratio estimation?",<regression><self-study><estimation><sampling>,CC BY-SA 4.0,"<p>There is a sample of size =100 of counties in U.S. where for each county a total population and amount of veterans are given. The sample is drawn using simple random sampling. What would be the best estimation method to use, in case I'd like to estimate the total amount of veterans in U.S. As I understand the ratio estimation is used when there is a correlation between the estimated variable and auxiliary variable. And regression estimation is used, when there is a linear relationship between variables, but there isn't proportionality. In this case, I suppose there should be both linear correlation between total population in a county and the amount of veterans and proportionality. Ratio estimation in this instance seems like a valid method to use, but I am not sure. Because I don't have a clear idea of what proportionality means in this instance. The total amount of all population is given as well. Can someone help?
I tried to estimate the total amount of veterans using formula: <span class=""math-container"">$\frac{y_T}{x_T} \cdot X_T$</span>, where <span class=""math-container"">$y_T$</span> is total amount of veterans from all the sample counties and <span class=""math-container"">$x_T$</span> - the total sample population. <span class=""math-container"">$X_T$</span> is total population in U.S. at the time. I also computed the regular total population estimator: <span class=""math-container"">$N\bar{y}$</span> and it was much larger than the ratio estimator. In what instances the ratio estimator is more precise than the regular one? Also, how to get standard error for the ratio estimate?</p>
",2020-12-30 14:40:12.087
333800,39005.0,2,,9524.0,,,,CC BY-SA 4.0,"<p><a href=""https://en.wikipedia.org/wiki/The_Man_Who_Knew_Infinity_(film)"" rel=""nofollow noreferrer"">The Man Who Knew Infinity</a> is based on the life of Srinivasa Ramanujan.</p>
<p>It's a beautiful movie directed by Matthew Brown. Mathematicians <a href=""https://en.wikipedia.org/wiki/Manjul_Bhargava"" rel=""nofollow noreferrer"">Manjul Bhargava</a> and <a href=""https://en.wikipedia.org/wiki/Ken_Ono"" rel=""nofollow noreferrer"">Ken Ono</a> collaborated on the film.</p>
",2020-12-30 14:42:41.227
333801,,2,,333796.0,Grand Torini,,,CC BY-SA 4.0,"<p>Cherry picking, suppressing evidence, or the fallacy of incomplete evidence ... you can check <a href=""https://en.wikipedia.org/wiki/Cherry_picking"" rel=""noreferrer"">Wikipedia</a>:</p>
<blockquote>
<p>Cherry picking, suppressing evidence, or the fallacy of incomplete evidence is the act of pointing to individual cases or data that seem to confirm a particular position while ignoring a significant portion of related and similar cases or data that may contradict that position.</p>
</blockquote>
",2020-12-30 14:43:22.700
333802,199619.0,2,,333797.0,,,,CC BY-SA 4.0,"<p>The G-test has become a favorite of mine for proportion testing. ;)</p>
<p>The way it works is by fitting a (multinomial) logistic regression model with a factor variable as a predictor, plus an intercept. Then it fits an intercept-only model. Finally, it performs a likelihood ratio test of the factor variable by comparing the two models. (I have verified this with simulations in R, and I would encourage skeptical readers to do the same.)</p>
<p>If you want to do something similar but for a continuous predictor, fit a model with that predictor (and intercept), fit a model with just the intercept, and perform a likelihood ratio test.</p>
",2020-12-30 14:46:09.127
333803,232582.0,2,,333775.0,,,,CC BY-SA 4.0,"<p>In what context are these terms? Surely they must be defined somewhere if needed. If you thought about them, then their definition is up to you.</p>
<p>&quot;Inter-annual variability&quot; doesn't tell much. It could be the variance between yearly averages, i.e. the average of the variance within year 1, variance within year 2 etc. but that would give you only one number. Or it could be the year-over-year variance, i.e. the variance of: variable at day 1 year 1, day 1 year 2 etc., like stock returns (though I don't see this making much sense here). It may make more sense to get the &quot;intra-annual variance&quot; here, i.e. the variance of the variable within a year.</p>
<p>&quot;Seasonal amplitude&quot; is just as vague. Could mean the absolute distance of the most extreme value observed (negative or positive) from the mean of that year/season. Or it could mean fitting a Fourier series (a sine or cosine curve may do) for each year/season and calculating the magnitude of the amplitude of the wave.</p>
",2020-12-30 14:51:12.787
333804,,2,,333796.0,Anton,,,CC BY-SA 4.0,"<p>Although the technical details may deter folk from answering, they are secondary to your main question about English terminology.</p>
<p>Nevertheless, let's get them out of the way. You have 95% confidence in the results; I take this to mean you <em><strong>knew</strong></em> that 5 in a hundred fail. The chance of randomly picking 1 success from a batch of 100 is 95/100, leaving 99 to choose from. A second success of 1 from 99 has chance 94/99, and so forth. It works out that the chance of randomly picking 5 successes from 100 is about 77%.</p>
<p>But you have presented 5 successes to the world as evidence that the drug works with 100% confidence rather the 77% confidence that you should have inferred. This is at least <em>statistical misrepresentation</em>.</p>
<p>Even if you chose your five examples at random, you have chosen to present a circumstance that happens only 77% of the time in such trials as one that always happens. This is <em>unwarranted inference</em>.</p>
<p>Or you may have chosen your five examples knowing them to be successes. This is selective bias and is <em>statistical falsification</em>.</p>
",2020-12-30 15:08:01.010
333805,1664.0,2,,333796.0,rajah9,,,CC BY-SA 4.0,"<h1><span class=""math-container"">$p$</span>-value hacking</h1>
<blockquote>
<p>I’ve learned that the headline-grabbing cases of misconduct and fraud are mere distractions. The state of our science is strong, but it’s plagued by a universal problem: Science is hard — really f**ing hard. If we’re going to rely on science as a means for reaching the truth — and it’s still the best tool we have — it’s important that we understand and respect just how difficult it is to get a rigorous result. I could pontificate about all the reasons why science is arduous, but instead I’m going to let you experience one of them for yourself. Welcome to the wild world of <strong><span class=""math-container"">$p$</span>-hacking</strong>.</p>
</blockquote>
<p>From an introductory paragraph at &quot;<a href=""https://fivethirtyeight.com/features/science-isnt-broken/#part1"" rel=""noreferrer"">Science isn't broken</a>,&quot; a feature at Fivethirtyeight.com (Christie Aschwanden, Aug. 19, 2015).</p>
<p>The article describes how you can achieve publishable results (and reject a null hypothesis) even though the results are not reproducible.</p>
<p>The <span class=""math-container"">$p$</span>-value is that &quot;due to random chance&quot; footnote that you are looking for. By hacking it, you can get your results published.</p>
",2020-12-30 15:15:50.223
333806,9081.0,2,,9524.0,,,,CC BY-SA 4.0,"<p><a href=""https://en.wikipedia.org/wiki/Stand_and_Deliver"" rel=""nofollow noreferrer"">Stand and Deliver</a> is a good film about the Bolivian-born math teacher <a href=""https://en.wikipedia.org/wiki/Jaime_Escalante"" rel=""nofollow noreferrer"">Jaime Escalante</a>. Inspiring! <a href=""https://www.npr.org/templates/story/story.php?storyId=125380015"" rel=""nofollow noreferrer"">See this commentary</a></p>
",2020-12-30 15:36:22.567
333807,154739.0,2,,333419.0,,,,CC BY-SA 4.0,"<p><strong>NOTE: I'm still editing this post, need a bit more time to finish</strong></p>
<p>Here's a philosophical aside on the limits of few-shot learning. It is not exactly the answer to the question, but I guess it could help set the expectations straight.</p>
<p><strong>Part 1: Naive estimate of number of samples needed for classifier convergence</strong>. Let's say we did some dimensionality reduction on the available images and extracted <span class=""math-container"">$N$</span> features. We could argue that these features are representative of the images as finer features are weak and likely imperceptible for the human eye. For simplicity, let us assume that all of the features are orthogonal. Further, for the sake of the argument, let's assume that exactly 1 of these features is used by the participant to classify the images as those they like vs those they don't. Our goal is to find which feature that is. We have to know this well, or we would not be able to make predictions. Let's assume that each feature has a standard normal distribution <span class=""math-container"">$\mathcal{N}(0,1)$</span> over the available images, and that the important feature will be greater than zero if the image is liked, otherwise below, meaning that the participant will like approximately half of all the images. The question now is: how many trials do we require to find the correctly-predicting feature. The expected number is <span class=""math-container"">$\log_2 N$</span> - there will be some features which will be predictive at random, and at every trial each of them will have 50% probability of still being predictive by chance, meaning that 50% of them will drop out. A related and a bit more realistic question is - how many trials do we require to guarantee that all non-informative features have dropped off (e.g. with 95% confidence). A little less intuitively, but this value also scales as <span class=""math-container"">$\log_2 N$</span>. The proof is as follows: after <span class=""math-container"">$t$</span> trials, the probability that one non-informative feature still looks informative by chance is <span class=""math-container"">$p_1 = 2^{-t}$</span>. We are looking for the probability that <span class=""math-container"">$N$</span> non-informative features have all been revealed, and we want this probability to be greater or equal to 95%. This can be written as <span class=""math-container"">$(1 - p_1)^N \geq 0.95$</span>. If we solve this inequality for <span class=""math-container"">$t$</span> and use series expansion, we will get <span class=""math-container"">$t \geq C + \log_2 N + O(\frac{1}{N})$</span> for some small constant <span class=""math-container"">$C$</span>.</p>
<p><strong>Part 2: Effect of noise</strong>. But this is the absolute lowest unrealistic estimate. The difficulty starts when we consider that participants such as humans and mammals are known to have variance in their choice, meaning that they do not stay consistent to their general strategy at all trials. To model this, we must allow for a fraction of trials to be non-informative. Let's say that fraction of correct trials for informative features is <span class=""math-container"">$p_I = 90\%$</span>. We need a condition to refute the feature if it is non-informative. Dropping some finer details of <a href=""https://en.wikipedia.org/wiki/Statistical_hypothesis_testing"" rel=""nofollow noreferrer"">hypothesis testing</a>, we will find the fraction <span class=""math-container"">$\phi=\frac{t_{good}}{t}$</span> of trials for which the feature is informative, and compare it with <span class=""math-container"">$p_I$</span>. The more trials we measure, the closer we would expect the observed fraction to be to the true fraction. So, we would consider the channel as informative, if the observed fraction of trials <span class=""math-container"">$\phi$</span> is greater or equal to some threshold <span class=""math-container"">$\phi_0$</span>, which can be written as</p>
<p><span class=""math-container"">$$\phi_0 = p_I+\frac{\sigma_I}{\sqrt{t}}K$$</span></p>
<p>where <span class=""math-container"">$\sigma_I$</span> is the variance of the fraction (in our binomial case it is <span class=""math-container"">$\sigma_I = \sqrt{p_I(1-p_I)}$</span>), and <span class=""math-container"">$K$</span> is some constant which depends on the desired confidence of the test (for normal approximation it is <span class=""math-container"">$K=\sqrt{2}erf^{-1}(2\alpha - 1)$</span> where <span class=""math-container"">$\alpha$</span> is the confidence level). Next, we need to find the probability that a non-informative feature fails the test <span class=""math-container"">$\phi \geq \phi_0$</span>, namely, that the result will be <span class=""math-container"">$\phi &lt; \phi_0$</span>. For a non-informative trial, the fraction of correct trials will be <span class=""math-container"">$p_{NI}=50\%$</span>. Using <a href=""https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem"" rel=""nofollow noreferrer"">DeMoivre-Laplace approximation</a>, the sample fraction for non-informative features will be normally distributed, namely <span class=""math-container"">$$\phi \sim \mathcal{N}(\frac{1}{2}, \frac{1}{4t})$$</span></p>
<p><strong>Part 3: Effect of feature synergy</strong>. Finally, situation is further complicated if individual features are insufficient for good prediction, and synergistic predictors are required. For example, if we require a pair of features to be simultaneously high (e.g. a person likes red images that are also very sharp), then we effectively have <span class=""math-container"">$N^2$</span> features to consider.</p>
<p><strong>TL;DR</strong>: Humans have to be cheating when performing few-shot learning. The only way to learn fast is to have prior information on what features are likely to be salient (predictive of outcome).</p>
",2020-12-30 15:40:56.757
333809,238585.0,2,,307384.0,,,,CC BY-SA 4.0,"<p>One of the differences is that bootstrap does not preserve the order of the observations. It will sample observations (or blocks of observations, if using Block Bootstrap) in random order. Because you mentioned MLdP, I assume that you are working with financial time series. A possible disadvantage of bootstrapping is that it will not always preserve some properties of the time series such as autocorrelation and volatility clustering. The Tapered Block Bootstrap mitigates but does not completely eliminate those limitations.</p>
<p>In contrast, CPCV preserves the original order of the samples and just uses different parts of the dataset as training and validation data.</p>
",2020-12-30 16:43:29.467
333810,184331.0,2,,20582.0,,,,CC BY-SA 4.0,"<p>Here is my understanding, gathered largely from the paper <a href=""https://www.sciencedirect.com/science/article/abs/pii/0165168494900299"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/abs/pii/0165168494900299</a></p>
<p>First, some non-rigorous reasoning. PCA produces an orthogonal matrix defined up to a +-1 scaling of the columns, while ICA (when applicable) yields a generic matrix defined up to any scaling and reordering of the columns. If ICA and PCA give the same matrix, then the true matrix <span class=""math-container"">$A$</span> can be obtained from it by column scaling and reordering, and thus has orthogonal columns, and this is the condition of PCA and ICA coincidence. When the sources are Gaussian, ICA simply does not work.</p>
<p>Now for an attempt at a more thorough argument. I am describing this in terms of random variables, not samples from them - the latter will behave similarly, but with an effect of sampling noise. (So, a 'known random vector <span class=""math-container"">$x$</span>' below means that the joint distribution of its components is fully known).</p>
<p>ICA</p>
<p>Suppose <span class=""math-container"">$x$</span> and <span class=""math-container"">$s$</span> are column vectors of random variables with zero means, and <span class=""math-container"">$x = As$</span>, where A is a full-rank constant matrix (possibly non-square with more rows than columns), and the components of <span class=""math-container"">$s$</span> are independent. ICA is defined as finding the matrix <span class=""math-container"">$A$</span> for a known vector <span class=""math-container"">$x$</span> (and then <span class=""math-container"">$s$</span> can also be found from the equation). It is not possible to find exactly <span class=""math-container"">$A$</span> as any matrix from the set obtained from <span class=""math-container"">$A$</span> by an arbitrary scaling of each column and an arbitrary permutation of the columns will also yield an <span class=""math-container"">$s$</span> whose components are independent. Thus, the best that can be done is finding this set of matrices. And this is possible if the components of true vector <span class=""math-container"">$s$</span> have finite, nonzero variances, and at most one of them is Gaussian (this is the theorem of ICA identifiability, see COROLLARY 13 in the paper).</p>
<p>PCA</p>
<p>Let <span class=""math-container"">$x$</span> be a column vector of random variables with zero mean and a finite covariance matrix. PCA is defined as finding an orthogonal matrix <span class=""math-container"">$Q$</span> such that for <span class=""math-container"">$\tilde{s}$</span> defined by <span class=""math-container"">$x = Q \tilde{s}$</span>, the components of <span class=""math-container"">$\tilde{s}$</span> are uncorrelated and their variances form a (weakly) decreasing sequence. Again, there is an indetermination: for any such <span class=""math-container"">$Q$</span>, a matrix obtained by multiplying any subset of the columns by -1 will also satisfy the conditions. If the variances of the components of <span class=""math-container"">$\tilde{s}$</span> are all different, this -1 scaling is the only indetermination, otherwise, for any set of components of <span class=""math-container"">$s$</span> with the same variance, <span class=""math-container"">$Q$</span> can be multiplied by an arbitrary rotation matrix in the space of the corresponding columns.</p>
<p>Now let us consider when ICA and PCA can give the same result. Let <span class=""math-container"">$x = As$</span>, with <span class=""math-container"">$A$</span> being a square matrix and <span class=""math-container"">$s$</span> satisfying the conditions of ICA identifiability. Let <span class=""math-container"">$x = Q \tilde{s}$</span> be the PCA of <span class=""math-container"">$x$</span>, and suppose for now that the variances of the components of <span class=""math-container"">$\tilde{s}$</span> are all different. If <span class=""math-container"">$Q$</span> satisfies the conditions of ICA then it must coincide with the true matrix <span class=""math-container"">$A$</span> up to a scaling and reordering of columns. Thus, <span class=""math-container"">$A$</span> must have orthogonal columns (which is equivalent to <span class=""math-container"">$A^T A$</span> being diagonal). Conversely, if <span class=""math-container"">$A$</span> has orthogonal columns, then by scaling them to unit norm and ordering by strictly decreasing variance of <span class=""math-container"">$s_i$</span>, we obtain an orthogonal matrix and uncorrelated sources in the proper order, and thus, one of the possible PCA decompositions.</p>
<p>To sum up, the orthogonality of the columns of <span class=""math-container"">$A$</span> is a necessary and sufficient condition of ICA and PCA 'coincidence' under the above assumptions. The 'coincidence' should be understood as the fact that the sets of matrices obtained by both methods intersect, and each has minimal indetermination.</p>
<p>Concerning the question about an example where the covariance of <span class=""math-container"">$x$</span> is not diagonal, we can take <span class=""math-container"">$s_1$</span> uniform on [-1,1], <span class=""math-container"">$s_2$</span> uniform on [-2,2], and <span class=""math-container"">$A$</span> a rotation in 2D by an angle that is not an integer multiple of <span class=""math-container"">$\pi / 2$</span> (otherwise <span class=""math-container"">$cov(x)$</span> is diagonal). Here is an example for a rotation by <span class=""math-container"">$\pi / 6$</span>, with 1000 samples:
<a href=""https://i.stack.imgur.com/B5xdV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B5xdV.png"" alt=""Comparison of PCA and ICA"" /></a>
The lines are: black - true axes, blue - principal component directions, red - independent component directions. PCA was computed by the MATLAB function pca, while ICA was obtained using the EEGLAB package with a call: &quot;runica(y, 'extended', 1);&quot; The extended algorithm is needed if some components have distributions with negative excess kurtosis (called subgaussian, or platykurtic, distributions), as is the case here for uniform distributions.</p>
<p>If we remove the assumption of different variances of <span class=""math-container"">$\tilde{s}_i$</span>, keeping the ICA identifiability and the orthogonality of the columns of <span class=""math-container"">$A$</span>, PCA will be unable to select the 'true' basis within any subspace corresponding to the same variance, while ICA, as always, will find this basis up to a reordering and rescaling. Thus, the set of matrices satisfying the PCA conditions will include the set of ICA matrices with normalized columns, but the PCA set will be substantially larger (and thus give less information about the true sources).</p>
<p>Now let us try to remove the assumption of ICA identifiability. As an example, consider the situation when <span class=""math-container"">$A$</span> is square and <span class=""math-container"">$s$</span> has a multivariate Gaussian distribution with a diagonal covariance matrix having different values on the diagonal. In this case, the <span class=""math-container"">$\tilde{s}$</span> obtained by PCA will also be multivariate Gaussian, and its components will be uncorrelated and thus independent. So, the set of matrices produced by PCA will be included in the set provided by ICA. However, the indetermination in ICA will be larger than usual (i.e., than in the case of ICA identifiability). Indeed, if <span class=""math-container"">$A$</span> is the true matrix, and <span class=""math-container"">$D$</span> is the (diagonal) covariance matrix of <span class=""math-container"">$s$</span> then <span class=""math-container"">$A D^{1/2} R$</span> will be an ICA matrix for any rotation R. This can be seen from the fact that <span class=""math-container"">$A D^{1/2}$</span> is an ICA matrix with such a scaling that the corresponding sources have unit covariance (and are multivariate Gaussian). Any rotation of these sources will conserve this property and thus keep them independent. So, under these assumptions, if <span class=""math-container"">$A$</span> does not have orthogonal columns, the PCA matrix set will not include the true <span class=""math-container"">$A$</span>, while the ICA set will, but will have a larger than usual indetermination. Conversely, if <span class=""math-container"">$A$</span> has orthogonal columns, PCA will find <span class=""math-container"">$A$</span> with its minimal indetermination, while the ICA matrix set will contain A together with many other matrices (in practice, a random one of them will be output by software).</p>
<p>As a conclusion for applications, if the columns of <span class=""math-container"">$A$</span> are known to be orthogonal and <span class=""math-container"">$s_i$</span> have different variances, go with PCA (regardless of the distribution of <span class=""math-container"">$s_i$</span>, but if more than one of them are Gaussian, then, out of the two methods, only PCA will recover <span class=""math-container"">$A$</span> and <span class=""math-container"">$s$</span> under these assumptions). In all other cases, ICA is the way to go.</p>
",2020-12-30 17:01:59.440
333811,255264.0,1,,,,overfitting on less training data,<machine-learning><neural-networks><classification><python><overfitting>,CC BY-SA 4.0,"<p>I have a fact as follows:</p>
<blockquote>
<p>If we have a classifier trained on less training data is more likely
to overfit.</p>
</blockquote>
<p>This Fact is  true.</p>
<p>I have a challenging question here. Is there any guarantee that when we have less training data we always get stuck in overfitting?</p>
",2020-12-30 17:13:00.183
333812,24753.0,2,,333773.0,,,,CC BY-SA 4.0,"<p>I think you are right to doubt the output. I tried to replicate the result by implementing the <a href=""https://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm"" rel=""nofollow noreferrer"">Chi-square test</a> as it is supposed to work for categorical data, e.g. see this <a href=""https://en.wikipedia.org/wiki/Chi-squared_test#Example_chi-squared_test_for_categorical_data"" rel=""nofollow noreferrer"">link</a>. I don't get the result that comes from <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html"" rel=""nofollow noreferrer"">sklearn.feature_selection.chi2</a>, here's my code:and outputs:</p>
<pre><code>import numpy as np
from scipy.stats import chi2
from sklearn import feature_selection 
np.random.seed(1)

c = 291
n = 343180
X = np.random.randint(c,size=(n,1))
y = np.random.randint(c,size=(n,1))

print('chi2 test stat, pval:',feature_selection.chi2(X, y))

# assuming classes are in rows, calculate expected for rows
p_class = np.zeros((c,1), dtype=np.double)
for yi in y:
  p_class[yi] = p_class[yi] + 1
p_class = p_class / n

n_feat = np.zeros((c,1))
for x in X:
  n_feat[x] = n_feat[x] + 1

exp = p_class @ n_feat.T

# feature levels are columns  
obs = np.zeros((c,c))
for i in np.arange(n):
  obs[y[i],X[i]] = obs[y[i],X[i]] + 1


#print(exp)
#print(obs)

stat = np.sum((obs-exp)**2/exp)
dof = (c-1)**2
chi2crit = chi2.ppf(1-0.05,df=dof)
pval = 1-chi2.cdf(stat,df=dof)
print('my stat,pval,crit(5%):',stat,pval,chi2crit)


chi2 test stat, pval: (array([13790.24430418]), array([0.]))
my stat,pval,crit(5%): 84025.71482666291 0.5712498907190868 84775.72566291611
</code></pre>
",2020-12-30 17:46:32.777
333813,255268.0,2,,8478.0,,,,CC BY-SA 4.0,"<p>KL divergence upper bounds Kolmogrov Distance and Total variation, meaning that if two distributions <span class=""math-container"">$\mathcal{D}_1, \mathcal{D}_2$</span> have a small KL divergence, then it follows that <span class=""math-container"">$\mathcal{D}_1, \mathcal{D}_2$</span> have a small total variation and subsequently a small Kolmogrov distance (in that order).</p>
<p>Also check out this paper for more information - <a href=""https://math.hmc.edu/su/wp-content/uploads/sites/10/2019/06/ON-CHOOSING-AND-BOUNDING-PROBABILITY.pdf"" rel=""nofollow noreferrer"">On Choosing and Bounding Probability Metrics</a>, by Gibbs and Su.</p>
",2020-12-30 17:53:08.230
333814,255261.0,1,333826.0,,,goodness of fit test to check for normality,<self-study><normal-distribution><chi-squared-test><goodness-of-fit>,CC BY-SA 4.0,"<p>I have task about hearing ability and here is frequency table</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">bad ability</th>
<th style=""text-align: center;"">average</th>
<th style=""text-align: center;"">good ability</th>
<th style=""text-align: center;"">total</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">19</td>
<td style=""text-align: center;"">64</td>
<td style=""text-align: center;"">17</td>
<td style=""text-align: center;"">100</td>
</tr>
</tbody>
</table>
</div>
<p>It is said that average ability is everything +/- 1sd. This is all data I have. I need to make hypotisses about normal distrubution and do normality distribution test. I am not sure what test?
I tried chi squared:</p>
<ul>
<li>Ho: distribution is normal</li>
<li>Ha: distribution is not normal</li>
</ul>
<p>I know that normal distribution has this characteristic</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">left tail</th>
<th style=""text-align: center;"">μ ± 1σ</th>
<th style=""text-align: center;"">right tail</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">15.85%</td>
<td style=""text-align: center;"">68.3%</td>
<td style=""text-align: center;"">15.85%</td>
</tr>
</tbody>
</table>
</div>
<p>So I have decided to do chi square test because I can compare observed and expected frequency.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>bad ability</th>
<th>average</th>
<th>good ability</th>
<th>total</th>
</tr>
</thead>
<tbody>
<tr>
<td>observed</td>
<td>19</td>
<td>64</td>
<td>17</td>
<td>100</td>
</tr>
<tr>
<td>expected</td>
<td>15.85</td>
<td>68.3</td>
<td>15.85</td>
<td></td>
</tr>
<tr>
<td>(obs-exp)^2/exp</td>
<td>0.626</td>
<td>0.271</td>
<td>0.0834</td>
<td>0.98</td>
</tr>
</tbody>
</table>
</div><p>So I got &#x1D6D8;<sup>2</sup> = 0.98 2<sub>crit, .025</sub> = 7.38 and I don't reject Ho.</p>
<p>But, I am nut sure in df. I have used 3-1 = 2 df and &#x1D6D8;<sup>2</sup> distibution table. Looking great 
<p><a href=""https://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/chi-square-test-for-normality/comment-page-1/#comment-1427900"" rel=""nofollow noreferrer"">Real Statistics in Excel</a></p>
<p>I have seen that maybe I should calculate df = 3-1-2 = 0, but that obviously isn't good result.</p></p>
<p>And I am not sure if should use &#x1D6D8;<sup>2</sup><sub>crit, .05</sub> = 5.99. Maybe this critical value is for one-sided test.</p>
<p>So I am suspicious about my test. And IComments and answers are welcome. Thanks</p>
",2020-12-30 17:56:17.277
333815,20286.0,2,,333689.0,,,,CC BY-SA 4.0,"<p>Most of what's going on here is the same for any type of regression involving categorical predictors having more than 2 levels. Some is specific to Cox proportional-hazards regressions, noted at the end.</p>
<p>With treatment coding of predictors, the default in R, the regression coefficient for each predictor represents the associated <em>difference</em> from a baseline. In ordinary least squares, that baseline is the intercept, the estimated mean value when continuous predictors have values of 0 and categorical predictors are at their reference values.</p>
<p>In a Cox regression, the baseline is an empirical baseline (log)hazard as a function of time. That is the &quot;reference level's association with outcome/time&quot; you seek. The predictor coefficients in Cox regression are the differences from that in log-hazard.</p>
<p>Although you say &quot;In regression I normally think of whole variates, not different levels, as having a regression coefficient,&quot; that's not correct for a multi-level categorical predictor. A categorical predictor with <span class=""math-container"">$k$</span> levels is treated as a set of <span class=""math-container"">$k-1$</span> predictors, so you get that many coefficients. Under treatment coding, each coefficient represents the difference of a level from the reference level. If you have an ordinal predictor, in R you can specify it as <code>ordered</code> and the internal coding will be changed from treatment coding to a <a href=""https://stackoverflow.com/q/41943789/5044791"">polynomial-contrast coding</a> that respects the ordering of the levels. Again, that's true whether you are doing OLS, Cox regressions, or generalized linear modeling.</p>
<p>So, when you ask:</p>
<blockquote>
<p>Does that mean that here each level of a variate has its own regression coefficient which is independent of all levels apart from the reference level? and if it is significant it can be interpreted as having an association with the outcome/time (=hazard) that is relative to the reference level's association with the outcome/time?</p>
</blockquote>
<p>the answer under treatment coding is almost &quot;Yes&quot;: except that the coefficient estimates might have covariances that need to be taken into account in testing and obtaining confidence intervals for predictions.</p>
<p><a href=""https://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr"">This answer</a> shows how to use likelihood-ratio tests to estimate the <em>overall</em> association of a multi-level categorical predictor with outcome. An alternative, implemented in the R <code>rms</code> package, is to use <a href=""https://en.wikipedia.org/wiki/Wald_test#Test(s)_on_multiple_parameters"" rel=""nofollow noreferrer"">Wald tests</a> to evaluate overall associations of a multi-level predictor, including its interaction terms, with outcome.</p>
<p>There is no problem in general for doing such analyses on time-stratified survival data. The problem is that you specify an interaction between the time strata and the breast-cancer types with <code>:</code>, without a main-effect predictor for the breast cancer types. I understand that the example in the <code>survival</code> package documentation also does that, but it can be prone to error.</p>
<p>As you found, you get an error when you try to perform <code>anova()</code> on such a model; the <code>rms</code> package won't even accept that coding. Try with the <code>*</code>  syntax, which provides the main effects as well as the interaction terms. Here's an example:</p>
<pre><code>&gt; library(rms) # also loads survival package and veteran dataset
&gt; vet2 &lt;- survSplit(Surv(time, status) ~ ., data= veteran, cut=c(90, 180), episode= &quot;tgroup&quot;, id=&quot;id&quot;)
&gt; vfit2 &lt;- cph(Surv(tstart, time, status) ~ trt + prior + karno*strat(tgroup), data=vet2)
&gt; anova(vfit2)
                Wald Statistics          Response: Surv(tstart, time, status) 

 Factor                                        Chi-Square d.f. P     
 trt                                            0.00      1    0.9535
 prior                                          0.09      1    0.7642
 karno  (Factor+Higher Order Factors)          62.07      3    &lt;.0001
  All Interactions                             18.86      2    0.0001
 karno * tgroup  (Factor+Higher Order Factors) 18.86      2    0.0001
 TOTAL                                         63.70      5    &lt;.0001
</code></pre>
",2020-12-30 17:57:03.643
333816,240922.0,2,,333731.0,,,,CC BY-SA 4.0,"<p>What the the Hausman test is doing is testing whether the results (i.e. the estimated coefficients) from a fixed effects and random effects model are significantly different.</p>
<p>(I haven't ever seen people talk about it testing whether whether the fixed and &quot;between&quot; effects are different, although because the random effects model is estimated as a weighted average of the between and fixed effects models, you could look at it that way, I suppose.)</p>
<p>However, the reason it's used to &quot;decide between fixed vs random effects&quot; is that if the results of the two models ARE different then that's a reason to prefer the fixed effects model.</p>
<p>Here's why.</p>
<p>A fixed effects (FE) model accounts for ALL omitted variable bias from variables at the higher &quot;group&quot; level, because a fixed effects model is basically just including a dummy indicator variable for each &quot;group.&quot; This means that if you run a FE model you don't have to worry about omitted variable bias at the group level.</p>
<p>A random effects (RE) model does NOT automatically account for all group level  bias (that is, bias due to differences &quot;between groups&quot;), although it allows you to include group level predictors (that is, predictor variables that vary ONLY between groups, but not within groups) that do that. Now, if you were to include ALL significant group level covariates in your random effects model then you would get the same answers (in terms of lower level coefficients) as the FE model (or rather, the two answers will approach each other as sample sizes goes to infinity). But the standard errors of the RE model will be smaller.</p>
<p>What all that means is that if the coefficients of a FE and RE model are basically the same, you should prefer the RE ones, because they have smaller standard errors, but if they are different you should prefer the FE ones because the fact that they are different suggests that there is omitted variable bias at the higher level that the RE model has not accounted for (but the FE model has). So that's why we use a Hausman test to see if the coefficients are different, and if the test says they are, we default to the FE model.</p>
<p>Now, I'll note that in &quot;real life&quot; there are many other criteria by which we might choose between these two models. Sometimes the independent variables we care about are at the higher level in which case we CAN'T use a FE model, since in a FE model you can't include any group-level predictors (because they are colinear with the dummy variables). So a Hausman test is only appropriate when you have other substantive reason to prefer one model over the other.</p>
<p>And I'll also note that in my experience the Hausman test is almost always significant: that is, it almost always tells you that the two models are significantly different even when that difference is so small as to be substantively meaningless. So, as is always the case in statistics: don't just blindly follow the results of the test.</p>
",2020-12-30 18:08:30.343
333841,79126.0,2,,333678.0,,,,CC BY-SA 4.0,"<p>There is nothing wrong with this and nothing you can do. Compared to a 400-400 split, you have less precision; compared to a 200-200 split, you have more precision; compared to a 300-300 split, you have less precision. But it doesn't matter; the data you have is the data you have. No weighting or sampling can change that. You can increase precision by including covariates in the estimation of the effect using the techniques recommended by <a href=""https://doi.org/10.1002/sim.6507"" rel=""nofollow noreferrer"">Culantuouni and Rosenblum (2015)</a>. It is rarely the case that all binary predictors in an analysis are exactly 50-50 distributed. It's true that among all possible allocations, an even split yields the best precision (all else equal), but that doesn't mean you have to have it that way. If an event is rarer in the experimental group than in the placebo, then you increase your ability to detect effects by allocating more participants to that group, which was probably the motivation here.</p>
",2020-12-31 04:16:01.183
333817,80394.0,2,,332131.0,,,,CC BY-SA 4.0,"<p>Thank you for adding the data. I tried <code>rstanarm::stan_polr</code> only to find the coefficients of three of the five predictors to be 0.0. Then I thought of a scale effect and apparently the large values in IV5 are the problem:</p>
<pre><code>&gt; summary(df)
 DV          IV1              IV2             IV3             IV4             IV5      
 1: 3   Min.   : 5.222   Min.   :3.733   Min.   :12.67   Min.   :21.68   Min.   :1651  
 2:12   1st Qu.:11.446   1st Qu.:4.692   1st Qu.:16.65   1st Qu.:28.47   1st Qu.:2885  
 3:19   Median :16.309   Median :4.860   Median :17.61   Median :30.83   Median :4563  
 4:12   Mean   :18.254   Mean   :4.878   Mean   :17.35   Mean   :31.46   Mean   :4592  
        3rd Qu.:23.197   3rd Qu.:5.101   3rd Qu.:18.11   3rd Qu.:34.16   3rd Qu.:5990  
        Max.   :53.505   Max.   :5.609   Max.   :19.82   Max.   :46.66   Max.   :8377  
</code></pre>
<p>If you scale the values of IV5 the functions runs with no problems:</p>
<pre><code>&gt; summary(polr(DV ~ IV1 + IV2 + IV3 + IV4 + scale(IV5), data = df, Hess=TRUE))
Call:
polr(formula = DV ~ IV1 + IV2 + IV3 + IV4 + scale(IV5), data = df, 
    Hess = TRUE)

Coefficients:
              Value Std. Error t value
IV1        -0.06206    0.03176 -1.9539
IV2        -0.85094    1.02221 -0.8325
IV3        -0.08293    0.22607 -0.3668
IV4         0.02918    0.05853  0.4986
scale(IV5) -0.94037    0.32998 -2.8498

Intercepts:
    Value   Std. Error t value
1|2 -8.8255  4.9492    -1.7832
2|3 -6.6140  4.8206    -1.3720
3|4 -4.5695  4.7956    -0.9529

Residual Deviance: 104.1743 
AIC: 120.1743 
</code></pre>
<p>Simply dividing <code>IV5</code> values by 1000 works as well:</p>
<pre><code>&gt; summary(polr(DV ~ IV1 + IV2 + IV3 + IV4 + I(IV5/1000), data = df, Hess=TRUE))
Call:
polr(formula = DV ~ IV1 + IV2 + IV3 + IV4 + I(IV5/1000), data = df, 
    Hess = TRUE)

Coefficients:
               Value Std. Error t value
IV1         -0.06205    0.03176 -1.9536
IV2         -0.84851    1.02206 -0.8302
IV3         -0.08289    0.22606 -0.3667
IV4          0.02918    0.05853  0.4985
I(IV5/1000) -0.49940    0.17530 -2.8488

Intercepts:
    Value    Std. Error t value 
1|2 -11.1059   5.2975    -2.0964
2|3  -8.8946   5.1598    -1.7238
3|4  -6.8500   5.1160    -1.3389

Residual Deviance: 104.1744 
AIC: 120.1744 
</code></pre>
",2020-12-30 18:12:26.007
333819,166693.0,4,,,,,,CC BY-SA 4.0,Latin hypercubes are stratified random samples of multi-dimensional space such that all strata of each marginal dimension are sampled exactly once.,2020-12-30 18:43:55.477
333818,166693.0,5,,,,,,CC BY-SA 4.0,"<p><strong>References</strong>:</p>
<ul>
<li>McKay, M.D.; Beckman, R.J.; Conover, W.J. (May 1979). &quot;A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code&quot;. Technometrics. 21 (2): 239–245.</li>
<li>Stein, M. (May 1987). &quot;Large Sample Properties of Simulations Using Latin Hypercube Sampling&quot;  Technometrics. 29 (1): 143:151.</li>
<li>Tang, B. (Dec 1993). &quot;Orthogonal Array-Based Latin Hypercubes&quot;. Journal of the American Statistical Association. 88 (424): 1392–1397.</li>
<li>Owen, A.B. (1992). &quot;Orthogonal arrays for computer experiments, integration and visualization&quot;. Statistica Sinica. 2: 439–452.</li>
</ul>
",2020-12-30 18:43:55.477
333820,170017.0,2,,333537.0,,,,CC BY-SA 4.0,"<p>If the order is random, then there are 8! different permutations that are equal likely.  However, we can partition those 8! permutations by considering what position the first member of each couple is in the order of choosing. There are only 14 choices for the locations of the first member of each couple:<br />
1, 2, 3, 4<br />
1, 2, 3, 5<br />
1, 2, 3, 6<br />
1, 2, 3, 7<br />
1, 2, 4, 5<br />
1, 2, 4, 6<br />
1, 2, 4, 7<br />
1, 2, 5, 6<br />
1, 2, 5, 7<br />
1, 3, 4, 5<br />
1, 3, 4, 6<br />
1, 3, 4, 7<br />
1, 3, 5, 6<br />
1, 3, 5, 7</p>
<p>The solution comes down to answering these questions:</p>
<ol>
<li>How many of the 8! permutations fall into each of those 14 partitions?<br />
and</li>
<li>What is the probability of a perfect pairing for couples within each partition?</li>
</ol>
<p>These are pretty simple counting problems.</p>
<p>For example, look at the case where the first member of each couple is in positions<br />
1, 2, 4, 5.
There are 8 choices for the person in the first position, 6 choices for the second position, 4 choices for the fourth position, 2 for the fifth.  Then, there are only 4 people remaining to place in positions 3, 6, 7, and 8.  But, in position 3 only the partner of the person in position 1 or position 2 can go there. So, there are two choices. Then, 3 for position 6; 2 for position 7; 1 for position 8. In all, 8<em>6</em>4<em>2</em>2<em>3</em>2*1.</p>
<p>In general, there are always 8<em>6</em>4*2 ways to fill the positions of the first members of each couple. Then, for each of a remaining position, <span class=""math-container"">$k$</span>, (a position for the second member of each couple), the number of choices is the number of first positions prior to <span class=""math-container"">$k$</span> minus the number of second positions prior to <span class=""math-container"">$k$</span>.</p>
<p>The probability of a perfect pairing for couples given the positions of the first member of each couple is also a fairly easy problem. Again, use the example where the first positions are 1, 2, 4, 5.  The probability the person in position 1 chooses their partner is <span class=""math-container"">$1/7$</span>.  The probability the second person chooses their partner given the first did is <span class=""math-container"">$1/6$</span> (they can't choose themselves and their partner was already chosen, so 6 choices remain).  The probability the third person chooses their partner given the first two did is <span class=""math-container"">$1/6$</span>. The probability the fourth chooses their partner given the first four did is <span class=""math-container"">$1/4$</span>. Fifth person: <span class=""math-container"">$1/3$</span>; sixth person: <span class=""math-container"">$1/3$</span>; seventh person: <span class=""math-container"">$1/2$</span>; In general, a first person from a partnership in position <span class=""math-container"">$k$</span> has <span class=""math-container"">$8-k+1$</span> names in the bowl to choose from, but one of them is their own name, so the probability they choose their partner given everyone prior has chosen their partner is <span class=""math-container"">$1/(8-k)$</span>.  For someone who is the second person from their partnership and in position <span class=""math-container"">$k$</span>, all names in the bowl are valid choices, so the probability of choosing their partner is <span class=""math-container"">$1/(8-k+1)$</span>.</p>
<p>The final probability is<br />
<span class=""math-container"">$$\frac{1}{8!} [ \left( \text{number of ways to have first members in positions 1, 2, 3, 4} \right) *\left( P[\text{perfect couple pairing given first positions }1, 2, 3, 4] \right)+...$$</span>
<span class=""math-container"">$$+  \left( \text{number of ways to have first members in positions 1, 3, 5, 7} \right) *\left( P[\text{perfect couple pairing given first positions }1, 3, 5, 7] \right) ]$$</span>
<span class=""math-container"">$$\approx5.95 \times 10^{-5} \approx 1.2008 \times \frac{1}{4 \times 7!}$$</span></p>
<p>In the general case with <span class=""math-container"">$n$</span> couples and <span class=""math-container"">$2n$</span> people total, the number of ways to position the first member of each couple is given by the Catalan number
<span class=""math-container"">$$\frac{1}{n+1} {{2n}\choose{n}} $$</span>
With <span class=""math-container"">$n$</span> large (I've tried up to 10,000), it appears that the probability gets close to
<span class=""math-container"">$$1.359 \times \frac{1}{n \times (2n-1)!}$$</span></p>
<p>R code:</p>
<pre><code>#exact calculation by enumerating all possible choices
n=4
firstpositions=c(1:n)
done=FALSE
cnt=factorial(n)
probperfectpair=1/(factorial(2*n-1)*n)
while (!done) {
  k=n
  while (firstpositions[k]==(2*k-1)) k=k-1
  if (k==n) firstpositions[k]=firstpositions[k]+1 else {
    firstpositions[k:n]=firstpositions[k]+c(1:(n-k+1))
  }
  if (sum(firstpositions==(2*c(1:n)-1))==n) done=T
  print(firstpositions)
  secondpositions=setdiff(1:(2*n),firstpositions)
  probperfectpair=c(probperfectpair,1/(prod(2*n-firstpositions)*prod(2*n-secondpositions+1)))
  cnt1=sum(firstpositions&lt;secondpositions[1])
  for (i in 2:(n-1)) cnt1=cnt1*(sum(firstpositions&lt;secondpositions[i])-(i-1))
  cnt=c(cnt,cnt1)
}
sum(cnt*probperfectpair)/sum(cnt)                        #actual probability
(sum(cnt*probperfectpair)/sum(cnt))/probperfectpair[1]   #probability normalized by 
                                                         #1/(factorial(2*n-1)*n) 



#estimated &quot;normalized&quot; probability and standard error using simulations
estprob=function(n,nsim=1000000) {
  lf=lfactorial(2*n-1)+log(n)
  estp=rep(0,nsim)
  firstpositions=secondpositions=c(1:n)
  for (isim in 1:nsim) {
    rs=sample(2*n,2*n)
    i=1
    if ((rs[1] %% 2)==1) j=match(rs[1]+1,rs) else j=match(rs[1]-1,rs)
    rs[i]=rs[j]=0
    secondpositions[1]=j
    for (i1 in 2:n) {
      i=match(T,rs&gt;0)
      firstpositions[i1]=i
      if ((rs[i] %% 2)==1) j=match(rs[i]+1,rs) else j=match(rs[i]-1,rs)
      rs[i]=rs[j]=0
      secondpositions[i1]=j
    }
    estp[isim]=exp(lf-sum(log(2*n-firstpositions))-sum(log(2*n-secondpositions+1)))
  }
  return(c(mean(estp),sqrt(var(estp)/nsim)))
}
estprob(4,nsim=10000)
<span class=""math-container"">```</span>
</code></pre>
",2020-12-30 18:51:40.697
333822,0.0,5,,,,,,CC BY-SA 4.0,,2020-12-30 18:53:33.120
333821,166693.0,4,,,,,,CC BY-SA 4.0,"The process of taking the difference between two values to achieve desirable statistical properties.  e.g. paired differences t-test, differences between time points in an AR(1) model, and difference-in-differences causal impact analysis.",2020-12-30 18:53:33.120
333823,82120.0,1,,,,Hidden Markov Models and How to Interpret Probability of the Overall Sequence?,<r><hidden-markov-model>,CC BY-SA 4.0,"<p>What can I do to improve the probability of a sequence given my data?</p>
<p>Consider the following MWE:</p>
<pre><code># Initialise HMM
hmm = initHMM(c(&quot;A&quot;,&quot;B&quot;), c(&quot;L&quot;,&quot;R&quot;), transProbs=matrix(c(.8,.2,.2,.8),2),
              emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)
# Sequence of observations
observations = c(&quot;L&quot;,&quot;L&quot;,&quot;R&quot;,&quot;R&quot;)
# forward
logForwardProbabilities = forward(hmm,observations)
forwardProbabilities = exp(logForwardProbabilities)
# probability of sequence
sum(forwardProbabilities[,4])
# 0.058276
</code></pre>
<p>The probability of the overall sequence here is very low, at .05. Should I then be very distrusting of this model given that the probability is so low?</p>
<p>Separately, to represent the confidence of a predicted state at any time step, is it fair to use the posterior (see below)?</p>
<pre><code># Initialise HMM
hmm = initHMM(c(&quot;A&quot;,&quot;B&quot;), c(&quot;L&quot;,&quot;R&quot;), transProbs=matrix(c(.8,.2,.2,.8),2),
              emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)
# Sequence of observations
observations = c(&quot;L&quot;,&quot;L&quot;,&quot;R&quot;,&quot;R&quot;)
# Calculate posterior probablities of the states
posterior = posterior(hmm,observations)
print(posterior)

      index
states         1       2       3         4
     A 0.6037344 0.56639 0.43361 0.3962656
     B 0.3962656 0.43361 0.56639 0.6037344
</code></pre>
<p>The overall sequence predicted probability is very low, but the model strongly believes that at the last time step, 4, there is a 60% chance it is at state B, as compared to state A.</p>
",2020-12-30 18:54:49.237
333824,255181.0,1,333825.0,,,How to choose hypothesis equation for logistic regression?,<regression><machine-learning><logistic>,CC BY-SA 4.0,"<p>For Simple Linear/Polynomial regression, we can choose hypothesis equation based on data shape, eg degree-1 for linear data shape, degree-2 for parabolic data shape etc.</p>
<p>But how do we choose the hypothesis equation degree for logistic regression?
Is it also based on data shape?</p>
",2020-12-30 19:00:12.470
333825,75350.0,2,,333824.0,,,,CC BY-SA 4.0,"<p>Looking at the data to determine what the functional form of the conditional mean should be is an example of <a href=""https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data"" rel=""nofollow noreferrer"">testing hypothesis suggested by the data</a>.  If your goal is inference of a particular type, you should be going into the experiment with an idea of what the functional form should look like and not consult the data.</p>
<p>If instead your goal is to build a predictive model, for example, you might benefit from using something like a spline rather than specifying polynomial terms in your fit.  Splines allow for a much wider class of functions to be fit as compared to polynomials, and if you use something like a natural spline (a.k.a. restricted cubic spline) then the estimates are linear in the tails of the predictor.</p>
",2020-12-30 19:14:08.013
333826,20286.0,2,,333814.0,,,,CC BY-SA 4.0,"<p>Your choice of a chi-square test is standard for this type of problem. With respect to the number of degrees of freedom (df), the <a href=""https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Definition"" rel=""nofollow noreferrer"">Wikipedia page</a> provides guidance for a goodness-of-fit test:</p>
<blockquote>
<p>For a test of goodness-of-fit, df = Cats − Parms, where Cats is the number of observation categories recognized by the model, and Parms is the number of parameters in the model adjusted to make the model best fit the observations: The number of categories reduced by the number of fitted parameters in the distribution.</p>
</blockquote>
<p>You have 3 categories, so that part is easy. It doesn't seem that you have any parameters that you &quot;adjusted to make the model best fit the observations.&quot;* With these tests, however, you always need to reduce the df by 1 because the total of observations must equal the sum of the individual categories; see the Wikipedia entry on a similar test for the <a href=""https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Discrete_uniform_distribution"" rel=""nofollow noreferrer"">uniform distribution</a>.</p>
<p>Unlike the t-test, the chi-square test is inherently <a href=""https://en.wikipedia.org/wiki/One-_and_two-tailed_tests#Applications"" rel=""nofollow noreferrer"">one-sided</a>.</p>
<hr />
<p>*As I read the problem, the SD of the population used to classify into &quot;bad,&quot; &quot;average&quot; or &quot;good&quot; ability was determined independently of this particular data sample. Otherwise you would have to correct for the 2 extra parameters estimated from the data (mean and standard deviation), leading to 0 degrees of freedom! Sometimes a simple homework question like this hides some important hidden assumptions, from which you can learn even more.</p>
",2020-12-30 19:24:09.503
333827,254634.0,2,,120364.0,,,,CC BY-SA 4.0,"<p>The way I understand this passage is as follows.</p>
<p>Assume the error function <span class=""math-container"">$E({\bf w})$</span> could be approximated well by a quadratic function in <span class=""math-container"">${\bf w}$</span> <span class=""math-container"">$${\bf w^Tb} +\frac{1}{2} {\bf w^T H w}.$$</span> The minimum of such function would be uniquely determined by knowing all the coefficients contained in <span class=""math-container"">${\bf b, H}$</span>, of which there are <span class=""math-container"">$W(W+3)/2=O(W^2)$</span> of those.</p>
<p>Now we have two ways of collecting <span class=""math-container"">$O(W^2)$</span> &quot;pieces of information&quot; to retrieve the <span class=""math-container"">$O(W^2)$</span> coefficients of the error function.</p>
<p>We could just do <span class=""math-container"">$O(W^2)$</span> point evaluations of <span class=""math-container"">$E({\bf w})$</span> at different <span class=""math-container"">${\bf w}$</span>. Each point evaluation further requires <span class=""math-container"">$O(W)$</span> computations which could be seen like this. Both in the sum-of-squares and the cross-entropy error functions, you need to perform <span class=""math-container"">$N$</span> evaluations (for each term in the sum) of an expression involving <span class=""math-container"">$y_n({\bf w,x})$</span>, and evaluating <span class=""math-container"">$y_n({\bf w,x})$</span> is <span class=""math-container"">$O(W)$</span>. Assuming that the data set is fixed, <span class=""math-container"">$N$</span> is just a constant, so evaluating <span class=""math-container"">$E({\bf w})$</span> at a point is <span class=""math-container"">$N \cdot O(W)=O(W)$</span>. So altogether to retrieve all the coefficients one needs <span class=""math-container"">$O(W^2) \cdot O(W) = O(W^3)$</span> computations.</p>
<p>Another way would be to collect <span class=""math-container"">$W$</span> units of information at a time by computing the gradient <em>vector</em> <span class=""math-container"">$\nabla E$</span> (instead of a point evaluation which gives one number, one unit of information). So to collect <span class=""math-container"">$O(W^2)$</span> pieces of information we need to restore the function, this time we need <span class=""math-container"">$O(W)$</span> evaluations of the gradient vector since <span class=""math-container"">$W \cdot O(W) = O(W^2)$</span>. The interesting part is that backpropagation allows to compute the gradient vector still in <span class=""math-container"">$O(W)$</span> computations (intuitively, one may have though that if we computed one number in <span class=""math-container"">$O(W)$</span>, we'd compute a vector of length <span class=""math-container"">$W$</span> in <span class=""math-container"">$O(W^2)$</span> steps; but this is the magic of backpropagation). So now miraculously, using <span class=""math-container"">$O(W)$</span> computations you are computing a vector <span class=""math-container"">$\nabla E$</span> instead of a number <span class=""math-container"">$E({\bf w})$</span>, and to collect <span class=""math-container"">$O(W^2)$</span> information you perform <span class=""math-container"">$O(W)$</span> computations instead of <span class=""math-container"">$O(W^2)$</span>. Each computation is still <span class=""math-container"">$O(W)$</span>, so the total cost is now <span class=""math-container"">$O(W^2)$</span> computations.</p>
",2020-12-30 19:43:06.260
333828,240043.0,2,,333796.0,Daniel R. Collins,,,CC BY-SA 4.0,"<p>If the trials are being tested with a variety of statistical methods, and only the &quot;successful&quot; ones publicized, then the primary term for this would be <strong>data dredging</strong>. From <a href=""https://en.wikipedia.org/wiki/Data_dredging"" rel=""nofollow noreferrer"">Wikipedia</a>:</p>
<blockquote>
<p><strong>Data dredging</strong> (also <strong>data fishing, data snooping, data butchery</strong>, and <strong>p-hacking</strong>) is the misuse of data analysis to find
patterns in data that can be presented as statistically significant,
thus dramatically increasing and understating the risk of false
positives. This is done by performing many statistical tests on the
data and only reporting those that come back with significant results.</p>
</blockquote>
<p>On the other hand, if only a single statistical test is being applied, and the failed trials literally discarded, then &quot;cherry picking&quot; (from another answer) would be the better term.</p>
",2020-12-30 19:55:32.533
333829,254769.0,1,,,,Log-Log regression not giving me elasticity,<regression><multiple-regression><econometrics><logarithm><elasticity>,CC BY-SA 4.0,"<p>Ive got a regression in the form log(y) = a +b1x1 + b2x1^2 + b3x2 + b4x3 + b5* log(x4).</p>
<p>I've interpreted my coefficients as they should be interpreted.I figured out the quadratic and e^b3 and e^b4 are percent changes in y for unit changes in x. I even tested this by changing x by a unit holding all else constant in both cases and saw it was true.</p>
<p>However, for x4 I know b5 should be the elasticity that is (dx4/x4)/(dy/y) however I am not seeing that result when plugging in new values of x4. For example, I found y using all variable averages, and then did the same but timesed x4 by 2: log(2*x4)(kept everything else constant). I expected the percent change in y to be that of b5. (As dx/x) = 1. Instead, I got a number far below the elasticity.</p>
<p>I did the same for a 1% change in x(of course this time dx/x was 0.01 so I had to divide through it) and I received a number much closer to b4 but still not exactly it.
Isn't the partial derivative of y with respect to x4 in a log-log function meant to give you elasticity  (dy/dx4)* (x4/y) and wouldn't this elasticity be constant?</p>
<p>Why is it not? Thanks.</p>
",2020-12-30 19:56:45.640
333830,255278.0,1,,,,"A variable composed of standardized scores does not necessarily have M=0, SD=1?",<standardization>,CC BY-SA 4.0,"<p>If you standardize scores using 10 cases(group 1), and standardize scores using another 10 cases (group 2), you select 5 scores from each group, and compose a variable. Does this variable have mean equal 0, sd=1?</p>
",2020-12-30 19:57:14.630
333831,147017.0,1,,,,Linear least-square fitting of two variables with uncertainty on both,<r><regression><correlation><bivariate><errors-in-variables>,CC BY-SA 4.0,"<p>I am trying to find an R function to calculate the linear least-square fitting of two variables when both have an error (expressed as standard deviation). I have found this problem referred to in half a dozen different ways: Williamson-York method, RMA, Deming regression, bivariate fit, weighted least squares, etc...</p>
<p>I am no statistician, so this may seem like a stupid question, but are all these names referring to the same methodology? And what is the correct name for it if one needs to look for information?</p>
<p>I need to use the procedure described in this paper: <a href=""https://acp.copernicus.org/articles/8/5477/2008/acp-8-5477-2008.pdf"" rel=""nofollow noreferrer"">https://acp.copernicus.org/articles/8/5477/2008/acp-8-5477-2008.pdf</a>. The assumptions are that the distribution of errors is normal and and the fit parameters do not depend on the choice of units. The R <code>deming</code> package seems to give the same results as in the paper, at least on the example dataset, so I think it is okay. But it does not return a Pearson correlation coefficient, like <code>lm</code> does.</p>
<p>Is it that this type of regression cannot calculate the correlation coefficient or is it just the particular implementation of the <code>deming</code> package that does not?</p>
<p><strong>EDIT:</strong> in a related paper, I found reference to this C function <a href=""http://numerical.recipes/webnotes/nr3web19.pdf"" rel=""nofollow noreferrer"">http://numerical.recipes/webnotes/nr3web19.pdf</a>. It seems to be doing what I want but it is unclear to me if it is the same fitting method.</p>
<p><strong>EDIT 2:</strong> specific questions:</p>
<ol>
<li>Are these terms referring to the same method: Williamson-York method, RMA, Deming regression, bivariate fit, weighted least squares?</li>
<li>It the methods are equivalent, which one is the correct term to use?</li>
<li>Are the methods described in the two linked pdf the same?</li>
<li>The R package <code>deming</code> implements a Deming regression. Why does it not return a correlation coefficient? Is it the method or its implementation in the package?</li>
</ol>
",2020-12-30 20:01:23.343
333832,55621.0,1,333882.0,,,Distributions in the Weibull max-domain of attraction,<extreme-value><weibull-distribution>,CC BY-SA 4.0,"<p>Can I please have a few examples of distributions that, when block-max sampled for extreme values, are in the max-domain of attraction of the Weibull distribution? I know the Beta distrution is, but any other examples?</p>
<p>Thank you,</p>
",2020-12-30 20:18:34.313
333833,255219.0,1,,,,why var of beta is not included in var of y in linear regression,<regression><variance>,CC BY-SA 4.0,"<p>I know as a known fact that in linear regression</p>
<p>y= X<em>beta +error, so var(y|X) = var(X</em>beta+error|X) = sigma^2, where sigma is the std. dev of errors.</p>
<p>But in regression we also have mean and variance of beta estimate?</p>
<p>counfused how they all played together. How come we consider beta to be fixed when getting variance of y?</p>
",2020-12-30 22:07:26.347
333834,255287.0,1,,,,goodness of fit for psychometric data (perceptual threshold),<hypothesis-testing><goodness-of-fit><threshold><psychology><audio>,CC BY-SA 4.0,"<p>I'm running an experiment on perceptual thresholds in audio. I'll try not to bog you down with too many details:</p>
<p>The experiment is about vibrato speed; specifically, when can you tell the difference between two stimuli that differ in <em>vibrato speed</em> only. Our software determines subjects' 50% threshold as a function of speed-difference; i.e., the difference in vibrato that is imperceivable.</p>
<p>It is run in 5 conditions, based on 5 &quot;base vibrato speeds&quot; (without getting into technical language, basically: slow, medium, fast (and steps in between)).</p>
<p>So, in the end our data look like this:</p>
<ul>
<li><p>Independent variable: &quot;base vibrato speed&quot; (slow, med, fast).</p>
</li>
<li><p>Dependent variable: largest imperceivable difference in vibrato speed (the 50% threshold as determined by the software/experiment).</p>
</li>
</ul>
<p>We hypothesize that as the base vibrato speed gets higher, the dependent variable (perceivable speed-difference) grows exponentially. Linear would be the null hypothesis.</p>
<p>So I'd like to fit both a linear regression, and some kind of exponential function to the data, and calculate a goodness of fit on both. Somehow I'm thinking chi-square won't cut the mustard here.</p>
<p>Do you have any thoughts?</p>
",2020-12-30 22:43:15.697
333835,58540.0,2,,333731.0,,,,CC BY-SA 4.0,"<p>Let's say the Hausman test favors the fixed effect model. Why might you still be interested in using the random effects model? The random effects or multilevel model allows a degree of flexibility in modeling that is much messier and in some cases impossible to implement in the fixed effect model. Examples include:</p>
<ol>
<li>Random slopes for the association between lower level variables and the outcome, which allow you to investigate whether the within-group association varies across groups. A classic example is growth curve modeling whereby the linear (or quadratic) rate of change in the outcome, codified by a continuous time variable) is allowed to vary across individuals or groups. In the fixed effect model, you can get at this in a crude way by interacting the lower-level variable with the cluster dummy variables. If you have a lot of groups, you have a lot of interactions.</li>
<li>Related to the above, random effect models allow for interactions between within- and between-level predictors, which are sometimes called cross-level interactions. To investigate these in a random effects models requires three parameters - a random slope for the lower-level predictor, the covariance between the random slope and random intercept, and the fixed effect term for the interaction. This is not possible to investigate in the fixed effects modeling framework because all cluster-level variables are thrown out.</li>
<li>Investigation of different within- and between- associations for lower-level variables. Here the interest is investigating whether a within-cluster association is different than the between-cluster association. This is sometimes termed contextual effects modeling and also allows you to identify cases of <a href=""https://en.wikipedia.org/wiki/Simpson%27s_paradox#:%7E:text=Simpson%27s%20paradox%2C%20which%20also%20goes,when%20these%20groups%20are%20combined."" rel=""nofollow noreferrer"">Simpson's paradox</a>. An example is the association between exercise and having a heart attack. While exercising, one is at higher risk for a heart attack, but on average, individuals who exercise a lot tend to have lower risk for a heart attack. Relatedly, adding in the cluster means of lower-level variables takes care of the endogeneity problem at level 1 often used as a reason to prefer fixed effects models.</li>
<li>Prediction - random effects models employ empirical Bayes prediction of the random intercepts and slopes. This is great because the procedure corrects for the reliability of the group's prediction. Smaller groups get <a href=""https://stats.stackexchange.com/questions/482555/single-observation-with-some-groups-multilevel-model-or-other-analysis/482562#482562"">pulled</a> toward the sample average prediction.</li>
</ol>
",2020-12-30 23:25:14.670
333836,62703.0,2,,333830.0,,,,CC BY-SA 4.0,"<p>It's very unlikely. Once the values are standardized so that mean=0 and sd=1 within a group, then they are only standardized within the context of that group. It'd be worth running this code a few times in R to see what happens in your situation:</p>
<pre><code>#Generate two groups of standardized data
group1 = rnorm(10,mean=0,sd=1)
group2 = rnorm(10,mean=0,sd=1)

#Choose the first five from both groups
group1_set = head(group1, 5)
group2_set = head(group2, 5)

#Create new variable
newgroup = c(group1_set, group2_set)

#Check whether mean is 0 and sd is 1
mean(newgroup)
sd(newgroup)
</code></pre>
",2020-12-31 00:25:54.127
333837,255298.0,2,,217992.0,,,,CC BY-SA 4.0,"<p>In Stata, one can use the &quot;rangestat&quot; function. Assume that we have daily stock returns for multiple firms for multiple years. We want to calculate first-order autocorrelations for each firm and for each one-year period. Assume that we have a variable &quot;group_variable&quot; to identify the firms and a &quot;year&quot; variable indicating the year for which the daily data exists. We calculate a lagged daily return variable and then use the &quot;rangestat&quot; function as follows:</p>
<pre><code>gen lag_var = var[_n-1]
rangestat (corr) var lag_var, by(group_variable) interval(year 0 0)
</code></pre>
<p>The by(group_variable) will not be needed if there is data on only one firm. For more details, first install the &quot;rangestat&quot; function in Stata and then see the help file.</p>
<pre><code>ssc install rangestat
help rangestat
</code></pre>
",2020-12-31 02:26:58.803
333838,106918.0,1,333845.0,,,Expected value of max of two discrete random variables,<expected-value><conditional-expectation><order-statistics><conditioning>,CC BY-SA 4.0,"<p>I'm reading this paper <a href=""https://arxiv.org/pdf/2006.12670.pdf"" rel=""nofollow noreferrer"">An Efficient PTAS for Stochastic Load Balancing with Poisson Jobs</a>. Which is solving a <a href=""https://en.wikipedia.org/wiki/Job_shop_scheduling"" rel=""nofollow noreferrer"">makespan</a> minimizing job-shop problem for Poisson job sizes. Basically, schedule the minimum work for random job sizes whose job size distributions are Poissonian. The authors give an equation that I hadn't seen before:</p>
<p><span class=""math-container"">$$\mathbb{E} \left[ \text{max}(X,Y) \right] = 
\sum_{x=0}^\infty \Pr\{X=x\} \left\{ x + \sum_{y=x+1}^\infty \Pr\{Y \geq y\} \right\}, $$</span>
where <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> are both random (independent) variables on support <span class=""math-container"">$\{0,1,\dots\}$</span>.</p>
<p>I can think of one general approach (conditional expectations) that seems reasonable to derive this but I'm not able to get the result.</p>
<p>Can someone derive this result?</p>
<p>Note: I'll accept any derivation, the approach need not use conditional expectations. I only mention because it seems like this is a conditional expectation identity.</p>
",2020-12-31 02:48:47.840
333839,255299.0,1,,,,Confidence interval for median - which is more appropriate bootstrap or binom/exact/SAS method? Why are results different?,<r><probability><confidence-interval><bootstrap><median>,CC BY-SA 4.0,"<p>I am unsure which method to use for calculating confidence interval for median values. I know the data set is small (n = 30).
I've read these discussions which suggest both &quot;bootstrap&quot; and &quot;binomial/SAS/exact&quot; methods are feasible:</p>
<ul>
<li><a href=""https://stats.stackexchange.com/questions/122001/confidence-intervals-for-median"">Confidence intervals for median</a></li>
<li><a href=""https://stats.stackexchange.com/questions/21103/confidence-interval-for-median"">Confidence interval for median</a></li>
<li><a href=""https://statisticsbyjim.com/hypothesis-testing/bootstrapping/"" rel=""nofollow noreferrer"">https://statisticsbyjim.com/hypothesis-testing/bootstrapping/</a></li>
</ul>
<p>Can anyone provide insights on which one is more appropriate for the example data set and also why the CI ranges for the median is so different between the two methods for these two data sets I have below using the &quot;DescTools&quot; library in R? I have other data sets (example data3) where the results between the two methods are similar.</p>
<pre><code>library(DescTools)
data1 = c(8,    7,  8,  9.5,    1,  20, 8,  7.5,    3,  20.5,   2.5,    5.5,    15.5,   2,  4,  1,
          17,   2,  3.5,    8.5,    8.5,    2.5,    11, 4,  10.5,   7.5,    12, 5,  16.5,   8.5)
data2 = c(7.1,  32.0,   3.8,    1.6,    19.6,   6.0,    7.2,    14.9,   0,  2.0,    5.7,
          19.4, 13.1,   15.5,   11.3,   9.6,    13.9,   5.6,    12.6,   1.0,    1.9,
          8.1,  15.9,   0.8,    6.1,    8.1,    18.0,   4.6,    5.5,    15.6)

data3 = c(16.1, 10.4,   0.5,    12.2,   7.2,    1.7,    21.6,   6.3,    0.8,    3.2,    12.6,   20.0,   3.4, 7.3,   3.5,
          7.5,  15.8, 4.7, 8.3, 11.9,   1.6,    9.0, 8.6,   11.7,   8.1, 5.8, 3.3,  7.9,    7.0,    8.5)


medianCI_Bootstrap_dF1 = MedianCI(data1, na.rm = TRUE, method = &quot;boot&quot;)
medianCI_Binom_dF1 = MedianCI(data1, na.rm = TRUE, method = &quot;exact&quot;)
medianCI_Bootstrap_dF1 
medianCI_Binom_dF1 

medianCI_Bootstrap_dF2 = MedianCI(data2, na.rm = TRUE, method = &quot;boot&quot;)
medianCI_Binom_dF2 = MedianCI(data2, na.rm = TRUE, method = &quot;exact&quot;)
medianCI_Bootstrap_dF2 
medianCI_Binom_dF2

medianCI_Bootstrap_dF3 = MedianCI(data3, na.rm = TRUE, method = &quot;boot&quot;)
medianCI_Binom_dF3 = MedianCI(data3, na.rm = TRUE, method = &quot;exact&quot;)
medianCI_Bootstrap_dF3 
medianCI_Binom_dF3
</code></pre>
<p><a href=""https://i.stack.imgur.com/iVnfe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iVnfe.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/fs1R2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fs1R2.png"" alt="" "" /></a></p>
",2020-12-31 03:02:51.713
333840,185306.0,2,,333634.0,,,,CC BY-SA 4.0,"<p>The absolute values in the confusion matrices suggest that you've resampled the data before doing the cross-validation splits. That's generally a big no-no, as your scores aren't representative of model performance on real representative data.</p>
<p>The first six score columns are all based on the confusion matrix, and so depend on a cutoff/threshold probability (presumably 0.5).  Resampling the data largely <a href=""https://stats.stackexchange.com/q/67903/232706"">just shifts the predicted probabilities</a>, so that this cutoff is effectively rather different.  For the same reason, the ROC curve is relatively unchanged, because it depends only on the ordering of the predicted probabilities.  For the PR curve, resampling does have a significant effect (for the rare positive class), see <a href=""https://datascience.stackexchange.com/a/66376/55122"">my answer on a DS.SE question</a>.</p>
",2020-12-31 03:10:56.023
333842,57214.0,2,,333839.0,,,,CC BY-SA 4.0,"<p>To begin, I confess I'm not familiar with either the
SAS/binomial method or (just from your bit of R code) the exact type of bootstrap CI you are
using.</p>
<p>I notice from your plot that the median is not near
the middle of either of these types of CIs.</p>
<p>There are various
correct methods for making CIs, based on different assumptions and criteria,
so both kinds of results might be 'correct'. [The ultimate goal is to get a style of confidence limits that bracket the true value of
the population median <span class=""math-container"">$\eta$</span> 95% of the time over the long run. Any style that does that is OK.]</p>
<p>A very elementary quantile bootstrap of your first sample in R
gives the result <span class=""math-container"">$(4.5, 8.5):$</span></p>
<pre><code>x1 = c(8, 7, 8, 9.5, 1, 20, 8, 7.5, 3, 20.5, 2.5, 5.5, 
       15.5, 2, 4, 1, 17, 2, 3.5, 8.5, 8.5, 2.5, 11, 4, 
       10.5, 7.5, 12, 5, 16.5, 8.5)
n = length(x1)
set.seed(2020)
h.re = replicate(10000, median(sample(x1, n, rep=T)))
quantile(h.re, c(.025,.975))
 2.5% 97.5% 
  4.5   8.5 
</code></pre>
<p>From a boxplot and a stripchart of your data the CI <span class=""math-container"">$(4.5,8.5)$</span>
[or <span class=""math-container"">$(4,8.5)]$</span> seems more reasonable (to me) than the interval <span class=""math-container"">$(7,11).$</span></p>
<pre><code>par(mfrow=c(2,1))
 boxplot(x1, horizontal=T, col=&quot;skyblue2&quot;)
  abline(v = c(4.5, 8.5), col=&quot;red&quot;)
 stripchart(x1, meth=&quot;stack&quot;, pch=20)
  abline(v = c(4.5, 8.5), col=&quot;red&quot;)
par(mfrow=c(1,1))
</code></pre>
<p><a href=""https://i.stack.imgur.com/RtUXC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RtUXC.png"" alt=""enter image description here"" /></a></p>
<p>Getting any kind of nonparametric CI from these data may be
problematic because you have only <span class=""math-container"">$n = 30$</span> observations with
only 21 uniquely different values.</p>
<pre><code>length(x1);  length(unique(x1))
[1] 30
[1] 21
</code></pre>
<p><em>Notes:</em> Three other possibilities come to mind.</p>
<p>(1) Another style of 95% bootstrap CI, gives the same result
<span class=""math-container"">$(7,11)$</span> as
your bootstrap CI.</p>
<pre><code>h.obs = median(x1)
d.re = replicate(5000, median(sample(x1, n, rep=T)) - h.obs)
UL = quantile(d.re, c(.975,.025))
h.obs - UL
97.5%  2.5% 
    7    11 
</code></pre>
<p>The rationale is that if we knew the distribution of the the difference <span class=""math-container"">$D$</span> between sample median <span class=""math-container"">$H$</span> and the population median <span class=""math-container"">$\eta,$</span> we could find values <span class=""math-container"">$L$</span> and <span class=""math-container"">$U$</span> that cut probability <span class=""math-container"">$0.025$</span> from the lower and upper tails, respectively of that distribution. Thus, <span class=""math-container"">$P(L \le D = H - \eta \le U) = 0.95.$</span> Then we could pivot to get the 95% CI <span class=""math-container"">$(H-U, H-L).$</span></p>
<p>Not knowing actual
values of <span class=""math-container"">$H$</span> and <span class=""math-container"">$L,$</span> we bootstrap <span class=""math-container"">$D,$</span> temporarily using the observed median <span class=""math-container"">$H$</span> as a proxy for unknown <span class=""math-container"">$\eta.$</span> Finally, take
quantiles of the bootstrap distribution of <span class=""math-container"">$D$</span> to get approximate
values <span class=""math-container"">$L^*$</span> and <span class=""math-container"">$U^*$</span> of <span class=""math-container"">$L$</span> and <span class=""math-container"">$U,$</span> respectively.</p>
<p>(2) Contemplating that your data might be exponential with mean <span class=""math-container"">$\mu,$</span>
so that <span class=""math-container"">$\bar X/\mu \sim \mathsf{Gamma}(30,30),$</span> a 95% CI for <span class=""math-container"">$\mu$</span> would
be <span class=""math-container"">$(5.8, 11.9).$</span> Then a 95% CI for the median <span class=""math-container"">$\eta$</span> would be
<span class=""math-container"">$(4.0,8.2)$</span> [not much different from your binomial method.]</p>
<pre><code>mean(x1)/qgamma(c(.975,.025), 30,30)
[1]  5.762466 11.857195
log(2)*mean(x1)/qgamma(c(.975,.025), 30,30)
[1] 3.994237 8.218782
</code></pre>
<p>(3) Jittering the data slightly to break ties, so that the
nonparametric 95% CI from Wilcoxon's signed rank procedure will
work, I got the interval <span class=""math-container"">$(5.5, 9.8).$</span></p>
<pre><code>wilcox.test(x1+runif(30,-.1,.1), conf.int=T)$conf.int
[1] 5.479651 9.751046
attr(,&quot;conf.level&quot;)
[1] 0.95
</code></pre>
<p>Two more runs:</p>
<pre><code>wilcox.test(x1+runif(30,-.1,.1), conf.int=T)$conf.int
[1] 5.479582 9.763138
attr(,&quot;conf.level&quot;)
[1] 0.95

wilcox.test(x1+runif(30,-.1,.1), conf.int=T)$conf.int
[1] 5.496640 9.752084
attr(,&quot;conf.level&quot;)
[1] 0.95
</code></pre>
",2020-12-31 05:37:53.073
333843,205701.0,1,,,,Can be p-value be defined as overlap of surface areas of two comparable distributions?,<statistical-significance><p-value>,CC BY-SA 4.0,"<p>P-value shows the probability of null hypothesis.</p>
<ul>
<li><p>Can we also say that if P = 0.5, then half of the surface areas of
two comparable distributions overlap?</p>
</li>
<li><p>And statistical significance, e.g. P &lt; 0.05, means that the overlap of the surface areas of two comparable distributions is under 5%?</p>
</li>
</ul>
",2020-12-31 05:53:31.623
333844,255219.0,1,,,,(Hypothesis testing of sum or count) standard error when underlying distribution is skewed and heavy tailed,<hypothesis-testing><variance><inference>,CC BY-SA 4.0,"<p>I'm trying to perform hypothesis testing to check if there is a difference in two groups w.r.t. sum of X (continuous RV such as revenue) and sum of Y (integer RV such as count); Each sample in a group will have numerical values for X and Y. I have very large sample for both of groups so CLT kicks in.</p>
<p>The issue is that the distribution of X is unknown but looks very skewed toward lower value and it's slightly heavy tailed (some outliers at the end), definitely not anywhere near normal.</p>
<p>For standard error, I have to know the sample variance of the underlying distribution and I'm not sure how to estimate. I don't think sample variance formula works here given that underlying distribution is not normal but want to double check if I can just use a standard formula for sample variance and devide it by sample size to calculate standard error or is there any other formulas to treat this.</p>
",2020-12-31 06:06:57.223
333845,15606.0,2,,333838.0,,,,CC BY-SA 4.0,"<p>Let <span class=""math-container"">$Z \in \{0,1,\dots\}$</span> be a nonnegative discrete random variable. Then, we have
<span class=""math-container"">$$
\mathbb E[ Z ]= \sum_{z = 1}^\infty \mathbb P(Z \ge z).
$$</span>
Try to prove this yourself <sub> --------------------------------------------------------------------------------------- Hint: <span class=""math-container"">$Z = \sum_{z=1}^\infty (\cdots)$</span>.</sub>.</p>
<p>Let <span class=""math-container"">$Z = \max(X,Y)$</span>. Then,
<span class=""math-container"">\begin{align}
\mathbb E[\max(X,Y)] &amp;= \sum_{z=1}^\infty \mathbb P(\max(X,Y) \ge z) 
\\
&amp;=\sum_{z=1}^\infty \sum_{x=0}^\infty \mathbb P(X = x) \mathbb P(\max(x,Y) \ge z \mid X = x) \\
&amp;=\sum_{x=0}^\infty \mathbb P(X = x) \Big[ \sum_{z=1}^\infty  \mathbb P(\max(x,Y) \ge z)\Big] \quad \text{(by indept.)} \\
&amp;=\sum_{x=0}^\infty \mathbb P(X = x) \Big[ x + \sum_{z=x+1}^\infty  \mathbb P(Y \ge z)\Big]. 
\end{align}</span></p>
",2020-12-31 06:16:59.727
333846,25720.0,1,,,,Hypothesis testing business case study: does this make sense?,<hypothesis-testing>,CC BY-SA 4.0,"<p>My boss has a PhD in Math from a top 30 university; point being he's a bright guy but doesn't have a formal background in statistics. For background, I have an MS in Statistics but focused more on machine learning. He asked me to do a hypothesis test but it's unlike anything I've ever seen before so I'm wondering if it even makes sense. We are both data scientists.</p>
<p>Presently, our company has a system that automatically classifies phone numbers and by extension, their calls, as &quot;bad&quot; or &quot;good.&quot; When the first call by a phone number is made through our system, we undertake an expensive manual investigation of various attributes related to the phone number to determine whether it is a &quot;bad&quot; or &quot;good&quot; phone number. The relevant aspect of this system is that there is a business rule in place where a previously classified &quot;bad&quot; phone call all subsequent calls labeled as &quot;bad,&quot; if those calls are made within, let's say, <strong>40 hours of the first call</strong>.</p>
<p>The stakeholders admit this 40 hour threshold is somewhat arbitrary and we've been tasked with finding a new threshold, supported by data. My boss said I should do this:</p>
<ol>
<li><p>Build a dataset using call data from the past 4 months. Each observation is a phone call. A single phone number can make multiple phone calls. This dataset should only contain numbers that started off the 4 month period classified as &quot;bad&quot; and was later classified as &quot;good&quot; at any point afterward, in the same 4 month period.</p>
</li>
<li><p>Calculate the time difference between ANY two calls made within the same week. A week is defined as a time period starting on Monday 12am and ending 11:59pm on Sunday. Clearly, this restriction makes it such that the max &quot;time gap&quot; between any two calls is 168 hours. For example, phone number 1 making 3 phone calls will result in 3 rows of data that looks like:</p>
</li>
</ol>
<pre><code>    phone_number     time_gap
    phone number 1   hours between call 1 &amp; 2
    phone number 1   hours between call 2 &amp; 3
    phone number 1   hours between call 1 &amp; 3
</code></pre>
<p>3a. Create a new dataset that is a filter of the above data set such that <code>time_gap</code> is in [20, 50].</p>
<p>3b. Create a new dataset that is a filter of the above data set such that <code>time_gap</code> is in [20, 70].</p>
<ol start=""4"">
<li><p>Find the mean of datasets 3a, 3b. Calculate the Z-score for this test statistic, using the formula:</p>
<p><code>Z = (Xbar - mu) / (StD / sqrt(n))</code></p>
</li>
</ol>
<p>where</p>
<pre><code>Xbar = mean of a sample with replacement from 3a, 3b
mu = 40 hours, the current business rule threshold
StD = the standard deviation of a sample with replacement from 3a, 3b
n = size of the sample from 3a, 3b
</code></pre>
<hr />
<p>My questions are:</p>
<ol>
<li><p>Does this even make sense to do? In my mind, by restricting the data in steps 3a and 3b, we are kind of manufacturing the results for our sample mean and just trying to show that calls in this range are statistically significantly different from 40. Then, so what? This seems like cherry-picking our data and seems to definitely break the rule of: &quot;don't throw away data you don't like just because you don't like it.&quot;</p>
</li>
<li><p>This whole thing doesn't make sense to me because we're just comparing two arbitrarily defined subpopulations of the &quot;bad to good phone number&quot; population. Doesn't it make more sense to find the mean number of hours between any two calls, between 2 different populations? For example, the &quot;bad to good phone number population&quot; and the &quot;bad to bad phone number population&quot;?</p>
</li>
<li><p>Does it make sense to take repeated samples from datasets 3a and 3b, i.e. bootstrapping to get confidence intervals for my mean? My boss saw that I did that and laughed, saying that it's pointless because doing so is basically just trying to prove the Central Limit Theorem.</p>
</li>
<li><p>Don't I need to check for normality of the underlying distribution of the time gap field?</p>
</li>
</ol>
<p>Detailed responses are appreciated. I've already made the points above to my boss on a few occasions but he basically said I just don't understand basic statistics (which is possible, lol) and has become pretty exasperated at this point. Normally, I'd just let it go and do the work without much argument but we (read: I am) are going to have to present and defend these results to external statisticians and I'm honestly not a believer.</p>
",2020-12-31 06:32:05.163
333847,235220.0,1,,,,Why the computational time of random forest is lower than the decision tree?,<machine-learning><python><random-forest><cart>,CC BY-SA 4.0,"<p>What I understood is that RF trains many decision trees. RF supposes to have higher training and testing time.</p>
<p>Based on my experiments, it seems like RF have a lower computational time.</p>
<p>Is this possible to happen?</p>
",2020-12-31 06:56:51.937
333848,165989.0,1,,,,Ho to explore the interaction between dependent variable and covariate?,<lme4-nlme><repeated-measures><interaction><random-effects-model><predictor>,CC BY-SA 4.0,"<p>I'm conducting LMM by adding three independent variables (A, B, C) and a covariate (E) as the fixed effect, and the random intercept for each subject as the random effect in the model, as shown here below:</p>
<pre><code>model &lt;- lmer(RT_log ~ A * B * C * E + (1|Subject)
</code></pre>
<p>The results showed a significant three-way interaction between A, B, and E, of which the covariate E is a continuous variable (0-100). As we know that if E has only two or three levels, we can further conduct two LMM analyses, by adding two dependent variables (A, B) as the fixed effect and the same random effect as here above, for the two levels of E, separately. I was wondering how should I do if the E is a continuous variable in my case?
Thanks in advance for your any suggestion!</p>
",2020-12-31 07:49:05.743
333849,59440.0,2,,333564.0,,,,CC BY-SA 4.0,"<p>b) is not correct. By the standard definition, neither is d, however d is the closest.</p>
<p>A more correct definition would go something along the lines of &quot;The expected total future <em>discounted</em> reward starting in a state <span class=""math-container"">$s$</span>, executing an action <span class=""math-container"">$a$</span> and thereafter following a specified policy <span class=""math-container"">$\pi$</span>.&quot;</p>
",2020-12-31 09:17:18.310
333850,128628.0,2,,333778.0,,,,CC BY-SA 4.0,"<p>Since the <code>Extraversion</code> score is just the average of the <code>Assertiveness</code> and <code>Enthusiasm</code> scores, each of these variables is a linear function of the other two.  Thus, once you already have two of the variables in the model, adding the third gives you non-identifiable effect terms.  I recommend you include only the latter variables in your regression model and exclude <code>Extraversion</code>.  If you wish to make inferences about the <code>Extraversion</code> variable then you can do so by looking at the average of the coefficients for the other two variables.</p>
",2020-12-31 09:20:17.940
333851,255290.0,1,,,,Re-Posted Average value of Normal distribution table question,<probability><normal-distribution><mean>,CC BY-SA 4.0,"<p>Re-posted this question, because after I created my account, the other question acts like it doesn't belong to me. Apologies if there was a way to tie that original question to my new account. Original question: <a href=""https://stats.stackexchange.com/questions/502956/average-value-of-normal-distribution-table-question"">Average value of Normal distribution table question</a></p>
<p>I hope this rewording of the question is more clear:</p>
<p>I have a normal distribution table for items looted in a video game, with 13 separate entries for items able to be looted. In my below example, each item has a mean value and a standard deviation.</p>
<p>I can calculate the random variable of any individual item , since I know the Mean and SD. I.E.</p>
<p>I am trying to find the correct way of calculating the probability of looting any random one of these items with a specific probability. In this case, the probability of the value being &lt; than the random variable is 0.05... or 1 in 20 of that item will be looted with that random variable.</p>
<p>I am using the following calculator to find the individual items' random variable, given said probability: <a href=""https://stattrek.com/online-calculator/normal.aspx"" rel=""nofollow noreferrer"">https://stattrek.com/online-calculator/normal.aspx</a></p>
<p>For example, the Cumulative probability: P(X &lt; 60474.549) = 0.05 of item with Mean 72380 and SD 7238 = 60474.549</p>
<p>Mean value and Standard deviation for each item below:</p>
<pre><code>Mean     SD      
72380   7238    Cumulative probability: P(X &lt; 60474.549) = 0.05
62040   6204    Cumulative probability: P(X &lt; 51835.328) = 0.05
59455   5945.5  Cumulative probability: P(X &lt; 49675.523) = 0.05
62040   3102    Cumulative probability: P(X &lt; 56937.664) = 0.05
77550   7755    Cumulative probability: P(X &lt; 64794.160) = 0.05
62040   6204    Cumulative probability: P(X &lt; 51835.328) = 0.05
67210   6721    Cumulative probability: P(X &lt; 56154.939) = 0.05
67210   6721    Cumulative probability: P(X &lt; 56154.939) = 0.05
72380   7238    Cumulative probability: P(X &lt; 60474.549) = 0.05
67210   6721    Cumulative probability: P(X &lt; 56154.939) = 0.05
77550   7755    Cumulative probability: P(X &lt; 64794.160) = 0.05 
62040   6204    Cumulative probability: P(X &lt; 51835.328) = 0.05
87550   8755    Cumulative probability: P(X &lt; 73149.307) = 0.05
</code></pre>
<p>Is it correct to take the average value of each Mean and SD, to find the probability of looting any item with that stat value? Or is there a a more complex statistical calculation that needs to be performed?</p>
<p>Example of what I think I might need to do, which could be wrong: I added the sum of the mean values and then divide by the number of items, then do the same for the standard deviations.</p>
<p>If I take the individual calculations and then average them that after the fact, I get the same result for the random variable.</p>
<p>The average of all the mean values came to 68973.46153846154  and the average of all the Standard Deviations came to 6658.730769230769, and the standard distribution calculator comes back with the the following.</p>
<p>Average Mean = 68973.46153846154<br />
Average SD = 6658.730769230769</p>
<p>Cumulative probability: P(X &lt; 58020.824) = 0.05</p>
<p>Or is there another, more complex calculation that has to be performed in order to calculate the probability of any one of these items being looted with said probability?</p>
<p>If it helps clarify what I'm trying to do. The game is a space game, and you can loot items such as shields, engines, guns, etc. Each one has a known mean mass value and a known standard deviation.</p>
<p>So, the part with the best probability of looting a low mass is the one with mean=59455 and SD=5945.5, which gives a 0.05 probability of looting one of that item below 49675.523 mass as the random variable. The worst one to loot is the one with mean value of 87550 and SD of 8755.</p>
<p>However, since there are 13 potential items that can be looted, we're trying to calculate for the probability of looting ANY ONE of those items, not just the best one or the worst one.</p>
",2020-12-31 10:48:38.873
333852,69840.0,2,,171590.0,,,,CC BY-SA 4.0,"<p>Feature Importance is generally speaking not uniquely defined, even though we all have some sort of intuitive understanding of what it means. The original poster talks about notions of improving the precision/accuracy of predicting Y.</p>
<p><strong>Shapley Values</strong></p>
<p>Let us assume (without any loss of generality, but to help us think about the problem clearly), that all features are contributing to accuracy in a measurably positive way (often this is not the case. If a feature contains zero or very little information, it might increase the model's capacity to overfit more than it adds value and you're better off removing it).</p>
<p>Then, intuitively, you might ask &quot;how much accuracy do I lose if I remove any one of the features?&quot; and surely the one feature which, upon removal, makes your accuracy go down the most, is the most important one.</p>
<p>If you do this however, you run into some slightly unintuitive issues. If features A and B are highly correlated, removing either might have very little effect on the classifier's accuracy, but removing both might be catastrophic. This is where Game Theorists come in and say that you should use Shapley values. The analogy is as follows:
Let's say you have balanced classes, but your ML algorithm gets a test accuracy of 85%. This means that it's adding 35% points to a baseline. There are thus 35 points of reward to be shared amongst the features you have. Shapley Values tell you how to fairly share the profits of a company amongst your employees. They require you to calculate how much money the company would have made, had all possible &quot;coalitions&quot; of the workforce been at the company (basically removing all combinations, from 1 to all employees from the workforce, there are <span class=""math-container"">$2^N$</span> such coalitions). In the ML case, you look at removing all combinations of features, and you see where (presumably somewhere between 50% and 85%) you land in test accuracy.</p>
<p>So Shapley values will give you a &quot;fair&quot; view of how much each feature is contributing.</p>
<p><strong>Comparing Shapley Values or Feature Importances of Different Models</strong></p>
<p>Now, to your second question, which I will paraphrase here: I've trained two models, one performs better than the other. Does that mean that the second model is a better model to use to measure feature importance?</p>
<p>An interesting question for sure. Shapley Values are model agnostic, you can apply the procedure (in theory anyway) to any model. The Shapley Values do however not tell you deep truths about the data, they tell you about the model. If feature X has a high Shapley Value according to model A and a lower one according to model B, that simply means that &quot;model A is making more use of feature X to improve its accuracy than model B is&quot;.</p>
<p><strong>Feature Importance Vs Causation</strong></p>
<p>This brings us to a general point about feature importance. People frequently ask about it, people have a general intuitive view of what they mean by it, but when you drill into it, people are often looking for an answer that standard supervised learning cannot provide.</p>
<p>Your original question implies there is some &quot;true&quot; feature importance out there, which is model independent. Consider the following:</p>
<p>Model 1 and Model 2 predict the weather tomorrow. They both have access to a tonne of data. Model 1 decides that the most important features are today's weather, and some long term seasonal average data. Model 2 decides that the most important features are the wind direction, and the current weather conditions in a few places a few hundred miles away. They get similar accuracy, so which do you trust?</p>
<p>Model 1 is making no attempt to get at the causal mechanism, it's saying something like &quot;I'm going to predict the seasonal average, making adjustments for today's weather&quot; whereas Model 2 is saying &quot;check the wind direction and see what's coming from us upwind&quot;. Clearly the latter is much closer at getting to the true cause, would you thus trust its feature importance more? What about if Model 1 is more accurate? Do you change your mind and say its feature importances are the true causal mechanisms?</p>
<p>Generally speaking, in ML prediction problems where you have no intuition about the causal mechanisms, it's not correct (albeit it's tempting, especially when under pressure from non domain experts) to interpret feature importances in a causal way. Don't do it, it's almost invariably incorrect to do so.</p>
<p><strong>Is Feature Importance a Useful Concept at all?</strong></p>
<p>Feature Importances are specific to a model, they don't necessarily give you general properties about the mechanisms which generate the data. They tell you how a model does what it does, not how nature does what it does.</p>
<p>So with causality out the window, what else can they be good for? Well, my personal opinion is that they're not that useful at all. The main use case, is to show them to end users of ML systems who are often not domain experts, in order to make your work slightly less black box and build trust. This can sometimes backfire, as they will interpret them with a causal lens, but clearly non-causal features can have high feature importances.</p>
<p>Low feature importance can be a useful indicator, it could tell you, before deploying your POC model, not to worry about certain features, and you could save yourself building production data pipelines for some features. Still, low feature importance is sufficient but not necessary. With most feature importance methods, you can have a feature with a high feature importance, which if removed, would still not make much of a difference, as other features would assume its place. If you want to use feature importance to learn which features you could do without, you probably can't do much better in practice, than removing features one by 1 and seeing which makes the most difference (this scales like <span class=""math-container"">$K^2$</span>, where K is the number of features)...unless you have enough compute power to investigate all possible coalitions of features, see which ones perform well, and then decide on tradeoffs between performance and difficult of deployment (this scales like <span class=""math-container"">$2^K$</span>)</p>
<p>One final use for feature importance, is when debugging. If you're getting weird results, it's good to know which features are contributing the most, these are the ones you should investigate first for possible corrupted data/buggy input pipelines.</p>
",2020-12-31 11:17:44.950
333853,243325.0,1,,,,Adjustment factor in logistic growth model of facebook-prophet,<time-series><growth-model><logistic-curve><prophet>,CC BY-SA 4.0,"<p>In the paper it emphasizes:</p>
<p>&quot;When the rate k is adjusted, the offset parameter m
must also be adjusted to connect the endpoints of the segments. The correct adjustment
at changepoint j is easily computed as:&quot;</p>
<p><span class=""math-container"">$$\gamma_j = \left(s_j - m - \sum_{l&lt;j}\gamma_l \right)\left(1 - \frac{k + \sum_{l&lt;j} \delta_l}{k + \sum_{l \leq j} \delta_l} \right)$$</span></p>
<p>My understanding is, adjustment makes sure function is still continuous.</p>
<p>I have 2 questions regarding this model that are not explained in the paper:</p>
<p>How do we come up with the result function is not continuous without the adjustment factor, and how do we compute the adjustment factor?</p>
<p>Link to paper: <a href=""https://peerj.com/preprints/3190/"" rel=""nofollow noreferrer"">https://peerj.com/preprints/3190/</a></p>
",2020-12-31 11:30:55.547
333854,255320.0,1,,,,"Comparing two sets of proportions/percentages from related samples (bibliometrics, qualitative data)",<nonparametric><qualitative>,CC BY-SA 4.0,"<p>I'm a newbie in statistics and bibliometrics, so I'm quite lost right now. I have two sets of data, A and B, both of them with frequencies of topics or subjects used in translation research publications. Set A (audiovisual translation) is a subset of B (all translation-related fields). I want to know whether the proportions of the topics in A are similar or not to those in B. As I'm dealing with qualitative data, I now know I cannot use Kendall's tau-b correlation coefficient (which I did before and works just fine, by the way). Absolute numbers are not comparable (eg. 100 / 5.000), so chi-squared test is not useful either, because I think I cannot use it with percentages. So which test should I use? Wilcoxon signed rank test?
Thanks for your help in advance and best of luck for the new year.
Paco</p>
",2020-12-31 11:51:58.550
333855,66869.0,1,333856.0,,,Augmented Dickey Fuller Test vs Ljung Box Test,<stationarity><augmented-dickey-fuller>,CC BY-SA 4.0,"<p>When do you use the Augmented Dickey Fuller Test and when do you use the Ljung Box test?</p>
<p>What are the pros and cons of using each to test for stationarity?</p>
",2020-12-31 12:13:06.777
102770,47667.0,1,120255.0,,,How do i get prediction accuracy when testing unknown data on a saved model in Scikit-Learn?,<machine-learning><python><prediction><scikit-learn>,CC BY-SA 3.0,"<p>i have a model i have trained for binary classification, i now want to use it to predict unknown class elements.</p>

<pre><code>     from sklearn.externals import joblib
     model = joblib.load('../model/randomForestModel.pkl')
     test_data = df_test.values # df_test is a dataframe with my test data
     output = model.predict(test_data[:,1:]) # this outputs the prediction either 1 or 0
</code></pre>

<p>I know how to get confusion_matrix, accuracy_score, classification_report given the training dataset, but in the case i do not have the train data.
i would like to get something akin to this from weka:</p>

<pre><code>       inst#     actual  predicted error prediction
           1        1:?        1:0       0.757 
</code></pre>

<p>Is it possible in Scikit-learn? if so, how do i do it? </p>
",2015-03-17 22:08:24.090
120364,56490.0,1,,,,"Using gradient information in minimizing error function, in Bishop's Pattern Recognition",<machine-learning><neural-networks><optimization><pattern-recognition><gradient-descent>,CC BY-SA 3.0,"<p>In Bishop's book <em>Pattern Recognition</em>, there appears the following paragraph on page 239, where I included the equation he refers to</p>

<blockquote>
  <p>In the quadratic approximation to the error function,  $$\displaystyle E(\mathbf{w}) \simeq E(\mathbf{\hat w}) + (\mathbf{w} − \mathbf{\hat w})^{\textrm T}\mathbf{b} + \frac 1 2 (\mathbf w − \mathbf{\hat w})^{\textrm T}\mathbf{H}(\mathbf{w} − \mathbf{\hat w})$$
  With $\mathbf{b}$ the gradient and $\mathbf{H}$ the Hessian, the error surface is specified by the quantities $\mathbf{b}$ and $\mathbf{H}$, which contain a total of $W(W + 3)/2$ independent elements (because the matrix $\mathbf{H}$ is symmetric), where $W$ is the dimensionality of $\mathbf{w}$ (i.e., the total number of adaptive parameters in the network). The location of the minimum of this quadratic approximation therefore depends on $O(W^2)$ parameters, and we should not expect to be able to locate the minimum until we have gathered $O(W^2)$ independent pieces of information. If we do not make use of gradient information, we would expect to have to perform $O(W^2)$ function evaluations, each of which would require $O(W)$ steps. Thus, the computational effort needed to find the minimum using such an approach would be $O(W^3)$. </p>
  
  <p>Now compare this with an algorithm that makes use of the gradient information. Because each evaluation of $\nabla E$ brings $W$ items of information, we might hope to find the minimum of the function in $O(W)$ gradient evaluations. As we shall see, by using error backpropagation, each such evaluation takes only $O(W)$ steps and so the minimum can now be found in $O(W^2)$ steps. For this reason, the use of gradient information forms the basis of practical algorithms for training neural networks.</p>
</blockquote>

<p>I can't see the intuition he's referring to. What is he referring to as information, and how is he estimating the number of steps to the minimum?</p>
",2015-09-02 09:21:11.367
133686,57794.0,1,,,,How to calculate influence of variables at ROW LEVEL?,<regression><neural-networks><modeling><random-forest><importance>,CC BY-SA 3.0,"<p>There are several algorithms which give relative importance of variables at OVERALL Model level.
But the most influencing variable might not be the reason why a particular row might get higher or lower scores.
 eg: different applications get different reasons for Credit decline though scored by the same Credit Risk model. This happens because one variable's value drives the overall score for that particular application down, although this variable might not be the one which is the most significant across all rows at a model level.</p>

<p>I know how to calculate the top influencing variables in a Logistic / Linear Regression Model for each ROW (by rank ordering the product of coefficient &amp; variable value for each variable).</p>

<p>But how do we calculate relative importance of variables at each ROW level using other algorithms like Neural network, Random Forest, etc.</p>
",2016-01-13 09:48:47.270
171590,47569.0,1,,,,How to choose the best algorithm for measuring attribute importance/relevance?,<svm><feature-selection><random-forest><model-selection><importance>,CC BY-SA 3.0,"<p>Let's say we want to conclude that attributes <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code> are the most relevant attributes to maximize the precision of predicting <code>Y</code>, and then rank those attributes based on their importance/relevance.  </p>

<p>Now let's say <em>SVM</em> and <em>Random Forest</em> both seem to be good fits to model the data and Random Forest provides better performance (higher precision). Does that mean Random Forest would also be a better choice to rank the attributes according to their relevance to <code>D</code>?  </p>

<p>In a more general sense, can we say the best algorithm for maximizing precision of predicting <code>Y</code> is also the best algorithm to rank the relevance of attributes to <code>Y</code>?</p>
",2016-12-13 03:29:32.103
172027,85141.0,1,172031.0,,,Vectorization of data before splitting in to test and train with Neural Network?,<neural-networks>,CC BY-SA 3.0,"<p>Is it better to split your dataset into train and test before vectorizing?</p>

<p>Or is it better to do it in reverse and vectorize inputs then perform train test split?</p>

<p>For example I'm trying to use some categorical inputs such as types of animals, should I vectorize them into one hot then split into train/test or split the data set randomly then vectorize into one hot?</p>
",2016-12-15 23:47:26.190
182063,109482.0,1,182191.0,,,Squared Loss for Multilabel Classification,<machine-learning><classification><least-squares><loss-functions><cross-entropy>,CC BY-SA 3.0,"<p>I have a classification problem with $K$ labels. I represent the correct label $y$ of an observation $x$ as a vector $y$ in $R^K$, with entries $y_{k'} = \delta_{kk'}$ if $x$ belongs to class $k$.</p>

<p>Given an observation $x$, I predict its label with a vector $f(x) \in R^K$, where the components $f_k(x)$ satisfy $f_k(x) \in (0,1)$ and $\sum_k f_k(x) = 1$. A larger value of some $f_k(x)$ means that $x$ is more likely to belong to class $k$.</p>

<p>We want to learn the best function $f$ by choosing an appropriate loss function $\ell$. I know that a common choice is the cross-entropy: $\ell(x,y) = -\sum_k y_k \log f_k(x)$. Is squared loss $\ell(x,y) = \frac{1}{2}||y - f(x)||_2^2$ ever used? If so, does it tend to produce classifiers with noticeably distinct performance profiles, compared to cross-entropy?</p>

<p>A <a href=""https://stats.stackexchange.com/questions/243090/square-loss-for-classification-via-regression"">comment</a> on a related question warns against the use of squared loss.</p>
",2017-03-11 00:16:56.000
212670,14355.0,1,212847.0,,Rickyfox,Sampling from Skew Normal Distribution,<distributions><simulation><random-generation><skew-normal-distribution><accept-reject>,CC BY-SA 3.0,"<p>I want to draw samples from a <a href=""https://en.wikipedia.org/wiki/Skew_normal_distribution"" rel=""noreferrer"">skew normal distribution</a> as part of a matlab project of mine. I already implemented the CDF and PDF of the distribution, but sampling from it still bothers me.<br>
Sadly, the <a href=""http://azzalini.stat.unipd.it/SN/faq-r.html"" rel=""noreferrer"">description of this process from the documentation of an R package</a> is riddled with dead links, so I did some reading on the process.</p>

<p>One way of sampling from the distribution would be <code>inverse transform sampling</code>, which uses a uniform random variable $U\sim Unif(0,1) $ and involves solving</p>

<p>$F(F^{-1}(u)) = u$</p>

<p>with $F(x)$ being the CDF of the distribution we want to sample from. Since I don't know how to find the inverse of $F(x)$ myself, I did some searching, finding this question asked several times but not answered.</p>

<p>Edit: <strong>another method</strong> to sample from the distribution would be <a href=""https://en.wikipedia.org/wiki/Rejection_sampling"" rel=""noreferrer"">rejection sampling</a>, however for that I need to find a distribution that</p>

<ol>
<li>I can draw samples from</li>
<li>Has a pdf ""which is at least as high at every point as the distribution we want to sample from, so that the former completely encloses the latter."" (from the rejection sampling wiki article)</li>
</ol>

<p>I've plotted the skew normal distribution with $\xi=1,\omega=1.5,\alpha=4$ and its truncated version (truncated to [0,2.5] here). </p>

<p><a href=""https://i.stack.imgur.com/asXAR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/asXAR.png"" alt=""unrestricted and truncated skew-normal distribution""></a></p>

<p>In my application of this, I will always truncate the distribution to a certain interval, so I'd need to find a distribution that 'contains' the SN pdf for (hopefully) all parameters.</p>

<p>Any ideas how I could go about sampling from such truncated skew-normal distributions?</p>
",2017-11-28 11:54:19.820
228594,136479.0,1,,,,Self Learning Neural network,<machine-learning><neural-networks><keras>,CC BY-SA 3.0,"<p>Is it possible for a neural network to learn based on its own predictions? So what I really want to do is to have my neural network model to learn from its own predictions but I'm not sure what the implications would be. On my own intuition, I think that it might lead the model into a state where it doesn't work well with new input since what it knows too much only about itself. I would want any advice for the matter. </p>

<p>Also I thought of having the model learn from its own predictions that pass a certain uncertainty. But I also have a hard time determining uncertainty for a neural network model and Im using keras. </p>

<p><a href=""https://i.stack.imgur.com/aMl2q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aMl2q.png"" alt=""A workflow of what Im trying to ""></a></p>
",2018-04-23 04:10:23.457
266712,196216.0,1,333758.0,,,ARMA vs ARIMA Models,<time-series><arima>,CC BY-SA 4.0,"<p><code>ARMA(p,q)</code> is generally denoted as a special case of <code>ARIMA(p,d,q)</code>, when <code>d = 0</code>. However, <code>ARIMA(p,d,q)</code> is actually <code>ARMA(p+d,q)</code> so an ARIMA is actually an ARMA model, right? Then, how come ARIMA model is generalized version of ARMA models? </p>
",2019-04-07 15:47:15.563
307384,171258.0,1,,,,Is CPCV similar to bootstrap?,<cross-validation><bootstrap>,CC BY-SA 4.0,"<p>I am writing to ask if the Combinatorial (Purged) Cross-Validation"" method of Marcos Lopez de Prado's ""Advances in Financial Machine Learning"" book is similar to the idea of bootstrap. If not, what is the key difference? It seems like bootstrap is based on permutation (resampling with replacement) and C(P)CV is based on combinations (resampling without replacement)? At the same time, I suppose the key goal between both methods is to generate as many artificial samples from the same dataset? </p>
",2020-04-30 14:06:48.817
309011,232778.0,1,,,,GPT 2 Model on french text,<natural-language>,CC BY-SA 4.0,"<p>Is there someone who worked on french text generation using GPT 2 model? Is there a pre-trained GPT 2 model on french text? Can we do finetuning on the Standard GPT 2 and obtain good results on french text? Another question : Adding POS Tagging on GPT can improve the generation or not?</p>

<p>Thanks,</p>
",2020-05-12 21:42:39.827
310030,235658.0,1,,,,Using pos_weight to improve recall in a multi-class multi-label problem,<neural-networks><classification><precision-recall><multi-class><multilabel>,CC BY-SA 4.0,"<p>I have a multi-label classification problem, and so I’ve been using the Pytorch's <a href=""https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss"" rel=""nofollow noreferrer"">BCEWithLogitsLoss</a>. I’d like to optimize my model for a higher F2 score, and so want to bias it to have greater recall (with decent precision too of course). Are there any guidelines for setting the values of <code>pos_weight</code> to be able to do this?</p>
",2020-05-20 21:23:36.233
316695,20742.0,1,,,,Measuring the confidence of a softmax classification outcome,<machine-learning><classification><decision-theory><calibration>,CC BY-SA 4.0,"<p>Suppose I have a softmax distribution produced by a classifier. There are four labels, and so the sum of the softmax probabilities over the four labels will be 1.0.</p>
<p><strong>I am looking for a measurement of how &quot;certain&quot; or &quot;confident&quot; the classifier's prediction is.</strong></p>
<p>For example, suppose for two data instances, the softmax distributions are:</p>
<pre class=""lang-py prettyprint-override""><code>outcome1 = [0.25, 0.25, 0.20, 0.30]
outcome2 = [0.02, 0.94, 0.02, 0.02]
</code></pre>
<p>It's clear that the classifier is more confident in <code>outcome2</code> since there is a large probability mass (0.94) on one of the labels. On the other hand, the classifier is less confident in <code>outcome1</code> since the probabilities are fairly equal.</p>
<p>So I'm looking for a way to quantify this degree of &quot;certainty&quot; in a classifier's prediction.</p>
<p>One thing I was thinking of was computing the Shannon entropy of each outcome:</p>
<pre class=""lang-py prettyprint-override""><code>from scipy.stats import entropy
print(entropy(outcome1, base=2)) # 1.9854752972273344
print(entropy(outcome2, base=2)) # 0.4225426691977457
</code></pre>
<p>Can I say that the classifier is <code> 1.98 / 0.42 = 4.7</code> times more confident in <code>outcome2</code> versus <code>outcome1</code>?</p>
",2020-07-15 20:03:03.220
318704,242632.0,1,,,,Why is the Treatments P-value halved after a Geisser-Greenhouse correction for sphericity,<r><p-value><excel><sphericity>,CC BY-SA 4.0,"<p>I have noticed that when Graphpad Prism does a sphericity correction using the Geisser-Greenhouse correction, it halves the P value for this source of variation (typically Treatments). For example, in the example I am working on, it calculates an F value of 20.25, with DF(num) = 1.391, and DF(den) = 2.783. The P value for this F-ratio should be 0.046 (which you will get if you use FDIST(20.25, 1.391, 2.783) in Excel. However, Prism gives a P value of 0.023 (half of 0.046). On the other hand, for the Individual or Between Rows source, there is no correction for sphericity, and the F-ratio is 0.25, with DF(num) = 2, and DF(den) = 4. It gives a P-value of 0.7901 for this F-ratio, and as you can verify, it is the 'full&quot; P-value. Why does it halve it when there is a Geisser-Greenhouse correction for sphericity? I redid the analysis with no correction for sphericity; it now gives an F-ratio of 20.25, with DF(num) = 2, and DF(den) = 4. And the P-value is the &quot;full&quot; value of 0.0081.</p>
<p>Excel gives the one-tail P-value when the formula FDIST(x, df_num, df_den) is used. Using the above values, it gives a P-value of 0.046 for FDIST(20.25, 1.391, 2.783). To get the two-tail value, we need to double this P-value, not halve it. That is why I am confused.</p>
<p>Any insights? Thank you.</p>
",2020-08-01 05:36:59.207
328393,250224.0,1,,,,Will Multiple Imputation (MICE) work on dataset with missing data on only one feature?,<data-imputation><multiple-imputation><mice>,CC BY-SA 4.0,"<p>Based on this <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/"" rel=""nofollow noreferrer"">article</a>, it is apparent that MICE works with the following logic:</p>
<ol>
<li>Fill missing values in every column apart from the column in question with either random or the mean of the given values</li>
<li>Use these feature sets as dependent values to regress given values in the column in question (dropping missing values)</li>
<li>Use the model trained on given values to predict missing values in the column in question</li>
<li>Cycle through the other columns using these given values and the predictions a set number of iterations (minimum of however many columns with missing data plus a certain factor)</li>
</ol>
<p>It seems that this method for imputing missing values requires at least 2 variables with missing values, as the iteration is done with predictions for each column with missing values to introduce error into the imputation process.</p>
<p>Say you only have one feature with missing values (ie 10 features; 1 feature is missing 10% of rows; 9 features are 100% filled). Will MICE work in this scenario, or will it be reduced to simple/singular imputation? In other words, does MICE require datasets with more than one feature to impute missing values on?</p>
",2020-11-02 18:48:18.500
331384,14484.0,1,333726.0,,,Fader/Hardie BG/NBD model. Interpretation of $a$ and $b$ of the beta distribution drop out process,<beta-distribution><customer-lifetime-value>,CC BY-SA 4.0,"<p><a href=""https://cran.r-project.org/web/packages/CLVTools/vignettes/CLVTools.pdf"" rel=""nofollow noreferrer"">On R's CLVTools package documentation</a>, there's a sentence referring to the pareto/NBD model</p>
<p>I'm working with the BG/NBD model not the pareto/NBD model. I'd like to understand if I can interpret the parameters of this model in a similar way to the documentation on the pareto/NBD model.</p>
<p>From that link above on the Pareto/NBD model:</p>
<blockquote>
<p>For the standardPareto/NBD model, we get 4 parameters , ,  and
. where ,  represent the shape and scale parameterof the gamma
distribution that determines the purchase rate and ,  of the
attrition rate across individualcustomers. / can be interpreted as
the mean purchase and / as the mean attrition rate</p>
</blockquote>
<p>The BG/NBD model I'm working with also returns 4 parameters:</p>
<ul>
<li>r: shape parameter of the Gamma distribution of the purchase process.</li>
<li>alpha: scale parameter of the Gamma distribution of the purchase
process.</li>
<li>a: shape parameter of the Beta distribution of the dropout
process.</li>
<li>b: shape parameter of the Beta distribution of the dropout
process.</li>
</ul>
<p>The first two of these, , , are the same as for the Pareto/NBD model. So I'm assuming that I can interpret those two in the same way? &quot;/ can be interpreted as the mean purchase (rate)&quot;?</p>
<p>My question is about the remaining two parameters of the BG/NBD model, a and b. Can I calculate the mean attrition rate in a similar manner by dividing one over the other?</p>
",2020-12-03 20:28:13.933
332131,216426.0,1,333817.0,,,Ordinal logistic regression - How to handle NaNs,<r><p-value><covariance-matrix><assumptions><ordered-logit>,CC BY-SA 4.0,"<p>I'm trying to run an ordinal logistic regression model with 5 IV's and a DV with 4 levels. I'm using the function polr from the MASS package in R. My data consists of 46 observations and all the IV's are continuous.</p>
<p>After fitting the model, I run the summary command, but I get the following warning:
In sqrt(diag(vc)) : NaNs produced</p>
<p>After checking the variance-covariance matrix I find that the calculated variance for 3 of the IV's are negative (and very close to zero). Hence, R is only able to calculate SE's for 2 IV's. One of these IV's is significant with a p-value of roughly 0,008.</p>
<p>After reading some other threads, a possible explanation for the negative variance seems to be that the true variance of the beta is zero, or very close to zero. Is this a plausible explanation in a situation like this?</p>
<p>Nevertheless, I still have two question that I'm wondering about:</p>
<ol>
<li>What would be a good approach for dealing with NaNs in a model like this (or in any type of regression model for that matter)? When I was a student in statistics I remember my teacher (it was a course on analysis of variance) telling us that one way of dealing with a negative variance would be to set it equal to zero, is that a reasonable approach here? That would however make it impossible to calculate a p-value, so I guess you would then assume the p-value to be basically zero as well?</li>
<li>Considering that my model produces NaNs, does that mean that the calculated p-values for my other two IV's are wrong or can I trust them? My guess is that the NaNs doesn't affect the two calculated SE's, but I am not very sure about this.</li>
</ol>
<p>Any help with this would be very appreciated!</p>
<p>Model:</p>
<pre><code>polr(DV ~ IV1 + IV2 + IV3 + IV4 + IV5, data = df, Hess=TRUE)
</code></pre>
<p>Data:</p>
<blockquote>
<pre><code>structure(list(DV = structure(c(1L, 2L, 3L, 4L, 4L, 3L, 3L, 2L, 
2L, 2L, 3L, 4L, 4L, 4L, 3L, 3L, 2L, 4L, 2L, 3L, 1L, 4L, 3L, 3L, 
3L, 1L, 4L, 2L, 3L, 4L, 3L, 3L, 2L, 3L, 3L, 2L, 4L, 2L, 4L, 3L, 
4L, 2L, 3L, 3L, 2L, 3L), .Label = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), class = &quot;factor&quot;), 
    IV1 = c(31.4471225882832, 26.7995926461918, 6.15597391098257, 
    32.2641357244643, 5.22243966268843, 50.905131621076, 15.9862670353658, 
    20.3080359821474, 8.65788372462158, 18.505603942379, 6.29152570809156, 
    24.9245052889569, 29.7359693719515, 21.859516176042, 22.8111791071324, 
    23.2948895757526, 7.48299489410315, 25.7653834495302, 11.4419627627679, 
    11.6797724921892, 18.9905104994504, 12.8525067743421, 23.7294286791083, 
    17.7572582793217, 6.51496922762868, 53.5047647614377, 19.7405021265905, 
    27.7157343854444, 20.2388180530257, 9.03173525976901, 15.3013141742333, 
    12.7120495663999, 15.7178643351835, 10.0053600142934, 16.6321423711387, 
    11.8383765639605, 11.4583741140051, 22.9047246959457, 7.72280996279261, 
    20.3566081932521, 7.17681029058905, 8.27263288950658, 27.9236215467482, 
    14.8812599466755, 11.9252118854612, 13.2169871327123), IV2 = c(5.60931899641577, 
    4.99118165784832, 5.40925266903915, 5.11764705882353, 4.89913544668588, 
    5, 5.07709251101322, 4.84575835475578, 3.73287671232877, 
    5.30413625304136, 5.52755905511811, 4.97975708502024, 5.02013422818792, 
    4.81481481481481, 4.69059405940594, 5.07853403141361, 4.31137724550898, 
    5.30913978494624, 4.73282442748092, 4.7065868263473, 5.18518518518519, 
    4.44909344490934, 4.55344070278185, 5.33557046979866, 5.37695590327169, 
    4.96981891348088, 4.60559796437659, 5.21172638436482, 5.10909090909091, 
    4.96318114874816, 5.19753086419753, 4.83164983164983, 4.62227912932138, 
    4.54188481675393, 4.79136690647482, 4.99156829679595, 4.67706013363029, 
    4.2027027027027, 4.81313703284258, 4.7979797979798, 4.29109159347553, 
    4.69798657718121, 4.48275862068965, 4.81012658227848, 4.86254295532646, 
    4.85776805251641), IV3 = c(18.4117647058824, 16.6470588235294, 
    17.8823529411765, 17.4, 18.8888888888889, 16.6875, 18.44, 
    18.85, 16.7692307692308, 19.8181818181818, 18.4736842105263, 
    17.5714285714286, 17, 17.6428571428571, 18.047619047619, 
    18.4761904761905, 16, 19.75, 18.0833333333333, 14.0357142857143, 
    18.1176470588235, 17.7222222222222, 16.3684210526316, 17.6666666666667, 
    18, 17.6428571428571, 16.4545454545455, 17.7777777777778, 
    16.5294117647059, 17.7368421052632, 18.304347826087, 15.9444444444444, 
    18.05, 17.35, 16.65, 17.4117647058824, 17.5, 14.8095238095238, 
    15.7407407407407, 16.7647058823529, 19, 17.5, 16.25, 12.6666666666667, 
    18.8666666666667, 16.4444444444444), IV4 = c(36.1498372555604, 
    29.4083258687817, 30.8237670147563, 30.6498598318535, 33.4609173395659, 
    33.8926923968973, 38.2607303267824, 35.6276276774419, 28.0350077556936, 
    29.8014773776547, 34.7520103344474, 34.2427136374588, 28.4133063999497, 
    30.8454122703714, 25.3434749682913, 31.7741243673625, 26.4398342610694, 
    30.0799370487075, 25.6232533845839, 22.8932982631499, 27.5351660757584, 
    40.9484363905134, 29.9603223354011, 41.4730169772058, 46.6645863988013, 
    25.463413385222, 38.0071453548843, 37.8674109498922, 25.1711130821037, 
    31.105850465851, 28.6739482669466, 33.4120721890402, 35.7158076753924, 
    29.1595387321196, 35.8018355105914, 33.826314306969, 31.3280107842738, 
    28.6294396753682, 26.8677745442452, 33.1564492854815, 23.3944257683981, 
    32.3845970978919, 21.6765100540051, 32.8688639919967, 29.5649229611823, 
    29.9880309522896), IV5 = c(4338.66666666667, 5806, 2450, 
    2624, 2259, 2831.5, 2879.66666666667, 4106.5, 4867, 8250, 
    3824.8, 3431, 1651.2, 2902.25, 3118, 1857.66666666667, 8377.2, 
    2022.33333333333, 4010, 5754.6, 5641, 4042.25, 5745.4, 5008.5, 
    6145.5, 3233.5, 2007.6, 2475.75, 1884.25, 6938.25, 3475, 
    6575.83333333333, 5564, 5474.8, 5665.75, 6787.66666666667, 
    3560.14285714286, 5707.4, 6354.25, 6548.75, 6883.66666666667, 
    7444.5, 6051.33333333333, 2750.5, 4788, 7105.2)), row.names = c(NA, 
-46L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
</blockquote>
",2020-12-11 12:37:25.730
332808,254239.0,1,333708.0,,,Dominance analysis in linear regressions with ARIMA errors,<regression><arima><importance><dynamic-regression>,CC BY-SA 4.0,"<p>I have a question regarding dominance analysis in linear regressions with ARIMA errors. I am currently working with stress models for the banking industry. In certain cases, we are using dynamic regressions for these purposes using macroeconomic data, such as exogenous variables. We want to assess the relative importance of the macroeconomic variable within the fitted equation. In any case, the macroeconomic variable should have a higher relative importance than the ARIMA errors.</p>
<p>I have been doing some research online, but I could not find anything related to dynamic regressions. On the contrary, for linear regressions I have found a variety of methods: standardized coefficients, Shapley regressions and dispersion importance (Johnson and Lebreton - 2004). I have found a R package &quot;relaimpo&quot; that implements some of these methods.</p>
<p>Are you aware of any methodology developed so far regarding this matter?</p>
",2020-12-18 08:52:10.763
333279,219155.0,1,,,,MSE of correlations,<optimization><loss-functions><constrained-optimization>,CC BY-SA 4.0,"<p>These might be dumb questions but I am having trouble to wrap my head around of a particular problem. I have a sparse count matrix <span class=""math-container"">$G $</span> that I want to optimize which is <span class=""math-container"">$N \times p$</span>. Also, I have correlation matrix <span class=""math-container"">$C_{ij}$</span> which is <span class=""math-container"">$N \times N$</span> that I want matrix <span class=""math-container"">$G$</span> to be optimized for. So, without any constraints I have the following loss function:</p>
<p><span class=""math-container"">$$ \text{loss} = \sum_i\sum_j (\operatorname{corr}(G_i, G_j) - C_{ij})^2 $$</span></p>
<p>where <span class=""math-container"">$G_i$</span> is a vector <span class=""math-container"">$1 \times p$</span> and  <span class=""math-container"">$i = j = 1,\ldots,N$</span>.</p>
<p>So, my first question is <strong>how can I solve/implement this particular problem</strong>?</p>
<p>My second question is related to my lack of knowledge regarding defining optimization problems :)</p>
<p>First of all, as I mentioned above <span class=""math-container"">$G$</span> is a count matrix and it is very sparse and I also want to keep the distribution of each <span class=""math-container"">$G_i$</span> while optimizing, not just randomly change it as it is described above because the initial values are important. So, my question is that <strong>is there a way for me to add these information into the problem design</strong>?</p>
",2020-12-23 19:04:16.020
333299,254734.0,1,,,,Comparing coefficient across groups in 3 way linear mixed model,<mixed-model><regression-coefficients><cohens-d><wald-test>,CC BY-SA 4.0,"<p>I wonder how to compare coefficients across group in linear mixed model.</p>
<p>Dependent variable (DV, continuous variable) is changed by time variable (T, continuous, fixed effect).
And hypothesis is that 2 moderator variables (MV1 and MV2, categorical by 0 or 1, independent variable, fixed effects) affect the association of DV with T.
There is one covariate as continuous variable
Study participants (S) are considered as random effect.</p>
<pre><code>m &lt;-lmer(DV ~ MV1*MV2*T + (1|S) + CV, REML=T, data=data)
</code></pre>
<p>And result was</p>
<pre><code>Fixed effects:
                                       Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)                            18.55909    4.74223  300.00698   3.914 0.000113 ***
MV1                                   -5.92450    1.32025  318.51178  -4.487 1.01e-05 ***
MV2                                   -0.65531    1.39709  324.53991  -0.469 0.639349    
T                                      0.14929    0.01166 1518.32489  12.804  &lt; 2e-16 ***
CV                                    -0.03221    0.06305  298.48239  -0.511 0.609864    
MV1:MV2                                3.24588    1.89431  321.91658   1.713 0.087584 .  
MV1:T                                 -0.09325    0.01333 1517.30915  -6.997 3.91e-12 ***
MV2:T                                  0.09113    0.01780 1517.49347   5.120 3.44e-07 ***
MV1:MV2:T                             -0.05137    0.02134 1518.24445  -2.407 0.016204 *
</code></pre>
<p>I found that interaction term (MV1 X MV2 X T) was significant.
When i divide whole population in 4 groups, like<br />
MV1 = 0 and MV2 = 0<br />
MV1 = 1 and MV2 = 0<br />
MV1 = 0 and MV2 = 1<br />
MV1 = 1 and MV2 = 1</p>
<p>How can i statistically compare coefficients of T on DV across 4 groups?<br />
I wonder how MV1 moderates the association of MV2 X T with DV<br />
and how MV2 moderates the association of MV1 X T with DV.</p>
<p>And i would like to know hot to express the group difference of coefficient in publish.
In previous articles, they used Wald test or Cohen's d.</p>
",2020-12-23 23:47:53.303
333419,66768.0,1,,,,How to set up a DL classification model so that it selects from an ever changing menu,<machine-learning><time-series><neural-networks><classification><siamese>,CC BY-SA 4.0,"<p><em>The question is edited for clarity after tchainzzz's comments about meta-learning.</em></p>
<p>Let's say we have 10,000 pet pictures and 10,000 kids. Each kid is presented with 10 randomly picked pet pictures at a time. Each time, they have to pick the one picture that they like best. Our goal, during inference, is to predict probabilities on which picture (from 0 to 9) the (same) kids will pick. My struggle is how to construct a NN to make this classification.</p>
<p>Paths I've been thinking about or tried:</p>
<ul>
<li><p>I have created embeddings for kids and pictures using a (Netflix competition winner style) factorization method. The embeddings are pretty good: Visualizing the picture embeddings in a projector, similar pets are grouped together.</p>
</li>
<li><p>The first thing I tried was to concatenate the embeddings of the 10 pictures and feed this together with the embedding of the kid. The output layer a softmax and CE loss. But it doesn't work - I guess it's to difficult for the model to &quot;understand&quot; where one picture embedding starts and another stops, and to relate each of the embeddings to the 10 categories in the output layer.</p>
</li>
<li><p>tchainzzz pointed me in the direction of meta-learning, including few-shot learning (before I had clarified my case). But these methods are mainly intended for classifying the entities (is the pet a dog or a cat?) and they are intended for limited training sets. In our case, we're not classifying the pictures (we already know which ones are cats and which ones are dogs) and we have ample training data.</p>
</li>
<li><p>Why not use metric learning with siamese networks? I don't think it will work here, because this method assumes that there is one ideal pet that each kid would select, and we just need to figure out which picture is more like that ideal pet. But we don't have an ideal pet for each kid, only the previously performed selections.</p>
</li>
<li><p>Why not use some kind of ranking solution? (We could probably create a system, like elo chess ranking. Every time a kid selects a picture, that picture would get a higher ranking, particularly for that kid, and more so if the competing picture already have a high ranking.) Because that's not a neural network classification architecture. I can add such a ranking as a feature, but the question is how to create an NN model so that it &quot;understands&quot; that the classification should happen from a menu of 10 available dishes.</p>
</li>
<li><p>There is, however, elements from 'siamese networks' that I've been thinking about. Not the metric part of siamese architechture, but the 'shared weights' part: A possible solution to my problem could be that I insert the embedding for the kid next to the embedding of one picture (i from 0 to 9) into 10 siamese twin networks (i from 0 to 9) sharing the same weights. Each twin would have one output mapped to 10 classes in a softmax layer. (The softmax layer is on the outside of the siamese part of the network.) I have tried this quickly, without much luck. But so far, this is my best idea and i'm continuing to work in this direction.</p>
</li>
</ul>
<p>Any further advice or ideas would be welcome!</p>
",2020-12-25 22:58:48.770
333536,254958.0,1,333686.0,,,Forecasting in R without auto.arima(),<r><forecasting><arima>,CC BY-SA 4.0,"<p>I am trying to forecast data regarding vehicle registrations year-wise using <code>auto.arima</code> in R. However, one of my variables (which is data for 3-wheeler registrations) gives me the same forecast: I used <code>auto.arima</code> for it too but the process generated was an ARIMA(0,0,0) process due to which the forecast values were the same throughout. For another similar variable, I got an ARIMA(0,1,0) process and my point forecasts were the same. So my question is, is there any other method of forecasting single variables without using the <code>auto.arima</code> function? Your responses and help will be much appreciated, thanking you all in anticipation!!</p>
<p><a href=""https://i.stack.imgur.com/Jll7m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jll7m.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/Ss2GX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ss2GX.png"" alt=""enter image description here"" /></a></p>
",2020-12-27 14:53:36.730
333537,170017.0,1,333820.0,,,Probability that Secret Santa arrangement will result in perfect pairings for couples,<combinatorics><permutation>,CC BY-SA 4.0,"<p>4 couples, 8 people total, participate in a Secret Santa gift exchange. Call the people A, B, C, D, E, F, G, H. Assume A+B are a married couple.  Likewise, assume C+D, E+F, and G+H are all couples. All 8 people put their names on a piece of paper and then the people randomly draw names from the bowl.  If a person draws their own name, they hold onto the paper with their name while drawing a second piece of paper, then they replace the piece of paper with their own name. This ensures that nobody draws their own name until the last person. When the last person draws, if the name in the bowl is their own, then that person switches names with the second last person to draw.  This is guaranteed to be valid because the second to last person did not choose the last person.  Thus, after switching the last person will have a valid name and the second to last person will have the last person's name.<br />
a) Assume they draw in this order: A, B, C, D, E, F, G, H.  What is the probability that everyone draws their own partner's name?<br />
b) Assume they draw in a random order.  What is the probability that everyone draws their own partner's name?</p>
",2020-12-27 15:33:22.287
333564,,1,,,user306613,Q-function in Q-Learning,<machine-learning><reinforcement-learning><supervised-learning><pattern-recognition><semi-supervised-learning>,CC BY-SA 4.0,"<p>I ran into solved old-exam question as follows:
<a href=""https://i.stack.imgur.com/sH9up.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sH9up.png"" alt=""enter image description here"" /></a></p>
<p>My notes tell me that option b is correct but I think option d is correct. is there any idea why (b) is correct?</p>
",2020-12-27 20:34:34.003
333601,254990.0,1,333713.0,,,Regressor overestimates low values and underestimates high values,<regression><machine-learning><random-forest><bias>,CC BY-SA 4.0,"<p>I have preprocessed the data and trained a regressor (random forest). Then i made a predicted v/s real values plot, to see the model behavior. Here, i noted that the regressor consistenly overestimates low values and underestimates high values:
<a href=""https://i.stack.imgur.com/2fJxW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2fJxW.png"" alt=""enter image description here"" /></a></p>
<p>¿Is there any wat to fix this bias? I tried adding manually a bias to the predictions (by summing) and the metrics actually got better, but i don't think that is the correct way.</p>
<p>Thanks!</p>
",2020-12-28 12:33:27.493
333613,211479.0,1,333697.0,,,Why does the PyTorch tutorial on DQN define state as a difference?,<machine-learning><reinforcement-learning><q-learning>,CC BY-SA 4.0,"<p>I'm a master's student in EECS working my way towards understanding how DQN [0] works.</p>
<p>I'm working towards solving the CartPole-v0 task in as few iterations as possible.</p>
<p>First of all I implemented a basic Q-learning algorithm which took forever to converge, then I added decaying learning rate and experimentation proportion which made a whole lot of difference. I'm not interpreting state from the image yet, I just take an observation and discretize it to simplify things - one complication at a time.</p>
<p>I'm now trying to add experience replay but keep the Q-matrix approach - I will substitute it with a function-approximation ANN later. I'm wondering why the seventh code snippet in the PyTorch tutorial on DQN, the second under section &quot;training loop&quot; [1] is representing state as a difference between two screens. Right now I'm not doing this, and my replay memory implementation is not helping learning - quite the opposite. Of course, a good RL algorithm is independent from the representation of state and the error is probably somewhere else - especially since my Q-learning algorithm does learn if I avoid using replay memory - but this tickled my curiosity. I skimmed over the DQN paper but did not find any references to such a representation. I'll admit that I haven't read it in detail yet because I don't want to get more confused, so I might have missed it.</p>
<p>Is there a specific reason for this kind of representation? Does it only make sense in the context of translating an image to state?</p>
<p>Thanks in advance!</p>
<p>[0] <a href=""https://doi.org/10.1038%2Fnature14236"" rel=""nofollow noreferrer"">https://doi.org/10.1038%2Fnature14236</a></p>
<p>[1] <a href=""https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html</a></p>
",2020-12-28 14:07:41.880
333630,188602.0,1,333674.0,,,Slope Coefficient on Exogenous Variables in 2SLS and Directly Regressing the instrument,<econometrics><instrumental-variables>,CC BY-SA 4.0,"<p>Suppose we have <span class=""math-container"">$k_1$</span> exogenous variables <span class=""math-container"">$X_1$</span>, <span class=""math-container"">$k_2$</span> endogenous variables <span class=""math-container"">$X_2$</span> and <span class=""math-container"">$k_2$</span> instruments <span class=""math-container"">$Z$</span> and the model <span class=""math-container"">$Y=X_1\beta_1+X_2\beta_2+e$</span>. Let the 2SLS estimates be <span class=""math-container"">$(\hat\beta_1,\hat\beta_2). $</span>If we directly run a regression with the instruments instead <span class=""math-container"">$Y=X_1\alpha_1+Z\alpha_2+e$</span>, then the claim is that <span class=""math-container"">$\hat\alpha_1=\hat\beta_1$</span> i.e. the estimated coefficient on the exogenous variables are the same.</p>
<p>I'm having trouble formally proving the result as well as understanding the intuition. I tried applying the Frisch–Waugh–Lovell theorem so that <span class=""math-container"">$\hat\alpha_1=(X_1'M_ZX_1)'(X_1'M_ZY)$</span> where <span class=""math-container"">$M_Z=(I-Z(Z'Z)^{-1}Z')$</span> and then tried showing that <span class=""math-container"">$\hat\beta_1$</span> will be equal to that. However, applying the FWL theorem in the 2SLS context, <span class=""math-container"">$\hat\beta_1=(X_1'M_{\hat{X_2}}X_1)'(X_1'M_{\hat{X_2}}Y)$</span>. This would require <span class=""math-container"">$M_{\hat{X_2}}=M_Z$</span>. However, I can't see why the two should be equal. Furthermore, that would imply that the residuals from regressing <span class=""math-container"">$X_1$</span> on <span class=""math-container"">$Z$</span> and <span class=""math-container"">$X_1$</span> on <span class=""math-container"">$\hat{X}_2$</span> would be the same which doesn't seem to make sense either since the predicted value <span class=""math-container"">$\hat{X}_2$</span> uses both <span class=""math-container"">$X_1$</span> and <span class=""math-container"">$Z$</span>. What am I missing here?</p>
",2020-12-28 17:54:38.210
333634,218101.0,1,,,,A question about a logistic regression classifier performance (with and without resampling),<logistic><classification><unbalanced-classes><scoring-rules><smote>,CC BY-SA 4.0,"<p>I am working on a dataset with 20 independent variables and 41188 instances. The task is a binary classification where the target variable has 36548 number of <em>no</em>'s and 4640 of <em>yes</em>'s. I have used logistic regression model with 10 folds of cross validation. Since the target variable is unbalanced, I decided to resample data. I made the model 3 times: first without resampling data, then resampling data once with under-sampling technique and once with SMOTE technique. Following are the reports gained:</p>
<p><strong>- Without resampling:</strong>
<a href=""https://i.stack.imgur.com/iZOVQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iZOVQ.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>- With Under-sampling technique</strong>
<a href=""https://i.stack.imgur.com/juad7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/juad7.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>- With SMOTE technique:</strong>
<a href=""https://i.stack.imgur.com/KEcJO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KEcJO.png"" alt=""enter image description here"" /></a></p>
<p>Usually with unbalanced data, <em>accuracy</em> alone is not sufficient to evaluate the performance of the classifier and thus <em>precision, recall</em> and <em>ROC</em> values should be taken into accounts as well. The first model made without any resampling techniques, delivers weighted average higher accuracy, precision and recall values than the the others while its ROC value is slightly less than the model with SMOTE resampling technique. Moreover, in the first model the recall value of class <em>yes</em> (0.423) is much lower than the recall value of class <em>no</em> (0.973).</p>
<p>My question is which model is more trustable? and why the accuracy, precision and recall values were decreased after resampling the data?</p>
",2020-12-28 19:00:25.707
333640,66768.0,1,333715.0,,,Probability of at least one success in a long string of connected events,<probability><bayesian><conditional-probability><bayesian-network><markov-random-field>,CC BY-SA 4.0,"<p>I have N events (i from 1 to N), each with an estimated probability of success, p(i).</p>
<p>If all my events were independent I'd be able to calculate the probability of at least one success as (1 - product of (1 - p(i))). But some events are not independent, such as {3,4,5}, {4,5,6} and {5,6,7}.</p>
<p>Within each of these groups, I know all conditional probabilities, such as p(3|4) =70%, p(3|5) = 30%, etc.</p>
<p>Now to the big question: <strong>Does it exist a formula to calculate the overall probability of at least one success?</strong> (A &quot;closed&quot; formula would be nice. But efficiency in calculation is more important!)</p>
",2020-12-28 20:00:35.957
120255,58559.0,2,,102770.0,,,,CC BY-SA 3.0,"<p>you can only get the prediction accuracy on any dataset, if you have the true classes / targets. so in a real world application of your model to a completely unknown testset without classes, you cannot get the prediction error. the prediction error is derived from the true classes.</p>

<p>but you can use your trainingset and split it into test / validation parts, to get prediction accuracy.</p>
",2015-09-01 09:35:33.257
333647,129041.0,1,333752.0,,,Which one is more likely to be random walk?,<time-series><random-walk>,CC BY-SA 4.0,"<p>Consider the two series in the chart below: <span class=""math-container"">$walkA$</span> and <span class=""math-container"">$walkB$</span>.</p>
<p>They are based on the same steps, although the steps come in a different order.</p>
<p>Indeed, <span class=""math-container"">$stepsA$</span> and <span class=""math-container"">$stepsB$</span> have identical sample mean <span class=""math-container"">$\hat \mu=0.5$</span> and sample std dev <span class=""math-container"">$\hat \sigma=3$</span>.</p>
<p>However, from a visual inspection, I would intuitively say that <span class=""math-container"">$walkA$</span> is more likely to be a random walk than <span class=""math-container"">$walkB$</span>.</p>
<p>Is it there a mathematical method to make such claims?</p>
<p><a href=""https://i.stack.imgur.com/kXgJk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kXgJk.png"" alt=""enter image description here"" /></a></p>
",2020-12-28 22:08:08.933
333649,37792.0,1,333658.0,,,Sampling from an AR(1) process using normal samples,<r><time-series><sampling><autoregressive>,CC BY-SA 4.0,"<p>This is probably a very straight forward question but I want to verify how I should sample from an AR(1) process in R using just the <code>rnorm()</code> function in R (or any similar function in another language). Say I want to sample 100 samples from the following AR(1) model:</p>
<p><span class=""math-container"">$$
X_t = \rho X_{t-1} + \epsilon_t, \qquad \epsilon_t \sim N(0, \sigma^2_\epsilon).
$$</span></p>
<p>Then the joint distribution of the vector <span class=""math-container"">$(X_1,\dots,X_{100})$</span> is multivariate Gaussian with zero mean and covariance matrix <span class=""math-container"">$\Sigma$</span>, where</p>
<p><span class=""math-container"">$$
BB^T :=\Sigma = \frac{\sigma^2_\epsilon}{1-\rho^2}
\begin{bmatrix}
1 &amp; \rho &amp; \rho^2 &amp; \rho^3 &amp; \rho^4&amp;\cdots\\
\rho &amp; 1 &amp;\rho &amp; \rho^2 &amp;  \rho^3 &amp;\cdots\\
\rho^2 &amp;\rho &amp; 1 &amp;\rho &amp;    \rho^2&amp;\cdots\\
\rho^3 &amp;\rho^2 &amp;\rho &amp; 1 &amp;    \rho&amp;\cdots\\
\vdots &amp;\vdots &amp;\vdots&amp; \vdots &amp;\ddots&amp;    \vdots\\
\end{bmatrix}
$$</span>
So I should be able to sample in R using the following:</p>
<pre><code>X = matrix(rnorm(100, 0, sigmaeps), 1, 100) %*% B
</code></pre>
<p>Is this approach correct?</p>
",2020-12-28 22:37:31.573
333653,254365.0,1,,,,"What is the probability that in 3 rolls of a pair of six-sided dice, exactly one total of 7 is rolled?",<probability><self-study><dice>,CC BY-SA 4.0,"<p>I tried to solve the exercise and it seems meaningful, but the problem is that in the book the result is <span class=""math-container"">$25/72$</span>, while mine is <span class=""math-container"">$1/7776$</span>. I don't know if maybe I didn't understand the problem, but by the text I understood that there are two dices rolled three times, and I have to calculate the probability to have 7 as result. Now, I could use the binomial probability density function, but I didn't know very well how to use the data I have (if someone can also explain me the resolution with this structure I'll thank him). By logic, throwing three times two dices, the total number of possibilities is 46'656, because I have <span class=""math-container"">$36^3$</span>. Now, to have 7 as result and a dice must have at least 1 as value, the only possibility that I have to reach 7 is 11/11/12 (the first two dices are 1, the 3th and 4th 1, the 5th 1 and the 7th 2) and all the combination of this sequence:
11/11/12
11/11/21
11/21/11
11/12/11
21/11/11
12/11/11</p>
<p>Calculating the probability, I have <span class=""math-container"">$3* (1/36 * 1/36 * 2/36)$</span>, and the probability of having 7 as result is <span class=""math-container"">$6/7776$</span></p>
<p>I don't know where I'm wrong.</p>
",2020-12-28 23:51:40.400
333655,255099.0,1,,,,Does convergent cross-mapping require you to control for other variables?,<time-series><causality><granger-causality><controlling-for-a-variable>,CC BY-SA 4.0,"<p>There's a really cool method called convergent cross-mapping (<a href=""https://link.springer.com/chapter/10.1007/978-3-319-58895-7_27"" rel=""nofollow noreferrer"">Tsonis et al. (2018)</a>) that's used to see if two time-series are causally linked within a dynamic system. It seems really powerful and like it might have a wide range of applications.</p>
<p>I'm wondering if it's sufficient to just &quot;plug in&quot; two variables you're interested in and go, or if you need to control for potential omitted variables like you do in multivariate regression studies.</p>
<p>Thanks!</p>
<hr />
<p>Tsonis, Anastasios A., Ethan R. Deyle, Hao Ye, and George Sugihara. <em>Convergent cross mapping: theory and an example.</em> In Advances in nonlinear geosciences, pp. 587-600. Springer, Cham, 2018.</p>
",2020-12-28 23:56:56.197
8504,115.0,2,,8478.0,,,,CC BY-SA 3.0,"<p>The KL-divergence is typically used in information-theoretic settings, or even Bayesian settings, to measure the information change between distributions before and after applying some inference, for example. It's not a distance in the typical (metric) sense, because of lack of symmetry and triangle inequality, and so it's used in places where the directionality is meaningful. </p>

<p>The KS-distance is typically used in the context of a non-parametric test. In fact, I've rarely seen it used as a generic ""distance between distributions"", where the $\ell_1$ distance, the Jensen-Shannon distance, and other distances are more common. </p>
",2011-04-08 04:07:17.223
9529,192.0,2,,9524.0,,,,CC BY-SA 3.0,"<p><a href=""http://www.imdb.com/title/tt0138704/"">Pi</a></p>
",2011-05-07 14:19:42.887
80677,20473.0,2,,80672.0,,,,CC BY-SA 4.0,"<p>Stylistic conventions, mainly, but with some underlying rationale.</p>
<p><span class=""math-container"">$\mathbb{P}()$</span> and <span class=""math-container"">$\Pr()$</span> can be seen as two ways to &quot;free up&quot; the letter <span class=""math-container"">$\text{P}$</span> for other use—it <em>is</em> used to denote other things than &quot;probability&quot;, for example in research with complicated and extensive notation where one starts to exhaust available letters.</p>
<p><span class=""math-container"">$\mathbb{P}()$</span> requires special fonts, which is a disadvantage. <span class=""math-container"">$\Pr()$</span> may be useful when the author would want the reader to think of probability in abstract and general terms, using the second lower-capital letter &quot;<span class=""math-container"">$r$</span>&quot; to disassociate the symbol as a whole from the usual way we write up functions.</p>
<p>For example, some problems are solved when one remembers that the cumulative distribution function of a random variable can be written and treated as a probability of an &quot;inequality-event&quot;, and apply the basic probability rules rather than functional analysis.</p>
<p>In some cases, one may also see <span class=""math-container"">$\text {Prob}()$</span>, again, usually in the beginning of an argument that will end up in a specific formulation of how this probability is functionally determined.</p>
<p>The <em>italics</em> version <span class=""math-container"">$P()$</span> is also used, and also in lower-case form, <span class=""math-container"">$p()$</span>—this last version is especially used when discussing discrete random variables (where the <em>probability mass function</em> <em>is</em> a probability).</p>
<p><span class=""math-container"">$\pi(\;,\;)$</span> is used for conditional (&quot;transition&quot;) probabilities in Markov Theory.</p>
",2014-07-18 18:15:52.797
182191,7483.0,2,,182063.0,,,,CC BY-SA 3.0,"<p>This scheme is called the <a href=""https://en.wikipedia.org/wiki/Brier_score"" rel=""nofollow noreferrer"">Brier loss</a>. It is a <a href=""https://en.wikipedia.org/wiki/Scoring_rule"" rel=""nofollow noreferrer"">proper scoring rule</a>, and hence only the optimal classifier is correct, etc. It corresponds, of course, to the $L_2$ distance between the predictive label distribution and the true label distribution (which is a point mass).</p>

<p>Deep learning types these days strongly prefer the cross-entropy loss, which corresponds to the KL divergence $KL( y \| \hat y)$. This will penalize giving very low probabilities to the correct class very harshly, perhaps encouraging a flattening out of predicted probabilities relative to the Brier loss.</p>

<p>Consider a $K$-way classification problem, where your estimate of the probability of the $i$th class is $\hat p_i$.
Let $y$ be the correct label for a given instance $x$, and $B = (\hat p_y(x) - 1)^2$ the Brier loss. Then
$$\nabla B = 2 (\hat p_y(x) - 1) \nabla \hat p_y(x),$$
whereas if $C(x, y, w) = - \log \hat p_y(x)$ is the cross-entropy loss, then
$$\nabla C = - \frac{1}{\hat p_y(x)} \nabla \hat p_y(x).$$
Plotting these:
<a href=""https://i.stack.imgur.com/fBCay.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fBCay.png"" alt=""enter image description here""></a></p>

<p>We can thus see that the cross-entropy really emphasizes wrong values, whereas Brier loss scales just linearly with the probability estimate.</p>



<p>Another interesting property: suppose that there are three categories, with the first one being correct. Cross-entropy would value the predictions $(.8, .2, 0)$ and $(.8, .1, .1)$ equally, whereas Brier loss would prefer the second one. I don't know if that's of huge practical importance, but only caring about the true category seems like a reasonable criterion to me, and that leads to cross-entropy being the only proper scoring rule.</p>
",2017-03-12 13:49:18.100
212847,5179.0,2,,212670.0,,,,CC BY-SA 3.0,"<p>The <a href=""http://www.nrbook.com/devroye/"" rel=""noreferrer"">most direct way of simulating a random variable</a> from a distribution with cdf $F$ is to first simulate a Uniform variate $U\sim\mathcal{U}(0,1)$ and second return the inverse cdf transform $F^{-1}(U)$. When the inverse $F^{-1}$ is not available in closed form, a <a href=""https://link.springer.com/chapter/10.1007/978-3-662-05946-3_7"" rel=""noreferrer"">numerical inversion</a> can be used. Numerical inversion may however be costly, especially in the tails.</p>

<p>One can also use <a href=""http://www.nrbook.com/devroye/"" rel=""noreferrer"">accept-reject algorithms</a> when the density $f$ is available and dominated by another density $g$, i.e., that there exists a constant $M$ such that$$f(x)&lt;M g(x)$$. In the case of the skew-Normal distribution, the density is$$f(x)=2\varphi(x)\Phi(\alpha x)$$when $\varphi$ and $\Phi$ are the pdf and cdf of the standard Normal distribution, respectively. (Adding a location and a scale parameter does not modify the algorithm, since the outcome simply needs to be rescaled and translated.)</p>

<p>This density seems ideally suited for accept-reject since$$2\varphi(x)\Phi(\alpha x)&lt; 2\varphi(x)$$as $\Phi$ is a cdf. This inequality implies that a first option to run accept-reject is to pick the Normal pdf for $g$ and $M=2$. This works out, as shown by the following picture when $\alpha=-3$:</p>

<p><a href=""https://i.stack.imgur.com/Gzerq.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Gzerq.jpg"" alt=""enter image description here""></a></p>

<p>and leads to an algorithm of the kind</p>

<pre><code>T=1e3 #number of simulations
x=NULL
while (length(x)&lt;T){
   y=rnorm(2*T)
   x=c(x,y[runif(2*T)&lt;pnorm(alpha*y)])}
x=x[1:T]
</code></pre>

<p>which returns a reasonable fit of the pdf by the histogram:</p>

<p><a href=""https://i.stack.imgur.com/7yn7c.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7yn7c.jpg"" alt=""enter image description here""></a></p>

<p>There are however <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.588.8&amp;rep=rep1&amp;type=pdf"" rel=""noreferrer"">transforms of standard distributions</a> that result in skew Normal variates: If $X_1,X_2$ are iid $\mathcal{N}(0,1)$, then</p>

<ol>
<li>$$\dfrac{\alpha|X_1|+X_2}{\sqrt{1+\alpha^2}}$$</li>
<li>$$\dfrac{1+\alpha}{\sqrt{2(1+\alpha^2)}}\max\{X_1,X_2\}+\dfrac{1-\alpha}{\sqrt{2(1+\alpha^2)}}\min\{X_1,X_2\}$$</li>
</ol>

<p>are skew Normal variates with parameter $\alpha$. (Both representations are identical when considering that $(X_1+X_2,X_1-X_2)/\sqrt{2}$ is an iid $\mathcal{N}(0,1)$ pair.)</p>
",2017-11-29 18:27:29.380
333856,243325.0,2,,333855.0,,,,CC BY-SA 4.0,"<p>The Ljung Box test does not test for stationarity. It tests if the autocorrelations up to lag m are jointly equal to 0. It is always possible to have a stationary time-series that consists of significant autocorrelations. Meaning the existence of significant autocorrelations does not imply that the process is not stationary or vice versa.</p>
<p>Edit:</p>
<p>Definition of covariance stationary:</p>
<p>AutoCovariance Function:</p>
<p><span class=""math-container"">$$\gamma (t,s) = Cov(Y_t, Y_s) = E((Y_t - \mu_t)(Y_s - \mu_s))$$</span></p>
<p>A stochastic process is covariance stationary if:
<span class=""math-container"">$$E(Y_T^2) \text{  is Finite}$$</span>
<span class=""math-container"">$$E(Y_T) = \mu$$</span>
For any <span class=""math-container"">$t$</span> and <span class=""math-container"">$s$</span>:
<span class=""math-container"">$$\gamma (s, t) = \gamma_k \quad with \quad k = t - s$$</span></p>
<p>Meaning covariance between 2 coordinates depend on time gap between 2 points rather than specific locations of the 2 points.</p>
<p>Example:
<span class=""math-container"">$$\gamma (1, 2) = Cov(Y_1, Y_2) = Cov(Y_{99}, Y_{100}) = \gamma (99, 100)$$</span></p>
<p>Implications of covariance stationary process:
<span class=""math-container"">$$\gamma (t, t) = \gamma (s, s) = Var(Y_t) = \sigma^2 \quad \forall t$$</span></p>
<p>Consider AR(1) Model:
<span class=""math-container"">$$Y_t = c + \phi_1 y_{t-1} + \epsilon_t$$</span></p>
<p>AR(1) Properties:</p>
<p>Mean:
<span class=""math-container"">$$E(Y_t) = c + \phi_1E(Y_{t-1}) \implies \mu = c + \phi_1 \mu \implies \mu = \frac{c}{1-\phi_1}$$</span></p>
<p>Variance:
<span class=""math-container"">\begin{equation}
\begin{split}
Var(Y_t) &amp; = Var(c + \phi_1 Y_{t-1} + \epsilon_t)\\
&amp; = Var(\phi_1 Y_{t-1} + \epsilon_t)\\
&amp; \implies \sigma^2 = \phi_1^2\sigma^2 + \sigma_{\epsilon}^2\\
&amp; \implies \sigma^2 = \frac{\sigma^2_{\epsilon}}{1-\phi^2_1}\\
&amp; = \sigma^2_{\epsilon}\sum_{i=0}^{\infty} \phi_1^{2i}
\end{split}
\end{equation}</span></p>
<p>AutoCovariance:</p>
<p><span class=""math-container"">$$Cov(Y_t, Y_{t-k}) = \frac{\phi^k_1\sigma^2_{\epsilon}}{1- \phi^2_1}$$</span></p>
<p>Where AutoCorrelation:
<span class=""math-container"">$$p_k = \frac{\gamma_k}{\gamma_0} = \phi^k_1$$</span>
Meaning AutoCorrelation decreases exponentially and PACF is 0 everywhere expect lag 1.</p>
<p>Given <span class=""math-container"">$|\phi_1| &lt; 1$</span>, mean is constant, variance is constant meaning it is finite and autocovariance only depends on <span class=""math-container"">$k$</span>. Thus AR(1) process is stationary, yet autocorrelation exists and decays 0 to exponentially. Thus AR(1) is an example where process is stationary and has significant autocorrelations.</p>
",2020-12-31 12:27:28.703
333874,170017.0,2,,333717.0,,,,CC BY-SA 4.0,"<p>The difference between the MLE for the variance and <span class=""math-container"">$s^2$</span> is dividing by <span class=""math-container"">$N$</span> instead of by <span class=""math-container"">$N-1$</span>. Therefore, the MLE is <span class=""math-container"">$\sqrt{\frac{N-1}{N}}s$</span>.  Hence, <span class=""math-container"">$$\sqrt{\frac{N}{N-1}}\times MLE/c_4(N)=s/c_4(N)$$</span>
is unbiased.</p>
",2020-12-31 19:33:49.397
333879,126602.0,2,,333790.0,,,,CC BY-SA 4.0,"<p>In general, if you don't have a dataset that contains those 7 classes, you wouldn't be able to get a neural network which performs your task of semantic segmentation.</p>
<p>My understanding of your question is the following:</p>
<ul>
<li>you own a dataset, each image labelled with a single class label</li>
<li>you are able to train a CNN or similar structure with these images</li>
<li>you want that neural network to perform semantic segmentation for you at inference</li>
</ul>
<p>This task seems to be a &quot;zero-shot learning task&quot; for image segmentation trying to leverage related image filters for the task.</p>
<p>You could try training a neural network for your classification task and then replacing the last layers with randomly initialised layers to provide the segmentation. However, that would hardly provide any useful performance for you.</p>
<p>Most likely, an already trained image segmentation model would be the best way to proceed in your case. The main problem is that you have in my opinion is that you won't be able to get away with segmenting at least part of the masks if you have completely novel segmentation classes, otherwise you won't be able to claim what method works for your dataset in a quantitative way.</p>
<p>If you really have to do the problem zero shot, I would try to look at zero shot segmentation papers. For example, <a href=""https://github.com/bcmi/CaGNet-Zero-Shot-Semantic-Segmentation"" rel=""nofollow noreferrer"">this work</a> might be interesting for your case. In their work, they exploit relations between semantic word embeddings to alleviate annotation needs where generalisation to new classes are required.</p>
",2020-12-31 20:42:54.147
333882,55621.0,2,,333832.0,,,,CC BY-SA 4.0,"<p>Jan Beirlant et al. &quot;Statistics of Extremes: Theory and Applications&quot; give a few examples in their table 2.2, uniform distribution, reversed Burr. Not exactly &quot;interesting&quot;, but I will work with that and, so will close this question. Thanks.</p>
<p><a href=""https://www.google.com/books/edition/Statistics_of_Extremes/GtIYLAlTcKEC?hl=en&amp;gbpv=0"" rel=""nofollow noreferrer"">https://www.google.com/books/edition/Statistics_of_Extremes/GtIYLAlTcKEC?hl=en&amp;gbpv=0</a></p>
",2020-12-31 21:19:06.080
