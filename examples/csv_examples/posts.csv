id,owner_user_id,post_type_id,accepted_answer_id,parent_id,owner_display_name,title,tags,content_license,body,creation_date
57184,1741.0,2,,57086.0,,,,CC BY-SA 3.0,"<p>Here there are some thoughts:</p>

<ol>
<li>All black-box models might be inspected in some way. You can compute the variable importance for each feature for example or you can also plot the predicted response and the actual one for each feature (<a href=""https://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest/21457#21457"">link</a>);</li>
<li>You might think about some pruning of the ensemble. Not all the trees in the forest are necessary and you might use just a few. Paper: [Search for the Smallest Random Forest, Zhang]. Otherwise just Google ""ensemble pruning"", and have a look at ""Ensemble Methods: Foundations and Algorithms
"" <a href=""http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/publication.htm"" rel=""nofollow noreferrer"">Chapter 6</a>;</li>
<li>You can build a single model by feature selection as you said. Otherwise you can also try to use Domingos' method in [Knowledge acquisition from examples via multiple models] that consists in building a new dataset with black-box predictions and build a decision tree on top of it. </li>
<li>As mentioned in <a href=""https://stats.stackexchange.com/questions/32125/how-to-make-random-forests-more-interpretable/32132#32132"">this</a> Stack Exchange's answer, a tree model might seem interpretable but it is prone to high changes just because of small perturbations of the training data. Thus, it is better to use a black-box model. The final aim of an end user is to understand why a new record is classified as a particular class. You might think about some feature importances just for that particular record.</li>
</ol>

<p>I would go for 1. or 2.</p>
",2013-10-10 00:04:43.820
57185,,1,,,user10619,What is the difference between the concept and treatment of measurement error in psychometry and in statistics?,<mathematical-statistics><bias><measurement-error>,CC BY-SA 3.0,"<p>There is some confusion with respect to the measurement error. What is the definition in statistics and definition in psychometry ?  The statistics does not seem to recognize the measurement error popularly called construct bias in psychometry.</p>
",2013-10-10 00:46:42.743
57186,22542.0,1,,,,Sample sizes for differences between three groups,<proportion><statistical-power><group-differences>,CC BY-SA 3.0,"<p>I am doing a study of two schools, and at each school I am sampling three groups. I am trying to determine why the person chose to go to that school and not another. I will be asking each group various questions about the school and options they may  have had for other schools, etc. I want to be able to detect:</p>

<ol>
<li>a difference of 20% in responses between the schools (e.g., proportion of students for whom current school was first school of preference, or proportion for whom there was no other choice), and</li>
<li><em>within</em> each school, 20% difference in responses between the three different groups (similar questions, but for students who enrolled in different eras).</li>
</ol>

<p>In both instances, I would like power of 80% for 0.05 significance.</p>

<p>So, if I do what I think is the appropriate calculation, I effectively come up with:
\begin{align} 
n &amp;= \frac{0.5(0.84 + 1.96)^2}{0.2^2}  \\
  &amp;= 98
\end{align}
Can I just assume then that I need three groups of 99 within each school? I guess I'm confused because nobody ever seems to talk about calculating sample sizes when comparing more than two groups.  </p>

<p>Furthermore, is there anything wrong with sampling the three groups of 98 at each school and assuming that the total sample of 294 at each school will be sufficient to detect the 20% difference between the two schools?</p>
",2013-10-10 01:51:45.710
57187,9792.0,1,,,,Centering when using splines in R,<r><splines><lm><centering>,CC BY-SA 3.0,"<p>I am having trouble understanding why centering seems to only work with simple linear models and not with splines for example. I am using centering to report the estimated group differences at different $x$, but also statistical values (ignoring multiple comparisons for the moment).</p>

<pre><code>set.seed(1)

# simulate data
N &lt;- 10
x &lt;- rep(seq(0.2,1,0.2),N)
group &lt;- factor(rep(c('I','II'),each=length(x)/N))
y &lt;- -x^2 + 2*x*as.numeric(group) + rnorm(length(x),mean=0,sd=0.1)
d &lt;- data.frame(group,x,y)

# fit a linear model with x-group interaction
l &lt;- lm(y~x*group,data=d)
d$lmfit &lt;- fitted(l)
coef(l)['groupII'] # group difference at x==0
#     groupII 
#  -0.1097071 

library(ggplot2)
ggplot(d,aes(x,y,colour=group)) + geom_point() + geom_line(aes(x,lmfit,colour=group))
</code></pre>

<p>The plot confirms the reported small group difference <code>groupII</code> of 0.05 at $x=0$ if we were to extrapolate back to 0.</p>

<p>Now let us centre the data at $x=1$ and estimate the group difference there.</p>

<pre><code># center data at x==1 and refit
l &lt;- lm(y~I(x-1)*group,data=d)
coef(l)['groupII'] # group difference at x==1
#   groupII 
#  2.08525 
</code></pre>

<p>In agreement with the plot the difference is about 2.</p>

<p>Now let us fit a spline model.    </p>

<pre><code># fit data with splines
library(splines)
l &lt;- lm(y~ns(x,2)*group,data=d)
d$lmsplinefit &lt;- fitted(l)
    coef(l)['groupII'] # group difference at x==0.2
    #     groupII 
    #  0.2987893 
    # compare to: d$lmsplinefit[6] - d$lmsplinefit[1]

ggplot(d,aes(x,y,colour=group)) + geom_point() + geom_line(aes(x,lmsplinefit,colour=group))
</code></pre>

<p>Interestingly, the spline fit reports the group difference at the first $x$, i.e. $x=0.2$.</p>

<p>If we try to centre at $x=1$ we get the same result, i.e. the difference at $x=0.2$.</p>

<pre><code>l &lt;- lm(y~ns(I(x-1),2)*group,data=d)
coef(l)['groupII']
# same result as un-centered data, i.e. 0.2987893
</code></pre>

<p>Why is that? And is there a way to show the group difference at a different $x$? Btw, centering $x$ manually before the model fit does not make a difference.</p>
",2013-10-10 02:44:04.330
57188,594.0,2,,56684.0,,,,CC BY-SA 3.0,"<p>Here's the probability function and distribution of the proportion of True (plus the normal approximation at which the chi-square will be exact) in a sample of size 10,000 and a proportion of True of only 1% (right below the low end of your suggested total sample size, and with expected number of True only one fifth of your suggested minimum):</p>

<p><img src=""https://i.stack.imgur.com/Se0Qx.png"" alt=""enter image description here""></p>

<p>Don't forget that you'll have about five times the expected successes shown here; your approximation will be much better than this.</p>

<p>A straight two sample proportions test or a chi-square test should do just fine. Indeed, one tenth of your proportion of True's would be just fine. One <em>hundredth</em>, you'd just go to exact methods.</p>
",2013-10-10 02:45:38.470
57189,22545.0,1,,,,Theoretical expected value and variance,<probability><self-study><variance><expected-value><mathematical-statistics>,CC BY-SA 3.0,"<p>Let $X$ be a random variable having expected value $\mu$ and variance $\sigma^2$. Find the Expected Value and Variance of $Y = \frac{Xâˆ’\mu}{\sigma}$.</p>

<p>I would like to show some progress I've made so far, but honestly I've been thinking about this problem for the past few days but just have no idea where to start. Any hint or insight on a starting point would be much appreciated.</p>

<p>Thanks!</p>
",2013-10-10 04:00:27.487
57190,449.0,2,,57183.0,,,,CC BY-SA 3.0,"<p>You can use multi-level logistic regression. You've only got one dependent variable, correctness. You have multiple independent variables nested within student. In R you can use <code>lmer</code> to construct the model. It would look something like.</p>

<pre><code>m &lt;- lmer( answer ~ treatment * Q + (treatment * Q | student), family = 'binomial', data = mydata)
</code></pre>

<p>That would allow for there to be random effects of question and treatment within student as well as overall correctness variability within student but you would also be able to assess fixed effects of treatment and question. What you seem to really want to know is all of the treatment by question interactions and that model provides them.</p>

<p>In order to analyze all of the questions with any kind of reliability you really should have a lot of students taking the test (hundreds). The general effect of treatment could be assessed with fewer. Also, if you know the categories, the kinds of questions you think differ, then you could replace the individual question variable with that. It would be much more sensible and make this look much less like a fishing expedition.</p>
",2013-10-10 04:04:00.123
57191,594.0,2,,57189.0,,,,CC BY-SA 3.0,"<p>Have you seen the following basic properties of expectation and variance?</p>

<p>(I'd be very surprised if some version of these hadn't been discussed)</p>

<p>$\text{E}(aX+b) = a\text{E}(X)+b$</p>

<p>$\text{Var}(aX+b) = a^2\text{Var}(X)$</p>

<p><a href=""http://en.wikipedia.org/wiki/Expected_value#Linearity"" rel=""nofollow"">http://en.wikipedia.org/wiki/Expected_value#Linearity</a></p>

<p><a href=""http://en.wikipedia.org/wiki/Variance#Basic_properties"" rel=""nofollow"">http://en.wikipedia.org/wiki/Variance#Basic_properties</a></p>

<p>If you apply these properties, or better, the versions you'll already have been given, the problem is trivial.</p>

<p>If you still can't see it, try finding $\text{E}(X-\mu)$ first and work from there.</p>
",2013-10-10 04:06:18.590
57192,22031.0,1,,,,How to check for normal distribution using Excel for performing a t-test?,<normal-distribution><excel>,CC BY-SA 3.0,"<p>I want to know <strong>how to check a data set for normality in Excel, just to verify that the requirements for using a t-test are being met</strong>.  </p>

<p>For the right tail, is it appropriate to just calculate a mean and standard deviation, add 1, 2 &amp; 3 standard deviations from the mean to create a range then compare that to the normal 68/95/99.7 for the standard normal distribution after using the norm.dist function in excel to test each standard deviation value.</p>

<p>Or is there a better way to test for normality?</p>
",2013-10-10 04:41:49.360
57193,1717.0,2,,57175.0,,,,CC BY-SA 3.0,"<p>When you use EM to obtain maximum likelihood estimates, you need a variable that describes your observations $x_{n}$, latent variables $z_{n}$ that are in some way related to your observations (e.g. in coin tossing experiments, $\{H, T\}$ are the latent variables and in gaussian mixtures, the mixing coefficients $\pi_{i}$ take the role of latent variables) and the parameters $\theta$ that you are trying to estimate.</p>

<p>At the risk of not answering your question at all, I think you want a maximum likelihood estimate of $\theta$ using EM based on known observations $x_{n}$ that are given by the following equation:</p>

<p>$$x_{t} = s_{t}(\theta_{0}) + n_{t}$$</p>

<p>If that is correct, a general idea is the following. Since $n_{t}$ is white noise $N(0, \sigma)$, $x_{t}$ can be described by a Gaussian $p(x_{t}|s_{t},\theta) = N(s_{t}(\theta), \sigma)$. In the EM formulation, $x_{t}$'s are known variables, $s_{t}$'s are latent variables and $\theta$ is the parameter. It is customary to group the variables $x_{n}$ in a variable $X$ and likewise, latent variables $s_{n}$ are grouped in a variable $S$.</p>

<p>As you should know, the EM algorithm consists of 2 steps: expectation and maximization. In the expectation step, we use an expression $Q$ as a proxy for the likelihood $L(\theta|X) = p(X|\theta)$, that is, the probability of 
getting the known data $X$ given a parameter $\theta$. This is the same likelihood used to obtain maximum likelihood estimates. However, in EM we use this $Q$ instead:</p>

<p>$$Q(\theta|\theta^{\text{old}}) = E_{S|X, \theta^{\text{old}}} \log p(X,S|\theta)$$</p>

<p>This odd-looking expression is actually a lower bound of the likelihood $L(\theta|X)$. <a href=""http://rads.stackoverflow.com/amzn/click/0387310738"" rel=""nofollow"">Bishop's book</a> contains a good derivation of $Q$.</p>

<p>In order to start the EM magic, you have to choose a random $\theta^{\text{old}}$ and calculate this expectation. Notice that you need $p(X,S|\theta)$ and $p(S|X,\theta^{\text{old}})$. $p(X,S|\theta)$ is equal to $p(X|S,\theta)p(S|\theta)$ and using Bayes' theorem, $p(S|X,\theta^{\text{old}})$ is 
proportional to $p(X|S,\theta^{\text{old}})p(S|\theta^{\text{old}})$.</p>

<p>At this point, I hope it is clear that $p(X|S,\theta)=\prod_{t} p(x_{t}|s_{t},\theta)$, so that part is not hard to calculate. However, $p(S|\theta)$, that is, $\prod_{t}p(s_{t}|\theta)$ is required. I don't know what distribution could be appropriate since this depends on the specifics of your problem so I will assume you know.</p>

<p>By now, you can calculate $Q(\theta|\theta^{\text{old}})$. </p>

<p>The maximization step is simply:</p>

<p>$$\theta = \text{arg max}_{\theta} Q(\theta|\theta^{\text{old}})$$</p>

<p>This is the new $\theta$ to be used in the expectation step again until convergence. </p>

<p>That is a general idea of how EM could work in this case. However, maybe you don't know a distribution for $s_{t}$ or it is difficult to calculate the expectation or the maximization step. </p>

<p>For the big picture, take a look at <a href=""https://docs.google.com/viewer?url=http://www.nature.com/nbt/journal/v26/n8/pdf/nbt1406.pdf"" rel=""nofollow"">this</a> nice explanation.</p>

<p><strong>UPDATE</strong></p>

<p>I think you changed the question quite a bit. Are you asking how to calculate maximum likelihood estimates? Basically, you apply a derivative to the likelihood on the parameter you want to estimate:</p>

<p>$$\frac{\partial}{\partial \theta}L(\theta|X) = 0$$</p>

<p>solve it and that's pretty much it. See more examples <a href=""http://en.wikipedia.org/wiki/Maximum_likelihood#Continuous_distribution.2C_continuous_parameter_space"" rel=""nofollow"">here</a>.</p>
",2013-10-10 05:41:08.343
57194,155.0,2,,57192.0,,,,CC BY-SA 3.0,"<p>You could <a href=""http://office.microsoft.com/en-au/excel-help/create-a-histogram-HP001098364.aspx"" rel=""nofollow noreferrer"">plot a histogram using the data analysis toolpack in Excel</a>. Graphical approaches are more likely to communicate the degree of non-normality, which is typically more relevant for assumption testing (see <a href=""https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless"">this discussion of normality</a>). </p>

<p>The data analysis toolpack in Excel will also give you <a href=""http://graphpad.com/guides/prism/6/statistics/index.htm?stat_skewness_and_kurtosis.htm"" rel=""nofollow noreferrer"">skewness and kurtosis</a> if you ask for descriptive statistics and choose the ""summary statistics"" option. You might for example consider values of skewness above plus or minus one be a form of substantive non-normality. </p>

<p>That said, the assumption with t-tests is that the residuals are normally distributed and not the variable. Furthermore, they also quite robust such that even with fairly large amounts of non-normality, p-values are still fairly valid.</p>
",2013-10-10 05:50:58.150
57195,22547.0,1,57381.0,,,Showing spatial and temporal correlation on maps,<r><regression><data-visualization><pca><spatial>,CC BY-SA 3.0,"<p>I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). </p>

<p>I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. </p>

<h2>Data set</h2>

<p>To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.</p>

<p><img src=""https://i.stack.imgur.com/aZm4N.jpg"" alt=""enter image description here""></p>

<p>Straight away you see one problem: there are lots of sites that have similar identifiers or are very close. FWIW, I identify them using both the USAF and WBAN codes. Looking deeper in to the metadata I saw that they have different coordinates and elevations, and data stop at one site then start at another. So, because I don't know any better, I have to treat them as separate stations. This means the data contains pairs of stations that are very close to each other.</p>

<h2>Preliminary Analysis</h2>

<p>I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. </p>

<p><img src=""https://i.stack.imgur.com/X4YZI.jpg"" alt=""correlation between daily data during each calendar month""></p>

<p>I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.</p>

<h3>Problems</h3>

<p>Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. </p>

<p>I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
<img src=""https://i.stack.imgur.com/NWzm2.jpg"" alt=""enter image description here""></p>

<p>The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.</p>

<p>I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.</p>

<h3>Questions</h3>

<p>I'm learning my way into this field and R at the same time, and would appreciate suggestions on:</p>

<ol>
<li>What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.</li>
<li>Are there more appropriate methods to show the correlation between multiple data sets separated in space?</li>
<li>... in particular, methods that are easy to show results from visually?</li>
<li>Are any of these implemented in R?</li>
<li>Do any of these approaches lend themselves to automation?</li>
</ol>
",2013-10-10 05:52:03.253
57196,668.0,2,,57192.0,,,,CC BY-SA 4.0,"<p>You have the right idea.  This can be done systematically, comprehensively, and with relatively simple calculations.  A graph of the results is called a <em>normal probability plot</em> (or sometimes a P-P plot).  From it you can see <em>much</em> more detail than appears in other graphical representations, especially <a href=""https://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-a-histogram/51753#51753"">histograms</a>, and with a little practice you can even learn to determine ways to re-express your data to make them closer to Normal in situations where that is warranted.</p>

<p>Here is an example:</p>

<p><img src=""https://i.stack.imgur.com/giJwL.png"" alt=""Spreadsheet with probability plot""></p>

<p>Data are in column <code>A</code> (and named <code>Data</code>).  The rest is all calculation, although you can control the ""hinge rank"" value used to fit a reference line to the plot.</p>

<p>This plot is a scatterplot comparing the data to values that would be attained by numbers drawn independently from a standard Normal distribution.  When the points line up along the diagonal, they are close to Normal; horizontal departures (along the data axis) indicate departures from normality.  In this example the points are remarkably close to the reference line; the largest departure occurs at the highest value, which is about <span class=""math-container"">$1.5$</span> units to the left of the line.  Thus we see at a glance that these data are very close to Normally distributed but perhaps have a slightly ""light"" right tail.  This is perfectly fine for applying a t-test.</p>

<p>The comparison values on the vertical axis are computed in two steps.  First each data value is ranked from <span class=""math-container"">$1$</span> through <span class=""math-container"">$n$</span>, the amount of data (shown in the <code>Count</code> field in cell <code>F2</code>).  These are proportionally converted to values in the range <span class=""math-container"">$0$</span> to <span class=""math-container"">$1$</span>.  A good formula to use is <span class=""math-container"">$\left(\text{rank}-1/6\right)/\left(n+2/3\right).$</span>  (See <a href=""http://www.quantdec.com/envstats/notes/class_02/characterizing_distributions.htm"" rel=""nofollow noreferrer"">http://www.quantdec.com/envstats/notes/class_02/characterizing_distributions.htm</a> for where that comes from.)  Then these are converted to standard Normal values via the <code>NormSInv</code> function.  These values appear in the <code>Normal score</code> column.  The plot at the right is an XY scatterplot of <code>Normal Score</code> against the data.  (In some references you will see the transpose of this plot, which perhaps is more natural, but Excel prefers to place the leftmost column on the horizontal axis and the rightmost column on the vertical axis, so I have let it do what it prefers.)</p>

<p><a href=""https://i.stack.imgur.com/iH7vw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iH7vw.png"" alt=""Spreadsheet: normal score calculation""></a></p>

<p>(As you can see, I simulated these data with independent random draws from a Normal distribution with mean <span class=""math-container"">$5$</span> and standard deviation <span class=""math-container"">$2$</span>.  It is therefore no surprise that the probability plot looks so nice.)  There really are only two formulas to type in, which you propagate downward to match the data: they appear in cells <code>B2:C2</code> and rely on the <code>Count</code> value computed in cell <code>F2</code>.  That's really all there is to it, apart from the plotting.</p>

<p>The rest of this sheet is not necessary but it's helpful for judging the plot: it provides a robust estimate of a reference line.  This is done by picking two points equally far in from the left and right of the plot and connecting them with a line.  In the example these points are the third lowest and third highest, as determined by the <span class=""math-container"">$3$</span> in the <code>Hinge Rank</code> cell, <code>F3</code>.  As a bonus, its slope and intercept are robust estimates of the standard deviation and mean of the data, respectively.</p>

<p>To plot the reference line, two extreme points are computed and added to the plot: their calculation occurs in columns <code>I:J</code>, labeled <code>X</code> and <code>Y</code>.</p>

<p><img src=""https://i.stack.imgur.com/ZdEYB.png"" alt=""Spreadsheet: reference line calculation""></p>
",2013-10-10 06:11:44.377
57197,22507.0,2,,57161.0,,,,CC BY-SA 3.0,"<p>If you select the equal number of cases and non-cases, it will bias the model.  For example, suppose that the features have zero correlation with the outcome, and the dataset is very large.  The model will predict the same probability of lung cancer for all patients.  If you select equal number of positive and negative example, the predicted probability will be 0.5, while in reality it is 0.1 .</p>
",2013-10-10 06:21:56.940
57198,22548.0,1,57215.0,,,What is random error in OLS regression? And how is it related to Gaussian noise?,<regression><least-squares><error>,CC BY-SA 4.0,"<p>In OLS regression:  </p>

<p><span class=""math-container"">$$Y=\beta_0+\beta_1 X_1+ \beta_2 X_2+\beta_3 X_3 + \beta_4 X_4+\beta_5 X_5+\beta_6 X_6 + \varepsilon,$$</span></p>

<p>what is <span class=""math-container"">$\varepsilon$</span>? Is it Gaussian noise or random error? What is a difference? Why we add it to multiple regression model? In most of papers authors refer it to random error but without clarification.</p>

<p>I need a simple and good reason why authors add it to their model.</p>
",2013-10-10 06:34:08.830
57199,22551.0,2,,47846.0,,,,CC BY-SA 3.0,"<p>Poisson/Negative binomial can also be used with a binary outcome with offset equal to one. Of course it necessitates that the data be from a prospective design (cohort, rct, etc). Poisson or NB regression gives the more appropriate effect measure (IRR) versus odds ratio from logistic regression.</p>

<p>NB regression is ""safer"" to run than Poisson regression because even if the overdispersion parameter (alpha in Stata) is not statistically significant, the results will be exactly the same as its Poisson regression form.</p>
",2013-10-10 07:32:59.240
57200,15563.0,1,,,,SVM prediction accuracy drops when using Test data,<r><machine-learning><svm><e1071>,CC BY-SA 3.0,"<p>I am using the <a href=""https://www.kaggle.com/c/data-science-london-scikit-learn/"" rel=""nofollow"">Kaggle Scikit</a> data to learn R.</p>

<p>I am using the R e1071 SVM function to predict classes.</p>

<p>When I use:</p>

<pre><code>svm(train, trainLabels, scale = TRUE, type = NULL, kernel = ""polynomial"")
</code></pre>

<p>I obtain this level of accuracy on a sample of the Train data:</p>

<pre><code>&gt; table(pred, trainLabels)
    trainLabels
pred   0   1
   0 478   8
1   12 502
</code></pre>

<p>which I interpret as being 98% accurate (8+12) / (478+8+12+502).</p>

<p>Though when I use the same prediction model on the Test data, Kaggle returns a <strong>0.82</strong> score, <a href=""https://www.kaggle.com/c/data-science-london-scikit-learn/details/evaluation"" rel=""nofollow"">based on classification accuracy</a>.</p>

<p>Can you explain why I can get such a different accuracy level?</p>
",2013-10-10 08:09:22.140
57201,21762.0,2,,57015.0,,,,CC BY-SA 3.0,"<p>A simple approach would be the following: </p>

<p>1) Take all observations sampled at random under condition A and obtain the <em>relevant one sided</em> p-value from Wilcoxon's rank sum test.</p>

<p>2) Do the same for the observations sampled under condition B.</p>

<p>3) If the smaller of the two p-values is below the level $\alpha/2$ and the other p-value is below $\alpha$, then your claim holds at the $\alpha$ level. (This would be the Bonferroni-Holm correction for multiple testing.)</p>

<p>Since the sample sizes are extremely low, you will get a ""significant"" result only if the signal is very strong.</p>
",2013-10-10 08:18:09.707
57208,13889.0,1,57214.0,,,Test to rank methods by AUCs on various benchmarks,<ranking><wilcoxon-signed-rank><auc>,CC BY-SA 3.0,"<p>Suppose I have N methods and M benchmarks. I have an AUC statistic (and some other similar statistics) for each combination of method with benchmark. What test should I use to test if one method is better than the rest? I have seen some authors do pairwise comparisons using a one-sided Wilcoxon signed-rank test but I would prefer to test all methods at once. In any case I'm not sure the assumptions for the one-sided Wilcoxon signed-rank test hold. If the average AUC for each benchmark varies widely can you say the samples are from the same population? Also I'm not sure the distribution of the AUCs is symmetric around the median. Any advice would be welcome.</p>
",2013-10-10 10:34:56.943
57311,22612.0,1,,,,Regressing a difference of ordinal variables?,<regression><modeling><ordinal-data><credit-scoring>,CC BY-SA 4.0,"<p>Dependent variables (ordinal): credit rating 1970 (cr70) and credit rating 1980 (cr80).</p>
<p>Here is what I want to do:</p>
<p>Regress cr80-cr70 = independent vars.</p>
<p>How could this be done and how could you interpret it!?</p>
<p>If the dependent variable is continuous it would be simple. But can you make a new var from the difference of two ordinal vars, and have that be the dependent var?</p>
",2013-10-11 16:46:12.820
57202,14799.0,2,,5015.0,,,,CC BY-SA 3.0,"<p>This may be a problem of interpretation, a misunderstanding of what a so-called ""direct effect"" coefficient really is.</p>

<p>In regression models with continuous predictor variables and no interaction terms -- that is, with no terms that are constructed as the product of other terms -- each variable's coefficient is the slope of the regression surface in the direction of that variable. It is constant, regardless of the values of the variables, and is obviously a measure of the effect of that variable.</p>

<p>In models with interactions -- that is, with terms that are constructed as the products of other terms -- that interpretation can be made without further qualification only for variables that are <strong>not</strong> involved in any interactions. The coefficient of a variable that <strong>is</strong> involved in interactions is the slope of the regression surface in the direction of that variable <strong>when the values of all the variables that interact with the variable in question are zero</strong>, and the significance test of the coefficient refers to the slope of the regression surface <strong>only in that region of the predictor space</strong>. Since there is no requirement that there actually be data in that region of the space, the apparent direct effect coefficient may bear little resemblance to the slope of the regression surface in the region of the predictor space where data were actually observed. There is no true ""direct effect"" in such cases; the best substitute is probably the ""average effect"": the slope of the regression surface in the direction of the variable in question, taken at each data point and averaged over all data points. For more on this, see <a href=""https://stats.stackexchange.com/questions/65898/answer/65917"">Why could centering independent variables change the main effects with moderation?</a></p>
",2013-10-10 08:31:00.453
57203,633.0,2,,57164.0,,,,CC BY-SA 3.0,"<p>Yes, the gamma distribution is the maximum entropy distribution for which the mean $E(X)$ and mean-log $E(\log X)$ are fixed.  As with all exponential family distributions, it is the unique maximum entropy distribution for a fixed expected sufficient statistic.</p>

<p>To answer your question about physical processes that generate these distributions:  The lognormal distribution arises when the logarithm of X is normally distributed, for example, if X is the product of very many small factors.  If X is gamma distributed, it is the sum of many exponentially-distributed variates.  For example, the waiting time for many events of a Poisson process.</p>
",2013-10-10 09:03:03.037
57204,4831.0,2,,57160.0,,,,CC BY-SA 3.0,"<p>You are correct that there's currently no good way to do seasonal ARIMA in statsmodels. Currently, I only have a half-baked solution for doing non-consecutive lags, but it's not public anywhere. It's a bit heavy, computations-wise. Unfortunately, I doubt I'll be able to work on this anytime soon (unless someone would be willing to fund the enhancement...). Contributions in this area would be very welcome.</p>

<p><a href=""https://github.com/statsmodels/statsmodels/issues/247"" rel=""nofollow"">https://github.com/statsmodels/statsmodels/issues/247</a>
<a href=""https://github.com/statsmodels/statsmodels/issues/232"" rel=""nofollow"">https://github.com/statsmodels/statsmodels/issues/232</a></p>
",2013-10-10 09:22:00.363
57205,22558.0,2,,20561.0,,,,CC BY-SA 3.0,"<p>I would not touch the data at all. Use this for autocorrelation with NaNs:</p>

<p><a href=""http://www.mathworks.com/matlabcentral/fileexchange/43840-autocorrelation-and-partial-autocorrelation-with-nans/content/nanautocorr.m"" rel=""nofollow"">http://www.mathworks.com/matlabcentral/fileexchange/43840-autocorrelation-and-partial-autocorrelation-with-nans/content/nanautocorr.m</a></p>

<p>""not touch the data"" means not to remove any data or time-step or replace with 0 or the mean, it would compromise the information about the specific-time-lag linear dependence. I would also avoid simulating the values in the gaps, if you are interested in the ""SAMPLE"" autocorrelation, anyway even the best simulation technique will not add any more information about the autocorrelation, being based on the data themselves.
I partially recoded the matlab (link above) autocorrelation and partial autocorrelation functions to deal with NaNs: any data couples including NaNs is excluded from the computation. This is done for each lag. It worked for me. Any suggestion is well accepted. </p>
",2013-10-10 09:58:27.403
57206,22555.0,2,,57192.0,,,,CC BY-SA 3.0,"<p>This question borders on statistics theory too - testing for normality with limited data may be questionable (although we all have done this from time to time).</p>

<p>As an alternative, you can look at kurtosis and skewness coefficients.  From <strong>Hahn and Shapiro: Statistical Models in Engineering</strong> some background is provided on the properties Beta1 and Beta2 (pages 42 to 49) and the Fig 6-1 of Page 197.  Additional theory behind this can be found on Wikipedia (see Pearson Distribution).</p>

<p>Basically you need to calculate the so-called properties Beta1 and Beta2.  A Beta1 = 0 and Beta2 = 3 suggests that the data set approaches normality.  This is a rough test but with limited data it could be argued that any test could be considered a rough one.</p>

<p>Beta1 is related to the moments 2 and 3, or variance and <a href=""http://en.wikipedia.org/wiki/Skewness"" rel=""nofollow"">skewness</a>, respectively.  In Excel, these are VAR and SKEW.  Where ... is your data array, the formula is:</p>

<pre><code>Beta1 = SKEW(...)^2/VAR(...)^3
</code></pre>

<p>Beta2 is related to the moments 2 and 4, or the variance and <a href=""http://en.wikipedia.org/wiki/Kurtosis"" rel=""nofollow"">kurtosis</a>, respectively.  In Excel, these are VAR and KURT.  Where ... is your data array, the formula is:</p>

<pre><code>Beta2 = KURT(...)/VAR(...)^2
</code></pre>

<p>Then you can check these against the values of 0 and 3, respectively.  This has the advantage of potentially identifying other distributions (including Pearson Distributions I, I(U), I(J), II, II(U), III, IV, V, VI, VII).  For example, many of the commonly used distributions such as Uniform, Normal, Student's t, Beta, Gamma, Exponential, and Log-Normal can be indicated from these properties:</p>

<pre><code>Where:   0 &lt;= Beta1 &lt;= 4
         1 &lt;= Beta2 &lt;= 10 

Uniform:        [0,1.8]                                 [point]
Exponential:    [4,9]                                   [point] 
Normal:         [0,3]                                   [point]
Students-t:     (0,3) to [0,10]                         [line]
Lognormal:      (0,3) to [3.6,10]                       [line]
Gamma:          (0,3) to (4,9)                          [line]
Beta:           (0,3) to (4,9), (0,1.8) to (4,9)        [area]
Beta J:         (0,1.8) to (4,9), (0,1.8) to [4,6*]     [area]
Beta U:         (0,1.8) to (4,6), [0,1] to [4.5)        [area]
Impossible:     (0,1) to (4.5), (0,1) to (4,1]          [area]
Undefined:      (0,3) to (3.6,10), (0,10) to (3.6,10)   [area]

Values of Beta1, Beta2 where brackets mean:

[ ] : includes (closed)
( ) : approaches but does not include (open)
 *  : approximate 
</code></pre>

<p>These are illustrated in Hahn and Shapiro Fig 6-1.</p>

<p>Granted this is a very rough test (with some issues) but you may want to consider it as a preliminary check before going to a more rigorous method.</p>

<p>There are also adjustment mechanisms to the calculation of Beta1 and Beta2 where data is limited - but that is beyond this post.</p>
",2013-10-10 09:59:06.940
57207,503.0,2,,57185.0,,,,CC BY-SA 3.0,"<p>For measurement error there really isn't a difference in the definitions. Psychometry defines ""true score"" as ""measured score"" + ""error"" and this is the same thing as the statistical definition. The confusion may come from different terminology; that developed because psychometry deals with tests while statistics can deal with almost anything. </p>

<p>""Bias"" is a bit more complex. @NickCox gave the definition in statistics. In psychometry, it is used (at least some of the time) in a slightly different way, again due to the specialized nature of the subject. A test is biased for/against a group if its predictions work differently in another setting. So, e.g. if we are using SAT scores to predict college GPA, bias would be that one group gets lower/higher GPA with the same SAT score.</p>

<p>In statistics, a scale could be biased against everyone - e.g. if my scale estimates everyone's weight as 5 pounds less than the actual value, that's bias. In the psychometrics definition, that can't be bias. </p>

<p>BUT psychometricians often use ""bias"" in the statistical sense as well. </p>
",2013-10-10 10:19:23.717
57528,22705.0,2,,57110.0,,,,CC BY-SA 3.0,"<p>i'm having a fabulous run with ucm. You could model this as daily seasonality &amp; with an annual cycle. You also have a very evident trend. </p>

<p>Post back here if you succeeded with ucm (proc ucm)</p>
",2013-10-15 13:43:10.627
57209,21896.0,2,,57137.0,,,,CC BY-SA 3.0,"<p>Following the suggestion of @Momo I will answer the question myself. What I had forgotten yesterday when I posted this question, is that I can just see what <code>glm.nb</code> does by typing ""glm.nb"" into the console. From the code it returns it can be inferred that indeed the variance equals $\mu + \mu^2/\theta$ so that $\theta = 1/\kappa$.</p>

<p>Also I'd like to use the opportunity to advertise this <a href=""http://www.jstatsoft.org/v27/i08/paper"" rel=""nofollow"">article</a> I found since then, also addressing these matters. </p>
",2013-10-10 10:37:03.140
57210,22560.0,1,,,,Why eigenvalues are greater than 1 in factor analysis?,<factor-analysis>,CC BY-SA 3.0,"<p>Why we take eigenvalue greater than 1 in factor analysis to retain factors?<br>
 And how can we decide which variables are to be chosen as factors?</p>
",2013-10-10 10:46:06.893
57211,503.0,2,,57210.0,,,,CC BY-SA 3.0,"<p>Using eigenvalues > 1 is only <em>one</em> indication of how many factors to retain. Other reasons include the scree test, getting a reasonable proportion of variance explained and (most importantly) substantive sense. </p>

<p>That said, the rule came about because the average eigenvalue will be 1, so > 1 is ""higher than average"". </p>

<p>On your second question: Are you asking how to know how many factors (latent variables) to retain? Or are you asking about which observed variables to retain?</p>

<p>If the former, see above and see any book on factor analysis. If the latter, each factor is a linear combination of <em>all</em> the observed variables (although some contribute very little). </p>
",2013-10-10 10:52:34.780
57212,22190.0,1,,,,Concordance and Discordance role in modelling,<regression><logistic><modeling>,CC BY-SA 3.0,"<p>I am new to statistics and was asked to develop a statistical model, which I had started, they ask me to carry out concordance and discordance now, however I don't know anything about these terms except that the concordance is the probability that a pair of individuals will both have a certain characteristic, given that one of the pair has the characteristic and the opposite for discordance.<br>
Still I don't know why I have to find them and what would be the appropriate value of both for a decent model.  </p>
",2013-10-10 10:53:56.520
57213,503.0,2,,57212.0,,,,CC BY-SA 3.0,"<p>In <a href=""http://www.nesug.org/proceedings/nesug08/sa/sa09.pdf"" rel=""nofollow"">this paper</a> I cover concordance and discordance. The paper is about <code>PROC LOGISTIC</code> in <code>SAS</code> but the section on concordance is more general. Briefly: Look at all possible pairs of observations. A pair is concordant if the observation with the higher observed value also has the higher predicted value. </p>
",2013-10-10 11:17:56.730
57214,1428.0,2,,57208.0,,,,CC BY-SA 3.0,"<p>If the M benchmarks are supposed to yield score identically distributed score estimates (e.g. cross-validation folds) then maybe you can estimate confidence intervals for the mean AUC score for each method by bootstrapping on the M benchmarks of that method and then compare methods by considering non-overlapping confidence intervals. As bootstrapped confidence interval is a non-parametric method, you do not make any assumption on the symmetry of AUCs around the median.</p>
",2013-10-10 11:28:17.020
57215,449.0,2,,57198.0,,,,CC BY-SA 3.0,"<p>The wikipedia definition is a fine definition that you can use for your paper if you need one but I think you're missing something.</p>

<p>The $\epsilon$ is random error, which is synonymous with noise. In practice, the random error can be Gaussian distributed, in which case it is Gaussian noise, but it could take on other distributions. If the distribution of $\epsilon$ happens to be Gaussian then you've met one of the theoretical assumptions of the model and things like interval estimation are better justified. If it's not Gaussian then, like Glen_b said, you still have that it's best linear unbiased.</p>

<p>Theoretically, the random error (noise) is supposed to be Gaussian distributed but the outcome could be anything. So, in order to answer your question you'd need to state whether you want to know the distribution of your particular noise or what the distribution of the noise should be. For the former you'd need data.</p>
",2013-10-10 11:41:48.027
57216,22561.0,2,,57128.0,,,,CC BY-SA 3.0,"<p>The total variance for combined regression results can be estimated using the same approach as in multiple imputations. In the attached file, the formulas for combining the regression results and total variance are presented.</p>

<p><img src=""https://i.stack.imgur.com/rFT00.png"" alt=""Combining (pooloing) results of several regression models into one""></p>
",2013-10-10 12:00:04.637
57217,15563.0,1,99688.0,,,R e1071 tune plot does not give me the best gamma?,<r><svm><e1071>,CC BY-SA 3.0,"<p>I am using the R e1071 library for the SVM (Support Vector Machine) algorithm.
And I used tune to find out the best Cost and gamma parameters.</p>

<p>Though the plot doesn't seem to provide the actual best prediction.</p>

<p>Here is some details:</p>

<pre><code>gammalist &lt;- c(0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)
obj &lt;- tune(svm, Class~., data = trainData, ranges = list(gamma = gammalist, cost = 2^(2:4)), tunecontrol = tune.control(sampling = ""fix"") )
plot(obj)
</code></pre>

<p>Here is the plot obtained:
<img src=""https://i.stack.imgur.com/Al0QG.png"" alt=""enter image description here""></p>

<p>The plot leads me to believe 0.02 is roundabout the best gamma.
But I actually tested manually several others and find better results for 0.042.</p>

<p>On a 200 sample, I get 23 errors with gamma=0.042, and 26 errors with gamma=0.02.</p>

<p>How do you explain this?</p>
",2013-10-10 12:05:58.447
57218,21398.0,1,57225.0,,,Multiple imputation - original model,<multiple-imputation>,CC BY-SA 3.0,"<p>I have a question. I have a dataset with some missing values that were not MCAR. I imputed them with fully conditional specification method iterations. I then executed my analysis on the basis of the imputed dataset. The results of the original model (with missing values: listwise deletion)  did not change much in the eventual pooled model. My idea would be to go back the missing values dataset. What do you think?</p>
",2013-10-10 12:44:27.567
57229,22567.0,1,57233.0,,,Dealing with missing data in the prediction set only,<regression><categorical-data><cross-validation><missing-data><prediction>,CC BY-SA 3.0,"<p>I have a regression problem where the independent variables are all factors (categorical).  I've been looking at the literature on missing data, and so far it all seems concerned with missing training data.  I was wondering if there is a standard way of dealing with missing data in the <em>prediction</em> set.  That is, you have all the data you need to train, but then you need to be able to make a prediction with only partial data. This must have been a studied problem. </p>

<p>My initial thought is to use an average of the dummy encoded variables, according to how common they are.  As a quick example, say we have a three level factor dummy encoded as</p>

<pre><code>level 1: [1 0]
level 2: [0 1]
level 3: [0 0]
</code></pre>

<p>Say level $i$ occurs fraction $f_i$ of the time in the training data (so $\sum_i{f_i}=1$), and the regression has the two coefficients $\beta_1$ and $\beta_2$.</p>

<p>Then a missing value in this factor might be estimated as:
$$
\beta_1*f_1 + \beta_2*f_2 + 0*f_3
$$
But given that the ""default"" level encoding are shared across factors, I'm not sure I'm handling level 3 correctly in this case. </p>
",2013-10-10 15:23:53.377
57219,10547.0,2,,57167.0,,,,CC BY-SA 3.0,"<p>If all the $Z$s are also determined by the $Y$ the system of equations, which you have proposed, cannot be identified. What you need to do is to reduce the equations such that the coefficients can be identified.</p>

<p>I recommend reading:</p>

<blockquote>
  <p>Wooldridge, Introductory Econometrics, 3d ed. Chapter 16: Simultaneous
  equations</p>
</blockquote>

<p>Here, your kind of problems gets explained, and there are some pretty nice examples.</p>

<p>I also recommend reading:</p>

<pre><code>Rummery,Vella,Verbeek (1998) - Estimating the Returns to Education for Australian Youth via Rank-Order Instrumental Variables
</code></pre>

<p>and </p>

<pre><code>Vella,Verbeek (1997) - Using Rank Order As An Instrumental Variable - An Applicaton To The Return To Schooling
</code></pre>

<p>Vella and Verbeek (also Rummery) estimate smth. like:</p>

<p>$y_i = x_i\beta + z_i\delta + e_i, \ \ \ \ i = 1,...,N$</p>

<p>Here $x_i$ is a $K$ vector of exogenous variables whereas $z_i$ is assumed to be endogenous. Hence the reduced form equation of $z_i$ is given by:</p>

<p>$z_i = x_i\alpha + v_i$</p>

<p>The advantage of this approach is, that you dont need any exclusion restrictions for the $x_i$, which are necessary to make 2SLS/3SLS work.</p>

<p>I've used this approach to solve a three equation system, i.e., i got three equations and in each of them there are two endogenous regressors which are also the dependend variable in some other equation.</p>

<p>I also applied a plug-in style of approach to deal with potential heteroscedasticity. </p>

<p>There are some issues which are not presented within this papers but I would be happy to talk to you about that.</p>
",2013-10-10 12:59:56.353
57220,22562.0,1,,,,Interaction wipes out my direct effects in regression (non zero variable),<regression><interaction><effects>,CC BY-SA 3.0,"<p>I have the following regression</p>

<p>$children = \beta_0 + \beta_1 \log(earnings) + \beta_2 grandparents + \epsilon$</p>

<p>and $\beta_1&gt;0$ with $p$=0.01 and $\beta_2&gt;0$ with $p$=0.01, and N is large (N>10.000) and grandparents takes values 0,1,2,3,4.</p>

<p>Then I add the interaction term ($\log(earnings)*grandparents$) to equation 1, such that: </p>

<p>$children = \beta_0 + \beta_1 \log( earnings) + \beta_2 grandparents+ \beta_3 \log( earnings)*grandparents + \epsilon$ </p>

<p>and $\beta_1&gt;0$ with $p$=0.01, $\beta_2$ is no longer statistically significant and also $\beta_3$ is not statistically significant. </p>

<p>I do not understand how to interpret the results and if the interaction term wipes out the direct effect of grandparents since $\log(earnings)$ is always different from 0.</p>

<blockquote>
  <blockquote>
    <blockquote>
      <blockquote>
        <p>There is a way to test the stat. sign. of the effect of Grandparents in the interacted model? (Thanks Maarten for your previous answer)</p>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>
",2013-10-10 13:11:58.220
57221,22563.0,1,,,,What's the approximate distribution? Replace the true mean with sample mean,<distributions>,CC BY-SA 3.0,"<p>If say for a random variable $X$, I have observation of $x_1,x_2,x_3,\ldots,x_n$. Let $m$ be the sample mean, and $s$ be the sample standard deviation. Does the new random variable $(X-m)/s$ follow some distribution? It's not $t$-distribution I guess, since for it to be $t$ distributed, $m$ needs to be replaced by true mean.</p>

<p>Can statistics expert shed some light on this?</p>
",2013-10-10 13:28:20.653
57222,16474.0,2,,57220.0,,,,CC BY-SA 3.0,"<p>$\beta_2$ in equation 2 is the effect of $grandparents$ when $\log(earnings) = 0$, i.e. $earnings = 1$. This is apperently outside the range of your data, so it is an extrapolation. The easiest way around that is to center $earnings$ before taking the logarithm or creating the interaction term at some meaningfull value withing the range of the data, for example, the median. That way the main effect of $grandparents$ will be the effect of grandparents when one has a median income instead of a fictional income of 1.</p>
",2013-10-10 13:29:02.837
57223,22564.0,1,57245.0,,,How to compare two groups with multiple measurements for each individual with R?,<r><statistical-significance><t-test><error-propagation>,CC BY-SA 3.0,"<p>I have a problem like the following:</p>

<p>1) There are six measurements for each individual with large within-subject variance </p>

<p>2) There are two groups (Treatment and Control)</p>

<p>3) Each group consists of 5 individuals</p>

<p>4) I want to perform a significance test comparing the two groups to know if the group means are different from one another.</p>

<p>The data looks like this:
<img src=""https://i.stack.imgur.com/55V9J.png"" alt=""http://s10.postimg.org/p9krg6f3t/examp.png""></p>

<p>And I have run some simulations using this code which does t tests to compare the group means. The group means were calculated by taking the means of the individual means. <strong>This ignores within-subject variability</strong>:</p>

<pre><code> n.simulations&lt;-10000
    pvals=matrix(nrow=n.simulations,ncol=1)
    for(k in 1:n.simulations){
      subject=NULL
      for(i in 1:10){
        subject&lt;-rbind(subject,as.matrix(rep(i,6)))
      }
      #set.seed(42)

      #Sample Subject Means
      subject.means&lt;-rnorm(10,100,2)

      #Sample Individual Measurements
      values=NULL
      for(sm in subject.means){
        values&lt;-rbind(values,as.matrix(rnorm(6,sm,20)))
      }

      out&lt;-cbind(subject,values)

      #Split into GroupA and GroupB
      GroupA&lt;-out[1:30,]
      GroupB&lt;-out[31:60,]

      #Add effect size to GroupA
      GroupA[,2]&lt;-GroupA[,2]+0

      colnames(GroupA)&lt;-c(""Subject"", ""Value"")
      colnames(GroupB)&lt;-c(""Subject"", ""Value"")

      #Calculate Individual Means and SDS
      GroupA.summary=matrix(nrow=length(unique(GroupA[,1])), ncol=2)
      for(i in 1:length(unique(GroupA[,1]))){
        GroupA.summary[i,1]&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
        GroupA.summary[i,2]&lt;-sd(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
      }
      colnames(GroupA.summary)&lt;-c(""Mean"",""SD"")


      GroupB.summary=matrix(nrow=length(unique(GroupB[,1])), ncol=2)
      for(i in 1:length(unique(GroupB[,1]))){
        GroupB.summary[i,1]&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
        GroupB.summary[i,2]&lt;-sd(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
      }
      colnames(GroupB.summary)&lt;-c(""Mean"",""SD"")

      Summary&lt;-rbind(cbind(1,GroupA.summary),cbind(2,GroupB.summary))
      colnames(Summary)[1]&lt;-""Group""

      pvals[k]&lt;-t.test(GroupA.summary[,1],GroupB.summary[,1], var.equal=T)$p.value
    }
</code></pre>

<p>And here is code for plots:</p>

<pre><code>#Plots
par(mfrow=c(2,2))
boxplot(GroupA[,2]~GroupA[,1], col=""Red"", main=""Group A"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupA[,2]~GroupA[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupA[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupA[,1]))){
  m&lt;-mean(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])
  ci&lt;-t.test(GroupA[which(GroupA[,1]==unique(GroupA[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(GroupB[,2]~GroupB[,1], col=""Light Blue"", main=""Group B"", 
        ylim=c(.9*min(out[,2]),1.1*max(out[,2])),
        xlab=""Subject"", ylab=""Value"")
stripchart(GroupB[,2]~GroupB[,1], vert=T, pch=16, add=T)
#abline(h=mean(GroupB[,2]), lty=2, lwd=3)

for(i in 1:length(unique(GroupB[,1]))){
  m&lt;-mean(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])
  ci&lt;-t.test(GroupB[which(GroupB[,1]==unique(GroupB[,1])[i]),2])$conf.int[1:2]

  points(i-.2,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.2,
           ci[1],i-.2,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Individual Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


boxplot(Summary[,2]~Summary[,1], col=c(""Red"",""Light Blue""), xlab=""Group"", ylab=""Average Value"",
        ylim=c(.9*min(Summary[,2]),1.1*max(Summary[,2])),
        main=""Individual Averages"")
stripchart(Summary[,2]~Summary[,1], vert=T, pch=16, add=T)

points(.9, mean(GroupA.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(.9,
         t.test(GroupA.summary[,1])$conf.int[1],.9,
         t.test(GroupA.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)

points(1.9, mean(GroupB.summary[,1]), pch=15,cex=1.5, col=""Grey"")
segments(1.9,
         t.test(GroupB.summary[,1])$conf.int[1],1.9,
         t.test(GroupB.summary[,1])$conf.int[2], lwd=4, col=""Grey""
)
legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


hist(pvals, breaks=seq(0,1,by=.05), col=""Grey"",
     main=c(paste(""# sims="", n.simulations),
            paste(""% Sig p-values="",100*length(which(pvals&lt;0.05))/length(pvals)))
)
</code></pre>

<p>Now, it seems to me that because each individual mean is an estimate itself, that we should be less certain about the group means than shown by the 95% confidence intervals indicated by the bottom-left panel in the figure above. Thus the p-values calculated are underestimating the true variability and should lead to increased false-positives if we wish to extrapolate to future data.</p>

<p>So what is the correct way to analyze this data?</p>

<p><strong>Bonus:</strong></p>

<p>The example above is a simplification. For the actual data: </p>

<p>1) The within-subject variance is positively correlated with the mean. </p>

<p>2) Values can only be multiples of two. </p>

<p>3) The individual results are not roughly normally distributed. They suffer from zero floor effect, and have long tails at the positive end. </p>

<p>4) Number of Subjects in each group are not necessarily equal. </p>

<p>Previous literature has used the t-test ignoring within-subject variability and other nuances as was done for the simulations above. Are these results reliable? If I can extract some means and standard errors from the figures how would I calculate the ""correct"" p-values.</p>

<p><strong>EDIT:</strong></p>

<p>Ok, here is what <em>actual</em> data looks like. There is also three groups rather than two:</p>

<p><img src=""https://i.stack.imgur.com/k1xWd.png"" alt=""enter image description here""></p>

<p>dput() of data:</p>

<pre><code>structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
    NULL, c(""Group"", ""Subject"", ""Value"")))
</code></pre>

<p><strong>EDIT 2:</strong></p>

<p>In response to Henrik's answer:
So if I instead perform anova followed by TukeyHSD procedure on the individual averages as shown below, I could interpret this as underestimating my p-value by about 3-4x? </p>

<p>My goal with this part of the question is to understand how I, as a reader of a journal article, can better interpret previous results given their choice of analysis method. For example they have those ""stars of authority"" showing me 0.01>p>.001. So if i accept 0.05 as a reasonable cutoff I should accept their interpretation? The only additional information is mean and SEM.</p>

<pre><code>#Get Invidual Means
summary=NULL
for(i in unique(dat[,2])){
sub&lt;-which(dat[,2]==i)
summary&lt;-rbind(summary,cbind(
dat[sub,1][3],
dat[sub,2][4],
mean(dat[sub,3]),
sd(dat[sub,3])
)
)
}
colnames(summary)&lt;-c(""Group"",""Subject"",""Mean"",""SD"")

TukeyHSD(aov(summary[,3]~as.factor(summary[,1])+ (1|summary[,2])))

#      Tukey multiple comparisons of means
#        95% family-wise confidence level
#    
#    Fit: aov(formula = summary[, 3] ~ as.factor(summary[, 1]) + (1 | summary[, 2]))
#    
#    $`as.factor(summary[, 1])`
#             diff       lwr       upr     p adj
#    2-1 -0.672619 -4.943205  3.597967 0.9124024
#    3-1  7.507937  1.813822 13.202051 0.0098935
#    3-2  8.180556  2.594226 13.766885 0.0046312
</code></pre>

<p><strong>EDIT 3:</strong>
I think we are getting close to my understanding. Here is the simulation described in the comments to @Stephane:</p>

<pre><code>#Get Subject Means
means&lt;-aggregate(Value~Group+Subject, data=dat, FUN=mean)

#Initialize ""dat2"" dataframe
dat2&lt;-dat

#Initialize within-Subject sd
s&lt;-.001
pvals=matrix(nrow=10000,ncol=2)

for(j in 1:10000){
#Sample individual measurements for each subject
temp=NULL
for(i in 1:nrow(means)){
temp&lt;-c(temp,rnorm(6,means[i,3], s))
}

#Set new values
dat2[,3]&lt;-temp

#Take means of sampled values and fit to model
dd2 &lt;- aggregate(Value~Group+Subject, data=dat2, FUN=mean)
fit2 &lt;- lm(Value~Group, data=dd2)

#Save sd and pvalue
pvals[j,]&lt;-cbind(s,anova(fit2)[[5]][5])

#Update sd
s&lt;-s+.001
}

plot(pvals[,1],pvals[,2], xlab=""Within-Subject SD"", ylab=""P-value"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/gMMDY.png"" alt=""enter image description here""></p>
",2013-10-10 13:47:37.470
57224,3731.0,2,,52910.0,,,,CC BY-SA 3.0,"<p>To close the loop for those who don't want to follow the comment thread above:</p>

<p>No. People do not normally compile these things because they are specific to both the particular model chosen and the particular data onto which the model is fit. To talk of a ""typical"" number is not well posed.</p>

<p>If someone would like to post a more comprehensive answer, I will be happy to ""unaccept"" this answer and accept theirs instead. </p>
",2013-10-10 14:09:23.710
57225,503.0,2,,57218.0,,,,CC BY-SA 3.0,"<p>I think the choice depends on the audience that will read whatever you write.</p>

<p>If they are mostly statistically unsophisticated, I'd say you could use the original data set and put a footnote about how multiple imputation did not change things much. If they are more sophisticated, I'd go with the MI analysis. Even if things don't change ""much"" they change <em>some</em> and the MI is a better approach.</p>

<p>Also, be careful that you looked at <em>all</em> the output for what changed (or didn't). Not just parameter estimates but their standard errors (or whatever your analysis involves - you didn't say what analysis you did, so it's hard to say what might be affected). </p>
",2013-10-10 14:16:53.693
57226,22566.0,2,,57128.0,,,,CC BY-SA 3.0,"<p>The above presented formulas are available in the SPSS help: Help > Algorithms > Multiple Imputation: Pooling Algorithms > Rubin's Rules (multiple imputation algorithms) > Combining Results after Multiple Imputation</p>
",2013-10-10 14:44:17.203
57227,9792.0,2,,57187.0,,,,CC BY-SA 3.0,"<p>I think the question why splines cannot be centered arose out of a misunderstanding of how splines function. It seems that splines don't model an intercept and thus centering is impossible. It would, however, be great if someone had another solution to estimating the group differences at different time points when modelling more complex dynamics.</p>
",2013-10-10 15:02:05.507
57228,22568.0,1,,,,How to create a GIS basemap in R?,<r><gis>,CC BY-SA 3.0,"<p>I am an expert GIS user moving towards R more and more.  I have been using R for some basic regressions and such, but I would like to begin to use and manipulate GIS data in R.</p>

<p>How can I create a basemap graphic similar to the one in this post:
<a href=""https://stats.stackexchange.com/questions/72421/showing-spatial-and-temporal-correlation-on-maps"">Showing spatial and temporal correlation on maps</a></p>

<p>Again, I am a beginner in R and haven't found any other related thread here.</p>
",2013-10-10 15:23:08.293
57230,22569.0,1,57409.0,,,Represent data across multiple categories and sub categories,<data-visualization><categorical-data><barplot>,CC BY-SA 3.0,"<p>The data contain category and sub-category distributions. </p>

<p>The categories are topics in a quiz such as: Music, Sports, Business. </p>

<p>Each category has three levels to choose from: Basic, Standard and Advanced.</p>

<p>For example: A user might take a quiz on Music across different levels. Say the number of questions attempted is 100. The user would have answered them across levels. 40 for basic, 40 for standard and 20 for advanced. The data consist of counts of the questions attempted within each category for each user.</p>

<p>What is the best way to represent these data on a graph? Each graph would contain up to 5 main categories. </p>
",2013-10-10 15:25:25.017
57231,22381.0,1,,,,Raw return vs. percentage return to calculate volatility,<finance><arma><garch>,CC BY-SA 3.0,"<p>I am using squared return as a proxy to calculate volatility, however i'm not sure whether to use raw return or percentage return. Under raw return all return estimates are below 1, however under percentage return there is a mix of return greater than 1 and less than 1. Percentage return below 1 would end up as a volatility figure less than the percentage return itself, on the other hand percentage return above 1 would end up as a volatility figure greater than percentage return. </p>

<p>My question is: Doesn't this pose a problem when calculating volatility in that there is an over estimation when the return is above 1?</p>

<p>I am going to use the data to fit an ARMA-GARCH model, would there be any difference if I used percentage or absolute values?</p>
",2013-10-10 15:42:26.310
57232,2873.0,2,,57187.0,,,,CC BY-SA 3.0,"<p>The <code>ns</code> function (and other spline functions) does its own ""centering"" of the data.  Consider this example:</p>

<pre><code>&gt; library(splines)
&gt; 
&gt; s1 &lt;- ns( 1:10, 3 )
&gt; s2 &lt;- ns( (1:10)-5, 3 )
&gt; 
&gt; all.equal(s1,s2)
[1] ""Attributes: &lt; Component 1: Mean relative difference: 0.9090909 &gt;""
[2] ""Attributes: &lt; Component 7: Mean relative difference: 0.9090909 &gt;""
&gt; all.equal(as.vector(s1),as.vector(s2))
[1] TRUE
</code></pre>

<p>So the centering of the data leads to the same splines as the uncentered data (other than the knot information in the attributes).  So centering your variable before computing a spline has no effect.  If you want to compare the values at a point other than 0 then just use the <code>predict</code> function to get the actual predictions at the point of interest and compare (subtract).</p>
",2013-10-10 15:46:38.177
57233,5237.0,2,,57229.0,,,,CC BY-SA 3.0,"<p><em>(I'll let someone else address the estimation of the missing data.  You may want to directly model the probability that the observation is each level of the unknown factor using knowledge of other covariate values, and possibly outside information, e.g., priors etc.  There are strategies such as <a href=""http://en.wikipedia.org/wiki/Propensity_score_matching"" rel=""nofollow"">propensity scores</a> that you might be able to use for this type of thing.  However, at first glance your approach looks reasonable to me.)</em>  </p>

<p>One note is that I can't tell from your description if you are weighting by <em>raw frequencies</em>.  If so, you want to divide these by $N$ to get the marginal <em>probabilities</em> instead.  </p>

<p>You are right that you are not handling level 3 correctly.  The coding scheme that you use in your question set up is known as <em>reference level coding</em>.  To use this approach correctly, you need to have an intercept (i.e., $\beta_0$), which estimates the mean of level 3.  I suspect you do have such, even though you didn't list it.  In this case, you would just add the intercept to your final equation.  That is: 
$$
\beta_0\!*\!f_3 + \beta_1\!*\!f_1 + \beta_2\!*\!f_2
$$
Note that you are multiplying the intercept (which encodes the reference level) by the marginal probability that the observation is actually the reference level.  </p>
",2013-10-10 15:56:30.450
57234,2873.0,2,,57228.0,,,,CC BY-SA 3.0,"<p>You should read through the Spatial and possibly SpatialTemporal <a href=""http://cran.r-project.org/web/views/"" rel=""nofollow"">Taskviews on CRAN</a>.  Those will give you an idea of what packages are available and gives brief descriptions of what they do and how they compare.</p>
",2013-10-10 16:05:27.410
57235,10060.0,2,,57228.0,,,,CC BY-SA 3.0,"<p>R by itself does not handle GIS type of work but with different add-ons it can be a quite potent GIS device. You'd need to understand the idea of ""package"" (user-contributed scripts) and how to use <code>install.packages(""whateverPackage"")</code> command to install them.</p>

<p>I don't use R in GIS enough to show you the whole topography (pun totally intended), but the most commonly used packages I have seen are <code>map</code>, <code>ggmap</code>, <code>ggplot2</code>, <code>RgoogleMaps</code>, and <code>plotGoogleMap</code>.</p>

<p>Also, check out some sites and tutorials about this topic: <a href=""http://www.nyu.edu/projects/politicsdatalab/workshops/GISwR.pdf"" rel=""nofollow"">1</a>, <a href=""http://cran.r-project.org/web/views/Spatial.html"" rel=""nofollow"">2</a>, <a href=""http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf"" rel=""nofollow"">3</a>, and <a href=""http://www.icesi.edu.co/CRAN/web/packages/plotGoogleMaps/vignettes/plotGoogleMaps-intro.pdf"" rel=""nofollow"">4</a>. These got me started and within a day I could make some silly maps.</p>

<p>Lastly, this <a href=""http://statacumen.com/teach/SC1/SC1_16_Maps.pdf"" rel=""nofollow"">pdf</a> probably contains some codes pertinent to the map you wish to create. Its $\LaTeX$ format is a bit off, but you can still get some general functionality and key commands.</p>

<p>Good luck!</p>
",2013-10-10 16:08:02.663
57236,22572.0,1,,,,"Kaplan Meier - Can I use to assess recovery of function, not just loss?",<survival><kaplan-meier>,CC BY-SA 3.0,"<p>In my experience and readings, Kaplan-Meier has always been used to calculate differential survival between a certain number of groups. However, I'm looking to assess how time to recovery from a certain event as measured by activity levels. At time zero, everyone is essentially ""dead"" (non-mobile), and with time they regain mobility. 
seems like a ""negative"" Kaplan-Meier, is that possible? Or should I be looking at a different modeling strategy?</p>
",2013-10-10 16:10:12.377
57237,22570.0,1,,,,Generating causally dependent random variables,<monte-carlo><random-generation>,CC BY-SA 3.0,"<p>I'm trying to generate sets of causally connected random variables and started off doing this with a monte carlo approach.</p>

<p>The baseline is a 2-dimensional measured histogram from which I draw random values.</p>

<p>In my concrete examples these variables are acceleration $\bf{a}$ and velocity $\bf{v}$ - so obviously
$v_{i+1} = v_{i} + a_i * dt$
has to hold.</p>

<p>My current naive approach is:</p>

<p>I start with a some $v_0$.
Then I generate a random $a_0$ according to the measured probability of $\bf{a}$ for the value of $v_0$. Using this $a_0$ I can calculate $v_1$ and the whole procedure starts over again.</p>

<p>So when I check the generated accelerations $\bf{a}$ in bins of $\bf{v}$ everything's fine.
But I obviously this does not at all respect the marginal distribution of $\bf{v}$.</p>

<p>I'm kind of familiar with basic monte carlo methods, though lacking some theoretical background as you might guess.
I'd be fine if the two variables where <em>just</em> connected by some correlation matrix, but the causal connection between the two gives me headaches.</p>

<p>I didn't manage to find an example for this kind of problem somewhere - I might be googl'ing the wrong terms.
I'd be satisfied if somebody could point me to some literature/example or promising method to get a hold on this.</p>

<p>(Or tell me that's is not really possible given my inputs - that's what I'm guessing occasionally...)</p>

<p><strong>EDIT:</strong></p>

<p>The actual aim of this whole procedure:
I have a set of measurements $\bf{a}$ and $\bf{v}$, represented in a two-dimensional histogram $N(a,v)$. Given this input I'd like to generate sets of random $\bf{a_r}$ and $\bf{v_r}$ that reproduce the measured distribution.</p>
",2013-10-10 16:26:50.467
57724,9175.0,2,,57668.0,,,,CC BY-SA 3.0,"<p>I had misunderstood the question originally. I have edited the question now and here is a brief answer since I have already put up the question.</p>

<p>I used the following transformations</p>

<p>$U =\frac{Y}{X}$ </p>

<p>$V = X$</p>

<p>Then use the standard bivariate transformation short cut for one-to-one functions</p>
",2013-10-17 18:40:14.853
57238,8414.0,1,,,,Simulating data to fit a mediation model,<r><regression><simulation><random-generation><mediation>,CC BY-SA 3.0,"<p>I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by <a href=""https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&amp;_Kenny_1986.pdf"" rel=""noreferrer"">Barron and Kenny (1986)</a> and described elsewhere such as <a href=""http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf"" rel=""noreferrer"">Judd, Yzerbyt, &amp; Muller (2013)</a>, mediation models for outcome $Y$, mediator $\newcommand{\med}{\rm med} \med$, and predictor $X$ and are governed by the following three regression equations:
\begin{align}
Y    &amp;= b_{11} + b_{12}X + e_1                \tag{1}  \\
\med &amp;= b_{21} + b_{22}X + e_2                \tag{2}  \\
Y    &amp;= b_{31} + b_{32}X + b_{32} \med + e_3  \tag{3}
\end{align}
The indirect effect or mediation effect of $X$ on $Y$ through $\med$ can either be defined as $b_{22}b_{32}$ or, equivalently, as $b_{12}-b_{32}$.  Under the old framework of testing for mediation, mediation was established by testing $b_{12}$ in equation 1, $b_{22}$ in equation 2, and $b_{32}$ in equation 3.</p>

<p>So far, I have attempted to simulate values of $\med$ and $Y$ that are consistent with values of the various regression coefficients using <code>rnorm</code> in <code>R</code>, such as the code below:</p>

<pre><code>x   &lt;- rep(c(-.5, .5), 50)
med &lt;- 4 + .7 * x + rnorm(100, sd = 1) 

# Check the relationship between x and med
mod &lt;- lm(med ~ x)
summary(mod)

y &lt;- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)

# Check the relationships between x, med, and y
mod &lt;- lm(y ~ x + med)
summary(mod)

# Check the relationship between x and y -- not present
mod &lt;- lm(y ~ x)
summary(mod)
</code></pre>

<p>However, it seems that sequentially generating $\med$ and $Y$ using equations 2 and 3 is not enough, since I am left with no relationship between $X$ and $Y$ in regression equation 1 (which models a simple bivariate relationship between $X$ and $Y$) using this approach.  This is important because one definition of the indirect (i.e., mediation) effect is $b_{12}-b_{32}$, as I describe above.</p>

<p>Can anyone help me find a procedure in R to generate variables $X$, $\med$, and $Y$ that satisfy constraints that I set using equations 1, 2, and 3?</p>
",2013-10-10 16:41:50.710
57239,22571.0,1,,,,Quantifying the relationship between two disparate time series,<regression><time-series>,CC BY-SA 3.0,"<p>I have two time series that have a roughly similar trend, though both variables are noisy. This graph shows means and standard errors throughout a season of measurements.</p>

<p><img src=""https://i.stack.imgur.com/7ThlT.png"" alt=""enter image description here""></p>

<p>I'd like to be able to make a quantitative statement about the relationship between these two data sets.</p>

<p>While the two data sets were collected from the same experimental plots, the individual samples from which the means and standard errors were calculated are not meaningfully paired with one another, and you can see that the carbohydrate data set was measured more frequently.</p>

<p>By taking a subset of the carbohydrate measurements that are closest to the microbial biomass measurement dates, I can make a scatterplot showing the means and standard errors that I think gives a fair visual representation of the relationship (TRS.ml is the carbohydrates):</p>

<p><img src=""https://i.stack.imgur.com/kLvc0.png"" alt=""enter image description here""></p>

<p>This is where I am stuck. I'm not sure how to estimate regression coefficients or calculate an r2 value for a regression of this sort where I have estimates of uncertainty for both variables. Here are some approaches I have been considering:</p>

<ol>
<li><p>Deming regression. I'm not sure that this would be the right approach. It seems to be more for data sets in which the same technique was used for both variables. If it is, my question is how would I calculate the variance ratio based on the information I have?</p></li>
<li><p>Regression of all underlying data points. This doesn't really work because the data are not meaningfully paired, so of the 80 or so microbial biomass measurements that underlie the data shown in the graphs here, I can't directly match them to individual measurements of carbohydrates. Matching them arbitrarily seems bad.</p></li>
<li><p>Regression of carbohydrate means by date against microbial biomass means by date. Basically regress the points in my scatterplot above but throw out the information about the uncertainty. This gives a high r2 driven by the coinciding peaks on July 1st, but to me, seems to overestimate the strength of the relationship.</p></li>
<li><p>Regression of all microbial biomass values against carbohydrate means by date or vice versa. This allows more of the underlying uncertainty to be incorporated while not forcing the pairing of unrelated data points in an arbitrary way. Again though, it does not incorporate the uncertainty in both variables.</p></li>
</ol>

<p>My question is which of these approaches, or any other unlisted approaches, would you recommend for quantifying the relationship between these two time series?</p>
",2013-10-10 16:58:21.140
57240,22527.0,2,,40870.0,,,,CC BY-SA 3.0,"<p>I believe you multiply by 2 because you need to control for your database being twice as large. There are other ways to do this calculation such as  </p>

<p>decoy spectra identified /  target spectra identified</p>

<p>The use of the term FDR for this calculation is totally confusing and is why people have started calling it target/decoy rate in the last year or so. It's also doubly confusing as people often fail to specify if they are using a spectra target/decoy or a peptide target/decoy...and there is no way to consistently calculate a protein target/decoy as different programs will weigh peptide --> protein evidence differently...It's a mess...Having said that I will always make this calculation just to double check I or the software has not done something stupid. For that it is very useful. </p>
",2013-10-10 16:59:34.360
57241,5237.0,2,,57238.0,,,,CC BY-SA 4.0,"<p>This is quite straightforward.  The reason you have no relationship between <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span> using your approach is because of the code:  </p>

<pre><code>y &lt;- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)
</code></pre>

<p>If you want some relationship between <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span> even when <span class=""math-container"">${\rm med}$</span> is included (that is, you want <em>partial</em> mediation), you would simply use a non-zero value for <span class=""math-container"">$b_{32}$</span> instead.  For example, you could substitute the following code for the above:  </p>

<pre><code>y &lt;- 2.5 + 3 * x + .4 * med + rnorm(100, sd = 1)
</code></pre>

<p>Thus, <span class=""math-container"">$b_{32}$</span> has been changed from <span class=""math-container"">$0$</span> to <span class=""math-container"">$3$</span>.  (Of course some other, specific value would probably be more relevant, depending on your situation, I just picked <span class=""math-container"">$3$</span> off the top of my head.)  </p>

<hr>

<p><em>Edit:</em><br>
With respect to the marginal <span class=""math-container"">$x\rightarrow y$</span> relationship being non-significant, that is just a function of <a href=""http://en.wikipedia.org/wiki/Statistical_power"" rel=""nofollow noreferrer"">statistical power</a>.  Since the causal force of <span class=""math-container"">$x$</span> is passed entirely through <span class=""math-container"">${\rm med}$</span> in your original setup, you have lower power than you might otherwise.  Nonetheless, the effect is still <em>real</em> in some sense.  When I ran your original code (after having set the seed using <code>90</code> as a value that I again just picked off the top of my head), I did get a significant effect:  </p>

<pre><code>set.seed(90)
x &lt;- rep(c(-.5, .5), 50)
med &lt;- 4 + .7 * x + rnorm(100, sd = 1) 

# Check the relationship between x and med
mod &lt;- lm(med ~ x)
summary(mod)

y &lt;- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)

# Check the relationships between x, med, and y
mod &lt;- lm(y ~ x + med)
summary(mod)

# Check the relationship between x and y -- not present
mod &lt;- lm(y ~ x)
summary(mod)

...
Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   3.8491     0.1151  33.431   &lt;2e-16 ***
x             0.5315     0.2303   2.308   0.0231 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

...
</code></pre>

<p>To get more power, you can increase the <span class=""math-container"">$N$</span> you are using, or use smaller error values (i.e., use <code>sd=</code> values less than the default <code>1</code> in the <code>rnorm()</code> calls).  </p>
",2013-10-10 17:09:45.623
57242,22573.0,1,,,,Estimating multivariate normal distribution by observing variance in different directions,<normal-distribution><multivariate-analysis>,CC BY-SA 3.0,"<p>Assume I am looking for a normal distribution $\mathcal{N}(\mu,\Sigma)$. For simplicity let's say we only have 2 random variables $x$ and $y$ and a known $\mu=0$.</p>

<p>Is it possible to estimate $\Sigma$ by observing the variance along multiple directions?</p>

<p>For example, I measure the variance $\sigma_1$ along the vector $\mathbb{v}_1 = (x_1,y_1)^T$. In another step I obtain a different variance $\sigma_2$ from a different direction $\mathbb{v}_2 = (x_2,y_2)^T$. Ideally one would continue to observe these single variations in different directions and combine them in one multivariate normal distribution.</p>

<p>Does this make sense?</p>

<p>EDIT:
Some additional background information might be useful: I have a sensor device with known position and orientation in 2D space (in a future step both may have an uncertainty). The sensor is able to measure only the distance of a point along its orientation. I'm also given the sensor model. So for each distance measure $d_i$, I obtain the standard error $\sigma(d_i)$ which depends on the distance.</p>

<p>Since I'm not able to manipulate the sensor position to my advantage or perform a large number of measurements, I'd like to combine these variances into one covariance matrix in order to make a more reliable prediction of the position of the measured point.</p>

<p>This is just a thought that is still under development with no guaranty to work out correctly. Hence my question of ""making sense""...</p>
",2013-10-10 18:29:33.973
58449,2164.0,2,,54234.0,,,,CC BY-SA 3.0,"<p>You should be using ARIMA, but you need to consider deterministic impacts like outliers, level shifts, changes in trend, changes in seasonality, changes in parameters and changes in variance.  Look to the work of Ruey Tsay, Nathan Balke, Gregory Chow, Fox, Box, I. Chang.</p>
",2013-10-29 18:32:41.107
57243,20473.0,2,,57221.0,,,,CC BY-SA 3.0,"<p>It appears you are confusing results that hold for a collection of random variables with the case of <em>one</em> random variable.</p>

<p>When you have a series of observations, $x_1,...,x_n$, then <strong><em>if</em></strong> they are <em>contemporaneous</em>, ($x_{1t},...,x_{nt}$) they are considered as realizations of <em>n distinct random variables</em> (that may be identically and independently distributed, or not). You <em>cannot</em>, in this case, assume that all are realizations of the <em>same</em> random variable, because a random variable is a real-valued function: this means that at a specific point in time, it can have only one realization (take one value), otherwise it wouldn't be a function but a correspondence: this is why when we have a <em>cross-sectional</em> sample of size $n$, we say that ""it is comprised of the realization of $n$ random variables"", and not  ""$n$ realizations of the same random variable"".  Note carefully that ""same"" does not just mean ""identically distributed"", but ontologically equal.</p>

<p>Assume now that you have a time-series, and the index $1,...,n$ represents different points in time. Can you say that they are all realizations of the <em>same</em> random variable? Well in principle you can, but here too, we tend to view a time series as a stochastic process of <em>distinct</em> random variables (one for each point in time), that, again, may be identically distributed.  </p>

<p>So in general, when looking at a sample, be it cross-sectional or time series, it is advisable to think of it as a collection of realizations of <em>many</em> random variables.</p>

<p>Now, when we subtract the mean from a random variable, and divide by the standard deviation, we create the ""standardized"" version of the variable, that has mean zero and variance (and standard deviation) unity. This is irrespective of the distribution that this variable follows, because, by standard universal properties of these distribution moments
$$Z = \frac {X-\mu}{\sigma} \Rightarrow E(Z) = \frac {1}{\sigma}E(X) - \frac {\mu}{\sigma} = 0$$</p>

<p>and</p>

<p>$$ \text {Var}(Z) = \text {Var}\left(\frac {X-\mu}{\sigma}\right) = \frac {1}{\sigma^2}\text {Var}(X) = \frac {\sigma^2}{\sigma^2} =1$$</p>

<p>The standardized version $Z$ of <em>one</em> random variable $X$ follows a distribution that belongs to the same family as the distribution of $X$, with different said parameters - the distribution <em>family</em> does not change. So if you don't know the distribution by other means, the distribution of the standardized version will remain unknown.</p>

<p>Now consider the random variable $S_n = \frac 1n\sum_{i=1}^nX_i$. It is for this and like quantities that the various <a href=""http://en.wikipedia.org/wiki/Central_limit_theorem"" rel=""nofollow"">Central Limit Theorems</a> talk about and tell that they approach a normal distribution asymptotically.</p>
",2013-10-10 18:52:09.813
57244,20179.0,1,,,,SNP genotype coding in regression,<regression><categorical-data><genetics>,CC BY-SA 3.0,"<p>I would like to conduct some analysis on some biological traits with regression model.
The response variable is continuous. One important independent variable is the SNP information (wildtype, heterozygous, or homozygous). There are different ways to code it.
It can be treated as a nominal or a ordinal variable (like 1, 2, 3). Any one familar with the difference and any classical references about it? 
Thank you for any suggestion.</p>
",2013-10-10 18:54:07.593
57245,346.0,2,,57223.0,,,,CC BY-SA 3.0,"<p>I take the freedom to answer the question in the title, how would I analyze this data.</p>

<p>Given that we have replicates within the samples, mixed models immediately come to mind, which should estimate the variability within each individual and control for it.</p>

<p>Hence I fit the model using <code>lmer</code> from <code>lme4</code>. However, as we are interested in p-values, I use <code>mixed</code> from <code>afex</code> which obtains those via <code>pbkrtest</code> (i.e., Kenward-Rogers approximation for degrees-of-freedom). (afex also already sets the contrast to <code>contr.sum</code> which I would use in such a case anyway)</p>

<p>To control for the zero floor effect (i.e., positive skew), I fit two alternative versions transforming the dependent variable either with <code>sqrt</code> for mild skew and <code>log</code> for stronger skew.</p>

<pre><code>require(afex)

# read the dput() in as dat &lt;- ...    
dat &lt;- as.data.frame(dat)
dat$Group &lt;- factor(dat$Group)
dat$Subject &lt;- factor(dat$Subject)

(model &lt;- mixed(Value ~ Group + (1|Subject), dat))
##        Effect    stat ndf ddf F.scaling p.value
## 1 (Intercept) 237.730   1  15         1  0.0000
## 2       Group   7.749   2  15         1  0.0049

(model.s &lt;- mixed(sqrt(Value) ~ Group + (1|Subject), dat))
##        Effect    stat ndf ddf F.scaling p.value
## 1 (Intercept) 418.293   1  15         1  0.0000
## 2       Group   4.121   2  15         1  0.0375

(model.l &lt;- mixed(log1p(Value) ~ Group + (1|Subject), dat))
##        Effect    stat ndf ddf F.scaling p.value
## 1 (Intercept) 458.650   1  15         1  0.0000
## 2       Group   2.721   2  15         1  0.0981
</code></pre>

<p>The effect is significant for the untransformed and <code>sqrt</code> dv. But are these model sensible? Let's plot the residuals.</p>

<pre><code>png(""qq.png"", 800, 300, units = ""px"", pointsize = 12)
par(mfrow = c(1, 3))
par(cex = 1.1)
par(mar = c(2, 2, 2, 1)+0.1)
qqnorm(resid(model[[2]]), main = ""original"")
qqline(resid(model[[2]]))
qqnorm(resid(model.s[[2]]), main = ""sqrt"")
qqline(resid(model.s[[2]]))
qqnorm(resid(model.l[[2]]), main = ""log"")
qqline(resid(model.l[[2]]))
dev.off()
</code></pre>

<p><img src=""https://i.stack.imgur.com/WXUEh.png"" alt=""enter image description here""></p>

<p>It seems that the model with <code>sqrt</code> trasnformation provides a reasonable fit (there still seems to be one outlier, but I will ignore it). So, let's further inspect this model using <code>multcomp</code> to get the comparisons among groups:</p>

<pre><code>require(multcomp)

# using bonferroni-holm correction of multiple comparison
summary(glht(model.s[[2]], linfct = mcp(Group = ""Tukey"")), test = adjusted(""holm""))
##          Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lmer(formula = sqrt(Value) ~ Group + (1 | Subject), data = data)
## 
## Linear Hypotheses:
##            Estimate Std. Error z value Pr(&gt;|z|)  
## 2 - 1 == 0  -0.0754     0.3314   -0.23    0.820  
## 3 - 1 == 0   1.1189     0.4419    2.53    0.023 *
## 3 - 2 == 0   1.1943     0.4335    2.75    0.018 *
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## (Adjusted p values reported -- holm method)

# using default multiple comparison correction (which I don't understand)
summary(glht(model.s[[2]], linfct = mcp(Group = ""Tukey"")))
##          Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lmer(formula = sqrt(Value) ~ Group + (1 | Subject), data = data)
## 
## Linear Hypotheses:
##            Estimate Std. Error z value Pr(&gt;|z|)  
## 2 - 1 == 0  -0.0754     0.3314   -0.23    0.972  
## 3 - 1 == 0   1.1189     0.4419    2.53    0.030 *
## 3 - 2 == 0   1.1943     0.4335    2.75    0.016 *
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## (Adjusted p values reported -- single-step method)
</code></pre>

<p>Punchline: group 3 differs from the other two groups which do not differ among each other.</p>
",2013-10-10 18:54:30.263
57246,22059.0,2,,57242.0,,,,CC BY-SA 3.0,"<p>You could, but it doesn't make too much sense to do it this way.  Since there are ${n+1}\choose 2$ independent dimensions in a covariance matrix, you'd need to measure the variance in ${n+1}\choose 2$ different directions, and then do some linear algebra to reconstruct the covariance matrix.  It's more straightforward just to measure the covariance matrix all at once.  </p>

<p>For example, in $2$ dimensions, you could measure the variance along $(0,1)$, $(0,1)$ and $(1,1)$, then since $v_{(1,1)}=\Sigma_{11}+\Sigma_{22}+2\Sigma_{12}$, you would end up with
$$\Sigma=\left(\array{v_{(1,0)} &amp; \frac{v_{(1,1)}-v_{(1,0)}-v_{(0,1)}}{2}\\ \frac{v_{(1,1)}-v_{(1,0)}-v_{(0,1)}}{2} &amp; v_{(0,1)}}\right)$$</p>

<p>I suppose if somehow you were constrained in such a way that you couldn't sample from the entire distribution, but you were only able to sample from projections onto a single dimension at a time, then this approach could be useful.</p>
",2013-10-10 19:06:15.007
57247,20286.0,2,,57012.0,,,,CC BY-SA 3.0,"<p>Unless you have reason to believe that LabA and LabB would systematically provide different results if they ever measured the same set of samples, your data from the 2 Labs ""are on the same scale"" as far as you can tell from these data. The problem is that the less-sensitive LabA will be unable to report a value for some samples that would have had values reported if LabB had instead done the analysis.</p>

<p>Perhaps the best way to proceed would be to define a class of results called ""&lt;0.2"", and include in that class all 0 readings from LabA and all readings &lt;0.2 from LabB. How you proceed from there depends on ""What is the question of interest?"" as @Glen_b put it in a comment.</p>

<p>All will be much more useful and reliable if it is possible to cross-compare a set of samples analyzed by both Labs, because there may be systematic differences between the 2 Labs' results that you don't suspect.</p>
",2013-10-10 19:08:35.603
57248,10278.0,2,,57244.0,,,,CC BY-SA 3.0,"<p>If you treat the variable as ordinal you are assuming a gene-dosage effect.
This is essentially a one degree of freedom test since you are testing whether the slope of the regression line is significantly different from $0$.  If you treat the variable as nominal you are not assuming any gene-dosage effect and instead you are doing a one way ANOVA with 3 groups so that's a two degrees of freedom test.
The gene-dosage model (treating genotypes as ordinal) is more powerful because you are using information about the genotype groups (whether the group has 0, 1 or 2 copies of the wild type allele) whereas in the categorical approach your model knows nothing about the genotype groups (they could just be called A, B and C).  Treating the genotype as ordinal is the preferred approach.  Also I should mention that if you believe that for example the wild-type allele is dominant then you can merge the heterozygous individuals into the wild-type homozygous group and treat them as one group.</p>
",2013-10-10 19:25:29.377
57249,19264.0,1,57256.0,,,Generic sum of Gamma random variables,<probability><distributions><gamma-distribution><sum><saddlepoint-approximation>,CC BY-SA 3.0,"<p>I have <a href=""http://en.wikipedia.org/wiki/Gamma_distribution#Summation"" rel=""noreferrer"">read</a> that the sum of Gamma random variables with the same scale parameter is another Gamma random variable. I've also seen the paper by <a href=""http://www.ism.ac.jp/editsec/aism/pdf/037_3_0541.pdf"" rel=""noreferrer"">Moschopoulos</a> describing a method for the summation of a general set of Gamma random variables. I have tried implementing <a href=""http://www.ism.ac.jp/editsec/aism/pdf/037_3_0541.pdf"" rel=""noreferrer"">Moschopoulos'</a> method but have yet to have success.</p>

<p>What does the summation of a general set of Gamma random variables look like? To make this question concrete, what does it look like for:</p>

<p>$\text{Gamma}(3,1) + \text{Gamma}(4,2) + \text{Gamma}(5,1)$</p>

<p>If the parameters above are not particularly revealing, please suggest others.</p>
",2013-10-10 19:49:21.903
57255,19545.0,1,,,,Why are ERR (Expected Reciprocal Ranking) scores not normalized?,<ranking><information-retrieval>,CC BY-SA 3.0,"<p>It seems to me that normalized ERR (<a href=""http://don-metzler.net/papers/metzler-cikm09.pdf"" rel=""nofollow"">Expected Reciprocal Ranking</a>) scores (ERR scores of your ranking algorithm divided by ERR score calculated for the ground truth ranking) are more useful than the unscaled ERR scores, but I have not seen normalized scores being reported in the literature. Is there a good reason that the ERR scores are reported in raw rather than normalized format?</p>
",2013-10-10 20:53:13.273
57283,9074.0,2,,57282.0,,,,CC BY-SA 4.0,"<p>You're asking two questions:</p>
<ol>
<li>Is there a generic test for unimodality?</li>
<li>Are there tests to test whether a sample is derived from a given distribution, say, a normal distribution?</li>
</ol>
<p>Ad 1): Yes, the Hartigan-Hartigan dip test, <a href=""https://projecteuclid.org/journals/annals-of-statistics/volume-13/issue-1/The-Dip-Test-of-Unimodality/10.1214/aos/1176346577.full"" rel=""nofollow noreferrer"">Ann. Statist. 13(1):70-84</a>.</p>
<p>Ad 2): There exists a number of special tests, but the <a href=""https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"" rel=""nofollow noreferrer"">Kolmogorov-Smirnov</a> test is a general-purpose nonparametric test, although with low statistical power.</p>
",2013-10-11 08:43:06.977
57250,22577.0,1,,,,How to determine degree of freedom for a certain test of interaction?,<interaction><degrees-of-freedom><stratification>,CC BY-SA 3.0,"<p>The scenario is like this:</p>

<p>I have a cohort with 2000 people, half of them taking DRUG, the other half not taking it. I would like to check interactions between DRUG and the other variables in the model:</p>

<ul>
<li><p><strong>Method 1:</strong></p>

<p>Firstly I got a original model: <code>y1=a1*AGE+b1*BMI+c1*DRUG</code>ï¼Œ[<code>DRUG</code> is binary: yes-1, no-0]; I got a likelihood 1;</p>

<p>If I want to test the interaction of <code>AGE</code>, <code>BMI</code> and <code>DRUG</code>, I need another model: <code>y2=a2*AGE+b2*BMI+c2*DRUG+d*(DRUG*AGE)+e*(DRUG*BMI)</code>; I got a likelihood 2;</p>

<p>Then I compare the likelihood of these two models using chi-square test (df=2), and see whether the difference (likelihood 2 minus likelihood) is significant. </p></li>
<li><p><strong>Method 2:</strong></p>

<p>Stratify people into two groups according to DRUG status:</p>

<p>Group 1: for people taking DRUG (n=1000), model 1: <code>y1=a1*AGE+b1*BMI</code>, I got a likelihood 1 (L1);</p>

<p>Group 2: for people not taking DRUG (n=1000), model 2: <code>y2=a2*AGE+b2*BMI</code>, likelihood 2 (L2);</p>

<p>Then I use all the people (n-2000), model 3:y3=a3*AGE+b3*BMI+d*(DRUG*AGE)+e*(DRUG*BMI), likelihood 3 (L3);</p></li>
</ul>

<p>So in order to test the interactions, chi-square=L3/(L1*L2). But the question is: What is the degree of freedom (df)?</p>

<p>Can anyone help? I cannot get the answer.</p>
",2013-10-10 19:57:14.333
57251,22578.0,1,,,,Convergence theorem for Gibbs sampling,<sampling><simulation><markov-process><convergence><gibbs>,CC BY-SA 4.0,"<p>The convergence theorem for Gibbs sampling states:</p>

<p>Given a random vector <span class=""math-container"">$X$</span> with components <span class=""math-container"">$X_1,X_2,...X_K$</span> and the knowledge about the conditional distribution of <span class=""math-container"">$X_k$</span> we can find the actual distribution using Gibbs Sampling infinitly often.</p>

<p>The exact theorem as stated by book (<a href=""https://amzn.to/2DhoZZq"" rel=""nofollow noreferrer"">Neural Networks and Learning Machines</a>): </p>

<blockquote>
  <p>The random variable <span class=""math-container"">$X_k^{(n)}$</span>
  converges in distribution to the true probability distributions of
  <span class=""math-container"">$X_k$</span> for k=1,2,...,K as n approaches infinity</p>
  
  <p><span class=""math-container"">$\lim_{n \rightarrow  \infty}P(X^{(n)}_k \leq x | X(0)) = P_{X_k}(x) $</span> for <span class=""math-container"">$k
&gt; = 1,2,...,K$</span></p>
  
  <p>where <span class=""math-container"">$P_{X_k}(x)$</span>  is the marginal cumulative distribution function
  of <span class=""math-container"">$X_k$</span></p>
</blockquote>

<p>While doing research on this, for a deeper understanding, I ran across <a href=""https://stats.stackexchange.com/a/10216/31349"">this</a> answer. Which explains quite well how to pick a single sample using the Method, but I am not able to extend/modify it to fit the convergence theorem, as the result of the given example is one sample (spell) and not a final/actual probability distribution.</p>

<p><strong>Therefore, how do I have to modify that example to fit the convergence theorem?</strong></p>
",2013-10-10 19:59:40.663
57252,22580.0,1,,,,What are the pros and cons of standardizing variable in presence of an interaction?,<standardization>,CC BY-SA 3.0,"<p>I put this question because while reading the benefits of  standardizing explanatory variables or not, I read <em>good but contrasting</em> opinions about standardizing when there are interaction in the model. </p>

<p>Some talk about how problems of collinearity are removed when standardizing (e.g. <a href=""https://stats.stackexchange.com/questions/60476/collinearity-diagnostics-problematic-only-when-the-interaction-term-is-included#61022"">Collinearity diagnostics problematic only when the interaction term is included</a>), which is basically the case of my GLMM. However, others claim that standard errors and p-values of interactions of standardized models are not reliable... (e.g.<a href=""https://stats.stackexchange.com/questions/19216/variables-are-often-adjusted-e-g-standardised-before-making-a-model-when-is"">Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?</a> or <a href=""http://quantpsy.org/interact/interactions.htm"" rel=""nofollow noreferrer"">http://quantpsy.org/interact/interactions.htm</a>)</p>

<p>So, any ideas on what is the right thing to do?</p>
",2013-10-10 19:59:51.410
57253,22582.0,1,,,,Box Cox Transformation with swift,<r><regression><data-transformation><regression-strategies>,CC BY-SA 3.0,"<p>I am trying to do a box-cox transformation with swift. I have a dependent variable, annual foreign sales of companies (in US\$ thousands) which contains zeros, for a set of panel data. I have been advised to add a small amount, for example, 0.00001 to the annual foreign sales figures so that I can take the log, but I think box-cox transformation will produce a more appropriate constant than 0.00001. I have done a box-cox transformation on R with the codes below, but it has given me a very large lambda2 of 31162.8.</p>

<pre><code>library(geoR)
boxcoxfit(bornp$ForeignSales, lambda2 = TRUE)
#R output - Fitted parameters:
# lambda lambda2 beta sigmasq 
# -1.023463e+00 3.116280e+04 9.770577e-01 7.140328e-11
</code></pre>

<p>My hunch is that the above value of lambda2 is very large, so I am not sure if I need to run the boxcoxfit with my independent variables like below:</p>

<pre><code>boxcoxfit(bornp$ForeignSales, bornp$family bornp$roa bornp$solvencyratio,lambda2=TRUE)
</code></pre>

<p>I am still trying to identify the best set of independent variables, so I am not sure if using the boxcoxfit with independent variables at this stage will work or is best.</p>

<p>Here's the description of the two lambda parameters from the help:</p>

<p><code>lambdaÂ Â Â Â Â Â </code> numerical value(s) for the transformation parameter $\lambda$. Used as the initial value<br>
<code>Â Â Â Â Â Â Â Â Â Â Â Â </code> in the function for parameter estimation. If not provided default values are as-<br>
<code>Â Â Â Â Â Â Â Â Â Â Â Â </code> sumed. If multiple values are passed the one with highest likelihood is used as<br>
<code>Â Â Â Â Â Â Â Â Â Â Â Â </code> initial value.<br>
<code>lambda2Â Â Â Â Â </code> logical or numerical value(s) of the additional transformation (see DETAILS<br>
<code>Â Â Â Â Â Â Â Â Â Â Â Â </code> below). Defaults to <code>NULL</code>. If <code>TRUE</code> this parameter is also estimated and the initial<br>
<code>Â Â Â Â Â Â Â Â Â Â Â Â </code> value is set to the absolute value of the minimum data. A numerical value is<br>
<code>Â Â Â Â Â Â Â Â Â Â Â Â </code> provided it is used as the initial value. Multiple values are allowed as for<br>
<code>Â Â Â Â Â Â Â Â Â Â Â Â </code> lambda.</p>
",2013-10-10 20:15:15.787
57254,22583.0,1,,,,variance of summation/compound variable?,<variance><aggregation><sum>,CC BY-SA 3.0,"<p>here is my situation. I am weighting a packet of material that has 10 individual units in it. In the end of the day I would like to know the average weight and variance of the individual units but the problem is that I cannot weight each unit individually since I would have to destroy the packet to get to the individual units. So in lieu of this, I am trying to make an inference of the individual units from what I know about the packets. I weighed 10 packets (hence I have 100 individual units). I was able to figure out the average weight of the units but am having trouble with the variance. Here is what I have done so far:</p>

<p>$$
\begin{split}
\bar{y}&amp;=\frac{1}{10}\sum^{10}_{i=1}y_i\\
       &amp;=\frac{1}{10}\sum^{10}_{i=1}  (x_{i,1}+x_{i,2}+...+x_{i,10})~~[since~y_i=x_{i,1}+x_{i,2}+...+x_{i,10}]\\
       &amp;=\frac{1}{10}\sum^{100}_{j=1}x_j\\
       &amp;=\frac{1}{10}(100~\bar{x})=10~\bar{x}
\end{split}
$$</p>

<p>thus we have the average of $x$, $\bar{x}=\frac{\bar{y}}{10}.$ But now my challenge is how to do I find variance of $x$ given the variance of $y$? Any suggestions? Thanks!</p>

<p>::::UPDATE::::</p>

<p>After some thought I came up with this reasoning:
$$
\begin{split}
\frac{1}{10}var(y)&amp;=var(\bar{y})\\
       &amp;=var(10~\bar{x})\\
       &amp;=100~var(\bar{x})\\
       &amp;=100~\frac{1}{100}var(x)~~[assuming~that~all~x~are~i.i.d.]\\
       &amp;=var(x)
\end{split}
$$</p>

<p>thus we have $var(x)=\frac{1}{10}var(y).$ I am correct in that if we assume that all the individual units share the same common variance and are independent of each other, this result holds?</p>
",2013-10-10 20:48:01.520
57256,668.0,2,,57249.0,,,,CC BY-SA 4.0,"<p>First, <strong>combine any sums having the same scale factor</strong>: a <span class=""math-container"">$\Gamma(n, \beta)$</span> plus a <span class=""math-container"">$\Gamma(m,\beta)$</span> variate form a <span class=""math-container"">$\Gamma(n+m,\beta)$</span> variate.</p>
<p>Next, observe that the characteristic function (cf) of <span class=""math-container"">$\Gamma(n, \beta)$</span> is <span class=""math-container"">$(1-i \beta  t)^{-n}$</span>, whence the cf of a sum of these distributions is the product</p>
<p><span class=""math-container"">$$\prod_{j} \frac{1}{(1-i \beta_j  t)^{n_j}}.$$</span></p>
<p>When the <span class=""math-container"">$n_j$</span> are all <em>integral,</em> <strong>this product expands as a partial fraction</strong> into a <em>linear combination</em> of <span class=""math-container"">$(1-i \beta_j  t)^{-\nu}$</span> where the <span class=""math-container"">$\nu$</span> are integers between <span class=""math-container"">$1$</span> and <span class=""math-container"">$n_j$</span>.  In the example with <span class=""math-container"">$\beta_1 = 1, n_1=8$</span> (from the sum of <span class=""math-container"">$\Gamma(3,1)$</span> and <span class=""math-container"">$\Gamma(5,1)$</span>) and <span class=""math-container"">$\beta_2 = 2, n_2=4$</span> we find</p>
<p><span class=""math-container"">$$\begin{aligned}&amp;\frac{1}{(1-i t)^{8}}\frac{1}{(1- 2i t)^{4}} = \\
&amp;\frac{1}{(t+i)^8}-\frac{8 i}{(t+i)^7}-\frac{40}{(t+i)^6}+\frac{160 i}{(t+i)^5}+\frac{560}{(t+i)^4}-\frac{1792 i}{(t+i)^3}\\
&amp;-\frac{5376}{(t+i)^2}+\frac{15360 i}{t+i}+\frac{256}{(2t+i)^4}+\frac{2048 i}{(2 t+i)^3}-\frac{9216}{(2t+i)^2}-\frac{30720 i}{2t+i}.
\end{aligned}$$</span></p>
<p>The inverse of taking the cf is the inverse Fourier Transform, which is <em>linear</em>: that means we may apply it term by term.  Each term is recognizable as a multiple of the cf of a Gamma distribution and so is readily <strong>inverted to yield the PDF</strong>.  In the example we obtain</p>
<p><span class=""math-container"">$$\begin{aligned}
&amp;\frac{e^{-t} t^7}{5040}+\frac{1}{90} e^{-t} t^6+\frac{1}{3} e^{-t} t^5+\frac{20}{3} e^{-t} t^4+\frac{8}{3} e^{-\frac{t}{2}} t^3+\frac{280}{3} e^{-t} t^3\\
&amp;-128 e^{-\frac{t}{2}} t^2+896 e^{-t} t^2+2304 e^{-\frac{t}{2}} t+5376 e^{-t} t-15360 e^{-\frac{t}{2}}+15360 e^{-t}
\end{aligned}$$</span></p>
<p>for the PDF of the sum.</p>
<p>This is a finite <em>mixture</em> of Gamma distributions having scale factors equal to those within the sum and shape factors less than or equal to those within the sum.  Except in special cases (where some cancellation might occur), the number of terms is given by the total shape parameter <span class=""math-container"">$n_1 + n_2 + \cdots$</span> (assuming all the <span class=""math-container"">$n_j$</span> are different).</p>
<hr />
<p>As a test, here is a histogram of <span class=""math-container"">$10^4$</span> results obtained by adding independent draws from the <span class=""math-container"">$\Gamma(8,1)$</span> and <span class=""math-container"">$\Gamma(4,2)$</span> distributions.  On it is superimposed the graph of <span class=""math-container"">$10^4$</span> times the preceding function.  The fit is very good.</p>
<p><img src=""https://i.stack.imgur.com/sOPCo.png"" alt=""Figure"" /></p>
<hr />
<p>Moschopoulos carries this idea one step further by expanding the cf of the sum into an <em>infinite</em> series of Gamma characteristic functions whenever one or more of the <span class=""math-container"">$n_i$</span> is non-integral, and then terminates the infinite series at a point where it is reasonably well approximated.</p>
",2013-10-10 20:58:56.470
57257,10570.0,2,,55043.0,,,,CC BY-SA 4.0,"<p>Your question is not really possible to answer unless you have additional information about the situation you are applying this to.</p>
<h3>Indistinguishable situations</h3>
<p>For the purposes of this, we'll assume that <span class=""math-container"">$X$</span>, <span class=""math-container"">$Y$</span>, and <span class=""math-container"">$Z$</span> are 0-mean multivariate normal distributions in <span class=""math-container"">$\mathbb{R}^d$</span>, and we're interested in one or more spectrum <span class=""math-container"">$\sigma_i$</span> (a vector of size <span class=""math-container"">$d$</span> with decreasing values, yada yada). I refer to the components of the spectrum as <em>eigenvalues</em>, without specifying that they're the eigenvalues of the covariance matrix.</p>
<ol>
<li><p>The true distribution is <span class=""math-container"">$X$</span> which has spectrum <span class=""math-container"">$\sigma_X$</span> with all non-zero values. There is no error, and we draw a large number of samples, estimating everything very accurately. Clearly all of the &quot;small&quot; eigenvalues still have &quot;information&quot; and aren't noise.</p>
</li>
<li><p>The true distribution  is <span class=""math-container"">$Y$</span> which has a spectrum <span class=""math-container"">$\sigma_Y$</span> with only 3 non-zero eigenvalues. There's noise, though, so we measure <span class=""math-container"">$Y+Z$</span>, where <span class=""math-container"">$\sigma_Z$</span> <em>does</em> have all non-zero eigenvalues. Let's suppose <span class=""math-container"">$Y$</span> and <span class=""math-container"">$Z$</span> are such that <span class=""math-container"">$\sigma_{Y+Z} = \sigma_X$</span>. Here, it's obvious that all but the top 3 eigenvalues are &quot;merely noise&quot;.</p>
</li>
</ol>
<p>My point is just that which parts of the spectrum can be attributed to &quot;noise&quot; is not a property of the sample.</p>
<h3>External criteria</h3>
<p>There potentially are external criteria that can help you distinguish the above situations, but they're sort of problem specific. For instance, in the <a href=""http://www.netflixprize.com/"" rel=""nofollow noreferrer"">Netflix Challenge</a>, a very successful technique for predicting movie ratings was based on SVD (which is also the basis of PCA). When using SVD-based algorithms for a prediction task, one is confronted with the same challenge you have: <em>&quot;How many non-zero components do I consider? How far do I reduce the dimensionality?&quot;</em> The answer is basically <a href=""http://en.wikipedia.org/wiki/Cross-validation_(statistics)"" rel=""nofollow noreferrer"">cross validation</a>. The more components you consider, the lower your training error is, but the more risk of overfitting. The validation error is a proxy for generalization error. So, you generally get a chart like:</p>
<p><img src=""https://i.stack.imgur.com/XZJfg.png"" alt=""Training/Validation Error as a function of Model Capacity"" /></p>
<p>If you're not doing a predictive problem, I don't really have useful advice, but I do imagine there might be <em>something</em> you want to measure that can help you define what it <em>means</em> for something to be &quot;signal&quot; vs &quot;noise&quot; in your application.</p>
",2013-10-10 21:11:46.330
57258,6162.0,2,,57223.0,,,,CC BY-SA 3.0,"<p>For information, the random-effect model given by @Henrik:</p>

<pre><code>&gt; f &lt;- function(x) sqrt(x)
&gt; library(lme4)
&gt; ( fit1 &lt;- lmer(f(Value) ~ Group + (1|Subject), data=dat) )
Linear mixed model fit by REML ['lmerMod']
Formula: f(Value) ~ Group + (1 | Subject) 
   Data: dat 
REML criterion at convergence: 296.3579 
Random effects:
 Groups   Name        Std.Dev.
 Subject  (Intercept) 0.5336  
 Residual             0.8673  
Number of obs: 108, groups: Subject, 18
Fixed Effects:
(Intercept)       Group2       Group3  
    3.03718     -0.07541      1.11886  
</code></pre>

<p>is equivalent to a generalized least-squares model with an exchangeable correlation structure for subjects:</p>

<pre><code>&gt; library(nlme)
&gt; fit2 &lt;-  gls(f(Value) ~ Group, data=dat, na.action=na.omit, correlation=corCompSymm(form= ~  1 | Subject)) 
</code></pre>

<p>The fitted variance matrix is then:</p>

<pre><code>&gt; getVarCov(fit2)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
[1,] 1.03690 0.28471 0.28471 0.28471 0.28471 0.28471
[2,] 0.28471 1.03690 0.28471 0.28471 0.28471 0.28471
[3,] 0.28471 0.28471 1.03690 0.28471 0.28471 0.28471
[4,] 0.28471 0.28471 0.28471 1.03690 0.28471 0.28471
[5,] 0.28471 0.28471 0.28471 0.28471 1.03690 0.28471
[6,] 0.28471 0.28471 0.28471 0.28471 0.28471 1.03690
  Standard Deviations: 1.0183 1.0183 1.0183 1.0183 1.0183 1.0183 
</code></pre>

<p>As you can see, the diagonal entry corresponds to the total variance in the first model:</p>

<pre><code>&gt; VarCorr(fit1)
 Groups   Name        Std.Dev.
 Subject  (Intercept) 0.53358 
 Residual             0.86731 
&gt; 0.53358^2+0.86731^2
[1] 1.036934
</code></pre>

<p>and the covariance corresponds to the between-subject variance:</p>

<pre><code>&gt; 0.53358^2
[1] 0.2847076
</code></pre>

<p>Actually the gls model is more general because it allows a negative covariance. The advantage of <code>nlme</code> is that you can more generally use other repeated correlation structures and also you can specify different variances per group with the <code>weights</code> argument.</p>

<p>I think that residuals are different because they are constructed with the random-effects in the first model. In order to get multiple comparisons you can use the <code>lsmeans</code> and the <code>multcomp</code> packages, but the $p$-values of the hypotheses tests are anticonservative with defaults (too high) degrees of freedom. Unfortunately, the <code>pbkrtest</code> package does not apply to <code>gls</code>/<code>lme</code> models.</p>
",2013-10-10 21:31:42.880
57259,6162.0,2,,57223.0,,,,CC BY-SA 3.0,"<p>Now, try to you write down the model: $y_{ijk} = ...$ where $y_{ijk}$ is the $k$-th value for individual $j$ of group $i$. Then look at what happens for the means $\bar y_{ij\bullet}$: you get a classical Gaussian linear model, with variance homogeneity because there are $6$ repeated measures for each subject:</p>

<pre><code>&gt; xtabs(~Group+Subject, data=dat)
     Subject
Group 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
    1 6 6 6 6 6 6 6 0 0  0  0  0  0  0  0  0  0  0
    2 0 0 0 0 0 0 0 6 6  6  6  6  6  6  6  0  0  0
    3 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  6  6  6
</code></pre>

<p>Thus, since you are interested in mean comparisons only, <strong>you don't need to resort to a random-effect or generalised least-squares model</strong> - just use a classical (fixed effects) model using the means $\bar y_{ij\bullet}$ as the observations:</p>

<pre><code>tdat &lt;- transform(dat, tvalue=f(Value))
dd &lt;- aggregate(tvalue~Group+Subject, data=tdat, FUN=mean)
fit3 &lt;- lm(tvalue~Group, data=dd)
</code></pre>

<p>I think this approach always correctly work when we average the data over the levels of a random effect (I show on <a href=""http://stla.github.io/stlapblog/posts/ModelReduction.html"" rel=""nofollow"">my blog</a> how this fails for an example with a fixed effect).</p>

<p>The ANOVA provides the same answer as @Henrik's approach (and that shows that Kenward-Rogers approximation is correct):</p>

<pre><code>&gt; anova(fit3)
Analysis of Variance Table

Response: tvalue
          Df Sum Sq Mean Sq F value  Pr(&gt;F)  
Group      2 3.3799 1.68994   4.121 0.03747 *
</code></pre>

<p>Then you can use <code>TukeyHSD()</code> or the <code>lsmeans</code> package for multiple comparisons: </p>

<pre><code>&gt; TukeyHSD(aov(fit3), ""Group"")
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = fit3)

$Group
           diff         lwr       upr     p adj
2-1 -0.07541248 -0.93627828 0.7854533 0.9719148
3-1  1.11885667 -0.02896441 2.2666777 0.0565628
3-2  1.19426915  0.06817536 2.3203629 0.0370434

&gt; library(lsmeans)
&gt; lsmeans(fit3, pairwise~Group)

$`Group pairwise differences`
         estimate        SE df  t.ratio p.value
1 - 2  0.07541248 0.3314247 15  0.22754 0.97191
1 - 3 -1.11885667 0.4418996 15 -2.53193 0.05656
2 - 3 -1.19426915 0.4335348 15 -2.75472 0.03704
    p values are adjusted using the tukey method for 3 means 
</code></pre>
",2013-10-10 21:54:03.493
57260,4779.0,2,,57065.0,,,,CC BY-SA 3.0,"<p>Looking at this as an outlier problem seems wrong to me.  If ""&lt; 10% of users spend at all"", you need to model that aspect.  Tobit or Heckman regression would be two possibilities.</p>
",2013-10-10 21:55:57.283
57267,22591.0,1,,,,Extract important features,<machine-learning><feature-selection><statistical-learning>,CC BY-SA 3.0,"<p>Here is my situation:
- A huge amount of data
- 600 features
- Only one class is provided
Now, my question is how can I reduce the number of features to important ones? In another word, all of these features (with data) are intending to predict only one class. but some of features have large impact on the prediction (means their variation come to higher probability). </p>
",2013-10-10 23:28:22.503
57268,18040.0,1,,,,Difference between two mixed effects models,<r><mixed-model><multilevel-analysis><random-effects-model>,CC BY-SA 3.0,"<p>I have a question about how to tell two different mixed effects models apart.  In the simple case both involve fitting a model with a random group effect and a covariate. I fit the model with <code>lme4</code> in <code>R</code>.  Here is a visualization of the two different scenarios.<br>
<img src=""https://i.stack.imgur.com/GgxYw.png"" alt=""enter image description here""></p>

<pre><code>library(ggplot2)
library(lme4)
gen_dat2 &lt;- function(group.m,group.v,int, sl,n){
      x &lt;- vector()
      y &lt;- vector()
      g &lt;- vector()
         for(i in 1:length(group.m)){
         x.t &lt;- rnorm(n,group.m[i],group.v[i])
         y.t &lt;- rnorm(n,group.m[i],group.v[i])+int + sl*x.t 
         x &lt;- c(x,x.t)
         y &lt;- c(y,y.t)
         g &lt;- c(g,rep(i,n))
        }
     return(cbind(x,y,g))
}

group.m &lt;- runif(5,1,20)
group.v &lt;- runif(5,1,1.5)

dat2 &lt;- data.frame(gen_dat2(group.m,group.v,1,4,14))
ggplot(dat2,aes(x=x,y=y,colour=as.factor(g),group=g))+geom_point()+stat_smooth(method=""lm"",se=F)
m2 &lt;- lmer(y~x + (x|g),data=dat2)
</code></pre>

<p>Then I can generate and fit the other scenario with similar code:</p>

<p><img src=""https://i.stack.imgur.com/pgGm7.png"" alt=""enter image description here""></p>

<pre><code> gen_dat &lt;- function(group.m,group.v,int, sl,n){
      x &lt;- vector()
      y &lt;- vector()
      g &lt;- vector()
         for(i in 1:length(group.m)){
         x.t &lt;- rnorm(n,0,1)
         y.t &lt;- rnorm(n,group.m[i],group.v[i])+int + sl*x.t 
         x &lt;- c(x,x.t)
         y &lt;- c(y,y.t)
         g &lt;- c(g,rep(i,n))
        }
     return(cbind(x,y,g))
}

group.m &lt;- runif(5,1,20)
group.v &lt;- runif(5,1,1.5)

dat1 &lt;- data.frame(gen_dat(group.m,group.v,1,4,14))
ggplot(dat1,aes(x=x,y=y,colour=as.factor(g),group=g))+geom_point()+stat_smooth(method=""lm"",se=F)
m1 &lt;- lmer(y~x + (x|g),data=dat1)
</code></pre>

<p>My central question is how do I tell these two models apart?  Am I incorrectly fitting the first one, and I need an extra term in there to model the relationships between groups and the x variable as well as y?  Both detect substantial between group variation in the intercept and not much in the slope as I would predict.  But I need a way to tell these two apart.  Any thoughts would be helpful.  </p>

<hr>

<p>Edits:</p>

<p>This has been helpful in me restating the question.  So I want to re-ask the question with an example which I hope will make it clear why I want to be able to tell these two models apart. Let's imagine that Y is the average student test score at a school, and X is spending per student in that school.  Our grouping variables are 5 different school districts.  </p>

<p>Data in the top figure shows that an increase in spending within a district means that test scores increase.  It also shows that between districts there are differences is scores, but that's clearly because some districts spend more student than others.</p>

<p>Data in the second figure show similarly that within a district student scores increase as spending increases.  It also shows that between districts there are differences in test scores.  However we don't know what is driving those differences, unlike in the first set of data.  This is a pretty common situation I've encountered in building models.  The former is not.  </p>

<p>So what I'm asking is what is the appropriate model that captures the following features from the first dataset:</p>

<ol>
<li>Test scores increase as spending per student does</li>
<li>There is also variance between districts in student test scores</li>
<li>Part of that difference between districts is because of the underlying relationship between spending and test scores, which also varies with district.</li>
</ol>

<p>More generally stated, how do you handle a scenario where you're building a hierarchical model where the grouping variable is correlated with one of your continuous independent variables (e.g. the first scenario).  I feel like the model I've presented get's at points 1. and 2., but not point 3.  So I'm really seeking a way to tease these two scenarios apart.</p>

<p>Normally I might add an extra level of hierarchy if there was another group level explanatory variable.  Continuing our example, maybe in the 2nd dataset there are differences between district because in some districts parents have more time to spend on homework with students.  So we would add that as a group level predictor in a hierarchical model.  But that wouldn't work in the first scenario. </p>
",2013-10-10 23:42:09.300
57261,16703.0,1,,,,Dealing with 0 values when calculating the mle for a Dirichlet distribution,<maximum-likelihood><dirichlet-distribution>,CC BY-SA 3.0,"<p>I have $N$ pmfs, and for each each $L$ samples. Each sample has a variable amount of $x$ values, but the $x$ values that they have can be matched. So for example:</p>

<p>$$sample_1 \rightarrow\ x_1 = 0, x_2 = 0, x_3 = 0.2, x_4 = 0.4, x_5 = 0.4$$
$$sample_2 \rightarrow\ x_1 = 0.3,x_2=0, x_3 = 0.4, x_4 = 0.3,x_5=0$$</p>

<p>I'm using a <a href=""https://pypi.python.org/pypi/dirichlet/0.7"" rel=""nofollow"">python program</a> to calculate the mle from the samples which is a port of Thomas P. Minka's Matlab Fastfit code (<a href=""http://research.microsoft.com/en-us/um/people/minka/papers/dirichlet/"" rel=""nofollow"">Estimating a Dirichlet Distribution</a>).  </p>

<p>The problem is that, for fitting, it sums over $logp+psi(\sum^k a_k) - logp$. Since some of the $x$ values are 0, some logp values are -inf. Therefore, summing over this makes everything -inf. </p>

<p>How can I deal with 0 values when calculating the mle for a Dirichlet distribution? </p>
",2013-10-10 22:12:17.237
57262,22587.0,1,,,,Poisson distribution vs multiplying probabilities,<probability><poisson-distribution>,CC BY-SA 3.0,"<p>I am a TA for a stats course for engineers, and I had a really good question from a student today, which I don't know the answer to.</p>

<p>We were going through the following word problem:</p>

<p>""4 computers run continuously for the Toronto Stock Exchange. The probability of a computer failure in a day is estimated at 5%. Assuming differing computers fail independently, what is the probability that all 4 computers fail in a day?""</p>

<p>Since the sampling takes place over an interval, the way I would approach this is using the Poisson distribution, with the average number of computers failing on a day $\equiv\lambda = 0.05$. If four computers fail, then $k = 4$. Thus,
\begin{align*}
   P(k; \lambda) &amp;= \frac{\lambda^{k} e^{-\lambda}}{k!} \\
   P(k=4; \lambda = 0.05) &amp;= \frac{0.05^{4} e^{-0.05}}{4!} \\
   &amp; = 2.477\times 10^{-7}
\end{align*}</p>

<p>However, a student asked why it would not be appropriate to just multiply the probability of each computer failing. Since the probability of each computer failing each day $\equiv p = 0.05$, and since each computer failure is independent, he argued that,</p>

<p>\begin{align*}
   P(k=4) &amp;= p^4 \\
          &amp;= 0.05^4 = 6.25\times 10^{-6}
\end{align*}</p>

<p>Which one of these approaches is wrong given the question? And why? What underlying assumption of the wrong approach is violated by the question?</p>

<p>Thank you for your help.</p>

<p>UPDATE: I left out some information in the problem the first time this was posted, and I apologize.</p>
",2013-10-10 22:54:50.603
57263,22585.0,1,,,,How to separate out the regression effect vs treatment effect without a control group?,<regression><psychology><pre-post-comparison>,CC BY-SA 4.0,"<p>I'm looking at a dataset that has pre-post test measurements on users' stress, depression and anxiety levels collected from a website's online health assessment. On average, the healthier participants at baseline got worse over time, and the sicker participants at baseline got much better, and the middle group gets a little better. There's definitely a regression effect going on here, but also a treatment effect too.</p>
<p>As this data was collected based on website usage, there isn't really a control group (all of the &quot;post&quot; measurements come from people that have used the online program). There are probably ways that I could synthesize a control group using the people who I can guess didn't make much use out of the treatment (based on number of logins or length of time between logins), but is there a way to separate out the treatment effect from the regression effect when you can't use difference-in-difference techniques using a control group or anything like that?</p>
",2013-10-10 23:01:51.663
57264,633.0,2,,57262.0,,,,CC BY-SA 3.0,"<p>The Poisson process that you're using assumes that 0.05 is the expected number of computers failing in one day in an unknown number of total computers (your answer also assumes that this rate is fixed after a computer fails, which implies that computers can fail multiple times, or are replaced immediately, or there are so many of them that this is negligible).</p>

<p>The independent probability that the student is using assumes that there are exactly four computers each of which has a 5% chance of failing.</p>

<p>The wording makes it sound to me like 5% is the chance of any individual computer failing (so the second interpretation).  In that case, we want to know the total number of computers and apply a binomial distribution.  Since the question doesn't give the total number of computers, it can't be answered.</p>

<p>Another possibility is that 5% is the probability that exactly one computer fails, and yet another possibility is that 5% is the probability that at least one computer fails.  In either case you can deduce the Poisson process intensity that gives this value.  For the first of these, I get 4.4997552907483822; for the second, I get an intensity of 0.051293294149203306.  From there you could calculate similarly to how you did.</p>

<hr>

<p>Per your update:  You can eliminate the Poisson process since you don't have a fixed rate.  You still have to decide whether 5% is the probability of a given computer failing, in which case the student is right.  If it's the probability of at least one computer failing, or the probability of exactly one computer failing, you'll have to reason back from that number to the probability of any individual computer failing before reasoning forwards.</p>
",2013-10-10 23:04:27.343
57265,2490.0,1,,,,Simple experimental design - should I counterbalance?,<experiment-design><counterbalancing>,CC BY-SA 3.0,"<p>I'm designing a pretty simple experiment that goes like this. Participants will be shown a series of stimuli and after viewing each one they will answer a few questions where they will make judgments about the stimulus - all Likert items. There are two kinds of stimuli. Probably obvious, but the hypothesis is that there will be a difference between answers for A vs B stimuli. There will be 30 or so stimuli, with an equal number of A and B stimuli. All participants will see all the stimuli (within-subjects).</p>

<p>I'm wondering if there would be a benefit to counterbalancing the order in which they receive the items, vs just showing everyone the same randomized sequence of stimuli (which is easier to setup).</p>

<p>If there's a better method I need to consider, I'd be interested in hearing about it. I also looked into blocking designs, but this is so simple that I don't think those apply here. I'm planning to analyze with t-tests or Mann-Whitney-Wilcoxon.</p>
",2013-10-10 23:12:54.873
57266,18040.0,2,,52871.0,,,,CC BY-SA 3.0,"<p>I've run into this problem before.  Often times it's due to a problem with passing negative values to a distribution that doesn't allow them.  So perhaps your log normal or wishart distributions are getting negative values somehow.  </p>

<p>Another issue is that you are giving priors on your variance parameters that is quite big.  This can cause unexpected values to be passed to your log-normald and wishart variables.  I'd start by severely constraining the variance priors and slowly expand the parameter space.  In my experience fitting BUGS models is both and art and a science.</p>
",2013-10-10 23:22:27.973
57282,22601.0,1,57283.0,,,Generic test for unimodality given sample,<hypothesis-testing><distributions><normal-distribution><t-distribution><chi-squared-distribution>,CC BY-SA 3.0,"<p>Are there any generic tests to validate if a given sample follows a unimodal distribution, like a Gaussian, Cauchy, Student's t or a chi-square?</p>
",2013-10-11 08:38:55.517
57269,1741.0,2,,57156.0,,,,CC BY-SA 3.0,"<p>Given that your model exhibits good accuracy you can just use it to predict the class labels of records in the unlabeled dataset. However, you cannot evaluate the performances on unlabeled data. </p>

<p>Be careful that you should assess the quality of your model on the labeled data by cross-validation. It is not enough to check the training error rate. </p>

<p>If your model is not accurate enough you might think about <a href=""http://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow"">semi-supervised learning</a>. The unlabeled data is used in order to improve the quality of your model via inductive learning. The accuracy should always be computed by cross-validation on your labeled data.</p>

<p>Have a look at [ Crimisini et al. Decision Forests: A Unified Framework
for Classification, Regression, Density Estimation, Manifold Learning and
Semi-Supervised Learning ] Chapter 7 about semi-supervised learning and 7.4 about induction with semi-supervised learning.</p>
",2013-10-11 00:17:06.150
57270,22593.0,1,,,,Simulating groups different with respect to the orthogonal complement in R,<distributions><correlation><pca><group-differences>,CC BY-SA 3.0,"<p>In a paper of Journal of Chemometrics (Naes &amp; Mevik 2001 : Understanding the collinearity problem in regression and discriminant analysis), the authors propose to make simulations by creating two groups which are different with respect to the smallest eigenvector direction.</p>

<blockquote>
  <p>Here the groups are different with respect to the orthogonal complement to the ï¬ve â€˜NIR loadingsâ€™. This is achieved in the following way. The constant 0 â‹… 18 is multiplied by a sixth loading vector (orthogonal to the other ï¬ve) and added to group 2. Both groups had initially the same means as group 1</p>
</blockquote>

<p>How can I compute such a simulation in R? The goal is to obtain group differences which are tied to the ""small eigenvectors"" space. Then to check whether the LDA results are best when using the whole dataset or better by using only the last components as variables.</p>

<hr>

<p><strong>Edit</strong><br>
I am trying to get the same results as these authors by simulation, but unfortunately...</p>

<p>I have tried to make two groups in a set of correlated data by adding differences between groups in the last eigen-vector (as it was suggested to me):</p>

<p>Here is a try:</p>

<pre><code>require(MASS)
R=matrix(0.9,10,10);
diag(R)=1;
random.normal=mvrnorm(n=20, rep(1,10), Sigma=R) ;
V &lt;- var(random.normal) ## vcv matrix
U &lt;- eigen(V)$vectors ## eigen vectors
Y &lt;- random.normal %*% U
group=rep(1:2,each=10)
Y10 &lt;- Y[,10] ## temporary data
Y10[group==1] &lt;- Y10[group==1]+1 ## add 1 to group 1
Y10 &lt;- Y10*sqrt(var(Y[,10])/var(Y10)) ## scaling to the variance of Y[,10]
Y[,10] &lt;- Y10-mean(Y10)+mean(Y[,10]) ## center

X2 &lt;- Y %*% t(U) #data back transformed

## Then I try LDA with MASS

test=lda(X2,group,CV=T)
tab=table(test$class,group)
sum(diag(tab))/sum(tab)
</code></pre>

<p>But unfortunately I never found the bad classification results that Naes &amp; Mevik 2001 found. when I use the complete dataset instead of using the last PC in the LDA as these authors found. (it's the same with more variables)
I found that results are even better with the firsts components (e.g. 1:5 for ten PC)</p>

<pre><code>test=lda(prcomp(X2)$x[,1:5],group,CV=T)
    tab=table(test$class,group)
sum(diag(tab))/sum(tab)
</code></pre>

<p>Any suggestions to found the effects these authors explain?
Thank a lot</p>
",2013-10-11 00:34:33.700
57271,20473.0,2,,56768.0,,,,CC BY-SA 3.0,"<p>Write your system explicitly for time $t$ as (""$L$"" for ""loss"", as a positive quantity, and ""$G$"" for ""gain"")
$$ A_t - A_{t-1} = - L^A_{t} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$</p>

<p>$$ B_t - B_{t-1} = - L^B_{t} + G_{t-1}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$</p>

<p>$$ C_t - C_{t-1} = - L^C_{t} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$</p>

<p>The following three relations hold exactly:
$$  L^A_{t} = G_{t}^{A\rightarrow B} +  G_{t}^{A\rightarrow C} $$
$$  L^B_{t} = G_{t}^{B\rightarrow A} +  G_{t}^{B\rightarrow C} $$
$$  L^C_{t} = G_{t}^{C\rightarrow A} +  G_{t}^{C\rightarrow B} $$</p>

<p>If you substitute in the first three you obtain</p>

<p>$$ A_t - A_{t-1} = - G_{t}^{A\rightarrow B} -  G_{t}^{A\rightarrow C} + G_{t}^{B\rightarrow A}+G_{t}^{C\rightarrow A}$$</p>

<p>$$ B_t - B_{t-1} = - G_{t}^{B\rightarrow A} -  G_{t}^{B\rightarrow C} + G_{t}^{A\rightarrow B}+G_{t}^{C\rightarrow B}$$</p>

<p>$$ C_t - C_{t-1} = - G_{t}^{C\rightarrow A} -  G_{t}^{C\rightarrow B} + G_{t}^{A\rightarrow C}+G_{t}^{B\rightarrow C}$$</p>

<p>You have $6$ unknown quantities to estimate <em>per time period</em>. There is just not enough information to do that. So you need assumptions that will impose structure (=restrictions) on the situation, and will permit you to estimate <em>something</em>. What? Let's say you assume that there is a relatively stable ""churn"" from one company to another, as a linear function of their market share in the previous period. This assumption brings in a set of unknown coefficients to be estimated (which will then give you an estimate of ""hidden transfers of market share""). Write $G_{t}^{A\rightarrow B} = a_bA_{t-1}$ (market share lost from $A$ to $B$ as a linear function of $A$'s market share in period $t-1$).
Your equations will become</p>

<p>$$ A_t - A_{t-1} = - a_bA_{t-1} -  a_cA_{t-1} + b_aB_{t-1}+c_aC_{t-1} $$</p>

<p>$$ B_t - B_{t-1} = - b_aB_{t-1} -  b_cB_{t-1} + a_bA_{t-1}+c_bC_{t-1}$$</p>

<p>$$ C_t - C_{t-1} = - c_aC_{t-1} -  c_bC_{t-1} + a_cA_{t-1}+ b_cB_{t-1}$$</p>

<p>We have turned a set of mathematical identities into a <em>model</em>. It is doubtful that this model will hold exactly for each $t$, so you should add a stochastic error term. Rearranging we obtain a first-order Vector Autoregression (VAR): </p>

<p>$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
1-a_b-a_c  &amp; b_a &amp; c_a \\
a_b &amp; 1-b_a-b_c &amp; c_b \\
a_c &amp; b_c &amp; 1-c_a-c_b \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$</p>

<p>or, to homogenize notation,</p>

<p>$$ \left[ \begin{matrix}
A_t  \\
B_t  \\
C_t  \\
\end{matrix} \right] = \left [\begin{matrix}
\gamma_{11}  &amp; \gamma_{12} &amp; \gamma_{13} \\
\gamma_{21} &amp; \gamma_{22} &amp; \gamma_{23} \\
\gamma_{31} &amp; \gamma_{32} &amp; \gamma_{33} \\
\end{matrix} \right]  \left[ \begin{matrix}
A_{t-1}  \\
B_{t-1}  \\
C_{t-1}  \\ 
\end{matrix} \right]+ \left[ \begin{matrix}
u^A_{t}  \\
u^B_{t}  \\
u^C_{t}  \\
\end{matrix} \right] $$</p>

<p>subject to the equality restrictions
$$  \begin{matrix}
\gamma_{11} +  \gamma_{21} + \gamma_{31} =1 \\
\gamma_{12} +  \gamma_{22} + \gamma_{32} =1  \\
\gamma_{13} +  \gamma_{23} + \gamma_{33} =1  \\
\end{matrix} $$</p>

<p>So you have essentially $6$ unknown coefficients and a sample of $T-1$ observations (for each company).<br>
Note that these restrictions <em>imply</em> the ""add up to unity"" restriction $A_t+B_t+C_t =1$ for each $t$, so this last one does not impose any additional structure on the unknown coefficients -but it does imply a relation between the error terms, namely that $u^A_{t} + u^B_{t}  +u^C_{t} =0$. Any additional assumptions on the three error terms should either come from knowledge of the specific real world phenomenon under study, and/or through a statistical specification search.</p>

<p>Then, an estimation for a hidden transfer of market share will be, for example</p>

<p>$$\hat G_{t}^{A\rightarrow B} = \hat \gamma_{21}A_{t-1}$$</p>

<p>etc.</p>

<p>Of course you may find that such a model does not fit your data sample well - for example you expect that all estimated coefficients should be positive and smaller than or equal to unity, but the estimation procedure may not give you that. But this is what we do: we come up with specification hypotheses and we test them against the data - ""success"" is never guaranteed. Then you should try to come up with a different model.</p>
",2013-10-11 01:09:15.643
57272,22595.0,2,,412.0,,,,CC BY-SA 3.0,"<p>The following are text books I used for my MSEE coursework and research and I found them to be pretty good. </p>

<ol>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0132311232"" rel=""nofollow"">Probability, Statistics and Random Processes for Engineers</a> by Henry Stark and John W. Woods
(Detailed explanation of concepts, good for Communications and Signal Processing people).</li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0071632891"" rel=""nofollow"">Schaum's Outline of Probability, Random Variables and Random Processes</a> by Hwei Hsu
(Concise explanation of concepts, has a good amount of solved examples).</li>
</ol>
",2013-10-11 01:37:35.967
57273,22596.0,1,,,,Difficulty with MCMC implementation,<modeling><python><markov-chain-montecarlo><fitting>,CC BY-SA 3.0,"<p>I could really use some guided help! I'm having difficulty understanding an MCMC implementation in terms of modeling a data set. I'm working on generating parameters from stellar light curves, and was asked to look into implementing an MCMC algorithm. A large chuck on the code is written in Python, so I've been trying to use <a href=""http://dan.iel.fm/emcee/"" rel=""nofollow"">emcee hammer</a> to generate parameter fits. But going through the code, it's just not ""clicking"" how the method works.</p>

<p>I have a set of data (time vs flux) of two stars orbiting each other such that from our point of view, they eclipse. There are dips in the light curve to signify this. All I'm attempting to do is get the parameters of the system dependent on the characteristics of these dips.</p>

<p>In the emcee implementation, there are a few functions that I understand: the posterior function which, I believe, simply generates a data set given the set of parameters. Then there's a prior function which, I assume, is the function given a previous set of parameters. Somehow the algorithm chooses whether or not the jump to the posterior parameter set is to be done? I'm guessing that's what the use of the likelihood function is? To describe whether or not to take the jump?</p>

<p>I apologize, I'm quite confused on how this is to be implemented in terms a defined set of data.</p>
",2013-10-11 02:05:57.810
57274,22594.0,1,,,,Is mixed measures ANOVA the correct test for my data?,<r><anova><mixed-model>,CC BY-SA 3.0,"<p>I'm fairly new to statistics and I'm still trying to figure out the best way to analyse the data I have. The experiment has 2 groups of participants who perform 2 repetitions of a task that consists of 5 stages. All participants completed both repetitions for all stages, but one group had 8 participants while the other group only had 6. I have a about 100 dependent variables that I wish to examine, so my data looks a bit like this:</p>

<pre><code>ID   Group    Repetition    Stage   DV1    DV2     ...
1    A        1             1       212.9  179.9   ...
1    A        2             1       144.8  134.7   ...
2    B        1             1       146.3  156.8   ...
2    B        2             1       128.6  178.2   ...
</code></pre>

<p>Group is a between-subjects factor while Repetition and Stage are within-subjects factors. I would like to determine whether Group and Repetition have a significant effect on each dependent variable within each stage (I am not interested in the effect of stage itself). I'm doing the analysis in R so I have the following code:</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mydata            = read.csv(""data.csv"",header=TRUE)
mydata$Group      = factor(mydata$Group)
mydata$Repetition = factor(mydata$Repetition)
mydata$Stage      = factor(mydata$Stage)
# for each stage
mydata = mydata[mydata$Stage==1,]
for (i in 5:(ncol(mydata))) 
{
   fit = aov(formula=as.formula(paste(names(mydata)[i], 
                                ""~ Group * Repetition + Error(ID/Repetition)"")), 
             data=mydata)
}
</code></pre>

<p>My questions are:</p>

<ol>
<li>Is mixed measures ANOVA a valid test for this data? What's the correct way to test whether my data fits the assumptions of ANOVA in R? If this is not a reliable test, what's a possible alternative?</li>
<li>Have I defined the mixed measures ANOVA in R correctly? The various tutorials I've read define it in different ways so I'm a bit confused.</li>
</ol>
",2013-10-11 02:25:06.173
57275,22598.0,1,,,,Training one class SVM using LibSVM,<svm><data-mining><outliers><libsvm>,CC BY-SA 3.0,"<p>I hope to use one-class SVM of LIBSVM to train a training samples so as to get a model. Then, I will use the model to predict whether the new test data and the training data is same type or not. In the training process, I have some questions as follows:</p>

<ul>
<li>Should the training samples all be positive examples or not?</li>
<li>Which kernel function can get better result, <strong>linear</strong> kernel or <strong>RBF</strong> kernel?</li>
<li>What is the effect of nu's values to the model?</li>
</ul>
",2013-10-11 02:49:49.953
57284,,1,,,user14650,How to set confidence level for wilcoxsign_test (package coin)?,<r><confidence-interval><wilcoxon-signed-rank>,CC BY-SA 3.0,"<p>In R, the function <code>wilcox.test</code> takes the argument <code>conf.level = 0.095</code> (for example). Giving the same argument to the function <code>wilcoxsign_test</code> from the <em>coin</em> package returns a warning:</p>

<pre><code>additional arguments conf.level will be ignored
</code></pre>

<p><strong>What default confidence level does <code>wilcoxsign_test</code> use, and how can I change it?</strong></p>

<p><em>Or :</em> Why do I not need a confidence level for this function?</p>
",2013-10-11 09:06:48.623
57276,22310.0,1,,,,"Path analysis, sample sizes, and alternative analysis",<r><structural-equation-modeling><small-sample><path-model>,CC BY-SA 3.0,"<p>I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence.</p>
<p>My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But, there are multiple factors, other than English ivy, that affect soil moisture.</p>
<p><img src=""https://i.stack.imgur.com/k65Ag.jpg"" alt=""enter image description here"" /></p>
<p>My questions are:</p>
<ol>
<li><p>I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?</p>
</li>
<li><p>Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?</p>
<p><img src=""https://i.stack.imgur.com/ArgZm.jpg"" alt=""The relationship between soil moisture and English ivy cover on cover objects (&quot;the number of overstory trees&quot; for the left graph) for different levels of the surrounding overstory trees (&quot;English ivy cover on cover objects&quot; for the left graph"" /></p>
</li>
<li><p>Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small <strong>AND</strong> regressions do not reflect my hypothesized causal relationships accurately.</p>
</li>
</ol>
<p>I am using R, so any recommended code would be greatly helpful (I am a relatively new R user, though).</p>
",2013-10-11 02:51:27.387
57277,20603.0,2,,57275.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>Should the training samples all be positive examples or not?</p>
</blockquote>

<p>Yes, in one class SVM (and any other outlier detection algorithm) you need just <strong>one</strong> class. If it is <strong>positive</strong> or <strong>negative</strong> depends on your naming convention, but it it more probable, that you will seek for <strong>positive</strong> examples which are underrepresented.</p>

<blockquote>
  <p>Which kernel function can get better result, linear kernel or RBF kernel?</p>
</blockquote>

<p>""There is no free lunch"". There is no general answer, the reason behind having many kernels (not just linear and rbf) is that they work well in different applications. It is <strong>data dependant</strong> decision, so you will have to test at least those two.</p>

<blockquote>
  <p>What is the effect of nu's values to the model?</p>
</blockquote>

<p>It corresponds to the bounds on fraction of points becoming support vectors, so it limits the model's complexity (smaller the number of SVs, simplier the model and less prone to overfitting, yet prone to underfitting). As in the <a href=""http://www.cms.livjm.ac.uk/library/archive/Grid%20Computing/NoveltyDetection/sch00support.pdf"" rel=""noreferrer"">http://www.cms.livjm.ac.uk/library/archive/Grid%20Computing/NoveltyDetection/sch00support.pdf</a> paper, it directly corresponds to:</p>

<ul>
<li>""an upper bound on the fraction of outliers""</li>
<li>""a lower bound on the fraction of SVs"".</li>
</ul>
",2013-10-11 05:28:45.147
57278,20144.0,1,,,,"Exponential family parameter estimation and fitting, references",<references><nonparametric><exponential-family>,CC BY-SA 3.0,"<p>First of all, I want to express my apologies if the question is too broad or wrong, but I am in need of references and I have no idea whom I can ask.</p>

<p>If you are interested, the question comes from a model I built, you can see some details <a href=""https://physics.stackexchange.com/questions/78524/boltzmann-distribution-with-interaction-between-particles"">here</a> and <a href=""https://physics.stackexchange.com/questions/80019/grand-canonical-ensemble-with-interaction-simulation-doubts"">here</a>. In this model I have:
$$f(\mathbb{x}|T,\mu)=\frac{h(\mathbb{x})e^{-\frac{E(\mathbb{x})}{kT}+\mu N(x)}}{\mathcal{Z}(T,\mu)}$$</p>

<p>There, my parameters are $\mu$ and $T$, and $\mathbb{x}=(x_1,\dots,x_M)$ where $x_i\in\{0,1\}$ and I have the restriction $\forall i\in\{1,\dots,M-D+1\}$
$$\sum_{j=0}^{D-1} x_{i+j} \leq 1$$
This is, $h(\mathbb{x})=0$ if that condition is not held.</p>

<p>I have the ""small"" inconvenience of not knowing $\mathcal{Z}(T,\mu)$, so I used a MCMC (Metropolis-Hastings) method to approximate this function. However I face two problems.</p>

<ul>
<li><p>The first of them regards the simulation and the model and I am on solving it (it depends too much on the initial condition). </p></li>
<li><p>The second is that these parameters are not fully known and I have no idea how can I estimate them. I have been reading about Bayesian inference and I know a bit of estimation theory but I am no expert (furthermore I don't know if not knowing the partition function can affect the result). If any of you were able to give me some clue in the form of a book that I can read, I would be eternally grateful.</p></li>
</ul>

<p>Thank you very much for your help.</p>

<p>Thanks to cardinal's comment, I have realized that I didn't explain one thing. It probably makes all more complex but there it goes:
The idea is that $E$ is known in each experiment, actually $E(\mathbf{x}) = \mathbf{E}\cdot\mathbf{x}$. However, $\mathbf{E}$ is not always the same, it represents an external potential for some particles. The ""good"" thing is that $T$, which accounts for the temperature, never changes whatever $\mathbf{E}$ is, so I thought that I could find a way of estimating it, given the fact that I have an empirical distribution of $x_{i}$ (so, a probability that a particle is in the position $i$) given a certain $\mathbf{E}$. So, in a way, what I have is
$$f(\mathbf{x}|T,\mu , \mathbf{E})$$, but I always know $\mathbf{E}$ and I know (can I say this?) that $T,\mu$ are independent of $\mathbf{E}$. I am sorry for not being clear enough before. I am starting to think that nothing of this makes sense...</p>
",2013-10-11 06:43:41.090
57279,22262.0,1,,,,Variable selection with groups of predictors that are highly correlated,<machine-learning><forecasting><feature-selection><lasso><multicollinearity>,CC BY-SA 3.0,"<p>What variable selection approach should I consider if I have thousands of predictors with clusters that are extremely correlated? </p>

<p>For example I might have a predictor set $X:= \{A_1,A_2,A_3,A_4,...,A_{39},B_1,B_2,...,B_{44},C_1,C_2,...\}$ with cardinality $|X| &gt; 2000$. Consider the case where all $\rho(A_i,A_j)$ are very high, and similarly for $B$, $C$, .... </p>

<p>Correlated predictors aren't correlated ""naturally""; it's a result of the feature engineering process. This is because all $A_i$ are hand engineered from the same underlying data with small variations in hand-engineering methodology, e.g. I use a thinner pass band on $A_2$ than I did for $A_1$ in my denoising approach but everything else is the same.</p>

<p>My goal is to improve out of sample accuracy in my classification model.  </p>

<p>One approach would just be to try everything:  non-negative garotte, ridge, lasso, elastic nets, random subspace learning, PCA/manifold learning, least angle regression and pick the one that's best in my out of sample dataset. But specific methods that are good at dealing with the above would be appreciated.</p>

<p>Note that my out of sample data is extensive in terms of sample size. </p>
",2013-10-11 06:57:56.613
57280,22600.0,1,57281.0,,,Binomial Conditional Probability of a an event,<probability><binomial-distribution><conditional-probability>,CC BY-SA 3.0,"<h2>Determining Binomial Condition Probability of a Random Sample</h2>

<p>I have a question about binomial probability involving a conditional event. This problem keeps tripping me up, because while I know how to calculate the binomial probability that a random variable is a failure, i don't know how to calculate the conditional probability of that variable.</p>

<hr>

<p>My question is as follows:</p>

<p><strong>70%</strong> of the total shipments come from <strong>factory A</strong>, of which <strong>10% are defective</strong>.</p>

<p><strong>30%</strong> of the total shipments come from <strong>factory B</strong>, of which <strong>5% are defective</strong>.</p>

<p>A random shipment comes in, and a sample of 20 pints is taken, and 1 of the pints is defective.</p>

<p><em>What is the probability that this shipment came from the Factory A?</em> </p>
",2013-10-11 07:24:30.003
57281,9074.0,2,,57280.0,,,,CC BY-SA 3.0,"<p><strong>Edit 1</strong>: didn't pay attention to the question. Will edit answer later today. </p>

<p><strong>Edit 2</strong>: I've attempted to provide an answer below, however I might be mistaken. Feel free to correct me if I am in error. </p>

<p>$P(1 defective|A) \approx 0.270 \wedge P(1 defective|B) \approx 0.377 \\
P(A) = 0.7 \wedge P(B) = 0.3 \\
P(1D) = 0.7*0.270+0.3*0.377 = 0.189+0.113 = 0.302 \\
P(A|1D) = P(1D|A)*P(A)/P(1D)=0.270*0.7/0.302 \approx 0.626$</p>
",2013-10-11 07:39:11.260
57285,221.0,2,,56372.0,,,,CC BY-SA 3.0,"<p>The topic is called <a href=""http://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">Association Rule Learning</a>, which is one of the most basic (and rather old-fashioned) ways to build a recommender system. The most widely known algorithms are called A Priori and FP Growth. Every good book about Data Mining should contain a chapter about it.</p>

<p>However, the formula seems to be wrong.</p>

<p>$P(A|B)$ means Probability of A given B, so</p>

<pre><code>P(A|B)=count_users(bought(A,B)) / count_users(bought_B)
</code></pre>

<p>is correct.</p>

<p>Furthermore, the mentioned algorithms do not take into account something like $P(\neg A|B)$, because the fact that a user has not bought A could have multiple meanings</p>

<ul>
<li>user does not like A</li>
<li>user does not know that A exists or is sold here</li>
<li>user does not bought A although he likes it for one of the thousand seemingly arbitrary motivatiors of human behavior.</li>
</ul>

<p>because not buying something is an implicit preference. If the user would have stated explicitly that he does not like A (may be in a survey), it is called an explicit preference. In case of implicit negative preferences, the negative preferences are often excluded from the model.</p>

<p>If explicit preferences are given, the overall formula $\frac{P(A|B)}{P(\neg A|B)}$ would make sense and represent the <a href=""http://en.wikipedia.org/wiki/Odds_ratio"" rel=""nofollow"">Odds Ratio</a>.</p>
",2013-10-11 09:40:49.687
57286,503.0,2,,57284.0,,,,CC BY-SA 3.0,"<p>You don't need (and in fact can't) set a confidence limit in <code>wilcoxsign_test</code> because the about of the function includes a p value. e.g.the first example in the help file for the function:</p>

<pre><code>RoundingTimes &lt;- data.frame(
  times = c(5.40, 5.50, 5.55,
            5.85, 5.70, 5.75,
            5.20, 5.60, 5.50,
            5.55, 5.50, 5.40,
            5.90, 5.85, 5.70,
            5.45, 5.55, 5.60,
            5.40, 5.40, 5.35,
            5.45, 5.50, 5.35,
            5.25, 5.15, 5.00,
            5.85, 5.80, 5.70,
            5.25, 5.20, 5.10,
            5.65, 5.55, 5.45,
            5.60, 5.35, 5.45,
            5.05, 5.00, 4.95,
            5.50, 5.50, 5.40,
            5.45, 5.55, 5.50,
            5.55, 5.55, 5.35,
            5.45, 5.50, 5.55,
            5.50, 5.45, 5.25,
            5.65, 5.60, 5.40,
            5.70, 5.65, 5.55,
            6.30, 6.30, 6.25),
  methods = factor(rep(c(""Round Out"", ""Narrow Angle"", ""Wide Angle""), 22)),
  block = factor(rep(1:22, rep(3, 22))))

### classical global test
friedman_test(times ~ methods | block, data = RoundingTimes)
</code></pre>

<p>gives as output </p>

<pre><code>Asymptotic Friedman Test

data:  times by
     methods (Narrow Angle, Round Out, Wide Angle) 
     stratified by block
chi-squared = 11.1429, df = 2, p-value =  0.003805
</code></pre>

<p>so, since p = 0.0038, you know it is significant at p = 0.05 (and, indeed, much below that). </p>
",2013-10-11 10:26:57.610
57287,21624.0,1,,,,How to decide bootstrap number of runs?,<bootstrap><convergence>,CC BY-SA 3.0,"<p>I am using bootstrap for my simulation.</p>

<p>The number of the population is flexible for each case, and the sample size is decided by a certain percentage. For example, I have a 10,000 population, and I decide to use 10% for each iteration of bootstrap, so the sample size is 1,000.</p>

<p>In practice, I found it is hard to decide how many times to run the bootstrap is enough. With less simulation, the results appear insufficiant, while with a large number of simulation they are purely redundant.</p>

<p>May I know if there is a method that can help me to decide the number of iterations to run?</p>
",2013-10-11 10:37:53.160
57288,19436.0,1,,,,Reversing Chebyshev inequality argument,<philosophical><statistical-learning>,CC BY-SA 3.0,"<p>One way one could state Chebyshev's inequality is </p>

<blockquote>
  <p>The probability that a realization deviates from the mean more
  than $k$ standard deviations is at most $\frac{1}{k^2}$.</p>
</blockquote>

<p>My question is: Can one rigorously reverse this logic and make a statement about the probability that the actual mean is close to the observation. One immediate technical problem is that one needs to define a probability space on possible probability distributions/means.</p>

<p>I'm asking because I think this type of argument (although slightly more convoluted) underlies Vapnik-Chervonenkis theory.  In their textbooks this issue is not discussed at all. They prove a large deviation principle and then simply invert all their inequalities. How does this work?  Does it?</p>
",2013-10-11 10:55:26.637
57289,20410.0,5,,,,,,CC BY-SA 3.0,"<p>Canonical correlation analysis (CCA) is a multivariate statistical technique that analyzes two sets of variables and looks for correlations between them. CCA finds linear combinations of variables in each set such that their correlation is maximal. These two linear combinations form one pair of ""canonical variates"". CCA then proceeds to find subsequent pairs, constrained to be uncorrelated with the previous ones.</p>
",2013-10-11 11:26:16.130
57290,2081.0,4,,,,,,CC BY-SA 3.0,Canonical correlation analysis (CCA) is a method to analyze correlations between two sets of variables. It finds linear combinations of variables in each set such that their correlation is maximal.,2013-10-11 11:26:16.130
57291,22262.0,1,57294.0,,,Function to find the quantile in a vector corresponding to constant $x$,<r><quantiles><function>,CC BY-SA 3.0,"<p>Suppose I have constant <code>x=0.1</code> in the language <code>R</code> and I have a vector <code>vec = rnorm(200)</code>. Is there a pre-packaged function to find the quantile of <code>vec</code> that corresponds the closest to <code>x</code>? </p>

<p>A solution is as follows:</p>

<pre><code>x = 0.1
vec = rnorm(100)
percentiles = quantile(vec,seq(0,1,by=0.01))
which(abs(x-percentiles)==min(abs(x-percentiles))) 
#returns closest match
</code></pre>

<p>... but I would like to know if there's a pre-packaged function.</p>
",2013-10-11 11:39:19.043
57292,19125.0,2,,48597.0,,,,CC BY-SA 3.0,"<p>Your supervisor may very well be right and the small sample size is the problem. You might want to do a bit of reading on <a href=""http://psych.wisc.edu/henriques/power.html"" rel=""nofollow noreferrer""><em>Power Analysis</em></a>. An introductory paper is that by Cohen (1992). </p>

<p>In short, there is a relation between sample size, effect size and power (which is the probability that the test detects a significant effect assuming that there is one). For example, if you have an estimate of the effect size you're looking for (in your example the difference between the means of the two groups) and you want to obtain a statistically significant result regardings this effect with a certain error probability (the $\alpha$-Level), then you can compute the size of the sample that is neccessary. Generally, when you have two of the numbers, you can compute the third one. </p>

<p>The difficult part is probably to get an idea of the effect size before doing the analysis. After all, ususally that is what one wants to find out about. An interesting discussion on this can be found on the <a href=""https://cogsci.stackexchange.com/questions/3384/how-to-estimate-an-expected-effect-size-for-a-planned-psychology-study?lq=1"">Cognitive Sciences SE site</a>.</p>

<p>One piece of free software to do power analysis is <a href=""http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/download-and-register"" rel=""nofollow noreferrer"">G Power</a>. There is also the <code>pwr</code>-package for R.</p>

<p>References:</p>

<p><sub>
Cohen, J. (1992). <a href=""http://classes.deonandan.com/hss4303/2010/cohen%201992%20sample%20size.pdf"" rel=""nofollow noreferrer"">A power primer</a>. <em>Psychological Bulletin</em>, 112(1), 155.
</sub> </p>
",2013-10-11 11:48:49.933
57293,18198.0,1,,,,Selecting optimal set of eigenvectors for Principal Components Regression,<pca><ridge-regression>,CC BY-SA 3.0,"<p>I am testing various techniques for dealing with strong multi-collinearity (MC) in a regression problem.</p>

<p>There have been various comparison papers written between competing techniques such as Ridge Regression (RR) and Principal Components Regression (PCR). There seems to be no clear winner though with the best technique seemingly problem specific. However one thing that bothers me about the PCR approach is the somewhat arbitrary way in which one simply excludes the smallest eigenvectors as has been proven in Hadi and Ling even the smallest eigenvector may have strong predictive power while the largest eigenvectors may have none.</p>

<blockquote>
  <p>""<a href=""http://www.jstor.org/discover/10.2307/2685559?uid=3738016&amp;uid=2&amp;uid=4&amp;sid=21102750270477"" rel=""noreferrer"">Some Cautionary notes on the use of Principal Components
  Regression</a>"" by Hadi and Ling. (<a href=""http://www.uvm.edu/~rsingle/stat380/F04/possible/Hadi+Ling-AmStat-1998_PCRegression.pdf"" rel=""noreferrer"">PDF</a>)</p>
</blockquote>

<p>They also show that the the SSE can be vastly improved by adding seemingly insignificant eigenvectors. </p>

<p>In their discussion they highlght two papers that try to address this 2nd deficiency--Lott(1973) and Gunst and Mason(1973)--but it has been shown that the Lott technique fails to pick the ""correct"" eigenvectors in the presence of strong MC, and my problem has strong MC. </p>

<p>Do you know of a paper that can select the optimum set of eigenvalues even in the presence of strong MC?  Or more recent papers that compare PCR and RR?</p>
",2013-10-11 11:56:09.380
57294,11489.0,2,,57291.0,,,,CC BY-SA 3.0,"<p>Yep, if had you bothered to read the manual of <code>quantile</code>, you would have found the function <code>ecdf</code> in the ""See Also"" section.</p>

<pre><code>x &lt;- 0.1
vec &lt;- rnorm( 100 )
ecdf( vec )( x )
# or
my.ecdf &lt;- ecdf( vec )
my.ecdf( x )
</code></pre>

<p><code>ecdf</code> is a function returning another function -- that in turn is the experimental distribution function of your distribution.</p>
",2013-10-11 12:03:59.843
57295,21762.0,2,,35249.0,,,,CC BY-SA 3.0,"<p>A data driven (and thus probably not so very good) approach</p>

<p>Calculate four correlation matrices: One for each layer and one for the pooled data (three lines per sample). If they all look quite similar, run a PCA based on the correlation matrix of the pooled sample and go on with the first few PCs.</p>

<p>Instead of comparing the four correlation matrices, you could also consider the four loading matrices of the corresponding PCAs and compare the loadings of the first few PCs. This is much easier if you have lots of variables.</p>
",2013-10-11 12:05:40.297
57296,21398.0,1,,,,Selection probability weight,<survey><weighted-sampling>,CC BY-SA 3.0,"<p>I have a question on my selection probability weight. Is it a correct weight?</p>

<p><em><strong>The research design:</em></strong> research areas were divided into strata according to size. Interviews were taken: 50 batches of 10 interviews in each area according to the relative size of strata. 
Clusters were made for each stratum. In each cluster: batches of 10 interviews were sampled in fixed intervals. A random walk selected households and within these households, respondents were randomly chosen.</p>

<p><em><strong>The selection probability weight:</em></strong> I had no population data on number of households. A selection probability weight was calculated for the within-household selection for each stratum. In each stratum, a weight was calculated and normalized so that the sum of the weights is 500 for each research area. The size of the eventual stratum was divided by the number of people in the stratum eligible for the survey. The result of this calculation was then multiplied by the number of eligible respondents in the household.</p>
",2013-10-11 12:11:24.820
57297,18198.0,1,,,,Testing whether two Eigen decompositions are equal,<pca>,CC BY-SA 3.0,"<p>I have an eigen decomposition of a 30 variable covariance matrix calculated using 5y of daily data and would like to compare it to a different 5y period to see if the eigenvalues are the same. Obviously they will not be exactly the same due to noise in the signal but can I test statistically that they are the the same?</p>

<p>""An asymptotic chi-square test for the equality of two correlation matrices"" by R. Jennrich</p>

<p>The closest match I have found is a paper to test the equivalence on two correlation matrices, but as I am working in the Eigenvector space I would prefer a test that is performed on the eigenvectors (plus the paper is quite old).</p>

<p>Also on a similar topic what is the minimum length of time I can run a PCA analysis over for 30 variables on daily data. Clearly if i can generate more eigenvector decompositions to compare I can be more confident in my results.</p>
",2013-10-11 12:13:44.087
57298,,1,57301.0,,user30602,Different answers for probability density function and cumulative density function,<density-function><cumulative-distribution-function>,CC BY-SA 3.0,"<p>I have a function $f(x)=2ae^{-ax}(1-e^{-ax})$, for $x&gt;0, a&gt;0$. This is a pdf. I need to find $P(X&gt;1)$. I have done all my work in such a way that I should get the same answer whether I use the pdf or the cdf to find this probability. However, I'm getting different answers. Can someone please help me?</p>

<p><strong>My attempt:</strong> </p>

<p>(using pdf) $P(X&gt;1)=\int_1^{\infty}2ae^{-ax}(1-e^{-ax})dx = 2e^{-a}-e^{-2a}$ </p>

<p>(using cdf) $P(X&gt;1)= 1-P(X\leq 1) = 1 - (F_X(1)) = 1-(e^{-ax}(e^{-ax}-2))|_{x=1}=1-2e^{-a}-e^{-2a}$</p>

<p>Why are my answers different? Thanks!</p>
",2013-10-11 12:36:44.077
57299,19395.0,1,57300.0,,,Before and after data: Which test for average comparison of Likert scale data?,<normal-distribution><t-test><likert>,CC BY-SA 3.0,"<p>I have one group of respondents which answer on a scale of 1-5 once before and once after an experiment. I want to see if the experiment made a difference to their responses.</p>

<p>I was told not to use a t-test because of the Likert scale (ordinal data does not seem to fit a t-test) and because my data are not nearly normally distributed (answers to the questions lean heavily to the 1 of the scale (which is not a mistake in the design)).</p>

<p>I am not sure if the Wilcoxon signed-rank test works, because it seems to be designed for differences in groups (as in ""Do men respond differently from women?"").</p>

<p>Any suggestions on what could actually be used here?</p>

<p>(The answer <a href=""https://stats.stackexchange.com/questions/20245/using-t-test-for-comparing-likert-responses-before-and-after-intervention"">here</a> refers to a ""special paired t-test"", but does not explain which one)</p>
",2013-10-11 13:10:18.780
57300,21762.0,2,,57299.0,,,,CC BY-SA 3.0,"<p>Wilcoxon's signed-rank test is usually a good choice in such a situation. It is the paired version of Wilcoxon's rank-sum test (aka Mann-Whitney-U-test). I think you are mixing these two procedures.</p>

<p>Make sure you use an exact/pseudo-exact implementation of the test to account for the highly discrete distribution.</p>

<p>EDIT: How you do it in R for x (pre) and y (post)</p>

<pre><code>library(coin)
set.seed(2)
x &lt;- sample(1:2, 20, T)
y &lt;- sample(2:3, 20, T)

#Basic R gives p value of 0.0007167
wilcox.test(x-y)                    

#Coin gives p value of 0.0001221
wilcoxsign_test(x~y, distribution = exact())
</code></pre>
",2013-10-11 13:17:25.363
57301,22059.0,2,,57298.0,,,,CC BY-SA 3.0,"<p>It looks like a simple calculation error:</p>

<p>\begin{align}
F_X(y)&amp;=\int_{x=0}^y 2ae^{-ax}(1-e^{-ax})\\
&amp;=e^{-ax}(e^{-ax}-2)|_{x=0}^y\\
&amp;=e^{-2ay}-2e^{-ay}+1.
\end{align}</p>
",2013-10-11 13:22:52.120
57302,750.0,2,,57268.0,,,,CC BY-SA 3.0,"<p>You don't need extra terms in the models, less actually. It is plain to see by your plots, but if you look at <code>summary(m2)</code> you will see that the variance for random effect for <code>x</code> is really small, and the variance for the intercept is quite small as well.</p>

<p>Similarly for the <code>m1</code> model, you can see from the plot that the slopes are all the same, but the intercept varies. You can use an F-test to check the model with <em>only</em> random intercepts versus the model with random slopes and intercepts you specified.</p>

<pre><code>m1 &lt;- lmer(y~x + (x|g),data=dat1)
m1RInt &lt;- lmer(y~x + (1|g),data=dat1)
anova(m1,m1RInt)
</code></pre>

<p>Also just looking at the variance estimates of the random intercepts and effects for <code>summary(m1)</code> you would have come to the same conclusion that using random slopes adds nothing to the model.</p>
",2013-10-11 13:41:25.093
57303,11506.0,1,,,,Quantitative results of cluster analysis,<machine-learning><clustering>,CC BY-SA 3.0,"<p>Currently, I am doing a clustering for two sets of data. One smaller dataset (about 100 data) got ground truth labels, and one larger dataset (about 2000 data) has no ground truth labels.</p>

<p>For the smaller dataset, obviously, I can obtain quantitative results like accuracy, sensitivity and specificity.</p>

<p>However, for the larger dataset, I have no ground truth and couldn't get any useful quantitative results.</p>

<ol>
<li><p>The only thing I found useful is the 'mean silhouette value', which can measure the cluster performance. However, it based on some distance measure that can only tell people how separate are the clusters. I am wondering if there are other 'better' or 'more appropriate' quantitative analysis for data without labels.</p></li>
<li><p>Because the data are without labels, I am also wondering if we can somehow have a 'uncertainty' measure about the clustering results like how confident about the cluster results?</p></li>
<li><p>For the smaller dataset with labels, except accuracy, sensitivity and specificity, any other quantitative results I can get? For the classification algorithm, we can do a cross-validation, is there any method we can use to do such a cross-validation for clustering? Also, can we get ROC analysis for clustering task?</p></li>
</ol>
",2013-10-11 13:50:46.157
57304,9522.0,1,57306.0,,,Which statistical test should be used to test for enrichment of gene lists?,<biostatistics>,CC BY-SA 3.0,"<p>I have performed an experiment to test the cellular sensitivity to a certain DNA damage agent. We have found 270 genes that were specifically sensitive to the drug and the total number of genes analyzed was 3668. 38 out of the 270 sensitive genes are classified as ""DNA repair genes"". If the number of ""DNA repair genes"" contained in the genome is 112 and the total number of genes in the genome is 3668, are the sensitive genes enrichment in DNA repair genes?
Which statistical test should be used? I would appreciate if you could also tell me some tool to calculate the p-value online. </p>
",2013-10-11 14:05:31.013
57305,22607.0,1,,,,Interpretation of a PDF squared,<probability><distributions><density-function><moments>,CC BY-SA 3.0,"<p>I have a problem where the crucial variable is the integral of the squared PDF of a random variable, i.e.</p>

<p>$\int f(x)^2dx$</p>

<p>How should I interpret this property of a distribution?  If $f(x)$ is gaussian, then this is inversely proportional to the variance, $\sigma^2$, but I don't think this is generally true.</p>

<p>(Note that this is also equal to $\int F(x)f'(x)dx$ ).</p>
",2013-10-11 14:20:05.910
57306,21638.0,2,,57304.0,,,,CC BY-SA 3.0,"<p>Standard practice to test for enrichment of gene lists is to do a hypergeometric test or, equivalently, a one-sided <a href=""http://en.wikipedia.org/wiki/Fisher%27s_exact_test"">Fisher's exact test</a>. You have the following $2\times2$ contingency table:</p>

<p>$$
\array{&amp; \text{DNA Repair} &amp; \text{Other} \\\text{Sensitive} &amp; 38 &amp; 232 &amp; 270\\\text{Not Sensitive} &amp; 74 &amp; 3324 &amp; 3398 \\ &amp; 112 &amp; 3556}
$$</p>

<p>You can carry out the test in <code>R</code> as follows:</p>

<pre><code>fisher.test(matrix(c(38,74,232,3324),nrow=2,ncol=2),alternative=""greater"")
</code></pre>

<p>Which gives a highly significant result:</p>

<pre><code>Fisher's Exact Test for Count Data

data:  matrix(c(38, 74, 232, 3324), nrow = 2, ncol = 2) 
p-value &lt; 2.2e-16
alternative hypothesis: true odds ratio is greater than 1 
95 percent confidence interval:
5.062107      Inf 
sample estimates:
odds ratio 
7.34918
</code></pre>

<p>Note that as we are testing for over-representation (rather than under-representation) the <code>alternative</code> parameter is set to <code>""greater""</code>.</p>
",2013-10-11 14:48:42.243
57307,9522.0,1,57310.0,,,Any online software to calculate pvalue of Fisher exact test?,<software><fishers-exact-test>,CC BY-SA 3.0,"<p>I would like to do a one-sided FisherÂ´s exact test for an analysis. I have not any statistic software to obtain the pvalues (no SAS, no SPSS). The 2x2 tables are of this type:</p>

<p>Do you know any online statistical software to calculate the pvalues? I have tried with some of them but the results indicate pvalue&lt;0.0001 but I need to know the exact number.
The 2x2 tables are of this type:</p>

<p><img src=""https://i.stack.imgur.com/BUTLq.png"" alt=""enter image description here""></p>

<p>Thanks a lot in advanced!</p>
",2013-10-11 15:15:59.620
57308,22507.0,2,,57279.0,,,,CC BY-SA 3.0,"<p>I would do the forward stepwise selection, adding predictors as long as the correlation with residuals is significant, and then do some regularization (ridge, lasso, elastic nets).  There are 2-3 metaparameters: forward stepwise termination constraint, and 1 or 2 regularization parameters. These metaparameters are determined via cross-validation.</p>

<p>If you want to take into account non-linearity, you could try random forest, which produces good results when there are many predictors.  But it is slow.</p>
",2013-10-11 15:33:10.407
57309,22611.0,1,,,,Covary two dummy variables in SEM?,<categorical-data><structural-equation-modeling>,CC BY-SA 3.0,"<p>I am running a structural equation model (SEM) in Amos 18, and I want to test the impact of marital status on several latent variables. Marital status is nominal, so I created three dummy variables:</p>

<ol>
<li>Mar_Single: 1 = yes, 0 = no</li>
<li>Mar_Married: 1 = yes, 0 = no</li>
<li>Mar_Other: 1 = yes, 0 = no</li>
</ol>

<p>I included Mar_Single and Mar_Married in the SEM, so their coefficients will be interpreted against the omitted (reference) group, Mar_Other. The modification indices suggested fit could be improved significantly if I covary Mar_Single and Mar_Married. Should I do this? In a way, this makes sense because they are perfectly correlated: If Mar_Single = 1 then Mar_Married will always = 0. </p>

<p>Larry</p>
",2013-10-11 16:00:23.980
57310,20972.0,2,,57307.0,,,,CC BY-SA 3.0,"<p>Microsoft research have an online tool <a href=""http://research.microsoft.com/en-us/um/redmond/projects/mscompbio/fisherexacttest"" rel=""nofollow"">here</a>. You can also download an Excel add-in from <a href=""http://www.real-statistics.com/free-download/"" rel=""nofollow"">here</a>.</p>

<p>Your result according to the Microsoft tool is 6.511E-017.</p>
",2013-10-11 16:12:37.340
57415,22507.0,2,,55260.0,,,,CC BY-SA 3.0,"<p>If you want to do logistic regression, a simple approach is:</p>

<ul>
<li>for each continuous feature with missing data, replace all missing values by the average or median value for this feature, and create one more boolean feature which indicates whether the data is missing or not</li>
<li>for each unordered categorical feature with missing data, put all missing values into a new category</li>
</ul>
",2013-10-14 02:23:17.417
57312,18198.0,1,,,,Degrees of Freedom for Ridge regression without knowing the Ridge Parameter?,<ridge-regression><degrees-of-freedom>,CC BY-SA 3.0,"<p>There is a very nice post here that gives a neat solution to the problem of finding the ridge parameter when the degrees of freedom are known:</p>

<p><a href=""https://stats.stackexchange.com/questions/8309/how-to-calculate-regularization-parameter-in-ridge-regression-given-degrees-of-f"">How to calculate regularization parameter in ridge regression given degrees of freedom and input matrix?</a></p>

<p>My question is how can you know what the degrees of freedom are before knowing the ridge parameter value is? ( I have added a comment to the other thread but its quite old so thought it best to create a new topic).</p>
",2013-10-11 16:59:04.847
57313,12544.0,2,,57309.0,,,,CC BY-SA 3.0,"<p>Yes, you should. You should always correlate exogenous variables - their correlations are not part of the model, and if you don't, you're either worsening fit, or getting degrees of freedom (which appear to improve fit) when you shouldn't.</p>

<p>When you do this in regression analysis, they are correlated (and there's no way of stopping them).</p>

<p>Also, you say ""If Mar_Single = 1 then Mar_Married will always = 0.""  Yes, but that doesn't make them perfectly correlated - because if mar_single = 0 mar_married might be 0 or might be 1.</p>

<p>If you use Mplus, it will correlate x-variables by default and will not include these correlations in the null model which it uses to calculate CFI and NFI.  </p>
",2013-10-11 17:21:17.923
57314,3446.0,1,,,,Coverage rates of confidence intervals in reality,<confidence-interval><references><application>,CC BY-SA 3.0,"<p>One proves mathematically that if assumptions of a model are satisfied, then the coverage rate of a $100p\%$ confidence interval is $100p\%$.  But then statistics gets applied to the world, where model assumptions may not be satisfied.  Are there any studies comparing the coverage rates of confidence intervals applied to the real world with theoretical coverage rates?</p>
",2013-10-11 17:31:46.623
57315,5984.0,1,,,,What does it mean that random effects are highly correlated?,<r><lme4-nlme><random-effects-model>,CC BY-SA 3.0,"<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
",2013-10-11 17:38:34.780
57316,22615.0,1,,,,Mean difference for count data,<hypothesis-testing><mean><count-data>,CC BY-SA 4.0,"<p>I have two samples $s_1$ and $s_2$ of count data. The sample size is > 1000 each. The distributions look similar to a Poisson distribution but the variance is much larger than the mean. </p>

<p>How do I test whether the mean of $s_1$ is larger than the mean of $s_2$?</p>
",2013-10-11 18:00:38.057
57317,22564.0,1,57321.0,,,"When making inferences about group means, are credible Intervals sensitive to within-subject variance while confidence intervals are not?",<r><confidence-interval><mixed-model><jags><error-propagation>,CC BY-SA 3.0,"<p>This is a spin off of this question:
<a href=""https://stats.stackexchange.com/questions/72453/how-to-compare-two-groups-with-multiple-measurements-for-each-individual-with-r"">How to compare two groups with multiple measurements for each individual with R?</a></p>

<p>In the answers there (if I understood correctly) I learned that within-subject variance does not effect inferences made about group means and it is ok to simply take the averages of averages to calculate group mean, then calculate within-group variance and use that to perform significance tests. I would like to use a method where the larger the within subject variance the less sure I am about the group means or understand why it does not make sense to desire that.</p>

<p>Here is a plot of the original data along with some simulated data that used the same subject means, but sampled the individual measurements for each subject from a normal distribution using those means and a small within-subject variance (sd=.1). As can be seen the group level confidence intervals (bottom row) are unaffected by this (at least the way I calculated them).</p>

<p><img src=""https://i.stack.imgur.com/eiRJ9.png"" alt=""enter image description here""></p>

<p>I also used rjags to estimate the group means in three ways.
1) Use the raw original data
2) Use only the Subject means
3) Use the simulated data with small within-subject sd</p>

<p>The results are below. Using this method we see that the 95% credible intervals are narrower in cases #2 and #3. This meets my intuition of what I would like to occur when making inferences about group means, but I am not sure if this is just some artifact of my model or a property of credible intervals.</p>

<p>Note. To use rjags you need to first install JAGS from here:
<a href=""http://sourceforge.net/projects/mcmc-jags/files/"" rel=""nofollow noreferrer"">http://sourceforge.net/projects/mcmc-jags/files/</a></p>

<p><img src=""https://i.stack.imgur.com/H2rnX.png"" alt=""enter image description here""></p>

<p>The various code is below.</p>

<p>The original data:</p>

<pre><code>structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 
3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 
3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 
6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 
10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 
12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 
15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 
18, 18, 18, 18, 18, 2, 0, 16, 2, 16, 2, 8, 10, 8, 6, 4, 4, 8, 
22, 12, 24, 16, 8, 24, 22, 6, 10, 10, 14, 8, 18, 8, 14, 8, 20, 
6, 16, 6, 6, 16, 4, 2, 14, 12, 10, 4, 10, 10, 8, 4, 10, 16, 16, 
2, 8, 4, 0, 0, 2, 16, 10, 16, 12, 14, 12, 8, 10, 12, 8, 14, 8, 
12, 20, 8, 14, 2, 4, 8, 16, 10, 14, 8, 14, 12, 8, 14, 4, 8, 8, 
10, 4, 8, 20, 8, 12, 12, 22, 14, 12, 26, 32, 22, 10, 16, 26, 
20, 12, 16, 20, 18, 8, 10, 26), .Dim = c(108L, 3L), .Dimnames = list(
    NULL, c(""Group"", ""Subject"", ""Value"")))
</code></pre>

<p>Get subject Means and simulate the data with small within-subject variance:</p>

<pre><code>#Get Subject Means
means&lt;-aggregate(Value~Group+Subject, data=dat, FUN=mean)

#Initialize ""dat2"" dataframe
dat2&lt;-dat

#Sample individual measurements for each subject
temp=NULL
for(i in 1:nrow(means)){
  temp&lt;-c(temp,rnorm(6,means[i,3], .1))
}

#Set Simulated values
dat2[,3]&lt;-temp
</code></pre>

<p>The function to fit the JAGS model:</p>

<pre><code> require(rjags) 

#Jags fit function
jags.fit&lt;-function(dat2){

  #Create JAGS model
  modelstring = ""

  model{
  for(n in 1:Ndata){
  y[n]~dnorm(mu[subj[n]],tau[subj[n]]) T(0, )
  }

  for(s in 1:Nsubj){
  mu[s]~dnorm(muG,tauG) T(0, )
  tau[s] ~ dgamma(5,5)
  }


  muG~dnorm(10,.01) T(0, )
  tauG~dgamma(1,1)

  }
  ""
  writeLines(modelstring,con=""model.txt"")

#############  

  #Format Data
  Ndata = nrow(dat2)
  subj = as.integer( factor( dat2$Subject ,
                             levels=unique(dat2$Subject ) ) )
  Nsubj = length(unique(subj))
  y = as.numeric(dat2$Value)

  dataList = list(
    Ndata = Ndata ,
    Nsubj = Nsubj ,
    subj = subj ,
    y = y
  )

  #Nodes to monitor
  parameters=c(""muG"",""tauG"",""mu"",""tau"")


  #MCMC Settings
  adaptSteps = 1000             
  burnInSteps = 1000            
  nChains = 1                   
  numSavedSteps= nChains*10000          
  thinSteps=20                      
  nPerChain = ceiling( ( numSavedSteps * thinSteps ) / nChains )            


  #Create Model
  jagsModel = jags.model( ""model.txt"" , data=dataList, 
                          n.chains=nChains , n.adapt=adaptSteps , quiet=FALSE )
  # Burn-in:
  cat( ""Burning in the MCMC chain...\n"" )
  update( jagsModel , n.iter=burnInSteps )

  # Getting DIC data:
  load.module(""dic"")


  # The saved MCMC chain:
  cat( ""Sampling final MCMC chain...\n"" )
  codaSamples = coda.samples( jagsModel , variable.names=parameters , 
                              n.iter=nPerChain , thin=thinSteps )  

  mcmcChain = as.matrix( codaSamples )

  result = list(codaSamples=codaSamples, mcmcChain=mcmcChain)

}
</code></pre>

<p>Fit the model to each group of each dataset:</p>

<pre><code>#Fit to raw data
groupA&lt;-jags.fit(dat[which(dat[,1]==1),])
groupB&lt;-jags.fit(dat[which(dat[,1]==2),])
groupC&lt;-jags.fit(dat[which(dat[,1]==3),])

#Fit to subject mean data
groupA2&lt;-jags.fit(means[which(means[,1]==1),])
groupB2&lt;-jags.fit(means[which(means[,1]==2),])
groupC2&lt;-jags.fit(means[which(means[,1]==3),])

#Fit to simulated raw data (within-subject sd=.1)
groupA3&lt;-jags.fit(dat2[which(dat2[,1]==1),])
groupB3&lt;-jags.fit(dat2[which(dat2[,1]==2),])
groupC3&lt;-jags.fit(dat2[which(dat2[,1]==3),])
</code></pre>

<p>Credible interval/highest density interval function:</p>

<pre><code>#HDI Function
get.HDI&lt;-function(sampleVec,credMass){ 
  sortedPts = sort( sampleVec )
  ciIdxInc = floor( credMass * length( sortedPts ) )
  nCIs = length( sortedPts ) - ciIdxInc
  ciWidth = rep( 0 , nCIs )
  for ( i in 1:nCIs ) {
    ciWidth[ i ] = sortedPts[ i + ciIdxInc ] - sortedPts[ i ]
  }
  HDImin = sortedPts[ which.min( ciWidth ) ]
  HDImax = sortedPts[ which.min( ciWidth ) + ciIdxInc ]
  HDIlim = c( HDImin , HDImax, credMass )
  return( HDIlim )
}
</code></pre>

<p>First Plot:</p>

<pre><code>layout(matrix(c(1,1,2,2,3,4),nrow=3,ncol=2, byrow=T))

boxplot(dat[,3]~dat[,2], 
xlab=""Subject"", ylab=""Value"", ylim=c(0, 1.2*max(dat[,3])),
col=c(rep(""Red"",length(which(dat[,1]==unique(dat[,1])[1]))/6),
rep(""Green"",length(which(dat[,1]==unique(dat[,1])[2]))/6),
rep(""Blue"",length(which(dat[,1]==unique(dat[,1])[3]))/6)
),
main=""Original Data""
)
stripchart(dat[,3]~dat[,2], vert=T, add=T, pch=16)
legend(""topleft"", legend=c(""Group A"", ""Group B"", ""Group C"", ""Individual Means +/- 95% CI""),
col=c(""Red"",""Green"",""Blue"", ""Grey""), lwd=3, bty=""n"", pch=c(15),
pt.cex=c(rep(0.1,3),1),
ncol=3)

for(i in 1:length(unique(dat[,2]))){
  m&lt;-mean(examp[which(dat[,2]==unique(dat[,2])[i]),3])
  ci&lt;-t.test(dat[which(dat[,2]==unique(dat[,2])[i]),3])$conf.int[1:2]

  points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.3,
           ci[1],i-.3,
           ci[2], lwd=4, col=""Grey""
  )
}



boxplot(dat2[,3]~dat2[,2], 
xlab=""Subject"", ylab=""Value"", ylim=c(0, 1.2*max(dat2[,3])),
col=c(rep(""Red"",length(which(dat2[,1]==unique(dat2[,1])[1]))/6),
rep(""Green"",length(which(dat2[,1]==unique(dat2[,1])[2]))/6),
rep(""Blue"",length(which(dat2[,1]==unique(dat2[,1])[3]))/6)
),
main=c(""Simulated Data"", ""Same Subject Means but Within-Subject SD=.1"")
)
stripchart(dat2[,3]~dat2[,2], vert=T, add=T, pch=16)
legend(""topleft"", legend=c(""Group A"", ""Group B"", ""Group C"", ""Individual Means +/- 95% CI""),
col=c(""Red"",""Green"",""Blue"", ""Grey""), lwd=3, bty=""n"", pch=c(15),
pt.cex=c(rep(0.1,3),1),
ncol=3)

for(i in 1:length(unique(dat2[,2]))){
  m&lt;-mean(examp[which(dat2[,2]==unique(dat2[,2])[i]),3])
  ci&lt;-t.test(dat2[which(dat2[,2]==unique(dat2[,2])[i]),3])$conf.int[1:2]

  points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.3,
           ci[1],i-.3,
           ci[2], lwd=4, col=""Grey""
  )
}


means&lt;-aggregate(Value~Group+Subject, data=dat, FUN=mean)

boxplot(means[,3]~means[,1], col=c(""Red"",""Green"",""Blue""),
ylim=c(0,1.2*max(means[,3])), ylab=""Value"", xlab=""Group"",
main=""Original Data""
)
stripchart(means[,3]~means[,1], pch=16, vert=T, add=T)

for(i in 1:length(unique(means[,1]))){
  m&lt;-mean(means[which(means[,1]==unique(means[,1])[i]),3])
  ci&lt;-t.test(means[which(means[,1]==unique(means[,1])[i]),3])$conf.int[1:2]

  points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.3,
           ci[1],i-.3,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3, col=""Grey"")


means2&lt;-aggregate(Value~Group+Subject, data=dat2, FUN=mean)

boxplot(means2[,3]~means2[,1], col=c(""Red"",""Green"",""Blue""),
ylim=c(0,1.2*max(means2[,3])), ylab=""Value"", xlab=""Group"",
main=""Simulated Data Group Averages""
)
stripchart(means2[,3]~means2[,1], pch=16, vert=T, add=T)

for(i in 1:length(unique(means2[,1]))){
  m&lt;-mean(means[which(means2[,1]==unique(means2[,1])[i]),3])
  ci&lt;-t.test(means[which(means2[,1]==unique(means2[,1])[i]),3])$conf.int[1:2]

  points(i-.3,m, pch=15,cex=1.5, col=""Grey"")
  segments(i-.3,
           ci[1],i-.3,
           ci[2], lwd=4, col=""Grey""
  )
}
legend(""topleft"", legend=c(""Group Means +/- 95% CI""), bty=""n"", pch=15, lwd=3,   col=""Grey"")
</code></pre>

<p>Second Plot:</p>

<pre><code>layout(matrix(c(1,2,3,4,4,4,5,5,5,6,6,6),nrow=4,ncol=3, byrow=T))

#Plot priors
plot(seq(0,10,by=.01),dgamma(seq(0,10,by=.01),5,5), type=""l"", lwd=4,
     xlab=""Value"", ylab=""Density"",
     main=""Prior on Within-Subject Precision""
)
plot(seq(0,10,by=.01),dgamma(seq(0,10,by=.01),1,1), type=""l"", lwd=4,
     xlab=""Value"", ylab=""Density"",
     main=""Prior on Within-Group Precision""
)
plot(seq(0,300,by=.01),dnorm(seq(0,300,by=.01),10,100), type=""l"", lwd=4,
     xlab=""Value"", ylab=""Density"",
     main=""Prior on Group Means""
)


#Set overall xmax value
x.max&lt;-1.1*max(groupA$mcmcChain[,""muG""],groupB$mcmcChain[,""muG""],groupC$mcmcChain[,""muG""],
               groupA2$mcmcChain[,""muG""],groupB2$mcmcChain[,""muG""],groupC2$mcmcChain[,""muG""],
               groupA3$mcmcChain[,""muG""],groupB3$mcmcChain[,""muG""],groupC3$mcmcChain[,""muG""]
)


#Plot result for raw data
#Set ymax
y.max&lt;-1.1*max(density(groupA$mcmcChain[,""muG""])$y,density(groupB$mcmcChain[,""muG""])$y,density(groupC$mcmcChain[,""muG""])$y)

plot(density(groupA$mcmcChain[,""muG""]),xlim=c(0,x.max), 
     ylim=c(-.1*y.max,y.max), lwd=3, col=""Red"",
     main=""Group Mean Estimates: Fit to Raw Data"", xlab=""Value""
)
lines(density(groupB$mcmcChain[,""muG""]), lwd=3, col=""Green"")
lines(density(groupC$mcmcChain[,""muG""]), lwd=3, col=""Blue"")

hdi&lt;-get.HDI(groupA$mcmcChain[,""muG""], .95)
segments(hdi[1],-.033*y.max,hdi[2],-.033*y.max, lwd=3, col=""Red"")

hdi&lt;-get.HDI(groupB$mcmcChain[,""muG""], .95)
segments(hdi[1],-.066*y.max,hdi[2],-.066*y.max, lwd=3, col=""Green"")

hdi&lt;-get.HDI(groupC$mcmcChain[,""muG""], .95)
segments(hdi[1],-.099*y.max,hdi[2],-.099*y.max, lwd=3, col=""Blue"")

####

#Plot result for mean data

#x.max&lt;-1.1*max(groupA2$mcmcChain[,""muG""],groupB2$mcmcChain[,""muG""],groupC2$mcmcChain[,""muG""])
y.max&lt;-1.1*max(density(groupA2$mcmcChain[,""muG""])$y,density(groupB2$mcmcChain[,""muG""])$y,density(groupC2$mcmcChain[,""muG""])$y)

plot(density(groupA2$mcmcChain[,""muG""]),xlim=c(0,x.max), 
     ylim=c(-.1*y.max,y.max), lwd=3, col=""Red"",
     main=""Group Mean Estimates: Fit to Subject Means"", xlab=""Value""
)
lines(density(groupB2$mcmcChain[,""muG""]), lwd=3, col=""Green"")
lines(density(groupC2$mcmcChain[,""muG""]), lwd=3, col=""Blue"")

hdi&lt;-get.HDI(groupA2$mcmcChain[,""muG""], .95)
segments(hdi[1],-.033*y.max,hdi[2],-.033*y.max, lwd=3, col=""Red"")

hdi&lt;-get.HDI(groupB2$mcmcChain[,""muG""], .95)
segments(hdi[1],-.066*y.max,hdi[2],-.066*y.max, lwd=3, col=""Green"")

hdi&lt;-get.HDI(groupC2$mcmcChain[,""muG""], .95)
segments(hdi[1],-.099*y.max,hdi[2],-.099*y.max, lwd=3, col=""Blue"")




####
#Plot result for simulated data
#Set ymax
#x.max&lt;-1.1*max(groupA3$mcmcChain[,""muG""],groupB3$mcmcChain[,""muG""],groupC3$mcmcChain[,""muG""])
y.max&lt;-1.1*max(density(groupA3$mcmcChain[,""muG""])$y,density(groupB3$mcmcChain[,""muG""])$y,density(groupC3$mcmcChain[,""muG""])$y)

plot(density(groupA3$mcmcChain[,""muG""]),xlim=c(0,x.max), 
     ylim=c(-.1*y.max,y.max), lwd=3, col=""Red"",
     main=c(""Group Mean Estimates: Fit to Simulated data"", ""(Within-Subject SD=0.1)""), xlab=""Value""
)
lines(density(groupB3$mcmcChain[,""muG""]), lwd=3, col=""Green"")
lines(density(groupC3$mcmcChain[,""muG""]), lwd=3, col=""Blue"")

hdi&lt;-get.HDI(groupA3$mcmcChain[,""muG""], .95)
segments(hdi[1],-.033*y.max,hdi[2],-.033*y.max, lwd=3, col=""Red"")

hdi&lt;-get.HDI(groupB3$mcmcChain[,""muG""], .95)
segments(hdi[1],-.066*y.max,hdi[2],-.066*y.max, lwd=3, col=""Green"")

hdi&lt;-get.HDI(groupC3$mcmcChain[,""muG""], .95)
segments(hdi[1],-.099*y.max,hdi[2],-.099*y.max, lwd=3, col=""Blue"")
</code></pre>

<p><strong>EDIT with my personal version of the answer from @StÃ©phaneLaurent</strong></p>

<p>I used the model he described to sample from a normal distribution with mean=0, between subject variance =1 and within subject error/variance= 0.1,1,10,100. A subset of the confidence intervals are shown in the left panels while the distribution of their widths is shown by the corresponding right panels. This has convinced me that he is 100% correct. However, I am still confused by my example above but will follow this up with a new more focused question.</p>

<p><img src=""https://i.stack.imgur.com/qVyVm.png"" alt=""enter image description here""></p>

<p>The code for the above simulation and charts:</p>

<pre><code>dev.new()
par(mfrow=c(4,2))


num.sims&lt;-10000
sigmaWvals&lt;-c(.1,1,10,100)
muG&lt;-0  #Grand Mean
sigma.between&lt;-1  #Between Experiment sd

for(sigma.w in sigmaWvals){

  sigma.within&lt;-sigma.w #Within Experiment sd

  out=matrix(nrow=num.sims,ncol=2)
  for(i in 1:num.sims){

    #Sample the three experiment means (mui, i=1:3)
    mui&lt;-rnorm(3,muG,sigma.between)

    #Sample the three obersvations for each experiment (muij, i=1:3, j=1:3)
    y1j&lt;-rnorm(3,mui[1],sigma.within)
    y2j&lt;-rnorm(3,mui[2],sigma.within)
    y3j&lt;-rnorm(3,mui[3],sigma.within)


    #Put results in data frame
    d&lt;-as.data.frame(cbind(
      c(rep(1,3),rep(2,3),rep(3,3)),
      c(y1j, y2j, y3j )
    ))
    d[,1]&lt;-as.factor(d[,1])

    #Calculate means for each experiment
    dmean&lt;-aggregate(d[,2]~d[,1], data=d, FUN=mean)

    #Add new confidence interval data to output
    out[i,]&lt;-t.test(dmean[,2])$conf.int[1:2]

  }

  #Calculate % of intervals that contained muG
  cover&lt;-matrix(nrow=nrow(out),ncol=1)
  for(i in 1:nrow(out)){
    cover[i]&lt;-out[i,1]&lt;muG &amp; out[i,2]&gt;muG
  }



  sub&lt;-floor(seq(1,nrow(out),length=100))
  plot(out[sub,1], ylim=c(min(out[sub,1]),max(out[sub,2])),
       xlab=""Simulation #"", ylab=""Value"", xaxt=""n"",
       main=c(paste(""# of Sims="",num.sims),
              paste(""% CIs Including muG="",100*round(length(which(cover==T))/nrow(cover),3)))
  )
  axis(side=1, at=1:100, labels=sub)
  points(out[sub,2])

  cnt&lt;-1
  for(i in sub){
    segments(cnt, out[i,1],cnt,out[i,2])
    cnt&lt;-cnt+1
  }
  abline(h=0, col=""Red"", lwd=3)

  hist(out[,2]-out[,1], freq=F, xlab=""Width of 95% CI"",
       main=c(paste(""muG="", muG), 
              paste(""Sigma Between="",sigma.between), 
              paste(""Sigma Within="",sigma.within))
  )

}
</code></pre>
",2013-10-11 18:05:20.930
57318,18198.0,1,,,,Iterative method to find Ridge Regression Parameter,<ridge-regression>,CC BY-SA 3.0,"<p>I have seen a method whereby instead of trying to estimate the ridge parameter (k) directly from the data (using one of the many many ridge parameter estimators in the literature) you solve for it iteratively.</p>

<p>The method is simple enough: You simply increase k (in suitably small steps) until the condition number is reduced blow 10.  </p>

<p>At first blush this seems like quite a nice solution to me but I've never seen a Ridge Regression paper/book that uses it. </p>

<p>Update OK this is basically the method suggested by Marquardt ""Generalized inverses, Ridge Regression, Biased Linear Estimation and Non-linear Estimation"" the only difference being he used VIF's to measure the MC while this method uses the condition number. McDonald and Galrneau ""A Monte-Carlo Evaluation of some Ridge-Type Estimators"" note that this method is may not be appropriate for all data sets as it does not include the y values (observations). I still have not found a paper where the Marquardt method is tested against other estimators for the ridge parameter does anybody know of such a paper?  </p>

<p>Is this method theoretically sound though? Even if (as I suspect) it isn't does it really matter for the average practitioner who just want to produce more stable estimates of their Beta's (the weights in the regression) rather than having them ""blow up"" to grossly unrealistic values when they experience severe MC?</p>

<p>Truly I would like to find a better method than this ideally with a solid theoretical underpinning but its hard to see from a practical view point it can be improved upon?</p>
",2013-10-11 18:05:44.790
57339,10987.0,2,,57316.0,,,,CC BY-SA 3.0,"<p>Give your large sample sizes, you could probably use a t-test on the means. If your sample sizes are equal, you are in pretty good shape whether you want to use a pooled estimate of the variance or unpooled (Welch's test). Do a one sided test, if you are sure that the population of s1 has a mean at least as large as the mean of the population of s2.</p>

<p>Note: If the variances are much larger than the means, your counts are not Poisson. But what matters here is the distribution of the sample averages, and that should be nearly normal, unless the data are super-skewed. In that case, you could do a non-parametric test like the Kruskal-Wallis.</p>
",2013-10-11 23:59:32.067
57340,10987.0,2,,57323.0,,,,CC BY-SA 3.0,"<p>You could try CART (tree) classification regression. That would select a decision tree algorithm for the outcomes based on the answers to the questions. As a bi-product, it would indicate which questions are most important in predicting outcome.  </p>
",2013-10-12 00:09:09.067
57416,22507.0,2,,57414.0,,,,CC BY-SA 3.0,"<p>Multinomial test, if I understand you correctly.</p>
",2013-10-14 02:36:19.860
57319,1693.0,1,,,,How high must logistic covariates' predictive accuracy be for a reversal effect to show up?,<regression><logistic><controlling-for-a-variable>,CC BY-SA 4.0,"<p>I am modeling an outcome for hospital patients, 'RA' (whether readmitted; 0=No, 1=Yes).  My predictor of interest is 'HHS' (whether referred to Home Health Services such as from a visiting nurse; 0=No, 1=Yes).  Those referred readmit at a 15.2% rate; others, 9.2%, but the former are needier, sicker patients.  Conventional thinking is that if we controlled for severity of illness this difference would not only be washed out but would reverse itself.  In other words, holding constant the severity of illness, having HHS should mean a lower RA rate.</p>

<p>With HHS as the sole predictor, its coefficient (B) in a <strong>logistic</strong> regression = 0.6 (N ~ 25k).  B is reduced to 0.2 with a group of covariates controlled, each accounting for some aspect of severity of illness, but B doesn't fall below zero.</p>

<p>HHS alone explains only about 1% of the variance in RA; with the other predictors, this becomes 4%.* Perhaps this is the problem--that these covariates are not explaining enough variance to ""succeed"" in reversing the sign of the coefficient of interest.  If this is true, is there a way to estimate how high <strong>their</strong> explained variance needs to be for such a reversal to show up?</p>

<p>EDIT:  Alecos Papadopoulos has come up with an impressive solution that answers this question, soon to be published in <em>The American Statistician</em>.  See <a href=""https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1704873"" rel=""nofollow noreferrer"">https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1704873</a></p>

<hr>

<p>*Using either of 2 pseudo-R-squared formulas; Cox &amp; Snell's or Menard's [-2LL0 - (-2LL1)] / [-2LL0.]</p>
",2013-10-11 18:20:07.563
57320,19331.0,2,,37182.0,,,,CC BY-SA 3.0,"<p>The spatial power covariance structure is a generalization of the first-order autoregressive covariance structure.  Where the first-order autoregressive structure assumes the time points are equally spaced, the spatial power structure can account for a continuous time point.  In reality, we could just forget the first-order autoregressive structure entirely, because if we fit the spatial power structure when the data are equally spaced we'll get the same answer as when using the first-order autoregressive structure.</p>

<p>All that aside, the correlation function you're looking for is <code>corCAR1()</code>, which is the continuous first-order autoregressive structure.  If you're looking to duplicate what you fit in SAS, then the code you're looking for is:</p>

<pre><code>gls(CD4t~T, data=df, na.action = (na.omit), method = ""REML"",
    corr=corCAR1(form=~T|NUM_PAT))
</code></pre>

<p>Of course, you don't need to specify <code>method = ""REML""</code>, since, as in SAS, the default method in <code>gls()</code> is already restricted maximum likelihood.</p>
",2013-10-11 19:13:11.093
57321,6162.0,2,,57317.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>In the answers there (if I understood correctly) I learned that
  within-subject variance does not effect inferences made about group
  means and it is ok to simply take the averages of averages to
  calculate group mean, then calculate within-group variance and use
  that to perform significance tests.</p>
</blockquote>

<p>Let me develop this idea here. The model for the individual observations is 
$$y_{ijk}= \mu_i + \alpha_{ij} + \epsilon_{ijk}$$, where :</p>

<ul>
<li><p>$y_{ijk}$ is the $k$-th measurement of individual $j$ of group $i$ </p></li>
<li><p>$\alpha_{ij} \sim_{\text{iid}} {\cal N}(0, \sigma^2_b)$ is the random effect for individual $j$ of group $i$ </p></li>
<li><p>$\epsilon_{ijk} \sim_{\text{iid}} {\cal N}(0, \sigma^2_w)$ is the within-error</p></li>
</ul>

<p>In <a href=""https://stats.stackexchange.com/a/72490/8402"">my answer to your first question</a>, I have suggested you to note that one obtains a classical (fixed effects) Gaussian linear model for the subjects means $\bar y_{ij\bullet}$. Indeed you can easily check that $$\bar y_{ij\bullet} = \mu_i + \delta_{ij}$$ with $$\delta_{ij} = \alpha_{ij} + \frac{1}{K}\sum_k \epsilon_{ijk} 
\sim_{\text{iid}} {\cal N}(0, \sigma^2) \quad \text{where } \quad \boxed{\sigma^2=\sigma^2_b+\frac{\sigma^2_w}{K}},$$
assuming $K$ repeated measurements for each individual. This is nothing but the one-way ANOVA model with a fixed factor.</p>

<p>And then I claimed that in order to draw inference about the $\mu_i$ you can simply consider the simple classical linear model whose observations are the subjects means $\bar y_{ij\bullet}$. <strong>Update 12/04/2014</strong>: Some examples of this idea are now written on my blog: <a href=""http://stla.github.io/stlapblog/posts/ModelReduction.html"" rel=""nofollow noreferrer"">Reducing a model to get confidence intervals</a>.  I'm under  the impression that this always work when we average the data over the levels of a random effect.</p>

<blockquote>
  <p>In the answers there (if I understood correctly) I learned that
  within-subject variance does not effect inferences made about group
  means and it is ok to simply take the averages of averages to
  calculate group mean, then calculate within-group variance and use
  that to perform significance tests. I would like to use a method where
  the larger the within subject variance the less sure I am about the
  group means or understand why it does not make sense to desire that.</p>
</blockquote>

<p>As you see from the boxed formula, the within-variance $\sigma^2_w$ plays a role in the model for the observed group means. </p>
",2013-10-11 19:18:23.050
57322,10964.0,1,,,,Explaining p-value to a sophisticated layman,<p-value><intuition>,CC BY-SA 3.0,"<p>I think I understand the concept of p-value but unfortunately I still have to exert a lot of brain cycles to get my arms around it.</p>

<p>I would like to get an explanation of the p-value that is rigorous enough for a sophisticated layman - something that would be intuitive.</p>
",2013-10-11 19:42:47.813
57323,22618.0,1,57344.0,,,Simple recommender system - where to start?,<data-mining><recommender-system>,CC BY-SA 3.0,"<p>Without going into specifics, I'm currently working on a system that involves 20-25 questions being answered as either Green, Yellow, Orange or Red. After completing a subset of these questions (many questions can be left as defaulting to Green), the system allows our users to choose one outcome out of four, roughly corresponding to the answers they entered (OutcomeGreen, OutcomeYellow, OutcomeOrange or OutcomeRed). The answer that was selected most tends to be a good indicator as to what outcome they will select, but that's not always the case.</p>

<p>After having this system in place for the last 2 years, now I've received a request to have the system itself make a recommendation as to which outcome the user should select. Using data already accumulated over this period, I'd like to get some insight as to which questions/answers tend to be most influential for specific outcomes, and possibly give them more weight when determining what to recommend.</p>

<p>My main dilemma is that my last class on statistics was more than 20 years ago, and just looking through the tags here made me feel that I'm out of my depth. With the description I've provided, and the vast knowledge contained within this SE: </p>

<ul>
<li>Is there anything I should be looking into (tools, subset of
CrossValidated tags) that would help gain better insight, and where I
should look for more information?  </li>
<li>Is there a quick way to get up-to-speed on what I'm missing?</li>
</ul>

<p><strong>Background:</strong> I'm a developer in many programming languages, and an amateur mathematician (mostly playing around in number theory and linear programming). I'm also a quick learner; I've been learning how to use R in my spare time. I just need some indication as to where I would find info quickly that would help me move forward with this.</p>
",2013-10-11 19:57:04.153
57350,11772.0,1,57351.0,,,Subscript notation in expectations,<conditional-expectation><notation>,CC BY-SA 3.0,"<p>What is the exact meaning of the subscript notation $\mathbb{E}_X[f(X)]$ in conditional expectations in the framework of measure theory ? These subscripts do not appear in the definition of conditional expectation, but we may see for example in <a href=""http://en.wikipedia.org/wiki/Law_of_total_expectation"">this page of wikipedia</a>. (Note that it wasn't always the case, <a href=""http://en.wikipedia.org/w/index.php?title=Law_of_total_expectation&amp;oldid=548089336"">the same page</a> few months ago).</p>

<p>What should be for example the meaning of $\mathbb{E}_X[X+Y]$ with $X\sim\mathcal{N}(0,1)$ and $Y=X+1$ ?</p>
",2013-10-12 11:04:38.997
57324,5045.0,2,,57322.0,,,,CC BY-SA 3.0,"<p>Take a look at the tooth brushing example at the very start of Chapter 14 of Andrew Vickers' book <a href=""http://www.pearsonhighered.com/vickers/"" rel=""noreferrer"">What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics</a>. It starts on page 57 or you can use the table of contents button in the bottom <em>left</em> corner to find it. </p>

<p>Here's an excerpt:</p>

<blockquote>
  <p>[I]f you do nothing else, please try to remember the following
  sentence: â€œthe $p$-value is the probability that the data would be at
  least as extreme as those observed, if the null hypothesis were true.â€
  Though Iâ€™d prefer that you also understood itâ€”about which, teeth
  brushing.</p>
  
  <p>I have three young children. In the evening, before we get to bedtime
  stories (bedtime stories being a nice way to end the day), we have to
  persuade them all to bathe, use the toilet, clean their teeth, change
  into pajamas, get their clothes ready for the next day and then
  actually get into bed (the persuading part being a nice way to go
  crazy). My five-year-old can often be found sitting on his bed, fully
  dressed, claiming to have clean teeth. The give-away is the bone dry
  toothbrush: he says that he has brushed his teeth, I tell him that he
  couldnâ€™t have.</p>
  
  <p>My reasoning here goes like this: the toothbrush is dry; it is
  unlikely that the toothbrush would be dry if my son had cleaned his
  teeth; therefore he hasnâ€™t cleaned his teeth. Or using
  statistician-speak: here are the data (a dry toothbrush); here is a
  hypothesis (my son has cleaned his teeth); the data would be unusual
  if the hypothesis were true, therefore we should reject the
  hypothesis. </p>
  
  <p>[...]</p>
  
  <p>So here is what to parrot when we run into each other at a bar and I
  still havenâ€™t managed to work out any new party tricks: â€œThe $p$-value
  is the probability that the data would be at least as extreme as those
  observed, if the null hypothesis were true.â€ When I recover from
  shock, you can explain it to me in terms of a toothbrush (â€œThe
  probability of the toothbrush being dry if youâ€™ve just cleaned your
  teethâ€).</p>
</blockquote>

<p>The other thing I really like about this example is that it also explains that failing to reject the null does not mean the null is necessarily true. Vickers writes that his son has now worked out the trick and has taken to running his toothbrush under the tap for a second or two before heading to bed. Just because the toothbrush is wet (and the data is consistent with the null hypothesis), it does not mean that his son has cleaned his teeth. </p>
",2013-10-11 20:00:54.043
57325,22622.0,1,,,,Marginal effect in model with interactions,<regression><interaction>,CC BY-SA 3.0,"<p>I am running cross-sectional regressions of the type</p>

<p>$$Y_c = \alpha + \beta X_1 + \gamma X_2 + \delta_1 X_3 + \delta_2 X_1 X_3 + \delta_3 X_2 X_3 + e_c.$$</p>

<p><strong>My theoretical model</strong> implies that </p>

<ul>
<li>$\delta_2$ should be negative, </li>
<li>$\delta_3$ should be positive, and</li>
<li>the marginal effect of $X_3$ should be negative. </li>
</ul>

<p><strong>My estimates</strong> imply that </p>

<ul>
<li>$\widehat\delta_2$ is negative and significant, </li>
<li>$\widehat\delta_3$ is positive and insignificant, </li>
<li>$\widehat\beta$ is significant, and </li>
<li>$\widehat\gamma$ is insignificant. </li>
</ul>

<p>Building on this evidence, can I calculate the marginal effect of $X_3$ as $\delta_1 + \delta_2 E(X_1)$ where $E(X_1)$ is the mean of $X_1$, justifying this procedure with the fact that all the terms incorporating $X_2$ are insignificant?</p>
",2013-10-11 20:30:29.607
57326,20742.0,1,,,,Sweeping across multiple classifiers and choosing the best?,<machine-learning><classification><clustering><data-mining><weka>,CC BY-SA 3.0,"<p>I'm using Weka to perform classification, clustering, and some regression on a few large data sets. I'm currently trying out all the classifiers (decision tree, SVM, naive bayes, etc.).</p>

<p>Is there an automated way (in Weka or other machine learning toolkit) to sweep through all the available classifier algorithms to find the one that produces the best cross-validated accuracy or other metric? I'm not talking about boosting; rather, I'm looking to just choose the best classifier using a given data set.</p>

<p>I'd like to find the best clustering algorithm, too, for my other clustering problem; perhaps finding the lowest sum-of-squared-error?</p>
",2013-10-11 20:36:25.720
57327,22623.0,1,,,,Binary features for prediction,<regression><predictive-models><binary-data>,CC BY-SA 3.0,"<p>I have a set of relatively long ($\sim 1000$) binary features with scalar values $[0-10]$ attached to them. My aim is to write a predictor that learns to map the features to the $[0-10]$ interval to predict new features when given a new binary vector. I used SVM and Lasso with leave-one-out performance analysis, but both always end up predicting the mean value of the distribution (correlates to the histogram of all the feature - scalar distribution). The histograms are also rather norm / Rayleigh distributions. Suggestions for algorithms / feature space mapping? My main problem is that I am dealing with binary features for the first time.</p>
",2013-10-11 20:40:31.053
57328,13396.0,1,,,,Improving the quality of pseudo-randomly generated uncorrelated unit normals,<normal-distribution>,CC BY-SA 3.0,"<p>Let's say I want to generate $N$ sequences $p_j$, where $j = 1,\ldots,N$.  Each sequence has a length of $M$.  I want $\mathbb{E}[ p_j ] \to 0$ and $\text{corr}(p_j, p_k) \to \delta_{j, k}$ as $M \to +\infty$.</p>

<p>In practice, I can generate an $M \times N$ matrix of i.i.d. unit normals.  For example, in MATLAB, <code>Z = randn(M, N)</code>.  Then I get $p_j$ from the $j$-th column of $Z$.</p>

<p>For a finite value of $M$, the sample mean $\mathbb{E}[ p_j ] \neq 0$, but I can ""fix"" the problem if I remove the sample mean by working with $q_j = p_j - \mathbb{E}[ p_j ]$.</p>

<p>My question is -- how do I continue to improve my sequences, so that I get the 2nd-order moments I want, i.e., $\text{corr}(q_j, q_k) = \delta_{j,k}$ even when $M$ is finite?</p>
",2013-10-11 20:40:44.083
57329,10135.0,1,,,,BIC vs. Out of sample performance,<time-series><generalized-linear-model><bic><model-evaluation><out-of-sample>,CC BY-SA 3.0,"<p>I have two statistical models. Model 1 uses a <a href=""http://en.wikipedia.org/wiki/Generalized_linear_model"" rel=""nofollow"">GLM</a> approach while model 2 uses a time series approach for fitting. I want to compare these two models. </p>

<p>Model 1 (i.e. GLM) has a better out of sample performance. Model 2 has a better <a href=""http://en.wikipedia.org/wiki/Bayesian_information_criterion"" rel=""nofollow"">BIC</a> criteria. So based on out of sample performance, I should pick up model 1 and based on BIC I should pick up model 2 as the preferred model.    </p>

<p>I should add that in this context and for the question I am trying to answer, Both the BIC and out of sample performance are important. The question is how to choose the best model in this case? Should I consider other criteria? Please let me know if you know any good reference with similar cases.</p>
",2013-10-11 20:54:18.747
57349,22630.0,1,57352.0,,,"What does ""Mean of each pixel over all images"" mean?",<standard-deviation><neural-networks><mean><image-processing>,CC BY-SA 3.0,"<p>I was reading a paper related to Auto encoders for my project work. It is required to input images as vectors to the neural network. I couldn't understand a certain sentence due to lack of knowledge of statistics (I guess). I Googled, but the problem is I don't know what it is exactly and searching the same phrase returns the same kind of documents but not their explanation.</p>

<p>Source: <a href=""http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf"" rel=""nofollow"">http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf</a></p>

<blockquote>
  <p>We train on 1.6 million 32*32 color images that have been preprocessed
  by subtracting from each pixel its mean value over all images and then
  dividing by the standard deviation of all pixels over all images.</p>
</blockquote>

<p>What does it mean by ""subtracting from each pixel its mean value over all images and then
dividing by the standard deviation of all pixels over all images"".</p>

<p>My interpretation is: ""Subtracting from each pixel its mean value over all images""
   It means, for a pixel position in an image, subtract the average of values of that pixel position over all images and subtract from the current pixel value.</p>

<p>Am I correct? </p>

<p>It is somewhat ambiguous to me.</p>

<p>Please explain in some math terms.</p>
",2013-10-12 10:43:14.610
57330,22359.0,2,,57303.0,,,,CC BY-SA 3.0,"<p>How are the data sets related? IF both data sets are drawn from the same distribution (they describe the same problem) than you can use the labeled set as a ""test set"" for the clustering. Basically you treat the clustering algorithm as a classifier. The only problem is that you must find a match between the output of the clustering algorithm and the actual labels. </p>

<p>You might use some simple matching (ex: instances labeled GREEN are more often clustered in cluster 2 and BLUE in cluster 1 so cluster 1== BLUE and cluster 2 == GREEN).</p>

<p>More elegantly you can compute the <a href=""http://en.wikipedia.org/wiki/Mutual_information"" rel=""nofollow"" title=""Mutual Information"">Mutual Information</a> between the clustering output and actual labels. Mutual Information has a nice property, that one doesn't need to know the exact matching. MI will give high scores if most of the matching are consistent. Think of it as a correlation coefficient between (cluster &lt;-> actual label) relation.</p>

<p>Also check <a href=""http://en.wikipedia.org/wiki/Cluster_analysis"" rel=""nofollow"">http://en.wikipedia.org/wiki/Cluster_analysis</a> for some measures. The key phrase there is:</p>

<blockquote>
  <p>[...] clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by human (experts). Thus, the benchmark sets can be thought of as a gold standard for evaluation. </p>
</blockquote>

<p>For ROC usually one needs some ""<em>a posteriori</em>"" probability, outputted by the classifier, but in your case, the distance between the instance and the cluster center will work. Keep in mind that ROC is computed for a specific label at a time (i.e. one vs all). So for 5 labels you will get 4 independent AUROC values.</p>

<p>IMHO I strongly advise yo to do the CV for clustering if you have labeled data! Iterate it several times and use the mean of your measure as the performance. </p>

<p>I would also try this: Use some percent (66% usually) of unlabeled data to perform clustering, measure performance using labeled data, repeat the experiment with different randomization (usually 5-10 times) and report mean performance. Unfortunately I don't know if this method will give a good estimate of your real performance. Is it possible that will overfit the labeled data set. This is not a textbook approach, so, use it with caution.</p>
",2013-10-11 21:08:48.940
57331,22763.0,1,,,mikepk,CDF (cumulative frequency) of multiple samples in summed normals?,<probability><normal-distribution><quantiles>,CC BY-SA 3.0,"<p>Say I have some normally distributed data. I have an application where I compute the percentile (or cumulative frequency less than sample) for a particular sample using a CDF function along with the mean $\mu$ and standard deviation $\sigma$ of the samples.</p>

<p>so $$F_x(x) = \frac 12\left[1 + \text{erf} \left (\frac {x - \mu}{\sqrt{2 \sigma^2}}\right)\right]$$</p>

<p>Now I find myself in a situation where I want to determine the cumulative frequency of multiple samples across multiple data sets (finding something akin to an overall percentile of, say, three samples). Now assuming the variables are independent, I can sum the normals using </p>

<p>$$(\mu_\text{sum}, \sigma_\text{sum}) = (\mu_x + \mu_y + \mu_z), (\sqrt{Ïƒ^2_x + Ïƒ^2_y + Ïƒ^2_z})$$</p>

<p>Can I then sum the individual samples I care about and compare them to the new summed normal to compute a percentile of the three samples compared to the sum of the normals? Something tells me this doesn't work but I'd like to be sure. So I'm thinking something like computing the CDF using the sum of the samples I'm interested in:</p>

<p>$$F_x(x_x + x_y + x_z)$$</p>

<p>and using the $\mu$sum and $\sigma$sum in the CDF function above.</p>
",2013-10-11 21:12:06.117
57332,22359.0,2,,57279.0,,,,CC BY-SA 4.0,"<p>Features extracted from image/signal processing tend to get correlated a lot! This is not a very bad thing if you have enough samples.</p>
<p>From my experience, a classifier with small variance tend to work well (ex. logistic regression). They have less chances of overfitting the train data.</p>
<p>Another idea that I employed is the Additive logistic regression <a href=""https://people.csail.mit.edu/torralba/courses/6.869/lectures/lecture6/boosting.pdf"" rel=""nofollow noreferrer"">here</a> and <a href=""https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-2/Additive-logistic-regression--a-statistical-view-of-boosting-With/10.1214/aos/1016218223.full"" rel=""nofollow noreferrer"">here</a> some references. They are already implemented in Weka. They are slower than the logistic models. In the same time they have the great advantage that they perform a feature selection while learning. Moreover, the model is human friendly so you can see what features are more relevant.</p>
<p>Hope it helps</p>
",2013-10-11 21:23:38.743
57333,22359.0,2,,57026.0,,,,CC BY-SA 3.0,"<p>Try <a href=""http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"" rel=""nofollow noreferrer"">this</a> paper. Your answer might be at chapter 3.2, figures 2 and 3. </p>

<p>Long story short: The same performance can be obtained for different pairs of C and kernel parameters. You shouldn't try to manually tune a SVM.</p>

<p><strong>Edit:</strong> Some details:</p>

<p>I usually tune C (the cost parameter) when I have largely imbalanced classes. That is, one class have 10% and the other 90%. Some SVM libraries (esp. libSVM which I use) lets you specify a cost for each class. According to <a href=""http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"" rel=""nofollow noreferrer"">libsvm</a> paper, $\frac{c_1}{c_2} = \frac{n_2}{n_1}$ where $n_2&gt;n_1$ , $n_i$ is the volume of the i'th class. If you let $c_2 = 1$ then $c_1 = n_2/n_1$ . There is also a ""global"" C, that is multiplied with the specific $c_i$ values.</p>

<p>When the learning algorithm computes the error for the current SVM parameters, it multiplies each wrongly classified instance with this cost. If the cost is the same for both classes, the lesser class errors will get diluted and your final model will tend not to predict very well (or not at all) the weakly represented class.</p>

<p><strong>Gamma</strong> acts as the $\sigma$ for a Gaussian kernel $G(x) = exp(-x^2/2\sigma^2)$. Note from the equation of RBF : $K(x,y)=exp(-\gamma||x-y||^2)$  that $\gamma$ is more or less proportional to $1/\sigma^2$. Basically $\gamma$ controls the width of the kernel.</p>

<p>The intuition behind this is that a large kernel will tend to produce a smoother border between classes and a narrower kernel a more intricate border. In extreme, the former will tend to give higher bias (it learns only the general aspect of the data) and the latter will tend to overfit (it learns all the details, including the outliers and errors in the data). None of these extremes are welcome in applications. A midpoint is desired, but this midpoint cannot be computed analytically and depends on the actual data.</p>

<p>This is why, the metaparameters are usually searched through cross validation. Please keep  in mind that you must optimize for BOTH parameters in the same time.</p>

<p>The <strong>cost parameter C</strong>.</p>

<p>The theory says that SVM is a large margin classifier. In layman terms this means that it tries to find a border that is somehow as far away as possible from both classes. See the figure below (<a href=""http://en.wikipedia.org/wiki/Support_vector_machine"" rel=""nofollow noreferrer"">wikipedia</a>).</p>

<p><img src=""https://i.stack.imgur.com/peYzY.png"" alt=""example of a large margin""></p>

<p>Both H2 and H3 are ok, but H3 is better because if new samples arrive, is more likely to be classified wrong by H2 than H3, because H2 crosses near the black group.</p>

<p>The math behind SVM ensures that if a border is found, this is the one gives the largest gap between the data. </p>

<p>And now the tricky part: In the data you have outliers, errors, etc. basically data that is labeled as white but resides near the black group and vice versa. You have two choices: Move the border so you minimize the number of samples that will be learned wrong OR ignore few samples here and there but ensure that the border gives you large separation between classes. </p>

<p>The cost parameter C ""tunes"" the algorithm between better fitting the available data or giving a larger margin. If I'm not mistaken, small C means that you prefer larger margin.</p>

<p>Hope it helps!</p>

<p>p.s. I am not an expert in SVM so I can't give you the intuition on how exactly the values for global cost parameter C actually influences the results or the convergence speed of SVM.</p>
",2013-10-11 21:33:36.120
57334,594.0,2,,47981.0,,,,CC BY-SA 4.0,"<p>There are two issues here:</p>

<p>1) If you're doing a formal hypothesis test (and if you're going as far as quoting a p-value in my book you already are), <strong>what is the formal rejection rule?</strong></p>

<p>When comparing test statistics to critical values, the critical value is <em>in the rejection region</em>. While this formality doesn't matter much when everything is continuous, it does matter when the distribution of the test statistic is discrete.</p>

<p>Correspondingly, when comparing p-values and significance levels, the rule is:</p>

<blockquote>
  <p>Â Â Â Â Â Â Â Â Â  Reject if <span class=""math-container"">$p\leq\alpha$</span></p>
</blockquote>

<p>Please note that, even if you rounded your p-value up to 0.05, indeed even if the <span class=""math-container"">$p$</span> value was exactly 0.05, formally, <em>you should still reject</em>.</p>

<p>2) In terms of 'what is our p-value telling us', then assuming you can even interpret a p-value as 'evidence against the null' (let's say that opinion on that is somewhat divided), 0.0499 and 0.0501 are not really saying different things about the data (effect sizes would tend to be almost identical).</p>

<p>My suggestion would be to (1) formally reject the null, and perhaps point out that even if it were exactly 0.05 it should still be rejected; (2) note that there's nothing particularly <em>special</em> about <span class=""math-container"">$\alpha = 0.05$</span> and it's very close to that borderline -- even a slightly smaller significance threshold would not lead to rejection.</p>
",2013-10-11 21:33:38.517
57335,13396.0,2,,57328.0,,,,CC BY-SA 3.0,"<p>I think I got it.  If $Z \sim \mathcal{N}(0, 1)$ but we want to generate $X$ such that its mean is $\mu$ and covariance matrix is $C$, we decompose $C = L L^T$, and let $X = L Z + \mu$.</p>

<p>Now we just need to carry out the reverse operations.</p>
",2013-10-11 21:39:45.703
57336,22624.0,1,57337.0,,,How to test if a result is statistically significant?,<statistical-significance>,CC BY-SA 3.0,"<p>I am trying to determine if a certain conversion on my site is statistically significant.  I remembered doing this type of stuff in school but I can't seem to remember how to do it now.</p>

<p>For 1st set: n = 7297 and conversion was 2.618%
For 2nd set: n = 6107 and conversion was 2.669%</p>

<p>Any tips on how to do this?</p>
",2013-10-11 22:55:22.160
57337,2069.0,2,,57336.0,,,,CC BY-SA 3.0,"<p>Percents are a proportion. The traditional way to test differences between proportions is the chi-square test. Based on the information you have given me (7106 and 191 [2.62%] in one half and 5944 and 163 [2.67%] in the other), the chi-square test results in a non-significant value of .88 (p value). Your proportions are 2.67 and 2.62, so it is no surprise that these are not statistically significant, despite your large sample. </p>
",2013-10-11 23:20:23.397
57338,21746.0,1,,,,Rescaling input features for neural networks regression,<regression><machine-learning><neural-networks>,CC BY-SA 3.0,"<p>In Neural Nets for the regression problem, we rescale the continuous labels consistently with the output activation function, i.e. normalize them if the logistic sigmoid is used, or adjusted normalize them if tanh is used. At the end we can restore original range but renormalizing the output neurons back.</p>

<p>Should we also normalize input features? And how? For example, if hidden activation differs from the output activation? E.g. if hidden activation is TANH and output activation is LOGISTIC, should the input features be normalized to lie in [0,1] or [-1,1] interval?</p>
",2013-10-11 23:29:41.150
57341,13549.0,1,63675.0,,,Temporal autocorrelation in perMANOVA?,<time-series><multivariate-analysis><autocorrelation>,CC BY-SA 4.0,"<p>I have a data set where samples are collected once per year for 15 years at a number of sites. I am worried that these data are temporally autocorrelated and was trying to figure out if I need to address that. However, the only time I will be using degrees of freedom with these data is in a perMANOVA. This test calculates a pseudo F-statistic by permuting the rows. I can't figure out if the exchangebility assumption means that I don't need to worry about autocorrelation at all (i.e., permuting rows will simply destroy the temporal structure, which I am not interested in anyway) or if it means that I can't use a perMANOVA even if I accounted for autocorrelation?</p>

<p>Edit: I am editing this in the hopes that clarification will help get it answered. The perMANOVA user's guide says:</p>

<p><em>""Recall that for traditional one-way ANOVA, the assumptions are that the errors are independent, that they are normally distributed with a mean of zero and a common variance, and that the treatment effects are additive. In the case of a one-way analysis, the PERMANOVA test using permutations assumes only that the observation units are exchangeable under a true null hypothesis. There are no explicit assumptions regarding the distributions of the original variables; they are certainly not assumed to be normally distributed. However, implicit in the notion of exchangeability is the notion of independence, <strong>for if observations are correlated with one another (e.g., temporally or 
spatially), then randomly shuffling them will destroy this kind of inherent structure, if it is there.</strong> Thus, in general, we would assume that the observation units are independent of one another.""</em></p>

<p>The meaning of this is ambiguous to me for the reasons stated in the first paragraph. I can't find any techniques for testing/correcting autocorrelation with perMANOVA, which maybe means that it isn't a problem to worry about? </p>

<p>User's guide: <a href=""https://web.archive.org/web/20180806183841/https://pdfs.semanticscholar.org/4d0c/430f6129b427e48fb407e59ac79ee29b4cae.pdf"" rel=""nofollow noreferrer"">https://web.archive.org/web/20180806183841/https://pdfs.semanticscholar.org/4d0c/430f6129b427e48fb407e59ac79ee29b4cae.pdf</a></p>

<p>Original 2001 paper describing technique: <a href=""https://web.archive.org/web/20180806184058/https://pdfs.semanticscholar.org/038e/8869b676aa365f2afdea935edf3f2003324d.pdf"" rel=""nofollow noreferrer"">https://web.archive.org/web/20180806184058/https://pdfs.semanticscholar.org/038e/8869b676aa365f2afdea935edf3f2003324d.pdf</a></p>
",2013-10-12 02:33:38.043
57342,19681.0,2,,57325.0,,,,CC BY-SA 3.0,"<p>You seem to be aware that the marginal effect of $X_3$ is $\delta_1 + \delta_2 X_1 + \delta_3 X_2$, which is just the derivative of the response with respect to $X_3$.  </p>

<p>Replacing $X_1$ with $E(X_1)$ is a reasonable way to summarize the marginal effect.</p>

<p>However, discarding the final term due to statistical insignificance is nonsense.  There are at least two relatively sensible alternatives:</p>

<ol>
<li><p>If your $n$ is so big that you believe the statistical result that $\delta_3$ is insignificant more than you believe your prior belief that $\delta_3$ should be positive, than you could get rid of the $\delta_3 X_1 X_2$ term in your model and refit the coefficients BEFORE using $\delta_1 + \delta_2 X_1$ as your marginal effect.</p></li>
<li><p>If you believe that the terms involving $X_2$ need to be in the model, regardless of statistical significance, than you need to keep the $X_2$ term in your marginal effect as well.</p></li>
</ol>
",2013-10-12 03:15:41.107
57343,22629.0,1,57345.0,,,K-Means clustering after first iteration,<k-means>,CC BY-SA 3.0,"<p>In <em>k-means clustering</em> we initially pick $k$ random centroids and assign the given data to one of these $k$ centroids (which ever is nearest). After this we create new centroids by taking the mean of the assigned points. </p>

<p>However there might be case that the initially selected random centroids may not be nearest to any point in the dataset and hence no points would be assigned to these centroids. So in such case what should be done in the step of creating new centroids?</p>
",2013-10-12 05:40:58.913
57344,21243.0,2,,57323.0,,,,CC BY-SA 3.0,"<p>Actually, this isn't by most definitions a recommender system, and anything you read in the literature about recommender systems might be geared toward solving a similar but slightly different problem (namely, where the input and output space are the same set).</p>

<p>This is, by most conventional definitions, a classification problem, so looking for tags related to Classification might help.</p>

<p>In terms of actual approaches: as Placidia mentioned, CART and random forest methods are quite popular right now. Additionally, a classic method is <a href=""http://en.wikipedia.org/wiki/Logistic_regression"" rel=""nofollow"">Logistic Regression</a>, which might be worth checking at (I'm not an R user, but I believe R has an implementation of it provided).</p>

<p>More than that is tough to provide without a knowledge of what your system is doing and what approaches might be valid. I'd recommend refreshing your basic probability and statistics and thinking about the way that your variables might be related, then taking a look at the classification methods included with R (and their respective Wikipedia pages).</p>
",2013-10-12 06:34:05.823
57345,436.0,2,,57343.0,,,,CC BY-SA 3.0,"<p>I am not sure if there is a ""standard"" thing to do in the case one of the initial centroids is completely off. </p>

<p>You can easily test this by specifying the initial centroids and see how things evolve!</p>

<p>For instance, R will just give you an error.</p>

<p>Say you do:</p>

<pre><code># Set the RNG seed to ensure reproducibility
set.seed(12345)

# Let's create 3 visually distinct clusters
n &lt;- c(1000, 500, 850)
classifier.1 &lt;- c(rnorm(n[1], 10, 0.9), 
                  rnorm(n[2], 25, 2),
                  rnorm(n[3], 35, 2))
classifier.2 &lt;- c(rnorm(n[1], 5, 1),
                  rnorm(n[2], 10, 0.4),
                  rnorm(n[3], 2, .9))

col = c(""blue"", ""darkgreen"", ""darkred"")
# Run k-means with 3 clusters and random initial centroids 
# to check the clusters are correctly recognized
km &lt;- kmeans(cbind(classifier.1, classifier.2), 3)
# Plot the data, colored by cluster
plot(classifier.1, classifier.2, pch=20, col=col[km$cluster])

# Mark the final centroids
points(km$centers, pch=20, cex=2, col=""orange"")

# Now impose some obviously ""wrong"" starting centroids
start.x &lt;- c(10, 25, 3000)
start.y &lt;- c(10, 10, -10000)
km.2 &lt;- kmeans(cbind(classifier.1, classifier.2), 
               centers=cbind(start.x, start.y))
</code></pre>

<p>Now, R has obviously no issue in discriminating the 3 clusters when you let it choose the initial centroids, but when you run it the second time it will just say:</p>

<pre><code>Error: empty cluster: try a better set of initial centers
</code></pre>

<p>I guess that if you are implementing your own algorithm you may choose to use this behaviour or rather give the user a warning and let the algorithm choose the centroids by itself.</p>

<p>Obviously, as others pointed out, there are algorithms such as <a href=""http://en.wikipedia.org/wiki/K-means%2B%2B"" rel=""nofollow"">k-means++</a> that help in choosing a good set of starting centroids.</p>

<p>Also, in R you can use the <code>nstart</code> parameter of the kmeans function to run several iterations with different centroids: this will improve clustering in certain situations.</p>

<p><strong>EDIT</strong>: also, note from the R <code>kmeans</code> help page</p>

<blockquote>
  <p>The algorithm of Hartigan and Wong (1979) is used by default. Note
  that some authors use k-means to refer to a specific algorithm rather
  than the general method: most commonly the algorithm given by MacQueen
  (1967) but sometimes that given by Lloyd (1957) and Forgy (1965). The
  Hartiganâ€“Wong algorithm generally does a better job than either of
  those, but trying several random starts (nstart> 1) is often
  recommended. For ease of programmatic exploration, k=1 is allowed,
  notably returning the center and withinss.</p>
  
  <p>Except for the Lloydâ€“Forgy method, k clusters will always be returned
  if a number is specified. If an initial matrix of centres is supplied,
  it is possible that no point will be closest to one or more centres,
  which is currently an error for the Hartiganâ€“Wong method.</p>
</blockquote>
",2013-10-12 07:23:28.157
57346,21586.0,2,,47981.0,,,,CC BY-SA 3.0,"<p><strong>It lies in the eye of the beholder.</strong></p>

<p>Formally, if there is a strict decision rule for your problem, follow it. This means $\alpha$ is given. However, I am not aware of any problem where this is the case (though setting $\alpha=0.05$ is what many practitioners do after Statistics101). </p>

<p><strong>So it really boils down to what AlefSin commented before. There cannot be a ""correct answer"" to your question. Report what you got, rounded or not.</strong></p>

<p>There is a huge literature on the ""significance of significance""; see for example the recent paper of one of the leading German statisticians Walter KrÃ¤mer on ""The cult of statistical significance - What economists should and should not do to make their data talk"", <em>Schmollers Jahrbuch</em> <strong>131</strong>, 455-468, 2011.</p>
",2013-10-12 07:43:00.677
57347,6162.0,2,,10911.0,,,,CC BY-SA 3.0,"<p>There is a natural exact confidence interval for the grandmean in the balanced random one-way ANOVA model $$(y_{ij} \mid \mu_i) \sim_{\text{iid}} {\cal N}(\mu_i, \sigma^2_w), \quad j=1,\ldots,J, 
\qquad 
\mu_i \sim_{\text{iid}} {\cal N}(\mu, \sigma^2_b), \quad i=1,\ldots,I.$$
Indeed, it is easy to check that the distribution of the observed means $\bar{y}_{i\bullet}$ is $\bar{y}_{i\bullet} \sim_{\text{iid}} {\cal N}(\mu, \tau^2)$ with $\tau^2=\sigma^2_b+\frac{\sigma^2_w}{J}$, 
and it is well known that the between sum of squares $SS_b$ has distribution $$SS_b \sim J\tau^2\chi^2_{I-1}$$ and is independent of the overall observed mean $$\bar y_{\bullet\bullet} \sim {\cal N}(\mu, \frac{\tau^2}{I})$$. 
Thus $$\frac{\bar y_{\bullet\bullet}  - \mu}{\frac{1}{\sqrt{I}}\sqrt{\frac{SS_b}{J(I-1)}}}$$ has a Student $t$ distribution with $I-1$ degrees of freedom, wherefrom it is easy to get an exact confidence interval about $\mu$.</p>

<p><strong>Note that this confidence interval is nothing but the classical interval for a Gaussian mean by considering only the group means $\bar{y}_{i\bullet}$ as the observations</strong>. 
Thus the simple approach you mention:</p>

<blockquote>
  <p>The simple approach is to first compute the mean of each experiment:
  38.0, 49.3, and 31.7, and then compute the mean, and its 95% confidence interval, of those three values. Using this method, the
  grand mean is 39.7 with the 95% confidence interval ranging from 17.4
  to 61.9.</p>
</blockquote>

<p>is right. And your intuition about the ignored variation:</p>

<blockquote>
  <p>The problem with that approach is that it totally ignores the
  variation among triplicates. I wonder if there isn't a good way to
  account for that variation.</p>
</blockquote>

<p>is wrong. I also mention the correctness of such a simplification in <a href=""https://stats.stackexchange.com/a/72578/8402"">https://stats.stackexchange.com/a/72578/8402</a></p>

<h2>Update 12/04/2014</h2>

<p>Some details are now written on my blog: <a href=""http://stla.github.io/stlapblog/posts/ModelReduction.html"" rel=""nofollow noreferrer"">Reducing a model to get confidence intervals</a>.</p>
",2013-10-12 10:02:03.747
57348,6136.0,1,57353.0,,,KL-divergence between two categorical/multinomial distributions gives negative values?,<information-theory><kullback-leibler>,CC BY-SA 3.0,"<p>If </p>

<p>$$P = [0,0.9,0,0.1]$$</p>

<p>$$Q = [0,1,0,0]$$</p>

<p>Then $$KL(P||Q) = 0 + \ln(0.9/1)\cdot0.9 + 0 + 0 = -0.094$$ </p>

<p>This shouldn't be possible from the Gibbs inequality. What am I misunderstanding? </p>
",2013-10-12 10:23:30.670
57417,22425.0,1,,,,Independent but not identically distributed,<probability><distributions><mathematical-statistics><iid>,CC BY-SA 3.0,"<p>Let $X_1, X_2,\ldots ,X_n$ be discrete random variables. </p>

<p>I'm looking for a way to prove the random variables are independent but not identically distributed.</p>

<p>Can anyone suggest some ideas ?</p>
",2013-10-14 03:02:05.353
57351,20473.0,2,,57350.0,,,,CC BY-SA 4.0,"<p>In an expression where more than one random variables are involved, the symbol <span class=""math-container"">$E$</span> alone does not clarify <em>with respect to which random variable</em> is the expected value ""taken"". For example</p>

<p><span class=""math-container"">$$E[h(X,Y)] =\text{?} \int_{-\infty}^{\infty} h(x,y) f_X(x)\,dx$$</span>
or
<span class=""math-container"">$$E[h(X,Y)] = \text{?} \int_{-\infty}^\infty h(x,y) f_Y(y)\,dy$$</span></p>

<p><em>Neither</em>. When many random variables are involved, and there is no subscript in the <span class=""math-container"">$E$</span> symbol, the expected value is taken with respect to their joint distribution:</p>

<p><span class=""math-container"">$$E[h(X,Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty h(x,y) f_{XY}(x,y) \, dx \, dy$$</span></p>

<p>When a subscript is present... in some cases it tells us <em>on which variable we should condition</em>. So</p>

<p><span class=""math-container"">$$E_X[h(X,Y)] = E[h(X,Y)\mid X] = \int_{-\infty}^\infty h(x,y) f_{h(X,Y)\mid X}(h(x,y)\mid x)\,dy  $$</span></p>

<p>Here, we ""integrate out"" the <span class=""math-container"">$Y$</span> variable, and we are left with a function of <span class=""math-container"">$X$</span>.</p>

<p>...But in other cases, it tells us which <em>marginal</em> density to use for the ""averaging""</p>

<p><span class=""math-container"">$$E_X[h(X,Y)] = \int_{-\infty}^\infty h(x,y) f_{X}(x) \, dx $$</span> </p>

<p>Here, we ""average over"" the <span class=""math-container"">$X$</span> variable, and we are left with a function of <span class=""math-container"">$Y$</span>.</p>

<p>Rather confusing I would say, but who said that scientific notation is totally free of ambiguity or multiple use? You should look how each author defines the use of such symbols.</p>
",2013-10-12 11:56:27.427
57352,15827.0,2,,57349.0,,,,CC BY-SA 3.0,"<p>Each image is composed of 32 $\times$ 32 pixels, so for a given pixel (say row 13, column 31) something measured is averaged over all the images, and the standard deviation (SD for short) for the same something is also calculated. </p>

<p>(value âˆ’ mean) / SD is often called a z-score and is a way of standardizing values to take account of mean and SD. Presumably that's done for every pixel, meaning every pixel position. </p>

<p>It is spelled out that they are ""dividing by the standard deviation of <em>all</em> pixels over <em>all</em> images"" [my emphasis] and that SD would usually be calculated with reference to the corresponding overall mean. However, division by that SD would be dividing by a constant, so it won't have any effect on the images beyond a question of units.</p>
",2013-10-12 12:16:10.913
57353,5875.0,2,,57348.0,,,,CC BY-SA 3.0,"<p>Letâ€™s remove the two categories with probability $0$ in both distributions. Your example is $P = (0.9, 0.1)$ and $Q = (1,0)$. </p>

<p>The KL divergence is $KL(P||Q) = \sum_i p_i \log\left( {p_i \over q_i }\right)$. It is not 
$$ 0.9 \times \log\, 0.9 + 0 $$
but 
$$ 0.9 \times \log\, 0.9 + 0.1 \times ( +\infty ) = + \infty.$$</p>
",2013-10-12 12:19:43.223
57354,8671.0,1,57356.0,,,Computing mutual information,<machine-learning><clustering><computational-statistics><mutual-information>,CC BY-SA 3.0,"<p>I have a problem when computing the mutual information between two variables.
Let's consider the following table:</p>

<pre><code>             k=1       k=2       k=3    
c = 1         10        20         5    
c = 2          5         3        20
</code></pre>

<p>I want to calculate the mutual information between the class ($c$) and the cluster ($k$).</p>

<p>E.g., for $\text{MI}(c=1, k=1)$, I will calculate the $P(c,k)$ as follows:</p>

<pre><code>             k=1            k~=1    
c=1        10/63           25/63    
c~=1        5/63           23/63
</code></pre>

<p>and I can calculate the MI based on the following:</p>

<p>$\text{MI}(c,k) = \sum_{c\in\{0,1\}} \sum_{k\in\{0,1\}} p(c,k)log_2 \frac{p(c,k)}{p(c)p(k)}$ </p>

<p>In this case: I am getting the same MI for $(c=1, k=1)$ and $(c=2, k=1)$. What is the reason for this? How can I calculate the MI for each $(c,k)$ based on the first table?</p>
",2013-10-12 13:58:25.187
57355,20927.0,1,,,,SPSS-independent-sample-t-test,<bonferroni>,CC BY-SA 3.0,"<p>I want to test mean differences between 4(0,1,2,3) groups for 6 different cell-types. In other words, I would have 6 comparisons to do for each cell-type (0-1,0-2,0-3,1-2,1-3,2-3). I have run a repeated-measures ANOVA before but it only gives me the main effects and interactions. To correct my t-tests for multiple comparisons, I would use Bonferroni, does SPSS let me do that, I just cannot seem to find it except as part of the means-testing using ANOVA? Or do I have to do it manually and how would i do that? alpha-level here is 0.05.Thanks</p>
",2013-10-12 14:02:41.167
57356,6630.0,2,,57354.0,,,,CC BY-SA 3.0,"<p>MI is between two random variables. In this case, $c$ and $k$ are random variables.</p>

<p>If you take $c=1$, it isn't really random anymore, and MI should be zero. Likewise for $k=1$.</p>

<p>(Also, one more thing to keep in mind is that your 'plug-in' estimation of MI is biased.)</p>

<p>EDIT: MI between $I(c=1)$ vs $I(k=1)$ where $I$ is the indicator function, is a different matter. Your double usage of the same variable confused me.</p>

<p>Now your $c$ only take 2 values, therefore, $I(c=1) = 1 - I(c=2)$. MI is invariant invertible transformation of variables, that's why $MI(I(c=1),I(k=1)) = MI(I(c=2),I(k=1))$.</p>
",2013-10-12 14:05:53.523
57357,14850.0,2,,56911.0,,,,CC BY-SA 3.0,"<p>A dependent mixture model (hidden Markov model) may be of use, depending on the type of deviations expected. </p>

<p>Assume that your observations come from two distributions (or states), both of which are normally distributed, but have different mean and variance.</p>

<p>A number of parameters can be estimated: The initial state probabilities (2 parameters), the state transition probabilities between neighbouring data points (4 parameters) and finally the mean and variance of the two distributions (4 parameters).</p>

<p>In R, this model can be estimated using the depmixS4 package:</p>

<pre><code>library(depmixS4)

set.seed(3)
y = rnorm(100)
y[30:35] &lt;- rnorm(6,mean=4,sd=2)
plot(1:100,y,""l"")

m &lt;- depmix(y~1,nstates=2,ntimes=100)
fm &lt;- fit(m)

means &lt;- getpars(fm)[c(7,9)]
lines(1:100,means[fm@posterior$state],lwd=2,col=2)
</code></pre>

<p><img src=""https://i.stack.imgur.com/HTvOv.png"" alt=""enter image description here""></p>

<p>See <a href=""http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf"" rel=""noreferrer"">http://cran.r-project.org/web/packages/depmixS4/vignettes/depmixS4.pdf</a> for references</p>
",2013-10-12 15:57:43.367
57358,22637.0,1,57364.0,,,Transformations(CDF technique),<probability>,CC BY-SA 3.0,"<p>Consider the following short example of transformations.</p>

<p>Let the joint density of X and Y be given by the unit square, i.e.</p>

<p>$$f_{X,Y}(x,y) = \begin{cases} 1\ \quad 0&lt;x&lt;1\ \text{ and }\ 0&lt;y&lt;1 \\ 0 \quad \text{elsewhere} \end{cases}$$</p>

<p>Then the Cumulative Distribution Function of $Z=X+Y$ is given by:</p>

<p>$$
F_Z = \begin{cases}\begin{array}{ll} 0\ &amp; \text{ for }\ z&lt;0 \\  \int_0^{z} \int_0^{z-x} dy\,dx\ &amp; \text{ for }\ 0\leq z &lt;1 \\1-\int_{z-1}^1 \int_{z-x}^1 dy\,dx\ &amp; \text{ for }\ 1\leq z&lt;2 \\1\ &amp; \text{ for }\ 2\leq{z} \end{array}\end{cases}
$$</p>

<p>I understand why we have to partition our CDF, what I am having trouble figuring out is why for the interval $[1,2)$ that specific form. What is the intuition here? Thanks.</p>
",2013-10-12 16:48:18.213
57359,346.0,1,57687.0,,,How to include a linear and quadratic term when also including interaction with those variables?,<centering><quadratic-form>,CC BY-SA 3.0,"<p>When adding a numeric predictor with categorical predictors and their interactions, it is usually considered necessary to center the variables at 0 beforehand. The reasoning is that the main effects are otherwise hard to interpret as they are evaluated with the numeric predictor at 0.</p>

<p>My question now is how to center if one not only includes the original numeric variable (as a linear term) but also the quadratic term of this variable? Here, two different approaches are necessary:</p>

<ol>
<li><strong>Centering both variables at their individual mean.</strong> This has the unfortunate downside that the 0 now is at a different position for both variables considering the original variable.</li>
<li><strong>Centering both variables at the mean of the original variable</strong> (i.e., subtracting the mean from the original variable for the linear term and subtracting the square of the mean of the original variable from the quadratic term). With this approach the 0 would represent the same value of the original variable, but the quadratic variable would not be centered at 0 (i.e., the mean of the variable wouldn't be 0).</li>
</ol>

<p>I think that approach 2 seems reasonable given the reason for centering after all.  However, I cannot find anything about it (also not in the related questions: <a href=""https://stats.stackexchange.com/q/67512/442"">a</a> and <a href=""https://stats.stackexchange.com/q/47178/442"">b</a>).</p>

<p>Or is it generally a bad idea to include linear and quadratic terms and their interactions with other variables in a model?</p>
",2013-10-12 17:07:43.177
57360,22381.0,1,,,,Does applying ARMA-GARCH require stationarity?,<time-series><arima><stationarity><garch><finance>,CC BY-SA 3.0,"<p>I am going to use the ARMA-GARCH model for financial time series and was wondering whether the series should be stationary before applying the said model. 
I know to apply ARMA model the series should be stationary, however I'm not sure for ARMA-GARCH since I'm including GARCH errors which imply volatility clustering and non-constant variance and hence non-stationary series no matter what transformation I do.</p>

<p>Are financial time series usually stationary or non-stationary?
I tried applying ADF test to a few volatile series and got p-value&lt;0.01 which seems to indicate stationarity but the principle of volatile series itself tells us that the series isn't stationary.</p>

<p>Can somebody clear that up for me?I'm getting really confused</p>
",2013-10-12 17:14:19.543
57361,20473.0,2,,57360.0,,,,CC BY-SA 3.0,"<p>Copying from the abstract of <a href=""http://www.jstor.org/stable/1912773"">Engle's original paper</a>:<br>
""These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance"".</p>

<p>Continuing with the references, as the author who introduced GARCH shows (Bollerslev, Tim (1986). ""<a href=""http://www.sciencedirect.com/science/article/pii/0304407686900631#"">Generalized Autoregressive Conditional Heteroskedasticity</a>"", Journal of Econometrics, 31:307-327)
for the GARCH(1,1) process, it suffices that $\alpha_1 + \beta_1 &lt;1$ for 2nd-order stationarity.</p>

<p>Stationarity (the one needed for estimation procedures), is defined relative to the <em>unconditional</em> distribution and moments.  </p>

<p><strong>ADDENDUM</strong><br>
To summarize here discussion in the comments, the GARCH modeling approach is an ingenious way to model suspected heteroskedasticity over time, i.e. of some form of <em>heterogeneity</em> of the process (which would render the process non-stationary) as an observed feature that comes from the existence of <em>memory</em> of the process, in essence <em>inducing</em> stationarity at the unconditional level.  </p>

<p>In other words, we took our two ""great opponents"" in stochastic process analysis (heterogeneity and memory), and used the one to neutralize the other -and this is indeed an inspired strategy.</p>
",2013-10-12 17:42:59.303
57362,1406.0,2,,57360.0,,,,CC BY-SA 3.0,"<p>Yes the the series should be stationary. GARCH models are actually white noise processes with not trivial dependence structure. Classical GARCH(1,1) model is defined as</p>

<p>$$r_t=\sigma_t\varepsilon_t,$$</p>

<p>with </p>

<p>$$\sigma_t^2=\alpha_0+\alpha_1\varepsilon_{t-1}^2+\beta_1\sigma_{t-1}^2,$$</p>

<p>where $\varepsilon_t$ are independent standard normal variables with unit variance.</p>

<p>Then</p>

<p>$$Er_t=EE(r_t|\varepsilon_{t-1},\varepsilon_{t-2},...)=E\sigma_tE(\varepsilon_t|\varepsilon_{t-1},\varepsilon_{t-2},...)=0$$</p>

<p>and</p>

<p>$$Er_tr_{t-h}=EE(r_tr_{t-h}|\varepsilon_{t-1},\varepsilon_{t-2},...)=Er_{t-h}\sigma_{t}E(\varepsilon_t|\varepsilon_{t-1},\varepsilon_{t-2},...)=0$$</p>

<p>for $h&gt;0$. Hence $r_t$ is a white noise process. However it is possible to show that $r_t^2$ is actually a $ARMA(1,1)$ process. So GARCH(1,1) is stationary process, yet has non-constant conditional variance.</p>
",2013-10-12 17:56:44.900
57363,22639.0,1,57506.0,,,Machine learning algorithms/approaches for class recommendations?,<machine-learning><clustering><algorithms><artificial-intelligence>,CC BY-SA 3.0,"<p>I am asking a theoretical question about machine learning in terms of clustering. Is it possible, given a set of data of classes that students have taken in a semester to recommend additional classes that students should take if they selected some classes?</p>

<p>I am thinking along the line of forming clusters of classes and figuring out if a particular set of picked classes match with a pre-existing set of classes. Then, recommend the class that are in the set. But I am new to machine learning, and so welcome any other suggestions of algorithms.</p>

<p>In addition, this is not particularly theoretical, so feel free to ignore: but does anyone know any particular software that can accomplish this? I know LensKit is a software to handle recommendations but it seems to need ratings (which I do not have).</p>

<p>I welcome any mathematical manipulations that can turn clusters into ""ratings."" Thanks.</p>
",2013-10-12 18:03:28.603
57364,10135.0,2,,57358.0,,,,CC BY-SA 3.0,"<p>OK, check out the following plot. <img src=""https://i.stack.imgur.com/OfF8D.jpg"" alt=""enter image description here""></p>

<p>You need to find the area of shaded region. So you need to take double integration over the shaded region. First you fix your $X$ and take your integration with respect to $Y$. Look at the double bar in the middle of that triangle (upper right corner). Its lower part goes from $Y=-X+z$ to its upper part $Y=1$. These are the bounds for the first integration. Now you need to move that little bar in the middle to left and right to cover all the shaded region. In other words, it means that this time, you need to take your integration with respect to $X$. So as you can see, the line $Y=-X+z$ for $1\leq z&lt;2$ intercepts the line $Y=1$ at $X=z-1$. This is the left boundary limit for your integration. Now move that little bar to the right, it should go up to $X=1$, that gives you the upper bound. Hope that helps.</p>
",2013-10-12 18:10:09.217
57469,22507.0,2,,57467.0,,,,CC BY-SA 3.0,"<p>You cannot. ""Accept that $b_1=0$"" is the same as ""reject that $b_1\ne 0$"".  But on what basis you could do this?  No matter how many observations you have, you cannot distinguish between 0 and sufficiently small value of $b_1$.  You can only accept that $|b_1|&lt;\epsilon$  (the smaller $\epsilon$ the more observations you need).</p>
",2013-10-14 22:07:54.383
57365,22640.0,1,,,,"Drawing data from ""population"" for regression analysis",<regression>,CC BY-SA 3.0,"<p>We have a response variable $Y$ and predictor $X$, and we draw $n$ samples $(Y_1,X_1), \ldots, (Y_n, X_n)$ from the population of interest to do a regression analysis. Under the assumptions of a simple linear regression model, my question is a conceptual one: how do we really think about the response on the $i$th unit, $Y_i$? Do we say it's drawn from the level or subpopulation of individuals with $ X = x_i $, or from the aggregate population over all the values of $X$? Moreover, while we assume that the response $Y$ in every subpopulation defined by $X$ is normal with equal variances, how do we think about the aggregate population from which $Y_i$ is drawn?  </p>
",2013-10-12 18:36:45.027
57366,306.0,2,,57360.0,,,,CC BY-SA 3.0,"<p>Stationarity is a theoretical concept which is then modified to other forms like Weak Sense Stationarity which can be tested easily. Most of the tests like adf test as you have mentioned test for linear conditions only. the ARCH effects are made for series which do not have autocorrelation in the first order but there is dependence in the squared series. </p>

<p>The ARMA-GARCH process you talk about, here the second order dependence is removed using the GARCH part and then any dependence in the linear terms is captured by the ARMA process. </p>

<p>The way to go about is to check for the autocorrelation of the squared series, if there is dependence, then apply the GARCH models and check the residuals for any linear time series properties which can then be modelled using ARMA processes.</p>
",2013-10-12 18:41:16.300
57367,22641.0,1,,,,"Test for differences between (among) related, but not matched, samples",<hypothesis-testing><modeling>,CC BY-SA 3.0,"<p>When two samples are related, or dependent, but the observations are not matched, are there any tests that will determine if the samples (means or otherwise) are different? I've searched extensively and have only found tests for matched samples, which is not what I need.</p>
",2013-10-12 18:47:43.087
57368,21840.0,1,,,,Compute probability,<self-study><density-function><joint-distribution>,CC BY-SA 3.0,"<p>Suppose that $X$, $Y$ and $Z$ are $\text{i.i.d.} \sim \text{Uniform}(0,1)$. Let $t &gt; 0$ be a ï¬xed constant.</p>

<p>(i) Compute $P(X/Y \leq t)$<br>
(ii) Compute $ P(XY \leq t)$<br>
(iii) Compute $ P(XY/Z \leq t)$   </p>

<p>I found the solution for (i) part undermining the different values of $t$.<br>
$ P(X/Y \leq t) = \int_0^1\int_0^{yt}dx dy  = t/2 $ ; when $t\leq 1$</p>

<p>I am finding hard time to lake limits for different values of $t$ i.e when $t\leq 1 $ and $t&gt;1$</p>
",2013-10-12 19:10:33.277
57369,22507.0,2,,57363.0,,,,CC BY-SA 3.0,"<p>Clustering is seldom, if ever, used for recommendations, since it is too crude.  The most common techniques used are:</p>

<ul>
<li>matrix factorization; read, for example, ""Matrix Factorization Techniques for Recommender Systems"" by Koren, Bell, and Volinsky.  If you use R, there is are packages NMFN and gnmf for non-negative matrix factorization.  In your case, this will be the matrix of 0's and 1's.  There are many modifications and versions of this technique.</li>
<li>KNN. For each class, find classes highly correlative with it. Then predict the probability for this class as a linear regression (or, in your case, logistic regression) of the correlative classes, with relaxation.</li>
<li>Restricted Boltzmann Machines. This is relatively hard to understand or implement.  Read, for example, ""Restricted Boltzmann Machines for Collaborative Filtering"" by Salakhutdinov, Mnih, and Hinton.  There are no Restricted Boltzmann Machine packages on R.</li>
<li>Often, a combination of different approaches (blending) is used, providing better results than each one separately. For example, Netflix uses a blending of Matrix Factorization and Restricted Boltzmann Machines.</li>
</ul>
",2013-10-12 19:31:39.603
57370,22507.0,2,,15542.0,,,,CC BY-SA 3.0,"<p>I recommend ""The Elements of Statistical Learning"", by Hastie, Tibshirani, and Friedman. Don't just read it, play with some algorithms described by them (most of them are implemented in R, or you could even implement some yourself), and learn their weak and strong points.</p>
",2013-10-12 19:39:43.643
57371,22507.0,2,,45804.0,,,,CC BY-SA 3.0,"<p>Calculate a correlation of two functions over a set of random examples. The two-sided Kolmogorov-Smirnov test compares one-dimensional distributions, not multidimensional functions.</p>
",2013-10-12 20:15:29.000
57372,21947.0,2,,47981.0,,,,CC BY-SA 3.0,"<p>The answer is <strong>absolutely not</strong>. There is no ""in the eye of the beholder"", there is no argument, the answer is <strong>no, your data is not significant at the $p=0.05$ level</strong>.  (Ok, there is one way out, but its a very narrow path.)</p>

<p>The key problem is this phrase: ""We <strong>came across</strong> some data..."". </p>

<p>This suggests that you looked at several other statistical hypothesis, and rejected them because they did not reach your significance level. You found one hypothesis that (barely) met your standard, and you are wondering whether it is significant. Unless your $p$ value accounts for such multiple hypothesis testing, it is overly optimistic.  Given that you are just three decimal points away from your threshold, considering even <em>one</em> additional hypothesis would surely push $p$ over the line.  </p>

<p>There is a name for this sort of statistical malfeasance: <a href=""https://en.wikipedia.org/wiki/Data_dredging"" rel=""nofollow"">data dredging</a>.  I'm ambivalent about reporting it in the paper as an interesting hypothesis; does it have some physical reason you expect it to hold?</p>

<p><strong>There is, however, one way out.</strong> Perhaps you decided <em>a priori</em> to perform just this <em>one</em> test on just this <em>one</em> data set. You wrote that down in your lab notebook, in front of someone so that you could prove it later. Then you did your test. </p>

<p>If you did this, then your result is valid at the $p=0.05$ level, and you can back it up to skeptics like me. Otherwise, sorry, it is not a statistically significant result.</p>
",2013-10-12 20:45:05.873
57373,17459.0,1,,,,what's the pdf and covariance for this distribution?,<covariance><density-function><covariance-matrix>,CC BY-SA 3.0,"<p>I am stuck on a problem and wonder if anyone can give me some suggestions.</p>

<p>$X_1, X_2, X_3$ all follow a $\text{Uniform}[0,1]$ distribution and are subject to the constraint $X_1+X_2+X_3\leq 1$.</p>

<p>What's the joint distribution for $(X_1, X_2, X_3)$, that is, what's $p(X_1, X_2, X_3)$, and what's the variance-covariance matrix for it?</p>

<p>I get the joint distribution by geometrical way, that the pdf should be $1/6$.</p>

<p>However, I can't calculate the variance-covariance matrix for it. I wonder how to get it?</p>
",2013-10-13 03:46:30.167
57374,17123.0,1,,,,How to estimate the confidence interval using sample average and sample size ONLY?,<confidence-interval><estimation>,CC BY-SA 3.0,"<p>Suppose there is a population, with goods and bads. The bad rate of the population(=bads/(bads+goods)) is of course unknown. </p>

<p>Now, I have a sample of $N$ from the population and I know the bad rate of this sample as $b$. The question is can I calculate the confidence interval based on $N$ and $b$ ONLY? In other words, can I calculate the confidence interval $x$ such that with, say, 95% confidence the population bad rate falls in $[\text{range}_1,\text{range}_2]$, where $\text{range}_1$ will be $b-x$ and $\text{range}_2$ will be $b+x$.</p>
",2013-10-13 04:36:36.017
57375,,2,,57374.0,anon,,,CC BY-SA 3.0,"<p>Exactly as Glen_b said. Under random sampling the confidence interval for a binomial proportion can be easily calculated with, e.g., using the normal approximation. The formula can be found from Wikipedia, among other sources (<a href=""http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"" rel=""nofollow"">http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval</a>).</p>

<p>As an example, the 95% confidence interval for a sample of 1000 with the proportion of bad of 0.5, the confidence interval would be from 0.5-sqrt((1/1000)*0.5*0.5)*1.96 to 0.5+sqrt((1/1000)*0.5*0.5)*1.96. In other words, in this case the 95% confidence interval would be 0.469-0.530.</p>
",2013-10-13 06:29:18.963
57376,19455.0,2,,57326.0,,,,CC BY-SA 3.0,"<p>I would suggest a different approach. Instead of sweeping across all possible classifiers,
stop and think about your problem. How does your feature space look like? For the case of binary classification, are there two large clusters with some boundary, or is your feature space ""segmented"" and contains many clusters? </p>

<p>In the former case, an SVM would be a good choice to separate the two clusters (with the right choice of kernel), in the latter a decision tree which splits the feature space into areas would probably be a better choice. Another issue is interpretability, do you need some sort of report or methodology for classification, or simply a prediction result? Decision tree can provide you with a methodology you can follow, enabling you to debug and check if you are overfitting. From my personal experience, understanding your dataset is at least as important as the choice of algorithm.</p>
",2013-10-13 07:01:11.173
57377,12756.0,1,57387.0,,,Usage of Linear optimization model,<self-study><optimization><model>,CC BY-SA 3.0,"<p>Young Energy operates a power plant. The power plant is a coal-fired boiler that produces steam which in turn drives a generator. The company can buy different types of coal, and then mix them to meet the demands placed on it which is fired in the boiler. The table shows the characteristics of the different types of coal are:</p>

<p><img src=""https://i.stack.imgur.com/Jg23l.png"" alt=""enter image description here""></p>

<p>The requirement to be burned in the pan is: </p>

<ul>
<li>BTU/lb: 11900,   </li>
<li>content of the ashes max 12,2% and </li>
<li>max moisture 9,4%.</li>
</ul>

<blockquote>
  <p>How should I implement a linear optimization model in this context?</p>
</blockquote>
",2013-10-13 07:04:53.763
57378,5001.0,1,57389.0,,,Interpretation of the p-value of the y-intercept coefficient in a linear regression,<regression><p-value>,CC BY-SA 3.0,"<p>I am trying to interpret one of the p-values in a one variable linear regression.  Some of the answers I've seen for similar questions were not worded as thoroughly as I would have liked.  My interpretation is deliberately verbose because it will aid my understanding if faults are found within it.</p>

<p>From Microsoft Excel the linear regression formula from 90 samples of (x,y) pairs is </p>

<p><code>y = 0.514x + 0.00087</code> </p>

<p>and the p-value of the first coefficient is 4e-16 (scientific notation) and for the second it is 0.0027.</p>

<p>Would it be correct to say that the interpretation of the p-value of the 0.00087 term is:</p>

<blockquote>
  <p>Under the assumption that the true value of the y-intercept is zero
  and the first coefficient is 0.514, random sampling of the same number
  of (x,y) pairs, specifically 90, would result in a least squares best
  fit line with a y-intercept at least as extreme as 0.00087, with a
  probability of 0.0027.</p>
</blockquote>

<p>If not, then what would be the correct interpretation?</p>

<p>Not so importantly, but just to be complete, I am also inquiring if it would be more accurate and complete to put the relevant phrase as </p>

<blockquote>
  <p>""at least as extreme as 0.00087 in the same direction, that is,
  positive"".</p>
</blockquote>

<p><strong>Edit</strong>: The Excel funcion is <code>Tools &gt; Data Analysis &gt; Regression</code> in Office 2003 with service pack 2. Excel regression p-values on coefficients are 2 sided.</p>

<p><strong>Edit</strong>: Regarding differentiation from this question <a href=""https://stats.stackexchange.com/questions/31/what-is-the-meaning-of-p-values-and-t-values-in-statistical-tests"">here</a>: The most up voted answer there discusses the p-value of a hypothesis, which seems ill defined or at least not specific.  I am not interested in that.  I am interested in the p-value of a coefficient that is not the coefficient of an independent variable. I am being very specific.</p>
",2013-10-13 08:09:51.043
57379,22646.0,1,,,,Standard error of the residuals for a non-linear model,<standard-error><nonlinear-regression>,CC BY-SA 3.0,"<p>Hi I am new to R and statistics and used to linear models. Can you please explain the output? I used it to make a growth curve.</p>

<pre><code>Formula: length ~ a * (1 - exp(-c * est_age))

Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)    
a 1.097e+03  1.026e+01 106.966  &lt; 2e-16 ***
c 1.539e-01  1.982e-02   7.765 2.33e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 41.74 on 38 degrees of freedom
Number of iterations to convergence: 6 
Achieved convergence tolerance: 7.32e-07 
</code></pre>
",2013-10-13 08:15:40.013
57380,22648.0,1,,,,Negative values in predictions for an always-positive response variable in linear regression,<regression><predictive-models>,CC BY-SA 3.0,"<p>I'm trying to predict a response variable in linear regression that should be always positive (cost per click). It's a monetary amount. In adwords, you pay google for clicks on your ads, and a negative number would mean that google pays you when people clicked :P</p>

<p>The predictors are all continuous values. The Rsquared and RMSE are decent when compared to other models, even out-of-sample:</p>

<pre><code>  RMSE        Rsquared 
1.4141477     0.8207303
</code></pre>

<p>I cannot rescale the predictions, because it's money, so even a small rescaling factor could change costs significantly.</p>

<p>As far as I understand, for the regression model there's nothing special about zero and negative numbers, so it finds the best regression hyperplane no matter whether the output is partly negative.</p>

<p>This is a very first attempt, using all variables I have. So there's room for refinement.</p>

<p>Is there any way to tell the model that the output cannot be negative?</p>
",2013-10-13 09:41:29.343
57381,8074.0,2,,57195.0,,,,CC BY-SA 3.0,"<p>I think there are a few options for showing this type of data:</p>

<p>The first option would be to conduct an ""Empirical Orthogonal Functions Analysis"" (EOF) (also referred to as ""Principal Component Analysis"" (PCA) in non-climate circles). For your case, this should be conducted on a correlation matrix of your data locations. For example, your data matrix <code>dat</code> would be your spatial locations in the column dimension, and the measured parameter in the rows; So, your data matrix will consist of time series for each location. The <code>prcomp()</code> function will allow you to obtain the principal components, or dominant modes of correlation, relating to this field:</p>

<pre><code>res &lt;- prcomp(dat, retx = TRUE, center = TRUE, scale = TRUE) # center and scale should be ""TRUE"" for an analysis of dominant correlation modes)
#res$x and res$rotation will contain the PC modes in the temporal and spatial dimension, respectively.
</code></pre>

<p>The second option would be to create maps that show correlation relative to an individual location of interest:</p>

<pre><code>C &lt;- cor(dat)
#C[,n] would be the correlation values between the nth location (e.g. dat[,n]) and all other locations. 
</code></pre>

<h3>EDIT: additional example</h3>

<p>While the following example doesn't use gappy data, you could apply the same analysis to a data field following interpolation with DINEOF (<a href=""http://menugget.blogspot.de/2012/10/dineof-data-interpolating-empirical.html"" rel=""nofollow noreferrer"">http://menugget.blogspot.de/2012/10/dineof-data-interpolating-empirical.html</a>). The example below uses a subset of monthly anomaly sea level pressure data from the following data set (<a href=""http://www.esrl.noaa.gov/psd/gcos_wgsp/Gridded/data.hadslp2.html"" rel=""nofollow noreferrer"">http://www.esrl.noaa.gov/psd/gcos_wgsp/Gridded/data.hadslp2.html</a>):</p>

<pre><code>library(sinkr) # https://github.com/marchtaylor/sinkr

# load data
data(slp)

grd &lt;- slp$grid
time &lt;- slp$date
field &lt;- slp$field

# make anomaly dataset
slp.anom &lt;- fieldAnomaly(field, time)

# EOF/PCA of SLP anom
P &lt;- prcomp(slp.anom, center = TRUE, scale. = TRUE)

expl.var &lt;- P$sdev^2 / sum(P$sdev^2) # explained variance
cum.expl.var &lt;- cumsum(expl.var) # cumulative explained variance
plot(cum.expl.var)
</code></pre>

<h3>Map the leading EOF mode</h3>

<pre><code># make interpolation
require(akima)
require(maps)

eof.num &lt;- 1
F1 &lt;- interp(x=grd$lon, y=grd$lat, z=P$rotation[,eof.num]) # interpolated spatial EOF mode


png(paste0(""EOF_mode"", eof.num, "".png""), width=7, height=6, units=""in"", res=400)
op &lt;- par(ps=10) #settings before layout
layout(matrix(c(1,2), nrow=2, ncol=1, byrow=TRUE), heights=c(4,2), widths=7)
#layout.show(2) # run to see layout; comment out to prevent plotting during .pdf
par(cex=1) # layout has the tendency change par()$cex, so this step is important for control

par(mar=c(4,4,1,1)) # I usually set my margins before each plot
pal &lt;- jetPal
image(F1, col=pal(100))
map(""world"", add=TRUE, lwd=2)
contour(F1, add=TRUE, col=""white"")
box()

par(mar=c(4,4,1,1)) # I usually set my margins before each plot
plot(time, P$x[,eof.num], t=""l"", lwd=1, ylab="""", xlab="""")
plotRegionCol()
abline(h=0, lwd=2, col=8)
abline(h=seq(par()$yaxp[1], par()$yaxp[2], len=par()$yaxp[3]+1), col=""white"", lty=3)
abline(v=seq.Date(as.Date(""1800-01-01""), as.Date(""2100-01-01""), by=""10 years""), col=""white"", lty=3)
box()
lines(time, P$x[,eof.num])
mtext(paste0(""EOF "", eof.num, "" [expl.var = "", round(expl.var[eof.num]*100), ""%]""), side=3, line=1) 

par(op)
dev.off() # closes device
</code></pre>

<p><a href=""https://i.stack.imgur.com/8Qewd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Qewd.png"" alt=""enter image description here""></a></p>

<h3>Create correlation map</h3>

<pre><code>loc &lt;- c(-90, 0)
target &lt;- which(grd$lon==loc[1] &amp; grd$lat==loc[2])
COR &lt;- cor(slp.anom)
F1 &lt;- interp(x=grd$lon, y=grd$lat, z=COR[,target]) # interpolated spatial EOF mode


png(paste0(""Correlation_map"", ""_lon"", loc[1], ""_lat"", loc[2], "".png""), width=7, height=5, units=""in"", res=400)

op &lt;- par(ps=10) #settings before layout
layout(matrix(c(1,2), nrow=2, ncol=1, byrow=TRUE), heights=c(4,1), widths=7)
#layout.show(2) # run to see layout; comment out to prevent plotting during .pdf
par(cex=1) # layout has the tendency change par()$cex, so this step is important for control

par(mar=c(4,4,1,1)) # I usually set my margins before each plot
pal &lt;- colorRampPalette(c(""blue"", ""cyan"", ""yellow"", ""red"", ""yellow"", ""cyan"", ""blue""))
ncolors &lt;- 100
breaks &lt;- seq(-1,1,,ncolors+1)
image(F1, col=pal(ncolors), breaks=breaks)
map(""world"", add=TRUE, lwd=2)
contour(F1, add=TRUE, col=""white"")
box()

par(mar=c(4,4,0,1)) # I usually set my margins before each plot
imageScale(F1, col=pal(ncolors), breaks=breaks, axis.pos = 1)
mtext(""Correlation [R]"", side=1, line=2.5)
box()

par(op)

dev.off() # closes device
</code></pre>

<p><a href=""https://i.stack.imgur.com/vLFE4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vLFE4.png"" alt=""enter image description here""></a></p>
",2013-10-13 10:19:34.250
57382,,1,57392.0,,user10619,Does sampling error include measurement error?,<sampling><standard-error><measurement-error>,CC BY-SA 3.0,"<p>Gross sampling error (MSE) appears to be a composite of two errors sampling and measurement error. How do we assess measurement error ? can we find out net sampling error ? </p>
",2013-10-13 11:56:43.490
57390,21985.0,1,57427.0,,,"Auto regressive process, maximum likelihood estimator",<self-study><maximum-likelihood><autoregressive>,CC BY-SA 3.0,"<p>A first-order autoregressive process, $X_0,\dots,X_n$, is given through the following conditional distributions:
$X_i | X_{i-1},\dots,X_0 \sim \mathcal{N}(\alpha X_{i-1},1)$,
for $i = 1,2,\dots,n$ and $X_0 \sim \mathcal{N}(0,1)$.</p>

<p>I know that the log-likelihood function $\ell{(\alpha)}$ is of the form:
$\ell(\alpha) = - \frac{1}{2} \sum (x_i - \alpha x_{i-1})^2 + c$. But I don't know how to show that.</p>

<p>I found for $\hat{\alpha}_{ML}$ the following solution: $\hat{\alpha}_{ML} = \frac{s}{t}, \mathrm{where} \; s = \sum x_1 x_{i-1} \mathrm{and} \; t = \sum (x_{i-1})^2$. Is this right?</p>

<p>Then I have to show that this is the global maximum. If I take the second derivative I get a constant. Is this the sign that I got the global maximum, because the first derivative is linear wrt to $\alpha$? Right?</p>
",2013-10-13 14:45:19.743
57540,22715.0,1,,,,Is there a statistical application that requires strong consistency?,<hypothesis-testing><mathematical-statistics><asymptotics><estimators><consistency>,CC BY-SA 3.0,"<p>I was wondering if someone knows or if there exists an application in statistics in which strong consistency of an estimator is required instead of weak consistency. That is, strong consistency is essential for the application and the application would not work with weak consistency. </p>
",2013-10-15 15:00:30.140
57383,,2,,44370.0,anon,,,CC BY-SA 3.0,"<p>This depends on what you mean by a genomic location. For each cytoband this would be rather straight forward to do. Roughly:</p>

<p>1) Get the cytoband locations for all genes. These are stored in the organism specific packages, e.g., org.Dm.eg.db, and are named as 'MAP' . You might need the chiptype specific annotation package to map between the probe identifiers and the genes first. </p>

<p>2) Once you have the cytoband annotations for the genes, you can then test each cytoband separately with the functionality offered by, e.g., the topGO package. There is a section with the heading 'Predefined list of interesting genes' in the vignette of the topGO package that shortly shows how to do this is a similar case.</p>

<p>For the smoothing approach you have thought of, it might be worth correcting the counts with the actual number of genes in any predefined window, taking into account that not all genes might be present on the chip. The exact gene locations are available in the organism specific annotation package (the same as above). Some difficulties might arise, since certain locations probably have a gene in both strands, so you just need to decide how to count them.</p>

<p>The cytoband based approach is available in, e.g., Chipster software (see the manual entry at <a href=""http://chipster.csc.fi/manual/stat-hyperG-cytoband.html"" rel=""nofollow"">http://chipster.csc.fi/manual/stat-hyperG-cytoband.html</a>), and the source code for the analysis is available at <a href=""https://github.com/chipster/chipster/blob/master/src/main/modules/microarray/R-2.12/stat-hyperG-cytoband.R"" rel=""nofollow"">https://github.com/chipster/chipster/blob/master/src/main/modules/microarray/R-2.12/stat-hyperG-cytoband.R</a>, which might in some details, if you decide to use the cytobands.</p>
",2013-10-13 12:11:47.693
57384,306.0,2,,57378.0,,,,CC BY-SA 3.0,"<p>Assuming the coefficients to be normally distributed with the mean of 0 and an estimated standard error which you have not mentioned, the p value tells the quantile of how far the calculated value is from the mean. In the given case if you think that the value is significant at 99.73% level, even then the coefficient is different from 0. If the confidence level that you want is higher than this, then you fail to reject the hypothesis that the coefficient is different from 0.</p>
",2013-10-13 12:29:25.737
57385,22651.0,1,,,,Testing linearity,<model>,CC BY-SA 3.0,"<p>I have two variables and I need to test if they exhibit a linear relationship so that I will be able to predict a response. Kindly assist in how to handle this problem. This is the data:task is to show that there is linear relationship between bricks used and wastes generated.</p>

<pre><code>Trials            1       2       3       4       5       6       7        8
No. Bricks (x)  1400    1800    2100    2400    2700    3000    3500    3800
Wastage, % (y)  10.31   12.26   13.32   15.65   15.12   18.93   20.72   19.04
</code></pre>
",2013-10-13 13:09:09.757
57386,503.0,2,,57385.0,,,,CC BY-SA 3.0,"<p>My first point would be that you do not need to have a linear relationship in order to predict a response.</p>

<p>Second, if you are trying to predict a response outside the range of the data (i.e. to less than 1400 or more than 3800 bricks) be very cautious.</p>

<p>To your question: The first thing I would do is make a graph. In <code>R</code> this could be done as follows:</p>

<pre><code>x &lt;- c(1400, 1800, 2100, 2400, 2700, 3000, 3500, 3800)
y &lt;- c(10.31, 12.26, 13.32, 15.65, 15.12, 18.93, 20.72, 19.04)
plot(x,y)
lines(lowess(x,y))
</code></pre>

<p>The last line adds a loess curve to the data. The relationship appears to be linear at the lower levels, but then flatten at higher levels of bricks. </p>

<p>I would not rely on any statistical test of linearity. With only 8 points, the deviation from linearity would have to be extreme for it to be significant and a much smaller deviation might be important. </p>
",2013-10-13 13:23:42.913
57387,12522.0,2,,57377.0,,,,CC BY-SA 3.0,"<p>A possible formulation of this model is as follows:</p>

<p>The purpose of the optimization problem is to obtain the % of each coal type to mix in order to minimize the cost of the mix without violating any operational constraint.</p>

<p>$i = $ index for coal type (1 = A, 2 = B, 3 = B, 4 = D)</p>

<p>$x_{i} =$ % of coal type $i$ to be included in the mix</p>

<p>$c_{i} =$ cost per pound of coal of type $i$ </p>

<p>$b_{i} =$ BTUs per pound of coal of type $i$</p>

<p>$a_{i} =$ % of ashes of coal of type $i$</p>

<p>$m_{i} =$ % of moisture of coal of type $i$</p>

<p>Objective Function: Minimize the cost of a pound of the mix</p>

<p>Min $Z = \sum_{i=1}^{4} c_{i} \cdot x_{i}$</p>

<p>Subject to the following contraints:</p>

<ul>
<li><p>$BTU/lb$ of the mix must be equal to 11,900:</p>

<p>$\sum_{i=1}^4 b_{i} \cdot x_{i} = 11900$</p></li>
<li><p>Content of ashes of the mix must be less than 12.2%:</p>

<p>$\sum_{i=1}^4 a_{i} \cdot x{i} \leqslant 12.2\%$</p></li>
<li><p>The percent of moisture of the mix must be less than 9.4%:</p>

<p>$\sum_{i=1}^4 m_{i} \cdot x{i} \leqslant 9.4\%$</p></li>
<li><p>The percent of each coal in the mix must add up to 100%:</p>

<p>$\sum_{i=1}^4 x{i} = 100\%$</p></li>
<li><p>Non-negativity constraint:</p>

<p>$x_{i} \geqslant 0, \forall i$</p></li>
</ul>

<p>You can implement the model in R using the Rglpk package or using the Excel Solver Add-in in MS Excel.</p>
",2013-10-13 13:26:01.097
57388,22653.0,1,,,,Meta Analysis: Pooling samples or determine an average effect size,<meta-analysis>,CC BY-SA 3.0,"<p>I am new to meta analysis and how I understood the terminology is that there are actually two ways of performing a meta analysis. Let's consider 5 clinical studies with fixed effects. Fixed effects in terms of the same medical treatment as well as demographic details of the participants. One way of analysing these data would be to pool all 5 studies together to obtain a very large study to increase the power to detect the effect of the medical treatment. The other would be to try to detect the effect in each analysis separately and then determine the average effect across the studies. As I understood meta analysis, both seem to be reasonable techniques. However, can anyone tell me pro's and con's for both techniques? When should I use which method? I would assume the results to be pretty similar anyhow or is that wrong to assume?</p>
",2013-10-13 13:49:31.680
57389,166.0,2,,57378.0,,,,CC BY-SA 4.0,"<p>Your interpretation is almost right.</p>

<p>A right interpretation should contain the following information:</p>

<ol>
<li><p>There are two approaches to interpretating of p-values:</p>

<ul>
<li><p>The Frequentist interpretation, which your answer correctly used: The p-value is the probability of observing a value (in your case, the association between y-intercept and response) as extreme or more ('extreme' implies a two-tailed test), if the null hypothesis is true (in your case that is, the association between y-intercept and response is truly absent in the population, i.e. y-intercept = 0. In some tests it can mean the difference is 0);</p></li>
<li><p>or, the probability of obtaining that estimate of the parameter (e.g. intercept; using this statistical approach), or a more extreme value, if the population value for that parameter is 0.  Your definition correctly uses the frequentist form.</p></li>
</ul></li>
<li>As you can see from point 1, you do not need to assume the other coefficients are correct when interpreting p-values in a regression model... just that the same approach was used.  However, it does assume that those parameters are estimated.  <strong>So, your definition lacks</strong> in saying that 'first coefficient is 0.514'. All you need to assert was that the first coefficient is being estimated, i.e. '...the true value of the y-intercept is zero, <em>in the presence of x.</em>'. The values of other coefficients are immaterial to the definition of the p-value of any coefficients.</li>
<li>The y-intecept is referring to the value of y when all xs are zero. You correctly implied this point.</li>
</ol>

<p>You should also note that your example, in using the frequentist approach, is not free from your wants and subjective beliefs. Specifically, the p-value is tied to the design of the experiment you ran. You acknowledged this when you mention using the same number of sampling pairs.</p>

<p>With regards to your second question, the typical p-value reported for a regression equation is implicitly two-tailed. So, it refers to the absolute value of the parameters obtained. You didn't provide the Excel function you used to calaculate the p-value, but I'd check there to see if Excel is calculating one-tailed (in the same direction) or two-tailed (extreme or more extreme) p-values.</p>
",2013-10-13 13:57:06.653
57391,2420.0,1,,,,How to find the input layer and the architecture for a Neural Network,<regression><machine-learning><forecasting><neural-networks>,CC BY-SA 3.0,"<p>I'm a software developer and I'll like to learn about neural networks. At this point I've find a problem which I'll like to solve at some point. It is about electrical load forecasting. I'm looking for similar problems and it will be great if I can find some similar examples with solutions. At this point I'm having troubles in finding the right model for the RNN, and more exactly I'm struggling with the input layer. As the output I need the forecast values for each hour.</p>

<p>Any reference to books, links resources or advices are welcome and very appreciated. </p>

<p>This is the problem that I'll like to solve:</p>

<p>A very small factory, use a number of equipments to produce bread. Some of them are electrical equipments which means that they consume electrical power.  Knowing which equipments will run on the next day, an electricity consumptions forecast can be computed.  </p>

<p>The equipment named E.V. is a special case of equipment. The human operator completes it's values in an empirically manner in order to have a good forecast for the next day. Those values can be positive or negative. The 'on' thing from the bellow table represents the status of the machines.</p>

<pre><code>+----------------------------------------------------+  
|equipment name|power| 1h| 2h| 3h| 4h| 5h| 6h| 7h| 8h|  
+----------------------------------------------------+  
|Equipment 1   |  2MW| - | - | on| on| on| - | - | - |
|Equipment 2   |  5MW| - | - | - | on| on| on| - | - |
|Equipment 3   |  1MW| on| on| on| on| on| on| on| on|
|E.V.          |     | .1|-.1|-.1| .1|-.2| .1| .1|-.1|
+--------------+-------------------------------------+
|total/(forecast)    |1.1|0.9|2.9|8.1|7.8|6.1|1.1|0.9|
+--------------+-------------------------------------+
|real consumption    |0.9|0.9|2.7|8.2|7.9|3.1|0.8|0.7|  
+--------------+-------------------------------------+
</code></pre>

<p>The problem is that the machines are not running at their maximal power, so it will be great if a more exactly forecast can be build. </p>

<p><strong>I have data from 2 years back for every day. Also, do you think that date or week day is a good candidate for the input layer?</strong></p>

<p><em>I'm not very efficient in understanding an answer with a math only approach. Any example with something more appropiated with my problem is verry appreciated</em></p>
",2013-10-13 15:03:39.740
57392,5203.0,2,,57382.0,,,,CC BY-SA 3.0,"<p>Yes, assuming by ""gross sampling error"" you mean mean-squared error or the $\epsilon$ term in a model like $Y=AX + \epsilon$</p>

<p>The error component of a model includes <em>all</em> sources of variability that are not explicitly included in the model. This includes sampling errors (uncertainty due to measuring only a subset of the population), measurement errors (uncertainty due to imprecisions in each measurement), and other things, like error attributable to a misspecified model (e.g., missing predictors/interactions). </p>

<p>Keep in mind that these are actually types of errors. For example, there may be measurement error associated with each variable in the model, and that error might be a combination of systematic error (essentially, a bias; e.g., someone forgot that the scale reports the weight of the container + its contents) and random error.  Given that, there isn't an automatic, all-purpose way of identifying the various error contributions. </p>

<p>One way to examine measurement errors is through <strong>calibration</strong>. For example, you could put a weight on the scale and compare the scale's reading to the known mass of the weight. In many cases, the phenomena causing measurement error are reasonably well understood and have a specific structure (e.g., <a href=""http://en.wikipedia.org/wiki/Shot_noise"">shot noise</a>), which allows them to be incorporated into the model. Some large-scale physics experiments take this to incredible extremes to compare an apparatus's expected performance to the real data. Surveys are sometimes <strong>benchmarked</strong> by comparing data collected during the survey to larger data sets. For example, you might ask participants for demographic information (e.g., age, gender, income). These values are then compared to known population values (e.g., from a census or tax records), which might tell you how representative your respondents are of the general population. </p>

<p>Sampling error is much harder to measure directly. You might expect sampling error to shrink as the number of samples approaches the size of the population, whereas a systematic measurement error would remain approximately the same, regardless of sample size. </p>
",2013-10-13 15:15:38.567
57393,14850.0,2,,57379.0,,,,CC BY-SA 3.0,"<p>It appears that you're using <code>nls</code>.</p>

<p>By typing </p>

<pre><code>?summary.nls
</code></pre>

<p>you can read about the output.</p>

<p>Estimates and standard errors are estimated by the Gauss-Newton algorithm (if the <code>nls</code> defaults are used)</p>

<p>The P-values are the results of a two sided test of whether the parameters are zero or not. </p>

<p>You can check the exact calculations used to create the output shown by typing:</p>

<pre><code>stats:::summary.nls
</code></pre>
",2013-10-13 16:17:54.617
57394,16046.0,1,,,,Causality in Time Series,<time-series><bayesian><causality>,CC BY-SA 3.0,"<p>I am reading an <a href=""http://www.aaai.org/Papers/JAIR/Vol38/JAIR-3812.pdf"" rel=""nofollow"">article</a> which is trying to justify the need for causal inference in their inferential framework. The thought experiment is as follows:</p>

<blockquote>
  <p>Suppose a statistician is asked to design a model for a simple time
  series $X_1,X_2,X_3,...$ and she decides to use a Bayesian method.
  Assume she collects a first observation $X_1 = x_1$. She computes the
  posterior probability density function (pdf) over the parameters
  $\theta$ of the model given the data using Bayesâ€™ rule: $$p(\theta|X_1
= x_1) = \int\frac{p(X_1 = x_1|\theta)p(\theta)}{p(X_1 = x_1|\theta')p(\theta')}, $$</p>
  
  <p>where $p(X_1 = x_1|Î¸)$ is the likelihood of $x_1$ given $\theta$ and
  p($\theta$) is the prior pdf of $\theta$. She can use the model to
  predict the next observation by drawing a sample $x_2$ from the
  predictive ï¿¼pdf: $$p(X_2 = x_2|X_1 = x_1) = \int p(X_2 = x_2|X_1 =
 x_1,\theta)p(\theta|X_1 = x_1)d\theta,$$</p>
  
  <p>where $p(X_2 = x_2|X_1 = x_1,\theta)$ is the likelihood of $x_2$ given
  $x_1$ and $\theta$. Note that $x_2$ is not drawn from $p(X_2 = x_2|X_1
&gt; = x_1, \theta)$. She understands that the nature of $x_2$ is very different from $x_1$: while $x_1$ is informative and does change the
  belief state of the Bayesian model, $x_2$ is non-informative and thus
  is a reflection of the modelâ€™s belief state. Hence, she would never
  use $x_2$ to further condition the Bayesian model. Mathematically, she
  seems to imply that: 
  $$p(\theta|X_1 =x_1,X_2 =x_2)=p(\theta|X_1 =x_1)$$</p>
</blockquote>

<p>However I hardly believe that what this poor statistician should imply is:
$$p(\theta|X_1 =x_1,\text{do}(X_2 =x_2))=p(\theta|X_1 =x_1)$$
Where ""do(or set)"" here comes from <a href=""http://bayes.cs.ucla.edu/jp_home.html"" rel=""nofollow"">Pearl</a>'s framework of causality which can be found <a href=""ftp://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"" rel=""nofollow"">here</a> and <a href=""http://rads.stackoverflow.com/amzn/click/052189560X"" rel=""nofollow"">here</a>.
Now am I right about this?</p>
",2013-10-13 16:53:32.577
57395,668.0,2,,57373.0,,,,CC BY-SA 3.0,"<p><strong>Such a distribution does not exist.</strong></p>

<p>To see why not, let $0 \lt t \lt 1/2$ and notice that $X_2\gt 1-t$ entails $X_1\le t$ and $X_3\gt 1-t$ also implies $X_1\le t$, for otherwise in either situation the sum of all the $X_i$ would exceed $1.$  The latter two events are disjoint, because we cannot simultaneously have $X_2\gt 1-t \gt 1/2$ and $X_3\gt 1-t\gt 1/2.$  Consequently the chance that $X_1\le t$ is no less than the sum of the chances that $X_2\ge 1-t$ and $X_3\ge 1-t$, each of which equals $t$ by the uniform distribution assumptions.  This shows that $t \ge t+t,$ which for $t\gt 0$ obviously is false.</p>

<p>This contradiction forces us to give up at least one of the assumptions: if indeed $X_1+X_2+X_3\le 1$, then the only other assumptions used in this argument are that each $X_i$ has a Uniform$[0,1]$ distribution.  Therefore <em>at least one of the $X_i$ cannot have a Uniform$[0,1]$ distribution,</em> QED.</p>
",2013-10-13 17:06:36.577
57396,22656.0,1,57448.0,,,Minimum Sample Size Required to Estimate the Probability $P(X \le c)$ for a Constant $c$ (Given a Confidence Level & Confidence Interval),<probability><distributions><mathematical-statistics>,CC BY-SA 3.0,"<p>I have a large population of size $n$ from an unknown continuous random variable $X$, and I do not know the underlying distribution of $X$. Given a constant number $c$, I want to determine the minimum sample size I need to estimate the probability $P(X \le c)$ given a confidence level, $p_c$, and confidence interval, $I_c$  (I am not sure if we need them! ). How can I find the minimum sample size to estimate this probability? </p>

<p>I have found the following discussion in <a href=""http://en.wikipedia.org/wiki/Sample_size_determination#Estimating_proportions_and_means"" rel=""nofollow noreferrer"">Wikipedia</a> which is independent of the number of population. I am not sure if it is a good way to determine sample size! 
<img src=""https://i.stack.imgur.com/t09Kn.jpg"" alt=""enter image description here""></p>

<p>I have also found some methods to determine sample size for data to be analyzed by nonparametric tests.you don't have to make any assumption about the distribution of the values. That is why it is called nonparametric. Now I am confused if these nonparametric methods can be used to solve my problem or the method I found in Wikipedia is the correct way to solve my problem, or there exists a better solution.</p>

<p>Thanks for your help. </p>
",2013-10-13 17:09:46.403
57397,20473.0,2,,57380.0,,,,CC BY-SA 3.0,"<p>I assume that you are using the OLS estimator on this linear regression model. You can use the <em><strong>inequality constrained least-squares estimator</strong></em>, which will be the solution to a minimization problem under inequality constraints. Using standard matrix notation (vectors are column vectors) the minimization problem is stated as</p>

<p>$$\min_{\beta} (\mathbf y-\mathbf X\beta)'(\mathbf y-\mathbf X\beta) \\s.t.-\mathbf Z\beta \le \mathbf 0 $$</p>

<p>...where $\mathbf y$ is $n \times 1$ , $\mathbf X$ is $n\times k$, $\beta$ is $k\times 1$ and $\mathbf Z$ is the $m \times k$ matrix containing the out-of-sample regressor series of length $m$ that are used for prediction. We have $m$ linear inequality constraints (and the objective function is convex, so the first order conditions are sufficient for a minimum).</p>

<p>The Lagrangean of this problem is</p>

<p>$$L =  (\mathbf y-\mathbf X\beta)'(\mathbf y-\mathbf X\beta) -\lambda'\mathbf Z\beta = \mathbf y'\mathbf y-\mathbf y'\mathbf X\beta -  \beta'\mathbf X'\mathbf y+ \beta'\mathbf X'\mathbf X\beta-\lambda'\mathbf Z\beta$$</p>

<p>$$= \mathbf y'\mathbf y -  2\beta'\mathbf X'\mathbf y+ \beta'\mathbf X'\mathbf X\beta-\lambda'\mathbf Z\beta $$</p>

<p>where $\lambda$ is a $m \times 1$ column vector of non-negative Karush -Kuhn -Tucker multipliers. The first order conditions are (you may want to review rules for matrix and vector differentiation)</p>

<p>$$\frac {\partial L}{\partial \beta}= \mathbb 0\Rightarrow  -  2\mathbf X'\mathbf y +2\mathbf X'\mathbf X\beta - \mathbf Z'\lambda  $$</p>

<p>$$\Rightarrow \hat \beta_R = \left(\mathbf X'\mathbf X\right)^{-1}\mathbf X'\mathbf y + \frac 12\left(\mathbf X'\mathbf X\right)^{-1}\mathbf Z'\lambda = \hat \beta_{OLS}+ \left(\mathbf X'\mathbf X\right)^{-1}\mathbf Z'\xi \qquad [1]$$</p>

<p>...where $\xi = \frac 12 \lambda$, for convenience, and $\hat \beta_{OLS}$ is the estimator we would obtain from ordinary least squares estimation.</p>

<p>The method is fully elaborated in <a href=""http://www.jstor.org/stable/2285614"" rel=""noreferrer"">Liew (1976)</a>.</p>
",2013-10-13 17:17:58.327
57398,22658.0,2,,49906.0,,,,CC BY-SA 3.0,"<p>There are 2 competing statistical models.  Model #1 (null hypothesis, McNemar): probability correct to incorrect = probability of incorrect to correct = 0.5 or equivalent b=c.  Model #2: probability correct to incorrect &lt; probability of incorrect to correct or equivalent b > c.  For model #2 we use maximum likelihood method and logistic regression to determine model parameters representing model 2.  Statistical methods look different because each method reflects a different model.  </p>
",2013-10-13 18:26:48.740
57399,6813.0,1,,,,Modelling probabilties within friend sets,<probability><modeling><binomial-distribution><model>,CC BY-SA 3.0,"<p>Recently, I was wondering about calculating the probability of a given individual in a given population ""knowing"" (let's say, present in individual's friend set) at least one person with a given trait A and at least one person with another given trait, B; where it is possible that any number of people in the given individual's friend set can possess both traits.</p>

<p>For example, using genetic traits, in a given population, how could one calculate the probability that a given individual in a given population ""knows"" at least one person with grey eye colour and at least one person who is greater than 200cm tall; where, naturally, it is possible that any number of people in the friend set can possess grey eye colour <em>and</em> be greater than 200cm tall.</p>

<p>I have developed a sort of model, but it may not be correctly specified; it is as follows:</p>

<p><strong>Assumptions and Qualifications:</strong></p>

<ul>
<li>First of all, for simplicity, let's assume that we define ""knowing"" as mutually connected friends on an online social network.</li>
<li>Secondly, the frequencies of genetic traits are (a) going to be determined by ethnicity of a given population as well as environmental factors (nutrition, healthcare) and (b) unlikely to be independently distributed across a given individual's friend set (e.g. family members will have greater genetic similarity); however, for this problem, let's adopt a simple model where both of the above conditions are violated.</li>
<li>Thirdly, assume that the individual's friend set provides a microcosmic representation of society; this facilitates a frequentist generation of probabilities from the instance rate in the population.</li>
<li>Finally, instances of genetic traits in the population are fabricated, but are used to generate probabilities for the examples.</li>
</ul>

<p><strong>Model Formulation:</strong></p>

<p>I have reasoned that a binomial random distribution can be applied to the probability of each genetic trait, where a ""success"" is defined as an individual in the friend set possessing that genetic trait.</p>

<p>Thus for trait A, we have:</p>

<p>$$P(A=k) = \binom{N}{k}p_{a}^{k}(1-p_{a})^{N-k}$$</p>

<p>and, for trait B:</p>

<p>$$P(B=k) = \binom{N}{k}p_{b}^{k}(1-p_{b})^{N-k}$$</p>

<p>where $N$ is number of friends in the friend set, $k$ is number of people containing the genetic trait and $p_{x}$ is the probability associated with possessing genetic trait $X$.</p>

<p>Because the scenario is concerned with the probability of <em>at least one</em> person possessing trait A and <em>at least one</em> person possessing trait B, it is easier to find the complement of no people in a friend set containing the trait; for both traits:</p>

<p>$$P(A &gt;= 1) = 1 - P(A = 0)$$</p>

<p>and </p>

<p>$$P(B &gt;= 1) = 1 - P(B = 0)$$</p>

<p>Furthermore, we know that probability of an intersection of events is given by:</p>

<p>$$\mathbb{P}(A \cap B) = \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(B|A)\mathbb{P}(A)$$</p>

<p>However, because we are assuming independence between genetic traits, $\mathbb{P}(A)$ and $\mathbb{P}(B)$ are independent, thus:</p>

<p>$$\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$$</p>

<p>Combining the information above, we get the following model for the above scenario:</p>

<p>$$\mathbb{P}(A &gt;=1, B &gt;=1) = \left(1-\binom{N}{k}p_{a}^{k}(1-p_{a})^{N-k}\right)\left(1-\binom{N}{k}p_{b}^{k}(1-p_{b})^{N-k}\right)$$</p>

<p>So, for the original example above, assuming a friend set of size $N = 300$, the instance of trait A in the population is $\frac{1}{800}$ and the instance of trait B in the population is $\frac{1}{5000}$; according to the model, we get the final probability:</p>

<p>$$\mathbb{P}(A &gt;=1, B &gt;=1)$$</p>

<p>$$ = \left(1-\binom{300}{0}\left(\frac{1}{800}\right)^{0}\left(\frac{799}{800}\right)^{300}\right)\left(1-\binom{300}{0}\left(\frac{1}{5000}\right)^{0}\left(\frac{4999}{5000}\right)^{300}\right)$$</p>

<p>$$\approx 0.018 = 1.8\%$$</p>

<p>Does this model seem reasonable given the assumptions?</p>

<p>Assuming this model is <em>not</em> correctly specified, maybe somebody could provide a more accurate representation.</p>
",2013-10-13 18:37:21.500
57400,7155.0,2,,57391.0,,,,CC BY-SA 3.0,"<p>Define neural network to be $f$, time-series to be $x$, lag order to be $n$ and forecast horizon to be $h$.</p>

<p>$ f(x_{t-1}, x_{t-2},..,x_{t-n}) = [x_t, x_{t+1},..,x_{t+h}]$</p>

<p>Assume you have the following time series,</p>

<pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
</code></pre>

<p>You define $n=2$, $h=1$.</p>

<p>Your inputs for that time-series are circulant matrix like.</p>

<p>x = </p>

<pre><code>[[ 1, 0],
 [ 2, 1],
 [ 3, 2],
 [ 4, 3],
 [ 5, 4],
 [ 6, 5],
 [ 7, 6],
 [ 8, 7]]
</code></pre>

<p>Your outputs are</p>

<p>y = </p>

<pre><code>[2, 3, 4, 5, 6, 7, 8, 9]
</code></pre>

<p>So the length of your input layer is given by $n$, the length of your output layer is given by $h$, where your first input neuron is $x_{t-1}$ and your last input in $x_{t-n}$. Same goes for the forecast horizon.</p>

<p>Instead of having multiple outputs for the forecast horizon, you can use a forecast horizon of 1 then recurse on the predictions to obtain any forecast horizon you want.</p>

<p>For classic parametric stationary time series models the limit of the recursive behaviour of the system is well-studied.</p>

<p>Your problem is a little more involved though. You have inputs and outputs of the system and you want the predict outputs to follow some reference trajectory.</p>

<p>One solution is to use Narma-L2, which approximates the system by linear feedback using two neural networks. Define control inputs to be $c$ and production outputs to be $p$. Define reference production outputs to be $r$</p>

<p>You train two neural networks of the forms $g(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n}) = c_{t}$ and $k(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n}) = p_{t}$.</p>

<p>The prediction for control inputs is then $c_t = \frac{r - k(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n})}{g(c_{t-1}, .., c_{t-n}, p_{t-1},..,p_{t-n})}$</p>

<p>Also, neural networks are a PITA. There's plenty of good nonparametric regression models that are easier to train, like Gaussian Process Regression for instance.</p>

<p>See: <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.4988&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">Neural Network NARMA Control of a Gyroscopic
Inverted Pendulum</a></p>
",2013-10-13 18:42:03.260
57401,15377.0,1,57407.0,,,A question on continuous random variable,<random-variable>,CC BY-SA 3.0,"<p>Let say, I have 2 continuous random variables X1 &amp; X2. Both have same location parameters. Other parameters may be same or may not.</p>

<p>Now say, the q1-th quantile of X1 is less than the q1-th quantile of x2. But the q2-th quantile of x1 is more than the q2th quantile of x2.</p>

<p>My question is, is that possible? Is there any example of x1 &amp; x2 which have that property?</p>

<p>I will be really grateful if someone can give me some pointer.</p>

<hr>

<p>Edit: At this point, I realize the question I asked for was not correctly specified. </p>

<p>I'm particularly interested in the case where the two quantiles being considered are on the same side of the location parameter. </p>
",2013-10-13 19:40:09.530
57402,7229.0,2,,57195.0,,,,CC BY-SA 3.0,"<p>I don't see clearly behind the lines but it seems to me that there are too much data points. </p>

<p>Since you want to show the regional homogeneity and not exactly stations, I'd suggest you firstly to group them spatially. For example, overlay by a ""fishnet"" and compute average measured value in every cell (at every time moment). If you place these average values in the cell centers this way you rasterize the data (or you can compute also mean latitude and longitude in every cell if you don't want overlaying lines). Or to average inside administrative units, whatever. Then for these new averaged ""stations"" you can calculate correlations and plot a map with smaller number of lines.</p>

<p><img src=""https://i.stack.imgur.com/EnuPm.png"" alt=""enter image description here""></p>

<p>This can also remove those random single high-correlation lines going through all area.</p>
",2013-10-13 20:29:43.190
57403,20062.0,1,58506.0,,,Why is Mantel's test preferred over Moran's I?,<r><correlation><spatial><pattern-recognition>,CC BY-SA 3.0,"<p><strong><a href=""http://en.wikipedia.org/wiki/Mantel_test"" rel=""noreferrer"">Mantel's test</a> is widely used in biological studies</strong> to
examine the correlation between the spatial distribution of animals (position in space) with, for example, their genetic relatedness, rate of aggression or some other attribute. Plenty of good journals are using it (<em>PNAS, Animal Behaviour, Molecular Ecology...</em>). </p>

<p>I fabricated some patterns which may occur in nature, but Mantel's test <strong>seems to be quite useless to detect them.</strong> On the other hand, <a href=""http://en.wikipedia.org/wiki/Moran%27s_I"" rel=""noreferrer"">Moran's I</a> had better results <em>(see p-values under each plot)</em>.</p>

<blockquote>
  <p><strong>Why don't scientists use Moran's I instead? Is there some hidden reason I do not see? And if there is some reason, how can I know (how the hypotheses must be constructed differently) to appropriately use Mantel's or Moran's I test? A real-life example will be helpful.</strong></p>
</blockquote>

<p><strong><em>Imagine this situation:</em></strong> There is an orchard (17 x 17 trees) with a crow is sitting on each tree. Levels of ""noise"" for each crow are available and you are want to know if the spatial distribution of crows is determined by noise they make.</p>

<p><strong><em>There are (at least) 5 possibilities:</em></strong></p>

<ol>
<li><p><strong>""Birds of a feather flock together.""</strong> The more similar crows are, the smaller the geographical distance between them <strong>(single cluster)</strong>.</p></li>
<li><p><strong>""Birds of a feather flock together.""</strong> Again, the more similar crows are, the smaller the geographical distance between them, <strong>(multiple clusters)</strong> but one cluster of noisy crows has no knowledge about the existence of second cluster (otherwise they would fuse into one big cluster).</p></li>
<li><p><strong>""Monotonic trend.""</strong> </p></li>
<li><p><strong>""Opposites attract.""</strong> Similar crows cannot stand each other.</p></li>
<li><p><strong>""Random pattern.""</strong> The level of noise has no significant effect on spatial distribution.</p></li>
</ol>

<p>For each case, I created a plot of points and used the Mantel test to compute a correlation (it is no surprise that its results are non-significant, I would never try to find linear association among such patterns of points).</p>

<p><img src=""https://i.stack.imgur.com/TWQqa.png"" alt=""enter image description here""></p>

<hr>

<p><strong>Example data:</strong> <em>(compressed as possible)</em></p>

<pre><code>r.gen   &lt;- seq(-100,100,5)
r.val   &lt;- sample(r.gen, 289, replace=TRUE)
z10     &lt;- rep(0, times=10)
z11     &lt;- rep(0, times=11)
r5      &lt;- c(5,15,25,15,5)
r71     &lt;- c(5,20,40,50,40,20,5)
r72     &lt;- c(15,40,60,75,60,40,15)
r73     &lt;- c(25,50,75,100,75,50,25)
rbPal   &lt;- colorRampPalette(c(""blue"",""red""))
my.data &lt;- data.frame(x = rep(1:17, times=17),y = rep(1:17, each=17),
             c1=c(rep(0,times=155),r5,z11,r71,z10,r72,z10,r73,z10,r72,z10,r71,
             z11,r5,rep(0, times=27)),c2 = c(rep(0,times=19),r5,z11,r71,z10,r72,
             z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=29),r5,z11,r71,z10,r72,
             z10,r73,z10,r72,z10,r71,z11,r5,rep(0, times=27)),c3 = c(seq(20,100,5),
             seq(15,95,5),seq(10,90,5),seq(5,85,5),seq(0,80,5),seq(-5,75,5),
             seq(-10,70,5),seq(-15,65,5),seq(-20,60,5),seq(-25,55,5),seq(-30,50,5),
             seq(-35,45,5),seq(-40,40,5),seq(-45,35,5),seq(-50,30,5),seq(-55,25,5),
             seq(-60,20,5)),c4 = rep(c(0,100), length=289),c5 = sample(r.gen, 289, 
             replace=TRUE))

# adding colors
my.data$Col1 &lt;- rbPal(10)[as.numeric(cut(my.data$c1,breaks = 10))]
my.data$Col2 &lt;- rbPal(10)[as.numeric(cut(my.data$c2,breaks = 10))]
my.data$Col3 &lt;- rbPal(10)[as.numeric(cut(my.data$c3,breaks = 10))]
my.data$Col4 &lt;- rbPal(10)[as.numeric(cut(my.data$c4,breaks = 10))]
my.data$Col5 &lt;- rbPal(10)[as.numeric(cut(my.data$c5,breaks = 10))]
</code></pre>

<p>Creating matrix of geographical distances (for Moran's I is inversed):</p>

<pre><code>point.dists           &lt;- dist(cbind(my.data$x, my.data$y))
point.dists.inv       &lt;- 1/point.dists
point.dists.inv       &lt;- as.matrix(point.dists.inv)
diag(point.dists.inv) &lt;- 0
</code></pre>

<p>Plot creation:</p>

<pre><code>X11(width=12, height=6)
par(mfrow=c(2,5))
par(mar=c(1,1,1,1))

library(ape)
for (i in 3:7) {
  my.res &lt;- mantel.test(as.matrix(dist(my.data[ ,i])), as.matrix(point.dists))
  plot(my.data$x,my.data$y,pch=20,col=my.data[ ,c(i+5)], cex=2.5, xlab="""", 
       ylab="""", xaxt=""n"", yaxt=""n"", ylim=c(-4.5,17))
  text(4.5, -2.25, paste(""Mantel's test"", ""\n z.stat ="", round(my.res$z.stat, 
   2), ""\n p.value ="", round(my.res$p, 3)))

  my.res &lt;- Moran.I(my.data[ ,i], point.dists.inv)
  text(12.5, -2.25, paste(""Moran's I"", ""\n observed ="", round(my.res$observed, 
   3), ""\n expected ="",round(my.res$expected,3), ""\n std.dev ="", 
       round(my.res$sd,3), ""\n p.value ="", round(my.res$p.value, 3)))
}

par(mar=c(5,4,4,2)+0.1)

for (i in 3:7) {
  plot(dist(my.data[ ,i]), point.dists,pch = 20, xlab=""geographical distance"", 
       ylab=""behavioural distance"")
}
</code></pre>

<p>P.S. in the examples on UCLA's statistics help website, both tests are used on the exact same data and the exact same hypothesis, which is <strong>not very helpful</strong> (cf., <a href=""http://www.ats.ucla.edu/stat/r/faq/mantel_test.htm"" rel=""noreferrer"">Mantel test</a>, <a href=""http://www.ats.ucla.edu/stat/r/faq/morans_i.htm"" rel=""noreferrer"">Moran's I</a>).</p>

<p><strong>Response to I.M.</strong>
You have write:</p>

<blockquote>
  <p>...it [Mantel]tests whether quiet crows are located near other quiet
  crows, while noisy crows have noisy neighbors.</p>
</blockquote>

<p>I think that <strong>such hypothesis could NOT be tested by Mantel test</strong>. On both plots the hypothesis valid. But if you suppose that one cluster of not noisy crows may not have knowledge about the existence of second cluster of not noisy crows - Mantels test is again useless. Such separation should be very probable in nature (mainly when you are doing data collection on larger scale).</p>

<p><img src=""https://i.stack.imgur.com/w4gav.png"" alt=""enter image description here""></p>
",2013-10-13 20:35:21.350
57404,19547.0,1,,,,Hypothesis Testing applied to real life,<hypothesis-testing>,CC BY-SA 3.0,"<p>I was wondering if it is possible to apply the method of hypothesis testing to real life. For example if someone can use it for decision making. I have always used this method for homework problems but maybe we can use this method as an aid in decision making. Therefore we could somehow know for example the probability of rejecting wrongly an alternative decision. What do you think on that? And it would be nice if someone can give an example if he thinks that this can be done.</p>
",2013-10-13 20:50:28.997
57405,19750.0,1,57408.0,,,Bayesian variable selection,<bayesian><model-selection>,CC BY-SA 3.0,"<p>Chapter 13 of Kevin Murphy's book <a href=""http://rads.stackoverflow.com/amzn/click/0262018020"" rel=""nofollow noreferrer"">Machine Learning: A Probabilistic Perspective</a> discusses Sparse Linear Models. After a short introduction on the benefits of sparse models, he introduces the following problem:</p>

<p><img src=""https://i.stack.imgur.com/AciWF.png"" alt=""enter image description here""></p>

<p>How does he derive equation 13.1 above? i.e. why does it take that form, and what is $f$ supposed to represent here?</p>
",2013-10-13 21:00:08.203
57413,22666.0,1,,,,Time persistence in panel data,<panel-data>,CC BY-SA 3.0,"<p>I am using dynamic model with panel quarter data using Stata. And my sample contain  16 nations from 2000 to 2010.
Is there an approximated number of observations in the panel data to be considered as a time persistent process? </p>
",2013-10-14 01:03:58.377
57414,22667.0,1,,,,"I have a discrete distribution and want to know to what extent other samples differ from it, what is the right test?",<statistical-significance><sampling><python>,CC BY-SA 3.0,"<p>This is kind of a basic stats question, but I want to make sure I am doing this right.</p>

<p>I have a distribution of objects. Specifically: 
    <code>array([   6072.,  112673.,  126874., 44366., 5384., 14697., 20323., 68197., 98024.,39483.,     103990., 18556., 32930., 23551., 6897.])</code></p>

<p>I then have a lot of samples like [1,4,0,0,0,0...] (same length) and I'd like to know how far the samples are from the distribution above. Correlation doesn't really do it. 
[32,0,0,0,..] should be further away than [4,0,0,0...].</p>
",2013-10-14 01:17:13.023
57406,22507.0,2,,20234.0,,,,CC BY-SA 3.0,"<p>Machine learning often deals with optimization of a function which has many local minimas. Feedforward neural networks with hidden units is a good example. Whether these functions are discrete or continuous, there is no method which achieves a global minimum and stops. It is easy to prove that there is no general algorithm to find a global minimum of a continuous function even if it is one-dimensional and smooth (has infinitely many derivatives).  In practice, all algorithms for learning neural networks stuck into a local minimum.  It is easy to check this: create a random neural network, make a big set of its responses to random inputs, then try to learn another neural network with the same architecture to copy the responses.  While the perfect solution exists, neither backpropagation not any other learning algorithm will be able to discover it, starting from a random set of weights.</p>

<p>Some learning methods, like simulated annealing or genetic algorithms, explore many local minimas.  For continuous functions there are methods like gradient descent, which  find the closest local minimum.  They are much faster, thats why they are widely used in practice.  But given enough time, the former group of methods outperforms the later in terms of training set error.  But with reasonable time constraints, for real world problems, the latter group is usually better.</p>

<p>For some models, like logistic regression, there is one local minimum, the function is convex, the minimization converges to the minimum, but the models themselves are simplistic.</p>

<p>Thats the bitter truth.</p>

<p>Note also that proof of convergence and proof of convergence to the best solution are two different things.  K-means algorithm is an example of this. </p>

<p>Finally, for some models we don't know how to learn at all. For example, if the output is an arbitrary computable function of inputs, we don't know good algorithms which, in reasonable time, find a Turing or equivalent machine implementing this function.  For instance, if f(1)=2, f(2)=3, f(3)=5, f(4)=7, ..., f(10)=29 (ten first primes), we don't know any learning algorithm which would be able to predict, in reasonable time, that f(11)=31, unless it already knows the concept of prime numbers.</p>
",2013-10-13 21:24:53.867
57407,594.0,2,,57401.0,,,,CC BY-SA 3.0,"<p>Since you appear to doubt the example offered, I have included a diagram. As Michael Mayer said, two normal distributions centered around 0, one with larger variance, is sufficient.</p>

<p>In the diagram, we compare the 0.1 and the 0.9 quantiles for $\sigma=1$ (blue) and $\sigma=0.8$ (dark orange)</p>

<p><img src=""https://i.stack.imgur.com/pT43v.png"" alt=""normcdfs sigma=1, sigma=0.8""></p>

<p>Michael Mayer's example fulfills the requirements of your question with $q_1=0.1$, $q_2=0.9$ and $X_1$ being the one with larger variance.</p>

<hr>

<p>Edit:</p>

<p>For the case where $q_1$ and $q_2$ must both be on the same side of whatever the measure of location is, let's take two symmetric distributions, which share the same mean and median.</p>

<p>Let $X_1$ be $\sim \text{N}(0,1^2)$ and let $X_2$ be an equal mixture of a $\text{N}(-0.8,0.1^2)$ and a  $\text{N}(0.8,0.1^2)$, and let $q_1 = 0.6$ and $q_2 = 0.9$:</p>

<p><img src=""https://i.stack.imgur.com/2dHFj.png"" alt=""normal 0,1 vs symmetric mixture of normals with small s.d.""></p>

<p>This example fulfills the new requirements of your question with $q_1=0.6$, $q_2=0.9$ and $X_1$ being the one with only a single normal component (shown in blue above).</p>

<p>Further, you should note that 'location parameter' isn't sufficiently specified. I could parameterize normal distributions by their 5th percentile and their standard deviation, and call the parameter based on the 5th percentile the location parameter (it's just a shift of the mean by $1.645\sigma$. and can work as a perfectly valid location parameter). Then Michael's original example suffices even under the new conditions. If that contradicts your intention, your intention needs to be stated specifically enough to exclude it.</p>
",2013-10-13 22:09:36.687
57408,7155.0,2,,57405.0,,,,CC BY-SA 3.0,"<p>The divisor is just a normalizing constant, so we can ignore it for the moment. If we plug in $f(\gamma)$ it simplifies to $p(D|\gamma)p(\gamma)$, by Bayes' rule is equal to $p(\gamma|D)p(D)$. Now since $p(D)$ isn't a function of $\gamma$ it falls into the normalizing constant. Thus it simplifies to $p(\gamma|D)$.</p>

<p>This expansion seems pointless until you realize that the $\gamma$ that $p(\gamma|D)$ is at its maximum is the same $\gamma$ that $f(\gamma)$ is at its minimum. So we can study $f(\gamma)$ by itself.</p>
",2013-10-13 22:17:17.043
57409,10060.0,2,,57230.0,,,,CC BY-SA 3.0,"<p>If the domain all have a base of 100, a stacked bar chart may be suitable.</p>

<p><img src=""https://i.stack.imgur.com/BaEoD.png"" alt=""enter image description here""></p>
",2013-10-13 22:36:21.843
57410,22662.0,1,,,,Joint entropy of two random variables,<entropy>,CC BY-SA 3.0,"<p>The joint entropy is the amount of information we get when we observe X and Y at the same time, but what would happen if we don't observe them at the same time.</p>

<p>For example, when i toss a coin, if i got tails i will only observe the variable X, but if i got heads i will only observe the variable Y. How could i find the entropy?</p>
",2013-10-13 22:51:56.027
57411,20473.0,2,,57410.0,,,,CC BY-SA 3.0,"<p>Entropy (joint entropy included), <em>is a property of the distribution</em> that a random variable follows. The available sample (and hence the timing of observation) plays no role in it.  </p>

<p>Copying for Cover &amp; Thomas, the joint entropy $H(X,Y)$ of two discrete random variables $X, Y,$ with joint distribution $p(x,y)$, is defined as</p>

<p>$$H(X,Y) = - \sum_{S_X}\sum_{S_Y}p(x,y)\log p(x,y) $$</p>

<p>Examine the expression: the sums are taken over <em>all possible values</em> of $X$ and $Y$, i.e. over all the values that belong to the support of each r.v. ($S_X$ and $S_Y$ respectively), irrespective of whether some of these values may not materialize or be observed in a sample. What we actually observe, or when, plays no role, in calculating entropy, and joint entropy in particular.  </p>

<p>Turning to your specific example: The side of a coin itself can not be modeled as a random variable. A random variable maps <em>events</em> into real numbers. The side of a coin is not an event. <em>Observing</em> one of the two sides is an event.  <em>Not observing</em> a side, is an event. So let's define a random variable $X$ by ""$X$ takes the value $1$ if heads is observed, $0$ otherwise"". And define $Y$ by ""$Y$ takes the value $1$ if tails is observed, $0$ otherwise"". Assume the coin is fair. The joint distribution of these two random variables is then described by 
$$\begin{align}
P(X=1,Y=1) &amp;= 0 \\
P(X=1,Y=0) &amp;= 0.5 \\
P(X=0,Y=1) &amp;= 0.5 \\
P(X=0,Y=0) &amp;= 0
\end{align}$$</p>

<p>Note that the numerical mapping we chose (the zero/one values) does not play, as numbers go, any decisive part in the probabilities assigned -we could have chosen a 5/6 mapping for $X$ and a 56/89 mapping for $Y$ (or whatever) -the allocation of probabilities in the joint distribution would have been the same (it is the underlying structure of <em>events</em> that is the critical factor).<br>
Next, as always, we consider the distribution at non-zero values, so </p>

<p>$$H(X,Y) = - 0.5\log(0.5) - 0.5\log(0.5) $$</p>

<p>and using base-2 for the logarithm we get</p>

<p>$$H(X,Y) = - 0.5(-1) - 0.5(-1) = 1 $$</p>

<p>Finally, you can easily find that the entropy of $X$ (and likewise for $Y$) is 
$$H(X) = - \sum_{S_X}p(x)\log p(x) = - 0.5(-1) - 0.5(-1) = 1 $$</p>

<p>So in this case $H(X,Y) = H(X) = H(Y)$. But the general expression for the decomposition of joint entropy is </p>

<p>$$H(X,Y) = H(X) + H(Y\mid X) = H(Y) + H(X\mid Y)$$ </p>

<p>where $H(Y\mid X)$ and  $H(X\mid Y)$ are conditional entropies. Then we conclude that $H(Y\mid X) = H(X\mid Y) = 0$ in this case. The intuition is straightforward: given $X$ what has happened to $Y$ is certain (and likewise in reverse), so conditional entropy is zero.</p>
",2013-10-14 00:19:58.667
57412,22665.0,1,,,,Understanding Bayesian Predictive Distributions,<bayesian><prediction>,CC BY-SA 3.0,"<p>I'm taking an Intro to Bayes course and I'm having some difficulty understanding predictive distributions. I understand why they are useful and I'm familiar with the definition, but there are some things I don't quite understand. </p>

<p><strong>1) How to get the right predictive distribution for a vector of new observations</strong></p>

<p>Suppose that we have built a sampling model $p(y_i | \theta)$ for the data and a prior $p(\theta)$. Assume that the observations $y_i$ are conditionally independent given $\theta$. </p>

<p>We have observed some data $\mathcal{D} = \{y_1, y_2, \, ... \, , y_k\}$, and we update our prior $p(\theta)$ to the posterior $p(\theta | \mathcal{D})$. </p>

<p>If we wanted to predict a vector of new observations $\mathcal{N} = \{\tilde{y}_1, \tilde{y}_2, \, ... \, , \tilde{y}_n\}$, I think we should try to get the posterior predictive using this formula
$$
p(\mathcal{N} | \mathcal{D}) = \int p(\theta | \mathcal{D}) p ( \mathcal{N} | \theta) \, \mathrm{d} \theta = \int p(\theta | \mathcal{D}) \prod_{i=1}^n p(\tilde{y}_i | \theta) \, \mathrm{d} \theta,
$$
which is not equal to
$$
\prod_{i=1}^n  \int p(\theta | \mathcal{D}) p(\tilde{y}_i | \theta) \, \mathrm{d} \theta,
$$
so the predicted observations are not independent, right?</p>

<p>Say that $\theta | \mathcal{D} \sim$ Beta($a,b$) and $p(y_i | \theta) \sim$ Binomial($n, \theta$) for a fixed $n$. In this case, if I wanted to simulate 6 new $\tilde{y}$, if I understand this correctly, it would be wrong to simulate 6 draws independently from the Beta-Binomial distribution that corresponds to the posterior predictive for a single observation. Is this correct? I don't know how to interpret that the observations are not independent marginally, and I'm not sure I understand this correctly.</p>

<p><strong>Simulating from posterior predictives</strong></p>

<p>Many times when we simulate data from the posterior predictive we follow this scheme:</p>

<p>For $b$ from 1 to $B$:</p>

<p>1) Sample $\theta^{(b)}$ from $p(\theta | \mathcal{D})$.</p>

<p>2) Then simulate new data $\mathcal{N}^{(b)}$ from $p(\mathcal{N} | \theta^{(b)})$. </p>

<p>I don't quite know how to prove this scheme works, although it looks intuitive. Also, does this have a name? I tried to look up a justification and I tried different names, but I had no luck.</p>

<p><strong>Thanks!</strong></p>
",2013-10-14 00:46:15.680
57418,19043.0,1,,,,Do only certain pairwise comparisons after significant interaction in two-way ANOVA,<r><anova><multiple-comparisons><post-hoc><tukey-hsd-test>,CC BY-SA 3.0,"<p>I am comparing measurements on a test group relative to a control group in three different environmental conditions. I am interested in both differences between environmental conditions and differences between test and control groups. I ran a two-way ANOVA with an interaction term and looked at pairwise comparisons when terms were significant. </p>

<p>When the interaction term was significant the Tukey HSD function in R automatically outputs all comparisons. Comparisons between test and test groups on different environmental conditions, comparisons between test and control groups on different environmental conditions and so forth. Needless to say, this resulted in a large number of tests to correct for.</p>

<p>My adviser thinks that I should only do three tests to compare test group to control group on each environmental condition (and then only adjust for three tests). I think that because I am interested in differences between environmental conditions in this study, I should run most tests. If I wasn't interested in differences in environmental conditions it should be a nested ANOVA, right? You can see from graphs that the interaction term come from differences between test and control groups on two ecological sites, but it doesn't seem valid to just only run comparisons between groups you 'suspect' will be different. The ones I'm not sure I care about are differences test and control groups on two different environmental conditions. </p>

<p><strong>Is it valid to only run comparisons between groups you are interested in to reduce the number of tests you have to adjust the Tukey HSD p-value for or should you run comparisons on all combinations of groups.</strong></p>

<p>Thanks for help in advance.</p>
",2013-10-14 04:02:43.010
57419,22668.0,1,,,,Weightining using TraMineR,<regression><panel-data><traminer><sequence-analysis>,CC BY-SA 3.0,"<p>I have read some posts on weighting. However, I am still unclear on the sort of weights I need to use. I am using data from the Longitudinal Survey of Australian Youth (LSAY). This survey provides longitudinal weights for each survey wave (i.e. correction for sampling error and attrition). Because I have weight variables for each period (10) and I can only specify one of them in TraMiner, I am not sure which I should use. I have read that whether weights for the first or last wave should be used. I have not read any reasons why.</p>

<p>Can anyone provide me some guidance on this issue?</p>
",2013-10-14 04:45:22.343
57420,306.0,2,,57417.0,,,,CC BY-SA 3.0,"<p>First things first. There needs to be greater information given as this does not have a universally correct answer. Different types of distributions have to be looked at with different types of procedures. </p>

<p>But just to show that yes this is possible, we assume that each of the variables that you have mentioned are normally distributed but the parameters of the normal distributions are different from each other for any given pair.</p>

<p>Now we take n samples each of these variables. Then calculate the correlation coefficients for each pair of the variables. If we cannot reject the hypothesis of these correlation coefficients being zero, we hypothesize that the variables are independent of each other. So we have a set of variables which are independent from each other, but they have different probability distributions.</p>
",2013-10-14 05:05:30.750
57421,22669.0,1,57423.0,,,"ARIMA (0,1,1) or (0,1,0) - or something else?",<time-series><modeling><arima>,CC BY-SA 3.0,"<p>I've just started learning time series so please excuse me if it's painfully obvious; I haven't managed to find the answer elsewhere.</p>

<p>I have a data series showing a pretty obvious trend although it's quite noisy. I can take pretty much any division of the data and run classical tests to show a highly significant difference in means.</p>

<p>I decided to have a look at time series analysis to see if it could help describe the trend. An ARIMA(0,1,1) model comes out with AIC,BIC=34.3,37.3 (Stata), whilst an ARIMA(0,1,0) model comes out with AIC,BIC=55.1,58.1 - so I understand I'm supposed to prefer the (0,1,1) model.</p>

<p>However, the coefficient for the MA(1) is displaying as -0.9999997 (and not showing any p-values). If I try the same in SPSS I get an MA(1) coefficient of 1.000 (I assume SPSS uses opposite signs) with a p-value of 0.990 - does this mean it suggests I drop the term?</p>

<p>My understanding is that the effect of a MA(1) coefficient of -1 is basically to remove the old error term and convert the whole series to a linear trend. Does this mean ARIMA is totally unsuitable for my needs? On the plus side it gives me a sensible value for the trend. If I use the (0,1,0) model then I still get a reasonable value for the trend but it's not significant any more.</p>

<p>Thanks for your help!</p>

<p>EDIT:
Thanks for looking in. The trend looks like a fairly linear decrease; the data points seen to fairly noisily rattle around above and below a trend line. The ARIMA (0,1,1) model produces something that's not far off a straight line decrease which seems sensible - the (0,1,1) produces what is essentially a lagged version of the data, translated down by one month of trend.
The data aren't stationary (due to the trend) - though the first differences seem to be. I don't think the (0,1,1) is a bad model - I'm just a little confused by the p-value seeming to suggest I should drop the MA term - or wondering if it means I should bin ARIMA entirely!</p>

<p>EDIT2
@vinux - thanks for the suggestion; that makes a lot of sense (and seems to be what the -1 MA term is trying to create?).
I've uploaded as many graphs as I could think of as people had requested.</p>

<p><img src=""https://i.stack.imgur.com/OR2sj.png"" alt=""tsline y - graph of raw values"">
<img src=""https://i.stack.imgur.com/xlzl2.png"" alt=""tsline D.y - graph of differences"">
<img src=""https://i.stack.imgur.com/8lQ57.png"" alt=""ac y - autocorrelations of y"">
<img src=""https://i.stack.imgur.com/IlqiP.png"" alt=""pac y - partial autocorrelations of y"">
<img src=""https://i.stack.imgur.com/gUa6p.png"" alt=""ac D.y - autocorrelations of first differences"">
<img src=""https://i.stack.imgur.com/KE5Sa.png"" alt=""pac D.y - partial autocorrelations of first differences""></p>

<p>I've also put the monthly data up in CSV format at <a href=""http://pastebin.com/7ih4mhfB"" rel=""nofollow noreferrer"" title=""Pastebin"">pastebin</a></p>
",2013-10-14 05:55:50.823
57422,9007.0,2,,57419.0,,,,CC BY-SA 3.0,"<p>TraMineR, and more generally sequence analysis, treats each sequence as a whole. Ideally, weights should therefore correct for selection bias of the sample of sequences you consider, and longitudinal weights should be used.
The choice of the weights depends on which sequences you retain for your analysis:</p>

<ol>
<li><p>If you use only complete sequences until the last wave, then the choice should be the longitudinal weights associated to the last wave, which accounts attrition until the last wave).</p></li>
<li><p>If you chose to retain all sequences complete until say the 6th wave and admit missing states for the last four waves, then you should chose the weights associated to wave 6.</p></li>
<li><p>If you retain all sequences, even for those for which you have valid data for the first wave only, then you should use weights of the first wave.</p></li>
<li><p>In case you select only sequences with no more than say 4 missing states, the choice may be more ambiguous. If most of the four missing states occur in the last four positions, then you could adopt the solution 2 as a good approximation.</p></li>
</ol>

<p>Hope this helps.  </p>
",2013-10-14 05:56:44.773
57423,5637.0,2,,57421.0,,,,CC BY-SA 3.0,"<p>It is difficult to give the right answer without looking at the data. Here are some points that may help you in your modelling. </p>

<p>The coefficient of MA(1) very close to 1 indicates the sign of overdifferencing.  This means unit root in Moving averages.</p>

<p>My suggestion would be: Check the original series is stationary (visually) or check the presence of unit root. If you observe deterministic trend (eg: linear), add the trend part with time series model. If the original series is stationary build the time series without differencing. </p>
",2013-10-14 06:12:22.867
57541,2149.0,2,,57539.0,,,,CC BY-SA 3.0,"<p>You could order the nob values from low to high and then use a search procedure to identify when and if local mean(s) changed significantly via Intervention Detection (trial and error). ID is esseentially a single dimension(characteristic) cluster analysis. Alternatively you could pre-specify the number of groups (classes) that you wished to have(n) and then find the n-1 breakpoints which optimally classifies the nob values. I have not ever done this but it might be worth a try.</p>
",2013-10-15 15:11:16.427
57424,1406.0,2,,57413.0,,,,CC BY-SA 3.0,"<p>Time series issues, such as unit roots, etc in panel data can be accounted for when there is enough time series dimension for single unit regression estimation. This means at least 30 observations. If you have less, you can only use ideas from time series regressions, such as doing regression on growth rates instead of levels, etc. </p>

<p>In fact J. Wooldridge in his book ""Econometric Analysis of Cross Section and Panel Data"" recommends to treat all the time series issues as a question of covariance matrix of the unit error term. Translated to Stata parlance, use cluster-robust standard errors for your analysis and you should be ok, with the usual caveat that there are no magical fixes in modelling, i.e. if your model is not sound, no fancy estimation method is going to help you.</p>
",2013-10-14 07:37:48.140
57425,11117.0,1,57431.0,,,Vocabulary: do we measure actual values or observations?,<probability><modeling>,CC BY-SA 3.0,"<p>Consider that $\theta$ is an hidden parameter and one has an observation such that $O$:
$$
O \sim N(\theta,\sigma^2).
$$
My question concerning vocabulary: </p>

<p>do we measure $\theta$ and it gives us $O$? (so we measure the true value)</p>

<p>or </p>

<p>do we measure $O$ ? (so we measure the observation)</p>

<p>I am looking for unquestionable sources.</p>
",2013-10-14 07:44:40.060
57426,20470.0,1,57446.0,,,Hidden Markov model for event prediction,<time-series><machine-learning><predictive-models><markov-chain><hidden-markov-model>,CC BY-SA 3.0,"<p><strong>Question</strong>: <em>Is the set-up below a sensible implementation of a Hidden Markov model?</em></p>

<p>I have a data set of <code>108,000</code> observations (taken over the course of 100 days) and approximately <code>2000</code> events throughout the whole observation time-span. The data looks like the figure below where the observed variable can take 3 discrete values $[1,2,3]$ and the red columns highlight event times, i.e. $t_E$'s:</p>

<p><img src=""https://i.stack.imgur.com/QkIn0.png"" alt=""enter image description here""></p>

<p>As shown with red rectangles in the figure, I have dissected {$t_E$ to $t_{E-5}$} for each event, effectively treating these as ""pre-event windows"". </p>

<p><strong>HMM Training:</strong> I plan to <a href=""http://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm"" rel=""noreferrer"">train</a> a Hidden Markov Model (HMM) based on all ""pre-event windows"", using the multiple observation sequences methodology as suggested on Pg. 273 of Rabiner's <a href=""http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf"" rel=""noreferrer"">paper</a>. Hopefully, this will allow me to train an HMM that captures the sequence patterns which lead to an event.</p>

<p><strong>HMM Prediction:</strong> Then I plan to use this HMM to <a href=""http://en.wikipedia.org/wiki/Forward_algorithm"" rel=""noreferrer"">predict</a> $log[P(Observations|HMM)]$ on a new day, where $Observations$ will be a sliding window vector, updated in real-time to contain the observations between the current time $t$ and $t-5$ as the day goes on. </p>

<p>I expect to see $log[P(Observations|HMM)]$ increase for $Observations$ that resemble the ""pre-event windows"". This should in effect allow me to predict the events before they happen.</p>
",2013-10-14 07:49:16.613
57427,20473.0,2,,57390.0,,,,CC BY-SA 3.0,"<p>Using the <strong>chain rule</strong>, the joint density here can be decomposed as (denoting $\mathbf X$ the collection of the $n+1$ random variables)</p>

<p>$$f_{\mathbf X}(x_n,x_{n-1},...,x_0) = f(x_n\mid x_{n-1},...,x_0)\cdot f(x_{n-1}\mid x_{n-2},...,x_0)\cdot f(x_{n-2}\mid x_{n-3},...,x_0) \cdot...\cdot f(x_0)$$</p>

<p>$$=\left(\prod_{i=1}^{n}\frac {1}{\sqrt{2\pi}}\exp\left\{-\frac {(x_i-\alpha x_{i-1})^2}{2}\right\}\right)\frac {1}{\sqrt{2\pi}}\exp\left\{-\frac {x_0^2}{2}\right\}$$</p>

<p>Viewed as a likelihood function of $\alpha$, and taking its natural logarithm, we have</p>

<p>$$\ln L(\alpha \mid \mathbf X) = -\frac 12\sum_{i=1}^n (x_i-\alpha x_{i-1})^2 +c$$</p>

<p>...where in $c$ is also included the density of $x_0$ (but $x_0$ affects estimation of $\alpha$ through its presence in the conditional density related to $X_1$).</p>

<p>Then </p>

<p>$$\frac {\partial \ln L(\alpha \mid \mathbf X)}{\partial \alpha} = \frac {\partial }{\partial \alpha} \left(-\frac 12\sum_{i=1}^n (x_i-\alpha x_{i-1})^2\right)$$</p>

<p>$$=-\frac 12\frac {\partial }{\partial \alpha} \left(\sum_{i=1}^n (x_i^2-2\alpha x_ix_{i-1}+\alpha^2x_{i-1}^2)\right)  $$</p>

<p>$$=-\frac 12\frac {\partial }{\partial \alpha} \left(\sum_{i=1}^n x_i^2-2\alpha \sum_{i=1}^nx_ix_{i-1}+\alpha^2\sum_{i=1}^nx_{i-1}^2)\right) $$</p>

<p>$$=\sum_{i=1}^n x_ix_{i-1} -\alpha\sum_{i=1}^nx_{i-1}^2$$</p>

<p>Setting </p>

<p>$$\frac {\partial \ln L(\alpha \mid \mathbf X)}{\partial \alpha} =0\Rightarrow \hat \alpha_{ML} = \frac {\sum_{i=1}^n x_ix_{i-1}}{\sum_{i=1}^nx_{i-1}^2}$$</p>

<p>while $$\frac {\partial^2 \ln L(\alpha \mid \mathbf X)}{\partial \alpha^2} = -\sum_{i=1}^nx_{i-1}^2 &lt;0$$</p>

<p>which guarantees a global and unique maximum, since it is negative irrespective of $\alpha$.</p>
",2013-10-14 08:16:58.337
57428,22629.0,1,,,,How to discretise continuous attributes while implementing the ID3 algorithm?,<data-mining><discrete-data><threshold><cart>,CC BY-SA 3.0,"<p>I am trying to implement the ID3 algorithm on a data set. However, all attributes are continuous and can have values between 1-10. I found that we have to specify the bin intervals for discretization but couldn't understand how to do this exactly. </p>

<p>Can some one explain on how to do this? The <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"" rel=""nofollow"">data set</a> I am using is Breast Cancer Data from Wisconsin hospitals.</p>
",2013-10-14 08:33:17.943
57429,20740.0,1,57544.0,,,Linear regression with independent variables with varying proportions,<regression><multiple-regression><interaction><linear-model>,CC BY-SA 3.0,"<p>I am looking to do a linear regression on two independent variables that will be present in varying proportions.</p>

<p>For example trying to do a linear regression on $Y$ which is payment behavior (payback rate) of customers based on the the quality (let's say Gini coefficient) of the new and existing customer credit scores ($X_1$ and $X_2$, respectively) adjusted for the proportion of new and existing customers in the sample.</p>

<p>Existing customers will be present in proportion $p$ and new customers in proportion $1-p = q$.</p>

<p>$Y$, payback rate is the percentage of total customers who pay back.  It could be expressed as the weighted average $Y = Y_1q + Y_2p$ where $Y_i$ is the payback rate of new/existing customers.</p>

<p>In general more new customers, $q$, has a negative effect.  Better scoring ($X_1, X_2$) and more existing customers p have a positive effect.</p>

<p>What is a good way to model this? </p>

<p>Would something like the following be a good solution trying to use $p$ and $q$ as some sort of interaction effect?  </p>

<p>$Y = X_1+X_2+\frac{X_1}{q}+X_2 p$</p>

<p>Would it be better to include p and q as variables themselves as well?</p>
",2013-10-14 08:35:58.630
57430,16474.0,2,,57429.0,,,,CC BY-SA 3.0,"<p>I would stick with $p$ only, as $q$ does not add any information on top of $p$. I would add interaction terms between $X_1$ and $p$ and $X_2$ and $p$ and then include the main effects of both $X_1$, $X_2$ and $p$. So:</p>

<p>$Y =\beta_0 + \underbrace{\beta_1 X_1 + \beta_2 X_2 + \beta_3 p}_{\textrm{main effects}} + \underbrace{\beta_4 X_1 p + \beta_5 X_2 p}_{\textrm{interactions}} + \varepsilon$</p>
",2013-10-14 08:46:34.843
57431,12683.0,2,,57425.0,,,,CC BY-SA 3.0,"<p>Statistics doesn't give a special meaning to 'measurement' in the way it does to 'estimate'. (As @Glen said, we 'estimate parameters'.) So it's going to depend on your area of application and on what $O$ and $\theta$ represent.</p>

<p>If the variance $\sigma^2$ describes the measurement error of some instrument or procedure, and $\theta$ is some property considered rather inherent to the thing being measured, it's natural to talk about 'measuring $\theta$', and about the $O$s as 'measurements of $\theta$'. E.g. the $O$s are several measurements of the length $\theta$ of a steel shaft.</p>

<p>If the variance $\sigma^2$ describes the variability of different individuals, and $\theta$ is some feature of the population considered rather contingent, it's not so natural to talk about 'measuring $\theta$'. E.g. the $O$s are single measurements of the lengths of each steel shaft from a batch, rather than measurements of the average length $\theta$ of a shaft in the batch .</p>

<p>In any case 'measuring an observation' is oddly worded; 'making an observation' is usual. </p>
",2013-10-14 09:18:20.040
57432,12683.0,2,,57365.0,,,,CC BY-SA 3.0,"<p>In regression analysis each response $Y_i$ is modelled conditional on the observed predictor value $x_i$; as (with a normal distribution of errors) $Y_i\sim\mathcal{N}(\beta_0+\beta_1 x_i,\sigma^2)$ where $\beta_0$ and $\beta_1$ are the intercept &amp; slope coefficients respectively, and $\sigma^2$ is the common error variance. Just as if the $x_i$s had been set by an experimenter rather than themselves sampled. The marginal distribution of the $Y_i$s is not necessarily thought about at all; but can be obtained using the conditional model, for any assumed distribution of the $X_i$s.</p>

<p>Marginal models are sometimes used for panel/longitudinal data instead of conditional multi-level models with random effects. See <a href=""http://projecteuclid.org/euclid.ss/1105714159"" rel=""nofollow"">Lee &amp; Nelder (2004), ""Conditional and Marginal Models: Another View"", <em>Statistical Science</em>, <strong>19</strong>, 2</a> for a (rather critical) account.</p>
",2013-10-14 11:06:43.030
57433,,2,,57086.0,user12555,,,CC BY-SA 3.0,"<p>I have experience of deploying random forests in a SQL Server environment via <code>User Defined Function</code>. The trick is to convert the <code>IF-THEN ELSE</code> rules that you get from each tree into a <code>CASE-WHEN END</code> or any other <code>Conditional Processing</code> construct (admittedly I've used JMP Pro's Bootstrap Forest implementation - 500k lines of SQL code).</p>

<p>There is absolutely no reason why this cannot be achived using the <code>rattle</code> <code>R</code> package. Have a look at <code>randomForest2Rules</code> &amp; <code>printRandomForests</code> functions in that package. Both take <code>random forest</code> object as input and visit each tree in the forest and output a set of <code>IF-THEN ELSE</code> rules. Taking this as a starting point it should not be difficult converting this logic into your desired language in an automated way, since the output from the above mentioned function is structured text. </p>

<p>The above, also makes it important to decide the smallest no. of trees you need in the forest to make predictions at a desired level of accuracy (hint: plot(rf.object) shows you at what point the forest predictions do not improve despite adding more trees.) in order to keep the no. of lines to represent the forest down.</p>
",2013-10-14 13:08:34.127
57434,1945.0,1,,,,How does RVM achieve sparsity?,<regression><machine-learning>,CC BY-SA 3.0,"<p>I have read several textbook descriptions on RVM and none of them provide an adequate (plain English) explanation of how RVM achieves sparsity.</p>

<p>I am left feeling like the authors left out a paragraph of text that would have connected the dots and instead decided to replace (rather than supplement) it with mathematical derivations.</p>

<p>Could someone please explain the basic idea as to how RVM works in relation to learning sparse regression models?</p>
",2013-10-14 13:13:22.673
57435,22677.0,1,,,,"Can someone enlighten me on what is ""Neglected Nonlinearity""?",<hypothesis-testing><econometrics><terminology><assumptions><nonlinear>,CC BY-SA 3.0,"<p>I ask this question out of curiosity</p>

<p>earlier today when i was trying to test for heteroscedasticity in <code>R</code>, i accidentally mistook <code>white.test</code> of <code>tseries</code> package for <code>white.test</code> of <code>bstat</code> package. </p>

<p>i found out later that the former tests for <strong>Neglected Non Linearity</strong> while the latter tests for <strong>Heteroscedasticity</strong></p>

<p>now this is something new, i hadn't heard about the <strong><em>""neglected""</em></strong> part before, can someone please enlighten me about the <strong><em>""neglected""</em></strong>?</p>
",2013-10-14 13:34:50.787
57436,7949.0,2,,57319.0,,,,CC BY-SA 3.0,"<p>There is no obvious relationship between $R^2$ and reversal of the sign of a regression coefficient. Assume you have data for which the true model is for example
$$
y_i = 0+5x_i -z_z + \epsilon_i
$$
with $\epsilon_i \sim N(0, sd_\text{error}^2)$. I show the zero to make explicit that the intercept of the true model is zero, this is just a simplification.</p>

<p>When x and y are highly correlated and centered about zero then the coefficient of z when regressing over just z will be positive instead of negative. Note that the true model coefficients do not change with $sd_\text{error}$ but you can make $R^2$ vary between zero and one by changing the magnitude of the residual error. Look for example at the following R-code:</p>

<pre><code>require(MASS)
sd.error &lt;- 1
x.and.z &lt;- mvrnorm(1000, c(0,0) , matrix(c(1, 0.9,0.9,1),nrow=2)) # set correlation to 0.9
x &lt;- x.and.z[, 1]
z &lt;- x.and.z[, 2]
y &lt;- 5*x - z + rnorm(1000, 0, sd.error) # true model
modell1 &lt;- lm(y~x+z)
modell2 &lt;- lm(y~z)
print(summary(modell1)) # coefficient of z should be negative
print(summary(modell2)) # coefficient of z should be positive   
</code></pre>

<p>and play a bit with sd.error. Look for example at $sd_\text{error}=50$.</p>

<p>Note that with a very large sd.error the coefficient estimation will become more unstable and the reversal might not show up every time. But that's a limitation of the sample size. </p>

<p>A short summary would be that the variance of the error does not affect the expectations and thus reversal. Therefore neither does $R^2$.</p>
",2013-10-14 13:45:26.510
57437,10147.0,1,,,,Ordered Response Variable,<regression><categorical-data><generalized-linear-model>,CC BY-SA 3.0,"<p>For regression with ordered response variable, there are different methods, for example, discriminant analysis, probit or logit model. I am wondering what are the different focuses of the different methods and  which one is more often used.</p>
",2013-10-14 14:23:05.487
57470,22143.0,2,,57452.0,,,,CC BY-SA 3.0,"<p><em>Try 2</em>:</p>

<p>This is a heuristic and I don't know of any statistical guarantees. The procedure is as follows:</p>

<ul>
<li>construct the empirical distribution function. If it looks exponential, convert the values to log scale to see a power-law tail.</li>
<li>Fit a curve on this modified histogram. That is, do a 1-D regression. Hopefully the curve mimics the tail of a well-behaved distribution.</li>
<li>Pick the point where the line intersects the x-axis in the interval $[\max_{i=1,...,N_s}x_i,\infty)$.</li>
</ul>

<p>This is another estimator of the max value of the support of the <em>population</em>.</p>
",2013-10-14 22:17:38.120
57438,13385.0,1,,,,Finding parameters to maximize expected utility of random variable,<regression><optimization>,CC BY-SA 3.0,"<p>I'm trying to analyze some data consisting of five randomized parameters and a utility function which indirectly depends on the parameters, by experimentation.  That is to say, the parameters of the experiment are chosen randomly, and successes and failures are counted up.  I want to find parameters for which the expected utility of successes and failures is highest.</p>

<p>From my days in calculus, I can see that an algorithm could consist of:</p>

<ol>
<li>Regression to a (hopefully analytically tractable) surface</li>
<li>Finding a maximum</li>
<li>Finding the pre-image of my maximum (if I use any of the C libraries I've seen, which seem to focus on the maximum value, not its pre-image)</li>
</ol>

<p>But I'm not sure about the ""fiddly bits"" like:</p>

<ul>
<li>The distribution of points (I don't have any data yet)</li>
<li>Any substantive idea of the shape of the surface, though I am expecting diminishing marginal utility, so it should be non-linear and have a bump.</li>
<li>Numerical stability</li>
</ul>

<p>This seems like it should be straight-forward, in terms of applied decision theory.  So, is my plan sensible?  Any pointers to literature, algorithms, C or Haskell libraries?</p>

<p>Addition in response to comment:</p>

<p>I'm trying to find the ""best"" parameters in terms of student performance.  The 5-tuple represents:</p>

<ol>
<li>$b$: The ""base"" waiting time before seeing a problem again.</li>
<li>$p_1$: A constant factor if the student says the problem was easy.</li>
<li>$p_2$: A constant factor if the student says it was hard.</li>
<li>$p_3$: A constant factor if the student says it was ""normal"".</li>
<li>$p_4$: A constant factor if the student got it wrong.</li>
</ol>

<p>The waiting time for the next viewing is computed by multiplying all of the responses the student has issued, and the base waiting time, and computing $e^{b \prod p_{i,j}}$.  So, for example, a wrong answer makes the waiting time much shorter.  An 'easy' report makes it quite a bit longer.</p>

<p>Now, if the student gets the next viewing wrong, we want to count it as a failure.  If the student gets it right (regardless of the difficulty the student reports), we count it as a success.</p>

<p>I want to maximize the utility function $\frac{|\text{success}|}{|\text{total}|}$ by varying the 5-tuple.  I guess $\frac{|\text{success}|}{|\text{failure}|}$ would serve the same purpose.</p>
",2013-10-14 14:28:38.417
57439,20831.0,2,,57421.0,,,,CC BY-SA 3.0,"<p>As you say, the data is not stationary, we can find the stationary transformed data by differencing, and checked by the unit root test (e.g Augmented Dickey-Fuller test, Elliott-Rothenberg-Stock test, KPSS test, Phillips-Perron test, Schmidt-Phillips test, Zivot-Andrews test...) We can talk about ARMA model only after confirming the stationarity.</p>

<p>It is a classical way to identify the ARMA(p, q) by the ACF plot and PACF plot. ARMA(0,1) and ARMA(0,0) can be told here. Another method to identify p, q is about the EACF, but it is not widely used for univariate time series.</p>

<p>Empirical studies show that AIC usually tends to overfitting. The advantage of using AIC is for automatic algorithm to find the best model, but it is not usually recommended in traditional time series textbook.</p>
",2013-10-14 14:41:48.740
57440,2666.0,2,,57437.0,,,,CC BY-SA 3.0,"<p>I don't think that discriminant analysis will be very efficient because it does not use the ordering.  There are 4 commonly used families for ordinal response that are based on direct probability modeling: logistic, probit, log-log (Cox model) and complementary log-log.  These are implemented in the R <code>rms</code> package <code>orm</code> function, which also handles continuous $Y$.  Graphical methods can be used to choose from among the 4.  Proportional odds is the easiest to interpret.</p>
",2013-10-14 14:56:24.637
57441,22678.0,2,,48125.0,,,,CC BY-SA 3.0,"<p>I would disagree on your first point. The $L_2$ regularized model is
$$
\parallel Y-K\beta \parallel_2^2 + \lambda \beta^T R \beta
$$
where K is the known kernel matrix and $R$ is the regularization matrix.
$K=R$ is only a good choice, when the gaussian kernel is used.
For more information please see
<em>A. Smola, B. SchÃ¶lkopf, On a Kernel-based Method for Pattern Recognition,
Regression, Approximation, and Operator Inversion, 1997</em></p>

<p>@author, the discussion about ""good kernels"" is rather popular.
See this post for example:
<a href=""https://stats.stackexchange.com/questions/48506/what-function-could-be-a-kernel"">What function could be a kernel?</a></p>

<p>However, there are ways to compute an optimized kernel based on your regularization idea.
You should find some approaches presented at NIPS.</p>
",2013-10-14 15:23:57.277
57442,22677.0,1,,,,How to do Univariate Heteroscedasticity Test,<heteroscedasticity><autoregressive><univariate>,CC BY-SA 3.0,"<p>I just wanted to know how to do <strong><em>Heteroscedasticity Test on a Univariate Model</em></strong>?</p>

<ul>
<li>ex: an univariate autoregressive model</li>
<li>ex: an univariate ARCH/GARCH model</li>
</ul>

<p>If it is possible, how does one do that in <code>R</code>? </p>
",2013-10-14 15:34:17.913
57443,2149.0,2,,57442.0,,,,CC BY-SA 3.0,"<p>This question was answered in 1988 <a href=""http://www.unc.edu/~jbhill/tsay.pdf"" rel=""nofollow"">http://www.unc.edu/~jbhill/tsay.pdf</a> by R.Tsay and implemented in AUTOBOX in 1990. As of this date (today) no other forecasting/time series package has implemented his elegant and creative solution. Simply adjust your series for time trend changes, level shift changes, seasonal pulses and pulses AND the correct ARIMA structure. Verify that the model parameters are constant over time and then search for change points in error variance as he recommends.</p>

<p>Edited to respond to Nick ..</p>

<p>As you may know ARCH/GARCH concerns itself with developing an ARIMA model for the squared residuals. The problem is if you have unusual (one-time) anomalies these are dealt with by incorporating pulse indicator series, yielding a zero residual for each identified point. Squaring these residuals leads to a distribution that has long tails and is not amenable to ARIMA. When I programmed and implemented ARCH/GARCH so that I could jump on the ""next new thing"" I found that it was fundamentally inconsistent with Intervention Detection schemes. Essentially ARCH/GARCH provides a possible solution for a ""change in variance"" that may well be more easily handled by Intervention Detection (violations in the expected value). Thus at this point in time my preferences (Occam's Razor) for the simplest solution/transformation/drug/remedy causes me to keep the solution as simple as possible but not too simple. The current release of AUTOBOX treats variance heterogeneity by identifying anomalies, parameter changes and deterministic variance changes and no need for power transformations via Box-Cox... If all this fails the user can square the residuals and build an arima model to construct his/her own ARCH/GARCH model. Here I stand, I can do no other!</p>
",2013-10-14 15:43:16.087
57444,22262.0,1,,,,Using quantile regression to predict probability of surpassing threshold,<probability><conditional-probability><quantiles><quantile-regression><conditioning>,CC BY-SA 3.0,"<p>Consider a continuous response $Y$ and design matrix vector $\mathbf{X}$. These are related through some function $f(X) = Y$. Suppose that I am interested in estimating the probability that $Y \leq 0.1$ conditional on observing $\mathbf{X}$. </p>

<p>I want to use quantile regression to do this - <strong>can I confirm that this is a legitimate methodology</strong>?</p>

<p>We have quantiles $\tau \in [0,1]$ and after estimating our quantile regression for each $\tau$ we have our quantile estimates $\mathbf{q} := \{\hat{Q}(\tau) : \tau \in \{0.01,0.02,...,0.99\}\}$. I want to select the $\tau$ such that $\hat{Q}(\tau) \approx 0.1$. When I find such a $\hat{Q}(\tau)$ it seems to then follow naturally that $P(Y \leq 0.1) = \tau$. The reason is that my model has estimated the $\tau$-th quantile to be $0.1$, which is point on the x-axis in $Y$'s pdf that I need to find to be able to determine $P(Y \leq 0.1)$.</p>

<p>In practice this may not work since an estimated quantile can be lower for higher $\tau$ under some $\mathbf{X}$.</p>

<p>Not looking for logistic regression with a discretized response as a solution (since I already know about this).</p>
",2013-10-14 15:45:47.440
57445,22143.0,2,,57434.0,,,,CC BY-SA 3.0,"<p>In Relevance vector machines (<a href=""http://en.wikipedia.org/wiki/Relevance_vector_machine"" rel=""nofollow"">RVM</a>) we have a prior on the weight vector $\mathbf{w}$ (which is $N+1$ dimensional, where $N$ is the number of examples) as shown in equation (5) of (<a href=""http://jmlr.org/papers/volume1/tipping01a/tipping01a.pdf"" rel=""nofollow"">1</a>):
$$p(\mathbf{w}|\alpha) = \Pi_{i=0}^{N}\mathcal{N}(w_i|0,\alpha_i^{-1}),$$
where $\mathbf{\alpha}$ is the $N+1$ dimensional vector of hyperparameters.</p>

<p>This prior is supposed to ensure that the weight vector $\mathbf{w}$ (which represents the number of ""support vectors"" which are active) is ""sparse"" if we can integrate out all the nuisance parameters ($\alpha$). See paragraph preceding Section 2.2 in (<a href=""http://jmlr.org/papers/volume1/tipping01a/tipping01a.pdf"" rel=""nofollow"">1</a>).</p>

<p><em>Potential points of confusion:</em></p>

<ul>
<li>the notation $\mathbf{w}$ is different from the $d$-dimensional linear model representation. Here, while comparing RVM with SVM, only think of the dual SVM formulation with the $N+1$ dimensional parameter $\mathbf{w}$.</li>
<li>""Sparse"" for (dual) SVMs means the number of support vectors is small. Do not confuse with number of non-zero coefficients in (the d-dimensional) linear models.</li>
</ul>
",2013-10-14 15:49:20.960
57446,4320.0,2,,57426.0,,,,CC BY-SA 3.0,"<p>One problem with the approach you've described is you will need to define what kind of increase in $P(O)$ is meaningful, which may be difficult as $P(O)$ will always be very small in general. It may be better to train two HMMs, say HMM1 for observation sequences where the event of interest occurs and HMM2 for observation sequences where the event <strong>doesn't</strong> occur. Then given an observation sequence $O$ you have
$$
\begin{align*}
P(HHM1|O) &amp;= \frac{P(O|HMM1)P(HMM1)}{P(O)} \\
&amp;\varpropto P(O|HMM1)P(HMM1)
\end{align*}
$$
and likewise for HMM2. Then you can predict the event will occur if 
$$
\begin{align*}
P(HMM1|O) &amp;&gt; P(HMM2|O) \\
\implies \frac{P(HMM1)P(O|HMM1)}{P(O)} &amp;&gt; \frac{P(HMM2)P(O|HMM2)}{P(O)} \\
\implies P(HMM1)P(O|HMM1) &amp;&gt; P(HMM2)P(O|HMM2).
\end{align*}
$$</p>

<p><em><strong>Disclaimer</strong>: What follows is based on my own personal experience, so take it for what it is.</em> One of the nice things about HMMs is they allow you to deal with variable length sequences and variable order effects (thanks to the hidden states). Sometimes this is necessary (like in lots of NLP applications). However, it seems like you have a priori assumed that only the last 5 observations are relevant for predicting the event of interest. If this assumption is realistic then you may have significantly more luck using traditional techniques (logistic regression, naive bayes, SVM, etc) and simply using the last 5 observations as features/independent variables. Typically these types of models will be easier to train and (in my experience) produce better results.</p>
",2013-10-14 16:03:34.203
57447,2666.0,2,,57444.0,,,,CC BY-SA 3.0,"<p>It doesn't appear that $Y$ is binary.  Ordinal regression is a good choice here.  With any of the ordinal models (proportional odds, proportional hazards, probit, etc.) you can compute the probability that $Y \geq y$ for all $y$.  That probability will change at the unique values of $y$.  The R <code>rms</code> package <code>orm</code> function implements this efficiently and has a function generator for exceedance probabilities.  If you were extremely fortunate and really have Gaussian residuals you can use the maximum likelihood estimator of the exceedance probabilities, which is a simple function of $\hat{\mu}$ and $\hat{\sigma}$.</p>
",2013-10-14 16:42:40.467
57448,16644.0,2,,57396.0,,,,CC BY-SA 3.0,"<p>The <a href=""http://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality"" rel=""nofollow"">Dvoretzky-Kiefer-Wolfowitz inequality</a> can be used here. The required sample size $b$ (I'm using $b$ to distinguish it from $n$ because you already set your population size as $n$ in the problem statement) is determined by $$b \geq \left( {1 \over 2 \epsilon^2 } \right) \mathrm{ln} \left( {2 \over \alpha} \right),$$ where $\epsilon$ is how close you want your empirical cdf to be and $1-\alpha$ is the confidence level. </p>

<p>So, for example, if you want to estimate $F(c)$ within $\epsilon = 0.01$ with 95% confidence, the formula gives a sample size of $$b \geq 18444.4,$$ or $b = 18445.$ </p>

<p>This will cover any and all $c,$ so it is possible you can do much better. Perhaps one of the commenters will fill in the details on a more efficient solution for a single value of $c.$ </p>
",2013-10-14 17:26:35.837
57449,21884.0,1,,,,Covariance matrix equality,<covariance><matrix>,CC BY-SA 3.0,"<p>The (unbiased) sample covariance matrix</p>

<p>$$\mathbf{S}=\dfrac{1}{n-1}\sum_{j=1}^{n}(\mathbf{X}_{j}-\bar{\mathbf{X}})(\mathbf{X}_{j}-\bar{\mathbf{X}})^{T}$$
can be rewritten as</p>

<p>$$\mathbf{S}=\dfrac{1}{n-1}\mathbf{X}^{T}\mathbf{X}-\dfrac{1}{n(n-1)}\mathbf{X}^{T}\mathbf{1}\mathbf{1}^{T}\mathbf{X}$$</p>

<p>where $$\mathbf{1}=\left(\begin{array}{c}
1\\
\vdots\\
1
\end{array}\right)_{(n\times1)}.$$</p>

<p>One (tedious) way of proving this is to expand out the left hand side and the right hand side of the equality, and showing that the entries of the matrices match. I've done this successfully.</p>

<p>My question: is there a neater / more concise way to prove such an equality?</p>
",2013-10-14 17:30:14.367
57450,19265.0,1,,,,What is the loss function for C - Support Vector Classification?,<svm><libsvm><loss-functions>,CC BY-SA 4.0,"<p>In article <a href=""http://www.csie.ntu.edu.tw/%7Ecjlin/papers/libsvm.pdf"" rel=""nofollow noreferrer"">LIBSVM: A Library for Support Vector Machines</a> it is written that C-SVC uses the loss function</p>
<p><span class=""math-container"">$$ \frac{1}{2}w^Tw+C\sum\limits_{i=1}^l\xi_i$$</span></p>
<p>I know what is <span class=""math-container"">$w^Tw$</span>.</p>
<p>But what is <span class=""math-container"">$\xi_i$</span>?  I know that it is somehow connected with misclassifications, but how it is calculated exactly?</p>
<p>P.S. I don't use any non-linear kernels.</p>
",2013-10-14 18:06:27.340
57451,21840.0,1,57457.0,,,Probability of having real roots,<self-study><multivariate-analysis><density-function><cumulative-distribution-function>,CC BY-SA 3.0,"<p>Let $U,V,W$ are independent random variables with $\mathrm{Uniform}(0,1)$ distribution. I am trying to find the probability that $Ux^{2}+Vx+W$ has real roots, that is, $P(V^{2}-4UW&gt; 0)$
I have solved this question using double integral but how to do this using triple integral.
My Approach:
I started with cdf:
$P(V^{2}-4UW &gt;0) =P(V^{2} &gt; 4UW) = P(V&gt;2\sqrt{UW})$ = $\int\int_{2\sqrt{uw}}^1 P(V&gt;2\sqrt{UW}) dU dW$
=$\int\int\int_{2\sqrt{uw}}^1 vdU dW dV$</p>

<p>I am finding hard time to get the limits of integral over the region in 3 dimensions.</p>

<p>Using double integral:
$P(V^{2}-4UW &gt;0) =P(V^{2} &gt; 4UW) = P(-2\ln V &lt;-\ln 4 - \ln U - \ln W) = P(X &lt;-\ln 4 +Y)$
where $X=-2 \ln V, Y = - \ln U -\ln W $
$X$ has $\exp(1)$ and $Y$ has $\mathrm{gamma}(2,1)$ distribution.
$P(X &lt;-\ln 4 +Y) = \int_{\ln4}^\infty P(X &lt; -\ln 4 +Y) f_Y(y) dy $
$$=\int_{\ln 4}^\infty\int_0^{-\ln 4+y} \frac{1}{2} e^{-\frac{x}{2}}ye^{-y} dxdy $$
Solving this I got $0.2545$.</p>

<p>Thanks!</p>
",2013-10-14 18:11:53.353
57567,22729.0,1,100174.0,,,Heteroscedasticity-consistent F-test,<r><regression><heteroscedasticity><f-test>,CC BY-SA 3.0,"<p>Why is the F-test for overall significance (OLS regression analysis) invalid when residuals are heteroscedastic? 
Is there a way to calculate it in a consistent way under heteroscedasticity?
Is there any function in R to accomplish that?</p>
",2013-10-15 20:02:04.407
57452,22627.0,1,57462.0,,,"Expected maximum given population size, mean, and variance",<estimation><sampling><spatial><extreme-value>,CC BY-SA 3.0,"<p>How would one estimate the maximum given population size, a few moments, and perhaps some additional assumption on the distribution?</p>

<p>Something like ""I'm going to do $N_sâ‰«1$ measurements out of population of size $N_pâ‰«N_s$; will record mean $Î¼_s$, standard deviation $Ïƒ_s$, and maximal value in the sample $X_s$; I am willing to assume binomial (or Poisson, etc) distribution; what is the expected maximal value $X_p$ of the entire population?""</p>

<p>Related question: does one need to make the assumptions on the nature of the population distribution, or the sample statistics would be enough to estimate $X_p$?</p>

<p>Edit: the background I just added in the comments may not be clear enough. So here it is:</p>

<p>The end purpose it to print a set of shapes (wires, gates, etc) on a VLSI circuit that matches the designed shapes (a.k.a. targets) as well as possible. The measure of fitness of the manufactured set of shapes is the MAXIMAL difference from the target, rather than the $\sigma$ along the $~10^9$ location. The reason for evaluating the maximum difference is clear: a single short circuit is bad enough to bring down the entire chip, and then it wouldn't matter how close you were to the target in the remaining 99.999999% of the chip's location. </p>

<p>The problem is that it's very costly to measure the printed shape in too many locations: you literally need to look though an electron microscope at the half-manufactured chip (that's going to get trashed after the destructive measurements), adjust for metrology errors, etc. Therefore more than $10^4$ measurements is hardly ever being done. The result of those measurement is the maximal target difference $X_s$ of the SAMPLE, as well as any other sample statistics you may wish for.</p>

<p>And now one needs to estimate the maximal difference $X_p$ for the entire population... And now one wishes that he paid more attention in the statistics class back in college...</p>
",2013-10-14 18:13:32.393
57453,22682.0,1,57454.0,,,Is there a way to remove individual trees from a forest in the randomForest package in R?,<r><random-forest><cart><max-margin>,CC BY-SA 3.0,"<p>I am trying to implement the ideas in this paper: <a href=""http://www.sciencedirect.com/science/article/pii/S0925231212003396"" rel=""noreferrer"">http://www.sciencedirect.com/science/article/pii/S0925231212003396</a>. </p>

<p>This requires me to be able to remove individual trees from the forest and reclassify my training data for each removal. I've been using the randomForest package in R and had a comb through the manual but couldn't find any way of running the forest with a subset of trees, or even with an individual tree. There is a getTree function but that only gives a matrix of the node structure of the tree.</p>

<p>Is there any way to do this, either in randomForest (preferably) or via another random forest implementation (e.g. scikit-learn)?</p>
",2013-10-14 18:27:13.640
57454,22507.0,2,,57453.0,,,,CC BY-SA 3.0,"<p>One idea is, instead of creating one forest with N trees, create N ""forests"" of 1 tree each by calling <code>randomForest()</code> N times. Then you could manipulate them as you wish.</p>
",2013-10-14 18:48:24.640
57455,14748.0,1,57465.0,,,Predictive algorithm validation,<machine-learning><predictive-models>,CC BY-SA 3.0,"<p>In putting a binary 1/0 predictive algorithm into production, what are the consequences where only the positive (1) predictions are checked, meaning only true or false positives are detected, and then fed back into training the model? Will that bias the algorithm in any way so that it progressively gets worse and worse because it never sees true or false negatives?</p>
",2013-10-14 19:13:34.737
57456,22143.0,2,,57450.0,,,,CC BY-SA 3.0,"<p>$\xi_i$ are the slack variables. They are typically nonzero when the 2-class data is non-separable. We are trying the minimize the slack as much as possible (by minimizing their sum, since they are non-negative) along with maximizing the margin ($w^Tw$) term.</p>

<p><em>Exact calculation</em>: Well, if the convex program has been solved to optimality without any optimization error, then yes, they are calculated exactly.</p>
",2013-10-14 20:04:52.890
57457,6162.0,2,,57451.0,,,,CC BY-SA 3.0,"<p>Here is a solution without multiple integrals calculation (because I don't like multiple integrals). Actually it only uses three elementary simple integrals. 
$$
P(V^{2}-4UW \leq  0) = E\bigl[P(V^{2}-4UW \leq 0 \mid U,W)\bigr] = E\bigl[f(U,W)\bigr]$$ where $f(u,w)=P(V^{2}-4uw \leq 0)= \min\bigl\{1, 2\sqrt{uw}\bigr\}$.
$$
E\bigl[f(U,W)\bigr] = E[g(W)]
$$
where 
$$\begin{align}
g(w) &amp; = E\bigl[\min\bigl\{1, 2\sqrt{Uw}\bigr\}\bigr] 
= 1 \times \Pr(2\sqrt{Uw}&gt;1) + E\bigl[2\sqrt{Uw} \mathbf{1}_{2\sqrt{Uw}\leq 1}\bigr] \\
&amp; = \Pr(U&gt;\frac{1}{4w}) + 2\sqrt{w}E\bigl[\sqrt{U} \mathbf{1}_{U \leq \frac{1}{4w}}\bigr]  \\
&amp; = \max\bigl\{0, 1 - \frac{1}{4w}\bigr\} +  2\sqrt{w} \times \frac{2}{3} \times \min\bigl\{1, \frac{1}{{(4w)}^{\frac{3}{2}}}\bigr\} \\ 
&amp; =\begin{cases} 
 0 + \frac{4}{3}\sqrt{w}  &amp; \text{if } w \leq \frac{1}{4} \\
1 - \frac{1}{4w} + \frac{1}{6w}  &amp; \text{if } w &gt; \frac{1}{4}
\end{cases}, \end{align}$$
and we get 
$$ E[g(W)] = \frac{1}{9} + \frac{3}{4} - \frac{1}{12} \log 4 = \frac{31}{36}-\frac{\log 2}{6},$$
and finally 
$$P(V^{2}-4UW &gt;  0) = \frac{5}{36} + \frac{\log 2}{6} \approx 0.2544134.$$</p>
",2013-10-14 20:07:35.763
57458,22684.0,1,,,,Questions about thresholding the data,<data-mining><data-transformation><outliers>,CC BY-SA 3.0,"<p>I came across a <a href=""http://www.kdnuggets.com/data_mining_course/assignments/final-project.html"" rel=""nofollow"">data mining course project</a> online.</p>

<p>The data is of samples with 7000 features as genes. Each gene is associated with a value. Some of the values are negative. The data looks like in this way:</p>

<pre><code>SNO ""U48730_at"" ""U58516_at"" ""U73738_at"" ""X06956_at"" ""X16699_at"" ""X83863_at""

X1 "" 27"" "" 161"" "" 0"" "" 34"" "" 2"" "" 116""
X2 "" 27"" "" 265"" "" 0"" "" 98"" "" 2"" "" 123""
X3 "" 24"" "" 126"" "" 0"" "" 21"" "" 0"" "" 142""
X4 "" 27"" "" 163"" "" -1"" "" 16"" "" -1"" "" 134""
X5 "" 41"" "" 138"" "" 1"" "" 29"" "" 1"" "" 153""
X6 "" 55"" "" 107"" "" -1"" "" 17"" "" 0"" "" 152""
X7 "" 27"" "" 99"" "" 0"" "" 57"" "" 1"" "" 139""
X8 "" 2"" "" 137"" "" -1"" "" 19"" "" -3"" "" 213""
X9 "" -5"" "" 161"" "" -3"" "" 23"" "" 2"" "" 193""
X10 "" 0"" "" 110"" "" -3"" "" 7"" "" -1"" "" 208""
X11 "" -7"" "" 67"" "" 1"" "" 2"" "" -2"" "" 149""
X12 "" 4"" "" 93"" "" 3"" "" 37"" "" 2"" "" 266""
X13 "" 2"" "" 75"" "" 3"" "" 30"" "" 6"" "" 205""
</code></pre>

<p>The professor advise the students to first do 'data cleaning'. The original sentence is Threshold both train and test data to a minimum value of 20, maximum of 16,000.</p>

<p>I first thought that it is to search over each gene and if there is a value out of the bounds, then just discard this gene as a feature. However, it seems for every gene, there must be a sample with the value out of bound.</p>

<p>What should I do by ""threshold this data""? Is that like if the value is below 20, then set it 20 or if the value is above 16000, then just set it as 16000?</p>

<p>In fact, I did the last operation in R by </p>

<pre><code>data[data&lt;20] &lt;- 20
</code></pre>

<p>and it turns out that the speed of the command is very slow. (79*7070 samples)</p>
",2013-10-14 20:08:14.273
57459,22143.0,2,,57458.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>What should I do by ""threshold this data""? Is that like if the value is below 20, then set it 20 or if the value is above 16000, then just set it as 16000?</p>
</blockquote>

<p>Yes.</p>
",2013-10-14 20:17:26.320
57479,64247.0,1,,,abe3,How would you use pair-wise plots to test the effectiveness of k-means clustering?,<machine-learning>,CC BY-SA 3.0,"<p>I am looking over slides for a big data class. The slides suggest doing a pairwise plot of data (if not too many variables) to evaluate the quality of output from k-means clustering -- with each data point color-coded by its cluster. The slides say: </p>

<blockquote>
  <p>If the (colored) clusters look separated in at least some of the plots. They wonâ€™t be very separated in all of the plots.</p>
</blockquote>

<p>How would this tell you if a pairwise plot is effective? You would want the colors to be mixed up in the plots to make sure that you have genuine multi-dimensional clusters and not just groups of data points that are very similar on one variable?</p>
",2013-10-15 00:37:03.313
57460,14799.0,2,,57319.0,,,,CC BY-SA 3.0,"<p>This is for OLS regression. Consider a geometric representation of three variables -- two predictors, $X_1$ and $X_2$, and a dependent variable, $Y$. Each variable is represented by a vector from the origin. The length of the vector equals the standard deviation of the corresponding variable. The cosine of the angle between any two vectors equals the correlation of the corresponding two variables. I will take all the standard deviations to be 1.</p>

<p><img src=""https://i.stack.imgur.com/io9v4.png"" alt=""enter image description here""></p>

<p>The picture shows the plane determined by the $X_1$ and $X_2$ when they correlate positively with one another. $Y$ is a vector coming out of the screen; the dashed line is its projection into the predictor space and is the regression estimate of $Y$, $\hat{Y}$. The length of the dashed line equals the multiple correlation, $R$, of $Y$ with $X_1$ and $X_2$.</p>

<p>If the projection is in any of the colored sectors then both predictors correlate positively with $Y$. The signs of the regression coefficients $\beta_1$ and $\beta_2$ are immediately apparent visually, because $\hat{Y}$ is the vector sum of $\beta_1 X_1$ and $\beta_2 X_2$. If the projection is in the yellow sector then both $\beta_1$ and $\beta_2$ are positive, but if the projection is in either the red or the blue sector then we have what appears to be suppression; that is, the sign of one of the regression weights is opposite to the sign of the corresponding simple correlation with $Y$. In the picture, $\beta_1$ is positive and $\beta_2$ is negative.</p>

<p>Since the length of the projection can vary between 0 and 1 no matter where it is in the predictor space, there is no minimum $R^2$ for suppression.</p>
",2013-10-14 20:20:50.680
57461,22685.0,1,,,,What is the point of measuring statistical distance?,<distance>,CC BY-SA 3.0,"<p>On <a href=""http://download.springer.com/static/pdf/305/chp%253A10.1007%252F978-3-642-22792-9_21.pdf?auth66=1381954902_7a8eccbd8188fded3878a75ad24f8c83&amp;ext=.pdf"" rel=""nofollow"">pg. 378</a> of ""Cryptography with Tamperable and Leaky
Memory"", Kalai et al. claim two probability distributions are $e(k)$ close if the distance between them is at most $e(k)$.  </p>

<p>What is significance of two distributions X and Y being ""close to"" or ""far from"" each other? Why would anybody care, especially in cryptography?</p>
",2013-10-14 20:32:28.583
57462,22143.0,2,,57452.0,,,,CC BY-SA 3.0,"<p><em>Try 1</em>:</p>

<p>If $X \sim U[a,b]$ (uniform, either discrete or continuous), then the MLE estimator for b (which is $\max_{x \in [a,b]} X$) is essentially $\max_{i=1,...,N_s}x_i$.</p>

<p>I chose uniform distribution because it is the worst case distribution in terms of entropy. This is in line with the MaxEnt (maximum entropy) principle. I also assumed a linear order in the values of the random variable.</p>

<p>We can make the following claim about the estimator $\max_{i=1,...,N_s}x_i$ to <em>its</em> mean using Hoeffdings inequality (without assuming that $X \sim U[a,b]$). Assuming $x_i$ are i.i.d from some distribution with bounded support $[a,b]$, we have
\begin{align*}
\mathbb{P}_{x_1,...,x_{N_s}}\left(|\max_{i=1,...,N_s}x_i - \mathbb{E}[\max_{i=1,...,N_s}x_i]| \geq \epsilon\right) \leq 2\exp\left(\frac{-2\epsilon^2}{N_s(b-a)}\right)
\end{align*}
Here we do not need to know $b$ exactly, any rough or crude upper bound will suffice. The above concentration is only saying that the estimator is close to the expected value of the estimator which is not the same as being close to the unknown $\max_{x \in [a,b]}X = b$.</p>

<p><em>Additional comment</em>: I would make the measurements uniformly at random over the plane/chip so that hopefully no region with high $X$ values is missed. This observation is independent of the above.</p>
",2013-10-14 21:10:03.567
57463,22687.0,1,58052.0,,,How does one generate the table mapping t-test values to p values?,<hypothesis-testing><t-test><p-value>,CC BY-SA 3.0,"<p>In the dark ages, we would map the results of a Student's t-test to a null hypothesis probability <em>p</em> by looking up <em>T</em> and degrees of freedom in a table to get an approximate result. </p>

<p>What is the mathematical algorithm that generates that table? <em>ie</em>, how can I write a function to generate a precise <em>p</em> given an arbitrary <em>T</em> and <em>df</em>?</p>

<p>The reason I ask is that I'm writing a piece of embedded software that continually monitors hundreds of populations with hundreds of samples each, and raises an alert if successive snapshots of a given population come to differ significantly. Currently it uses a crude <em>z</em>-score comparison, but it would be nice to use a more valid test. </p>
",2013-10-14 21:14:24.377
57464,22507.0,2,,57455.0,,,,CC BY-SA 3.0,"<p>The algorithm which never received 0's will be grossly biased and predict amost exclusively 1's. </p>
",2013-10-14 21:19:18.890
57465,22143.0,2,,57455.0,,,,CC BY-SA 3.0,"<p>I am thinking of the following two points:</p>

<ul>
<li><p>You are observing the true labels and their associated predictors, a.k.a the pair $y_i,x_i$ only when the algorithm is predicting a label of $1$. The algorithm is updated regardless of whether it made an error or not. This means that there is no feedback on mistakes (like in online learning). We get new data irrespective of our prediction performance.</p></li>
<li><p>The question we need to ask is then: <em>Does the algorithm's output influence the data source?</em> If the algorithm is not influencing the source, then this aspect where we 'conditionally observe new data' will not bias the algorithm by itself (everything else held constant).</p></li>
</ul>
",2013-10-14 21:24:50.677
57466,22690.0,1,,,,Naive Bayes with invalid independence assumption,<classification><naive-bayes><non-independent>,CC BY-SA 3.0,"<p>I'm trying to understand the effects of adding non-conditionally independent features to a naive Bayes classifier. Let's say I have the features vector $X = [x_1,x_2,x_3,x_4]$ and that for each value of $x_3$ I get the same value for $x_4$:</p>

<p>For all $i \in \{samples\}$, $x_{3}^{i} = x_{4}^{i}$</p>

<p>I could say that the conditionally independent assumption of $x_n$ given the class $Y = y_k$ does not hold anymore since the value of $x_{3}^{i}$ foresee $x_{4}^{i}$, and that naive Bayes classifier may not produce the expected results. I'm not really sure about that explanation and I would appreciate your point of view about it.</p>
",2013-10-14 21:58:00.910
57467,16469.0,1,57475.0,,,How to test (and accept) that a coefficient in a linear regression model equals zero,<hypothesis-testing><statistical-significance><equivalence>,CC BY-SA 3.0,"<p>I understand that in a linear regression model like:</p>

<p>$y_i = b_0 + b_1  x_i  + \epsilon_i$</p>

<p>I can have a null and an alternative hypothesis:</p>

<p>$H_0: b_1 = 0$ and $H_1: b_1 \neq 0$. </p>

<p>And then I can reject $H_0$ or fail to reject $H_0$. But what if I want to accept that $b_1 = 0$?</p>
",2013-10-14 21:58:30.230
57468,22693.0,1,,,,Whether to log transform variable when untransformed variable has positive skew and transformed has negative skew with additional missing data?,<normal-distribution><data-transformation><missing-data>,CC BY-SA 3.0,"<p>I have performed a log transformation on my skewed data, however on my DV it went from positive skew to negative skew after the (log) transformation, further data was missing from my DV after the transformation. Please help</p>
",2013-10-14 22:01:18.457
57471,155.0,2,,57468.0,,,,CC BY-SA 3.0,"<h3>Additional missing data after log transformation</h3>

<p>If you have additional missing data after log transformation, it is likely that you have data that is less than or equal to zero. (i.e., log(0), log(-1), etc. is not defined). So if you want to use a log transformation on data with negative numbers, you need to add a constant to the raw variable so that the minimum of the resulting variable is greater than zero. So your transformation could be</p>

<p>$$\log(x + c)$$</p>

<p>where $x$ is your untransformed variable and $c = 1 - \textrm{min}(x)$.</p>

<h3>Transformation flips the skewness</h3>

<p>There is plenty of discussion on this site about when and whether transformations are useful. You might also like this <a href=""http://pareonline.net/getvn.asp?v=8&amp;n=6"" rel=""nofollow"">discussion of issues surrounding transformations</a>. In general, if a log transformation is flipping the direction of your skewness, then there is a good chance that you did not have very much skewness to begin with. To test whether the transformation makes a substantive difference with the context of multiple regression, examine your correlations, R-squares, and standardised betas before and after transformation, and see what changes you observed. In many cases you will see that it makes little difference.</p>

<p>Another point, is that the assumption pertains to the residuals of a multiple regression and not the dependent variable itself.</p>

<p>If you really care about optimising the transformation to make the variable approximate a normal distribution, then you can use the <a href=""http://en.wikipedia.org/wiki/Power_transform"" rel=""nofollow"">Box-Cox transformation</a>. Or a simpler approach is just to try a range of transformations. A common set of transformations from greater to less change is:</p>

<pre><code>-1/x^2
-1/x
log(x)
sqrt(x)
</code></pre>

<p>So if <code>log(x)</code> is transforming too much, you could try <code>sqrt(x)</code>.</p>
",2013-10-14 22:29:32.767
57472,21991.0,1,57501.0,,,Numerical example to understand Expectation-Maximization,<regression><probability><mathematical-statistics><intuition><expectation-maximization>,CC BY-SA 3.0,"<p>I am trying to get a good grasp on the EM algorithm, to be able to implement and use it. I spent a full day reading the theory and a paper where EM is used to track an aircraft using the position information coming from a radar. Honestly, I don't think I fully understand the underlying idea. Can someone point me to a numerical example showing a few iterations (3-4) of the EM for a simpler problem (like estimating the parameters of a Gaussian distribution or a sequence of a sinusoidal series or fitting a line). </p>

<p>Even if someone can point me to a piece of code (with synthetic data), I can try to step through the code. </p>
",2013-10-14 22:37:36.997
57473,5643.0,1,57477.0,,,Intuition for the standard error of the difference of sample means,<mean><standard-error><group-differences><intuition>,CC BY-SA 3.0,"<p>I read in Wilcox, 2003 p. 247 that the standard error of the difference between two sample means is (assuming the normality and homoskedasticity assumptions):</p>

<p>$\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$</p>

<p>Rather than simply adding the two sample standard errors as in:</p>

<p>$\frac{\sigma_1}{\sqrt{n_1}} + \frac{\sigma_2}{\sqrt{n_2}}$</p>

<p>What is the intuition behind taking the square of the sum of the two variances divided into their respective sample size, rather than the sum of the standard errors?</p>
",2013-10-14 23:00:43.557
57474,22163.0,1,,,,Inverted SPSS results: Logistic regression command vs. Genlin?,<regression><logistic><spss><generalized-linear-model>,CC BY-SA 3.0,"<p>I want to do a logistic regression in SPSS. However, since I analyse unemployment spells the subjects are sometimes repeated (violating the independence assumption of the regression). One way of removing the within subject variation is by applying a Genlin model with the repeated subject subcommand (in essence a GEE model). Thus, I tried out a Genlin model with binomal probability and the logit link, comparing it to a standard logistic regression. I used the exact same variables in the two procedures. </p>

<p>However, the results that was delivered from the Genlin procedure was inverted relative to that of the logistic regression. For instance: Exp(B) for women (of the independent variable sex/gender) was just above 2.0 in logistic regression while being at 0.49 in Genlin. The same happened with every independent variable.</p>

<ul>
<li>Any suggestions to why the results of the Genlin procedure is
inverted?  </li>
<li>Is there any way to get the Genlin results in accordance to the logistic regression?</li>
</ul>
",2013-10-14 23:12:42.747
57475,503.0,2,,57467.0,,,,CC BY-SA 3.0,"<p>Look into equivalence testing.  See <a href=""https://stats.stackexchange.com/search?q=equivalence%20testing"">this search</a> for lots of threads. Also see <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3019319/"" rel=""nofollow noreferrer"">Esteban &amp; Nowacki</a></p>
",2013-10-14 23:15:43.797
57476,503.0,2,,57473.0,,,,CC BY-SA 3.0,"<p>You don't square the sum of the variances, you take the square root of the sum of the variances. You do this for the same reason that the standard deviation is the square root of the variance: It make the units the same as the original ones, rather than squared units. </p>

<p>Although we often lose sight of it while doing statistics, the square of a measure involve squaring the <em>measure</em> as well as the number of units. For example, the square of 2 meters is not 4 meters, it is 4 meters squared, more commonly called 4 square meters. The same thing happens with other units that we aren't used to thinking of in this way: e.g. if you are measuring IQ, the square of an IQ is not an IQ of 10,000; it is a squared IQ of 10,000. </p>

<p>You divide by the sample size as a scaling technique. Variances (tend to) go up with sample size; you divide by $n$ to deal with that. </p>
",2013-10-14 23:21:40.057
57477,594.0,2,,57473.0,,,,CC BY-SA 3.0,"<p>You seem to be thinking that $\sqrt{\text{Var}(\bar X-\bar Y)} = \sqrt{\text{Var}(\bar X)} + \sqrt{\text{Var}(\bar Y)}$. </p>

<p>This is not the case for independent variables.</p>

<p>For $X,Y$ independent, $\text{Var}(\bar X-\bar Y) = \text{Var}(\bar X) + \text{Var}(\bar Y)$</p>

<p>Further, </p>

<p>$\text{Var}(\bar X) = \text{Var}(\frac{1}{n}\sum_iX_i) = \frac{1}{n^2}\text{Var}(\sum_iX_i)= \frac{1}{n^2}\sum_i\text{Var}(X_i)= \frac{1}{n^2}\cdot n\cdot\sigma^2_1= \sigma^2_1/n$</p>

<p>(if the $X_i$ are independent of each other).</p>

<p><a href=""http://en.wikipedia.org/wiki/Variance#Basic_properties"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Variance#Basic_properties</a></p>

<p>In summary: the correct term:</p>

<p><img src=""https://i.stack.imgur.com/TUqrV.png"" alt=""annotated se formula""></p>

<p>$\color{red}{(1)}$ has $\sigma^2/n$ terms because we're looking at averages and that's the variance of an average of independent random variables;</p>

<p>$\color{red}{(2)}$ has a $+$ because the two samples are independent, so their variances (of the averages) add; and</p>

<p>$\color{red}{(3)}$ has a square root because we want the standard deviation of the distribution of the difference in sample means (the standard error of the difference in means). The part under the bar of the square root is the variance of the difference (the square of the standard error). Taking square roots of squared standard errors gives us standard errors. </p>

<p>The reason why we don't just add standard errors is <em>standard errors don't add</em> - the standard error of the difference in means is NOT the sum of the standard errors of the sample means for independent samples - the sum will always be too large. The variances <em>do</em> add, though, so we can use that to work out the standard errors.</p>

<hr>

<p>Here's some intuition about why it's not standard deviations that add, rather than variances.</p>

<p>To make things a little simpler, just consider <em>adding</em> random variables.</p>

<p>If $Z = X+Y$, why is $\sigma_Z &lt; \sigma_X+\sigma_Y$?</p>

<p>Imagine $Y = kX$ (for $k\neq 0$); that is, $X$ and $Y$ are perfectly linearly dependent. That is, they always 'move together' in the same direction and in proportion.</p>

<p>Then $Z = (k+1)X$ - which is simply a rescaling. Clearly $\sigma_Z = (k+1)\sigma_X = \sigma_X+\sigma_Y$. </p>

<p>That is, when $X$ and $Y$ are perfectly positively linearly dependent, always moving up or down together, <em>standard deviations add</em>.</p>

<p>When they don't always move up or down together, sometimes they move opposite directions. That means that their movements partly 'cancel out', yielding a smaller standard deviation than the direct sum.</p>
",2013-10-14 23:40:17.447
57478,7860.0,1,,,,Adding errors to Gaussian kernel density estimator,<python><density-function><error><kde>,CC BY-SA 3.0,"<p>I'm using the <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html"" rel=""nofollow"">scipy.stats.gaussian_kde</a> function to generate a <code>KDE</code> from a set of $N$ points in a 2D space: $A = \{(x_1,y_1), (x_2,y_2), (x_3,y_2), ..., (x_N,y_N)\}$</p>

<p>Each one of these points has a given error attached to it. So for example, the point $(x_1,y_1)$ has errors $(e_{x_1},e_{y_1})$ and so on. I can assume the errors are normally distributed in both axis.</p>

<p>The <code>python</code> function that I use to generate the <code>KDE</code> has no way to integrate these errors into the calculations and I wonder how I would even do such a thing if I did it manually.</p>

<p>Ie: what is the statistically correct way to generate a <code>KDE</code> accounting for errors in the data used?</p>
",2013-10-15 00:04:46.003
57480,2121.0,2,,56955.0,,,,CC BY-SA 3.0,"<p>I think if each dataset is already weighted to your satisfaction, then you have a couple of different options. Which one is the right one may vary based on your objectives and the particulars of your existing data collection and weighting. </p>

<ul>
<li>(#1) Union all of the datasets, along with their pre-calculated weights, and that's it.</li>
</ul>

<blockquote>
  <p>This would be the right choice if each dataset was weighted towards a proper total count and didn't over-state the importance of any individual record relative to another dataset. If one dataset was weighted to reflect Total US Population, and another dataset was weighted in place to its own total count of respondents, then this would not be the right choice.</p>
</blockquote>

<ul>
<li>(#2) Calculate a weight for each dataset to multiply by each record's existing weight</li>
</ul>

<blockquote>
  <p>This would be the right choice if each of your datasets are of equal importance regardless of their size. Example below...</p>
</blockquote>

<ul>
<li>(#3) Union all of the raw data and re-calculate the weights on the new, entire dataset</li>
</ul>

<blockquote>
  <p>This would be the right choice if the reasons for non-response are similar across your different surveys - it results in the simplest data for you to work with, and it's the least likely to produce extreme weights.</p>
</blockquote>

<p>Example for #2: each dataset is weighted to equal importance, with this ""dataset weight"" being multiplied by whatever weight has already been calculated within the dataset. </p>

<pre><code>&gt; Survey 1: 100 people   weight:  2
&gt; Survey 2: 200 people   weight:  1
&gt; Survey 3: 300 people   weight:  2/3
&gt; Survey 4: 150 people   weight:  4/3
&gt; Survey 5: 250 people   weight:  4/5
</code></pre>
",2013-10-15 01:07:38.337
57481,14548.0,1,,,,Combining prediction intervals in regression,<regression><confidence-interval>,CC BY-SA 4.0,"<p>Having performed a linear regression, I can find the confidence interval for the response conditioned on a particular x value. However, I am interested in a C.I for the <em>mean</em> response for a set of N new observations. That is, I need to combine the N prediction intervals. </p>

<p>The closest post I could find was <a href=""https://stats.stackexchange.com/questions/8755/calculating-the-mean-using-regression-data"">Calculating the mean using regression data</a>, but it only handles the univariate case.</p>

<p>I tried deriving the standard error of the mean response below, but I'm not sure if this correct.</p>

<p>$\begin{align}
var(\hat{\bar{y}}) &amp;= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_1 \ldots x_n \right) \\
&amp;= var \left( \frac{1}{n} \sum_i \hat{y}_i|x_i \right), \quad \text{where the } \hat{y_i}|x_i \text{ are independent} \\
&amp;= \frac{1}{n^2}  \sum_i var(\hat{y}_i|x_i) \\
\end{align}$ </p>

<p>where $var(\hat{y}_i|x_i) = \sqrt{\sigma^2 x_i^T (X^TX)^{-1}x_i}$ for $x_i$ in the training data and $var(\hat{y}_i|x^*_i) = \sqrt{\sigma^2 (1+ x_i^{*T} (X^TX)^{-1}x^*_i)}$ for $x^*_i$ in the test data.</p>

<p>Am I on the right track here? Also, is there an R implementation somewhere, or should I do it from scratch?</p>

<p>Edit: I am also reading up on Bayesian regression methods which specify the predictive distribution $P(y_i|x_i^*)$, and a credible interval for the response. We face a similar problem here, namely, how to compute the predictive distribution for the mean response $P(\overline{y}|x_1^* \ldots x_n^*)$?  </p>
",2013-10-15 01:08:53.047
57482,22695.0,1,,,,Finding the full conditonal distribution when there are multiple distributions involved,<bayesian><poisson-distribution><conditional-probability><prior><posterior>,CC BY-SA 3.0,"<p>6 neighboring countries have the following disease instances: $y = (y_1, y_2,...,y_n)$ with a population of $x = (x_1, x_2,...,x_n)$.</p>

<p>The following model and prior distributions are considered:</p>

<p>$y_i|\theta_i,p_i  \sim \text{Poisson}(\theta_i x_i)$</p>

<p>$\theta_i | \alpha, \beta \sim \text{gamma}(\alpha, \beta)$</p>

<p>$\alpha \sim \text{gamma}(1,1)$</p>

<p>$\beta \sim \text{gamma}(10,1)$</p>

<p>a) Find the full conditional rate $p(\theta_i | \theta_{-i}, \alpha, \beta, x, y)$</p>

<p>b) Find the posterior distribution.</p>

<p>Attempt: </p>

<p>a) For finding the conditional rate with two variables, I would use Bayes' theory. I am not sure if this applies with multiple distributions.</p>

<p>$$p(\theta_i | \theta_{-i}, \alpha, \beta, x, y) = \frac{P(\theta_i \bigcap \theta_{-i} \bigcap \alpha \bigcap \beta \bigcap x \bigcap y)}{P( \theta_{-i}, \alpha, \beta, x, y)}$$</p>

<p>$$ = \frac{P(\theta_{-i}, \alpha, \beta, x, y | \theta_i)P(\theta_i)}{\sum_{i=1}^6 P(\theta_{-i}, \alpha, \beta, x, y | \theta_i)P(\theta_i)}$$</p>

<p>b) The posterior probability is the (prior)x(likelihood). So this would be $$\text{Poisson}(\theta_i x_i) \times L(\theta_i x_i)$$</p>

<p>I'm not sure how to do the pdf of a Poisson variable as it is variable. The likelihood function is $L(\theta_i y_i) = \frac{\theta_i^{\sum_{i=1}^n y_i} e^{-n \theta_i}}{y_1!,y_2!,..,y_n!}$</p>
",2013-10-15 01:09:29.977
57483,22659.0,1,,,,Scikit-learn's Gaussian Processes: How to include multiple hyperparameters in kernel/cov function?,<machine-learning><gaussian-process><scikit-learn>,CC BY-SA 3.0,"<p>I'm using the scikit-learn's implementation of Gaussian processes. A simple thing to do is to combine multiple kernels as a linear combination to describe your time series properly. So I'd like to include both the squared exponential kernel and the periodic kernel. Linear combinations of valid kernels produce valid kernels, and same goes for multiplying valid kernels (given by Rasmussen and Williams).</p>

<p>Unfortunately I haven't figured out how to give the theta parameters properly to the model. For example, if we have:</p>

<p>$$
k_{Gauss}(x,x') = \exp{(\theta (x-x')^2)}
$$</p>

<p>then it is alright (this is how the squared-exponential kernel is defined in scikit-learn). But if I wanted:</p>

<p>$$
k_{Gauss}(x,x') = \theta_0 \exp{(\theta_1 (x-x')^2)}
$$</p>

<p>then it is impossible, it seems. The $\mathbf{\theta}$ thing is supposed to be an array, in case you have multiple dimensions/features (even though scikit-learn doesn't support multidimensional GPs, someone developed it, and it will be merged soon). So there is one row with the columns being the parameter in such-and-such dimension. But you cannot have more rows, otherwise it screams at you.</p>

<p>So question: has anyone actually been able to use kernels that use more than one hyperparameter? If so, what am I doing wrong? And if it is indeed not possible with the current code in scikit, does anyone have some tips on how to extend it so that it can? This is a really important feature that I need. Thanks.</p>
",2013-10-15 01:19:13.100
57484,22698.0,1,,,,Bound for the correlation of three random variables,<correlation><correlation-matrix>,CC BY-SA 3.0,"<p>There are three random variables, $x,y,z$. The three correlations between the three variables are the same. That is,</p>

<p>$$\rho=\textrm{cor}(x,y)=\textrm{cor}(x,z)=\textrm{cor}(y,z)$$</p>

<p>What is the tightest bound you can give for $\rho$?</p>
",2013-10-15 01:55:03.623
57503,5637.0,2,,57497.0,,,,CC BY-SA 3.0,"<p>The names of the parameters are suggestive. Location, and scale parameters are associated with central tendency, dispersion respectively. For eg: If you change location parameters, mostly it change only the central tendency measures. </p>

<p>Try this online tool. <a href=""http://socr.ucla.edu/htmls/SOCR_Distributions.html"" rel=""noreferrer"">Distributions</a></p>

<p>See how the distribution changes for different values of parameters. You could try this with generalized extreme value distribution.</p>

<p>Not all standard distributions have all three parameters. Some distributions have only one or two of the parameters (eg: gamma distribution-shape and scale parameters) </p>
",2013-10-15 07:33:28.403
57485,7155.0,2,,57483.0,,,,CC BY-SA 3.0,"<p>On scikit-learn==0.14.1.</p>

<p>$\theta_0$ can be a vector. The following code works for me.</p>

<pre><code>import numpy as np
from sklearn.gaussian_process import GaussianProcess
from sklearn.datasets import make_regression
X, y = make_regression()
bad_theta = np.abs(np.random.normal(0,1,100))
model = GaussianProcess(theta0=bad_theta)
model.fit(X,y)
</code></pre>

<p>You can pass any kernel you want as the parameter corr. The following is the radial basis function that sklearn uses for Gaussian processes.</p>

<pre><code>def squared_exponential(theta, d):
    """"""
    Squared exponential correlation model (Radial Basis Function).
    (Infinitely differentiable stochastic process, very smooth)::

                                            n
        theta, dx --&gt; r(theta, dx) = exp(  sum  - theta_i * (dx_i)^2 )
                                        i = 1

    Parameters
    ----------
    theta : array_like
        An array with shape 1 (isotropic) or n (anisotropic) giving the
        autocorrelation parameter(s).

    dx : array_like
        An array with shape (n_eval, n_features) giving the componentwise
        distances between locations x and x' at which the correlation model
        should be evaluated.

    Returns
    -------
    r : array_like
        An array with shape (n_eval, ) containing the values of the
        autocorrelation model.
    """"""

    theta = np.asarray(theta, dtype=np.float)
    d = np.asarray(d, dtype=np.float)

    if d.ndim &gt; 1:
        n_features = d.shape[1]
    else:
        n_features = 1

    if theta.size == 1:
        return np.exp(-theta[0] * np.sum(d ** 2, axis=1))
    elif theta.size != n_features:
        raise ValueError(""Length of theta must be 1 or %s"" % n_features)
    else:
        return np.exp(-np.sum(theta.reshape(1, n_features) * d ** 2, axis=1))
</code></pre>

<p>It looks like you're doing something pretty interesting, btw.</p>
",2013-10-15 01:56:23.430
57486,22677.0,1,,,,How does one determine what ARL0 should be used on CPM package to test for Structural Change,<r><time-series><structural-change>,CC BY-SA 3.0,"<p>I'm trying to find multiple break points by using <code>processStream</code> from <code>CPM</code> package on R.
Can someone enlighten me on what is <code>ARL0</code> how does one determine what <code>ARL0</code> should be used for?</p>

<pre><code>processStream(ret.fin.chn,""Kolmogorov-Smirnov"",ARL0=500,lambda=NA)
$changePoints
    [1]   59   75  250  286  443  448  663 1037 1042 1261 1576 1842 1853 2013 2035 2621 2633
    $detectionTimes
[1]   73   89  285  334  447  503  670 1040 1145 1428 1639 1951 1874 2030 2078 2632 2644
</code></pre>

<p>while </p>

<pre><code>processStream(ret.fin.chn,""Kolmogorov-Smirnov"",ARL0=2000,lambda=NA)
$changePoints
    [1]   59   75  663 1037 1261 1559 1842 2013 2035 2621 2633
    $detectionTimes
[1]   75   90  691 1041 1480 1688 2026 2032 2266 2633 2646
</code></pre>

<p>and</p>

<pre><code>processStream(ret.fin.chn,""Kolmogorov-Smirnov"",ARL0=3000,lambda=NA)
$changePoints
    [1]   59   75  663 1037 1261 1559 1842 2013 2149
    $detectionTimes
[1]   75   92  692 1041 1490 1690 2026 2032 2284
</code></pre>

<p>Tt seems that different <code>ARL0</code> will give fewer break point detection, is that a good thing? </p>

<p><strong>Note</strong>: the time series <code>ret.fin.chn</code> contains 2749 rows.</p>

<p>Below are excerpts from <code>R</code> help:</p>

<blockquote>
  <p>ARL0</p>
  
  <p>Determines the <code>ARL_0</code> which the CPM should have, which corresponds to the average number of observations before a false positive occurs, assuming that the sequence does not undergo a chang. Because the thresholds of the CPM are computationally expensive to estimate, the package contains pre-computed values of the thresholds corresponding to several common values of the <code>ARL_0</code>. This means that only certain values for the <code>ARL_0</code> are allowed. Specifically, the <code>ARL_0</code> must have one of the following values: 370, 500, 600, 700, ..., 1000, 2000, 3000, ..., 10000, 20000, ..., 50000</p>
</blockquote>
",2013-10-15 02:10:01.050
57487,13037.0,1,62181.0,,,Weighted Least Squares Estimate,<self-study><least-squares>,CC BY-SA 3.0,"<p>Here is a problem from a practice test. Suppose that $$X_i = \mu + \epsilon_i,\quad i=1,\ldots,n\quad \epsilon_i\sim N(0,\sigma^2_1)$$ $$Y_i = \mu + \delta_i,\quad i=1,\ldots,m\quad \delta_i\sim N(0,\sigma^2_2)$$ All $\epsilon_i$'s and $\delta_i$'s are independent. The paramters $\mu, \sigma_1^2, $ and $\sigma_2^2$ are unknown. Let $\theta=m/n$, $\rho=\sigma_2^2/\sigma_1^2$. Suppose $\rho$ is known. Show that the least squares (weighted) estimator of $\mu$ is $$ \hat{\mu} = \dfrac{\rho\bar{X} + \theta\bar{Y}}{\rho+\theta}$$</p>

<p>MY ATTEMPT:</p>

<p>I can't figure out how to use the fact that $\rho$ is known. I tried $$\hat{\mu} = \text{argmin}\left\{\sum_{i=1}^n (X_i-\mu)^2 + \sum_{i=1}^m (Y_i-\mu)^2\right\}$$ and arrived that the weighted averaged $$ \hat{\mu} = \dfrac{n\bar{X} + m\bar{Y}}{n+m}$$ But again this does not use the fact that we know what the ratio $\sigma_2^2/\sigma_1^2$ is. Any ideas?</p>
",2013-10-15 02:16:00.033
57488,17730.0,1,57495.0,,,How to express joint conditional probability with multiple conditions,<conditional-probability><joint-distribution>,CC BY-SA 3.0,"<p>I want to express the joint probability of $\Phi_A$ and $\Phi_B$: $p(\Phi_A, \Phi_B)$  conditioned that $\Phi_A$ and $\Phi_B$ are both greater than some value C.  How would I express this mathematically?  I guess my intuition says:</p>

<p>$p(\Phi_A, \Phi_B | \bf{\Phi} &gt;C)$</p>

<p>Is this correct? Is there a better way to express this?</p>
",2013-10-15 02:45:34.410
57489,4656.0,2,,57484.0,,,,CC BY-SA 3.0,"<p>The common correlation $\rho$ can have value $+1$ but not $-1$. If $\rho_{X,Y}= \rho_{X,Z}=-1$, then $\rho_{Y,Z}$ cannot equal $-1$ but is in fact $+1$.
The smallest value of the common correlation of three random variables
is $-\frac{1}{2}$. More generally,
the minimum common correlation of $n$ random variables is $-\frac{1}{n-1}$
when, regarded as vectors, they are at the vertices of a simplex (of dimension $n-1$)
in $n$-dimensional space.</p>

<p>Consider the variance of the sum of
$n$ unit variance random variables $X_i$.  We have that
$$\begin{align*}
\operatorname{var}\left(\sum_{i=1}^n X_i\right)
&amp;= \sum_{i=1}^n \operatorname{var}(X_i) + \sum_{i=1}^n\sum_{j\neq i}^n \operatorname{cov}(X_i,X_j)\\
&amp;= n + \sum_{i=1}^n\sum_{j\neq i}^n \rho_{X_i,X_j}\\
&amp;= n + n(n-1)\bar{\rho} \tag{1}
\end{align*}$$
where $\bar{\rho}$ is the <em>average value</em> of the $\binom{n}{2}$correlation coefficients.
But since $\operatorname{var}\left(\sum_i X_i\right) \geq 0$, 
we readily get from
$(1)$ that 
$$\bar{\rho} \geq -\frac{1}{n-1}.$$</p>

<p>So, the average value of a correlation coefficient is
<em>at least</em> $-\frac{1}{n-1}$. If <em>all</em> the correlation coefficients
have the <em>same</em> value $\rho$, then their average also
equals $\rho$ and so we have that 
$$\rho \geq -\frac{1}{n-1}.$$
Is it possible to have random variables for which the common
correlation value $\rho$  <em>equals</em>
$-\frac{1}{n-1}$? Yes.  Suppose that the $X_i$ are <em>uncorrelated</em> 
unit-variance random variables  and set
$Y_i = X_i - \frac{1}{n}\sum_{j=1}^n  X_j = X_i -\bar{X}$. Then, $E[Y_i]=0$, while
$$\displaystyle \operatorname{var}(Y_i) 
= \left(\frac{n-1}{n}\right)^2 + (n-1)\left(\frac{1}{n}\right)^2
= \frac{n-1}{n}$$ 
and
$$\operatorname{cov}(Y_i,Y_j) = -2\left(\frac{n-1}{n}\right)\left(\frac{1}{n}\right) +
(n-2)\left(\frac{1}{n}\right)^2 = -\frac{1}{n}$$
giving 
$$\rho_{Y_i,Y_j} 
= \frac{\operatorname{cov}(Y_i,Y_j)}{\sqrt{\operatorname{var}(Y_i)\operatorname{var}(Y_j)}}
=\frac{-1/n}{(n-1)/n} 
= -\frac{1}{n-1}.$$
Thus the $Y_i$ are random variables achieving the minimum common
correlation value of $-\frac{1}{n-1}$. Note, incidentally, that
$\sum_i Y_i = 0$, and so, regarded as vectors, the random variables
lie in a $(n-1)$-dimensional hyperplane of $n$-dimensional space.</p>
",2013-10-15 02:58:22.840
57490,22677.0,2,,15281.0,,,,CC BY-SA 3.0,"<p>@Dail if you're more inclined to the applied rather than the theoretical behind detection of structural break, you might want try <code>http://cran.r-project.org/web/packages/cpm/index.html</code> this is the link for <code>CPM</code> package of <code>R</code>, where you can use <code>processStream</code> to find multiple break point in your time series. </p>
",2013-10-15 03:22:22.863
57491,668.0,2,,57484.0,,,,CC BY-SA 3.0,"<p><strong>The tightest possible bound is $-1/2 \le \rho \le 1$.</strong>  All such values can actually appear--none are impossible.</p>

<p>To show there is nothing especially deep or mysterious about the result, this answer first presents a completely elementary solution, requiring only the obvious fact that variances--being the expected values of squares--must be non-negative.  This is followed by a general solution (which uses slightly more sophisticated algebraic facts).</p>

<h2>Elementary solution</h2>

<p><strong>The variance of any linear combination of $x,y,z$ must be non-negative.</strong>  Let the variances of these variables be $\sigma^2, \tau^2,$ and $\upsilon^2$, respectively.  All are nonzero (for otherwise some of the correlations would not be defined).  Using the basic properties of variances we may compute</p>

<p>$$0 \le \text{Var}(\alpha x/\sigma + \beta y/\tau + \gamma z/\upsilon) = \alpha^2 +\beta^2+\gamma^2 + 2\rho(\alpha\beta+\beta\gamma+\gamma\alpha)$$</p>

<p>for all real numbers $(\alpha, \beta, \gamma)$.</p>

<p>Assuming $\alpha+\beta+\gamma\ne 0$, a little algebraic manipulation implies this is equivalent to</p>

<p>$$\frac{-\rho}{1-\rho} \le \frac{1}{3} \left(\frac{\sqrt{(\alpha^2+\beta^2+\gamma^2)/3}}{(\alpha+\beta+\gamma)/3}\right)^2.$$</p>

<p>The squared term on the right hand side is the ratio of two power means of $(\alpha, \beta, \gamma)$.  The <a href=""http://en.wikipedia.org/wiki/Generalized_mean#Inequality_between_any_two_power_means"" rel=""noreferrer"">elementary power-mean inequality</a> (with weights $(1/3, 1/3, 1/3)$) asserts that ratio cannot exceed $1$ (and will equal $1$ when $\alpha=\beta=\gamma\ne 0$).  A little more algebra then implies</p>

<p>$$\rho \ge -1/2.$$</p>

<p>The explicit example of $n=3$ below (involving trivariate Normal variables $(x,y,z)$) shows that all such values, $-1/2 \le \rho \le 1$, actually do arise as correlations.  This example uses only the definition of multivariate Normals, but otherwise invokes no results of Calculus or Linear Algebra.</p>

<h2>General solution</h2>

<h3>Overview</h3>

<p>Any correlation matrix is the covariance matrix of the standardized random variables, whence--like all correlation matrices--it must be positive semi-definite.  Equivalently, its eigenvalues are non-negative.  This imposes a simple condition on $\rho$: it must not be any less than $-1/2$ (and of course cannot exceed $1$).  Conversely, any such $\rho$ actually corresponds to the correlation matrix of some trivariate distribution, proving these bounds are the tightest possible.</p>

<hr>

<h3>Derivation of the conditions on $\rho$</h3>

<p>Consider the $n$ by $n$ correlation matrix with all off-diagonal values equal to $\rho.$ (The question concerns the case $n=3,$ but this generalization is no more difficult to analyze.)  Let's call it $\mathbb{C}(\rho, n).$  By definition, $\lambda$ is an eigenvalue of provided there exists a nonzero vector $\mathbf{x}_\lambda$ such that</p>

<p>$$\mathbb{C}(\rho,n) \mathbf{x}_\lambda = \lambda \mathbf{x}_\lambda.$$</p>

<p>These eigenvalues are easy to find in the present case, because </p>

<ol>
<li><p>Letting $\mathbf{1} = (1, 1, \ldots, 1)'$, compute that</p>

<p>$$\mathbb{C}(\rho,n)\mathbf{1} = (1+(n-1)\rho)\mathbf{1}.$$</p></li>
<li><p>Letting $\mathbf{y}_j = (-1, 0, \ldots, 0, 1, 0, \ldots, 0)$ with a $1$ only in the $j^\text{th}$ place (for $j = 2, 3, \ldots, n$), compute that</p>

<p>$$\mathbb{C}(\rho,n)\mathbf{y}_j = (1-\rho)\mathbf{y}_j.$$</p></li>
</ol>

<p>Because the $n$ eigenvectors found so far span the full $n$ dimensional space (proof: an easy row reduction shows the absolute value of their determinant equals $n$, which is nonzero), they constitute a basis of <em>all</em> the eigenvectors.  We have therefore found all the eigenvalues and determined they are either $1+(n-1)\rho$ or $1-\rho$ (the latter with multiplicity $n-1$).  In addition to the well-known inequality $-1 \le \rho \le 1$ satisfied by all correlations, non-negativity of the first eigenvalue further implies</p>

<p>$$\rho \ge -\frac{1}{n-1}$$</p>

<p>while the non-negativity of the second eigenvalue imposes no new conditions.</p>

<hr>

<h3>Proof of sufficiency of the conditions</h3>

<p>The implications work in both directions: provided $-1/(n-1)\le \rho \le 1,$ the matrix $\mathbb{C}(\rho, n)$ is nonnegative-definite and therefore is a valid correlation matrix.  It is, for instance, the correlation matrix for a multinormal distribution.  Specifically, write</p>

<p>$$\Sigma(\rho, n) = (1 + (n-1)\rho)\mathbb{I}_n - \frac{\rho}{(1-\rho)(1+(n-1)\rho)}\mathbf{1}\mathbf{1}'$$</p>

<p>for the inverse of $\mathbb{C}(\rho, n)$ when $-1/(n-1) \lt \rho \lt 1.$  For example, when $n=3$</p>

<p>$$\color{gray}{\Sigma(\rho, 3) = \frac{1}{(1-\rho)(1+2\rho)} \left(
\begin{array}{ccc}
 \rho +1 &amp; -\rho  &amp; -\rho  \\
 -\rho  &amp; \rho +1 &amp; -\rho  \\
 -\rho  &amp; -\rho  &amp; \rho +1 \\
\end{array}
\right)}.$$</p>

<p>Let the vector of random variables $(X_1, X_2, \ldots, X_n)$ have distribution function</p>

<p>$$f_{\rho, n}(\mathbf{x}) = \frac{\exp\left(-\frac{1}{2}\mathbf{x}\Sigma(\rho, n)\mathbf{x}'\right)}{(2\pi)^{n/2}\left((1-\rho)^{n-1}(1+(n-1)\rho)\right)^{1/2}}$$</p>

<p>where $\mathbf{x} = (x_1, x_2, \ldots, x_n)$. For example, when $n=3$ this equals</p>

<p>$$\color{gray}{\frac{1}{\sqrt{(2\pi)^{3}(1-\rho)^2(1+2\rho)}}
\exp\left(-\frac{(1+\rho)(x^2+y^2+z^2) - 2\rho(xy+yz+zx)}{2(1-\rho)(1+2\rho)}\right)}.$$</p>

<p>The correlation matrix for these $n$ random variables is $\mathbb{C}(\rho, n).$</p>

<p><img src=""https://i.stack.imgur.com/Fm2r6.png"" alt=""Figure""></p>

<p><em>Contours of the density functions $f_{\rho,3}.$  From left to right, $\rho=-4/10, 0, 4/10, 8/10$.  Note how the density shifts from being concentrated near the plane $x+y+z=0$ to being concentrated near the line $x=y=z$.</em></p>

<p>The special cases $\rho = -1/(n-1)$ and $\rho = 1$ can also be realized by <em>degenerate</em> distributions; I won't go into the details except to point out that in the former case the distribution can be considered supported on the hyperplane $\mathbf{x}.\mathbf{1}=0$, where it is a sum of identically distributed mean-$0$ Normal distribution, while in the latter case (perfect positive correlation) it is supported on the line generated by $\mathbf{1}'$, where it has a mean-$0$ Normal distribution.</p>

<hr>

<h3>More about non-degeneracy</h3>

<p>A review of this analysis makes it clear that the correlation matrix $\mathbb{C}(-1/(n-1), n)$ has a rank of $n-1$ and $\mathbb{C}(1, n)$ has a rank of $1$ (because only one eigenvector has a nonzero eigenvalue).  For $n\ge 2$, this makes the correlation matrix degenerate in either case.  Otherwise, the existence of its inverse $\Sigma(\rho, n)$ proves it is nondegenerate.</p>
",2013-10-15 04:52:22.687
57492,20130.0,1,,,,Good econometrics handbooks?,<econometrics><references>,CC BY-SA 3.0,"<p>There's a question for <a href=""https://stats.stackexchange.com/questions/4612/good-econometrics-textbooks"">textbooks</a> here, but I would like to ask similar question about handbooks: what econometrics handbooks would you recommend?</p>

<p>Assumed audience is researchers and graduate-level students. It needs to include the material of Greene's <em>Econometric Analysis</em> and Wooldridge's <em>Econometric Analysis of Cross Section and Panel Data</em> in a denser form (without discussions and proofs), as well as mainstream techniques not mentioned in these two books.</p>

<p>One option is Elsevier's 6 volumes of <em>Handbook of Econometrics</em> series edited by Griliches and Instriligator. However, perhaps you would recommend other handbooks, more concise or otherwise?</p>

<p>I suggest two possible formats: one is a reference-card format with minimum explanations, and the other one is a more extended format with proofs and more detailed exposition.</p>
",2013-10-15 04:59:24.173
57493,3183.0,2,,57472.0,,,,CC BY-SA 3.0,"<p>It sounds like your question has two parts: the underlying idea and a concrete example.  I'll start with the underlying idea, then link to an example at the bottom.</p>
<hr />
<p>EM is useful in Catch-22 situations where it seems like you need to know <span class=""math-container"">$A$</span> before you can calculate <span class=""math-container"">$B$</span> and you need to know <span class=""math-container"">$B$</span> before you can calculate <span class=""math-container"">$A$</span>.</p>
<p>The most common case people deal with is probably mixture distributions. For our example, let's look at a simple Gaussian mixture model:</p>
<blockquote>
<p>You have two different univariate Gaussian distributions with different means and unit variance.</p>
<p>You have a bunch of data points, but you're not sure which points came from which distribution, and you're also not sure about the means of the two distributions.</p>
</blockquote>
<p>And now you're stuck:</p>
<ul>
<li><p>If you knew the true means, you could figure out which data points came from which Gaussian.  For example, if a data point had a very high value, it probably came from the distribution with the higher mean.  But you don't know what the means are, so this won't work.</p>
</li>
<li><p>If you knew which distribution each point came from, then you could estimate the two distributions' means using the sample means of the relevant points.  But you don't actually know which points to assign to which distribution, so this won't work either.</p>
</li>
</ul>
<p>So neither approach seems like it works: you'd need to know the answer before you can find the answer, and you're stuck.</p>
<p><em><strong>What EM lets you do is alternate between these two tractable steps instead of tackling the whole process at once.</strong></em></p>
<p>You'll need to start with a guess about the two means (although your guess doesn't necessarily have to be very accurate, you do need to start somewhere).</p>
<p>If your guess about the means was accurate, then you'd have enough information to carry out the step in my first bullet point above, and you could (probabilistically) assign each data point to one of the two Gaussians.  Even though we know our guess is wrong, let's try this anyway. And then, given each point's assigned distributions, you could get new estimates for the means using the second bullet point. It turns out that, each time you do loop through these two steps, you're improving a lower bound on the model's likelihood.</p>
<p>That's already pretty cool: even though the two suggestions in the bullet points above didn't seem like they'd work individually, you can still use them together to improve the model.  The <em><strong>real</strong></em> magic of EM is that, after enough iterations, the lower bound will be so high that there won't be any space between it and the local maximum.  As a result, and you've locally optimized the likelihood.</p>
<p>So you haven't just <em>improved</em> the model, you've found the <em>best</em> possible model one can find with incremental updates.</p>
<hr />
<p><a href=""http://en.wikipedia.org/wiki/File:Em_old_faithful.gif"" rel=""noreferrer"">This</a> page from Wikipedia shows a slightly more complicated example (two-dimensional Gaussians and unknown covariance), but the basic idea is the same.  It also includes well-commented <code>R</code> code for implementing the example.</p>
<p>In the code, the &quot;Expectation&quot; step (E-step) corresponds to my first bullet point: figuring out which Gaussian gets responsibility for each data point, given the current parameters for each Gaussian.  The &quot;Maximization&quot; step (M-step) updates the means and covariances, given these assignments, as in my second bullet point.</p>
<p>As you can see in the animation, these updates quickly allow the algorithm to go from a set of terrible estimates to a set of very good ones: there really do seem to be two clouds of points centered on the two Gaussian distributions that EM finds.</p>
",2013-10-15 05:25:55.930
57494,155.0,2,,57473.0,,,,CC BY-SA 3.0,"<h3>Algebraic intuition</h3>

<p>The standard error of the mean for $n$ independent observations is $\frac{\sigma}{\sqrt{n}}$ where $\sigma$ is the standard deviation.</p>

<p>So if we have two independent samples we have the standard errors for the means of group 1 and group 2.</p>

<p>$$\sigma_{\mu_1}=\frac{\sigma_1}{\sqrt{n_1}}$$
$$\sigma_{\mu_2}=\frac{\sigma_2}{\sqrt{n_2}}$$</p>

<p>If we square these values we get the variance of the mean:</p>

<p>$$\sigma^2_{\mu_1}=\frac{\sigma^2_1}{n_1}$$
$$\sigma^2_{\mu_2}=\frac{\sigma^2_2}{n_2}$$</p>

<p>The variance of the sum or difference of two independent random variables 
is the sum of the two variances. Thus,</p>

<p>$$\sigma^2_{\mu_1 - \mu_2} =\sigma^2_{\mu_1} + \sigma^2_{\mu_2} =  \frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2} $$ </p>

<p>So if we want the standard error of the difference we take the square root of the variance:</p>

<p>$$\sigma_{\mu_1 - \mu_2} =\sqrt{\sigma^2_{\mu_1} + \sigma^2_{\mu_2}} =  \sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}} $$ </p>

<p>So I imagine this is intuitive if the component steps are intuitive. In particular it helps if you find intuitive the idea that the variance of the sum of independent variables is the sum of the variances of the component variables.</p>

<h3>Fuzzy Intuition</h3>

<p>In terms of more general intuition, if $n_1 = n_2$ and $\sigma=\sigma_1=\sigma_2$ then the standard error of the difference between means will be $\sqrt{2}\sigma_\mu\approx 1.4\times \sigma_\mu$. It makes sense that this value of approximately 1.4 is greater than 1 (i.e., the variance of a variable after adding a constant; i.e., equivalent to one sample t-test) and less than 2 (i.e., the standard deviation of the sum of two perfectly correlated variables (with equal variance) and the standard error implied by the formula you mention: $\frac{\sigma_1}{\sqrt{n_1}} + \frac{\sigma_2}{\sqrt{n_2}}$).</p>
",2013-10-15 05:35:05.007
57495,11440.0,2,,57488.0,,,,CC BY-SA 3.0,"<p>Well, it is your choice which notation to use, but you certainly can just use logical operators:</p>

<p>$p(\Phi_A, \Phi_B \; |\; \Phi_A&gt;C \,\cap \Phi_B &gt; C)$</p>

<p>Your current notation is not clear as $\Phi$ is not defined and not obvious what it means.</p>
",2013-10-15 05:35:17.670
57496,12314.0,1,,,,Forecasting time-series ahead by multiple time horizons,<time-series><predictive-models><forecasting><autocorrelation><ordinal-data>,CC BY-SA 3.0,"<p>Suppose that I have daily data on the population of a small village, given by $Y(t)$, as well as daily data on various factors that are relevant to the size of the population in the future, given by vector $X(t)$. These explanatory variables include untransformed variables as well as features engineered to be informative over long horizons (e.g. one of the variables captures the number of deaths over the last 30 days). I have collected this data for 8 years. </p>

<p>My objective is to forecast $Y(t)$ ahead by 1,2,3,...,365 days. I expect long-run forecasts to be different to short-run forecasts. If a holiday season is coming up I might expect a downwards spike in a few months time (people visiting the city), but if someone is on their deathbed then I will expect a downwards spike in a few days.</p>

<p>Since the population is sufficiently small that $\Delta Y(t+k)$ is typically in $\{-2,-1,0,1,2\}$ for the forecasting horizon under question, I will use a multiple categorical response variable classification model that will assign probabilities to the various class labels being observed.</p>

<p>My question centers on the specific considerations I need to make when constructing forecasts of the change from $Y(t)$ to $Y(t+k)$ where $k$ is large (e.g. 100 days). </p>

<p>Basically there will be the most hideous autocorrelation structure in $\Delta Y(t+k)$ over these time scales. If someone dies on day $2$, they are also dead on day $3, 4, ..., k$, meaning a string of $k$ or so $\Delta Y(t+k)$ will contain this same information. </p>

<p>These queries result:</p>

<ul>
<li>What are some ways of dealing with this immense autocorrelation structure in my response. Is it even a problem?</li>
<li>Are there alternative methodologies to the ones I've proposed for forecasting these horizons (aside from typical machine learning methods such as random forests which I'm already working with).</li>
<li>Any other handy advice.</li>
</ul>
",2013-10-15 06:05:55.207
57497,22703.0,1,,,,Parameters of a Statistical Distribution,<distributions>,CC BY-SA 3.0,"<p>Any statistical distribution is described in terms of shape, scale and location parameters. But what do these parameters mean, geometrically, statistically and for a layman with minimum statistical knowledge?</p>

<p>I have explored wikipedia and still, this doubt continues to exist.</p>
",2013-10-15 06:36:38.530
57498,22703.0,1,57509.0,,,Motivation for statistical distributions,<distributions>,CC BY-SA 3.0,"<p>As statisticians, we come across many distributions under the banners ""discrete"" or ""continuous"", and ""univariate"" or ""multivariate"". But can anyone provide a good reason behind the existence and motivation for so many distributions? How do we get them? And what can a layman understand from it?</p>

<p>What is the logic behind the existence of distributions?</p>
",2013-10-15 06:42:30.540
57499,22703.0,2,,57472.0,,,,CC BY-SA 3.0,"<p>Well, I would suggest you to go through a book on R by Maria L Rizzo. One of the chapters contain the use of EM algorithm with a numerical example. I remember going through the code for better understanding. </p>

<p>Also, try to view it from a clustering point of view in the beginning. Work out by hand, a clustering problem where 10 observations are taken from two different normal densities. This should help.Take help from R :)</p>
",2013-10-15 07:03:26.383
57500,22703.0,1,,,,Regression methods,<regression><mathematical-statistics><modeling><nonparametric><nonlinear-regression>,CC BY-SA 3.0,"<p>What is the fundamental difference between:  </p>

<ol>
<li>Linear regression </li>
<li>Non linear regression </li>
<li>Parametric regression, and  </li>
<li>Non-parametric regression?</li>
</ol>

<p>  </p>

<p>When should we use each type? How do we know what to choose? What kind of data are required? What are the assumptions unique to each?</p>

<p>At times, if you go through papers you get to see a combination of the names above.</p>

<hr>

<p>Well, the ideas presented above have led me to the following conclusions:</p>

<p>1) Linear Regression : Regression methods associated with a linear model, linear with regard to the parameters of interest</p>

<p>2) Non-Linear Regression : Regression methods associated with a non-linear model, non linear with regard to the parameters of interest.</p>

<p>3) Parametric Regression: Regression methods associated with a linear model/non-linear model (accordingly called as Linear Parametric / Non-linear Parametric), but the basic assumptions of regression including those associated with errors have to hold truth.</p>

<p>4) Non-Parametric Regression: Regression methods associated with a linear model/non-linear model (accordingly called as Linear Non-Parametric / Non-linear Non-Parametric), but the basic assumptions of regression including those associated with errors are not true.</p>

<p>Am I right ? Is there an error or misleading idea here? Please respond.</p>
",2013-10-15 07:09:16.460
57501,20470.0,2,,57472.0,,,,CC BY-SA 3.0,"<p>This is a recipe to learn EM with a practical and (in my opinion) very intuitive 'Coin-Toss' example:  </p>

  

<ol>
<li><p>Read this short <a href=""http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf"" rel=""noreferrer"">EM tutorial paper</a> by Do and Batzoglou. This is the schema where the coin toss example is explained:</p>

<p><img src=""https://i.stack.imgur.com/mj0nb.gif"" alt=""enter image description here""></p></li>
<li><p>You may have question marks in your head, especially regarding where the probabilities in the Expectation step come from. Please have a look at the explanations on this maths stack exchange <a href=""https://math.stackexchange.com/questions/25111/how-does-expectation-maximization-work"">page</a>.</p></li>
<li><p>Look at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:   </p>

<pre class=""lang-python prettyprint-override""><code>import numpy as np
import math
import matplotlib.pyplot as plt

## E-M Coin Toss Example as given in the EM tutorial paper by Do and Batzoglou* ##

def get_binomial_log_likelihood(obs,probs):
    """""" Return the (log)likelihood of obs, given the probs""""""
    # Binomial Distribution Log PDF
    # ln (pdf)      = Binomial Coeff * product of probabilities
    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)

    N = sum(obs);#number of trials  
    k = obs[0] # number of heads
    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))
    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])
    log_lik = binomial_coeff + prod_probs

    return log_lik

# 1st:  Coin B, {HTTTHHTHTH}, 5H,5T
# 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T
# 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T
# 4th:  Coin B, {HTHTTTHHTT}, 4H,6T
# 5th:  Coin A, {THHHTHHHTH}, 7H,3T
# so, from MLE: pA(heads) = 0.80 and pB(heads)=0.45

# represent the experiments
head_counts = np.array([5,9,8,4,7])
tail_counts = 10-head_counts
experiments = zip(head_counts,tail_counts)

# initialise the pA(heads) and pB(heads)
pA_heads = np.zeros(100); pA_heads[0] = 0.60
pB_heads = np.zeros(100); pB_heads[0] = 0.50

# E-M begins!
delta = 0.001  
j = 0 # iteration counter
improvement = float('inf')
while (improvement&gt;delta):
    expectation_A = np.zeros((len(experiments),2), dtype=float) 
    expectation_B = np.zeros((len(experiments),2), dtype=float)
    for i in range(0,len(experiments)):
        e = experiments[i] # i'th experiment
          # loglikelihood of e given coin A:
        ll_A = get_binomial_log_likelihood(e,np.array([pA_heads[j],1-pA_heads[j]])) 
          # loglikelihood of e given coin B
        ll_B = get_binomial_log_likelihood(e,np.array([pB_heads[j],1-pB_heads[j]])) 

          # corresponding weight of A proportional to likelihood of A 
        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) 

          # corresponding weight of B proportional to likelihood of B
        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) 

        expectation_A[i] = np.dot(weightA, e) 
        expectation_B[i] = np.dot(weightB, e)

    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); 
    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); 

    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - 
                    np.array([pA_heads[j],pB_heads[j]]) )) )
    j = j+1

plt.figure();
plt.plot(range(0,j),pA_heads[0:j], 'r--')
plt.plot(range(0,j),pB_heads[0:j])
plt.show()
</code></pre></li>
</ol>
",2013-10-15 07:21:33.433
57502,22704.0,2,,48658.0,,,,CC BY-SA 3.0,"<p>Take a look at a post in Healthy Algorithm:
<a href=""http://healthyalgorithms.com/2011/11/23/causal-modeling-in-python-bayesian-networks-in-pymc/"" rel=""noreferrer"">http://healthyalgorithms.com/2011/11/23/causal-modeling-in-python-bayesian-networks-in-pymc/</a></p>

<p>also in PyMC's totorial:
<a href=""http://pymc-devs.github.io/pymc/tutorial.html"" rel=""noreferrer"">http://pymc-devs.github.io/pymc/tutorial.html</a></p>

<p>Maybe you would try the following code clip (assuming you have imported pymc as mc):</p>

<pre><code>A = mc.Normal('A', mu_A, tau_A)
B = mc.Normal('B', mu_B, tau_B)
p_C = mc.Lambda('p_C', lambda A=A, B=B: &lt;&lt;dependency spec goes here&gt;&gt;, doc='Pr[C|AB]')
C = mc.Bernoulli('C', p_C)
</code></pre>
",2013-10-15 07:29:18.930
57504,22678.0,2,,57500.0,,,,CC BY-SA 3.0,"<p>Basically, it depends on the function type you are trying to model from data:</p>

<ul>
<li><p>Linear. $f(x)=a_1x_2+a_2x_2+ \cdots$ where $a_i$ are the parameters of interest.</p></li>
<li><p>Nonlinear: $f(x)=x_1a_1 \frac{a_2}{a_4}+\exp(-a_2/(a_1*x_2))$ $a_i$ are also here the parameters of interest, but they form a nonlinear term now.</p></li>
<li><p>Parametric: actually, the both from top, but where you have physical/application meaning for the parameters $a_i$. e.g. splines, where the parameters of interest represent the path of a trajectory. </p></li>
<li><p>Non-Parametric: Linear model for nonlinear problems. Same as splines, but the bases are called kernels. This is good, when you have a nonlinear/complex model but would like to do some kind of model selection (which abstract $x_i$ is the most important for your data e.g.). See Kernel (ridge) regression for details on this.</p></li>
</ul>

<p>Edit: Thanks to whuber's comments.</p>
",2013-10-15 07:41:50.867
57505,22678.0,2,,57478.0,,,,CC BY-SA 3.0,"<p>You will need a robust loss function in the kernel estimation model. 
However, this topic may become quite advances very fast. :) 
For a good start, I would suggest the one class SVM from sklearn. 
<a href=""http://scikit-learn.org/stable/modules/svm.html#density-estimation-novelty-detection"" rel=""nofollow"">http://scikit-learn.org/stable/modules/svm.html#density-estimation-novelty-detection</a></p>
",2013-10-15 07:46:47.790
57506,5671.0,2,,57363.0,,,,CC BY-SA 3.0,"<p>Actually the simplest approach would be <strong>Association Rule Mining</strong>, aka Frequent Itemset Mining (FIM). ""Clustering"" is an attempt to uncover structure, but not so much to make recommendations. It's explorative, not predictive; the clusters will most often be something rather obvious to the domain expert.</p>

<p>FIM will learn rules of the form that students, which have taken class A and B, have also taken class C with x% probability, i.e.</p>

<p>$$ {A,B} \rightarrow {C} \text{ with confidence }x\%$$</p>

<p>You <em>really</em> need to go through some introductory course. APRIORI is discussed everywhere, and is an obbvious fit here. In particular as you don't have quantities to predict (you don't have users that take class A 5 times and class B 2 times and thus are likely to buy -2 times class C...) Depending on your data, FPGrowth or Eclat algorithms may be more performat though.</p>
",2013-10-15 07:47:16.070
57507,22706.0,1,,,,How do I interpret the credibility interval in a Bayesian Regularized Regression?,<bayesian><lasso><credible-interval><regularization>,CC BY-SA 3.0,"<p>A penalized regression provides biased estimates of the regression coefficients (bias-variance trade-off principle).  Therefore, standard errors and confidence intervals are regarded as not very meaningful for those biased estimates arising from (frequentist) penalized regression method, see e.g. the discussion <a href=""https://stats.stackexchange.com/questions/7225/estimating-r-squared-and-statistical-significance-from-penalized-regression-modes"">Estimating R-squared and statistical significance from penalized regression model</a>
. I would assume that the same problems exists in an Bayesian approach  but Kyung, Gill, Ghaosh and Casella (2010) say that the Bayesian formulation produces valid standard errors. Does it mean that a 95% credibility intervals includes with 95% probability the true biased estimate and if yes, is this a useful information?</p>
",2013-10-15 08:17:27.980
57508,3993.0,1,59166.0,,,Relative variances of higher-order vs. lower-order random terms in mixed models,<mixed-model><variance><references><multilevel-analysis><random-effects-model>,CC BY-SA 4.0,"<p><em><strong>TL, DR summary:</strong></em></p>
<p>Is there any theoretical or empirical basis to support the following statement being true as a general rule of thumb?</p>
<p>&quot;When estimating a mixed model, typically the estimated variances/standard deviations of random effects associated with 'higher-order' terms (e.g., random effects of two-way, three-way, and beyond interaction terms) turn out to be <em>smaller</em> than the estimated variances/standard deviations of random effects associated with 'lower-order' terms (e.g., the residual variance, variances associated with simple effects of grouping factors).&quot;</p>
<p>The source of this claim is me. ;)</p>
<hr />
<p>Okay, now for the longer version ...</p>
<p>Typically when I sit down to start analyzing a new dataset which I know will call for a mixed model, one of the first models that I fit (after the statistical foreplay of looking through the observations in the dataset, plotting various things, cross-tabulating different factors, etc.) is one that is pretty close to the &quot;maximal&quot; random effects specification, where every random effect that is in-principle possible to estimate from the data, is estimated.</p>
<p>Naturally, it is not uncommon that this nearly-maximal model will have some computational problems (convergence errors, or wacky variance/covariance estimates, or etc.) and that I have to trim back this model to find one that my data can more easily support. Fine.</p>
<p>In these situations, the method I have come to prefer for trimming random terms is not to rely on significance tests or likelihood ratios, but rather to just identify the random effects that seem to have the smallest standard deviations (which can admittedly be a little tricky when predictors are on very different scales, but I try to take account of this in my appraisal) and remove these terms first, sequentially in an iterative process. The idea being that I want to alter the predictions of the model as little as possible while still reducing the complexity of the model.</p>
<p>One pattern that I seem to have noticed after a pretty good amount of time spent doing this is that following this method very often leads me to trim random effects associated with higher-order terms (as defined above) of the model first. This is not always true, and occasionally some of the higher-order terms explain a lot of variance, but this doesn't seem to be the general pattern. In sharp contrast, I usually find that lower-order random terms -- particularly those associated with simple effects of the grouping factors -- explain a pretty good amount of variance and are fairly essential to the model. At the extreme, the residual term commonly accounts for close to the most variance, although of course removing this term wouldn't be sensible.</p>
<p>This entirely informal observation leads me to form the hypothesis that I stated at the beginning of this question.</p>
<p>If it is true, then it constitutes a useful piece of advice that might be passed down to people who are less experience with this kind of model selection process. But before I begin doing so, I want to check with other, more experienced users of mixed models about their reactions to this observation. Does it seem more or less true to you? Is it roughly consistent with your experience fitting many different mixed models to many different datasets? Do you know of any sensible, theoretical reasons why we might actually <em>expect</em> this to be true in a lot of cases? Or does it just seem like bullshit?</p>
<p>One possible answer here is that it is not true even in my own case, and I have simply deceived myself. Certainly a possibility that I am open to.</p>
<p>Another possibility is that it might be true in my own case, but that this could simply be a kind of coincidence having to do with the kinds of datasets that I tend to work with routinely (which, FYI, are datasets in psychological / social sciences, a slight majority being experimental in origin, but also a fair proportion of non-experimental stuff). If this is the case then there is probably no good reason for expecting my observations to hold in general in other fields that handle very different kinds of data. Still, if there is a coherent non-coincidental reason for why this might be expected to be true, even if only for these particular kinds of datasets, I would love to hear it.</p>
<p>And of course another possibility is that others <em>have</em> noticed similar patterns in their own data, and that it represents some kind of general rule of thumb that people find useful to keep in mind as they fit mixed models to various different data. If this is the case then it seems like there must be some compelling statistical-theoretical reason for why this pattern arises. But I really don't know what that reason would look like.</p>
<p>I welcome anyone's thoughts and opinions about this. Note that as far as I'm concerned, totally legitimate responses to this question might be as simple as comments like &quot;Yeah I have noticed something similar in the data I've worked with, but I have no idea why it should be true&quot; or conversely &quot;I have noticed nothing remotely like this in the data I've worked with.&quot; Of course I also welcome longer and more involved discussions ...</p>
",2013-10-15 08:21:54.933
57509,16474.0,2,,57498.0,,,,CC BY-SA 3.0,"<p>In many cases a distribution can be described as a result of some idealized experiment. For example if we flip a fair coin $n$ times the number of heads will follow a binomial distribution with parameters $n$ and .5. These idealized experiments are often used as models; they are used as simplified representation of how the data came to be. There are obviously many such models, and as a consequence many distributions. If you want the logic behind all distributions, then that will require a book of many volumes, e.g.:</p>

<p>N. L. Johnson, S. Kotz and N. Balakrishnan (2000). <em>Continuous Multivariate Distributions</em>, Vol. 1 (second edition), New York: Wiley &amp; Sons.</p>

<p>N. L. Johnson, S. Kotz and N. Balakrishnan (1997). <em>Discrete Multivariate Distributions</em>. New York: John Wiley &amp; Sons.</p>

<p>N. L. Johnson, S. Kotz and N. Balakrishnan (1995). <em>Continuous Univariate Distributions</em>, Vol. 2 (second edition), New York: John Wiley &amp; Sons.</p>

<p>N. L. Johnson, S. Kotz and N. Balakrishnan (1994). <em>Continuous Univariate Distributions</em>, Vol. 1 (second edition), New York: John Wiley &amp; Sons.</p>

<p>N. L. Johnson, A. W. Kemp and S. Kotz (1992). <em>Univariate Discrete Distributions</em> (second edition), New York: John Wiley &amp; Sons.</p>

<p>A shorter list of distributions that is more suitable/affordable for owning yourself is:</p>

<p>Forbes, C., Evans, M., Hastings, N., &amp; Peacock, B. (2011). Statistical distributions. Wiley</p>
",2013-10-15 08:28:49.833
57510,19557.0,2,,57428.0,,,,CC BY-SA 3.0,"<p>ID3 is an <a href=""http://www.cise.ufl.edu/~ddd/cap6635/Fall-97/Short-papers/2.htm"" rel=""nofollow"">algorithm</a> for building a decision tree classifier based on maximizing information gain at each level of splitting across all available attributes. It's a precursor to the <a href=""http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/dtrees/c4.5/tutorial.html"" rel=""nofollow"">C4.5 algorithm</a>. </p>

<p>With this data, the task is to correctly classify each instance as either benign or malignant. Since each attribute takes on whole integer values in the range 1-10, strictly speaking the values aren't continuous in that they can't take decimal values. For each integer value of each attribute, you'll need to calculate which split provides the most homogenous grouping of instances at each level of splitting. This is done by calculating the <a href=""http://www.autonlab.org/tutorials/infogain11.pdf"" rel=""nofollow"">information gain</a> for each possible split and selecting the greatest (ID3 is known as a <a href=""http://www.c3.lanl.gov/mega-math/gloss/compute/greedy.html"" rel=""nofollow"">greedy algorithm</a>). </p>

<p>You can do this by hand, but it's obviously better to run the algorithm in a tool such as <a href=""http://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow"">Weka</a> or <a href=""http://cran.r-project.org/web/views/MachineLearning.html"" rel=""nofollow"">R</a>.  If you're creating your own implementation, then you'll need to test each possible split and select the one with the greatest information gain, assuming you don't already have a homogenous group (in which case you'd assign the class attribute and change the node to a leaf).</p>
",2013-10-15 08:47:11.623
57568,,1,57573.0,,user30490,Sum of autoregressive processes?,<time-series><self-study>,CC BY-SA 3.0,"<p>I am working on a research topic where I need to add together two AR processes and I was wondering if the distribution of these processes is of a recognizable form/structure.  More formally, if $x_t$ is a AR(p) process with characteristic polynomial $\Phi_x(u)$ and $y_t$ is a AR(q) process with characteristic polynomial $\Phi_y(u)$, then what is the structure of $z_t=x_t+y_t$?</p>
",2013-10-15 20:10:19.377
58691,23276.0,2,,58689.0,,,,CC BY-SA 3.0,"<p>Note that for $\def\N{\mathbb N}n \in \N$ we have $$\{T\le n\} = \bigcup_{i=1}^n \{X_i \in B\} = \bigcup_{i=1}^n X_i^{-1}(B) $$
As $X_i$ is $\mathcal F$-measurable for $i \le n$ by definition, we have $X_i^{-1}(B)\in \mathcal F_n$ for $i \le n$ and hence $\{T \le n\}\in \mathcal F_n$. That is, $T$ is an $(\mathcal F_n)$-stopping time.</p>
",2013-11-02 09:29:15.103
57511,8819.0,1,,,,Integration with respect to Multivariate normal distribution,<normal-distribution><multivariate-analysis><numerical-integration>,CC BY-SA 3.0,"<p>I am working on the numerical integration of an integral of the following functional form:</p>

<p>$$ \int\limits_{R^{G}} F(x_{1},x_{2},\text{â€¦}x_{G})d\text{Î¦}_{\text{Î£}}(x_{1},x_{2},\text{â€¦},x_{G}) $$</p>

<p>Here<br>
$$ \text{Î¦}_{\text{Î£}}(x_{1},x_{2},\text{â€¦},x_{G}) $$ </p>

<p>is the G-dimensional multivariate normal distribution with correlation matrix $\Sigma$ and F is some function of the constituent marginals. </p>

<p>What I am essentially doing is calculating the expectation of a function over a correlated multivariate normal distribution. Practically, G is expected to be equal to or less than 4 and most often just 2 or 3.</p>

<p>Can some one know-how share any of the fundamental references that tackles the issue.My research yielded some information, and it appears that Gaussian quadrature is one of the preferred ways to approach the problem. I am referring to the book <strong>Applied Computational
Economics and Finance</strong> by Miranda and Fackler for addressing the implementation aspects of the algorithm. </p>

<p>But, I wanted to get some help from the expert community here on if I am on the right track. </p>

<p>Sorry if it is a repeat, however I searched on the site, and was not able to find a question that matches with what I had.</p>
",2013-10-15 08:47:23.500
57512,22707.0,2,,57338.0,,,,CC BY-SA 3.0,"<p>The output of TANH is already between -1 and 1. So, if you normalise the inpu, be sure to normalise for the hidden activation functions. In theory it is not required to normalise, because tanh(1000) is mathematically different from tanh(10000). But in practice these are the same, so you should indeed normalise the input in most applications.</p>
",2013-10-15 09:31:11.227
57513,10594.0,1,57534.0,,,"A categorical variable in glm shows significance from analysis of deviance, but each level is not significant in z-test",<hypothesis-testing><logistic><statistical-significance><generalized-linear-model><separation>,CC BY-SA 3.0,"<p>I am fitting a generalized linear model (glm). The explanatory variable is categorical with three levels (control, treat1, treat2). The response variable is 0 or 1.
The response rate for each treatment level is ploted as the figure below (from left to right: control, treat1, treat2):</p>

<p><img src=""https://i.stack.imgur.com/Abi8S.png"" alt=""enter image description here""></p>

<p>There seems to be a big treatment effect between treat1 vs. control and treat2 vs. control. I applied glm:</p>

<p><code>fit &lt;- glm(response ~ treatment, family = binomial, data = dat)</code></p>

<pre><code>Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)        
(Intercept)   -21.57    6536.57  -0.003    0.997
treat1        23.76    6536.57   0.004    0.997
treat2        43.13    9364.95   0.005    0.996
</code></pre>

<p>The z-test shows that neither treat1 nor treat2 is significant compared to the reference level control.</p>

<p>However, the analysis of deviance confirmed that the treatment factor as a whole is highly significant:</p>

<pre><code>drop1(M2, test=""Chisq"")

response ~ treatment
            Df   Deviance    AIC    LRT  Pr(&gt;Chi)    
 &lt;none&gt;          13.003    19.003                     
 treatment   2   77.936    79.936 64.932 7.946e-15 ***
</code></pre>

<p>How shall I interpret such a strange result? Why does the individual z-test not give me any significant result, while according to the plot there is obviously an effect between treat1 and control, and between treat2 and control?</p>
",2013-10-15 09:49:58.953
57514,14874.0,1,57517.0,,,Constructing a bivariate distribution from two gamma-distributed random variables with nonlinear dependence?,<gamma-distribution><bivariate><nonlinear>,CC BY-SA 3.0,"<p>I've got 2 gamma-distributed random variables $(X,Y)$ with arbitrary scale and shape parameters. Further, $Y$ should be a non-linear function of $X$, lets say $Y=\sqrt{X}$. What I am interested in is the joint probability $F_{X,Y}(\cdot)$. </p>

<p>All suggestions or general comments are welcome.</p>

<p>Thank you in advance</p>
",2013-10-15 10:00:24.350
57515,22677.0,2,,57486.0,,,,CC BY-SA 3.0,"<p>Nevermind, apparently the $ARL0=\frac1\alpha$ where alpha is <strong><em>false alarm probability</em></strong></p>

<p>a further reading wold be</p>

<blockquote>
  <p>Nonparametric monitoring of data streams for changes in location and scale
  GJ Ross, DK Tasoulis, NM Adams - Technometrics, 2011 - Taylor &amp; Francis</p>
</blockquote>
",2013-10-15 10:13:54.107
57516,21884.0,1,57519.0,,,Sample variance order,<variance>,CC BY-SA 3.0,"<p>Is it true that (and if so, how does one prove) the following.
 $$E\left|\hat{Var}_{n}(X)-Var(X)\right|^{2}=O(n^{-1})$$
  where:</p>

<p>â€¢ $X$ is a random variable with mean $\mu$ and variance $\sigma^{2}$</p>

<p>â€¢ $\hat{Var}_{n}(X)$ is the sample variance of $X$ from $n$
  i.i.d. random variables $X_{1},\cdots,X_{n}$ with mean $\mu$
  and variance $\sigma^{2}$.</p>

<p>Many thanks in advance. (Feel free to change my notation).</p>
",2013-10-15 10:40:17.880
57517,17328.0,2,,57514.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>OP wrote: I've got 2 gamma-distributed random variables (X,Y) with ... say $Y=\sqrt{X}$.</p>
</blockquote>

<p>Your question is internally inconsistent. In particular, if $X$~Gamma$(a,b)$ with pdf $f(x)$, say:</p>

<p>$$f(x) =\frac{x^{a-1} e^{-\frac{x}{b}}}{b^a \Gamma (a)}, \text{ for }  x &gt; 0 $$</p>

<p>... and $Y =\sqrt{X}$, then the pdf of $Y$, say $g(y)$, is:</p>

<p>$$g(y) = \frac{2 b^{-a} y^{2 a-1} e^{-\frac{y^2}{b}}}{\Gamma (a)},  \text{ for }  y &gt; 0 $$</p>

<p>... which is <em>not</em> Gamma$(\alpha, \beta)$, as originally assumed.</p>
",2013-10-15 11:03:36.747
57518,18296.0,1,57599.0,,,Deterministic components in covariates/exogenous variables in time series models,<time-series><trend><seasonality><predictor>,CC BY-SA 3.0,"<p>Actually, I have read a pair of books about time series analysis, but I am still not sure about how to treat deterministic components, like trend and seasonality, in the exogenous variables in a time series model. Do I have to detrend and deseasonalize the covariates before I use them as axplanatory variables in a time series model? I would also be thankful for a reference.</p>

<p>Thank you in advance!</p>
",2013-10-15 11:15:55.800
57526,21884.0,1,,,,Choice of variance estimator,<mathematical-statistics><variance><estimation><unbiased-estimator>,CC BY-SA 3.0,"<p>Consider the problem of the choice of estimator of $\sigma^2$ based on a random sample of size $n$  from a $N(\mu,\sigma^2)$ distribution. </p>

<p>In undergraduate, we were always taught to use the sample variance</p>

<p>$$\hat{s}^2 = \dfrac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$$ </p>

<p>instead of the maximum likelihood estimator</p>

<p>$$\hat{\sigma}^2 = \dfrac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}.$$</p>

<p>This is because we learned that $\hat{s}^2$ is an <strong>unbiased estimator</strong> and that $\hat{\sigma}^2$ is <strong>biased</strong>.</p>

<p>However now I'm studying for a PhD and I've read that we choose estimators based on minimizing mean square error (=bias$^2$ + var).</p>

<p><a href=""http://mcs.une.edu.au/~stat354/notes/node63.html"" rel=""nofollow"">It can be shown</a> that $$mse(\hat{\sigma}^2) &lt; mse(\hat{s}^2 ).$$</p>

<p>So, why do most people use $\hat{s}^2$?</p>
",2013-10-15 13:24:29.923
58692,11383.0,2,,58688.0,,,,CC BY-SA 3.0,"<p>The last step simply uses the fact that for each real number $t$,
$$\exp(t)=\sum_{i=0}^\infty\frac{t^i}{i!}.$$
Here $t=\lambda s$. (the introduction of $\frac{e^{\lambda s}}{e^{\lambda s}}$ does not seem to be of use here) </p>
",2013-11-02 09:38:27.413
57519,17328.0,2,,57516.0,,,,CC BY-SA 3.0,"<p>I assume by the term, sample variance, you are referring to the <em>unbiased</em> estimator of population variance $\mu_2$, <em>i.e.</em> the 2nd h-statistic, namely:</p>

<p>$$ h_2 = \hat{Var}_{n}(X) = \frac{1}{n-1}\sum _{i=1}^n \left(X_i-\bar{X}\right){}^2$$</p>

<p>Either way, the expectation you seek is just the MSE of $\hat{Var}_{n}(X)$. Note that the absolute value is irrelevant due to the squaring. That should be enough to do a google search and find an answer somebody has worked out in a journal paper or book. </p>

<p>More generally, these sorts of calculations are known as <em>moments of moments</em> and can be solved by working with power sum notation $s_r=\sum _{i=1}^n X_i^r$. First, express h2 in terms of power sums:</p>

<p><a href=""https://i.stack.imgur.com/z9jc6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z9jc6.png"" alt=""enter image description here""></a></p>

<p>where I am using the <code>HStatistic</code> function in the mathStatica software (of which I am one of the authors). Next: find $E[(h_2-\mu_2)^2]$ ... which is just the 1st RawMoment of $(h_2-\mu_2)^2$, so we can find it with:</p>

<p><a href=""https://i.stack.imgur.com/hgoMa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hgoMa.png"" alt=""enter image description here""></a></p>

<p>The ___ToCentral bit expresses the answer in terms of central moments $\mu_i$ of the population.</p>

<p>All done ... you can now work out what happens as $n$ gets large etc</p>
",2013-10-15 11:38:31.180
57520,7155.0,2,,57338.0,,,,CC BY-SA 3.0,"<p>For regression tasks you should be using a linear neuron in the output. Your logit output likely looks sigmoidal when plotted against the response variable. Plus the loss function you're using, probably makes little sense in this context.</p>

<p>Input features should always be de-meaned and divided by standard deviation. That has nothing to do with the unit types, and everything to do with training by backprop. Gradient descent will be cradling around a minima if you don't normalize properly because the error surface will be a thin ellipsoid.</p>

<p>Finally, consider rectified linear units in the hidden because they train much faster than logit or tanh.</p>
",2013-10-15 11:39:25.197
57521,22709.0,1,,,,Fraction confidence intervals for small sample size,<confidence-interval><small-sample>,CC BY-SA 3.0,"<p>What is 95% confidence interval for the fraction of 5 successes out of 12 trials?</p>

<p>Is it possible to compute confidence intervals for a sample of this size?</p>
",2013-10-15 11:46:03.933
57522,503.0,2,,57521.0,,,,CC BY-SA 3.0,"<p>It is certainly possible to do this. There are several methods. The literature is surprisingly large; for a good entry point see <a href=""http://www.stat.ufl.edu/~aa/articles/agresti_coull_1998.pdf"">Agresti &amp; Coull (1998)</a> </p>

<p>If you are using <code>R</code> you can use this:</p>

<pre><code>install.packages(""binom"")
library(binom)
binom.confint(5, 12)
</code></pre>
",2013-10-15 12:02:04.813
57523,10278.0,2,,57479.0,,,,CC BY-SA 3.0,"<p>I think this is best illustrated with an example in R:</p>

<pre><code>library(GGally)
data(iris)
</code></pre>

<p><strong>Actual labeling according to Species</strong></p>

<pre><code>ggpairs(iris, columns=c(""Sepal.Length"", ""Sepal.Width"", ""Petal.Length"", ""Petal.Width""), colour='Species', lower=list(continuous='points'), axisLabels='none', upper=list(continuous='blank'))
</code></pre>

<p><img src=""https://i.stack.imgur.com/jTP84.png"" alt=""Clustering according to Species.""></p>

<p><strong>Labelling according to kmeans clustering</strong></p>

<pre><code>set.seed(1234)
iris$Cluster &lt;- factor(kmeans(iris[,c(""Sepal.Length"", ""Sepal.Width"", ""Petal.Length"", ""Petal.Width"")], centers=length(levels(iris$Species)))$cluster)
ggpairs(iris, columns=c(""Sepal.Length"", ""Sepal.Width"", ""Petal.Length"", ""Petal.Width""), colour='Cluster', lower=list(continuous='points'), axisLabels='none', upper=list(continuous='blank'))
</code></pre>

<p><img src=""https://i.stack.imgur.com/koxWo.png"" alt=""Clustering according to kmeans.""></p>

<p>From these pair-wise plots you can compare visually what elements kmeans assigns to the same group compared what elements belong to the same species.</p>
",2013-10-15 12:02:20.597
57524,12683.0,2,,57497.0,,,,CC BY-SA 3.0,"<p>Some technicalities to complement @vinux's answer:</p>

<p>If you have a density $f_X(\cdot)$ for a random variable $X$ where
$$f_X\left(x;\theta,\phi;\vec\xi\right)=\tfrac{1}{\phi}f_X\left(\frac{x-\theta}{\phi};0,1;\vec\xi\right)$$
then $\theta$, $\phi$, &amp; $\vec\xi$ are location, scale, &amp; shape parameters respectively.</p>

<p>Location parameters only shift the density, changing its mean (if it has one) &amp; other measures of central tendency, but no higher moments.</p>

<p>Scale parameters only stretch the density, changing its variance (if it has one) &amp; other measures of dispersion, &amp; the mean when $\newcommand{\ex}{\operatorname{E}}\ex X\neq\theta$, but no higher moments.</p>

<p>Shape parameters change the shape of the density, perhaps stretching or shifting too, so may change any moments. They tend to get called 'shape' parameters only in contrast to location &amp; scale parameters; e.g. the Weibull distribution has scale &amp; shape parameters but no-one talks about the shape parameter of the Poisson distribution (though they do about the two shape parameters of the beta distribution).</p>

<p>It's perhaps worth emphasizing @Nick's point that a parameter's being equal to the expectation of the random variable  doesn't imply that it's a location parameter: $$\psi=\ex{X} \quad \not\Rightarrow \quad f_X\left(x;\psi\right)=f_X\left(x-\psi;0\right)$$</p>
",2013-10-15 12:06:38.393
57525,15563.0,1,,,,How to use R prcomp results for prediction?,<r><pca>,CC BY-SA 4.0,"<p>I have a data.frame with 800 obs. of  40 variables, and would like to use Principal Component Analysis to improve the results of my prediction (which so far is working best with Support Vector Machine on some 15 hand-picked variables).</p>
<p>I understand a prcomp can help me improve my predictions, but I do not know how to use the results of the <code>prcomp</code> function.</p>
<p>I obtain the result:</p>
<pre><code>&gt; PCAAnalysis &lt;- prcomp(TrainTrainingData, scale.=TRUE)
&gt; summary(PCAAnalysis)
Importance of components:
                          PC1    PC2    PC3    PC4    PC5   PC6    PC7    PC8    PC9   PC10   PC11   PC12   PC13   PC14
Standard deviation     1.7231 1.5802 1.3358 1.2542 1.1899 1.166 1.1249 1.1082 1.0888 1.0863 1.0805 1.0679 1.0568 1.0520
Proportion of Variance 0.0742 0.0624 0.0446 0.0393 0.0354 0.034 0.0316 0.0307 0.0296 0.0295 0.0292 0.0285 0.0279 0.0277
Cumulative Proportion  0.0742 0.1367 0.1813 0.2206 0.2560 0.290 0.3216 0.3523 0.3820 0.4115 0.4407 0.4692 0.4971 0.5248
                         PC15   PC16   PC17   PC18  PC19   PC20   PC21   PC22   PC23   PC24   PC25   PC26   PC27   PC28
Standard deviation     1.0419 1.0283 1.0170 1.0071 1.001 0.9923 0.9819 0.9691 0.9635 0.9451 0.9427 0.9238 0.9111 0.9073
Proportion of Variance 0.0271 0.0264 0.0259 0.0254 0.025 0.0246 0.0241 0.0235 0.0232 0.0223 0.0222 0.0213 0.0208 0.0206
Cumulative Proportion  0.5519 0.5783 0.6042 0.6296 0.655 0.6792 0.7033 0.7268 0.7500 0.7723 0.7945 0.8159 0.8366 0.8572
                         PC29   PC30   PC31   PC32   PC33   PC34   PC35   PC36    PC37                 PC38
Standard deviation     0.8961 0.8825 0.8759 0.8617 0.8325 0.7643 0.7238 0.6704 0.60846 0.000000000000000765
Proportion of Variance 0.0201 0.0195 0.0192 0.0186 0.0173 0.0146 0.0131 0.0112 0.00926 0.000000000000000000
Cumulative Proportion  0.8773 0.8967 0.9159 0.9345 0.9518 0.9664 0.9795 0.9907 1.00000 1.000000000000000000
                                       PC39                 PC40
Standard deviation     0.000000000000000223 0.000000000000000223
Proportion of Variance 0.000000000000000000 0.000000000000000000
Cumulative Proportion  1.000000000000000000 1.000000000000000000
</code></pre>
<p>I thought I would obtain the parameters that are the most important to use, but I just don't find this information. All I see are &quot;standard deviation&quot; etc. on the PCs. But how do I use this for prediction?</p>
",2013-10-15 12:19:10.007
57527,22705.0,2,,57518.0,,,,CC BY-SA 3.0,"<p>It may not be a good idea to consider trend &amp; Seasonality as deterministic components of your dependent variable.</p>

<p>The Un-observed component model approach is an ideal way to handle such ambiguities. It estimates the trend, seasonality &amp; other exogenous variables as well.</p>

<p><a href=""http://ideas.repec.org/h/eee/ecofch/1-07.html"" rel=""nofollow"">http://ideas.repec.org/h/eee/ecofch/1-07.html</a> is your starting point.</p>
",2013-10-15 13:28:09.540
57529,,2,,57525.0,anon,,,CC BY-SA 3.0,"<p>The information from the summary() command you have attached to the question allows you to see, e.g., the proportion of the variance each principal component captures (Proportion of variance). In addition, the cumulative proportion is computed to output. For example, you need to have 23 PCs to capture 75% of the variance in your data set. </p>

<p>This certainly is not the information you typically use as input to further analyses. Rather, what you usually need is the rotated data, which is saved as 'x' in the object created by prcomp.</p>

<p>Using R code as a short example.</p>

<pre><code>pr&lt;-prcomp(USArrests, scale = TRUE)
summary(pr) # two PCs for cumulative proportion of &gt;80% 
newdat&lt;-pr$x[,1:2]
</code></pre>

<p>Then you can use the data in the newdat for further analyses, e.g., as input to SVM or some regression model. Also, see, e.g., <a href=""https://stackoverflow.com/questions/1805149/how-to-fit-a-linear-regression-model-with-two-principal-components-in-r"">https://stackoverflow.com/questions/1805149/how-to-fit-a-linear-regression-model-with-two-principal-components-in-r</a> for more information.</p>
",2013-10-15 13:46:29.443
57530,8074.0,2,,57525.0,,,,CC BY-SA 3.0,"<p>While I'm unsure as to the nature of your problem, I can tell you that I have used PCA as a means of extracting dominant patterns in a group of predictor variables in the later building of a model. In your example, these would be found in the principle components (PCs), <code>PCAAnalysis$x</code>, and they would be based on the weighting of variables found in <code>PCAAnalysis$rotation</code>. One advantage of this process is that PCs are orthogonal, and so you remove issues of multicollinearity between the model predictors. The second, is that you might be able to identify a smaller subset of PCs that capture the majority of variance in your predictors. This information can be found in <code>summary(PCAAnalysis)</code> or in <code>PCAAnalysis$sdev</code>. Finally, if you are interested in using a subset of the PCs for prediction, then you can set the <code>tol</code> parameter in <code>prcomp</code> to a higher level to remove trailing PCs.  </p>

<p>Now, you can ""project"" new data onto the PCA coordinate basis using the <code>predict.prcomp()</code> function. Since you are calling your data set a ""training"" data set, this might make sense to then project a validation data set onto your PCA basis for the calculation of their respective PC coordinates. Below is an example of fitting a PCA to 4 biometric measurements of different iris species (which are correlated to some degree). Following this, I project biometric values of a new data set of flowers that have similar combinations of these measurements for each of the three species of iris. You will see from the final graph that their projected PCs lie in a similar area of the plot as the original data set.</p>

<h3>An example using the <code>iris</code> data set:</h3>

<pre><code>### pca - calculated for the first 4 columns of the data set that correspond to biometric measurements (""Sepal.Length"" ""Sepal.Width""  ""Petal.Length"" ""Petal.Width"")
data(iris)

# split data into 2 parts for pca training (75%) and prediction (25%)
set.seed(1)
samp &lt;- sample(nrow(iris), nrow(iris)*0.75)
iris.train &lt;- iris[samp,]
iris.valid &lt;- iris[-samp,]

# conduct PCA on training dataset
pca &lt;- prcomp(iris.train[,1:4], retx=TRUE, center=TRUE, scale=TRUE)
expl.var &lt;- round(pca$sdev^2/sum(pca$sdev^2)*100) # percent explained variance

# prediction of PCs for validation dataset
pred &lt;- predict(pca, newdata=iris.valid[,1:4])

###Plot result
COLOR &lt;- c(2:4)
PCH &lt;- c(1,16)

pc &lt;- c(1,2) # principal components to plot

png(""pca_pred.png"", units=""in"", width=5, height=4, res=200)
op &lt;- par(mar=c(4,4,1,1), ps=10)
plot(pca$x[,pc], col=COLOR[iris.train$Species], cex=PCH[1], 
 xlab=paste0(""PC "", pc[1], "" ("", expl.var[pc[1]], ""%)""), 
 ylab=paste0(""PC "", pc[2], "" ("", expl.var[pc[2]], ""%)"")
)
points(pred[,pc], col=COLOR[iris.valid$Species], pch=PCH[2])
legend(""topright"", legend=levels(iris$Species), fill = COLOR, border=COLOR)
legend(""topleft"", legend=c(""training data"", ""validation data""), col=1, pch=PCH)
par(op)
dev.off()
</code></pre>

<p><a href=""https://i.stack.imgur.com/eP5E0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eP5E0.png"" alt=""enter image description here""></a></p>
",2013-10-15 13:50:46.937
57531,20304.0,1,57532.0,,,survival analysis without enough data,<survival>,CC BY-SA 3.0,"<p>I have computed and plotted the survival function for a subscription-based service and the following is the result.</p>

<p><img src=""https://i.stack.imgur.com/qm7dm.png"" alt=""enter image description here""></p>

<p>The problem is that there does not seem to be enough data to get a full curve.  This is because most of the oldest accounts are still active. So my question is would it still be useful to compare survival curves for different segments of accounts given that there is clearly not enough data for a full curve.</p>
",2013-10-15 14:09:15.703
57532,3999.0,2,,57531.0,,,,CC BY-SA 3.0,"<p>It is still useful - I wouldn't make really bold claims about what happens past 600 days if I were you, but seeing a clear departure in one category or the other, even if they don't eventually hit zero, is still useful.</p>

<p>Consider this: The ideal randomized clinical trial of a perfect, flawless drug will have one of the curves not only never going to zero, but <em>never going below ~1.00</em>. The fact that a curve doesn't drop in a meaningful time horizon is, in and of itself, useful information. And since all accounts that have not yet closed are censored, the techniques you're using are already accounting for ""They'll close someday far in the future"".</p>
",2013-10-15 14:27:38.403
57533,19265.0,1,,,,Is it discriminant analysis?,<discriminant-analysis>,CC BY-SA 3.0,"<p>I have samplings of one-dimensional data of two classes: $A$ and $B$.</p>

<p>I have to predict the posterior probability of class $A$.</p>

<p>$$\tilde{P}(A|x) \approx \frac{N_A\tilde{f_A}(x)}{N_A\tilde{f_A}(x) + N_B\tilde{f_B}(x)}.$$</p>

<p>$f_A$, $f_B$ are the estimation of densities of classes $A$ and $B$.</p>

<p>I estimate the densities as normal distributions, whose $\mu$ and $\sigma$ are average values and standard deviations of all the points of each class ($A$ and $B$ accordingly).</p>

<p>Will it be correct to say, that I'm using <code>discriminant analysis</code> in this case?</p>
",2013-10-15 14:38:31.047
57534,232.0,2,,57513.0,,,,CC BY-SA 3.0,"<p>You cannot use Wald's z-test when the maximum likelihood estimates are infinite (they just look finite in the model fit). However you can still use a likelihood-ratio test as you see with the deviance analysis. You just have to set up model comparisons that test the hypotheses that you are interested in.</p>

<p>For example, to test $\mu_c = \mu_{treat1}$, you fit a model under this assumption, and compare it to the full model.</p>

<pre><code>fit2 &lt;- glm(response ~ I(treatment==""treat2""), family = binomial, data = dat)
anova(fit1, fit2, test=""Chisq"")
</code></pre>
",2013-10-15 14:38:53.907
57535,22713.0,1,,,,Should I use Pearson Correlation or Linear Regression to show the linearity of two sets of numbersï¼Ÿ,<regression><correlation><dataset><interpretation><pearson-r>,CC BY-SA 3.0,"<p>I have two sets of acceleration data in terms of time. In one set the acceleration increases more linearily than another set. How can I show the linearity difference of the two set of data best? Should I use Pearson Correlation or Linear Regression? If I get r = 0.8 and 0.98 by Pearson Correlation, how I can interpretate the result? Do they have very big difference on the linearity based on 0.8 and 0.98?</p>

<p>Thanks.</p>
",2013-10-15 14:44:13.583
57536,22671.0,1,57542.0,,,Fit a regression line by using `MATLAB`,<regression><matlab><linear-model><fitting>,CC BY-SA 3.0,"<p>I have the following data </p>

<pre><code>  Individual     Heart rate     Weight    Hours of exercise per week
       1           72            134             3.2
       2           81            201             3.5
       3           60            156             7.1
       4           82            148             2.4
       5           75            170             1.2
</code></pre>

<p>Now i have to fit a regression line by using <code>MATLAB</code>.</p>

<p>But which one is independent variable ? Hours of exercise per week? And is that  Heart rate &amp; Weight response variables ?</p>

<p>Then how can i fit <code>one</code> regression line ? If i have two response or two independent variables, won't i get <code>two</code>  regression line ?</p>
",2013-10-15 14:44:44.693
57537,11886.0,1,,,,Warranty claims count prediction,<forecasting><predictive-models><prediction><count-data>,CC BY-SA 3.0,"<p>I have weekly data of number of warranty claims from week of manufacturing. I want to use this historical data and predict the number of claims that may arise in next few weeks.</p>

<p>I have read about count data models like Poisson regression and NB regression but they expect the counts in the range of 0~30 (is this correct?). In my case the claims can be of the range 0 to several thousands. What would be an ideal way to solve this problem? </p>
",2013-10-15 14:48:21.490
57538,8351.0,1,57548.0,,,Derivation of conditional distribution from other two distributions,<distributions><normal-distribution><conditional-probability><joint-distribution>,CC BY-SA 3.0,"<p>$$Y|X=x \sim N(x,1)\\X\sim N(\mu,\sigma^2 )$$
What distribution does $X|Y=y$ follow?</p>

<p>My initial startegy was to  $f_{Y|X}f_X=f_{X,Y}$ and solve for $f_{X|Y}=f_{X,Y}/f_{Y}$
. Computing for $f_{X,Y}$, I get the following:
$$f_{X,Y}=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}\frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{(y-x)^2}{2}\right\}$$
And in trying to compute for $f_{Y}$, I was trying to integrate above w.r.t $x$, but I was stuck. I am not sure if that's integratable, and if this is a right approach to solve this question. I am curious if there is some kind of tricks/insights I am missing.</p>
",2013-10-15 14:54:42.960
57539,22714.0,1,,,,How to do a classification with only one variable?,<classification>,CC BY-SA 3.0,"<p>I would like to classify individuals of a database by a single quantitative variable. Is hierachical clustering suitable to do this? If it is possible, how the algorithm work? If use hierachical clustering is not right, which other procedures or techniques could I use?
My purpose is not classify the variable, but individuals. So decision trees are not suitable.</p>
",2013-10-15 14:55:36.683
18335,5038.0,1,18345.0,,,Non-nested model selection,<aic><likelihood-ratio><nested-models>,CC BY-SA 3.0,"<p>Both the likelihood ratio test and the AIC are tools for choosing between two models and both are based on the log-likelihood.</p>

<p>But, why the likelihood ratio test can't be used to choose between two non-nested models while AIC can?</p>
",2012-01-01 12:15:03.760
57542,5203.0,2,,57536.0,,,,CC BY-SA 3.0,"<p>It sounds like you're mixed up on a few different things.</p>

<p>First, <strong>independent variables</strong> are the inputs, causes, or explanatory variables, or predictors to your model, while the <strong>dependent variables</strong> are the outputs from the model, since they ""depend"" on the values of the independent variables (hopefully!).</p>

<p>You can definitely have a single regression line with multiple independent variables. For example, you might model something like
$$ \textrm{Heart Rate} = \beta_0 + \beta_1 \textrm{Weight} + \beta_2\textrm{Exercise}$$
People sometimes make a distinction between <strong>multiple linear regression</strong>, where the model has two or more explanatory variables, from <strong>simple linear regression</strong>, which has only one. Neither of these should be confused with <strong>multivariate regression</strong>, where one predicts multiple variables at once, as in
$$&lt;\textrm{Heart Rate}, \textrm{Weight}&gt; = \beta_0 + \beta_1 \textrm{Exercise} + \ldots$$</p>

<p>Since the independent and dependent variables depend on your hypothesis, only you can decide which are which. For example, you might suspect that resting heart rate is affected by one's weight and exercise habits. If so, you'd use weight and exercise as the independent variables, while heart rate is the dependent variable. This would give you the first model, show above. On the other hand, you might want to predict weight from someone's resting heart rate and exercise. In this case, your model would look something like:
$$ \textrm{Weight} = \beta_0 + \beta_1 \textrm{Heart Rate} + \beta_2\textrm{Exercise}$$</p>

<p>There are several ways to do a regression in matlab. The <code>regress</code> function (documentation <a href=""http://www.mathworks.com/help/stats/regress.html"" rel=""nofollow"">here</a>) might be a reasonable place to start. You'll need to make an $n \times 1$ vector of responses (call it $y$; it is the dependent variable) and an $n \times p$ vector of predictors (the matching values of the dependent variable(s); call this $x$). Then, you run something like <code>b = regress(y,x);</code> to get the associated coefficients (e.g., b(1) is the coefficient for the values in x(:,1)). Note that if you want a constant term in your model, you need to add a column of all ones to your predictor matrix!</p>

<p>There are several other methods in matlab for fitting linear regression models. The statistics toolbox has a <a href=""http://www.mathworks.com/help/stats/linearmodelclass.html"" rel=""nofollow"">Linear Model</a> class. The curve fitting app <a href=""http://www.mathworks.com/help/curvefit/cftool.html"" rel=""nofollow"">cftool</a> can interactively fit linear (and other models); the toolbox also includes a programmatic <a href=""http://www.mathworks.com/help/curvefit/fit.html"" rel=""nofollow"">fit</a> function. Due to how the math works out, you can also just use <a href=""http://www.mathworks.com/help/matlab/ref/lscov.html"" rel=""nofollow"">matrix division</a> b = X\y. Take a look and see which of these fits your workflow the best. Beware that <em>some</em> of the methods insert a constant/intercept term into your model but others do not. Make sure you try at least a few out and get what you want. </p>
",2013-10-15 15:23:01.013
57543,19359.0,1,57575.0,,,"""Bayesglm"", p-values and degrees of freedom?",<regression><logistic><bayesian><generalized-linear-model>,CC BY-SA 3.0,"<p>I am trying to perform some logistic regressions (and I am a neophyte user of R).  Initially I used ""glm"" to compute coefficients, AIC and p-values; this worked great until I ran across a data set suffering from <a href=""http://en.wikipedia.org/wiki/Separation_%28statistics%29"" rel=""noreferrer"">complete separation</a>. In [1], Gelman et alia suggest using an (informative) prior to address this problem; the corresponding algorithm is implemented in R as ""bayesglm"" (in the ARM package).</p>

<p>Here is my problem.  Previously, with ""glm"", I would compute p-values as follows:</p>

<pre><code>mylogit &lt;- bayesglm(a ~ b+c+d+e+f+g+h, data = mydata, family=""binomial"")
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
</code></pre>

<p>There are 53-48=5 degrees of freedom:</p>

<pre><code>Null deviance: 71.188  on 53  degrees of freedom
Residual deviance: 37.862  on 48  degrees of freedom
</code></pre>

<p>However, if I use ""bayesglm"" instead of ""glm"", the resulting degrees of freedom are a bit surprising to me:</p>

<pre><code>Null deviance: 22.279  on 53  degrees of freedom
Residual deviance: 39.030  on 54  degrees of freedom
</code></pre>

<p>If I plug in the preceding formula for a p-value, I have -1 degrees of freedom!  Can someone help me get a more sensible answer (or help me interpret this)?</p>

<p>By the way, the documentation on the ""bayesglm"" command includes the following ominous comment:</p>

<blockquote>
  <p>We include all the glm() arguments but we havenâ€™t tested that all the options (e.g., offests, contrasts, deviance for the null model) all work.</p>
</blockquote>

<p>[1] Gelman, Andrew, et al. ""A weakly informative default prior distribution for logistic and other regression models."" The Annals of Applied Statistics (2008): 1360-1383.</p>
",2013-10-15 16:06:23.393
57544,20473.0,2,,57429.0,,,,CC BY-SA 3.0,"<p>From what I can understand, the real-world phenomenon under study can be described as follows:<br>
 There are at every period $N_t$ customers which are divided given some criterion into ""existing"" and ""new"" (think why the categorization criterion is not necessarily obvious).  </p>

<p>For each of these two subgroups, we define the ""payback rate = percentage of customers of this subgroup who pay back"". For new customers we denote this payback rate $Y_1$ and for existing customers we denote this payback rate $Y_2$. We also have as possible explanatory variables the credit scores for these customers. I presume that $X_1$ symbolizes the <em>average</em> credit score of new customers, and $X_2$ the <em>average</em> credit scores of existing customers.</p>

<p>Now, these payback rates should be examined <em>separately</em>, before attempting to build a model for their weighted average.</p>

<p>We may assume therefore that 
$$Y_{1t}= a_1 + b_1X_{1t} + u_{1t} \qquad [1]$$
and 
$$Y_{2t}= a_2 + b_2X_{2t} + u_{2t} \qquad [2]$$</p>

<p>with $t=1,...,T$ being the length of the time series, and the two error terms assumed white noises, independent of each other, and independent of the regressors.</p>

<p>What we want is to estimate the <em>weighted average pay back rate</em>. Denoting $p_t$ the existing customers as a percentage of the customer base $N_t$, this weighted average payback rate is <em>exactly defined</em> as</p>

<p>$$Y_t = (1-p_t)Y_{1t} + p_tY_{2t} \qquad [3]$$</p>

<p>Relation [3] is a mathematical identity. We turn it into a causal/associative/covariance relationship by inserting into it equations $[1]$ and $[2]$ that refelct theoretical/behavioral assumptions:</p>

<p>$$\{[1],\,[2],\, [3]\} \Rightarrow Y_t = (1-p_t)\left(a_1 + b_1X_{1t} + u_{1t}\right) + p_t\left(a_2 + b_2X_{2t} + u_{2t}\right)$$</p>

<p>$$\Rightarrow Y_t = \left[(1-p_t)a_1 + p_ta_2\right] + (1-p_t)b_1X_{1t} + p_tb_2X_{2t} + (1-p_t)u_{1t} + p_tu_{2t} $$ </p>

<p>$$\Rightarrow Y_t = a_1 + (a_2-a_1)p_t + b_1X_{1t}^* + b_2X_{2t}^* + \varepsilon_t \qquad [4]$$ </p>

<p>with  </p>

<p>$X_{1t}^* = (1-p_t)X_{1t}$ and $X_{2t}^*= p_tX_{2t}$ sub-group credit scores weighted by the relative size of each sub-group,</p>

<p>but most importantly with</p>

<p>$\varepsilon_t = (1-p_t)u_{1t} + p_tu_{2t} $  </p>

<p>This means that the error term is <em>contemporaneously correlated with all three regressors</em>.</p>

<p>Denoting by $\mathbf X$ the regressor matrix containing the time series for $\left(p,X_{1}^*,X_{2}^*\right)$ The <em>conditional</em> moments of $\varepsilon_t$ are</p>

<p>$$E(\varepsilon_t\mid \mathbf X) =E((1-p_t)u_{1t} + p_tu_{2t}\mid \mathbf X)  = (1-p_t)E(u_{1t} \mid \mathbf X)  + p_tE(u_{2t}\mid \mathbf X) = 0$$</p>

<p>since the $u$-errors are independent of $\mathbf X$ and white noises. Also<br>
$$\operatorname {Var}(\varepsilon_t\mid \mathbf X) = E(\varepsilon_t^2\mid \mathbf X) = (1-p_t)^2E\left(u_{1t}^2\mid \mathbf X\right) + p_t^2E\left(u_{2t}^2\mid \mathbf X\right) $$
$$= (1-p_t)^2Eu_{1t}^2 + p_t^2Eu_{2t}^2=(1-p_t)^2\sigma_1^2 + p_t^2\sigma_2^2$$</p>

<p>i.e. the error term is conditionally heteroskedastic, with the conditional variance depending on the regressor $p_t$.</p>

<p>We have arrived at the regression specification $[4]$ by recognizing that the dependent variable is necessarily <em>constructed</em> as a function of the regressors, and so our <em>behavioral/association</em> assumptions should be ""placed"" one step earlier (at sub-group level). From this, the interaction between the regressors emerged naturally. But also, we ended up with endogenous regressors and a heteroskedastic error term, the variance of which changes in each time period and is a function of one of the regressors.
Finally, one should think that, if the model is to be used for prediction, perhaps the $p_t$ should be modeled as an autoregressive scheme, since it is difficult to think that the existing/new customers allocation can exhibit wide variations from one period to the next.  </p>

<p>Concluding, this is your model, or at least, a model that is consistent with the real-world phenomenon under study. As you can see, you have much more serious issues to deal with than just ad hoc ways to represent the interaction between the regressors...</p>
",2013-10-15 16:06:45.203
57545,21985.0,1,,,,Sample space of pmf,<self-study>,CC BY-SA 3.0,"<p>I have several exercises to solve that deal with sample space of PMF.</p>

<p>One is:</p>

<p>Let $X_1,\dots , X_n$ be independent random variables with pmf $p(x;\pi) = (1-\pi)^x \pi$</p>

<p>What is the sample space of $X_1?$ Try to give a probabalistic interpretation of such a sample space. Hint: for example. a Bernoulli random variable can be used to model a coin with probability of success $p \in \;\rbrack0,1\lbrack$.</p>

<p>Any hint? I do not know how to ""see"" from the distribution what the sample space is without knowing more about this distribution...</p>

<h2>Possible Solution?</h2>

<p>I have the following idea:
If I plug in all values in the sample space and calculate the sum I have to get $1$ (because it is a probability). But obviously here we can have just one value for the sample space which is $\log({\frac{1}{\pi}})/{\log(1-\pi)}$ (I set the formula for pmf to one and solve the equation for x).</p>

<p>Is that correct?</p>
",2013-10-15 16:09:01.513
57546,9716.0,1,57553.0,,,Design fitness function for polynomial approximation,<r><optimization><stochastic-processes><genetic-algorithms>,CC BY-SA 3.0,"<p>I'm trying to apply a <code>polynomial approximation</code> for a given function (via Genetic Algorithms), and so far  the results are not so good:</p>

<pre><code> # or any other GA package
  require(gaoptim)

  # for polyval
  require(pracma)

  # polynomial of degree 10
  ndeg = 10  
  ndim = ndeg + 1

  # search limits
  search.low = rep(-1, ndim)
  search.up = rep(1, ndim)

  # no. of data points
  m = 101 
  xi = seq(-1, 1, length = m)
  yi = 1 / (1 + (5*xi)^2)

  # fitness function
  pfn = function(p) max(abs(polyval(c(p), xi) - yi))

  # gaoptim perform maximization, so transform the fitness function
  pfninv = function(p){ 1/(pfn(p) + 1) }

  ## set up the ga
  ga = GAReal(pfninv, search.low, search.up, popSize = 500)
  ga$evolve(100)
  y2 = polyval(ga$bestIndividual(), xi)

  plot(xi, yi, ylim = range(c(yi, y2)), type = 'l', main = 'Runge function')
  lines(xi, y2, col = 'red')
</code></pre>

<p><img src=""https://i.stack.imgur.com/cJ5kX.png"" alt=""enter image description here""></p>

<p>Is there any strategy i can apply here, or this is a No-no approach? Maybe a better fitness function, or expand the search limits? Higher values of <em>popSize</em> doesn't seem to help too much.</p>

<p>Thanks for any insight!</p>
",2013-10-15 16:14:24.953
57547,22718.0,1,,,,How to report significance of factor levels shown in summary of glm?,<generalized-linear-model><reporting><z-statistic>,CC BY-SA 3.0,"<p>The summary of my GLM shows day 12 of factor Date to be significant, but anova(model, test=""Chisq"") shows Date to be not significant overall. I know how to report the statistics from the Chisq table, but as I have z values in the summary table I am unsure how to report, or if I should report, that day 12 is significant. </p>

<p>Similarly when finding that Date is significant, how to report what specific dates seem to be important.</p>

<p>Thanks in advance</p>

<p>Lara</p>

<p>I have Fertility and Fecundity of female flies measured over 12 days. I want to check if there is a decline (or otherwise) in fertility/fecundity over this time.   </p>

<p>Additionally, at day 13 females are mated (with a male from one of two different groups), and fertility and fecundity are measured until day 20. I want to use Date as a factor to identify significant peaks in fertility/fecundity i.e. after mating, and potential difference in peaks between groups.</p>

<pre><code>Call:
glm(formula = dda$Fertility.Absolute ~ sqrt(dda$Fecundity) + 
    dda$Group + dda$Date + sqrt(dda$Fecundity):dda$Group + sqrt(dda$Fecundity):dda$Date, 
    family = poisson)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1397  -0.6786  -0.4797   0.3596   3.7588  

Coefficients:
                                   Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)                    -1.91501    0.51539  -3.716 0.000203 ***
    sqrt(dda$Fecundity)             0.72372    0.12441   5.817 5.99e-09 ***
    dda$Group2                      0.19540    0.19230   1.016 0.309585    
    dda$Date4                       0.18117    0.62648   0.289 0.772439    
    dda$Date6                      -0.28952    0.68983  -0.420 0.674706    
    dda$Date8                       0.07111    0.60531   0.117 0.906480    
    dda$Date10                      0.19557    0.62232   0.314 0.753325    
    dda$Date12                      0.79619    0.60710   1.311 0.189696    
    dda$Date14                      1.93702    0.53938   3.591 0.000329 ***
    dda$Date16                      0.75623    0.58296   1.297 0.194554    
    dda$Date18                     -0.05392    0.67805  -0.080 0.936618    
    dda$Date20                     -0.26291    0.68841  -0.382 0.702530    
    sqrt(dda$Fecundity):dda$Group2 -0.07309    0.04822  -1.516 0.129583    
    sqrt(dda$Fecundity):dda$Date4   0.27388    0.17555   1.560 0.118734    
    sqrt(dda$Fecundity):dda$Date6   0.37684    0.22832   1.651 0.098836 .  
    sqrt(dda$Fecundity):dda$Date8   0.13017    0.13861   0.939 0.347674    
    sqrt(dda$Fecundity):dda$Date10  0.04552    0.15345   0.297 0.766722    
    sqrt(dda$Fecundity):dda$Date12 -0.16593    0.14861  -1.117 0.264186    
    sqrt(dda$Fecundity):dda$Date14 -0.24864    0.12754  -1.949 0.051240 .  
    sqrt(dda$Fecundity):dda$Date16  0.05496    0.14578   0.377 0.706170    
    sqrt(dda$Fecundity):dda$Date18  0.15439    0.19341   0.798 0.424715    
    sqrt(dda$Fecundity):dda$Date20 -0.02006    0.16314  -0.123 0.902161    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 1327.36  on 359  degrees of freedom
Residual deviance:  298.26  on 338  degrees of freedom
AIC: 873.65

Number of Fisher Scoring iterations: 5

###

Analysis of Deviance Table    
Model: poisson, link: log    
Response: dda$Fertility.Absolute    
Terms added sequentially (first to last)    

                                  Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                            359    1327.36              
    sqrt(dda$Fecundity)            1   893.88       358     433.48 &lt; 2.2e-16 ***
    dda$Group                      1     0.03       357     433.45    0.8699    
    dda$Date                       9    82.16       348     351.29  6.01e-14 ***
    sqrt(dda$Fecundity):dda$Group  1     0.07       347     351.22    0.7859    
    sqrt(dda$Fecundity):dda$Date   9    52.96       338     298.26  2.97e-08 ***
</code></pre>
",2013-10-15 16:40:46.753
57548,20473.0,2,,57538.0,,,,CC BY-SA 3.0,"<p>Separate the exponents to terms that do not contain $x$ and those that contain $x$ . You will obtain an integrand that can be written in the form $e^{-ax^2-bx}$ (all other terms go out of the integral, since you want to integrate w.r.t $x$). Then  Gradshteyn &amp; Ryzhik (2007), ""Table of Integrals, Series and Products"", 7th ed. p.336, eq. 3.322(2) give the formula:</p>

<p>$$\int_{0}^{\infty}\exp\left\{âˆ’\frac {x^2}{4\beta}âˆ’\gamma x\right\}dx = \sqrt {\pi\beta} \exp\left\{\beta \gamma^2\right\} \left[1-\operatorname{erf}(\gamma \sqrt \beta)\right]$$</p>

<p>I presume you can turn a $\int_{-\infty}^{\infty}$ integral into $\int_{0}^{\infty}$ integrals.</p>

<p>It's going to be a bit long and tedious, easy to make an algebraic mistake. After you're done, don't forget to remember the connection of the error function with the cdf of the standard normal.</p>
",2013-10-15 16:43:38.873
57549,14436.0,1,59371.0,,,Metric for nearest neighbor method,<distance-functions><k-nearest-neighbour><metric>,CC BY-SA 3.0,"<p>Is there a requirement that the measure used in Nearest Neighbor methods be a proper metric distance? What will happen if I use an arbitrary function (e.g., one that does not satisfy the triangle inequality)?</p>

<p>How can you convert an arbitrary function to a valid metric distance?</p>
",2013-10-15 17:10:39.983
57550,22719.0,1,,,,Proper Sampling - can I collect a two-group sample this way without issues?,<logistic><sampling><bias><case-control-study><oversampling>,CC BY-SA 3.0,"<p>I need to collect a two group sample for a comparison analysis (perhaps using logistic regression).</p>

<p>The population that I need to extract a sample from is all firms from country A with activities in country B. The firms are classified into two categories: having a subsidiary in country B (S), or not having a subsidiary in country B (NS). I expect the share of S firms to be small relative to NS firms (but I have no way of knowing for sure).</p>

<p>I already hold the entire population of S firms (because this data was available to me). However data on NS firms is not readily available and I have to collect that, and I will probably not get access to identify and collect all NB firms. </p>

<p>So my situation is I have the entire population of S firms, and need to collect enough NS firms for subsequent analysis to be significant. Most likely my final sample will consist of all S firms and some share of the population of NS firms. Without much experience in doing these kinds of studies, I can't help to think that there is some kind is bias/reliability issue when sampling this way (one group: entire group population, other group: some part of group population). I have learned that <em>if</em> it so happens that the population of NS firms is indeed much larger than S firms (again there is no way to know without data for the entire population of firms), and I e.g. end up with similar-sized samples of each group, there will be a case of oversampling the minority group. However I cannot find any remarks anywhere that consider this a problem for a comparison study, as a correct sample representation of the entire population is less important in this manner.</p>

<p>Is my concern justified? Or is it fine to do it that way for e.g. logistic regression? If not, how can I get around the issue?</p>
",2013-10-15 17:19:39.267
57551,1805.0,1,,,,Mean Reciprocal Rank with GBM in R,<boosting>,CC BY-SA 3.0,"<p>Let's say I'm optimizing <a href=""http://en.wikipedia.org/wiki/Mean_reciprocal_rank"" rel=""nofollow noreferrer"">MRR</a> with a GBM in R:</p>

<pre><code>library(gbm)
generate.data &lt;- function(N) {

  # create query groups, with an average size of 25 items each
  num.queries &lt;- floor(N/25)
  query &lt;- sample(1:num.queries, N, replace=TRUE)

  # X1 is a variable determined by query group only
  query.level &lt;- runif(num.queries)
  X1 &lt;- query.level[query]

  # X2 varies with each item
  X2 &lt;- runif(N)

  # X3 is uncorrelated with target
  X3 &lt;- runif(N)

  # The target
  Y &lt;- X1 + X2

  # Add some random noise to X2 that is correlated with
  # queries, but uncorrelated with items

  X2 &lt;- X2 + scale(runif(num.queries))[query]

  # Add some random noise to target
  SNR &lt;- 5 # signal-to-noise ratio
  sigma &lt;- sqrt(var(Y)/SNR)
  Y &lt;- Y + runif(N, 0, sigma)
  Y &lt;- ifelse(Y&gt;median(Y), 1, 0)

  data.frame(Y, query=query, X1, X2, X3)
}

set.seed(10)
data.train &lt;- generate.data(1000)
gbm.mrr &lt;- gbm(Y~X1+X2+X3,          # formula
                data=data.train,     # dataset
                distribution=list(   # loss function:
                  name='pairwise',   # pairwise
                  metric=""mrr"",     # ranking metric:
                  max.rank=1,
                  group='query'),    # column indicating query groups
                n.trees=2000,        # number of trees
                shrinkage=0.005,     # learning rate
                interaction.depth=3, # number per splits per tree
                bag.fraction = 0.5,  # subsampling fraction
                train.fraction = 1,  # fraction of data for training
                n.minobsinnode = 10, # minimum number of obs for split
                keep.data=TRUE,      # store copy of input data in model
                cv.folds=5,          # number of cross validation folds
                verbose = FALSE,     # don't print progress
                n.cores = 1)         # use a single core

best.iter.mrr &lt;- gbm.perf(gbm.mrr, method='cv')
title('Training of pairwise model with mrr metric')
</code></pre>

<p>(Code <a href=""http://gradientboostedmodels.googlecode.com/svn-history/r68/pkg/demo/pairwise.R"" rel=""nofollow noreferrer"">adapted from here</a>)</p>

<p>This produces the following plot, where the black line is in-sample MRR, and the green line is out-of-sample MRR.  Clearly, GBM is mimizing MRR: it chooses the model with the smallest out-of-sample MRR (~0.3):
<img src=""https://i.stack.imgur.com/x07om.png"" alt=""RPlot""></p>

<p>I'm confused as to why gbm MINIMIZES mrr.  It's my understanding that a MRR of 1 is perfect (best document is the first result, 1/1=1) and a MRR of close to zero is very bad (e.g. best document is the 1000th result, 1/1000 is close to zero).</p>

<p>Is my understanding of mmr correct?  If so, then why does the gbm package minimize it?</p>
",2013-10-15 17:41:26.703
57552,22721.0,1,,,,Include lower level (2-way and 3-way) interactions in 3-level multilevel model?,<interaction><multilevel-analysis>,CC BY-SA 3.0,"<p>I have a three level multilevel model (therapists, patients, repeated measures) in which I have included 4-way interactions, for instance:</p>

<p>therapist self-efficacy * treatment condition * patient on track (yes/no) * time</p>

<p>This tests the hypothesis that therapist who are in condition A and are higher no self-efficacy have a steeper slope with patients that are not on track</p>

<p>Most of the lower order interactions are non-significant and also not that relevant for interpretation. Should I still include all of them? I feel like I would put a lot of parameters in the model, whereas it would be difficult to interpret the lower order interactions that are not significant. </p>

<p>Most fora state that it would be adviceable to include lower order interaction, but there are also some places where it is stated that there is no statistical reason to include them. Any advice or references on this situation?</p>
",2013-10-15 17:49:56.123
57553,9483.0,2,,57546.0,,,,CC BY-SA 3.0,"<p>1) Your target function is <code>yi = 1 /(1 + (5*xi)^2)</code>, which is not a polynomial, so it's going to be hard to approximate with a polynomial:</p>

<p><img src=""https://i.stack.imgur.com/ZpTC0.png"" alt=""enter image description here""></p>

<p>2) If you invert the target function, <code>yi = 1 + (5*xi)^2</code>, increase your bounds, e.g. <code>search.low = rep(-50, ndim)</code>; <code>search.up= rep(50, ndim)</code> </p>

<p><img src=""https://i.stack.imgur.com/5vYTh.png"" alt=""enter image description here""></p>

<p>3) It's more common to take the <a href=""http://en.wikipedia.org/wiki/Root-mean-square_deviation"" rel=""nofollow noreferrer"">root-mean-square error (RMSE)</a> as the fitness function instead of <code>max(abs(polyval(c(p), xi) - yi))</code>.</p>
",2013-10-15 17:55:17.723
57554,15363.0,1,,,,Modified ECDF in R,<r><normal-distribution><simulation>,CC BY-SA 3.0,"<p>The ecdf (empirical cumulative distribution function in CDF) in R, instead of giving  $P(X \le x)$ for a random variable $X$, it gives the proportion of observations in the data that are $\le X$. </p>

<p>I tried to look up modified ECDF but couldn't find any. Is there any standard function to do the mathematical ECDF? Or any workaround would be appreciated! </p>
",2013-10-15 18:13:52.963
57555,22723.0,1,,,,Randomness and Probability,<probability>,CC BY-SA 3.0,"<p>Suppose the probability of a ""random"" event is very small (call this probability $p$). In real life, true randomness seems impossible. So would the actual true probability of the event be greater than $p$?</p>
",2013-10-15 18:20:33.817
57556,22724.0,2,,57461.0,,,,CC BY-SA 3.0,"<p>Remember that continuous probability distributions can be represented analytically, as a curve in the plane. 
Suppose we have two curves represented by f(x) and g(x). One way to define the distance between them is the greatest value of the absolute value of f(x)-g(x), the distance between the two ordinates at the abscissa x.
If this value is small, then the functions are close. Otherwise, nothing is guaranteed. This forms the ""statistical distance.""</p>

<p>Often, we want to see if our guess of the true probability distribution is correct. One way of doing this is to estimate, using computational methods, the estimated density of the sample. Then, using the test above, we can see if our guess is correct.</p>

<p>This is just reason I can think of.</p>
",2013-10-15 18:23:29.187
57557,5045.0,2,,57492.0,,,,CC BY-SA 3.0,"<p>I think Cameron and Trivedi's <a href=""http://books.google.com/books/about/Microeconometrics.html?id=Zf0gCwxC9ocC"">Microeconometrics</a> fits the bill. It strikes a nice balance between breadth, intuition, and rigor (if you follow up on the references). The target audience is the applied researcher. Not everything is Greene is covered:</p>

<pre><code>I: PRELIMINARIES
1. Overview
2. Causal and Noncausal Models
3. Microeconomic Data Structures

II: CORE METHODS
4. Linear models 
5. ML and NLS estimation
6. GMM and Systems Estimation
7. Hypothesis Tests
8. Specification Tests and Model Selection
9. Semiparametric Methods 
10. Numerical Optimization

III: SIMULATION-BASED METHODS
11. Bootstrap Methods
12. Simulation-based Methods
13. Bayesian Methods

IV:  CROSS-SECTION DATA MODELS
14. Binary Outcome Models
15. Multinomial Models
16. Tobit and Selection Models
17. Transition Data: Survival Analysis
18. Mixture Models and Unobserved Heterogeneity
19. Models of Multiple Hazards
20. Count Data Models

V:  PANEL DATA MODELS
21. Linear Panel Models: Basics
22. Linear Panel Models: Extensions
23. Nonlinear Panel Models

VI: FURTHER TOPICS
24. Stratified and Clustered Samples
25. Treatment Evaluation
26. Measurement Error Models
27. Missing Data and Imputation

APPENDICES
A. Asymptotic Theory
B. Making Pseudo-Random Draws
</code></pre>

<p>Peter Kennedy's <a href=""http://books.google.com/books?id=ax1QcAAACAAJ&amp;dq=editions%3a-UeHRuJqA8MC&amp;hl=en&amp;sa=X&amp;ei=t4ddUr7wNKXCigLjqYDwAg&amp;ved=0CDMQ6AEwAQ"">Principles of Econometrics</a> book does the same with more emphasis on intuition. The target audience is students of different sorts. All the material is covered at three levels. The first one provides the intuition and the main idea, the second one introduces some very basic notation, and the last one provide references to the more complicated topics. Sadly the 6th edition is the last one. TOC is <a href=""http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP001025.html"">here</a>.</p>
",2013-10-15 18:28:05.363
57576,14888.0,1,57577.0,,,Is this a repeated measures design or not?,<anova><repeated-measures>,CC BY-SA 3.0,"<p>How do you describe, or what do you call, a test that uses two factors as independent variables but uses a dependent variable that is a difference between two measures taken repeatedly from the same individuals?</p>

<p>Usually, repeated measures means that multiple independent variables are measured on the same individuals. Here, it is the dependent, outcome variable that is the repeated measure. But, since the difference is taken for each individual, is this simply a two-way ANOVA?</p>
",2013-10-15 21:57:50.220
25072,7421.0,1,25087.0,,,"Differences on exploratory factor analysis, confirmatory factor analysis and principal component analysis",<pca><factor-analysis><confirmatory-factor>,CC BY-SA 3.0,"<p>Before it is pointed, I am aware that a <a href=""https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi"">very similar question was already asked</a>. Still, I am in doubt regarding the concept.</p>
<p>More specifically, it is mentioned by the most voted answer that:</p>
<blockquote>
<p>In terms of a <strong>simple rule of thumb</strong>, I'd suggest that you:</p>
<ol>
<li><p>Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.</p>
</li>
<li><p>Run principal components analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.</p>
</li>
</ol>
</blockquote>
<p><strong>Question 1:</strong></p>
<p>I am having difficulties on understanding based on the results I obtained from R where exactly I am inputing my <em>theoretical model of latent factors</em>. I am using the functions from <a href=""http://www.statmethods.net/advstats/factor.html"" rel=""nofollow noreferrer"">statsmethods</a>. On both <strong>factanal()</strong> and <strong>princomp()</strong> the inputs were the same: A table where each row represented one data point and the columns consisted of different attributes I was interested on reducing. Thus, this add to my confusion on where is this pre assumed model play its role. I noticed that for factor analysis function I used parallel analysis also suggested by the site using the nScree() function to determine the number of factors and I specified if I wanted a varimax (orthogonal) or promax (oblique) rotation. Is that what is it mean by the model? Being able to choose the amount of factors and the type of rotation?</p>
<p>The results being provided as visual graphs for both PCA and EFA also doesn't seem to highlight this difference which adds to my confusion. Where does this distinction can be observed on them?</p>
<p><img src=""https://i.stack.imgur.com/SGK56.jpg"" alt=""PCA"" />
PCA</p>
<p><img src=""https://i.stack.imgur.com/Eg4MN.jpg"" alt=""EFA"" />
EFA</p>
<p><strong>Question 2:</strong> -- Answered</p>
<p>I bought a book to study about this from Richard L. Gorsuch. On this book there is something that the author caught attention on the difference between PCA (Principal Component Analysis) and EFA (Exploratory Factor Analysis): It is mentioned that PCA is for <strong>population</strong> while EFA is for <strong>sample</strong>. Is that true? I didn't see that being mentioned on any discussion I read so far. Is it irrelevant?</p>
<p><strong>Question 3:</strong></p>
<p>I noticed that all those methods seems to impose the normal distribution constraint. I also read that for larger sets this constraint can be ignored. Is that true or PCA, EFA and CFA are sensible to distribution constraint violations?</p>
<p><strong>Question 4:</strong> Where from the results of PCA and EFA should I note that one is talking about latent factors (EFA) and the other is just clustering on components (factors) the variables? The outputs from R looks the same to me. Is it just the way I perceive what the factors being shown as output? I noted that both show me the table where I can see which I can observe which of my variables are expressed the most of my factors. <strong>What is the difference on the interpretation I should have on which variable belongs to which factor</strong> in respect to PCA and EFA? EFA is saying those with higher expression seems to be more explained by that latent factor while PCA is trying to say that factor is holding those variables from what is it observed?</p>
<p><strong>Question 5</strong>
Finally the last question is regarding CFA (Confirmatory Factor Analysis).</p>
<p>On the same function website the following image is being shown:</p>
<p><img src=""https://i.stack.imgur.com/NNrvh.gif"" alt=""Confirmatory Factor Analysis"" /></p>
<p>I read that CFA is usually followed after EFA for hypothesis testing. In that sense, EFA tells you which are the latent factors (which are the output factors) and then you use CFA assuming those factors you observed from EFA for hypothesis testing?</p>
<p><strong>Question 6</strong></p>
<p>For EFA one of the available rotations on the literature is direct oblimium. I heard that it can accounts for both promax and varimax so 'it takes the best of two words'. Is that true? I am also trying to find a function that employs them on R, since the one suggested on the site does not. I would be happy to get any suggestion on this one.</p>
<hr />
<p>I hope it is noted that this question is way more specific on the doubts regarding EFA and PCA and also adds to CFA so not to get closed for being repeated on the subject. If at least one of the questions is answered I am more than happy too as to clarify the confusion in my head.</p>
<p>Thank you.</p>
",2012-05-14 05:55:32.040
57558,19559.0,1,57564.0,,,Puzzling behavior of glmer(),<categorical-data><r><lme4-nlme>,CC BY-SA 3.0,"<p>I'd like your opinion on a very strange behavior that I recently encountered running glmer(). The problem is that when I make the dependent variable into a logical vector, glmer behaves weirdly. My dependent variable is Accuracy, and it is coded in terms of 1 (accurate response) and 0 (wrong response). What puzzles me is that transforming accuracy to a logical vector should work the same way for glmer, as a logical vector is coded in terms of TRUE or FALSE, having also 2 levels. However, glmer gives me different results depending on the transformation of the dependent variable I use. Have you guys encountered this before? Do you know why it happens? Below is sample code so you can replicate the problem yourselves.</p>

<pre><code>#Create fake data
Subject   &lt;- c(rep(""S1"",4), rep(""S2"",4), rep(""S3"",4), rep(""S4"",4))
Item      &lt;- rep(c(""I1"",""I2"",""I3"",""I4""),4)
Factor1   &lt;- c(c(rep(""e1"",2),rep(""e2"",2)), c(""e1"",""e2"",""e2"",""e1""), 
           c(rep(""e2"",2),rep(""e1"",2)), c(""e2"",""e1"",""e1"",""e2""))                  
Accuracy  &lt;- c(1,1,0,0,1,0,1,0,1,0,1,1,1,1,1,1)

#Create data frame and make ""Accuracy"" into a factor with 2 levels
data          &lt;- data.frame(Subject,Item,Factor1, Accuracy)
data$Accuracy &lt;- factor(data$Accuracy)  #Accuracy is a factor w/ 2 levels
#Run glmer
m1 &lt;- glmer(Accuracy ~ Factor1 + (1+Factor1|Subject) + (1+Factor1|Item), family = ""binomial"", data= data)  
summary(m1)
Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)    1.946      1.069   1.820   0.0687 .
Factor1e2     -1.946      1.282  -1.518   0.1290  
---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>That is the output of the first model. Now, look at what happens if I transform data$Accuracy into a logical vector when I run the model:</p>

<pre><code>m2 &lt;- glmer(as.logical(as.numeric(Accuracy)) ~ Factor1 + (1+Factor1|Subject) + (1+Factor1|Item), family = ""binomial"", data= data)  
summary(m2)

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) 2.557e+01  1.259e+05       0        1
Factor1e2   2.223e-06  1.781e+05       0        1
</code></pre>

<p>As you can see, now the coefficient estimates are very different. As I said, this seems very puzzling to me and I'd like yo know if you have some thoughts on why this should be.</p>

<p>Thanks a lot!</p>

<p>--Sol</p>
",2013-10-15 18:35:31.263
57559,9456.0,1,,,,Confusion relative to derivative of partition function,<optimization><graphical-model>,CC BY-SA 3.0,"<p>I have this partition function</p>

<p><img src=""https://i.stack.imgur.com/ve5sd.png"" alt=""enter image description here""></p>

<p>Now if I take the derivative of log(Z(x)) wrt $\lambda_k$</p>

<p>the result is </p>

<p><img src=""https://i.stack.imgur.com/6Qikg.png"" alt=""enter image description here""></p>

<p>I didn't get how this was derived. This is the <a href=""http://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf"" rel=""nofollow noreferrer"">paper</a> </p>
",2013-10-15 18:36:50.983
57560,22724.0,2,,57555.0,,,,CC BY-SA 3.0,"<p>When you flip a coin, you cannot know if it will fall on heads or tails.
There is a chance in will fall of heads and a chance it will fall on tails, both being a half. This seems like true randomness. The theory of probability has the purpose of stamping on events a certain probability. </p>

<p>Logically, your question makes no sense. You assume that something has a probability, and then ask if its probability is greater. You assume, for instance, that the odds of flipping a heads is 0.2, then you say there is no randomness, and conclude that the odds of flipping a heads is 0.4...</p>
",2013-10-15 18:39:42.453
57561,22725.0,2,,27120.0,,,,CC BY-SA 3.0,"<p>They are absolutely NOT the same. </p>

<p>mean SQUARE error:  square the quantity => calculate the error => calculate the mean</p>

<p>mean SQUARED error: calculate the error => square the result   => calculate the mean</p>
",2013-10-15 18:43:28.630
57562,21958.0,1,,,,Model validation in Bayesian statistics from a model with latent variables,<bayesian><markov-chain-montecarlo><residuals><validation><latent-variable>,CC BY-SA 3.0,"<p>I am working with some two-regime autoregressive models first introduced by Hamilton in 1989.  The specific models is of no great concern to my question, but some variables within my autoregressive models are latent binary variables - non-observable binary variables, that is. I have a dataset and use MCMC to find posterior densities of all the parameters of my models, including all the latent binary variables. </p>

<p>I have a dataset of about 1000 observations, and for every observation I have a latent variable in my models, which can wither be 1 or 0. If my data suddendly changes, I can assume that a switch has been turned on and the latent variable has changed. So if the data, at a specific time, is (most likely) best described by the latent variable being 1, my sampler will produce a posterior distribution (for this specific latent variable) with high probability of 1 and low probability of 0.</p>

<p>So this is all good, I have managed to do this. The question is, how do I validate my model? AIC/BIC is just for comparing models. 
What I am thinking is that I have to be able to plot the residuals somehow, like in normal regression where you just say that the residual is the difference of observed and predicted result. 
I understand that for all other parameters than the latent parameters/variables, i can just use the mean of each posterior distribution and treat it like a Maximum Likelihood Estimator. But I cant take the mean of my posterior distributions for all the latent variables, because they are either 0 or 1. It does not make any sense using, for example, 0.8 for one of my latent variables. This is no option.</p>

<p>So how should I go on to validate my model? I am really stuck here. If I have done a bad job explaining my problem, I apologize and will try to explain better.</p>

<hr>

<p>So I have been thinking. Can I use a predictive posterior distribution to validate my model? And if too many observations, lies above, say, the 95 percentile of the predictive posterior distribution, I throw away my model?</p>

<p>Generally, for any model whose parameters are called theta, observations called x, and new observation called x_new, one calculate the predictive posterior distribution as follows:
For i = 1, . . . , n (samples), we sample:
1. theta[i]
from p(theta|x) and
2. x_new[i]
from p(x_new|theta[i])
Then x_new[1], . . . ,x_new[n]
are a sample from the predictive posterior distribution. But this method must surely only work when we have a iid sample. I am working with an autoregressive model. So I am stuck...</p>
",2013-10-15 18:51:33.203
57563,22726.0,1,,,,Sampling and Conditions,<sampling>,CC BY-SA 3.0,"<p>Suppose we consider all the people in the planet. We are interested in randomly selecting 10 people who have heart disease. Is it better to repeatedly sample 10 people and then choose the one selection in which all 10 people have heart disease? Or is it better to only look at the people who have heart disease and sample 10 from that population? </p>
",2013-10-15 19:10:55.417
57564,2857.0,2,,57558.0,,,,CC BY-SA 3.0,"<p>More of a programming question. Compare:  </p>

<pre><code>&gt; as.logical(as.numeric(data$Accuracy)) 
 [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE 
[16] TRUE 
&gt; as.logical(as.numeric(Accuracy)) 
 [1]  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE 
[13]  TRUE  TRUE  TRUE  TRUE 
</code></pre>

<p>You're performing the former with your call to glmer since you are using the <code>data = ...</code> argument  </p>

<pre><code>m2 &lt;- glmer(as.logical(as.numeric(Accuracy)) ~ Factor1 + (1+Factor1|Subject) + (1+Factor1|Item), family = ""binomial"", data= data)
</code></pre>

<p>As to why this is happening:</p>

<pre><code>&gt; as.numeric(data$Accuracy) 
 [1] 2 2 1 1 2 1 2 1 2 1 2 2 2 2 2 2 
&gt; as.numeric(Accuracy) 
 [1] 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1
</code></pre>

<p>Basically <code>as.numeric</code> returns the numeric representation of the levels of a factor variable, and then <code>as.logical</code> treats all non-zero values as <code>TRUE</code> (not entirely sure about negative values, actually). To get the original values back, you need to use</p>

<pre><code>&gt; as.numeric(levels(data$Accuracy)[data$Accuracy]) 
 [1] 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 
</code></pre>

<p>Thus...</p>

<pre><code>&gt; m2 &lt;- glmer(as.logical(as.numeric(levels(Accuracy)[Accuracy])) ~ Factor1 + (1+Factor1|Subject) + (1+Factor1|Item), family = ""binomial"", data= data)   
&gt; summary(m2) 
... 
Fixed effects: 
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)    1.946      1.069   1.820   0.0687 . 
Factor1e2     -1.946      1.282  -1.518   0.1290  
</code></pre>
",2013-10-15 19:12:09.373
57565,22727.0,2,,57563.0,,,,CC BY-SA 3.0,"<p>So long as it is truly random, and your hypothetical list covers everyone on the planet, then it would be most efficient to only select from the group with heart disease as that is what you are interested in studying. </p>

<p>There shouldn't be any difference between randomly sampling 10 until you have 10 with heart disease and randomly sampling 10 from those that you know have have heart disease. This is assuming your list of those with heart disease includes everyone with heart disease on the planet.</p>
",2013-10-15 19:21:21.580
57566,22728.0,1,,,,symmetric r.v. raised to an odd power,<mean><symmetry>,CC BY-SA 3.0,"<p>My prof claims that raising a symmetric r.v., like N(0,1), to an odd power gives a distribution with expectation 0.  What's the best way to see this?</p>
",2013-10-15 19:27:55.210
57569,21243.0,2,,57555.0,,,,CC BY-SA 3.0,"<p>Your question raises the philosophical question of whether or not randomness is possible - while it's an interesting subject, we pretty genuinely don't care. </p>

<p>We do care, however, about your particular interpretation of probability, which is the important part (though they both agree that the answer to your question is ""No, the probability is $p$"", based on their individual criteria). Briefly consider the two prevailing interpretations:</p>

<ol>
<li>A <a href=""http://en.wikipedia.org/wiki/Frequentist_probability"" rel=""nofollow"">Frequentist</a> would say that the given probability $p$ is the fraction of times that the event occurs in a very large number of trials. In this case, the number $p$ arises as a result of a large number of empirical trials; it is our best estimate based on the experimental evidence at hand.</li>
<li>A <a href=""http://en.wikipedia.org/wiki/Bayesian_probability"" rel=""nofollow"">Bayesian</a> would say that based on some sufficient evidence, we have calculated the number $p$ to express how sure we are of the outcome of the event. Note that we need not to have seen a large number of trials or any trials at all; this is a direct epistemological claim about our sureness in the outcome of the event.</li>
</ol>

<p>Both views agree that we could be wrong; however, $p$ is still the best bet we have. So in a way it is true that under both interpretations $p$ <em>could</em> be incorrect; however, it is not particularly helpful to say so and undermines the usefulness of using $p$ in the first place. </p>
",2013-10-15 20:18:20.350
57570,668.0,2,,57545.0,,,,CC BY-SA 3.0,"<p><strong>There are two huge problems with this question that make it unanswerable.</strong></p>

<p><strong>First, a <a href=""http://en.wikipedia.org/wiki/Sample_space"" rel=""nofollow"">sample space</a> is a set of outcomes of an experiment.</strong>  A random variable is a function assigning a unique real value to each outcome.  A probability measure on the sample space determines the distribution of the random variable.  <em>Given only the distribution, we cannot possibly identify the sample space.</em>  For instance, the random variable assigning the value $0$ to ""tails"" and $1$ to ""heads"" to describe outcomes of the flip of a fair coin has sample space {""heads"", ""tails""}.  It has a Bernoulli$(1/2)$ distribution.  The random variable assigning the value $0$ to all points in the Earth's northern hemisphere and $1$ to all points in its southern hemisphere has a sample space consisting of all points on Earth.  If all points are considered equally probable, then this variable, too, has a Bernoulli$(1/2)$ distribution--but obviously points on the Earth are not flips of a coin!</p>

<p>Second, let's re-interpret the question to ask about the set of possible values of a random variable (its <em>range</em> as a function), because maybe there is a chance this could be answered with the information given.  Unfortunately, <strong>the question is still ambiguous.</strong> This can be shown by exhibiting two different distributions with different ranges that nevertheless satisfy the given conditions: namely, there exists some number $\pi$ such that the probability of each possible value $x$ is given by the formula $(1-\pi)^x\pi.$  Such a formula suggests (but does not explicitly indicate) that $x$ is intended to be integral, which helps limit our search for counterexamples.</p>

<ol>
<li><p>Let the range be $\{-1, 0\}$.  The total probability is</p>

<p>$$1 = (1-\pi)^{-1}\pi + (1-\pi)^0\pi.$$</p>

<p>One solution is $\pi = (3 - \sqrt{5})/2 \approx 0.381966.$</p></li>
<li><p>Let the range be $\{-1, 0, 1\}$.  The total probability is</p>

<p>$$1 = (1-\pi)^{-1}\pi + (1-\pi)^0\pi + (1-\pi)^1\pi.$$</p>

<p>One solution is a root of $x^3 - 3 x^2 + 4 x - 1$ approximately equal to $0.317672.$  (It is the only root lying between $0$ and $1.$)</p></li>
</ol>

<p>The first distribution assigns probabilities $0.618034$ to $-1$ and $0.381966$ to $0$; the second distribution assigns probabilities $0.465571$ to $-1$, $0.317672$ to $0$, and $0.216757$ to $1$: obviously they are different and have different ranges.</p>
",2013-10-15 20:50:42.840
57571,21840.0,2,,57451.0,,,,CC BY-SA 3.0,"<p>By drawing the regions and taking integrals we get:</p>

<p>$P(V^{2} - UW \geq 0) = P(V^{2} \leq  UW, 0\leq U,V,W \leq 1)$
$=P(0 \leq V \leq 2\sqrt{UW},0\leq U,V,W \leq 1)$
$=P(0 \leq V \leq min(1,2\sqrt{UW}),0\leq U,V,W \leq 1)$
$=\int_0^\frac{1}{4}\int_0^1\int_0^{2\sqrt{uw}}dudvdw + \int_\frac{1}{4}^1\int_0^{\frac{1}{4u}}\int_0^{2\sqrt{uw}}dudvdw + \int_{\frac{1}{4}}^1\int_\frac{1}{4u}^1\int_0^1dudvdw $</p>

<p>Solving the integrals we get the same answer, that is , approx 0.2544.</p>
",2013-10-15 21:03:50.640
57572,22732.0,1,,,,Prediction based on repeated measure with binary outcome,<mixed-model><repeated-measures><survival><logit><binary-data>,CC BY-SA 3.0,"<p>I'd like to make a prediction about a subject's likelihood that a certain outcome will occur based on a series of measurements taken over time. </p>

<p>The easiest way to explain the problem is as follows. Imagine that I'm tracking an individual over time to generate a probability of ever getting lung cancer.</p>

<p>The training data would look something like this with the outcome being ""has cancer""</p>

<pre><code>
ID           age   cigarettes  asbestos   lung        has cancer
                     per day   exposure   capacity
S_1           5       0           N          20             N
S_1           17      20          N          50             N
S_1           31      50          N          30             N
S_1           42      0           N          40             Y
S_2           15      0           N          55             N
S_2           37      0           N          45             N
S_3           5       3           Y          15             N
S_3           50      0           Y          30             Y
etc...
</code></pre>

<p>The test data would be taken before an individual has gotten cancer and would predict their probability of getting cancer at any time in the future.</p>

<p>What's the best way to model this?</p>

<p>I'm currently experimenting with mixed models, treating this like a repeated measure problem. However, the dependent variable (whether someone ever gets cancer) is not being repeatedly measured over time. Instead it is fixed, so it seemed this might not be the best approach. As you can see there are additional problems of the dataset being unbalanced, with measures taken at irregular periods.</p>

<p><strong>EDIT 1:</strong> Would it be better to think of this in terms of survival analysis? I assume the event in this case would be contracting cancer. If so how do I deal with deaths not related to lung cancer?</p>

<p><strong>Edit 2:</strong> I'm thinking now of using a joint model that combines a mixed effect model with a survival analysis model: <a href=""http://rwiki.sciviews.org/doku.php?id=packages:cran:jm"" rel=""nofollow"">http://rwiki.sciviews.org/doku.php?id=packages:cran:jm</a>. Any thoughts as to whether this might work for my problem? It seemed that I could use the hazard function to get a prediction for likelihood of ever getting cancer.</p>
",2013-10-15 21:24:40.827
57573,132.0,2,,57568.0,,,,CC BY-SA 3.0,"<p>This was studied by <a href=""http://www.jstor.org/stable/2345178"" rel=""noreferrer"">Granger and Morris (1976)</a> who showed that</p>

<p>AR($p$) + AR($q$) = ARMA($p+q,\max(p,q)$).</p>
",2013-10-15 21:33:45.000
57574,668.0,2,,57563.0,,,,CC BY-SA 3.0,"<p>The question of equivalence is settled by considering <em>every possible outcome of the sampling procedure</em> and determining its chance of being <em>the</em> sample.  <strong>When two procedures select all outcomes with the same probabilities, they are statistically equivalent.</strong></p>

<hr>

<p>In the first case, we repeatedly sample subsets of $10$ people out of all $N$ on the planet.  We do so in a way that selects every one of the $\binom{N}{10}$ distinct subsets with equal probability.  We repeat until the one we select is a subset of the $M\le N$ people who have heart disease.  (Let's call such a subset ""desirable"" and all other subsets ""undesirable."")  That makes the procedure somewhat complicated: it is a <em>process</em> that can take an arbitrarily large number of steps.</p>

<p>One way to compute the chance of a particular set of diseased patients being selected is to break down the process according to how many steps were taken: that set was either selected on the first attempt, or on the second attempt, or, ..., or on the $K^\text{th}$ attempt, or ... .  The chance of being selected on the $K^\text{th}$ attempt is the chance of being selected on that attempt given that on <em>every one</em> of the preceding $K-1$ attempts, one of the $\binom{N}{10}-\binom{M}{10}$ undesirable subsets was selected.  <em>Assuming</em> the separate attempts were independent, this chance is computed by multiplying the chances at each attempt, giving</p>

<p>$$\left(\frac{\binom{N}{10}-\binom{M}{10}}{\binom{N}{10}}\right)^{K-1}\frac{\binom{M}{10}}{\binom{N}{10}}$$</p>

<p>for the chance of being selected on the $K^\text{th}$ attempt.  Summing these values for $K=1, 2, \ldots$ gives the chance of being selected.  Fortuately, <em>we do not actually have to calculate this chance</em>: it suffices to observe that it does not depend on the particular set we have under consideration, but only on the numbers $N,$ $M,$ and $10.$  Thus, <em>the first sampling procedure selects all desirable sets with equal probability.</em>  Obviously that's what the second procedure does (by design), so the procedures are statistically equivalent, even though they have been conducted differently.</p>

<hr>

<p>This analysis ought to look like overkill, because it is: it should be obvious that all desirable sets have the same chance of being selected using either procedure, because no individual is favored in the selection nor are the selections of any individuals interdependent (as would happen if, for instance, entire households of people were selected at a time).  The reason for presenting this analysis in such detail is to demonstrate, with a simple example, how one might go about evaluating <em>any</em> sampling procedure of a defined population: namely, you determine the chance of every possible sample.  </p>

<p>Although the concept is simple, the results can be very illuminating in complex situations, such as hierarchical sampling or spatial sampling schemes.  For instance, a common way to sample soils in a field is to pick a random origin and a random orientation, lay out a regular grid of points starting at that origin and oriented accordingly, and taking a sample at each grid point that falls within the field.  By emulating the analytical process exemplified here, you will be led to discover that <em>many possible sets of samples have no chance of being selected at all.</em>  For instance, they won't all be in the same half of the field.  This provides valuable insight into why the usual statistical procedures are not valid to apply to such samples (except, occasionally, as approximations).</p>
",2013-10-15 21:39:35.160
57575,3183.0,2,,57543.0,,,,CC BY-SA 3.0,"<p>I'm not sure how you got 5 degrees of freedom with seven independent variables for your <code>glm</code>-based model, but I'll assume that's just a typo somewhere or that I'm missing something minor.</p>

<p>Anyway, counting degrees of freedom with models that are constrained by a prior can be tricky, and there isn't necessarily a ""correct"" way to do it in many cases.  Perhaps the authors of <code>arm</code> used -1 degrees of freedom as a way to keep people from blindly misinterpreting the results.</p>

<p>Although we can't easily calculate the number of degrees of freedom for most regularized models, we can at least put an upper bound on it: the number of degrees of freedom must be less than or equal to the degrees of freedom for the corresponding un-regularized model.</p>

<p>So (assuming the 5 degrees of freedom you reported above is correct), you can plug in 5 and be confident that the true P-value will be no larger than what your Chi-square test predicts.  Thus, if it's significant with 5 degrees of freedom, the true value will also be significant.</p>

<p>If you want something more exact, you might want to look into using the lasso or ridge regression for regularization instead: statisticians have invested a lot of effort into counting degrees of freedom for these models, and have even developed some significance tests for them.  Andrew Gelman talks about one recent advance on his blog <a href=""http://andrewgelman.com/2013/03/18/tibshirani-announces-new-research-result-a-significance-test-for-the-lasso/"" rel=""noreferrer"">here</a>.</p>

<p><strong>Edited to add</strong>: If you do stick with <code>bayesglm</code> but don't trust the null deviance estimates, you can find it yourself by running a model with no predictors except the intercept.  The formula syntax for this would be <code>a ~ 1</code>.</p>
",2013-10-15 21:46:27.687
58281,3580.0,2,,58279.0,,,,CC BY-SA 3.0,"<p>Obviously whenever $X_1,X_2$ are independent but I guess that's not the point.</p>

<p>My go-to for dependent rvs is $U$ uniform on $[0,1]$ and take $X_1 = \sin(2\pi U), X_2=\cos(2\pi U)$. This basically says that if you pick a point uniformly on the unit circle then the coordinate functions are uncorrelated. This fact boils down to showing 
$$
\int_0^{2\pi} \sin(t)\cos(t) \ dt = 0.
$$</p>
",2013-10-26 22:11:17.093
57577,503.0,2,,57576.0,,,,CC BY-SA 3.0,"<p>It could be called a change score design.</p>

<p>If you have only two repetitions of the measure, then it is one reasonable choice. However, it is very good to have more than two repetitions. If the dependent variable is measured with error (and which ones aren't?) then the change score is partly due to statistical error. e.g. suppose two people have identical true scores at both time 1 and time 2. But, for random reasons, they won't score the same at time 1 or time 2. Indeed, since </p>

<p>$O = T + E$</p>

<p>where O is observed score, T is true score and E is error, then, if $T_{11} = T_{21}$ and $T_{12} = T_{22}$  (where $T_{ij}$ is the score for person i at time j) then the change scores are just the changes in the errors and it's all regression to the mean. </p>
",2013-10-15 22:08:23.610
57578,16144.0,1,,,,GLM with two related predictors (X and X-squared),<multiple-regression>,CC BY-SA 3.0,"<p>I am running a general linear model in which I have two predictors: X and X-squared.</p>

<p>I entered both these predictors in my analysis because I think X might explain the variance in the outcome measure partially linearly and partially in a quadratic fashion.</p>

<p>Obviously, there is multicollinearity in this example. However, I was wondering if there are maybe some reasons why it is not a good idea to put both these predictors in my model.</p>
",2013-10-15 22:18:53.827
57579,22736.0,1,57580.0,,,In statistics what does NA stand for?,<notation><abbreviation>,CC BY-SA 3.0,"<p>I understand that NA means data is missing, null or not present.  But what do the letters NA stand for?  ""Not Available""?</p>
",2013-10-15 22:50:21.217
57580,9483.0,2,,57579.0,,,,CC BY-SA 3.0,"<p>In datasets, NA can mean:</p>

<ul>
<li>""<em>Not Available</em>"": e.g. the sensor was down at the time of the measure,</li>
<li>""<em>Not Applicable</em>"": e.g. when asking a bachelor the name of his wife,</li>
<li>""<em>No Answer</em>"": e.g. the respondent to a questionnaire skipped a question.</li>
</ul>
",2013-10-15 22:53:52.320
57581,1741.0,2,,57453.0,,,,CC BY-SA 3.0,"<p>It depends on which language you are more familiar with. </p>

<p><code>randomForest</code> package implements the original Fortran version of Breiman's random forest. You should try to modify the Fortran code then. </p>

<p><code>party</code> packages has everything implemented in C. So, you can try to modify the C code.</p>

<p>WEKA <code>RandomForest</code> is implemented in Java and involves the classes <code>Bagging</code> and <code>RandomTree</code>.</p>

<p>Honestly, I am more familiar with Java and I would use WEKA then. I actually implemented some ensemble pruning techniques in WEKA and it was pretty simple.</p>
",2013-10-15 23:13:06.160
57582,22740.0,1,,,,R correlation between two time series analysis,<r><time-series><correlation>,CC BY-SA 3.0,"<p>This is my very first post on CV so comment if I can improve my post.</p>

<p>I have two webs that sell very similar products and also have a very similar group of customers.</p>

<p>I was trying to prove that concept that ""Fast Moving Inventory From Web A Will Also Move Fast on Web B""</p>

<p>I have the daily snapshots of the inventory of both WebA and WebB. For example, for the product A from Web A, data looks like this:</p>

<pre><code>TimeStamp  Product  Stock  UnitPrice
Oct 1st    A        100    1.2
Oct 2nd    A        90     1.2
Oct 3rd    A        40     1.2
Oct 4th    A        240    1.2
..
</code></pre>

<p>For those two Webs, some of their products are exactly the same. For example, WebA and WebB both have product A. I am wondering is there a way to use R to do some time series analysis so I can prove that for those products that they have in common. There is a very high correlation (WebA sell product A crazy last weekend, which is also a good sale reflected on Web B's data)?</p>

<p>Is there some R functions to do what I want? Then I can tell Web B that you need to carry those top selllers of Web A but Web B doesn't carry now.</p>
",2013-10-15 23:42:13.630
57583,22741.0,1,,,,Estimate the parameters of beta exponential distribution via L-Moments,<estimation><censoring><l-moments>,CC BY-SA 3.0,"<p>Estimate 3 parameters of beta exponential distribution in the case of censored type 1 samples via L-moments</p>
",2013-10-16 00:11:38.053
57584,15539.0,1,57587.0,,,Partial F ratio from ANOVA table,<regression><anova><multiple-regression>,CC BY-SA 3.0,"<p>In multiple regression, if you have just an ANOVA table, and nothing else, no specific data, how can you do a partial F test on X1, given X2 is already in the model?</p>

<p>So, you have the ANOVA table:</p>

<pre><code>source        df   SS    MS     F
-----------------------------------
regression    2    1.44  0.72   9.72
error         3    0.22  0.07
total         5    1.66
-----------------------------------
</code></pre>

<p>All values are filled in. With only this information, how can you do the partial F test where: </p>

<ul>
<li>F = MSR(X1|X2) / MSE(X1, X2)</li>
<li>MSR(X1|X2) = SSR(X1, X2) - SS(X2) = 1.44 - ????</li>
<li>MSE (X1, X2) = MSE = 0.07</li>
</ul>

<p>SSR(X1, X2) can be obtained from the table (SS regression)
MSE(X1, X2) can also be obtained from the table (just MSE)
but I cannot get SS(X2) from the table, as far as I know......</p>

<p>As far as I know, you need specific X and Y values to do this. Any other way from just the table?</p>
",2013-10-16 01:00:58.310
57585,10684.0,2,,57559.0,,,,CC BY-SA 3.0,"<p>Write $\mathbf{x} = \mathbf{x}^{(i)}$ just to avoid writing superscript $(i)$'s everywhere.
$$\frac{\partial}{\partial \lambda_k} \log Z(\mathbf{x}) = \frac{1}{Z(\mathbf{x})} \frac{\partial}{\partial \lambda_k} Z(\mathbf{x}) = \frac{1}{Z(\mathbf{x})} \sum_\mathbf{y} \frac{\partial}{\partial \lambda_k} \exp\left(\sum_{j=1}^K \lambda_j f_j(y_t, y_{t-1}, \mathbf{x}_t)\right)$$
which, using the ordinary chain rule for functions of one variable, gives
$$\frac{1}{Z(\mathbf{x})} \sum_\mathbf{y} f_k(y_t, y_{t-1}, \mathbf{x}_t) \exp\left(\sum_{j=1}^K \lambda_j f_j(y_t, y_{t-1}, \mathbf{x}_t)\right) = \sum_\mathbf{y}f_k(y_t, y_{t-1}, \mathbf{x}_t) \frac{\exp\left(\sum_{j=1}^K \lambda_j f_j(y_t, y_{t-1}, \mathbf{x}_t)\right)}{Z(\mathbf{x})} $$
By the definition of $p(\mathbf{y}|\mathbf{x})$ in Equation 1.16 of the paper, this is
$$\sum_\mathbf{y} f_k(y_t, y_{t-1}, \mathbf{x}_t) p(\mathbf{y}|\mathbf{x}).$$
Now $\mathbf{y}$ is shorthand for $(y,y')$ and the sum over all possible $\mathbf{y}$ has been rewritten as $\sum_{y, y'}$.</p>
",2013-10-16 01:03:06.297
57597,22747.0,1,,,,Which AIC value to use from R's sarima() function for model comparison,<time-series><arima>,CC BY-SA 3.0,"<p>I'm using R's 'astsa' package and I get the following output from sarima. </p>

<p>Which AIC value would I use to compare this model (let's call it A) against others? When trying another model (B), model A's fit$AIC (858.19) is greater than model B's, but model A's AIC (12.38841) is less than model B's, so I'm not sure which model to choose.</p>

<p>What's the difference between the two AIC's, AICc's, and BIC's? I've checked the sites below, among others, but haven't been able to figure it out. Any help is much appreciated.</p>

<p><a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/arima.html"" rel=""nofollow"">http://stat.ethz.ch/R-manual/R-patched/library/stats/html/arima.html</a></p>

<p><a href=""http://www.inside-r.org/packages/cran/astsa/docs/sarima"" rel=""nofollow"">http://www.inside-r.org/packages/cran/astsa/docs/sarima</a></p>

<pre><code>$fit
Series: xdata 
ARIMA(0,1,1)(1,1,1)[12]                    

Coefficients:
         ma1    sar1     sma1
     -0.3282  0.5529  -0.8835
s.e.   0.3290  0.4751   0.8635

sigma^2 estimated as 82513:  log likelihood=-425.1
AIC=858.19   AICc=858.93   BIC=866.5

$AIC
[1] 12.38841

$AICc
[1] 12.42448

$BIC
[1] 11.48327
</code></pre>
",2013-10-16 06:55:06.430
57586,20473.0,2,,57319.0,,,,CC BY-SA 3.0,"<p><em>(This answer uses results from W.H. Greene (2003), Econometric Analysis, 5th ed. ch.21)</em>  </p>

<p>I will answer the following modified version, which I believe accomplishes the goals of the OP's question : <strong>""If we only estimate a logit model with one binary regressor of interest and some (dummy or continuous) control variables, can we tell whether dropping the control variables will result in a change of sign for the (coefficient of) the regressor of interest?""</strong></p>

<p><strong>Notation:</strong> Let $RA\equiv Y$ be the dependent variable, $HHS \equiv X$ the binary regressor of interest and $\mathbf Z$ a matrix of control variables. The size of the sample is $n$. Denote $n_0$ the number of zero-realizations of $X$ and $n_1$ the number of non-zero realizations, $n_0+n_1=n$. Denote $\Lambda()$ the cdf of the logistic distribution.<br>
Let the model including the control variables (the ""unrestricted"" model) be</p>

<p>$$M_U : \begin{align} &amp;P(Y=1\mid X,\mathbf Z)=\Lambda(X, \mathbf Z,b,\mathbf c)\\ &amp;P(Y=0\mid X,\mathbf Z)=1-\Lambda(X, \mathbf Z,b,\mathbf c) \end{align}$$</p>

<p>where $b$ is the coefficient on the regressor of interest.<br>
Let the model including only the regressor of interest (the ""restricted"" model) be</p>

<p>$$M_R : \begin{align} &amp;P(Y=1\mid X)=\Lambda(X, \beta)\\ &amp;P(Y=0\mid X)=1-\Lambda(X,\beta) \end{align}$$</p>

<p><strong>STEP 1</strong>  </p>

<p>Consider the unrestricted model. The first-derivative of the log-likelihood w.r.t to $b$ and the condition for a maximum is</p>

<p>$$\frac {\partial \ln L_U}{\partial b}= \sum_{i=1}^n\left[(y_i-\Lambda_i(x_i, \mathbf z_i,b,\mathbf c)\right]x_i=0 \Rightarrow  b^*: \sum_{i=1}^ny_ix_i=\sum_{i=1}^n\Lambda_i(x_i, \mathbf z_i,b^*,\mathbf c^*)x_i \;[1]$$</p>

<p>The analogous relations for the restricted model is
$$\frac {\partial \ln L_R}{\partial \beta}= \sum_{i=1}^n\left[(y_i-\Lambda_i(x_i,\beta)\right]x_i=0 \Rightarrow  \beta^*: \sum_{i=1}^ny_ix_i=\sum_{i=1}^n\Lambda_i(x_i, \beta^*)x_i \qquad[2]$$</p>

<p>We have </p>

<p>$$\Lambda_i(X,\beta^*) = \frac {1}{1+e^{-x_i\beta^*}}$$</p>

<p>and since $X$ is a zero/one binary variable relation $[2]$ can be written</p>

<p>$$\beta^*: \sum_{i=1}^ny_ix_i=\frac {n_1}{1+e^{-\beta^*}} \qquad[2a]$$</p>

<p>Combining $[1]$ and $[2a]$ and using again the fact that $X$ is binary we obtain the following equality relation between the estimated coefficients of the two models:</p>

<p>$$\frac {n_1}{1+e^{-\beta^*}} = \sum_{i=1}^n\Lambda_i(x_i, \mathbf z_i,b^*,\mathbf c^*)x_i $$</p>

<p>$$\Rightarrow \frac {1}{1+e^{-\beta^*}} = \frac {1}{n_1}\sum_{x_i=1}\Lambda_i(x_i=1, \mathbf z_i,b^*,\mathbf c^*) \qquad [3]$$</p>

<p>$$\Rightarrow \hat P_R(Y=1\mid X=1) =  \hat {\bar P_U}(Y=1\mid X=1,\mathbf Z) \qquad [3a]$$</p>

<p>or in words, that the estimated probability from the restricted model will equal the restricted <em>average</em> estimated probability from the model that includes the control variables.  </p>

<p><strong>STEP 2</strong><br>
For a sole binary regressor in a logistic regression, its marginal effect $m_R(X)$ is</p>

<p>$$ \hat m_R(X)= \hat P_R(Y=1\mid X=1) - \hat P_R(Y=1\mid X=0)$$</p>

<p>$$ \Rightarrow \hat m_R(X) = \frac {1}{1+e^{-\beta^*}} - \frac 12$$</p>

<p>and using $[3]$</p>

<p>$$  \hat m_R(X) = \frac {1}{n_1}\sum_{x_i=1}\Lambda_i(x_i=1, \mathbf z_i,b^*,\mathbf c^*) - \frac 12 \qquad [4]$$</p>

<p>For the unrestricted model that includes the control variables we have</p>

<p>$$ \hat m_U(X)= \hat P_U(Y=1\mid X=1, \bar {\mathbf z}) - \hat P_U(Y=1\mid X=0, \bar {\mathbf z})$$</p>

<p>$$\Rightarrow \hat m_U(X) = \frac {1}{1+e^{-b^*-\bar {\mathbf z}'\mathbf c^*}} - \frac {1}{1+e^{-\bar {\mathbf z}'\mathbf c^*}} \qquad [5]$$</p>

<p>where $\bar {\mathbf z}$ contains the <em>sample means</em> of the control variables.</p>

<p>It is easy to see that the marginal effect of $X$ has the same sign as its estimated coefficient. Since we have expressed the marginal effect of $X$ from both models in terms of the estimated coefficients from the unrestricted model, we can estimated only the latter, and then calculate the above two expressions ($[4]$ and $[5]$) which will tell us whether we will observe a sign reversal for the coefficient of $X$ or not, without the need to estimate the restricted model.  </p>
",2013-10-16 01:22:22.130
57587,5237.0,2,,57584.0,,,,CC BY-SA 3.0,"<p>As @PatrickCoulombe hints, you can't conduct a partial F-test with (only) the information in that ANOVA table.  Let's assume you want to conduct the partial F-test for X1, where your full model includes both X1 and X2.  In that case, you would need the ANOVA table for the full model, and the ANOVA table for the reduced model, which would only include X2.  The reason you can't find the number to put in place of the ""????"" is because that number isn't listed in the ANOVA table you have access to--you need the reduced model ANOVA table as well.  </p>

<p>The equations you list in the question aren't quite right.  Your equation for the F ratio is right, and MSE is right, but your equation for MSR is actually for <strong>SS</strong>(X1|X2).  Having calculated that, you get the MS(X1|X2) by dividing by the appropriate degrees of freedom, which is the degrees of freedom for those regressors that were dropped / you are testing (in your case, I'm guessing the df for X1=1).  You calculate the F by dividing the two MSs, as you list; the realized F can be assessed against the theoretical distribution for F with numerator degrees of freedom equal to the df for dropped predictors, and denominator df equal to the df(residual) in the <em>full</em> model.  </p>

<p>For a fuller understanding of this topic, it might help you to read my answers here: <a href=""https://stats.stackexchange.com/questions/33059//33090#33090"">Testing for moderation with continuous vs categorical moderators</a>, and possibly here: <a href=""https://stats.stackexchange.com/questions/20452//20455#20455"">how to interpret type I (sequential) ANOVA and MANOVA</a>.</p>
",2013-10-16 01:38:21.057
57588,15539.0,1,,,,"Best regression model, given coefficient of variation $R^2$ and mean-squared error",<regression><self-study><anova><multiple-regression>,CC BY-SA 3.0,"<p>If you have 3 separate models in a multiple regression problem (and 3 ANOVA tables), which would be best given that you have the coefficient of determination, $R^2$, and mean-squared error values?</p>

<p>So you have 1 table, with just $X_1$, another with just $X_2$ and a third with $X_1$, $X_2$ combined. Which is best given specific $R^2$ values and MSE values?</p>

<p>I assume you're looking for the highest $R^2$ values, but how does MSE play into this?</p>
",2013-10-16 02:18:25.063
57589,22743.0,1,,,,Plotting data from several files on one plot,<dataset><ggplot2><scatterplot>,CC BY-SA 3.0,"<p>I have more than one file of data from different experiments. For the sake of the argument here, let's say I have three files <code>E1.txt</code>, <code>E2.txt</code>, <code>E3.txt</code>. </p>

<p>I anticipate that I will have more files in future, say <code>E4.txt</code>, <code>E5.txt</code>, etc. </p>

<p>Each of these files has two columns, Time(h) and Growth. Plotting data from one file, for instance, E1.txt, is simple. I just read the data from <code>E1.txt</code> using the <code>read.table</code> function in R. I then assign the data from <code>E1.txt</code> to variable <code>E1</code>. </p>

<p>Then I just use the plot function this way, <code>plot(E1)</code>. </p>

<p>Now, what do I need to write in a script that will plot all the data from the <code>E1.txt</code>, <code>E2.txt</code>, and <code>E3.txt</code> files on the same plot? </p>

<p>I thought of writing <code>plot(c(E1,E2,E3))</code>, but it doesn't work. </p>

<p>Thanks for the help.</p>
",2013-10-16 03:44:01.090
57590,9483.0,2,,57589.0,,,,CC BY-SA 3.0,"<p>One way to do it is to use <a href=""http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/points.html"" rel=""nofollow noreferrer""><code>points</code></a>:</p>

<pre><code>x &lt;- seq(0, 2*pi, len = 51)
y1 = sin(x)
y2 = cos(x)
plot(x, y1)
points(x, y2, col = ""red"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/HzZK2.png"" alt=""enter image description here""></p>

<p>If your data files share a common axis, you can use <a href=""http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/matplot.html"" rel=""nofollow noreferrer""><code>matplot</code></a>:</p>

<pre><code>a &lt;- matrix (rnorm(20), c(5,1))
matplot(a, type = ""b"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/aboSf.png"" alt=""enter image description here""></p>
",2013-10-16 04:11:24.593
57598,95860.0,1,,,user2885313,How to calculate the p-value for a binomial test using pbinom?,<r><hypothesis-testing><binomial-distribution>,CC BY-SA 3.0,"<p>25 patients receive treatment, and they each get a difference score (After-Before). 18 of them get positive scores. I would like to test the null hypothesis that p=0.5.</p>

<p>I know that <code>1-pbinom(17,25,0.5)</code> gives me the probability of 18 or more patients getting a positive score. My question is, to calculate the p-value, why do we have to do</p>

<pre><code>2*(1-pbinom(17,25,0.5))
</code></pre>

<p>That is, why do we multiply by 2 here?</p>
",2013-10-16 07:12:18.720
57599,20463.0,2,,57518.0,,,,CC BY-SA 3.0,"<p>Unfortunately there are many possibilities.</p>

<p>1) Are seasonal factors  stochastic or deterministic?
2) Does these seasonal factors affect both dependent and independent variables?
3) Do you have a system of simultaneous dynamic equations which has to be estimated jointly?</p>

<p>Following article by K. Wallis is interesting.</p>

<p><a href=""http://www.nber.org/chapters/c3905.pdf"" rel=""nofollow"">http://www.nber.org/chapters/c3905.pdf</a></p>

<p>In most detailed level you have a multivariate input-output system with very complicated transfer function - polynomial matrix operator structure.</p>

<p>It seems that doing seasonal adjustment for the time-series separately leads at least to the inefficient estimates.</p>

<p>Regards,</p>

<p>-A</p>
",2013-10-16 07:15:01.223
57591,21243.0,2,,53439.0,,,,CC BY-SA 3.0,"<p>As the user above points out, this is not simply an issue of choosing a machine learning method and letting it go wild on the data. Particularly, the introduction of decision theory (even in a basic form) is vital. While the inclusion of something like ""decision theory"" might seem a bit complex, this really only means in your case that you are not simply estimating a quantity and figuring out if you are right; that is, not every wrong solution is ""equally wrong"", in the sense that you'll need to take into account your profit and your funds left in reserve.</p>

<p>A general approach is difficult to recommend without intimate knowledge of the data. However, it sounds like you'll need to formalize the aspects of your model. Think about the following questions as you decide on your model:</p>

<ol>
<li>What is the quantity I am trying to predict? (In this case, it sounds like you are looking for the particular markup at which you should sell, and/or the price at which to buy so that a profit can be made, which is just as much optimization as it is machine learning).</li>
<li>What examples can I use to train my model, and what are my inputs? What market factors can I train my model with to predict the quantities in (1)?</li>
</ol>

<p>As a final point, it is really quite unlikely that reinforcement learning is the approach you'll want to take in this case. Reinforcement learning is quite powerful in certain situations, but is somewhat unpredictable (depending on the particular formulation), and tends to make an awful lot of errors before it gets anything right (something that likely is not an option when there is money on the line). As I said, try to figure out which quantities you want to estimate, then figure out what market factors might affect those quantities.</p>
",2013-10-16 04:37:57.570
57592,22744.0,1,,,,How to determine if short strings of text are closely related to a larger text?,<machine-learning><classification><text-mining>,CC BY-SA 3.0,"<p>I have 1 short string of text (let's say it's a tweet, max 140 characters):<br>
<code>""A review of my beloved Roku 3 media player""</code></p>

<p>I also have a larger body of text (like a blog article, hundreds of words) which I know is related to the tweet:<br>
<code>""The Roku 3 media player is a great way to watch your favorite ....""</code></p>

<p>The tweet and blog article are both about the <strong>Roku 3 media player</strong> specifically. Same author, and they share many of the same phrases, words, collocations, etc. It's likely that the string ""Roku 3"" appears in the text, along with variations like """"Roku 3 streaming media player"", ""Roku 3 player"" etc</p>

<p>I then have 10 other tweets, some which are related to the ""Roku 3 media player"", some of which are not (but very similar):  </p>

<p>RELATED ""A good review of the <strong>Roku 3 media player</strong>""<br>
UNRELATED ""The Roku 2 media player review""<br>
RELATED ""<strong>Roku 3</strong> is amazing""<br>
RELATED ""The <strong>Roku 3</strong> is better than the Roku 2 by far""<br>
RELATED ""The <strong>Roku</strong> version <strong>3</strong> streaming media player, fully reviewed""<br>
UNRELATED ""A comparison review of the top <strong>3</strong> media player boxes. <strong>Roku</strong>, Android, Toshiba""<br>
RELATED ""<strong>Roku 3 streaming media player</strong> reviewed""  </p>

<p>Those are some examples, and I would have 10 in total. All of the tweets contain ""Roku"", some are about ""Roku 2"" and are unrelated, one uses ""Roku version 3"" and is related etc. Obviously, this is a very small data set.</p>

<p>What is the best method to classify each of the 10 tweets as relevant or not, in relation to the first tweet and blog article? What sort of features would be useful?</p>
",2013-10-16 05:39:24.313
57593,21243.0,2,,57592.0,,,,CC BY-SA 3.0,"<p>On a data set this size, it can be pretty tough to learn anything at all; as a baseline, it might be worth it to learn the <a href=""http://en.wikipedia.org/wiki/Ngram"" rel=""nofollow"">N-Grams</a> shared by the two examples, and run from there. For a training pair consisting of one short string and a longer article, it would be relatively easy to find the common N-Grams between the article and tweet, and then analyze the frequency with which those N-Grams appear in the test data. In the case you presented, the important N-Grams seem to be ""Roku 3"" and ""Roku 3 Media Player"" (a bigram and 4-gram, respectively), both of which are shared by the training data strings.</p>
",2013-10-16 06:02:26.797
57594,22705.0,2,,57588.0,,,,CC BY-SA 3.0,"<p>Hope you are also looking at adjusted r square. </p>

<p>A high r square and low mape could indicate over fitting. </p>

<p>The p values of the regressor and its sign in the two models could be compared.</p>

<p>aic can be compared. lower the better. 
hope it helps. </p>
",2013-10-16 06:05:05.923
57595,22746.0,1,,,,"If $X_{n+1}$ is a martingale subject to $Y_0,\ldots,Y_n$, then is it a martingale with respect to $Y_0^2,\ldots,Y_n^2$?",<probability><self-study><random-variable><stochastic-processes><conditional-expectation>,CC BY-SA 3.0,"<p>I don't have a very solid foundation in measure theory, and this always seems a bit confusing to me so I would appreciate any help.</p>

<p>We are given 
$
E \left( X_{n+1} | Y_0,\ldots,Y_n \right) = X_n.
$
Prove or disprove
$
E \left( X_{n+1} | Y_0^2,\ldots,Y_n^2 \right) = X_n
$</p>

<p>I am thinking that if $F=\sigma \left(Y_0,\ldots,Y_n \right)$ and
$G=\sigma \left(Y_0^2,\ldots,Y_n^2 \right)$, I need to prove that F = G? Is this correct?</p>

<p>Then I can do something like this:
$
E \left( X_{n+1} | G \right) = E \left( E \left( X_{n+1} | F \right) | G \right) = E \left( X_{n+1} | F \right) = X_{n+1}
$.</p>

<p>Also is $E \left( X_{n+1}^2 | G \right) = X_n^2$ (a martingale) given
$
E \left( X_{n+1} | Y_0,\ldots,Y_n \right) = X_n.
$</p>
",2013-10-16 06:19:34.817
57596,22600.0,1,,,,How to compute expectations from a probability density function?,<probability><distributions><self-study><density-function>,CC BY-SA 3.0,"<p>How to find a tax/subsidy in an income probability density function situation?</p>

<p>I am asked the following question:</p>

<blockquote>
  <p>Suppose all families with $Y \lt 20$ are given transfer payments equal to $(20-Y)/2$;
      and suppose the cost of these transfers is raised by taxing all families
      with $Y \gt 20$ an amount $t(Y-20).$</p>
  
  <p>Solve for $t.$</p>
</blockquote>

<p>I am trying to understand how to develop an equation to solve for $t,$ but none of my answers work.</p>

<p>The probability density function is triangular:</p>

<p><img src=""https://i.stack.imgur.com/aaXda.jpg"" alt=""income probability density function""></p>

<hr>

<p>The values I have solved for are:</p>

<ul>
<li><strong>Height</strong> :  1/30</li>
<li><strong>Mean</strong>   :  26.67</li>
<li><strong>Median</strong> :  25.36</li>
<li><strong>slope of $Y\gt 20$</strong>    : 1/600</li>
<li><strong>slope of $20\lt Y\lt 60$</strong>  : 1/1200</li>
</ul>

<p>Any help would be greatly appreciated.</p>
",2013-10-16 06:21:29.603
57659,22775.0,1,,,,Why one can fit Poisson GLM on non-integer values in Stata?,<generalized-linear-model><poisson-distribution>,CC BY-SA 3.0,"<p>I have aggregated death counts and some categorical predictors e.g. era, drug use, and I'm running a Poisson GZLM in SPSS, also a <code>glm</code> in Stata (generalized linear model). There are many, but not excessive, zeros and it has been suggested to me to add 0.5 to the observed values. When I do that the GENLIN procedure in SPSS won't run, because the Poisson dependent should have integer values. However, the <code>glm</code> with family Poisson and log link in Stata runs and just makes a note that the dependent has non-integer values. </p>

<p>Why does Stata run the Poisson <code>glm</code> if the values of the dependent should be integers only?</p>
",2013-10-16 21:06:10.343
57600,651.0,2,,50982.0,,,,CC BY-SA 3.0,"<p>If there are only two methods, A and B, I would calculate the probability that for an arbitrary training/test partition that the error (according to some suitable performance metric) for model A was lower than the error for model B.  If this probability were greater than 0.5, I'd chose model A and otherwise model B (c.f. Mann-Whitney U test?)  However, I strongly suspect that will end up choosing the model with the lower mean unless the distributions of the performance statistic are very non-symmetric.</p>

<p>For grid search on the other hand, the situation is a bit different as you are not really comparing different methods, but instead tuning the (hyper-) parameters of the same model to fit a finite sample of data (in this case indirectly via cross-validation).  I have found that this kind of tuning can be very prone to over-fitting, see my paper</p>

<p>Gavin C. Cawley, Nicola L. C. Talbot, ""On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"", Journal of Machine Learning Research, 11(Jul):2079âˆ’2107, 2010. (<a href=""http://jmlr.org/papers/v11/cawley10a.html"" rel=""nofollow"">www</a>)</p>

<p>I have a paper in review that shows that it is probably best to use a relatively coarse grid for kernel machines (e.g. SVMs) to avoid over-fitting the model selection criterion.  Another approach (which I haven't investigated, so caveat lector!) would be to choose the model with the highest error that is not statistically inferior to the best model found in the grid search (although that may be a rather pessimistic approach, especially for small datasets).</p>

<p>The real solution though is probably not to optimise the parameters using grid-search, but to average over the parameter values, either in a Bayesian approach, or just as an ensemble method.  If you don't optimise, it is more difficult to over-fit!</p>
",2013-10-16 07:36:28.433
57601,22750.0,1,,,,Calculating joint probabilities from conditional probabilities,<probability><conditional-probability>,CC BY-SA 3.0,"<p>I need to calculate the probability of $P(L,D)$, where $L$ and $D$ are not independent. I have estimated $P(L)$ and $P(D)$ with two distinct models and I also know $P(L|D)$ and $P(D|L)$. </p>

<p>As far as I can see it I have two different estimates of $P(L,D)$:</p>

<p>$P(L,D) =P(L) * P(D|L)$ </p>

<p>$P(L,D) =P(D) * P(L|D)$</p>

<p>Am I best off just averaging these two estimates for the joint $P(L,D)$?</p>
",2013-10-16 08:30:42.900
57602,11210.0,1,,,,What is the statistical efficiency of L-moments?,<robust><scales><efficiency><dispersion><l-moments>,CC BY-SA 3.0,"<p>In particular I am interested in the scale estimator. Hopefully it is much better than that of IQR.</p>
",2013-10-16 09:45:32.003
57603,22751.0,2,,57601.0,,,,CC BY-SA 3.0,"<p>The answer depends on the answer to this question: </p>

<p>""Why are (P(L) * P(D|L)) and (P(D) * P(L|D)) different?"". </p>

<p>If it depends on the fact that the model for P(L|D) is unreliable you should only keep (P(L) * P(D|L)), if you have no idea you can consider taking the average.  </p>
",2013-10-16 09:54:34.153
57604,22262.0,2,,49879.0,,,,CC BY-SA 3.0,"<p>It means re-estimation on a rolling basis. However, I do not understand why one would want to do this when there's perfectly good parametric dynamic copulas invented by Patton (2006) and extended by others with various forcing equations, as well as the more recent stochastic autoregressive copula (SCAR). Read here [1][www.wisostat.uni-koeln.de/Institut/.../Manner_Reznikova(2010ER).pdfâ€Ž].</p>
",2013-10-16 10:31:13.157
57605,22752.0,1,57607.0,,,Covariance matrix of least squares estimator $\hat{\beta}$,<regression><least-squares><covariance>,CC BY-SA 3.0,"<p>I read that the $Cov(\hat{\beta})=\sigma^2(Z'Z)^{-1}$, where $\hat{\beta}=(Z'Z)^{-1}Z'y$. However, I have yet been unable to find a proof of this fact online. Could anyone please provide a proof and/or a reference?</p>
",2013-10-16 10:39:39.203
57606,20473.0,2,,57605.0,,,,CC BY-SA 3.0,"<p>This is the expression for the <em>conditional</em> variance-covariance matrix of the estimator. For the model $$Y=Z\beta + U, \; E(U\mid Z) =0,\; E(UU'\mid Z) = \sigma^2I$$ 
we have 
$$\operatorname {Cov}(\hat\beta \mid Z)=\operatorname {Cov} \left[(Z'Z)^{-1}Z'y\mid Z\right]$$</p>

<p>$$=\operatorname {Cov} \left[(Z'Z)^{-1}Z'(Z\beta +U)\mid Z\right] = \operatorname {Cov} \left[\beta +(Z'Z)^{-1}Z'U)\mid Z\right] = \operatorname {Cov} \left[(Z'Z)^{-1}Z'U)\mid Z\right] $$</p>

<p>Since $\beta$ is treated as a constant in the frequentist approach. Now </p>

<p>$$\operatorname {Cov} \left[(Z'Z)^{-1}Z'U)\mid Z\right] = E\Big\{\left[(Z'Z)^{-1}Z'U\right]\left[(Z'Z)^{-1}Z'U)\right]'\mid Z\Big\} - E\left[(Z'Z)^{-1}Z'U)\mid Z\right]E\left[(Z'Z)^{-1}Z'U)\mid Z\right]'$$</p>

<p>Since 
$$E\left[(Z'Z)^{-1}Z'U)\mid Z\right]' = (Z'Z)^{-1}Z'E\left[U\mid Z\right]' = 0$$
we are left with </p>

<p>$$\operatorname {Cov} \left[(Z'Z)^{-1}Z'U)\mid Z\right] = E\Big\{\left[(Z'Z)^{-1}Z'U\right]\left[(Z'Z)^{-1}Z'U)\right]'\mid Z\Big\} $$</p>

<p>$$= (Z'Z)^{-1}Z'E(UU'\mid Z)Z(Z'Z)^{-1}= (Z'Z)^{-1}Z'\sigma^2IZ(Z'Z)^{-1} $$</p>

<p>$$=\sigma^2(Z'Z)^{-1} $$</p>
",2013-10-16 11:45:37.907
57607,17573.0,2,,57605.0,,,,CC BY-SA 3.0,"<p>A good reference is Greene, <i>Econometric Analysis</i>.  You should be able to pick up an older version (sixth edition or before) online for relatively cheap.  Seventh is not noticeably better than sixth.  I am changing your notation $Cov(\hat{\beta})$ to 
$V(\hat{\beta}_{\textrm{OLS}})$, but I mean the same thing by it.</p>

<p>Here is the proof:</p>

<p>If </p>

<ol>
<li>$Y=Z\beta+\epsilon$</li>
<li>$E\left\{\epsilon|Z \right\}=0$</li>
<li>$V\left(\epsilon|Z \right)=\sigma^2I$</li>
<li>The OLS estimator exists and is unique (i.e. $Z'Z$ invertible)</li>
</ol>

<p>then the OLS estimator is unbiased for $\beta$ and 
$V\left(\hat{\beta}_{\textrm{OLS}}|Z \right)=\sigma^2(Z'Z)^{-1}$.</p>

<p>Proof:
Using the definition of the OLS estimator and then substituting in using 1:
\begin{align}
\hat{\beta}_{\textrm{OLS}} &amp;= \left( Z'Z\right)^{-1}Z'Y\\
                           &amp;= \left( Z'Z\right)^{-1}Z'\left( Z\beta+\epsilon \right)\\
                           &amp;= \left( Z'Z\right)^{-1}Z'Z\beta 
                              + \left( Z'Z\right)^{-1}Z'\epsilon\\
                           &amp;= \beta + \left( Z'Z\right)^{-1}Z'\epsilon
\end{align}
Taking expectations of both sides conditional on $Z$ gives you that the OLS estimator is unbiased.  Taking variances on both sides conditional on $Z$ gives you:
\begin{align}
V\left( \hat{\beta}_{\textrm{OLS}} | Z \right)
   &amp;= V\left( \beta + \left( Z'Z\right)^{-1}Z'\epsilon | Z \right)\\
   &amp;= V\left(\left( Z'Z\right)^{-1}Z'\epsilon | Z \right) \\
   &amp;= \left( Z'Z\right)^{-1}Z'V\left(\epsilon | Z \right) Z \left( Z'Z\right)^{-1} \\
   &amp;= \left( Z'Z\right)^{-1}Z'\sigma^2I Z \left( Z'Z\right)^{-1} \\
   &amp;= \sigma^2\left( Z'Z\right)^{-1}Z'Z \left( Z'Z\right)^{-1} \\
   &amp;= \sigma^2\left( Z'Z\right)^{-1}
\end{align}
QED</p>

<p>This does not quite give you what you asked for, since the variance is conditional on $Z$ rather than unconditional.  If you want the variance to be unconditional, you have to additionally assume that $Z$ is fixed, so that the conditional variance becomes just an unconditional variance.  On the other hand, this is the right variance conditional on the dataset you used to estimate $\beta$ with OLS, and inference based on this variance gives (asymptotically, if you don't assume $\epsilon$ normal) you correctly-sized hypothesis tests and confidence intervals.</p>
",2013-10-16 11:53:52.693
57615,2149.0,2,,57613.0,,,,CC BY-SA 3.0,"<p>Forecasting weekly data using weekly history is problematic as what we do in say week 4 is probably not what we did in week 4 last year whereas what we do in month 4 is probably systematic with what we did in month 4 last year ,save special effects such as Easter or Thanksgiving. Furthermore the different number of weeks in a year can throw a monkey-wrench into the analysis. More importantly the effect of holidays on weekly sums can be quite dependent on when the holiday occurs thus effectively distorting pattern. I have seen very few examples of where weekly data is consistent/predictable and can be used reliably to obtain weekly forecasts.</p>

<p>With the development of statistically aggressive daily models taking into account the window of response around each holiday/event, day-of-the-week effects,day-od-the-month effects,month-of-the-year effects,level shifts and or local time trends... users are now developing daily models to obtain weekly predictions. Additionally they can compute probabilities of making month-end numbers or of meeting a plan/goal number.</p>

<p>THe other item dealing with missing values is easily handled by Intervention Detection schemes which would identify pulses for the missing values and effectively replace the missing value with an imputed value based upon the full model. </p>
",2013-10-16 14:19:22.403
57608,22262.0,1,,,,Can logistic regression estimates suffering from subsample abuse be salvaged?,<logistic><sampling><binary-data><bias><probit>,CC BY-SA 3.0,"<p>Suppose we have some logistic regression modelling problem; $f(X) = Y$, where $Y$ is binary and $X$ is a vector of normally distributed variables. </p>

<p>In industry it is sometimes the case that practitioners will delete rows of data where 'nothing happens' (according to an algorithmic criteria), often to reduce the dimensions of the regression (data can get quite big) or because of the perception that this 'nothing happening' is not what we want to model - we want the model only to capture when interesting events occur.</p>

<p>Needless to say this can bias out of sample probability estimates (i.e., given some new $X$, our estimate of $P(Y=1)$ is biased). A quick <code>R</code> demonstration will show this, where the true data generating process has $Y=1$ half the time and $Y=0$ the other half but our subsampling approach has this ratio to $\frac19$:</p>

<pre><code>predictions &lt;- rep(NA,1000)

y &lt;- c(rep(1,5000),rep(0,45000))
x &lt;- rnorm(50000)
#y &lt;- c(rep(1,25000),rep(0,25000))
#x &lt;- rnorm(50000)

fit = glm(y~x,family=binomial(logit))

for(i in 1:1000) {
    newX = data.frame(x=rnorm(1))
    predictions[i]=predict(fit,newX,type=""response"")
}

    mean(predictions)
plot(predictions)
</code></pre>

<p><strong>Question</strong></p>

<p>What I would like to ask is whether our fit, $\hat{f}$, can be salvaged in the following way. For every new $X$ that we observe, we only estimate $\hat{P}(Y-1)$ if and only if it would have passed through our filter had this new $X$ been part of our in sample dataset. Intuitively I want to say yes because in-sample we have the data generating process that we artificially created, $f_\text{intrusion}(X) = Y_\text{intrusion}$, and this is the same DGP that we are drawing from out of sample to get our probability estimates.</p>
",2013-10-16 12:32:09.917
57609,10060.0,2,,57589.0,,,,CC BY-SA 3.0,"<p>Use <code>[FileName]$[VariableName]</code> to refer to different variables in different datasets:</p>

<pre><code>set.seed(276)

# Simulate data:

E1 &lt;- data.frame(time=seq(1:365), growth=rnorm(365))
E2 &lt;- data.frame(time=seq(1:365), growth=rnorm(365))

# Determine the right y-axis limits:

ymax &lt;- max(c(E1$growth, E2$growth), na.rm=T)
ymin &lt;- min(c(E1$growth, E2$growth), na.rm=T)

# Plot the first graph:

plot(E1$time, E1$growth, type=""l"", col=""red"", ylim=c(ymin, ymax),
     xlab=""Time"", ylab=""Growth"")

# Plot the second line onto the first graph:

points(E2$time, E2$growth, type=""l"", col=""blue"")
</code></pre>

<p>Result:</p>

<p><img src=""https://i.stack.imgur.com/hGxwj.png"" alt=""enter image description here""></p>
",2013-10-16 12:42:50.790
57610,22753.0,2,,57472.0,,,,CC BY-SA 3.0,"<p>All of the above look like great resources, but I must link to this great example.  It presents a very simple explanation for finding the parameters for two lines of a set of points.  The tutorial is by Yair Weiss while at MIT.</p>

<p><a href=""http://www.cs.huji.ac.il/~yweiss/emTutorial.pdf"">http://www.cs.huji.ac.il/~yweiss/emTutorial.pdf</a><br>
<a href=""http://www.cs.huji.ac.il/~yweiss/tutorials.html"">http://www.cs.huji.ac.il/~yweiss/tutorials.html</a></p>
",2013-10-16 12:43:56.013
57611,20456.0,1,,,,Propensity score and Cox regression,<cox-model><propensity-scores><fuzzy>,CC BY-SA 3.0,"<p>I have a retrospective dataset of patients treated with a certain drug (treatment, $n=46$) or with placebo (control $n=96$). The stored variables are age, sex, stage of disease. I want to assess the effect of treatment on overall survival with propensity score. Here are the steps I followed:</p>

<ol>
<li>I calculated propensity score with a binary logistic regression
model using treatment as dependent variable and age,   sex, stage as
covariates.</li>
<li>I used fuzzy matching to create a 1:1 matching with 0.05 tolerance.</li>
<li>I deleted the unmatched cases and obtained a dataset of 46*2 cases
(46 treated, 46 controls).</li>
<li>I used a Cox proportional regression model using propensity score
and treatment as covariates.</li>
</ol>

<p>Is my procedure correct? I'm using SPSSv19.</p>
",2013-10-16 12:59:52.663
57612,17573.0,2,,57601.0,,,,CC BY-SA 3.0,"<p>If you have two estimators, call them $\hat{\beta}_1$ and $\hat{\beta}_2$, of the same parameter $\beta$, then you can combine them in a variety of ways.  Let's suppose you know that the two estimators are consistent and asymptotically normal --- this is generally true of estimators you get from maximum likelihood methods, method of moments methods, and some other methods as well.  Furthermore, suppose you know the (asymptotic) variances of the two estimators, $V(\hat{\beta}_1)$ and $V(\hat{\beta}_2)$ and the covariance of the two estimators $Cov(\hat{\beta}_1,\hat{\beta}_2)$.</p>

<p>You propose a combined estimator of $\beta$ given by $\frac{1}{2}\hat{\beta}_1+\frac{1}{2}\hat{\beta}_2$.  This is consistent and asymptotically normal if $\hat{\beta}_1$ and $\hat{\beta}_2$ are.  What is its variance?
\begin{align}
V(\frac{1}{2}\hat{\beta}_1+\frac{1}{2}\hat{\beta}_2) &amp;= \frac{1}{4}V(\hat{\beta}_1) +\frac{1}{4}V(\hat{\beta}_2) +\frac{1}{2}Cov(\hat{\beta}_1,\hat{\beta}_2) 
\end{align}</p>

<p>This estimator might be better (lower variance) or worse than either $\hat{\beta}_1$ or $\hat{\beta}_2$.  If, say, $\hat{\beta}_1$ has a crazy-high variance, then the variance of $\frac{1}{2}\hat{\beta}_1+\frac{1}{2}\hat{\beta}_2$ might be higher than the variance of $\hat{\beta}_2$.  We would like to avoid this.  Also, we would like to find the best (i.e. lowest variance) way of combining the two estimators while preserving consistency.  That is, we want to solve:
\begin{align}
&amp;min_{\lambda}V(\lambda\hat{\beta}_1+(1-\lambda)\hat{\beta}_2)\\
&amp;min_{\lambda}\lambda^2V(\hat{\beta}_1)+(1-\lambda)^2V(\hat{\beta}_2)
+2\lambda(1-\lambda)Cov(\hat{\beta}_1,\hat{\beta}_2)
\end{align}
The first order condition is:
\begin{align}
2\lambda V(\hat{\beta}_1)-2(1-\lambda)V(\hat{\beta}_2)
+2(1-2\lambda)Cov(\hat{\beta}_1,\hat{\beta}_2)&amp;=0 \\
\frac{V(\hat{\beta}_2)-Cov(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)} &amp;= \lambda
\end{align}
The combined estimator is then:
\begin{align}
\hat{\beta}_* = \frac{V(\hat{\beta}_2)-Cov(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)} \hat{\beta}_1 +
\frac{V(\hat{\beta}_1)-Cov(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)} \hat{\beta}_2
\end{align}
Because of the way we set up $\lambda$ in the minimization, you are assured that this new estimator is consistent.  Notice, if the covariance between the two estimators is zero, then this new estimator is just the weighted sum of the two original estimators where the original estimators are weighted inversely to their variance---the new estimator ""pays attention"" to the old estimators inversely according to their variances.</p>

<p>Finally, the variance of the new estimator is:
\begin{align}
V(\hat{\beta}_*) = \frac{V(\hat{\beta}_1)V(\hat{\beta}_2)-2Cov^2(\hat{\beta}_1,\hat{\beta}_2)}{V(\hat{\beta}_1)+V(\hat{\beta}_2)-2Cov(\hat{\beta}_1,\hat{\beta}_2)}
\end{align}</p>

<p>This process of optimally combining multiple estimators is called ""minimum distance estimation"" in econometrics.  A cite is chapter 13 in Greene, <i>Econometric Analysis, seventh ed</i>.</p>
",2013-10-16 13:09:06.063
57613,22369.0,1,57615.0,,,Building a forecast model based on past year data in R,<r><time-series><predictive-models><forecasting>,CC BY-SA 3.0,"<p>I am attempting to build a model to forecast attendance in a given week in the current year based on this year's attendance values up until the present, and data from two previous years. My data looks like this:</p>

<pre><code>   Week 11-12 Cumulative ADA    12-13 Cumulative ADA    13-14 Cumulative ADA
   1    0.9941                  0.9941                  0.9914
   2    0.9907                  0.991                   0.989
   3    0.9888                  0.9888                  0.9879
   4    0.9877                  0.987                   0.9869
   5    0.9869                  0.9865                  0.9867
   6    0.9862                  0.985                   0.9859
   7    0.9856                  0.9842                  0.9857
   8    0.9856                  0.984                   NA
   9    0.9852                  0.9839                  NA
   10   0.9848                  0.9834                  NA
</code></pre>

<p>Any guidance on how to predict the three NAs based on the past two years data and this year's values would be much appreciated.</p>

<p>Thanks!</p>
",2013-10-16 13:42:40.313
57614,19298.0,2,,47447.0,,,,CC BY-SA 3.0,"<p>Have you looked at the LaplacesDemon package?  They have some examples for autoregressive poisson.  <a href=""http://cran.cermin.lipi.go.id/web/packages/LaplacesDemon/vignettes/Examples.pdf"" rel=""nofollow"">http://cran.cermin.lipi.go.id/web/packages/LaplacesDemon/vignettes/Examples.pdf</a></p>
",2013-10-16 14:05:52.853
57652,12980.0,1,71920.0,,,Understanding tail dependence coefficients,<independence><non-independent><copula><extremal-dependence>,CC BY-SA 3.0,"<p>How can I analyze the $\lambda_U$ and $\lambda_L$ results (estimated by non-parametric method)? What does higher or lower coefficients mean? Does $\lambda_U = 0.5$ mean there's some kind of linear dependence between $X$ and $Y$?</p>

<p>This is what I mean by $\lambda_U$ and $\lambda_L$:</p>

<p><img src=""https://i.stack.imgur.com/i63q0.jpg"" alt=""enter image description here""></p>
",2013-10-16 20:27:28.370
57616,22756.0,1,,,,Determining sample size with a proportion and binomial distribution,<self-study><binomial-distribution><proportion><statistical-power><type-i-and-ii-errors>,CC BY-SA 3.0,"<p>I am trying to learn some statistics using the book, Biometry by Sokal and Rohlf (3e). This is an exercise in the 5th chapter which covers probability, the binomial distribution, and Poisson distribution. 
<img src=""https://i.stack.imgur.com/T0Tth.jpg"" alt=""enter image description here""></p>

<p>I realize there is a formula to produce an answer to this question:
$$
n = \frac 4 {( \sqrt{p} - \sqrt{q} )^2}
$$
However, this equation is not in this text. I'd like to know how to calculate sample size knowing only the probability, the desired level of confidence, and the binomial distribution. Are there any resources covering this topic that I can be pointed to? I've tried Google, but what I've seen so far requires information I don't have access to in this problem.</p>
",2013-10-16 14:27:26.077
57617,22399.0,1,,,,Visual display of multiple comparisons test,<data-visualization><multiple-comparisons>,CC BY-SA 3.0,"<p>Suppose, the data below shows the mean response time on a task for respondents among four different groups:</p>

<pre><code>A     B     C    D  
1.2   2.3   4.5  6.7
</code></pre>

<p>In order to assess which one of the means are different from one another I do a multiple comparisons test (after an omnibus ANOVA test is cleared) and the multiple comparisons test tells me that the mean for group D is significantly different from the ones for groups A and B and no other pair of differences is significantly different.</p>

<p>What is the best way to present this information visually? </p>
",2013-10-16 14:34:52.500
57618,22729.0,2,,57598.0,,,,CC BY-SA 3.0,"<p>If you do not multiply by 2, you will be evaluating the probability of having scores ranging from 18 to 25 (one-sided test). </p>

<p>Multiplying by 2, you are evaluating the probability of having scores ranging from 0 to 7 and 18 to 25 (two-sided test).
Your command results in an answer similar to this one:</p>

<pre><code>binom.test(18, 25, 0.5, alternative=""two.sided"") 
</code></pre>
",2013-10-16 14:45:41.947
57619,22757.0,2,,46070.0,,,,CC BY-SA 3.0,"<p>As I understand it, sigma2 is the constant variance the model assumes, and it is the variance for the innovations or random shocks (uncorrelated zero mean random variables) that the model uses behind the scenes. An ARIMA model is one where the current random variable $X_t$ can be written as the sum of a linear filter of the previous $X_{t-h}, h&gt;0$ (the AR part, and also the I (order of differencing) if it has unit roots in a certain sense), and a linear filter of the random shocks $Z_{t-j}, j\geq 0$ in which the coefficient of $Z_t$ is 1 (the MA part, not counting $Z_t$ itself). These random shocks are the ones that have to have a common variance.</p>
",2013-10-16 15:00:49.397
57620,22759.0,2,,1248.0,,,,CC BY-SA 3.0,"<p>I forget where it's from, but there's the one that when asked how he felt on reaching his 90th birthday, the old statistician replied, ""I'm very happy about it - the numbers show that few people die after their 90th birthday.""</p>
",2013-10-16 15:03:08.217
57621,16046.0,1,57632.0,,,Cumulants in chinese restaurant process?,<distributions><python><stochastic-processes>,CC BY-SA 3.0,"<p>I already wrote a similar question on StackOverflow but was not welcomed there! So I decided to ask it from the folks here.
I have wrote a code in Python for CRP problem. I think everybody here is familar with the subject but nevertheless:</p>

<p>Short description of it:
Suppose we want to assign people entering to a restaurants to potentially infinite number of tables. If $z_i$ represents the random variable assigned for the $i$'th person entering the restaurant the following should hold:</p>

<p>With probability $p(z_i=a|z_1,...,z_{i-1})=\frac{n_a}{i-1+\alpha}$ for $n_a&gt;0$, $i$'th person will sit in table $a$ and with probability $p(z_i=a|z_1,...,z_{i-1})=\frac{\alpha}{i-1+\alpha}$ $i$'th person will sit around a new table.</p>

<p>I am not quite sure if my code is correct cause I am surprised how small the final number of tables are. I would be happy if somebody could give me cumulants for the distribution associated with this process.</p>

<pre><code>import numpy as np
def CRP(alpha,N):
    """"""Chinese Restaurant Process with alpha as concentration parameter and N 
    the number of sample""""""
    #Array which will save for each i, the number of people people sitting
    #until table i
    summed=np.ones(1) #first person assigned to the first table
    for i in range(1,N):
        #A loop that assigns the people to tables

        #randind represent the random number from the interval [1,i-1+alpha]
        randind=(float(i)+alpha)*np.random.uniform(low=0.0, high=1.0, size=1)
        #update is the index for the table that the person should be placed which
        #if greater than the total number, will be placed in a new table
        update=np.searchsorted(summed,randind,side='left')
        if randind&gt;i:
            summed=np.append(summed,i+1)
        else:
            zerovec=np.zeros(update)
            onevec=np.ones(summed.size-update)
            summed+=np.append(zerovec,onevec)
    #This part converts summed array to tables array which indicates the number
    #of persons assigned to that table
    tables=np.zeros(summed.size)
    tables[0]=summed[0]
    for i in range(1,summed.size):
        tables[i]=summed[i]-summed[i-1]
    return tables
a=CRP(5,1000)
print a
</code></pre>
",2013-10-16 15:05:33.317
57622,20062.0,2,,57617.0,,,,CC BY-SA 4.0,"<p>Point is, that <strong>your dataset is too small</strong> (4 groups 5 values each). The means obtained from such data are not very accurate representative values for each group - and therefore <strong>you should not run ANOVA</strong> to make inference about differences among group.</p>
<hr />
<p><em><strong>One thing is to be understandable to the audience but more important is to be scientifically accurate.</strong></em></p>
<hr />
<p>I suggest to solve this issue by <strong>Kruskal-Wallis</strong> followed by multiple comparisons.</p>
<p><strong>Boxplots</strong> (with <em><strong>medians</strong></em>) is probably the most used graphical representation of multiple comparisons of groups. To display differences you either make <strong>brackets</strong> above pairs which are statistically different and add (<code>***</code>-symbols or <code>N.S.</code>) This looks good if you have <em>small number of groups</em>. Or can make <strong>notches</strong> on each boxplot (very helpful in <em>large number of groups</em>) by which anyone will found desired comparison be eye.</p>
<hr />
<p>You may created boxplots for example in <code>R</code>:</p>
<pre class=""lang-r prettyprint-override""><code>data &lt;- data.frame(value=c(rnorm(60), rnorm(20)+3), 
    group=rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), each=20))

              value group
    1  -1.206926025     A
    2  -0.311125313     A
    3   1.336579675     A
    ......
    21  1.543827796     B
    22 -1.874257866     B
    ......
    80  4.383037868     D
    etc.  

boxplot(data<span class=""math-container"">$value ~ data$</span>group, notch=TRUE,
    col = &quot;red&quot;, xlab=&quot;group&quot;, ylab=&quot;value&quot;)
</code></pre>
<p><img src=""https://i.stack.imgur.com/dVzGb.png"" alt=""enter image description here"" /></p>
<p>Boxplots shows <strong>median</strong> values instead of mean.
I strongly suggest to <strong>not display ONLY mean values</strong> for each group. Raw data are the last possibility.</p>
",2013-10-16 15:28:50.510
57623,2857.0,2,,57608.0,,,,CC BY-SA 3.0,"<p>In logistic regression, oversampling events does not affect non-intercept coefficients - they remain unbiased. Only the intercept $\beta_0$ is affected, and this is what drives the incorrect probability predictions. So if you only cared about the coefficients for explanatory variables and not the actual predicted probabilities, you don't need to do anything, but that's not the case here.</p>

<p>See <a href=""http://gking.harvard.edu/files/0s.pdf"" rel=""noreferrer"">King and Zheng (2001)</a> [equation 7] for the direct correction to $\beta_0$</p>

<p>$$
\hat{\beta_0} - ln[({1-\tau \over \tau})({\hat{y} \over 1-\hat{y}})] 
$$</p>

<p>$\hat{\beta_0}$ being the incorrectly estimated intercept<br>
$\tau$ being the proportion in the actual population<br>
$\hat{y}$ being the proportion in the sample population</p>

<p>So for your simple example, you should have something like ${\beta_0} = -2.2 - ln[({0.5 \over 0.5})({0.1 \over 0.9})] = -2.2 + 2.2 = 0 $. </p>
",2013-10-16 15:33:03.820
57624,7700.0,2,,57617.0,,,,CC BY-SA 3.0,"<p>Based on your question and follow-up comments, I'd start with a dot-plot. They're quick and easy (even in Excel).  Here's s sample with your data:</p>

<p><img src=""https://i.stack.imgur.com/vHFq3.png"" alt=""DotPlot""></p>

<p>This chart type scales well, handles large numbers of data points well and is very easy to understand-even to a non-tech audience.</p>
",2013-10-16 15:42:10.740
57625,10060.0,2,,57617.0,,,,CC BY-SA 4.0,"             
<pre class=""lang-r prettyprint-override""><code>iv &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;)
dv &lt;- c(1.2,2.3,4.5,6.7)
gp &lt;- c(1,1,1,2)

par(mai=c(1,1,0,0))
plot(dv, gp, axes=F, xlab=&quot;Average time&quot;, ylab=&quot;Grouping based on 
                   \n mean comparison&quot;,
     ylim=c(0,3), xlim=c(0,7), pch=16)
text(dv, gp-.2, iv)
axis(side=2, label=c(&quot;i&quot;, &quot;ii&quot;), at=c(1,2))
axis(side=1)
abline(h=c(1,2),col=&quot;blue&quot;,lty=3)
</code></pre>
<p>Provide a footnote: <strong>Means on the same horizontal reference line are not statistically different from each other. Alpha = 0.05, Bonferroni adjustment</strong></p>
<p><img src=""https://i.stack.imgur.com/w3Xen.png"" alt=""enter image description here"" /></p>
<p>And I really like this design because you can flexibly accomodate group means with multiple memberships. Like in this case, C is not different from D and also not different from A and B:</p>
<pre class=""lang-r prettyprint-override""><code>iv &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;, &quot;C&quot;, &quot;D&quot;)
dv &lt;- c(1.2,2.3,4.5, 4.5, 6.7)
gp &lt;- c(1,1,1,2,2)

par(mai=c(1,1,0,0))
plot(dv, gp, axes=F, xlab=&quot;Average time&quot;, ylab=&quot;Grouping based on 
            \n mean comparison&quot;,
     ylim=c(0,3), xlim=c(0,7), pch=16)
text(dv, gp-.2, iv)
axis(side=2, label=c(&quot;i&quot;, &quot;ii&quot;), at=c(1,2))
axis(side=1)
abline(h=c(1,2),col=&quot;blue&quot;,lty=3)
</code></pre>
<p><img src=""https://i.stack.imgur.com/TRUpr.png"" alt=""enter image description here"" /></p>
",2013-10-16 15:51:05.500
57626,22762.0,1,217429.0,,,How to estimate vector autoregression & impulse response function with panel data,<r><econometrics><panel-data><vector-autoregression><impulse-response>,CC BY-SA 3.0,"<p>I am working on vector auto-regression (VARs) and impulse response function (IRFs) estimation based on panel data with 33 individuals over 77 quarters.  How should this type of situation be analyzed?  What algorithm's exist for this purpose?  I would prefer to conduct these analyses in R, so if anyone is familiar with R code or a package designed for this purpose that they could suggest, that would be especially helpful.  </p>
",2013-10-16 15:53:54.087
57627,19298.0,2,,55609.0,,,,CC BY-SA 3.0,"<p>I'd suggest taking a look  at <a href=""http://rads.stackoverflow.com/amzn/click/0789749416"" rel=""nofollow"">http://www.amazon.com/Predictive-Analytics-Microsoft-Conrad-Carlberg/dp/0789749416</a> if you are restricted to Excel.  There are also example R codes in the book to help transition from thinking in Excel to thinking in R.  The spreadsheet examples from the book can be found here: <a href=""http://www.quepublishing.com/store/predictive-analytics-microsoft-excel-9780789749413"" rel=""nofollow"">http://www.quepublishing.com/store/predictive-analytics-microsoft-excel-9780789749413</a>.  Chapter 5 is about time series.</p>
",2013-10-16 15:57:56.437
57628,18198.0,1,,,,Estimators for linear regression when multicollinearity is present,<pca><lasso><ridge-regression><elastic-net><isotonic>,CC BY-SA 3.0,"<p>I have a multicollinearity problem in a linear regression model and ridge regression was suggested as a solution. So I have spent quite some time researching different ridge regressors in the literature (there's at least a dozen), however in the course of my research I have found that Principal Components Regression can be considered a special case of ridge regression and so I am including it too. Clearly Partial Least Squares regression is closely linked to Principal Component Regression so it seems it should be added too (although I haven't researched that yet). Also when deriving Ridge regression from the Bayesian viewpoint it became obvious that a Kalman filter approach could be used to implement the Bayesian approach so I'm including that too. The Bayes approach seems fairly natural in my application as I have several years worth of data some in which the collinearity effects are present some where they are not so I can build up a decent prior for the Beta parameters for the cases where little or no MC is present.</p>

<p>I have reviewed various comparison papers and they usually compare the estimators on the basis of their Mean Square Error Performance. Generally in the Ridge v Bayes/Kalman there seems to be no clear winner with the performance depending on the orientation of the betas being estimated to the principal components of the design matrix, the level of multicollinearity and the signal to noise ratio. So it would seem that the best estimator can only be defined in the sense of the best for a given problem.    </p>

<p><a href=""https://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge"">When should I use lasso vs ridge?</a></p>

<p>Now through my research I have stumbled into Lasso (and Least Angle regression) and from the answers to the previous thread (see above) I can add ""Elastic Net"" and ""non-negative garrote"" (and probably more). I just wonder whether it makes sense to add these to my comparison too? If I'm totally honest it seems that I should. The theory behind the Lasso is pretty similar to that used for the Ridge estimator with the optimization being done in the L1 rather than L2 space.    </p>

<p>Ideally I would like to compare all the different classes of shrinkage estimators using the best estimator from each class. Right now I have at least a dozen ridge regressors and no way a priori of knowing which would be best for my problem (given the inconclusive results of the comparison papers I mentioned previously). If I only have to add say the ""Lasso"" and ""Elastic-net"" and even the ""Non-negative Garrotte"" then probably I can do it but if they have as many variations as the Ridge literature has and similar problems identifying which one is likely to be best for a given problem then it seems that the whole thing may become rather unwieldy and I can't cover every approach that's ever been suggested in the literature. So my questions are: </p>

<ol>
<li>What are the different classes of estimators to deal with Multicollinearity in linear regression: eg. Ridge, PCR, PLS, Lasso...</li>
<li>Apart from Ridge regression do these other classes have one implementation that is generally regarded as best. Any paper I have seen that compares Ridge to other classes of estimators, the authors generally use the basic Ridge regression method which is shown to be one of the poorest performing ridge methods.</li>
<li>Do you have experience in comparing Ridge to these other techniques and which was found to be best?</li>
<li>Is it realistic to compare these different classes all together?</li>
</ol>

<p>Here is a list of the lasso techniques I have found from a review article by Tibshirani (2011):</p>

<p>Grouped Lasso;
Elastic Net Lasso;
Fusd Lasso;
Adaptive Lasso;
Graphical Lasso; 
Dantzig selector;
Near Isotonic Regulation;
Matrix Completion;
Compressive Sensing;
Multivariate Methods;</p>

<p>Now my problem consists of only 4 different X values and around 30 y values. The really high multicollinearity occurs between X variables 3 &amp; 4. Although there can be significant multicollinearity with variable 2 as well. (Variable 1 is a constant).  </p>

<p>Most of these techniques are suited for p>>N but this seems directly at odds with the comment from Gary here:</p>

<p><a href=""https://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge"">When should I use lasso vs ridge?</a></p>

<p>That lasso should be used when you have high multicollinearity effects and few variables?</p>

<p>In any case given my problem I think that the Lasso and Elastic Net are most suitable.
Can anyone shed any light as to whether any of the remaining techniques may be helpful for my problem? </p>
",2013-10-16 16:02:21.723
57629,21864.0,1,57635.0,,,How to define the multiplier range for variance test based outliers detection algorithm?,<standard-deviation><mean><outliers><variance-test>,CC BY-SA 4.0,"<p>I have a variance test based outliers detection algorithm. The algorithm is exposed with a visual application where the user can configure its parameters namely the multiplier. </p>

<p>The question is what is the range of values for the algorithm multiplier? </p>

<p>the algorithm basically is calculating the mean and standard deviation (sigma) of the data set and then comparing the dataset elements to the upper and lower bounds to flag an element as an outlier or not.  </p>

<pre><code>multiplier &lt;- 2.3; 
upper_bound &lt;- mean + multiplier * sigma; 
lower_bound &lt;- mean - multiplier * sigma; 
</code></pre>

<p>the evolution of the two functions upper_bound (red) and lower_bound(blue) with the multiplier values is as follows: </p>

<p><img src=""https://i.stack.imgur.com/Gzcm6.png"" alt=""enter image description here""></p>

<p>But this graph doesn't give any clue on what range to define. Do you have any idea how to define this range? </p>
",2013-10-16 16:17:09.840
57630,17628.0,2,,13058.0,,,,CC BY-SA 3.0,"<p>Other answerers assume that you deal with raster image of a graph. But nowadays the good practice is to publish graphs in vector form. In this case you can achieve much higher exactness of the recovered data and even estimate the recovery error if you work with the code of the vector graph directly, without converting it to raster image.</p>

<p>Since the papers are published online as PDF files, I assume that you have a PDF file which contains vector plot with data you wish to recover from it (get in numerical form) and estimate introduced recovery error.</p>

<p>First of all, PDF is a vector format which is basically textual (can be read by a text editor). The problem is that it can (and almost always) contain compressed data streams which require to be uncompressed in order to read them by a text editor. These compressed data streams usually contain the information we need.</p>

<p>There are several ways to uncompress data streams in order to convert PDF file to a textual document with readable PDF code. Probably the simplest way is to use free <a href=""http://qpdf.sourceforge.net/"" rel=""nofollow noreferrer"">QPDF utility</a> with <a href=""http://qpdf.sourceforge.net/files/qpdf-manual.html#ref.advanced-transformation"" rel=""nofollow noreferrer""><code>--stream-data=uncompress</code> option</a>:</p>

<pre><code>qpdf infile.pdf --stream-data=uncompress -- outfile.pdf
</code></pre>

<p>Some other ways are described <a href=""https://stackoverflow.com/q/18813227/590388"">here</a> and <a href=""https://stackoverflow.com/a/3438394/590388"">here</a>.</p>

<p>The generated outfile.pdf can be opened by a text editor. Now you need <a href=""http://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf"" rel=""nofollow noreferrer"">PDF Reference Manual 1.7</a> to understand what you see. Do not panic at this moment! You need to know only few operators described in the ""TABLE 4.9 Path construction operators"" on pages 226 - 227. The most important operators are (the first column contains coordinate specification for an operator, the second contains the operator and the third is operator name):</p>

<pre><code>x y               m   moveto 

x y               l   lineto 

x y width height  re  rectangle

                  h   closepath
</code></pre>

<p>In most cases it is sufficient to know these four operators for recovering the data. </p>

<p>Now you need to import the outfile.pdf file as text into some program where you can manipulate the data. I'll show how to do it with <a href=""http://www.wolfram.com/mathematica/"" rel=""nofollow noreferrer""><em>Mathematica</em></a>.</p>

<p>Importing the file:</p>

<pre><code>pdfCode = Import[""outfile.pdf"", ""Text""];
</code></pre>

<p>Now I assume the simplest case: the graph contains a line which consists of many two-point segments. In this case each segment of the line is encoded like this:</p>

<pre><code>268.79999 408.92975 m
272.39999 408.92975 l
</code></pre>

<p>Extracting all such segments from the PDF code:</p>

<pre><code>lines = StringCases[pdfCode, 
   StartOfLine ~~ x1 : NumberString ~~ "" "" ~~ y1 : NumberString ~~ "" m\n"" ~~ 
                  x2 : NumberString ~~ "" "" ~~ y2 : NumberString ~~ "" l\n"" 
                                        :&gt; ToExpression@{{x1, y1}, {x2, y2}}]; 
</code></pre>

<p>Visualizing them:</p>

<pre><code>Graphics[{Line[lines]}]
</code></pre>

<p>You get something like this (the paper I am working with contains four graphs):</p>

<p><img src=""https://i.stack.imgur.com/qu32M.png"" alt=""plot""></p>

<p>Each two adjacent segments share one point. So in this case you can turn the sequences of adjacent segments into paths:</p>

<pre><code>paths = Split[lines, #1[[2]] == #2[[1]] &amp;];
</code></pre>

<p>Now you can visualize all the paths separately:</p>

<pre><code>Graphics[{Line /@ paths}]
</code></pre>

<p>From this figure you can select (by double-clicking) the path you are looking for, copy graphics selection and paste as new <code>Graphics</code>. For converting it backward to list of points you take the element <code>{1, 1, 1}</code>. Now we have the points not in the coordinate system of the graph but in the coordinate system of the PDF file. We need to establish relationship between them. </p>

<p>From the above plot you select ticks by hand (holding <code>Shift</code> for multiple selection), then copy them and paste as new <code>Graphics</code>. Here is how you can extract coordinates of horizontal ticks:</p>

<p><img src=""https://i.stack.imgur.com/wmXNB.png"" alt=""screenshot""></p>

<p>Now check the differences between ticks:</p>

<pre><code>Differences[reHorTicks]
</code></pre>

<p>From these differences you can see how precise is positioning of the ticks in the PDF file. It gives an estimate of error introduced by converting original datapoints into vector graph included in the PDF file. If there are appreciable errors in ticks positioning you can reduce the error by fitting the coordinates of ticks to a linear model. This linear function now can be used to get original coordinates of points of the path (that is in the coordinate system of the plot).</p>
",2013-10-16 16:20:30.000
57631,,1,57637.0,,user30490,Backshift operator applied to a constant,<time-series><self-study><autoregressive>,CC BY-SA 3.0,"<p>This questions is two part:</p>

<p>1)  What happens when you apply the backshift operator to a constant?  For example, if I have the AR process
$$(1-\phi B)(y_t-\mu)=\epsilon_t$$
does that equal
$$y_t-\mu-\phi By_t-\phi B\mu = \epsilon_t$$
which (I believe reduces to)
$$y_t-\mu-\phi y_{t-1}-\phi \mu = \epsilon_t\longrightarrow y_t=\mu+\phi y_{t-1}+\phi \mu+\epsilon_t  $$</p>

<p>So am I correct in assuming that the backshift of a constant (in my example $\mu$) is just the constant?</p>

<p>2)  If I assume that $\epsilon_t\sim N(0,v)$, then what is the likelihood of the above AR process in 1? </p>
",2013-10-16 16:40:19.333
57632,3580.0,2,,57621.0,,,,CC BY-SA 3.0,"<p>I don't know how many tables you were expecting, but the mean and variance of the number of total tables is available in closed form. For $\alpha = 5$ and $N = 1000$
$$
E[\mbox{Num Tables}] = \sum_{i=1}^N \frac{\alpha}{\alpha + i - 1} \approx 27,
$$
and
$$
\mbox{Var}[\mbox{Num Tables}] = \sum_{i=1}^N \frac{\alpha(i-1)}{(\alpha + i - 1)^2} \approx 21.5.
$$</p>

<p>I ran your code in python and it seems consistent with these formulas. Note that the expected number of tables grows logrithmically in $N$. </p>
",2013-10-16 17:06:27.293
57633,16746.0,1,,,,Vector space model: cosine similarity vs euclidean distance,<distance>,CC BY-SA 3.0,"<p>I have corpora of classified text. From these I create vectors. Each vector corresponds to one document.  Vector components are word weights in this document computed as TFIDF values. Next I build a model in which every class is presented by a single vector. Model has as many vectors as there classes in the corpora. Component of a model vector is computed as mean of all component values taken from vectors in this class. 
For unclassified vectors I determine similarity with a model vector by computing cosine between these vectors.</p>

<p>Question: Can I use Euclidean Distance between unclassified and model vector to compute their similarity? If not - why?<br>
Thanks!</p>
",2013-10-16 17:33:04.630
57634,22507.0,2,,57237.0,,,,CC BY-SA 3.0,"<p>It seems that in order to reproduce the joint distribution $\rho(a,v)$, you should select new $a$ not only based on $v$, but based on the old $a$ also:</p>

<p>$a_{i+1} \sim \rho'(a_{i+1}|a_i, v_i)$</p>

<p>The question (to which I don't know the answer yet) is how to find $\rho'$ which produces $\rho$.</p>

<p>UPD:
You are to solve the following integral equation:</p>

<p>$$\rho(a, v) = \int da' \rho'\left(a|a', v-{a+a'\over 2}\Delta t\right) \rho(a', v-{a+a'\over 2}\Delta t)$$</p>

<p>Approximating the function $\rho$ with a histogram, you turn this to a system of linear equations:</p>

<p>$$\cases{
\rho(a, v) = \sum_{a'} \rho'\left(a|a', v-{a+a'\over 2}\Delta t\right) \rho(a', v-{a+a'\over 2}\Delta t) \\
\sum_a \rho'\left(a|a', v'\right) = 1}$$</p>

<p>This system is underdetermined. You may apply a smoothness penalty to obtain a solution.</p>
",2013-10-16 17:35:22.410
58282,22637.0,2,,58279.0,,,,CC BY-SA 3.0,"<p>You need independent RV's since then the covariance is identically equal to zero. An example might be the following distributions </p>

<p>$$ \begin{align} f_Y (y)= 2y\ \text{for}\ 0&lt;y&lt;1 \\ f_X(x)= 2x\ \text{for}\ 0&lt;x&lt;1 \\ f_(x,y)=4xy \end{align} $$
Try it!</p>
",2013-10-26 22:12:25.427
57635,450.0,2,,57629.0,,,,CC BY-SA 3.0,"<p>To find the outliers, you cannot use the distance of an observation to a model through a rule such as:</p>

<p>$$\frac{|\hat{\mu}-x_i|}{\times \hat{\sigma}},\;i=1,\ldots,n$$</p>

<p>if your estimates of $(\hat{\mu},\hat{\sigma})$ are the classical ones (the usual mean/standard deviation) because the fitting procedure you use to obtain them is itself liable to being pulled towards the outliers (this is called the masking effect). </p>

<p>One simple way to reliably detect outliers however is to use the general idea you suggested (distance from fit) but replacing the classical estimators by robust ones much less susceptible to be swayed by outliers. Below I present a general illustration of the idea. If you give more information about your specific problem I can append my answer to address the particulars of your situation.</p>

<p>An illustration: consider the following 20 observations 
drawn from a $\mathcal{N}(0,1)$ (rounded to the second 
digit):</p>

<pre><code>x&lt;-c(-2.21,-1.84,-.95,-.91,-.36,-.19,-.11,-.1,.18,
.3,.31,.43,.51,.64,.67,.72,1.22,1.35,8.1,17.6)
</code></pre>

<p>(the last two really ought to be .81 and 1.76 but have
 been accidentally misstyped).</p>

<p>Using a outlier detection rule based on comparing the statistic </p>

<p>$$\frac{|x_i-\text{ave}(x_i)|}{\text{sd}(x_i)}$$ </p>

<p>to the quantiles of a normal distribution would never 
lead you to suspect that 8.1 is an outlier, leading you 
to estimate the $\text{sd}$ of the 'trimmed' series to be
 2 (for comparison the raw, e.g. untrimmed, estimate of
 $\text{sd}$ is  4.35).</p>

<p>Had you used a robust statistic instead:</p>

<p>$$\frac{|x_i-\text{med}(x_i)|}{\text{mad}(x_i)}$$ </p>

<p>and comparing the resulting robust $z$-scores to the 
choosen quantiles of a candidate distribution (typically the 
standard normal if you can assume the $x_i$'s to be 
symetrically distributed) you would have correctly 
 the last two observations as outliers (and correctly 
estimated the $\text{sd}$ of the trimmed series to be 
0.96). </p>
",2013-10-16 17:48:18.097
57636,18848.0,1,57694.0,,,How to identify variable (from many variables) which is able to discriminate between groups?,<r><correlation>,CC BY-SA 3.0,"<p>I currently have a data frame with 98 observations and 107 variables.  All of the variables are numeric, but one variable is binary (yes or no).  My goal is to determine which correlation and/or variable give the greatest segregation between the yes and no samples.  I have been using the pairs () function to do this, but I can only do a few variables at a time.  Is there a way to determine which correlation gives the greatest discernment between yes and no?</p>

<p>To Clarify - My table is 98 observations and 107 variables, but doing a correlation matrix with the pairs function is not able to fit all of the variables.</p>

<p>I have used this function:</p>

<pre><code>pairs(x[70:80], ch=21, bg=c(""red"",""green"")[unclass(x$outcome)])
</code></pre>
",2013-10-16 17:50:57.100
57637,20473.0,2,,57631.0,,,,CC BY-SA 3.0,"<p>The Backshift operator operates normally on a constant as on every other symbol. So it shifts the constant one period back -where we find that the constant has the same value as in the current period, since this is what the essence of a constant is.  </p>

<p>For the likelihood of an AR(1) process, in <a href=""https://stats.stackexchange.com/questions/72669/auto-regressive-process-maximum-likelihood-estimator/72719#72719"">this answer</a> there is the likelihood for the case without the constant -but from there it is just a small step to here.  </p>

<p><strong>ADDENDUM</strong><br>
The chain rule will be the same, but the conditional density will be</p>

<p>$$Y_i | Y_{i-1},\dots,Y_0 \sim \mathcal{N}\left((1+\phi) \mu+\phi Y_{i-1},v\right) $$</p>

<p>You need to specify what the distribution of $Y_0$ will be (will it contain the unknown parameters  $\phi$, $v$? If not, it doesn't really matter.</p>
",2013-10-16 17:54:32.043
57638,5237.0,4,,,,,,CC BY-SA 3.0,The response of an endogenous system to an exogenous shock. This is an important topic in time-series econometrics.,2013-10-16 18:34:20.250
57639,5237.0,5,,,,,,CC BY-SA 3.0,,2013-10-16 18:34:20.250
57640,22767.0,1,,,,Summation of a product,<standard-deviation><approximation><sum>,CC BY-SA 3.0,"<p>I need to calculate the following expression:</p>

<p>$$\sum_{k=1}^N a_k b_k$$</p>

<p>${a_k}$ and $b_k$ are real positive numbers. N and k are integers.</p>

<p>I know the average values of $a_k$ , defined as $\overline {a} = {\sum_{k=1}^N a_k \over N } $ and $b_k$ , defined as $\overline {b} = {\sum_{k=1}^N b_k \over N } $. I also know the standard deviation of $a_k$ and $b_k$, $\sigma_a$ and $\sigma_b$.</p>

<p>If only knowing these quantities, I have to make some approximation, I would like to know how much error I am producing with that approximation. $N$ is relatively big. </p>

<p>Any help is appreciated.</p>
",2013-10-16 18:47:58.330
57641,22631.0,1,,,,Find weight of features for feature selection,<machine-learning><feature-selection><distance>,CC BY-SA 3.0,"<p>I have a data set of videos from which I need to recognize the emotion of the speaker. For that reason I have some markers on the face of the speaker. I detect their movement as the speaker speaks and for every frame find the change in the marker coordinates.</p>

<p>There are a total of 65 markers (blue dots) on the face of a speaker so at the end of one frame I have 130 (x and y coordinates) stored. I have chosen the mean and standard deviation of those points over the video sequence as my features (based on a paper published in 2009)</p>

<p>My question is from these 260 features per video (mean and standard deviation of each x and y coordinate) how do I reduce this to something more understandable or which produces a meaningful probability output(in my case an emotion). In the paper it used the plus l and take away r algorithm based on the Bhattacharyya Distance criterion but I just couldn't understand how to actually say one feature is better than another based on that criterion.</p>
",2013-10-16 18:51:55.967
57642,22765.0,2,,41914.0,,,,CC BY-SA 3.0,"<p>As I know from Lewandowsky Algorithm, it works like Holt-Winters algorithm.
You will define three parameters:  </p>

<ol>
<li>Î± is the data smoothing factor and it's 0 &lt; Î± &lt; 1  </li>
<li>Î² is the trend smoothing factor, 0 &lt; Î² &lt; 1,   </li>
<li>Î³ is the seasonal change smoothing factor, 0 &lt; Î³ &lt; 1.  </li>
</ol>

<p>If you select the big number (near to 1) for Î±, it means you rely more on recent past data rather than old past data.<br>
If you choose the big number (near to 1) for Î², it means you rely more on past data's trend and you believe the trend will go on in future too. (you increase the weight of trend smoothing)<br>
If you choose the big number (near to 1) for Î³, it means you rely more on past data's seasonality and you believe the seasonality factors will remain in future.<br>
My suggestion is to start with some numbers for Î±,Î² and Î³ then after each period try to calculate your error, and find the numbers which reduce your errors.<br>
I used this model in Health-care and it gives me the accurate numbers.  </p>
",2013-10-16 19:01:11.903
57643,18767.0,1,57675.0,,,Latent Dirichlet Allocation - understanding the posterior,<bayesian><latent-variable><posterior><dirichlet-distribution>,CC BY-SA 3.0,"<p>I have a problem understanding the posterior for computing LDA, stated in page 7 of <a href=""http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf"" rel=""nofollow"">Blei (2007)</a>. From my point of view, it's not exactly consistent with Bayes' theorem, as described <a href=""http://en.wikipedia.org/wiki/Bayes%27_theorem#Statement_and_interpretation"" rel=""nofollow"">here</a>. Could anyone give me a simple explanation of how this formula was derived? I don't really understand how the $p(\beta, \theta, z, w)$ corresponds to $P(B|A)P(A)$ in Bayes theorem. I will be extremely grateful for any help.</p>
",2013-10-16 19:06:39.283
58283,21762.0,2,,58279.0,,,,CC BY-SA 3.0,"<p>A simple example of an uncorrelated but dependent pair:</p>

<p>$X_1=(0,0.1,\dots,1)\cdot \pi$ </p>

<p>and</p>

<p>$X_2=\sin(X_1)$</p>

<p>Edit: Since it is not particularly easy to work with trigonometric functions, you might as well work with a triangle:</p>

<p>$X_1=(-2,-1,0,1,2)$ with mean 0 and</p>

<p>$X_2=2-|X_1|$</p>
",2013-10-26 22:12:46.530
57644,12140.0,1,,,,Explaining why process obeys Central Limit Theorem,<central-limit-theorem>,CC BY-SA 3.0,"<p>I'm trying to explain why some complex process obeys Central Limit Theorem.</p>

<p>The process is a chip compiler that runs complex place &amp; route algorithms. The input is an integer seed. It initializes the algorithms in a random way. The output is a real number, which determines quality of results; the higher the number - the better. Exact implementation of place &amp; route algorithms is not known. But their goal is to reach quality of results be positive.</p>

<p>I run 100 compiles with different seeds. When I plot a histogram of the results, it looks like a normal distribution. I tried different designs, tool versions, etc., and always get nicely shaped normal distribution, but with different mean and variance.</p>

<p>I strongly suspect that Central Limit Theorem plays a role here. But why? </p>

<p>Why would a complex place&amp;route algorithm obey CLT, if it has nothing to do with any random distribution. Or maybe the interpretation of the results has nothing to do with the CLT.</p>

<p>Below is a process block diagram and example of the results.</p>

<p><img src=""https://i.stack.imgur.com/qHBWd.jpg"" alt=""enter image description here""></p>
",2013-10-16 19:13:11.033
57645,22769.0,1,,,,Binary Logistic Regression Multicollinearity Tests,<regression><logistic><multicollinearity><variance-inflation-factor>,CC BY-SA 3.0,"<p>I like Peter Flom's answer to an earlier question about multicollinearity in logistic regression, but David Garson's Logistic Binomial Regression states that there is no valid test for multicollinearity for binary-dependent logistic regression, even if the independent variables are ratio scale. Can anyone supply one or more references? My own experience is that OLS correlation matrices and VIF worked for me, as my logistic coefficients went haywire before removing entangled independent variables based on the OLS tests for multicollinearity. But I have to publish my results and methods, and would like a reputable way to cite the practice, if one or more exist.</p>
",2013-10-16 19:53:44.243
57646,22729.0,2,,40030.0,,,,CC BY-SA 3.0,"<p>Cross-validation article in Encyclopedia of Database Systems says:</p>

<blockquote>
  <p>Stratification is the process of rearranging the data as to ensure
  each fold is a good representative of the whole. For example in a
  binary classification problem where each class comprises 50% of the
  data, it is best to arrange the data such that in every fold, each
  class comprises around half the instances.</p>
</blockquote>

<p>About the importance of the stratification, <a href=""http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf"">Kohavi</a> (A study of cross-validation and bootstrap for accuracy estimation and model selection) concludes that:</p>

<blockquote>
  <p>stratification is generally a better scheme, both in terms of bias and  variance, when compared to regular cross-validation.</p>
</blockquote>
",2013-10-16 20:09:48.570
57647,22555.0,2,,57237.0,,,,CC BY-SA 3.0,"<p>Doesn't the gps data contain position $p$?  I would have thought that, not only is $v_{i+1}$ dependent upon $v_{i}$ and $a_{i}$ but $a_{i+1}$ would also be dependent upon $p_{i}$.  Consider:  in any road network there are bottlenecks, speed limits, signals, intersections, steep gradients, etc. that are geolocated.  So something like an ensemble (distribution) defined by:</p>

<p>$F_{a} = Pr ( A_{i+1} \le a_{i+1}\ |\ a_{i},v_{i},p_{i} )$<br>
$v_{i+1} = v_{i} + a_{i}dt$  </p>

<p>For such an ensemble, the difficulty will lay in the nature of the data.  It is likely that the true population will be asymmetric, non-linear (piece-wise) and may not have defined moments.  These characteristics may not be evident within the sample you have at hand.</p>

<p>As @whuber has stated, the problem, ie exactly what you are seeking to produce, does not yet seem fully and clearly defined.  It is not clear as to whether you are interested in the ensemble or more so the individuals.</p>
",2013-10-16 20:14:15.467
57648,4871.0,1,,,,Conditional Logit for recommender systems?,<econometrics><logit><recommender-system>,CC BY-SA 3.0,"<p>Are conditional multinomial logits used for recommendation engines? Although they are commonly used in econometrics, I've never heard it used or discussed in the context of recommender systems.</p>

<p>Economists use multinomial conditional logits to model which of several options a person would choose and how much they value each characteristic of the items being chosen. This is often referred to as the hedonic model.</p>

<p>The classic multinomial logit deals with discrete items (predict whether a commuter would ""walk,"" ""take the bus,"" or ""take the subway."" </p>

<p>The conditional multinomial logit uses data on ANY sets of observed choices and does not require that each person choose among the same set of things. It also puts values on various characteristics/variables. For example, you may see people decide which of several houses to buy. Each house is different -- square footage, number of rooms, price, etc. Based on observed choices, the model estimates the importance of various characteristics and you can derive a predicted ""utility"" score that each person has for each house. The model then predicts that the house with the highest score is chosen. </p>

<p>Here is a description:
<a href=""http://data.princeton.edu/wws509/notes/c6s3.html"">http://data.princeton.edu/wws509/notes/c6s3.html</a></p>
",2013-10-16 20:16:59.357
57649,22772.0,2,,18335.0,,,,CC BY-SA 3.0,"<p>The derivation of AIC as an estimator of Kullback-Leibler information loss makes no assumptions of models being nested.   </p>
",2013-10-16 20:18:28.967
57650,13051.0,1,57655.0,,,Use of linear regression or logistic regression when testing conditioned group differences,<regression><correlation><logistic><generalized-linear-model><t-test>,CC BY-SA 3.0,"<h3>Background</h3>

<p>I have a colleague interested in a particular disease and specifically if the continuous variable $X$ is different between controls and patients. Preliminary results suggest that patients have higher values of $X$ than controls.</p>

<p>The straightforward approach would be a unpaired <strong>t-test</strong> to test if there is a difference in mean values between patients and controls.</p>

<p>However, the literature suggests that <strong>age</strong> and <strong>sex</strong> are correlated with $X$ so my colleague wanted to control for this when testing. Hence, she decided to use regression to solve the problem.</p>

<h3>Problem</h3>

<p>My colleague's supervisor reasons that since they do not know if it is the disease that causes raised $X$ or if raised $X$ causes the disease they could use either linear regression or logistic regression to solve the problem. The supervisor also argued that logistic regression was the preferable approach.</p>

<p>If $d$ is a categorical variable with levels {control, disease} then the first model could be written</p>

<p>$$X = \beta_0 + \beta_1 d + \beta_2 \text{age} + \beta_2\text{sex} + \epsilon$$</p>

<p>Where the interpretation was that there is a correlation between disease and $X$ if $\beta_1$ was found significant.</p>

<p>and the second model</p>

<p>$$\log(\text{Odds}(d=\text{disease})) = \beta_0 + \beta_1 X  + \beta_2\text{age} + \beta_2\text{sex} + \epsilon$$</p>

<p>where the interpretation was that there is a correlation between disease and $X$ if $\beta_1$ was found significant.</p>

<p>What is the opinion among the experts on cross validated? Are there other methods?</p>

<h3>comments</h3>

<p>The patient and control groups were not perfectly matched and my colleague wants to make sure that she has controlled for both sex and age to avoid upsetting the reviewers.
I do not know if there were any significant differences between the sex and age distributions between patients and controls.</p>

<p>There were also indications that the variance in $X$ was different in the two groups and questions were asked if this would influence the regression models.</p>

<p>My personal opinion is that because my colleague is interested in the conditional expected value $E(X|d,\text{age},\text{sex})$ she should use the ordinary regression model or just a t-test (if there are no age or sex differences between groups).</p>
",2013-10-16 20:20:48.940
57651,15183.0,2,,57626.0,,,,CC BY-SA 3.0,"<p>I would suggest using the <code>{vars}</code> library in R. It has a function for estimating a VAR-model and for estimating an impulse response function from this model and for investigating Granger causality etc.</p>

<p>I suggest you look into the following functions:</p>

<pre><code>&gt; VARselect()
&gt; VAR()
&gt; irf()
&gt; causality()
</code></pre>
",2013-10-16 20:26:25.067
57653,22773.0,1,57660.0,,,Denominator is Zero for Matthews correlation coefficient and F-measure,<classification><unbalanced-classes>,CC BY-SA 3.0,"<p>Recently, I built a classification model based on the imbalanced data set(positive sample is minority and negative sample is majority), and the model gave the following result for the test set:</p>
<blockquote>
<p>True Positives = 0</p>
<p>True Negatives = 139</p>
<p>False Positives = 0</p>
<p>False Negatives = 10.</p>
</blockquote>
<p>My question is: for the result, can Matthews correlation coefficient (<a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow noreferrer"">MCC</a> ) and <a href=""http://en.wikipedia.org/wiki/F-measure"" rel=""nofollow noreferrer"">F-measure</a> be used for estimating the classifier?</p>
<p>Since the denominators for MCC and F-measure are zero, it seems meaningless. If so, MCC and F-measure is not always works for estimating the classifier, and sensitivity and specificity as well as g-mean should be better. Is that right?</p>
<p>Any help is appreciated.</p>
",2013-10-16 20:30:15.897
57654,503.0,2,,57645.0,,,,CC BY-SA 3.0,"<p>I'm glad you like my answer :-)</p>

<p>It's not that there is no valid method of detecting collinearity in logistic regression: Since collinearity is a relationship among the independent variables, the dependent variable doesn't matter. </p>

<p>What is problematic is figuring out how much collinearity is too much for logistic regression. David Belslely did extensive work with condition indexes. He found that indexes over 30 with substantial variance accounted for in more than one variable was indicative of collinearity that would cause severe problems in OLS regression. However, ""severe"" is always a judgment call. Perhaps the easiest way to see the problems of collinearity is to show that small changes in the data make big changes in the results. </p>

<p>[this paper http://www.medicine.mcgill.ca/epidemiology/joseph/courses/epib-621/logconfound.pdf] offers examples of collinearity in logistic regression. It even shows that <code>R</code> detects exact collinearity, and, in fact, some cases of approximate collinearity will cause the same warning:</p>

<pre><code>Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred
</code></pre>

<p>Nevertheless, we can ignore this warning and run</p>

<pre><code>set.seed(1234)
x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
x3 &lt;- x1 + x2 + rnorm(100, 0, 1)

y &lt;- x1 + 2*x2 + 3*x3 + rnorm(100)
ylog &lt;- cut(y, 2, c(1,0))

m1&lt;- glm(ylog~x1+x2+x3, family = binomial)
coef(m1)
</code></pre>

<p>which yields -2.55, 1.97, 5.60 and 12.54</p>

<p>We can then slightly perturb x1 and x2, add them for a new x3 and run again:</p>

<pre><code>x1a &lt;- x1+rnorm(100,0,.01)
x2a &lt;- x2+rnorm(100,0, .01)
x3a &lt;- x1a + x2a + rnorm(100, 0, 1)

ya &lt;- x1a + 2*x2a + 3*x3a + rnorm(100)
yloga &lt;- cut(ya, 2, c(1,0))


m2&lt;- glm(ylog~x1a+x2a+x3a, family = binomial)
coef(m2)
</code></pre>

<p>this yields wildly different coefficients: 0.003, 3.012, 3.51 and -0.41</p>

<p>and yet, this set of independent variables does not have a high condition index:</p>

<pre><code>library(perturb)
colldiag(m1)
</code></pre>

<p>says the maximum condition index is 3.54.</p>

<p>I am unaware if anyone has done any Monte Carlo studies of this; if not, it seems a good area for research</p>
",2013-10-16 20:42:49.420
57655,5237.0,2,,57650.0,,,,CC BY-SA 3.0,"<p>I say you are right.  You should use OLS regression here, not logistic regression.  </p>

<p>The question of causality is a red herring.  Causality is not required for either linear regression or logistic regression, and it is fine to model a cause as a (e.g., linear) function of an effect.  In fact, there are predictive models that do so.  As an example, researchers studying the collapse of the <a href=""http://en.wikipedia.org/wiki/Maya_civilization#The_Maya_collapse"" rel=""nofollow"">Mayan civilization</a> have hypothesized that drought may have initiated its decline.  Predictive models have been built that allow researchers to make an educated guess about rainfall levels (i.e., causes) from traces that remain (i.e., effects; e.g., analyses of core samples from lake beds), to clarify this possibility.  </p>

<p>Which variable should be made the <em>response</em> variable, and which the <em>explanatory</em> variable should be decided based on the question you want to answer.  It is clear from your setup that you are wondering about possible differences in the level of $X$ given the disease state.  Thus, $X$ should be the response variable, and disease state should be the explanatory variable.  </p>
",2013-10-16 20:50:03.253
57656,16039.0,1,,,,Fitting a gam model with simple Gaussian/Student-t heteroskedasticity,<r><regression><generalized-additive-model>,CC BY-SA 3.0,"<p>I am fitting a gam model in R (using the <code>gam</code> function in <code>mgcv</code>) to account for some non-linear effects in my data. A stripped down example of what I am doing in R is:</p>

<pre><code>mod=gam(y~s(x)+s(z),data=df)
</code></pre>

<p>However, I want to add a slightly more complicated variance model to my regression of the form </p>

<p>$$\epsilon \sim N(0,\sigma^2),\ \sigma = f(\hat{\mu})$$</p>

<p>where $\hat{\mu}$ is the fitted value of the model. (Actually, it would be nice if $\epsilon \sim t_\nu$ for some $\nu$ but sticking to this for now. I have managed to do this in the <code>gls</code> function from <code>nlme</code> using the <code>varFunc(form=fitted(.))</code> type approach, but can't figure out if there is an option to do the same kind of thing using <code>gam</code>.</p>

<p>I recognise this is not really the intention of a GLM/GAM model, but I don't want to reinvent the wheel if I am just missing something obvious</p>

<p>Edit: In response to the question in the comment below, I am hoping to fit a linear or quadratic function for $f$. I do not know the exact form of $f$ but plan to iteratively estimate it from the residuals if this can't be done automatically.</p>

<p>Edit2: Typo in R code - first spline is not meant to be a function of y!</p>
",2013-10-16 20:50:26.200
57657,15583.0,1,57666.0,,,How to use liblbfgs for fitting?,<optimization>,CC BY-SA 3.0,"<p>I am trying to use <a href=""http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm"" rel=""nofollow"">bfgs</a> algorithm in order to fit a set of $\{(x,y),f(x,y)\}$ to a function in the form of let's say $a\cdot cos(x)+b \cdot y=f(x,y)$.</p>

<p>I try to understand how to use bfgs algorithm with <a href=""http://www.chokkan.org/software/liblbfgs/group__liblbfgs__api.html"" rel=""nofollow"">liblbfgs</a>, but I don't understand the <a href=""http://www.chokkan.org/software/liblbfgs/"" rel=""nofollow"">example</a>  and it is not clear what function the author tried to fit.</p>
",2013-10-16 20:55:33.907
57658,449.0,2,,57650.0,,,,CC BY-SA 3.0,"<p>I agree with your supervisor, logistic regression is the better choice. The continuous predictor allows you to make a more nuanced probabilistic statement about the disease relationship rather than the more blunt categorization statement of the relationship resulting from using disease as the predictor.</p>

<p>You did word your initial question about whether X was different between controls and patients but it isn't clear that future use of the information might better be served by being able to make statements about the probability of disease given the value of X. I'm trusting that your supervisor has better knowledge of the more general use of the findings and therefore would offer more appropriate advice than any of us can here.</p>

<p>It gets a bit tricky looking at your covariates of age and sex when they're primarily supposed to be related to X. My guess is they're related to both X and disease, in which case you really need to treat them not as covariates, that are generally ignored, but as fully analyzed predictors. You'll need to know the correlations among your predictors and how they influence your model.</p>
",2013-10-16 21:04:07.037
57725,2873.0,2,,57718.0,,,,CC BY-SA 3.0,"<p>Do you have any other information available?  previous studies of similar cases? number and types of foods that were not measured? guesses about the process from biologists?</p>

<p>If you can find other existing information then possibly your best bet (other than the simple descriptives) is a Bayesian analysis.  But be very careful in how you create your priors and the relationships, without a lot of data it can be easy to over interpret results that are more a reflection of the prior than the data.</p>
",2013-10-17 18:44:32.173
57660,5203.0,2,,57653.0,,,,CC BY-SA 3.0,"<p>This is only really a problem if you compute the precision and recall first, then plug them in. </p>

<p>One can also compute the $F_1$ score as 
$$F_1 = \frac{2 \cdot \textrm{True Positive}}{2 \cdot \textrm{True Positive} + \textrm{False Positive} + \textrm{False Negative}}$$</p>

<p>Plugging in your numbers, you'll arrive at an $F_1$score of zero, which seems appropriate since your classifier is just guessing the majority class. </p>

<p>There is an information-theoretic measure called <strong>proficiency</strong> that might be of interest if you are working on fairly unbalanced data sets. The idea is that you want it to remain sensitive to both classes as either the number of true positives or negatives approaches zero. It's essentially $$
\frac{I(\textrm{predicted labels}; \textrm{actual labels})}{H(\textrm{actual labels)}}$$</p>

<p>See pages 5--7 of <a href=""http://www.interfacesymposia.org/I04/I2004Proceedings/WhiteJim/WhiteJim.paper.pdf"" rel=""nofollow"">White et al. (2004)</a> for more details about its calculation and interpretation</p>
",2013-10-16 21:31:57.827
57661,22777.0,1,,,,Forecasting an ADL model,<forecasting><stata><ardl>,CC BY-SA 3.0,"<p>I am fairly new to Stata, currently taking an undergrad time series econometric class. The economic significance of the regression I am running and attempting to forecast is all but zero; this is simply my learning how to use Stata somewhat efficiently. I have run an Autoregressive Distributed Lag (ADL) regression of Industrial Production (<code>ip</code>) using two lags of <code>ip</code> and one lag of the Federal Funds Rate (<code>ffr</code>). </p>

<p>My goal: forecast this model 30 periods into the future. I was hoping I could run an AR(2) model of the <code>ffr</code> and forecast it 30 periods (this is straightforward and I have done so) and then use these values to forecast my ADL model 30 periods. </p>

<p>Does this make sense and could someone inform me on how I could do this or suggest another method? </p>
",2013-10-16 21:40:34.343
57662,12544.0,2,,57659.0,,,,CC BY-SA 3.0,"<p>You can fit a Poisson model to any kind of data. But it doesn't really make sense to fit it to data that have non-integers, because it's supposed to be counts.</p>

<p>SPSS is worrying about that. Stata isn't.  </p>

<p>Have you tested to see if you do have too many zeroes?  (I <em>always</em> do.)  If you do, you should run negative binomial regression instead.  In Stata, run:</p>

<pre><code>estat gof
</code></pre>

<p>after a Poisson regression to get a test of goodness of fit.</p>

<p>(Just as an aside, it's the opposite way around with non-integer frequency weights, Stata won't allow them, SPSS will.)</p>
",2013-10-16 21:42:30.623
57663,21586.0,2,,57500.0,,,,CC BY-SA 3.0,"<p>I fear that there exist some notational differences across different sub-disciplines of statistics. Let me stick to a pragmatic, non-technical notation quite commonly used in Econometrics. Further, in my answer let me add point 5. to the list above, denoting semi-parametric regression models.</p>

<p>As an illustrative example consider the case of an <strong>additive regression model</strong> with response $Y$, regression function $g(X)$ and error process $U$,
\begin{equation} 
Y=g(X)+U 
\end{equation}
Usually we distinguish between 
1. linear and
2. non-linear 
<strong>regression functions</strong>, where ""linearity"" refers to linearity-in-parameters. Common examples used in Econometrics are</p>

<ol>
<li><p>$g(X)=\beta_0+\beta_1 X$</p></li>
<li><p>$g(X)=\beta_0X^{\beta_1}$</p></li>
</ol>

<p>Now both cases 1. and 2., respectively, can be present in 3. parametric and 5. semi-parametric <strong>regression models</strong>. A prominent example of 3. is $U \sim N(\mu,\sigma^2)$, while case 5. is present if we do not wish to impose a parametric assumption about the distribution of $U$.</p>

<p>Finally, the regression function $g(X)$ may not contain parameters. If in addition we do not wish to impose a parametric assumption about the distribution of $U$ we have a 4. non-parametric regression model.</p>

<p><em>Remarks</em>. As noted above there are different perceptions on how to define a non- or a semi-parametric model. Further, in case of non-additive regressions notational distinction becomes even more complicated. A now classical text trying to clarify the discussion is ""Econometric Foundations"" by Mittelhammer, Judge and Miller (2000, Cambridge Univ. Press).</p>
",2013-10-16 21:59:00.847
57664,633.0,2,,41244.0,,,,CC BY-SA 3.0,"<p>Because the $N$ (independent) coin flips occur with probability proportional to $p^k(1-p)^{N-k}$, the likelihood induced on the coin's bias is $\textrm{Beta}(k + 1, N-k + 1)$.</p>

<p>You could have picked any parametrization of the bias.  You chose to represent it as a probability $0 \le p \le 1$, but it could have been an ""odds"" $0\le o$, or a log-odds $\ell$.  Since this choice is arbitrary, your prior should be independent of this choice. Jeffreys found the only prior that satisfies this ""indifference"" to the choice of parametrization: the Jeffreys prior, $\textrm{Beta}(\frac12, \frac12)$. </p>

<p>Pointwise product of densities gives the posterior $\textrm{Beta}(k+\frac12, N-k+\frac12)$.</p>
",2013-10-16 22:01:59.733
57665,19325.0,1,85353.0,,,Why discriminative models are preferred to generative models for sequence labeling tasks?,<machine-learning><predictive-models><hidden-markov-model><natural-language>,CC BY-SA 3.0,"<p>I understand that discriminative models, such as CRF(Conditional Random Fields), model conditional probabilities $P(y|x)$, while generative models, such as HMM(Hidden Markov Model), model joint probabilities $P(y,x)$. </p>

<p>Take CRF and HMM for example. I know that CRF can have a larger range of possible features. Apart from that, what else makes CRF (discriminative models) preferable to HMM(generative models) in sequence labeling tasks such as Part-of-Speech tagging and NER(Name Entity Recognition)? </p>

<p><strong>Edit:</strong><br>
I found out that HMMs will have to model $P(x)$, while CRFs don't. Why would it make a big difference in sequence labeling tasks?</p>
",2013-10-16 22:29:27.990
57666,22143.0,2,,57657.0,,,,CC BY-SA 3.0,"<p>The example is doing a 100-dimensional (see #define N 100 in the code) optimization. The author is only printing the first two dimensions of x = (x[0],x[1],...,x[N]) as shown below for iteration 1.</p>

<pre><code>Iteration 1:
fx = 254.065298, x[0] = -1.069065, x[1] = 1.053443
xnorm = 10.612828, gnorm = 325.365479, step = 0.000607
</code></pre>

<p>Now f(x) is defined in the function <em>evaluate</em>.</p>

<pre><code>for (i = 0;i &lt; n;i += 2) {
    lbfgsfloatval_t t1 = 1.0 - x[i];
    lbfgsfloatval_t t2 = 10.0 * (x[i+1] - x[i] * x[i]);
    g[i+1] = 20.0 * t2;
    g[i] = -2.0 * (x[i] * g[i+1] + t1);
    fx += t1 * t1 + t2 * t2;
}
</code></pre>

<p>fx is for the function value at $x$ and $g(x)$ is a $N\times 1$ (or $100\times 1$) dimensional gradient.</p>

<p>\begin{align}
f(x) = \sum_{i=0,2,4,...,N-2}(1-x_i)^2 + \left(10(x_{i+1} - x_i^2)\right)^2
\end{align}
The gradient at odd components ($i=1,3,5,...$) is
$$200(x_{i+1} - x_i^2) $$
The gradient component at even coordinates ($i=0,2,4,6,...$) is 
$$-2(1-x_i) - 400*x_i*(x_{i+1} - x_i^2)$$</p>

<p><em>Note</em>:</p>

<ol>
<li>indexing starts from 0</li>
<li>I believe the gradients in the code are incorrect. When I change the appropriate line g[i+1] = 20.0 * t2; to g[i+1] = 200.0 * t2; I am getting a different answer. Potentially I may be making a mistake here. Nonetheless, hopefully I have answered your question.</li>
</ol>

<p><strong>Our fitting problem</strong>
In our case, we have a two dimensional problem. Rename our $f(x,y)$ to $z$. Then, we have an $m\times 3$ dimensional matrix of values with each row being a tuple $(x_j,y_j,z_j), j=1,...,m$ which are fixed. We could now minimize the function $h(a,b)$
\begin{align}
h(a,b) = \sum_{j=1}^{m}(a\cos(x_j) +b y_j - z_j)^2
\end{align}
With
\begin{align}
\frac{\partial h(a,b)}{\partial a} = -2\sum_{j=1}^{m}\left((a\cos(x_j) +b y_j - z_j)\sin(x_j)\right)\\
\frac{\partial h(a,b)}{\partial b} = 2\sum_{j=1}^{m}\left((a\cos(x_j) +b y_j - z_j)y_j\right) 
\end{align}
as the gradient functions.
All that you need to do is encode these in place of the for loop above, change #define N 100 to 2 and initialize some initial value of $a,b$ to be passed into the lbfgs function.</p>
",2013-10-16 22:32:05.073
57667,20752.0,2,,57665.0,,,,CC BY-SA 3.0,"<p>CRFs and HMMs are not necessarily exclusive model formulations. In the formulation you have above, X in the HMM is usually a state variable that is unobserved, so a generative model is somewhat necessary. In the CRF though, X is some feature vector that is observed and affects Y in the traditional way. But you can have a combination of both: a sequence of states and outputs where the state is unobserved, and a set of observed features that affects the conditional probabilities of the outputs given the states (or transition probabilities between states). </p>

<p>I believe that ultimately the CRF admits some more flexible models where the conditional probabilities are more dynamic, and could be affected by, for example, the output from several observations ago, or something like that. They can get awfully large and difficult to train when they start including many more free parameters like that though.</p>
",2013-10-16 22:50:58.400
57668,9175.0,1,57724.0,,,Prove the following are independent,<self-study><independence>,CC BY-SA 3.0,"<p>I have been unable to figure out the following question</p>

<p>$f_{Y|X}(y|x)=N(x,x^2)$</p>

<p>$f_X(x)=U(0,1)$</p>

<p>Prove that $\frac{Y}{X}$ and $X$ are independent</p>
",2013-10-16 23:16:46.210
57669,1145.0,1,57671.0,,,Calculate $t$ value for expanded sample using just $t$ and $n$,<t-test><small-sample>,CC BY-SA 3.0,"<p>Imagine a sample of observations $x_1 \dots x_n$ for which a $t$-value (i.e. $\bar x/SEM$) is known. Say an additional observation of zero is added to the sample so that we have $x_1 \dots x_{n+1}$ where $x_{n+1}=0$. </p>

<p>Can a new $t$-value be calculated from just the known original $t$ and $n$? (I suspect that it can, but my mathematics is inadequate for me to work out exactly how.)</p>
",2013-10-16 23:18:37.720
57670,16205.0,1,,,Manas,Comparing statistics of networks of different sizes,<graph-theory>,CC BY-SA 3.0,"<p>Hi This maybe a basic Stats question.</p>

<p>Let say I have 3 networks of different sizes. Size in terms of number of nodes and links. 
Network n1, n2 and n3 have v1, v2 and v3 nodes and l1, l2 and l3 links. That is all we know about the networks.</p>

<p>For each of these networks we compute some parameters based on some criteria. For example,</p>

<p>param1 = no. of nodes with criteria 1</p>

<p>param2 = no. of nodes with criteria 2</p>

<p>param3 = no. of nodes with criteria 3</p>

<p>Result</p>

<pre><code>   nodes links param1 param2 param3 
n1   v1    l1     a      b      c   
n2   v2    l2     d      e      f   
n3   v3    l3     g      h      i
</code></pre>

<p>Question</p>

<p>If I average (a+d+g/3) to get the expected param1 across networks, would that make sense knowing that the size of the networks is different and the parameters are a function of the size of the network? How to reconcile with the fact that different networks have different sizes and therefore cannot be compared directly.</p>
",2013-10-16 23:34:59.187
57671,594.0,2,,57669.0,,,,CC BY-SA 3.0,"<p>You have $$t = \frac{\bar x}{s/\sqrt n} = \sqrt n \frac{\bar x}{s}$$</p>

<p>So $$\frac{\bar x}{s} =t/\sqrt{n}$$</p>

<p>Using the subscript $n$ to denote ""calculated from the sample of size $n$"":</p>

<p>$$\bar{x}_{n+1} = \frac{n\bar{x}_n + 0}{n+1} = \frac{n}{n+1} \bar{x}_n$$</p>

<p>$$s^2_{n+1} = \frac{1}{n}[(n-1)s^2_n + (0-\bar{x}_{n})(0-\bar{x}_{n+1})] = \frac{n-1}{n}s^2_n+\bar{x}_{n}\bar{x}_{n+1}/n$$</p>

<p>$$= \frac{n-1}{n}s^2_n+\frac{\bar{x}_{n}^2}{n+1}$$</p>

<p>$$s_{n+1} = s_n\sqrt{\frac{n-1}{n}+\frac{\bar{x}_{n}^2}{(n+1)s^2_n}}= \frac{s_n}{\sqrt{n}}\sqrt{n-1+\frac{t_n^2}{n+1}}$$</p>

<p>So:</p>

<p>\begin{eqnarray}
t_{n+1} &amp;=&amp; \frac{\bar{x}_{n+1}}{s_{n+1}/\sqrt{n+1}}\\
&amp;=&amp; \frac{\frac{n}{n+1} \bar{x}_n}{\frac{s_n}{\sqrt{n}} \sqrt{n-1+\frac{t_n^2}{n+1}}/\sqrt{n+1}}\\
&amp;=&amp; \frac{\bar{x}_n}{s_n/\sqrt{n}}\frac{\frac{n}{n+1} }{ \sqrt{n-1+\frac{t_n^2}{n+1}}/\sqrt{n+1}}\\
  &amp;=&amp; t_n \frac{n }{ \sqrt{n^2-1+t_n^2}}
\end{eqnarray}</p>

<hr>

<p>Numerical example in R; first computed using the formula above, then the actual expanded sample:</p>

<pre><code>&gt; x=rnorm(10);(t=mean(x)/(sd(x)/sqrt(10)))
[1] -0.2219399
&gt; t*(10/sqrt(99+t^2))
[1] -0.2230025
&gt; x1=c(x,0);(t1=mean(x1)/(sd(x1)/sqrt(11)))
[1] -0.2230025
</code></pre>

<p>Looks like it works. </p>
",2013-10-17 00:00:26.417
57672,,1,,,user30490,Relationship between inverse gamma and gamma distribution,<gamma-distribution><inverse-gamma-distribution>,CC BY-SA 3.0,"<p>I have the following posterior distribution for $v$
$$f(v)\propto v^{-p/2}\exp\left(-\frac{1}{v}\frac{s}{2}\right)$$
and so clearly 
$$v\sim\text{Inverse-Gamma}\left(\frac{p}{2}-1,\frac{s}{2}\right)$$</p>

<p>Now can I say that 
$$v^{-1}\sim\text{Gamma}\left(\frac{p}{2}-1,\frac{s}{2}\right)$$</p>
",2013-10-17 00:18:07.753
57673,22779.0,1,57674.0,,,Relationship between test stat and p-value in relation to t-test,<statistical-significance><t-test><p-value>,CC BY-SA 3.0,"<p>I was wondering  why it is necessarily true that if a test statistic exceeds the critical value of t, then it will also be true that the p-value will not exceed the level of significance.</p>
",2013-10-17 00:23:22.743
57674,5237.0,2,,57673.0,,,,CC BY-SA 3.0,"<p>I suppose this depends on what is meant by ""exceed"", but generally when people say a test statistics <em>exceeds</em> the critical value, they mean $|t|\boldsymbol{&gt;}t_{\rm crit}$, and when they say the p-value <em>exceeds</em> the level of significance, they mean $p\boldsymbol{&lt;}\alpha$.  Thus, when the test statistic exceeds the critical value of t, the p-value <em>also</em> exceeds the level of significance.  </p>

<p>As to why that fact is the case, it is simply because the value of $t_{\rm crit}$ is <em>determined</em> by the point where $p&lt;\alpha$.  </p>
",2013-10-17 00:31:51.443
57675,4537.0,2,,57643.0,,,,CC BY-SA 3.0,"<p>Do you mean equation (2)? I think he's not using Bayes' theorem at all -- he's just using the definition of conditional probability.</p>

<p>Recall that if $A$, $B$ are events, then
$$
P(A | B) = \frac{P(A \cap B)}{P(B)}.
$$
If you want to know more about conditional probability, I think the <a href=""http://en.wikipedia.org/wiki/Conditional_probability"" rel=""nofollow"">Wiki article</a> is pretty good.</p>
",2013-10-17 01:22:03.280
57676,13045.0,1,94362.0,,,Inverse transformation sampling for mixture distribution of two normal distributions,<sampling><random-generation><mixture-distribution><cumulative-distribution-function><weighted-sampling>,CC BY-SA 3.0,"<p>I am confused by the special way required to use inverse method in the following problem,</p>

<p>Here is the problem:</p>

<blockquote>
  <p>Consider a mixture distribution of two normal distributions, where the
  desired PDF $f(x)$ is given by:</p>
  
  <p>$f(x) = r\, f_a(x) + (1 âˆ’ r)\, f_b(x)$,where $f_a$ and $f_b$ are
  normal PDFs with means $a$ and $b$, respectively (standard deviation
  is 1 for both). Using two uniform random variables $u_1$ and $u_2$,
  explain how we can use the inversion method to sample from $f(x)$.
  Note, the <code>qnorm</code> command in R may be helpful here.</p>
</blockquote>

<p>My confusion is from ""two uniform random variables $u_1$ and $u_2$"". My thought is that we find out the cdf, $F(x)$ (which can be obtained via <code>pnorm()</code> in R), and then we can use some numerical method (such as Newton-Raphson) to generate $x\sim f(x)$, so here it only needs one uniform distribution and does not need <code>qnorm()</code>. </p>

<p>What's wrong with my method? Does the problem suggest a better method? </p>
",2013-10-17 01:37:34.807
57677,18865.0,2,,30862.0,,,,CC BY-SA 4.0,"<p>The point of low-rank approximation is not necessarily just for performing dimension reduction.</p>

<p>The idea is that based on domain knowledge, the data/entries of the matrix will somehow make the matrix low rank. But that is in the ideal case where the entries are not affected by noise, corruption, missing values etc. The observed matrix typically will have much higher rank.</p>

<p>Low-rank approximation is thus a way to recover the ""original"" (the ""ideal"" matrix before it was messed up by noise etc.) low-rank matrix i.e., find the matrix that is most consistent (in terms of observed entries) with the current matrix and is low-rank so that it can be used as an approximation to the ideal matrix. Having recovered this matrix, we can use it as a substitute for the noisy version and hopefully get better results.</p>
",2013-10-17 01:40:25.533
57678,17448.0,1,57682.0,,,How can the Kolmogorov-Smirnov test be used/interpreted?,<hypothesis-testing><kolmogorov-smirnov-test>,CC BY-SA 3.0,"<p>The test of Kolmogorov-Smirnov (K-S) is a traditional test of normality, although Shapiro-Wilk test (S-W) is applied more frequently than K-S (<a href=""http://www.grupogen.com.br/ch/prod/8045/3677/3438/bioestatistica-teorica-e-computacional.aspx"" rel=""nofollow"">Arango, 2012</a>). </p>

<p>I am not an expert in statistics, so my question concerns about the use of K-S test. </p>

<ol>
<li>Is it possible to use K-S test to another purposes besides normality test?  </li>
<li>Why some times we should use K-S test instead of S-W test? Is it related to sample size?</li>
</ol>
",2013-10-17 01:46:24.010
57679,4537.0,2,,57672.0,,,,CC BY-SA 3.0,"<p>Yes, but I think the first parameter of the Gamma should be $1-p/2$ instead of $1+p/2$.
$$
v \sim \text{Gamma}(1-p/2, s/2)  
$$
I'm using the shape-rate parametrization, as in <a href=""http://en.wikipedia.org/wiki/Gamma_distribution"" rel=""nofollow"">here</a>.</p>
",2013-10-17 01:52:23.197
57680,2075.0,1,57777.0,,,Linear kernel and non-linear kernel for support vector machine?,<machine-learning><classification><svm><references><kernel-trick>,CC BY-SA 3.0,"<p>When using support vector machine, are there any guidelines on choosing linear kernel vs. nonlinear kernel, like RBF? I once heard that non-linear kernel tends not to perform well once the number of features is large. Are there any references on this issue?</p>
",2013-10-17 02:21:02.553
57681,10684.0,2,,57670.0,,,,CC BY-SA 3.0,"<p>A typical strategy would be to compare the <em>proportion</em> of the nodes in each network satisfying each of the criteria instead of the <em>number</em> of nodes. So for example, if your networks had $100, 200, 400$ nodes respectively and the <code>param1</code> for each of these networks was $50, 60, 80$, then the corresponding proportions of nodes of this particular type would be $50/100, 60/200$ and $80/300$, or $0.5, 0.3, 0.2$. Then it would make sense to average these numbers; the average proportion of nodes of this type is $(0.5+0.3+0.2)/3 = 0.333$. So if, say, you had a new network with $1000$ nodes, you might guess that it would have about $0.333 \times 1000 = 333$ nodes of the desired type, all other things being equal.</p>
",2013-10-17 02:27:02.890
57682,594.0,2,,57678.0,,,,CC BY-SA 3.0,"<p>The Kolmogorov-Smirnov test is a test of <em>any</em> completely specified continuous distribution against general alternatives.</p>

<p>The Shapiro-Wilk is a test of normality without specifying the mean or variance.</p>

<p>That is, the K-S and the S-W apply to different circumstances. To apply the K-S to the situation of the S-W, you'd get the Lilliefors test for normality (which allows for the effect of the parameter estimation, via simulation). Alternatively, to apply the S-W to the situation of the K-S on normal distributions you'd need to add a test for the specified mean and variance and combine the two in some way.</p>

<p>The Shapiro-Wilk has excellent power against a wide range of alternatives from normality.</p>

<p>There are other alternatives to the Shapiro-Wilk, such as the Anderson-Darling test. The Anderson-Darling is usually preferred to the K-S on the basis that it generally has better power against interesting alternatives.</p>

<p>If you adjust the distribution of the A-D for estimated parameters, it's reasonably competitive with the Shapiro-Wilk at the normal, but the S-W would generally be slightly preferred.</p>
",2013-10-17 02:46:15.200
57683,594.0,2,,57331.0,,,,CC BY-SA 3.0,"<p>Your question implies that for independent random variables, $Ïƒ_\text{sum} = Ïƒ_x + Ïƒ_y + Ïƒ_z$. This is not the case.</p>

<p>The <em>squares</em> are additive: $Ïƒ^2_\text{sum} = Ïƒ^2_x + Ïƒ^2_y + Ïƒ^2_z$. So $Ïƒ_\text{sum} = \sqrt{Ïƒ^2_x + Ïƒ^2_y + Ïƒ^2_z}$.</p>

<p>However, otherwise you're correct - if you add three independent normal random variables, the distribution of the sum is normal with mean equal to the sum of their means and variance equal to the sum of their variances (indeed that applies to adding any number of terms).</p>

<p>Variances of correlated random variables are a little more complicated, but still straightforward. For correlated multivariate normals, you also still have normality.</p>
",2013-10-17 02:59:28.050
57684,22781.0,1,,,,Sobel test with survey data,<references><survey><mediation>,CC BY-SA 3.0,"<p>I would like to ask if there is any problem / concern with Sobel test when I use survey data? In particular, I was using the web-based calculation tool <a href=""http://quantpsy.org/sobel/sobel.htm"" rel=""nofollow"">here</a> based on coefficients derived from regression analysis using survey command. I also hope to know if there is a good reference article about this. </p>
",2013-10-17 03:10:06.457
57685,18268.0,1,,,,Weighing probabilities into a polygon,<data-visualization><clustering>,CC BY-SA 3.0,"<p>I have a collection of 4-member probability vectors (essentially proportions over 4 mutually exclusive categories). Is there a method to represent this data as a cloud of points inside a square? If each of the values of the 4-tuple represents a weight towards one of the 4 edges, can we appropriately place each inside the 2D space?</p>

<p>What I am looking for is a method to plot data of the following sort:</p>

<pre><code>1: 1 0 0 0
2: 0 1 0 0
3: 0.5 0.5 0 0
4: 0.25 0.25 0.25 0.25
</code></pre>

<p>In the following fashion (sorry the bottom border got clipped)</p>

<p><img src=""https://i.stack.imgur.com/UTRbM.png"" alt=""enter image description here""></p>

<p>Thanks.</p>
",2013-10-17 04:21:25.697
57686,22783.0,1,,,,Plot of copula (based on data set) - R,<distributions><correlation><data-visualization><data-transformation><copula>,CC BY-SA 3.0,"<p>I have to do an empirical analysis for a statistics paper. For this I want to show the differences of dependence structure for a specific data set.</p>

<p>So I selected 2 stock prices, transformed them into the returns and started to measure the dependency with R. So far it is no problem, I have a result for Bravais-Pearson, Kendall and Spearman. Additionally I plotted the regression model for this two values.</p>

<p>I have read in many papers, according to Sklar's Theorem, that it is easy to get the copula function out of the distribution function, just by use the inverse.</p>

<p>So my question is, if there is a possibility with R to plot the copula function (and density) just by having this data set (2 returns) or if I must first estimate the parameters to be able to plot this function.   </p>

<p>And how can I do this with R? I tried to search the answer in the handbook of the package ""copula"", but my search wasn't really helpful.</p>

<p>Thanks in advance for your help!</p>
",2013-10-17 05:20:18.950
57687,594.0,2,,57359.0,,,,CC BY-SA 3.0,"<p>When including polynomials and interactions between them, multicollinearity can be a big problem; one approach is to look at orthogonal polynomials.</p>

<p>Generally, orthogonal polynomials are a family of polynomials which are orthogonal with 
respect to some inner product.</p>

<p>So for example in the case of polynomials over some region with weight function $w$, the 
inner product is $\int_a^bw(x)p_m(x)p_n(x)dx$ - orthogonality makes that inner product $0$ 
unless $m=n$.</p>

<p>The simplest example for continuous polynomials is the Legendre polynomials, which have 
constant weight function over a finite real interval (commonly over $[-1,1]$).</p>

<p>In our case, the space (the observations themselves) is discrete, and our weight function is also constant (usually), so the orthogonal polynomials are a kind of discrete equivalent of Legendre polynomials. With the constant included in our predictors, the inner product is simply $p_m(x)^Tp_n(x) = \sum_i p_m(x_i)p_n(x_i)$.</p>

<p>For example, consider $x = 1,2,3,4,5$</p>

<p>Start with the constant column, $p_0(x) = x^0 = 1$. The next polynomial is of the form $ax-b$, but we're not worrying about scale at the moment, so $p_1(x) = x-\bar x = x-3$. The next polynomial would be of the form $ax^2+bx+c$; it turns out that $p_2(x)=(x-3)^2-2 = x^2-6x+7$ is orthogonal to the previous two:</p>

<pre><code>x         p0  p1  p2   
1          1  -2   2   
2          1  -1  -1
3          1   0  -2
4          1   1  -1
5          1   2   2
</code></pre>

<p>Frequently the basis is also normalized (producing an orthonormal family) - that is, the sums of squares of each term is set to be some constant (say, to $n$, or to $n-1$, so that the standard deviation is 1, or perhaps most frequently, to $1$).</p>

<p>Ways to orthogonalize a set of polynomial predictors include Gram-Schmidt orthogonalization, and Cholesky decomposition, though there are numerous other approaches.</p>

<hr>

<p>Some of the advantages of orthogonal polynomials:</p>

<p>1) multicollinearity is a nonissue - these predictors are all orthogonal.</p>

<p>2) The low-order coefficients <em>don't change as you add terms</em>. If you fit a degree $k$ polynomial via orthogonal polynomials, you know the coefficients of a fit of all the lower order polynomials without re-fitting.</p>

<hr>

<p>Example in R (<code>cars</code> data, stopping distances against speed):
<img src=""https://i.stack.imgur.com/O3BzE.png"" alt=""enter image description here""></p>

<p>Here we consider the possibility that a quadratic model might be suitable:</p>

<p>R uses the <code>poly</code> function to set up orthogonal polynomial predictors:</p>

<pre><code>&gt; p &lt;- model.matrix(dist~poly(speed,2),cars)
&gt; cbind(head(cars),head(p))
  speed dist (Intercept) poly(speed, 2)1 poly(speed, 2)2
1     4    2           1      -0.3079956      0.41625480
2     4   10           1      -0.3079956      0.41625480
3     7    4           1      -0.2269442      0.16583013
4     7   22           1      -0.2269442      0.16583013
5     8   16           1      -0.1999270      0.09974267
6     9   10           1      -0.1729098      0.04234892
</code></pre>

<p>They're orthogonal:</p>

<pre><code>&gt; round(crossprod(p),9)
                (Intercept) poly(speed, 2)1 poly(speed, 2)2
(Intercept)              50               0               0
poly(speed, 2)1           0               1               0
poly(speed, 2)2           0               0               1
</code></pre>

<p>Here's a plot of the polynomials:
<img src=""https://i.stack.imgur.com/SMVuV.png"" alt=""enter image description here""></p>

<p>Here's the linear model output:</p>

<pre><code>&gt; summary(carsp)

Call:
lm(formula = dist ~ poly(speed, 2), data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-28.720  -9.184  -3.188   4.628  45.152 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       42.980      2.146  20.026  &lt; 2e-16 ***
poly(speed, 2)1  145.552     15.176   9.591 1.21e-12 ***
poly(speed, 2)2   22.996     15.176   1.515    0.136    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 15.18 on 47 degrees of freedom
Multiple R-squared:  0.6673,    Adjusted R-squared:  0.6532 
F-statistic: 47.14 on 2 and 47 DF,  p-value: 5.852e-12
</code></pre>

<p>Here's a plot of the quadratic fit:
<img src=""https://i.stack.imgur.com/S7xMV.png"" alt=""enter image description here""></p>
",2013-10-17 06:04:30.500
57688,22426.0,1,,,,Meaning of link functions (GLM),<regression><link-function>,CC BY-SA 3.0,"<p>I am performing ordinal regression on several datasets, I have 5 ordered response categories and only one explanatory variable X.
For each dataset I run the analysis 3 times, each time using a different link function (1. probit, 2. logit, 3. comploglog) and I calculate the AIC to see which function fits my data best. </p>

<p>It seems that for different datasets I get different link functions significantly providing a ""best"" fit; for example probit is better for dataset 1 and logit is better for dataset 2 etc. 
I am trying to find an explanation for such difference. </p>

<p>So my question is, what is the ""physical"" meaning of each link function? 
For example, I understand the probit link function assumes the response scale can be related to a latent continuous, normally distributed variable but for the other 2 I have no idea.</p>

<p>Any insight on this would be great!</p>
",2013-10-17 06:08:37.423
57689,12787.0,1,,,,Not specifying the main effect of a term that is part of a tensor product interaction,<nonparametric><interaction><generalized-additive-model><scale-invariance>,CC BY-SA 3.0,"<p>Say you've got a model 
$$
y = f(X_1,X_2)+\epsilon
$$</p>

<p>and you're OK with linear (or other parametric) functional forms.  Say you think that the effect of $X_1$ on $y$ depends on $X_2$.  The standard wisdom is that one should include the main effect of the interaction to capture the effect of $X_1$ on $y$ when $X_2$ = 0, and to render coefficients invariant to location changes.  </p>

<p>I am curious whether this applies to generalized additive models estimated by penalized splines, with interactions represented by tensor products.  Intuitively, it seems like it would NOT apply.  </p>

<p>WRT the first issue, the tensor product smooth estimates the ""height"" of a surface defined by two variables, along a grid.  Thus you've always got a function for one variable at a fixed value of the other.  If you had a univariate smooth along with it, you'd have to simply add the two, at a cost of degrees of freedom (which would get penalized/reduced).</p>

<p>WRT the second issue, <a href=""http://www.jstor.org/stable/4124523"" rel=""nofollow"">this paper</a> by Simon Wood lays out how the tensor products implemented in <code>gam</code> are scale invariant.  </p>

<p>Is this a reasonable interpretation?  If I wanted to estimate the model above nonparametrically, is it unreasonable to leave out smooth functions of $X_1$ and $X_2$, and simply model $X_1 \otimes X_2$.</p>

<p>If yes, how would you quickly and imply explain why to an audience that isn't used to GAMs?</p>
",2013-10-17 06:43:51.323
57690,8719.0,1,57693.0,,,Simulating p-values as a function of sample size,<r><hypothesis-testing><statistical-significance>,CC BY-SA 3.0,"<p>We are trying to prove a very subtle effect occurring in cells after a certain treatment.  Let's assume that the measurements are normally distributed. Let's also assume the untreated cells have $\mu = 1$ and $\sigma = 0.1$ and the treated cells have $\mu = 1.1$ and $\sigma = 0.22$. The question is:</p>

<p>How large must the sample size be in order for the observed effect to be statistically significant ($\alpha = 0.05$)?</p>

<p>I know that very subtile effects require a larger sample size than more apparent effects, but how many? I'm still learning statistics, so please be patient with me. I tried to perform a little simulation in R. Assuming that you randomly pick $n$ samples from a normal distribution, I tried to calculate the mean p-value as a function of $n$. </p>

<p><img src=""https://i.stack.imgur.com/Fn5d3.png"" alt=""enter image description here""></p>

<p>Is this a correct way to do find the right sample size? Or am I completely off the track with this approach?</p>

<p>Code:</p>

<pre><code>library(ggplot2)

ctrl.mean &lt;- 1
ctrl.sd &lt;- 0.1
treated.mean &lt;- 1.1
treated.sd &lt;- 0.22

# Function that repeats t-test a number of times (rpt) with given sample size, means and sds.
# Returns a list of p-values from the test

tsim &lt;- function(rpt, n, mean1, sd1, mean2, sd2) {
  x &lt;- 0
  ppool &lt;- NULL
  while (x &lt;= rpt) {
    ppool &lt;- c(ppool, t.test(rnorm(n,mean1,sd1), y = rnorm(n,mean2,sd2))$p.value)
    x &lt;- x + 1
  }
  return(ppool)
}

# Iterate through sample sizes and perform the function
# Returns data frame with list of mean p-values at a given sample size

i &lt;- 2
num &lt;- 50
res &lt;- NULL

while (i &lt;= num) {
  sim &lt;- tsim(1000, i, ctrl.mean, ctrl.sd, treated.mean, treated.sd)
  res &lt;- rbind(res, cbind(i, mean(sim), sd(sim)))
  i &lt;- i + 1
}

# Plot the result

res &lt;- as.data.frame(res)

ggplot(res, aes(x=i, y=-log10(V2))) +
  geom_line() +
  geom_ribbon(aes(ymin=-log10(V2)-log10(V3), ymax=-log10(V2)+log10(V3)), alpha = 0.2) +
  annotate(""segment"", x = 6, xend = num, y = -log10(0.05), yend = -log10(0.05), colour = ""red"", linetype = ""dashed"") +
  annotate(""text"",  x = 0, y=-log10(0.05), label= ""p = 0.05"", hjust=0, size=3) +
  annotate(""segment"", x = 6, xend = num, y = -log10(0.01), yend = -log10(0.01), colour = ""red"", linetype = ""dashed"") +
  annotate(""text"",  x = 0, y=-log10(0.01), label= ""p = 0.01"", hjust=0, size=3) +
  annotate(""segment"", x = 6, xend = num, y = -log10(0.001), yend = -log10(0.001), colour = ""red"", linetype = ""dashed"") +
  annotate(""text"",  x = 0, y=-log10(0.001), label= ""p = 0.001"", hjust=0, size=3) +
  xlab(""Number of replicates"") +
  ylab(""-log10(p-value)"") +
  theme_bw()
</code></pre>
",2013-10-17 07:48:14.250
57691,9047.0,1,58379.0,,,Construct confidence interval of the mean for auto-correlated data,<r><confidence-interval><mean><autocorrelation>,CC BY-SA 3.0,"<p>I feel like I'm missing something obvious, but here we go. I have auto-correlated data measured in triplicate for two (or more) treatments. Something like this:</p>

<pre><code>t &lt;- 3:20 #times in my real dataset are possibly not always equidistant
a &lt;- structure(c(0.652492388457625, 0.905172522010166, 1.23437705454616, 
                 1.48003667490842, 1.77876898946135, 1.99175317367897, 2.31666502140984, 
                 2.43520651415548, 2.67903421794922, 2.84115747823017, 2.89693734873647, 
                 2.91199679761145, 2.85645436179354, 2.99371033437697, 2.99965220711105, 
                 2.84984814715963, 2.64275376547326, 2.64060469520379, 0.481029734912324, 
                 0.8466803252367, 1.31126162780809, 1.56745630574946, 1.74865844658142, 
                 1.80367117155375, 2.06688393210808, 2.24500095501872, 2.52978288460243, 
                 2.69073206006205, 2.89657418056785, 2.93759772556246, 2.99305951550274, 
                 2.89146932307489, 2.88890777189028, 2.7974672802907, 2.70933381639295, 
                 2.66799551352975, 0.624178180970784, 0.867127935268765, 1.09752295578438, 
                 1.35037796202753, 1.60094288950107, 1.97949255710341, 2.15496378191076, 
                 2.42556913246041, 2.54331160179646, 2.67440414122285, 2.84249532365163, 
                 2.95278639560433, 3.06192227561515, 3.03297885461444, 3.04101341059534, 
                 3.01736966686846, 2.80061410999215, 2.69852643323913), 
               .Dim = c(18L, 3L), .Dimnames = list(NULL, c(""a1"", ""a2"", ""a3"")))
b &lt;- structure(c(0.516527990622755, 0.84883434472028, 1.04202664437099, 
                 1.3100841689546, 1.48050413266838, 1.7824492800856, 1.96557179831706, 
                 2.17419105778186, 2.2453178060978, 2.35460428313729, 2.49308342865959, 
                 2.62343038370418, 2.70831189685371, 2.79459971623943, 2.94938536147398, 
                 3.04822554887815, 3.00287042052314, 2.91673487674283, 0.589490441973075, 
                 0.751768045201717, 0.917973959434798, 1.17617337222852, 1.39497560590896, 
                 1.65920945485901, 1.87749014780468, 2.11880355292648, 2.372755207219, 
                 2.46211141942227, 2.59688733749884, 2.72270421752644, 2.79848710425447, 
                 2.81134394947587, 2.75390203306788, 2.78499114431362, 2.86001341271914, 
                 2.95652300178809, 0.558662398944567, 0.834996005844121, 0.988238211915554, 
                 1.27569591423003, 1.38577342414377, 1.62664982549252, 1.83299700801392, 
                 2.04943560731628, 2.22950648854987, 2.38533269800646, 2.49845003387994, 
                 2.60036098089373, 2.61941602504858, 2.71298500309883, 2.78126388719353, 
                 3.04792375845498, 3.02691814463875, 3.06667590650438), 
               .Dim = c(18L, 3L), .Dimnames = list(NULL, c(""b1"", ""b2"", ""b3"")))

matplot(t,a,pch=1,xlab="""",ylab="""",col=""blue"")
matlines(t,a,col=""blue"", lty=2)

matpoints(t,b,pch=16,col=""red"")
matlines(t,b,col=""red"", lty=2)
</code></pre>

<p><img src=""https://i.stack.imgur.com/RbEPX.png"" alt=""simulated auto-correlated data""></p>

<p>I would like to know in which time periods the treatments differ. I would like to avoid fitting any kind of model. (There are models for my kind of data from science, but they are known to be only an approximation for some ranges of my data and I'm afraid that model error might mask differences.) My idea is to calculate the mean and construct confidence intervals (using an assumption of normality) like this:</p>

<pre><code>a_means &lt;- apply(a,1,mean)
a_sds &lt;- apply(a,1,sd)
a_lwr &lt;- a_means-qt(0.975,3)*a_sds/sqrt(3)
a_upr &lt;- a_means+qt(0.975,3)*a_sds/sqrt(3)

b_means &lt;- apply(b,1,mean)
b_sds &lt;- apply(b,1,sd)
b_lwr &lt;- b_means-qt(0.975,3)*b_sds/sqrt(3)
b_upr &lt;- b_means+qt(0.975,3)*b_sds/sqrt(3)

DF &lt;- data.frame(treat=factor(rep(1:2, each=length(t))), 
                 time=rep(t, 2),
                 mean=c(a_means,b_means),
                 lwr=c(a_lwr,b_lwr),
                 upr=c(a_upr,b_upr))

library(ggplot2)
p &lt;- ggplot(DF, aes(x=time, y=mean, ymin=lwr, ymax=upr)) +
  geom_ribbon(aes(fill=treat), alpha=0.3) +
  geom_line(aes(color=treat))
print(p)
</code></pre>

<p><img src=""https://i.stack.imgur.com/2SUyM.png"" alt=""means and 95 % confidence intervals""></p>

<p>The way I'm constructing the confidence intervals obviously doesn't consider auto-correlation. </p>

<ul>
<li>Is there a way to construct some kind of ""auto-correlated confidence interval""?</li>
<li>Can I use the ""un-correlated confidence interval""? Can I somehow estimate if it is too narrow or too wide in comparison to the auto-correlated confidence interval?</li>
<li>Is there a better approach to my problem?</li>
</ul>
",2013-10-17 07:52:01.927
57692,22784.0,1,57697.0,,,How to check if there is any â€œdependenceâ€ between columns?,<correlation>,CC BY-SA 3.0,"<p>How to check if those values are ""dependent"" ? Take a look on the values in second column coz the difference between them is really huge so it's hard to choose the scale. Any propositions of checking ""dependence"" between them are welcome. I was thinking about correlation but I am new in statistic so please let me know step by step what should I do to analyse those data.</p>

<pre><code>diff    abund_mean
 0   3444804.79
 1   847887.02
 2   93654.19
 0   721692.76
 2   382711.04
 1   428656.65
 1   120933.91
 0   157528.72
 1   159650.70
 0   124602.80
 0   90844.33
 2   501825.37
 1   270592.56
</code></pre>

<p>I am learning R aswell so I can calculate evertyhing in R if you let me know how to do that.</p>
",2013-10-17 08:07:59.677
57693,4910.0,2,,57690.0,,,,CC BY-SA 3.0,"<p>You have almost performed what is usually called a <a href=""http://en.wikipedia.org/wiki/Statistical_power"" rel=""noreferrer"">power analysis</a>. I say almost, because what you usually measure in a power calculation is not the mean p-value, but rather the probability that, given the sample size and the hypothesised mean difference, you would get a p-value lower than say 0.05. </p>

<p>You can make small changes to your calculations in order to get this probability, however. The following script is a modification of your script that calculates the power for sample sizes from 2 to 50:</p>

<pre><code>ctrl.mean &lt;- 1
ctrl.sd &lt;- 0.1
treated.mean &lt;- 1.1
treated.sd &lt;- 0.22

n_range &lt;- 2:50
max_samples &lt;- 50
power &lt;- NULL
p.theshold &lt;- 0.05
rpt &lt;- 1000

for(n in n_range) {
  pvals &lt;- replicate(rpt, {
    t.test(rnorm(n,ctrl.mean, ctrl.sd), y = rnorm(n, treated.mean, treated.sd))$p.value
  })
  power &lt;- rbind(power, mean(pvals &lt; p.theshold) )
}

plot(n_range, power, type=""l"", ylim=c(0, 1))
</code></pre>

<p><img src=""https://i.stack.imgur.com/XyrnP.png"" alt=""power analysis""></p>

<p>The way I would read this graph goes like: ""Given my assumptions of the two groups, the probability that I would find a significant effect at n = 30 is roughly 50%"". Often an 80% chance of finding an actual effect is considered a high level of power. By the way, power analysis is generally considered <em>a good thing</em>. :)</p>
",2013-10-17 08:27:11.787
57743,8926.0,2,,57680.0,,,,CC BY-SA 4.0,"<p>Andrew Ng gives a nice rule of thumb explanation in <a href=""https://www.youtube.com/watch?v=hDh7jmEGoY0"" rel=""noreferrer"">this video</a> starting 14:46, though the whole video is worth watching.</p>

<p>Key Points</p>

<ul>
<li>Use linear kernel when number of features is larger than number of observations.</li>
<li>Use gaussian kernel when number of observations is larger than number of features.</li>
<li>If number of observations is larger than 50,000 speed could be an issue when using gaussian kernel; hence, one might want to use linear kernel.</li>
</ul>
",2013-10-18 02:01:22.970
57694,20062.0,2,,57636.0,,,,CC BY-SA 3.0,"<p>When you have <strong>multiple variable</strong> and you are looking for variable(s) <strong>which is the best for discriminating between groups</strong> (""yes"" and ""no"" samples in this case) a tool for this is <strong>MANOVA</strong>.</p>

<pre><code># Suppose we have a data.frame with 7 variables and one group:
my.data&lt;-data.frame(v1=rnorm(100),v2=rnorm(100),v3=rnorm(100),
v4=rnorm(100),v5=rnorm(100),v6=rnorm(100), v7=c(rnorm(50),
rnorm(50)+20),response=rep(c(""yes"",""no""), each=50))

# run MANOVA
my.mnv&lt;-manova(cbind(v1,v2,v3,v4,v5,v6,v7) ~ response, data=my.data)

# and look on p-values (if p-value &lt; 0.05 then it is able to 
# significantly discriminate between ""yes"" and ""no"")
summary.aov(my.mnv)

# plot
pairs(my.data[c(""v1"",""v2"",""v3"",""v4"",""v5"",""v6"",""v7"")], pch=22,
bg=c(""red"", ""yellow"")[unclass(my.data$response)])
</code></pre>

<hr>

<p><strong>It's not good to make conclusions about statistical significance based on looking on the plot</strong> (although it is necessary to look on it). In you case of 107 variables the <code>pairs()</code> plot will be very chaotic.</p>
",2013-10-17 08:31:52.307
57695,,1,,,Matteo,How to estimate errors on a sample with very few data points,<standard-error><small-sample><median>,CC BY-SA 3.0,"<p>I have a very simple question to ask, but I can't figure this on my own.
I have two samples: sample A with only three data points, and sample B with hundreds of points. For each sample I measure the median of certain quantity.
Now my question is, what is the error associated to the median?
I can consider the quartiles but, what if sample A consisted of only 1 single point? This would virtually assign no error to median value. I expect my measurement more accurate for sample B.</p>
",2013-10-17 08:41:56.497
57696,22787.0,1,57709.0,,,Interpreting 5-way Mixed Model ANOVA,<anova><mixed-model><interpretation>,CC BY-SA 3.0,"<p>I'm running a 2x2x2x2x2 mixed model ANOVA (on SPSS v21) for my study and found two 4-way interactions, one 5-way interactions and a couple of 3-way interactions. Whilst I understand how to interpret a 3-way ANOVA. I'm having quite a hard time trying to interpret the 4/5-way interactions. </p>

<p>My variables are:</p>

<ul>
<li><em>Within-subject factors</em>: lineup sex (female, male), 
lineup ethnicity (Asian, Caucasian) </li>
<li><em>Between-subject factors</em>:
lineup procedure (sequential, simultaneous),
participant ethnicity (Asian, Caucasian), 
participant sex (female, male)</li>
</ul>

<p>Sample size: 552</p>

<p>I've gotten to the stage where I have done separate ANOVAs (splitting the data) on each factor following a 5-way or 4-way interactions obtained from the initial analysis (e.g., AxB at C1, AxB at C2 and so on) and obtained a ton of output. Some of the output had only significant main effect while others had no sig. effects. But the graph that the SPSS produced with the estimated marginal means clearly indicates a significant interaction. </p>

<p><strong>E.g</strong>: 
Lineup ethnicity x Participant sex at two levels of Lineup Sex.</p>

<blockquote>
  <p>Male Caucasians Lineup: Lineup ethnicity x Participant Sex</p>
  
  <ul>
  <li>All main effects &amp; interaction are non. sig.</li>
  </ul>
  
  <p>Female Caucasian Lineup: Lineup ethnicity x Participant Sex</p>
  
  <ul>
  <li>All main effects &amp; interaction are non. sig.</li>
  </ul>
  
  <p>But the graph indicates an interaction effect only for Female
  Caucasians.</p>
</blockquote>

<p>I've consulted with my supervisor about this and due to the time constraints, he has advised me to just compare and contrast the graphs of each ANOVA. That much I understand but I am clueless as to what to do next. Yes, I compare between the graphs but am unsure on how that would help explain the 5/4-way interactions..</p>

<p>Any help would be greatly appreciated!</p>
",2013-10-17 09:15:19.207
57697,20470.0,2,,57692.0,,,,CC BY-SA 3.0,"<p>You can check linear association between the two columns using <a href=""http://en.wikipedia.org/wiki/Correlation"" rel=""nofollow"">correlation</a>. The advantage of correlation over <a href=""http://en.wikipedia.org/wiki/Covariance"" rel=""nofollow"">covariance</a> is that it is normalised and not dependent on the scales of the column values you are comparing. It takes a value between $-1$ and $1$.</p>

<p>If <code>diff</code> and <code>abund_mean</code> are numeric sequences, in R:</p>

<pre><code>data &lt;- cbind(matrix(diff, ncol=1),matrix(abund_mean, ncol=1)) 
cor(data, use=""complete.obs"")
</code></pre>

<p>produces:</p>

<pre><code>&gt; cor(dat)
               [,diff]   [,abund_mean]
[diff,]        1.0000000 -0.2813283
[abund_mean,] -0.2813283  1.0000000
</code></pre>

<p>This is the correlation matrix. As you can see on the diagonals, the correlation of a column-variable with itself is maximum at $1$. The correlation between <code>diff</code> and <code>abund_mean</code> is negative at $-0.2813283$.</p>
",2013-10-17 09:20:19.100
57698,22788.0,1,,,,Single sample versus multiple sample,<estimation><sampling><binomial-distribution>,CC BY-SA 3.0,"<p>I have a jar with white and black balls. Total number of balls in the jar is 100000. I want to estimate the proportion of white balls. My constraint is that the sample size for estimation should be low, lets assume 500 balls. I am debating between two approaches.</p>

<ol>
<li>Draw a single sample of 500 balls, $\hat{p}$ = number of white balls
divided by 500</li>
<li>Draw 10 samples of 50 balls each. Calculate the proportion of white balls in each sample, i. e., $[r_1, r_2, ..., r_{10}]$. Estimated $\hat{p}$ = average of $[r_1, r_2, ..., r_{10}]$.</li>
</ol>

<p>Which method should I use so that I am less susceptible to sampling error?</p>
",2013-10-17 09:53:49.753
57699,22790.0,1,,,,Propensity scores and patient comparability,<propensity-scores>,CC BY-SA 3.0,"<p>I have two groups of patients who underwent a surgery using method A or method B. The first group are patients who were operated in 1980's and 1990's only with method A. The second group are patients operated recently with mostly method B, but also in some cases A. In addition to that, I have various variables about patients (gender, age, medical indicators, etc.) that capture the pre-operation medical history and types of symptoms that patient developed.</p>

<p>The goal of the study is to compare the ""effecitveness"" of methods A and B in terms of patients' survival times after operation.</p>

<p>Somewhat different patients (in terms of age, gender, etc.) are operated with A and B. For example, quite some more older people were operated with B than with A. I want to used propensity score matching to balance the data. </p>

<p>My question is:</p>

<p>Does it make sense to estimate propensity scores (method ~ age + gender + ...) and use them for creating a matched dataset for further analysis (e.g. Cox regression)?</p>

<p>In particular, is it a problem that for patients in the first group method B was not yet available, so none of them could potentially received the alternative treatment?</p>
",2013-10-17 10:18:03.660
57700,22793.0,1,,,,How to transform this dataset to make classes linearly separable?,<self-study><pca><kernel-trick>,CC BY-SA 3.0,"<p>I have this data set:</p>

<p><img src=""https://i.stack.imgur.com/ObZgv.png"" alt=""dataset""></p>

<p>And I want to transform the data (with a RBF kernel?) in order to be able to do a simple linear ridge-classifier. </p>

<p>I know I can do more or less the same thing using a kernel SVM with a RBF kernel but I have to use the ridge classifier. </p>

<p>Does anyone know how it (if?) can be done? This is kinda homeworkish so I don't want a full solution I just want some input. </p>
",2013-10-17 10:51:21.157
57744,1150.0,2,,57644.0,,,,CC BY-SA 3.0,"<p>Although I don't know anything about circuit design, I'm confused as everyone else why you would expect results to cluster around the average. The CLT has to do not with the shape (as many have correctly pointed out) but the probability of each a data point landing into a random a subset. If the underlying process had a bi-modal distribution, most of the samples would have skinnier peaks.  </p>

<p>If your random seeds determine the efficiency of their placement and there is some sort of upper and lower bound on performance then your numbers simply reflect the distribution of such locations.</p>
",2013-10-18 02:05:58.343
57745,2081.0,2,,57710.0,,,,CC BY-SA 4.0,"<p><strong>Nominal vs Interval</strong></p>

<p>The most classic ""correlation"" measure between a nominal and an <strong><em>interval</em></strong> (""numeric"") variable is <strong>Eta</strong>, also called correlation ratio, and equal to the root R-square of the one-way ANOVA (with p-value = that of the ANOVA). Eta can be seen as a symmetric association measure, like correlation, because Eta of ANOVA (with the nominal as independent, numeric as dependent) is equal to Pillai's trace of multivariate regression (with the numeric as independent, set of dummy variables corresponding to the nominal as dependent).</p>

<p>A more subtle measure is intraclass correlation coefficient (<strong>ICC</strong>). Whereas Eta grasps only the difference between groups (defined by the nominal variable) in respect to the numeric variable, ICC simultaneously also measures the coordination or agreemant between numeric values inside groups; in other words, ICC (particularly the original unbiased ""pairing"" ICC version) stays on the level of values while Eta operates on the level of statistics (group means vs group variances).</p>

<p><strong>Nominal vs Ordinal</strong></p>

<p>The question about ""correlation"" measure between a nominal and an <strong><em>ordinal</em></strong> variable is less apparent. The reason of the difficulty is that ordinal scale is, by its nature, more ""mystic"" or ""twisted"" than interval or nominal scales. No wonder that statistical analyses specially for ordinal data are relatively poorly formulated so far.</p>

<p>One way might be to convert your ordinal data <em>into ranks</em> and then compute <strong>Eta</strong> as if the ranks were interval data. The p-value of such Eta = that of Kruskal-Wallis analysis. This approach seems warranted due to the same reasoning as why Spearman rho is used to correlate two ordinal variables. That logic is ""when you don't know the interval widths on the scale, cut the Gordian knot by linearizing any possible monotonicity: go rank the data"".</p>

<p>Another approach (possibly more rigorous and flexible) would be to use <em>ordinal logistic regression</em> with the ordinal variable as the DV and the nominal one as the IV. The square root of <strong>Nagelkerkeâ€™s</strong> pseudo R-square (with the regression's p-value) is another correlation measure for you. Note that you can experiment with various link functions in ordinal regression. This association is, however, not symmetric: the nominal is assumed independent.</p>

<p>Yet another approach might be to <em>find</em> such a monotonic transformation of ordinal data into interval - instead of ranking of the penultimate paragraph - that would <em>maximize</em> R (i.e. <strong>Eta</strong>) for you. This is <em>categorical regression</em> (= linear regression with optimal scaling).</p>

<p>Still another approach is to perform <em>classification tree</em>, such as CHAID, with the ordinal variable as predictor. This procedure will <em>bin together</em> (hence it is the approach opposite to the previous one) adjacent ordered categories which do not distinguish among categories of the nominal predictand. Then you could rely on Chi-square-based association measures (such as Cramer's V) as if you correlate nominal vs nominal variables.</p>

<p>And @Michael in his comment suggests yet one more way - a special coefficient called Freeman's <strong>Theta</strong>.</p>

<p>So, we have arrived so far at these opportunities: (1) Rank, then compute Eta; (2) Use ordinal regression; (3) Use categorical regression (""optimally"" transforming ordinal variable into interval); (4) Use classification tree (""optimally"" reducing the number of ordered categories); (5) Use Freeman's Theta.</p>
",2013-10-18 02:28:34.133
57701,8386.0,2,,57698.0,,,,CC BY-SA 3.0,"<p>If there is no replacement of balls after drawing, then approaches 1 and 2 are equivalent. With approach 2, you can find the average of the proportions of white balls in each of the 10 samples, or find the total number of white balls in the 10 samples combined and express this as a proportion of 500.  Both calculations will give the same result (given that, as is the case here, the samples are all of the same size).</p>

<p>If however for approach 2 the balls are replaced after each sample of 50, then the sampling error will be slightly higher than with approach 1.  One way to see this is to consider the extreme case in which rather than 100000 there are only 500 balls in the jar. In that case approach 1, which would then be a 100% sample, would be guaranteed to estimate the true proportion correctly.  But approach 2 would still be subject to sampling error because each sample of 50 would be only a 10% sample.  With a much larger number of balls this effect is still present, albeit greatly diminished.</p>
",2013-10-17 11:07:46.470
57702,20426.0,1,,,,Simple regression assumptions (homoscedasticity),<regression><residuals><assumptions>,CC BY-SA 3.0,"<p>There is a simple regression model table I was looking at in a textbook with IQ values grouped into 5 intervals and each group had an N number associated with it. There was also information given about the residuals for each group (mean and variance for the residuals). 
e.g For the &lt; 75 IQ group, N = 23, Mean of residuals = -0.407 and variance = 71.288</p>

<p>The conclusion from the table was merely stated as ""assumptions for regression have been met"".  I am unable to figure out what method was used to suggest if the homogeneity of variance (homoscedasticity) assumption is reasonable for the model (based on the information in the table).  I'd like to know how the book arrived at its conclusion; are there plots of residuals' means/variances that indicate violations clearly?  Is it like an ANOVA, where visually, one can make the simplistic estimation that if the ratio of variances exceeds a certain number, the assumption has been violated?  Given a table like that, how does one proceed to test the assumptions of a regression model?  Thanks!</p>
",2013-10-17 11:30:20.973
57703,16474.0,2,,57702.0,,,,CC BY-SA 3.0,"<p>Based on your description I would guess that the authors just looked at the variances of the residuals and concluded that they were similar enough. They have given the variances, so you can make up your own mind if you agree with them.</p>
",2013-10-17 11:39:58.580
57704,12358.0,2,,57700.0,,,,CC BY-SA 3.0,"<p>Have you tried putting the data into $r,\theta$?  You could pick the origin as as the mean value of all of the data.  You'd end up with two, slightly overlapping ellipsoidal blobs.</p>
",2013-10-17 11:57:10.503
57705,19395.0,1,,,,Dependent is the difference between two Likert scales: Which regression to use?,<regression><multinomial-distribution><likert><ordered-logit>,CC BY-SA 3.0,"<p>I asked participants in an experiment the same question once before and once after the experiment, to see the effect of the experiment on the answer. The answers were given on a 5-point scale. To me the change, so the difference between the before and the after answer to the question, is interesting. So I want to see which of multiple factors in the experiment caused the difference and accordingly want to run a regression on the DIFFERENCE in answers.</p>

<p>The difference is always an integer, ranging from -4 to 4. Since its not continuos, I'd like to use something like multinomial logit or ordered logit, but cannot decide which one would be exactly right. Is there a 'right' regression to use?</p>

<p>(Related, I have the problem that always a group of 5 participants took part in the same experiment. But I guess that can be solved with an FE or clusters.)</p>

<p>edit(clarification): I have 200 observations of 40 experimental groups with 5 participants each. Dependent variables in the above scenario would be changes in the experiment-setup (same in each group but different among groups), the relation of participants in a group (personal relation, class-difference, etc.) and socio-economics of participants</p>
",2013-10-17 11:58:57.590
57706,3922.0,2,,57705.0,,,,CC BY-SA 3.0,"<p>I would run a bivariate ordinal model with the original responses, rather than their differences, constraining the demographic coefficients between the equations and letting the experimental condition coefficients to be free in the second response. In Stata, this can be done with <code>cmp</code>. You won't get much with 5 clusters, and that would be the limit to the # of explanatory variables if you cluster by them. If you have multiple observations per person, so that you have a few dozen data points, you can still do the random effects with <code>cmp</code> though.</p>
",2013-10-17 12:09:52.073
57707,21918.0,1,57754.0,,,Hessian matrix and initial guess in logistic regression,<regression><logistic>,CC BY-SA 3.0,"<p>The log-likelihood function for logistic function is $$l(\theta) = \sum_{i=1}^m(y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log(1 - h(x^{(i)})))$$, where $$h(x^{(i)}) = \frac{1}{1 + e^{-\theta^Tx^{(i)}}}\,.$$ </p>

<p>In order to obtain maximum likelihood estimation, I implemented fitting the logistic regression model using <strong>Newton's method</strong>. I encountered 2 problems:</p>

<ol>
<li><p>I try to fit the model to my data, but during the iterations, a <em>singular</em> Hessian matrix is encountered, what do I do with this kind of problem?</p></li>
<li><p>With different initial guess $\theta$, will the model converge to different results?</p></li>
</ol>
",2013-10-17 12:30:26.707
57708,2666.0,2,,57359.0,,,,CC BY-SA 3.0,"<p>I don't feel that centering is worth the trouble, and centering makes the interpretation of parameter estimates more complex.  If you use modern matrix algebra software, algebraic collinearity is not a problem.  Your original motivation of centering to be able to interpret main effects in the presence of interaction is not a strong one.  Main effects when estimated at any automatically chosen value of a continuous interacting factor are somewhat arbitrary, and it's best to think of this as a simple estimation problem by comparing predicted values.  In the R <code>rms</code> package <code>contrast.rms</code> function, for example, you can obtain any contrast of interest independent of variable codings.  Here is an example of a categorical variable x1 with levels ""a"" ""b"" ""c"" and a continuous variable x2, fitted using a restricted cubic spline with 4 default knots.  Different relationships between x2 and y are allowed for different x1.  Two of the levels of x1 are compared at x2=10.</p>

<pre><code>require(rms)
dd &lt;- datadist(x1, x2); options(datadist='dd')
f &lt;- ols(y ~ x1 * rcs(x2,4))
contrast(f, list(x1='b', x2=10), list(x1='c', x2=10))
# Now get all comparisons with c:
contrast(f, list(x1=c('a','b'), x2=10), list(x1='c', x2=10))
# add type ='joint' to get a 2 d.f. test, or conf.type='simultaneous'
# to get simultaneous individual confidence intervals
</code></pre>

<p>With this approach you can also easily estimate contrasts at several values of the interacting factor(s), e.g.</p>

<pre><code>contrast(f, list(x1='b', x2=10:20), list(x1='c', x2=10:20))
</code></pre>
",2013-10-17 12:39:51.070
57746,22817.0,1,,,,Deriving the optimum value of a function,<optimization><convergence>,CC BY-SA 3.0,"<p>I have a function $f(t) = \sum_{i=1}^{N} |y_i-t|$.</p>

<p>What will be the optimum value of t that will minimize it. How to derive it?</p>

<p>Similarly what is the optimal value of t which minimizes $f(t) = \sum_{i=1}^{N} |y_i-t|^{\infty}$?</p>
",2013-10-18 02:40:52.280
57709,22716.0,2,,57696.0,,,,CC BY-SA 3.0,"<p>Generally, you should start from the highest order interactions. You are probably aware that it is usually not sensible to interpret a main effect A when that effect is also involved in an interaction A:B. This is because the interaction tells you that the effect of A actually depends on the level of B, rendering any simple main effect interpretation of A impossible.
In the same way, if you have factors A, B, C, then A:B should not be interpreted if A:B:C is significant.</p>

<p>Thus, when you have a 5-way interaction, none of the lower-order interactions can be sensibly interpreted. Therefore, if I understand you correctly and you have interpreted your lower order interactions, you should probably not continue along those lines.</p>

<p>Rather, what you can do is to split up your data set and continue to analyze factor levels of your data set separately. Which of the factors you use to split up the dataset is arbitrary, but often it is very useful to split up the data for each variable and assess what you see. In your example, you might start with sex, and calculate an ANOVA for males, and another one for females (each ANOVA contains the 4 remaining factors). Just as well, you could split up the data according to ethnicity (one ANOVA for Asian, one for Caucasian).
You could also split up by one of the within-subject factors.</p>

<p>I will assume that you have decided to split the data by sex (just to continue with the example here).
Then, assume that for males, you get a 4-way interaction. You would then go on to split up the male data by one of the remaining variables (say, ethnicity). You would then calculate ANOVAs for male Asians (over the remaining 3 factors), and for male Caucasians.</p>

<p>Importantly, if you get only a lower-order interaction, then you are only ""allowed"" to analyze these further. This is because the other factors did not show significant differences. Thus, if your males ANOVA gives you only a 2-way interaction, then you would average over the other factors and calculate only an ANOVA over the 2 interacting factors (and, because we are in the male part of the ANOVAs, this would be for the males alone).</p>

<p>For the females, everything may look different, and so the decision which follow-up ANOVAs to calculate is separate for this group. So, what you did for males should be done for females in the same way ONLY if you got the same interactions.</p>

<p>Thus, you will potentially have a lot of ANOVAs, and it might not be easy to decide which ones to report. You should report 1 complete line down from the hightest interaction to the last effects (possibly t-tests to compare only 1 of your factors at the end). You should not usually report several lines (e.g., one starting the split-up by sex, then another one starting by ethnicity). However, you must report a complete line, and cannot simply choose to report only some of the ANOVAs of that line. So, you report one complete analysis, not more, not less. Which way to go in terms of splitting up / follow-up ANOVA is a subjective decision (unless you have clear hypotheses you can follow), and might depend on which results can be understood best etc.</p>
",2013-10-17 12:42:10.667
57710,22795.0,1,57745.0,,,Correlation coefficient between a (non-dichotomous) nominal variable and a numeric (interval) or an ordinal variable,<correlation><matlab><ordinal-data><categorical-data><continuous-data>,CC BY-SA 4.0,"<p>I've already read all the pages in this site trying to find the answer to my problem but no one seems to be the right one form me...</p>

<p>First I explain you the kind of data I'm working with...</p>

<p>Let's say that I have an array vector with several names of city, one for each of 300 users. I also have another array vector with scores response to a survey of each user or a continuous value for each user.</p>

<p>I would like to know if exist a correlation coefficient that compute the correlation between these two variables so, between a nominal and a numeric/continuous or ordinal variables.</p>

<p>I've searched on the Internet and in some pages they suggest to use the contingency coefficient or Cramer's V or Lambda coefficient or Eta . For each of this measure the just say that they could be applied for such data in which we have a nominal variable and interval or numerical variable.
The thing is that searching and searching, trying to understand every one of them, sometime is written or watching the examples that they are reasonable to use them if you have dichotomous nominal variable, except for Cramer's V, other time is not written any requirement for the type of data.
A lot of other pages say that is right to apply regression instead, that is right, but I would just simply like to know if there is a coefficient like pearson/spearman for this kind of data.</p>

<p>I also think that is no so properly to use Spearman Correlation coeff since the cities are not sortable.</p>

<p>I have also built the function of Cramer'sV and Eta by myself (I'm working with Matlab) but for Eta they don't talk about any p-value to see if the coefficient is statistically significant...</p>

<p>In the matlabWorks site there is also a nice toolbox that says to compute eta^2 but the kind of input it needs is not understandable.</p>

<p>Is here someone that have done a test like mine? If you need more detail to understand the kind of data I'm using just ask me and I'll try to explain you better.  </p>
",2013-10-17 13:05:10.583
57711,22796.0,1,,,,How to apply Bonferroni correction when including an interaction term?,<multiple-comparisons><bonferroni>,CC BY-SA 3.0,"<p>Suppose we have two variables $x_1$ and $x_2$ and an interaction term $x_1 \cdot x_2$. Suppose we set the family-wise error rate to $\alpha = 0.05$. For the Bonferroni correction, would we look at $\alpha/2$ or $\alpha/3$?</p>
",2013-10-17 14:17:45.533
57712,22798.0,1,,,,How can I explain these linear regression charts well on my scientific poster?,<regression><self-study><data-visualization><interpretation>,CC BY-SA 3.0,"<p>I know that these laboratory analysis reports each have a linear regression relationship and two have a positive slope and one has a negative slope. I am taking my first statistics class and want to be able to explain these very well in a research poster I am doing for work. Am I missing any important information? The negative slope indicates that the values are decreasing together negatively. The positive means they are increasing together. The ADL concentration chart does not have a very good relation but there is still a relation.
<img src=""https://i.stack.imgur.com/xxZYO.png"" alt=""ADL linear regression""></p>

<p><img src=""https://i.stack.imgur.com/F1mSM.png"" alt=""ADF linear regression"">
<img src=""https://i.stack.imgur.com/vWwFv.png"" alt=""r^2, p-value""></p>

<p><img src=""https://i.stack.imgur.com/qxpuk.png"" alt=""NDF linear regression""></p>
",2013-10-17 14:57:33.620
57713,668.0,2,,40104.0,,,,CC BY-SA 3.0,"<p>Provided not a whole lot of probability is concentrated on any single value in this linear combination, it looks like a <a href=""http://en.wikipedia.org/wiki/Cornish%E2%80%93Fisher_expansion"" rel=""noreferrer"">Cornish-Fisher expansion</a> may provide good approximations to the (inverse) CDF.</p>

<p>Recall that this expansion adjusts the inverse CDF of the standard Normal distribution using the first few cumulants of $S_2$.  Its skewness $\beta_1$ is</p>

<p>$$\frac{a_1^3 \lambda_1 + a_2^3 \lambda_2}{\left(\sqrt{a_1^2 \lambda_1 + a_2^2 \lambda_2}\right)^3}$$</p>

<p>and its kurtosis $\beta_2$ is</p>

<p>$$\frac{a_1^4 \lambda_1 + 3a_1^4 \lambda_1^2 + a_2^4 \lambda_2 + 6 a_1^2 a_2^2 \lambda_1 \lambda_2 + 3 a_2^4 \lambda_2^2}{\left(a_1^2 \lambda_1 + a_2^2 \lambda_2\right)^2}.$$</p>

<p>To find the $\alpha$ percentile of the standardized version of $S_2$, compute</p>

<p>$$w_\alpha = z +\frac{1}{6} \beta _1 \left(z^2-1\right) +\frac{1}{24} \left(\beta _2-3\right) \left(z^2-3\right) z-\frac{1}{36} \beta _1^2 z \left(2 z^2-5 z\right)-\frac{1}{24} \left(\beta _2-3\right) \beta _1 \left(z^4-5 z^2+2\right)$$</p>

<p>where $z$ is the $\alpha$ percentile of the standard Normal distribution.  The percentile of $S_2$ thereby is</p>

<p>$$a_1 \lambda_1 + a_2 \lambda_2 + w_\alpha \sqrt{a_1^2 \lambda_1 + a_2^2 \lambda_2}.$$</p>

<p>Numerical experiments suggest this is a good approximation once both $\lambda_1$ and $\lambda_2$ exceed $5$ or so.  For example, consider the case $\lambda_1 = 5,$ $\lambda_2=5\pi/2,$ $a_1=\pi,$ and $a_2=-2$ (arranged to give a zero mean for convenience):</p>

<p><img src=""https://i.stack.imgur.com/ZiLkF.png"" alt=""Figure""></p>

<p>The blue shaded portion is the numerically computed CDF of $S_2$ while the solid red underneath is the Cornish-Fisher approximation.  The approximation is essentially a smooth of the actual distribution, showing only small systematic departures.</p>
",2013-10-17 15:02:12.897
57714,5448.0,2,,51047.0,,,,CC BY-SA 3.0,"<p>Here you go - three examples.  I've made the code much less efficient than it would be in a real application in order to make the logic clearer (I hope.)</p>

<pre><code># We'll assume estimation of a Poisson mean as a function of x
x &lt;- runif(100)
y &lt;- rpois(100,5*x)  # beta = 5 where mean(y[i]) = beta*x[i]

# Prior distribution on log(beta): t(5) with mean 2 
# (Very spread out on original scale; median = 7.4, roughly)
log_prior &lt;- function(log_beta) dt(log_beta-2, 5, log=TRUE)

# Log likelihood
log_lik &lt;- function(log_beta, y, x) sum(dpois(y, exp(log_beta)*x, log=TRUE))

# Random Walk Metropolis-Hastings 
# Proposal is centered at the current value of the parameter

rw_proposal &lt;- function(current) rnorm(1, current, 0.25)
rw_p_proposal_given_current &lt;- function(proposal, current) dnorm(proposal, current, 0.25, log=TRUE)
rw_p_current_given_proposal &lt;- function(current, proposal) dnorm(current, proposal, 0.25, log=TRUE)

rw_alpha &lt;- function(proposal, current) {
   # Due to the structure of the rw proposal distribution, the rw_p_proposal_given_current and
   # rw_p_current_given_proposal terms cancel out, so we don't need to include them - although
   # logically they are still there:  p(prop|curr) = p(curr|prop) for all curr, prop
   exp(log_lik(proposal, y, x) + log_prior(proposal) - log_lik(current, y, x) - log_prior(current))
}

# Independent Metropolis-Hastings
# Note: the proposal is independent of the current value (hence the name), but I maintain the
# parameterization of the functions anyway.  The proposal is not ignorable any more
# when calculation the acceptance probability, as p(curr|prop) != p(prop|curr) in general.

ind_proposal &lt;- function(current) rnorm(1, 2, 1) 
ind_p_proposal_given_current &lt;- function(proposal, current) dnorm(proposal, 2, 1, log=TRUE)
ind_p_current_given_proposal &lt;- function(current, proposal) dnorm(current, 2, 1, log=TRUE)

ind_alpha &lt;- function(proposal, current) {
   exp(log_lik(proposal, y, x)  + log_prior(proposal) + ind_p_current_given_proposal(current, proposal) 
       - log_lik(current, y, x) - log_prior(current) - ind_p_proposal_given_current(proposal, current))
}

# Vanilla Metropolis-Hastings - the independence sampler would do here, but I'll add something
# else for the proposal distribution; a Normal(current, 0.1+abs(current)/5) - symmetric but with a different
# scale depending upon location, so can't ignore the proposal distribution when calculating alpha as
# p(prop|curr) != p(curr|prop) in general

van_proposal &lt;- function(current) rnorm(1, current, 0.1+abs(current)/5)
van_p_proposal_given_current &lt;- function(proposal, current) dnorm(proposal, current, 0.1+abs(current)/5, log=TRUE)
van_p_current_given_proposal &lt;- function(current, proposal) dnorm(current, proposal, 0.1+abs(proposal)/5, log=TRUE)

van_alpha &lt;- function(proposal, current) {
   exp(log_lik(proposal, y, x)  + log_prior(proposal) + ind_p_current_given_proposal(current, proposal) 
       - log_lik(current, y, x) - log_prior(current) - ind_p_proposal_given_current(proposal, current))
}


# Generate the chain
values &lt;- rep(0, 10000) 
u &lt;- runif(length(values))
naccept &lt;- 0
current &lt;- 1  # Initial value
propfunc &lt;- van_proposal  # Substitute ind_proposal or rw_proposal here
alphafunc &lt;- van_alpha    # Substitute ind_alpha or rw_alpha here
for (i in 1:length(values)) {
   proposal &lt;- propfunc(current)
   alpha &lt;- alphafunc(proposal, current)
   if (u[i] &lt; alpha) {
      values[i] &lt;- exp(proposal)
      current &lt;- proposal
      naccept &lt;- naccept + 1
   } else {
      values[i] &lt;- exp(current)
   }
}
naccept / length(values)
summary(values)
</code></pre>

<p>For the vanilla sampler, we get:</p>

<pre><code>&gt; naccept / length(values)
[1] 0.1737
&gt; summary(values)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.843   5.153   5.388   5.378   5.594   6.628 
</code></pre>

<p>which is a low acceptance probability, but still...  tuning the proposal would help here, or adopting a different one.  Here's the random walk proposal results:</p>

<pre><code>&gt; naccept / length(values)
[1] 0.2902
&gt; summary(values)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.718   5.147   5.369   5.370   5.584   6.781 
</code></pre>

<p>Similar results, as one would hope, and a better acceptance probability (aiming for ~50% with one parameter.)</p>

<p>And, for completeness, the independence sampler:</p>

<pre><code>&gt; naccept / length(values)
[1] 0.0684
&gt; summary(values)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  3.990   5.162   5.391   5.380   5.577   8.802 
</code></pre>

<p>Because it doesn't ""adapt"" to the shape of the posterior, it tends to have the poorest acceptance probability and is hardest to tune well for this problem.</p>

<p>Note that generally speaking we'd prefer proposals with fatter tails, but that's a whole other topic.</p>
",2013-10-17 15:06:27.020
57722,21182.0,2,,57644.0,,,,CC BY-SA 3.0,"<p>One important point that many seem to be confused about is the application of the Central Limit Theorem (CLT). The CLT applies to the <strong>arithmetic mean</strong> of a distributionâ€”<strong><em>not</em></strong> the distribution itself. Given an increasing number of samples, the average of those samples tends to be normally distributed with the ""mean mean"" equal to the overall mean and the variance of that mean estimator proportional to the variance and the number of samples. The actual distribution itself is not going to be normal. If anything, the distribution of the samples (NOT their average) will flesh out the shape of the distribution and you will have a greater chance of seeing an extreme value the more samples are generated.</p>

<p>My hunch is that your routing score is probably an average of some values, and, as such, is the mean of some distribution. If so, as the mean of a distribution, under many conditions <em>its own</em> distribution will tend to the normal after enough samples are generated.</p>
",2013-10-17 17:48:45.993
57723,8414.0,2,,57684.0,,,,CC BY-SA 3.0,"<p>From a mathematical standpoint, there's nothing wrong with doing a Sobel test with survey data (by the way, and slightly off-topic -- you should consider using a bootstrapping method to test your indirect effects instead of a Sobel test; bootstrapping methods <a href=""http://www.public.asu.edu/~davidpm/classes/publications/2007PsychologicalScience.pdf"" rel=""nofollow"">are uniformly more powerful</a> than Sobel tests).  The real question is what conclusions you would be able to draw from your Sobel test.</p>

<p>To get a clear sense of the problem, consider a simple study in which the researcher measures people's scores on a self-report measure of trait empathy and the amount of money these people donated to charitable causes within the last year.  Assuming that the researchers observed a relationship between trait empathy and donations, few people would make the mistake of concluding that trait empathy causes donations (i.e., empathy -> donations), since the people in the study were not randomized to their values of trait empathy.  Thus, it is possible that people who thought about their levels of donations reported higher levels of trait empathy (i.e., donations -> empathy) or that a third variable caused the observed values of both donations and empathy.</p>

<p>Let's now consider a study in which the researchers measured trait empathy, charitable donations, and positive emotions.  The researcher wishes to show that the experience of positive emotions mediates the link between trait empathy and charitable donations (i.e., that empathy -> positive emotions -> donations).  In order to convincingly establish mediation, we must show both that empathy -> positive emotions and that empathy -> donations.  However, because people were not randomized to their values of trait empathy, we cannot conclude that empathy caused either positive emotions or donations.</p>

<p>However, even if we had randomized people to their empathy scores, we would still not necessarily be able to conclude that positive emotions were a mediator for the empathy -> donations effect because, after people's assignment to their empathy scores, people were not randomized to their values of positive emotions.  Thus, even if we established a non-zero indirect effect, it is possible that, for example, an unobserved candidate mediator causes both positive emotions and donations, and it is this unobserved candidate mediator that creates the observed empathy -> positive emotions -> donations indirect effect (for more information about this problem, see some of the references added below).</p>

<p>In short, there is nothing wrong with doing a Sobel test or any other test of mediation with survey data.  However, just as when you examine simple bivariate relationships with survey data, such a test probably will not reveal much about causal mechanisms because the assumptions required to draw these conclusions are implausible at best.</p>

<p>I recommend reading some of the references below for more information about assumptions in mediation models.</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927874/"" rel=""nofollow"">Jo, B. (2008).  Causal inference in randomized experiments with mediational processes. Psychological Methods, 13, 314â€“336.</a></p>

<p><a href=""http://arxiv.org/pdf/1011.1079.pdf"" rel=""nofollow"">Imai, K., Keele, L., &amp; Yamamoto, T. (2010). Identiï¬cation, inference and sensitivity
analysis for causal mediation eï¬€ects. Statistical Science, 25, 51-71.</a></p>

<p><a href=""http://imai.princeton.edu/research/files/mediationP.pdf"" rel=""nofollow"">Imai, K., Keele, L., Tingley, D., &amp; Yamamoto T. (2011). Unpacking the black box of causality: Learning about causal mechanisms from experimental and observational studies. American Political Science Review, 105, 765-789.</a></p>
",2013-10-17 18:12:15.970
57715,13846.0,1,57753.0,,,"Logistic regression with categorical predictors, do log-odds differ from 0?",<regression><logistic><logit>,CC BY-SA 3.0,"<p>I have a 3 by 2 design, with a total of 6 conditions. The outcome is binary (0 or 1). Below is a sample dataset generated in <code>R</code>:</p>

<pre><code>set.seed(2)
mockdata&lt;-data.frame(outcome=sample(1:0, 48, prob=c(0.5, 0.5), replace=TRUE),
                     f1=rep(letters[1:2], each=24), 
                     f2=rep(letters[1:3], each=8))

head(mockdata)
#  outcome f1 f2
#1       0  a  a
#2       1  a  a
#3       1  a  a
#4       0  a  a
#5       1  a  a
#6       1  a  a
</code></pre>

<p><em>One of the things I would like to look at is whether the log-odds of the outcome for each of the 6 conditions is significantly different from 0.</em> I can create a new condition variable as follows:</p>

<pre><code>mockdata$f12 &lt;- paste(mockdata$f1, mockdata$f2, sep=""."")
</code></pre>

<p>then, I can do logistic regression using the newly created variable (see below for output). The intercept below tells me that for the condition that is treated as the baseline condition, the log-odds is not significant different 0. </p>

<p><strong>My questions are:</strong></p>

<p><strong>(1). to check the other conditions, should I simply change the baseline condition, and after testing all 6 conditions, I adjust the p-values accordingly?</strong></p>

<p><strong>(2). Are there better ways of testing what I want to test?</strong></p>

<pre><code>summary(glm(outcome ~f12, family=""binomial"", data=mockdata))

Call:
glm(formula = outcome ~ f12, family = ""binomial"", data = mockdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6651  -1.1774  -0.5168   1.0215   2.0393  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -0.1412     0.3281  -0.430   0.6669  
f121          0.6520     0.6806   0.958   0.3380  
f122          0.1412     0.6641   0.213   0.8316  
f123         -0.3696     0.6806  -0.543   0.5871  
f124         -1.8047     0.9325  -1.935   0.0530 .
f125          1.2398     0.7430   1.669   0.0952 .
---
</code></pre>

<p><strong>EDIT</strong></p>

<p>I would also like to check how the log-odds differ amongst the different conditions. For that, I was gonna just run a regular logistic regression with the main effects of f1 and f2, and the interaction of the two, and conduct additional multiple comparisons dependent on the kind of result I get from the omnibus test.</p>
",2013-10-17 15:15:49.887
57716,14799.0,2,,57710.0,,,,CC BY-SA 3.0,"<p>Do a one-way anova on the response, with city as the grouping variable. The $F$ and $p$ it gives should be the same as the $F$ and $p$ from the regression of the response on the dummy-coded cities, and $SS_{between\, cities}/SS_{total}$ should equal the multiple $R^2$ from the regression. The multiple $R$ is the correlation of city with the response.</p>
",2013-10-17 15:51:14.580
57717,22800.0,1,,,,Analyzing reflected and transformed variables II,<data-transformation>,CC BY-SA 3.0,"<p>I've searched past posts and have not seen an answer to this specifically, perhaps because it is assumed to be known so it was not stated in previous related posts.</p>

<p>I have a very simple single variable model with a negatively skewed dependent variable Y.</p>

<p>The residuals from Y = b0 + b1X are non-normal, which, by my understanding will violate regression assumptions and impact my residuals and hence inferences but should not affect the properties of unbiassedness or consistency of my estimate 'b1'.  (correct me where I am wrong)</p>

<p>To make correct inferences it seems the practice is to reflect and log the dependent variable as such Y* = ln(1 + max(Y) -Y) and run the following regression:</p>

<p>Y* = b0 + b1X.</p>

<p>If I am correct so far then my primary question is do we interpret b from the model above the same as a traditional log lin model:</p>

<p>i.e. if ln(Y) = b0 + b1x </p>

<p>Then b1 is interpreted as the b1*100% change in Y due to a 1 unit change in x. or exp(b1) = the change in the 'geometric' mean of Y due to a 1 unit change in x. </p>

<p>If this is correct, does the same interpretation apply in the reflected and transformed case? </p>

<p>Thank you.</p>
",2013-10-17 16:11:49.973
57718,13549.0,1,57725.0,,,Modeling what should be a logistic regression but has no negative responses,<logistic><missing-data>,CC BY-SA 3.0,"<p>I have a data set of reported food-borne illnesses and we're trying to determine what environmental conditions during food cultivation led to high bacterial counts in the food, and thus caused the illnesses. Unfortunately, I only have data of foods that caused confirmed illnesses. I requested that we go back and ""randomly"" sample from food tags that did not cause a reported illness but am not allowed to do so for various reasons. Even that would have had problems (because just because an illness is not reported doesn't mean it didn't occur), but at least this would have given me some negative observations.</p>

<p>I was originally planning to model these data using a logistic regression but I am stuck at what to do now. Without negative observations, I can only really provide univariate descriptive statistics, right? I'm hoping that someone else has had this problem and perhaps there's some model I haven't heard of before that can handle this. Thank you. </p>
",2013-10-17 16:18:09.787
57719,22756.0,2,,57616.0,,,,CC BY-SA 3.0,"<p>That would be the probability of obtaining a false negative in 5 slides:</p>

<p>(0.80)^5 = 0.32768</p>

<p>Ahhh, so in order to decrease the probability of false negatives below 1% you can do:</p>

<pre><code>&gt; x &lt;- matrix(c(0), nrow=25)
&gt; for(i in 1:25) x[i] = (0.8)^i
&gt; x
             [,1]
 [1,] 0.800000000
 [2,] 0.640000000
 [3,] 0.512000000
 [4,] 0.409600000
 [5,] 0.327680000
 [6,] 0.262144000
 [7,] 0.209715200
 [8,] 0.167772160
 [9,] 0.134217728
 [10,] 0.107374182
 [11,] 0.085899346
 [12,] 0.068719477
 [13,] 0.054975581
 [14,] 0.043980465
 [15,] 0.035184372
 [16,] 0.028147498
 [17,] 0.022517998
 [18,] 0.018014399
 [19,] 0.014411519
 [20,] 0.011529215
 [21,] 0.009223372
 [22,] 0.007378698
 [23,] 0.005902958
 [24,] 0.004722366
 [25,] 0.003777893
</code></pre>

<p>And find that the false positive rate is less than 1% at i = 21.</p>

<p>Great! Thanks. I can't believe I didn't see that. I was trying all kinds of conditional probabilities and such for some reason. Keep it simple, stupid...</p>
",2013-10-17 16:36:36.770
57720,13549.0,2,,57712.0,,,,CC BY-SA 3.0,"<p>In addition to the regression line that Jeremy suggested, it would be helpful to your audience to have the p-value of the slope and the R-squared. I'm not sure <em>how</em> new you are to this, but you can the p-value of the slope is really part of a test of whether or not the slope = 0. If the p-value is statistically significant (p &lt; 0.05 usually) then you can be pretty confident that the slope is not zero and that there is likely to be a ""real"" relationship between the independent and dependent variables. The R-squared shows how much of the variation in y is explained by variation in x. For instance, I would suspect that the ADL relationship is probably real (significant p-value) but that the R-squared will be low-ish. This isn't bad, but simply means that there are probably other things affecting ADL sorghum silage aside from ADL fresh sorghum. </p>
",2013-10-17 16:40:07.893
57721,436.0,1,,,,Analysis of temporal patterns,<pattern-recognition>,CC BY-SA 3.0,"<p>I am analysing data on events that I have categorized into groups.</p>

<p>So, for instance, say I have 3000 events categorized into 5 groups, which we call A to E.</p>

<p>I will have something like</p>

<pre><code>    Event  | Group | Time
   --------+-------+-------
       1   |   A   |   0
       2   |   A   |   5
       3   |   C   |   7
       4   |   D   |   16
      ...  |       |
     3000  |   B   |   6000
</code></pre>

<p>Now, I would like to see whether there is some sort of n-event long temporal sequence appearing repeatedly (higher than chance).</p>

<p>So for instance one 4-event long pattern may be:<br>
<strong>A</strong> - <em>3 seconds</em> - <strong>A</strong> - <em>2 seconds</em> - <strong>D</strong> - <em>5 seconds</em> - <strong>C</strong> </p>

<p>I found <a href=""http://people.mbi.ohio-state.edu/cdiekman/DSU_JNM_2009.pdf"">this paper</a> which proposes some interesting method (I am not actually working on spike trains, but the issue is similar enough), but before implementing that I would like to see whether anyone knew of other methods/statistics that can be applied to this kind of problems.</p>
",2013-10-17 17:37:34.770
57726,22802.0,1,,,,Nonparametric test for trend using Python,<nonparametric><biostatistics><wilcoxon-mann-whitney-test><scipy><isotonic>,CC BY-SA 4.0,"<p>I am looking to perform a nonparametric test for trend on a continuous outcome across three groups, preferably in Python.  For example height (pretend height is not normal) in 4th, 5th and 6th graders.  </p>

<p>I would like to implement something like the <a href=""http://www.ncbi.nlm.nih.gov/pubmed/3992076"" rel=""nofollow noreferrer"">Cuzick method</a>.  Scipy has Wilcoxon rank sum  and other nonparametric methods but only for two groups. Similarly, Scipy has a Kruskal-Wallis method for three groups but it does not indicate direction or trends.  Does anything like this exist for exploring a directional trend across three groups?</p>

<hr>

<p>To clarify I am trying to determining whether there is a significant shift in a continuous trait measured across three groups.  The groups will be of very different size: group 1 has 1000's of samples and likely to be normally distributed, group 2 100's os samples, group 3 ~10 or less.  Group 1 serves as the ""control"" group, and my hypothesis is that the mean value of group 1 will be shifted in either direction relative to group 0, and group 2 will be shifted further <strong>in the same direction as group 1</strong>.  Because group 3 will always be very small compared to the other group, my instinct was to use nonparametric methods, but I am open to other suggestions. </p>

<p>Can anyone suggest a method to explore this type of directional trend?   </p>
",2013-10-17 18:51:01.530
57727,22341.0,1,,,,How to show that the mean is (weakly) consistent,<mean><consistency>,CC BY-SA 3.0,"<p>How can I show that the mean is weakly consistent? Is weakly consistent the same as consistent?</p>
",2013-10-17 19:26:42.837
57728,22805.0,2,,4705.0,,,,CC BY-SA 3.0,"<p>Joseph Hilbe (1944-), first president of the International Astrostatistics Association and author of over 10 books on statistical modeling, including popular texts on count models, logistic regression, generalized estimating equations (GEE), generalized linear models, and statistical methodology. Hilbe is an emeritus professor at the University of Hawaii and adjunct professor of statistics at Arizona State University.</p>
",2013-10-17 19:49:01.727
57729,14806.0,1,57730.0,,,"ANOVA failed to model data, what is a more appropriate test?",<r><anova><statistical-significance>,CC BY-SA 3.0,"<p>This is a cross-post (<a href=""https://stackoverflow.com/questions/19432964/anova-error-in-levelsxx"">https://stackoverflow.com/questions/19432964/anova-error-in-levelsxx</a>) about an error I received in R while trying to run an ANOVA on my data. But error aside, I need help understanding why an ANOVA can't deal with my data and what other statistical models could be applied instead.</p>

<p>So here's my objective: I have 3 people (speaker) who recorded a bunch of words that I analyzed. The analysis yielded 3 continuous variables: skewness, kurtosis and Center of Gravity (CoG)*. I need to find out what combinations of these 3 variables best model the difference between each speaker. For example, are skewness and CoG together more significant than just CoG in finding the difference between speakers?</p>

<p>I have a basic knowledge of stats, but erring on the side of assuming I'm an idiot might be better for any complex explanations.</p>

<p>Thanks in advance!</p>

<ul>
<li>The skewness is a measure for how much the shape of the spectrum below the center of gravity is different from the shape above the mean frequency.</li>
<li>The kurtosis is a measure for how much the shape of the spectrum around the center of gravity is different from a Gaussian shape.</li>
<li>The center of gravity is a measure for how high the frequencies in a spectrum are on average weighted by their energy.</li>
</ul>
",2013-10-17 19:53:55.743
57730,13037.0,2,,57729.0,,,,CC BY-SA 3.0,"<p>Sounds like you are trying to do Multinomial regression. Perhaps look up information on that. </p>

<p>Here is a great start:</p>

<p><a href=""http://www.ats.ucla.edu/stat/r/dae/mlogit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/mlogit.htm</a></p>

<p>e.g.</p>

<pre><code>install.packages('nnet')
library(nnet)

test&lt;-multinom(formula = as.factor(speaker) ~ CoG * skewness * kurtosis, data = total)


z &lt;- summary(test)$coefficients/summary(test)$standard.errors
# 2-tailed z test
p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2
</code></pre>
",2013-10-17 20:05:44.610
57731,22806.0,1,,,,Is there any way to model this grouping process?,<probability><modeling>,CC BY-SA 4.0,"<p>I've been working on this problem for three days now, and it doesn't seem that it can be solved using pure thinking. Maybe there is some distribution that can model this but I couldn't find any solution till now.</p>

<p>This is the process I'm trying to model: I have $K$ points randomly dispersed in an area of radius $R$ using 2D-poisson random variable. I start by putting the first point in a group. Then, the distance $d$ between the first and the second points is found. If this distance is larger than a certain threshold ($Dth$) then the second point will be put in the same group as the first point. Otherwise, the second point will be put in a group alone. The process continues until the last point $K$. </p>

<p>A point will be put in a group if it has its distance from all the points in that group larger than $Dth$.</p>

<p>I've derived $P(d&gt;Dth)$ which is the probability that two points have a distance between them larger than $Dth$, so let's denote it by $p$, and $P(d&lt;Dth)$ by $q$. </p>

<p>What I want to find is either the average number of groups resulting given $K$ points (don't care about the radius $R$ because it is incorporated in the given probabilities) or the average number of points per group given $K$. </p>
",2013-10-17 20:22:25.147
57732,14803.0,1,,,,distribution of sample variance or sample coefficient of variation under gamma parent distribution,<variance><sample><gamma-distribution><coefficient-of-variation>,CC BY-SA 3.0,"<p>I am looking for anything related to the distribution of the sample variance or sample coefficient of variation (or joint with sample mean or conditional on sample mean) when the parent distribution is Gamma. Any information of reference would be greatly appreciated.</p>
",2013-10-17 21:01:28.510
57733,22340.0,1,,,,Setting intercept to zero: Will this change both standard deviations and the error term?,<regression><multiple-regression><variance><standard-deviation><intercept>,CC BY-SA 3.0,"<p>After running a single regression with a forced zero intercept, I understand that $\beta$ (slope(s)) will change as $\alpha$ (intercept) will be set to zero. Easy.</p>

<p>$\rho = \beta(\sigma_x / \sigma_y)$ is left in question...</p>

<p>by way of....   $\beta =  \frac{\mathrm{Cov}(X,Y)}{\mathrm{Var}(X)}$.....or......${\mathrm{Cov}(X,Y)}=  \beta(\sigma_x^2)$</p>

<p>Given the forced change, will the $\sigma_x$ and $\sigma_y$ (std.devs) both remain as their non-forced original values, with $\rho$ (correlation) changing to match the new forced slope? What will happen to the error term given the forced zero regression?</p>

<p>Note: Calculations confirmed in Excel via using <code>StdevP</code> &amp; <code>VarP</code> functions in small sample group.</p>
",2013-10-17 21:59:44.060
57747,,2,,57740.0,mrip,,,CC BY-SA 3.0,"<p>What you've written is a one-sided test.  A two sided test would be</p>

<pre><code>pvalue&lt;-mean(abs(reps)&gt;=ts)
</code></pre>
",2013-10-18 03:35:40.610
57748,1895.0,2,,56784.0,,,,CC BY-SA 3.0,"<p><strong>The basic results of chi-square goodness-of-fit testing can be understood hierarchically</strong>.</p>

<p><strong>Level 0</strong>. The classical Pearson's chi-square test statistic for testing a multinomial sample against a fixed probability vector $p$ is
$$
X^2(p) = \sum_{i=1}^k \frac{(X^{(n)}_i - n p_i)^2}{n p_i} \stackrel{d}{\to} \chi_{k-1}^2 \&gt;,
$$
where $X_i^{(n)}$ denotes the number of outcomes in the $i$th cell out of a sample of size $n$. This can be fruitfully viewed as the squared norm of the vector $\mathbf Y_n =  (Y_1^{(n)},\ldots,Y_k^{(n)})$ where $Y_i^{(n)} = (X_i^{(n)} - n p_i)/\sqrt{n p_i}$ which, by the multivariate central limit theorem converges in distribution as
$$
\mathbf Y_n \stackrel{d}{\to} \mathcal N(0, \mathbf I - \sqrt{p}\sqrt{p}^T) \&gt;.
$$
From this we see that $X^2 = \|\mathbf Y_n\|^2 \to \chi^2_{k-1}$ since $\mathbf I - \sqrt{p}\sqrt{p}^T$ is idempotent of rank $k-1$.</p>

<p><strong>Level 1</strong>. At the next level of the hierarchy, we consider composite hypotheses with multinomial samples. Since the exact $p$ of interest is unknown under the null hypothesis, we have to estimate it. If the null hypothesis is composite and composed of a linear subspace of dimension $m$, then maximum likelihood estimates (or other efficient estimators) of the $p_i$ can be used as ""plug-in"" estimators. Then, the statistic
$$
X^2_1 = \sum_{i=1}^k \frac{(X^{(n)}_i - n \hat{p}_i)^2}{n \hat{p}_i} \stackrel{d}{\to} \chi_{k-m - 1}^2 \&gt;,
$$
under the null hypothesis.</p>

<p><strong>Level 2</strong>. Consider the case of goodness of fit testing of a parametric model where the cells are fixed and known in advance: For example, we have a sample from an exponential distribution with rate $\lambda$ and from this we produce a multinomial sample by binning over $k$ cells, then the above result still holds provided that we use efficient estimates (e.g., MLEs) <em>of the bin probabilities themselves using only the observed frequencies</em>.</p>

<p>If the number of parameters for the distribution is $m$ (e.g., $m = 1$ in the exponential case), then
$$
X^2_2 = \sum_{i=1}^k \frac{(X^{(n)}_i - n \hat{p}_i)^2}{n \hat{p}_i} \stackrel{d}{\to} \chi_{k-m - 1}^2 \&gt;,
$$
where here $\hat{p}_i$ can be taken to be the MLEs of the cell probabilities of the fixed, known cells corresponding to the given distribution of interest.</p>

<p><strong>Level 3</strong>. But, wait! If we have a sample $Z_1,\ldots,Z_n \sim F_\lambda$, why shouldn't we estimate $\lambda$ efficiently first, and then use a chi-square statistic with our fixed, known cells? Well, we can, but in general we no longer get a chi-square distribution for the corresponding chi-square statistic. In fact, Chernoff and Lehmann (1954) showed that using MLEs to estimate the parameters and then plugging them back in to get estimates of the cell probabilities results in a non-chi-square distribution, in general. Under suitable regularity conditions, the distribution is (stochastically) between a $\chi_{k-m-1}^2$ and a $\chi_{k-1}^2$ random variable, with the distribution depending on the parameters.</p>

<p>Untuitively, this means that the limiting distribution of $\mathbf Y_n$ is $\mathcal N(0, \mathbf I - \sqrt{p_\lambda}\sqrt{p_\lambda}^T - \mathbf A(\lambda))$.</p>

<p>We haven't even talked about <em>random</em> cell boundaries yet, and we're already in a bit of a tight spot! There are two ways out: One is to retreat back to Level 2, or at the very least <em>not</em> use efficient estimators (like MLEs) of the underlying parameters $\lambda$. The second approach is to try to undo the effects of $\mathbf A(\lambda)$ in such a way as to recover a chi-square distribution.</p>

<p>There are several ways of going the latter route. They basically amount to premultiplying $\mathbf Y_n$ by the ""right"" matrix $\mathbf B(\hat{\lambda})$. Then, the quadratic form 
$$
\mathbf Y_n^T \mathbf B^T \mathbf B \mathbf Y_n \stackrel{d}{\to} \chi_{k-1}^2 \&gt;,
$$
where $k$ is the number of cells.</p>

<p>Examples are the <em>Rao&ndash;Robson&ndash;Nikulin statistic</em> and the <em>Dzhaparidze&ndash;Nikulin statistic</em>.</p>

<p><strong>Level 4</strong>. Random cells. In the case of random cells, under certain regularity conditions, we end up in the same situation as in Level 3 if we take the route of modifying the Pearson chi-square statistic. Location-scale families, in particular, behave very nicely. One common approach is to take our $k$ cells each to have probability $1/k$, nominally. So, our random cells are intervals of the form $\hat{I}_j = \hat \mu + \hat\sigma I_{0,j}$ where $I_{0,j} = [F^{-1}((j-1)/k), F^{-1}(j/k))$. This result has been further extended to the case where the number of random cells grows with the sample size.</p>

<p><strong>References</strong></p>

<ol>
<li><p>A W. van der Vaart (1998), <em><a href=""http://rads.stackoverflow.com/amzn/click/0521784506"">Asymptotic Statistics</a></em>, Cambridge University Press. <strong>Chapter 17</strong>: <em>Chi-Square Tests</em>.</p></li>
<li><p>H. Chernoff and E. L. Lehmann (1954), <a href=""http://projecteuclid.org/euclid.aoms/1177728726"">The use of maximum likelihood estimates in $\chi^2$ tests for goodness of fit</a>, <em>Ann. Math. Statist.</em>, vol. 25, no. 3, 579&ndash;586.</p></li>
<li><p>F. C. Drost (1989), <a href=""http://projecteuclid.org/euclid.aos/1176347269"">Generalized chi-square goodness-of-fit tests for location-scale models when the number of classes tends to infinity</a>, <em>Ann. Stat</em>, vol. 17, no. 3, 1285&ndash;1300.</p></li>
<li><p>M. S. Nikulin, M.S. (1973), <a href=""http://epubs.siam.org/doi/pdf/10.1137/1118069"">Chi-square test for continuous distribution with
shift and scale parameters</a>, <em>Theory of Probability and its Application</em>, vol. 19, no. 3, 559&ndash;568.</p></li>
<li><p>K. O. Dzaparidze and M. S. Nikulin (1973), <a href=""http://dx.doi.org/10.1137/1119098"">On a modification of the standard statistics of Pearson</a>, <em>Theory of Probability and its Application</em>, vol. 19, no. 4, 851&ndash;853.</p></li>
<li><p>K. C. Rao and D. S. Robson (1974), <a href=""http://www.tandfonline.com/doi/abs/10.1080/03610927408827216"">A chi-square statistic for goodness of fit tests within exponential family</a>, <em>Comm. Statist.</em>, vol 3., no. 12, 1139&ndash;1153.</p></li>
<li><p>N. Balakrishnan, V. Voinov and M. S. Nikulin (2013), <em><a href=""http://rads.stackoverflow.com/amzn/click/0123971942"">Chi-Squared Goodness of Fit Tests With Applications</a></em>, Academic Press.</p></li>
</ol>
",2013-10-18 03:36:15.927
57734,22808.0,1,,,,Confidence interval for multiple regression parameter,<r><regression><confidence-interval><multiple-regression>,CC BY-SA 3.0,"<p>I'm given the least squares model:</p>

<pre><code>Y = B0 + B1x1 + B2x2 + B3x1x2
Y = 12 -2x1 + 7x2 +5x1x2

n = 20
</code></pre>

<p>as well as some RSS's </p>

<pre><code>&gt; sum( lm( y ~ 1 )$residuals^2 )                                  #$ (to fix display bug)
[1] 456 
&gt; sum( lm( y ~ x1 )$residuals^2 )                                 #$
[1] 320 
&gt; sum( lm( y ~ x2 )$residuals^2 )                                 #$
[1] 360 
&gt; sum( lm( y ~ x1 + x2 )$residuals^2 )                            #$
[1] 288 
&gt; sum( lm( y ~ x1 + x2 + I(x1*x2) )$residuals^2 )                 #$
[1] 240
</code></pre>

<p>So, I know the least squares estimate for <code>B3</code> is 5. </p>

<p>I did ANOVA on the full model versus the model where <code>B3 = 0</code>. I found the F statistic for <code>B3 = 0</code> to be 3.2. </p>

<p>Now I need to find a 95% confidence interval for B3. I'm not sure where to go from here. </p>
",2013-10-17 22:08:41.807
57735,594.0,2,,57695.0,,,,CC BY-SA 3.0,"<p>In the absence of a clearly identified aim I'll begin with some general comments in the hope that the purpose of your analysis becomes clarified. It would be nice to know if you're after an interval, hypothesis test or simply a standard error - but if the last, to what end?]</p>

<p>If the distribution of the population from which the sample is drawn is known, the distribution of the median may be computed. </p>

<p>The density of the $r$-th order statistic for a sample of size $n$ for a continuous random variable is</p>

<p>$$f_{Y_r}=\frac{n!}{(r-1)!(n-r)!}[F(x)]^{r-1}[1-F(x)]^{n-r}f(x)$$</p>

<p>For $n=3$,</p>

<p>$$f_{Y_2}= 6F(x)[1-F(x)]f(x)$$</p>

<p>For even $n$ it's more complex, but sometimes still doable.</p>

<p>If the sample size is 1, the distribution of the median is trivial - it's just the distribution of a single observation.</p>

<p>If the density is available, it should be possible to compute the standard deviation of the distribution of the order statistic.</p>

<p>(If you don't know the distribution, it's also possible to get an asymptotic standard error for the median, but it relies on knowing the height of the density at the median, which - while a much weaker requirement - would seem unlikely unless you knew the distribution.)</p>

<p>Additionally, you can generate nonparametric intervals for a median from the order statistics, but I don't think this gets at your present problem.</p>
",2013-10-17 22:19:18.220
57736,22807.0,2,,57676.0,,,,CC BY-SA 3.0,"<p>Is this the problem from the STA511 class?:)</p>

<p>pnorm() won't give you the right result, because it's a CDF. What you are looking is an inverse of the CDF, so you have to use qnorm() to get it.</p>
",2013-10-17 22:41:36.583
57737,20981.0,2,,57715.0,,,,CC BY-SA 3.0,"<p>Try running ANOVA on your model, e.g.</p>

<pre><code>anova(glm(...),test=""Chisq"")
</code></pre>

<p>The <code>drop1</code>, <code>add1</code> and <code>step</code> functions might also be useful.</p>

<p><a href=""http://data.princeton.edu/R/glms.html"" rel=""nofollow"">http://data.princeton.edu/R/glms.html</a></p>
",2013-10-17 23:40:36.663
57738,22815.0,1,,,,"ANOVA using R - unsure of if analysis is appropriate, and if variables need to be numeric",<r><anova>,CC BY-SA 3.0,"<p>I'm doing an ANOVA to compare the results of two trials. </p>

<p>Individuals had to walk towards a bucket blindfolded, with their deviation from the target recorded. I want to see if individuals get better with practice (are trial-2 deviations lower than trial-1) and is there a relationship between the trials regarding gender and handedness (left handed/right handed).</p>

<p><strong>Is an ANOVA appropriate?</strong> </p>

<p>Also, I'm having trouble in R because I've put gender as M and F, and Handedness as L or R. Do I need to make these numerical or not?</p>
",2013-10-18 00:37:48.237
57739,22816.0,1,,,,"Basic GARCH (1,1) question",<r><time-series><garch>,CC BY-SA 3.0,"<p>Background to question:</p>

<p>I was trying to fit a GARCH(1,1) model to the variance of log returns of a series, and ARMA(0,0) for the mean. I was using the <code>fGarch</code> package in R to do this. The aim of the modeling is to generate a predicted volatility number to feed into the Black-Scholes model to an generate option price and therefore option deltas. I plan to backtest the delta from GARCH volatility to hedge my option positions (as opposed to deltas derived from implied vol prices).</p>

<p>Questions:</p>

<p>A) I used the <code>predict</code> function in the package to generate a 'n-day ahead' volatility forecast. As I understand GARCH, these numbers are annualized standard deviation numbers. To hedge a 1 month option I want to forecast 30 day volatility. I can simply put 'n-days ahead = 30 to get the numbers, but <strong>how do I combine those 30 numbers to get a annualized vol number?</strong></p>

<p>B) Could anyone also please explain how to use the <code>nroll</code> argument in the package? Basically I want rolling GARCH estimates of volatility. For example, at day 10, I want to use the past 10 days of data to get a vol prediction for day 11, at day 50 I want to use 50 days of data for vol prediction of day 51 etc.</p>
",2013-10-18 01:18:25.060
57740,22844.0,1,,,user2892710,Two sided permutation test,<r><hypothesis-testing><permutation-test><sample>,CC BY-SA 3.0,"<p>I'm trying to write a two sided permutation test to test the alternative hypothesis there is a difference in the medians of 2 independent samples. My question is this: am I calculating the p-value correctly? Thanks SO much!</p>

<pre><code>sample1 &lt;- groundwater$West[!is.na(groundwater$West)]
sample2 &lt;- groundwater$East
ts &lt;- median(sample1) - median(sample2)
&gt; ts
[1] 0.105
R &lt;- 9999
all &lt;- c(sample1, sample2)
k &lt;- 1:length(all)
reps &lt;- numeric(R)
for (i in 1:R) {
    m &lt;- sample(k, size=length(sample1), replace=FALSE)
    permsample1 &lt;- all[m]
    permsample2 &lt;- all[-m]
    reps[i] &lt;- median(permsample1) - median(permsample2)
}
pvalue &lt;- sum(c(ts, reps) &lt;= ts)/9999
&gt; pvalue
[1] 0.9223922
</code></pre>
",2013-10-18 01:19:42.453
57741,11656.0,1,,,,Who is the best writer among statisticians?,<descriptive-statistics>,CC BY-SA 3.0,"<p>I'd like to learn their writing style.
Could you recommend a good writer statistician?</p>
",2013-10-18 01:50:55.030
57742,594.0,2,,56784.0,,,,CC BY-SA 3.0,"<p>I've found at least partial answers to my question, below. (I'd still like to give someone that bonus, so any further information appreciated.)</p>

<p>Moore (1971) said that Roy (1956) and Watson (1957,58,59) showed that when the cell boundaries 
for a chi-square statistic are functions of best asymptotic normal estimated parameter values, then under certain conditions, the asymptotic null distribution of the 
chi-square statistic is still that of a sum of a $\chi^2_{k-p-1}$ and a weighted 
sum of $p$ $\chi^2_1$ variables (for $k$ cells, $p$ parameters) 
where the weights are between 0 and 1 (making the cdf of the 
distribution between that of a $\chi^2_{k-p}$ and a $\chi^2_{k}$, as alluded to in my question for the distribution when using ML estimation), and the weights on those last $p$ terms are unaffected by that estimation.</p>

<p><strong>References</strong></p>

<p>Moore D.S. (1971), <a href=""http://projecteuclid.org/euclid.aoms/1177693502"" rel=""nofollow"">A Chi-Square Statistic with Random Cell 
Boundaries</a>, <em>Ann. Math. Stat.</em>, Vol 42, No 1, 147&ndash;156. </p>

<p>Roy A.R. (1956), <a href=""http://statistics.stanford.edu/~ckirby/techreports/ONR/CHE%20ONR%2001.pdf"" rel=""nofollow"">On $\chi^2$ statistics with variable intervals</a>, 
<em>Technical Report No. 1</em>, Dept of Statistics, Stanford University.</p>

<p>Watson, G.S. (1957), <a href=""http://biomet.oxfordjournals.org/content/44/3-4/336.abstract"" rel=""nofollow"">The $\chi^2$ goodness-of-fit test for normal 
distributions</a>, <em>Biometrika</em>, <strong>44</strong>, 336&ndash;348.</p>

<p>Watson, G.S. (1958), <a href=""http://www.jstor.org/stable/2983906"" rel=""nofollow"">On $\chi^2$ goodness-of-fit tests for 
continuous distributions</a>, <em>J. Royal Statist. Soc. B</em>, <strong>20</strong>, 44&ndash;61.</p>

<p>Watson, G.S. (1959), <a href=""http://www.jstor.org/stable/2527749"" rel=""nofollow"">Some recent results in $\chi^2$ goodness-of-
fit tests</a>, <em>Biometrics</em>, <strong>15</strong>, 440-468</p>
",2013-10-18 01:51:22.243
57749,594.0,2,,57746.0,,,,CC BY-SA 3.0,"<p>Here's some major hints to get you started:</p>

<p>For the first one, you might want to consider what $f'$ looks like (as a formula) between the sorted data values (it's discontinuous at the data values). How does it behave as you move through the data? Where does it change from below 0 to above 0?</p>

<p>Here's $f$ vs $t$ (the grey vertical bars on top of the x-axis are the data values):</p>

<p><img src=""https://i.stack.imgur.com/nHXEd.png"" alt=""enter image description here""></p>

<p>Can you see how to show what will minimize $f$ now?</p>

<hr>

<p>For the second case, consider $f_k(t) = ( \sum_{i=1}^{n} |y_i-t|^{k} )^{1/k}$ (you would need to argue that $f_k$ and $f$ share an argmin for a given $k$ - that is taking the $1/k$ power doesn't change the location of the minimum. Below (for the same data as above) $f_k$ is plotted for $k = 2,3,5,9,$ and $99$ (the ""$k=2$"" case is faint dotted grey at the top, the ""$k=99$"" case is purple near the bottom):</p>

<p><img src=""https://i.stack.imgur.com/80diF.png"" alt=""enter image description here""></p>

<p>The ""$k=2$"" case corresponds to least squares, while $k=1$ would be the previous example. As $k$ increases, it's getting 'pointier'. Can you figure where the 'point' is headed, and why? Can you work out $f_k'$? What happens to that as $k$ increases?</p>
",2013-10-18 04:47:18.360
57750,22678.0,1,,,,"Lasso ||a|| and ""General Lasso"" ||Da||",<lasso><algorithms><regularization>,CC BY-SA 4.0,"<p>Ryan Tibshirani introduced once a more general type of Lasso, where the regularizer is
$$\parallel D \alpha \parallel_1$$
instead of $\parallel \alpha \parallel_1$.
<a href=""http://www.stat.cmu.edu/~ryantibs/papers/genlasso.pdf"" rel=""nofollow noreferrer"">See paper</a></p>

<p>However, there is nearly no discussion about this form and I wonder why since its a great way to deal with derivative smoothness regularizers.</p>

<ul>
<li><p>Is there an easy way I overlooked  to transform a general Lasso to the standard Lasso form? </p></li>
<li><p>Which algorithm can be used for the gen. lasso? Currently I only tested quadratic programs, but this is quite slow.   </p></li>
</ul>
",2013-10-18 07:28:18.987
57751,5671.0,2,,57685.0,,,,CC BY-SA 3.0,"<p>You may want to look at <strong>Multidimensional Scaling</strong>. This will try to find the 2d projection that best preserves your distances. For obvious reasons, There is no lossless 4d to 2d projection.</p>
",2013-10-18 07:46:48.313
57752,22601.0,1,60010.0,,,Find k of n items with least pairwise correlations,<correlation><ranking>,CC BY-SA 3.0,"<p>I have a matrix of pairwise correlations between n items. Now I want to find a subset of k items with the least correlation. Thus there are two questions:</p>

<ol>
<li>Which is the appropriate measure for the correlation within that group?</li>
<li>How to find the group with the least correlation?</li>
</ol>

<p>This problem appears like a kind of inverse factor analysis to me and I'm pretty sure that there is a straight-forward solution. </p>

<p>I think this problem actually equals the problem to remove (n-k) nodes from a complete graph so the remaining nodes are connected with minimum edge weights. What do you think?</p>

<p>Thanks for your suggestions in advance!</p>
",2013-10-18 07:50:14.530
57753,16474.0,2,,57715.0,,,,CC BY-SA 3.0,"<p>One thing you can do is to exclude the constant and main effects from your model and don't leave out the reference categories. That way the coefficients will be the (adjusted) log odds, and the test commonly reported next to the coefficients will be the test you are looking for. I wrote a brief discussion on that trick for Stata <a href=""http://www.maartenbuis.nl/publications/ref_cat.html"" rel=""nofollow"">here</a>. I don't know enough about R to tell you which commands to type, but I am certain one can also do it R.</p>
",2013-10-18 07:52:11.500
57754,16474.0,2,,57707.0,,,,CC BY-SA 3.0,"<p>One trick that often helps for logistic regression type problems is to realize that:</p>

<p>$1 - h(x^{(i)}) = h(-x^{(i)})$ </p>

<p>and that $h(-x^{(i)})$ is more numerically stable than $1 - h(x^{(i)})$. </p>

<p>You can find a discussion of that <a href=""http://www.stata-journal.com/article.html?article=pr0025"" rel=""nofollow"">here</a>. This is an article in the Stata Journal so the examples are in Stata/Mata, but the problem has to do with the way computers store numbers and is thus more general. For example, I have been able to reproduce the first anomalous example <em>exactly</em> in R, i.e. not just the general pattern but the exact values.</p>
",2013-10-18 08:06:21.207
57755,23201.0,1,58862.0,,Chris K,"Is there a better name than ""average of the integral""?",<terminology>,CC BY-SA 3.0,"<p>I'm testing throttle position sensors (TPS) my business sells and I print the plot of voltage response to the throttle shaft's rotation. A TPS is a rotational sensor with $\approx$ 90Â° of range and the output is like a potentiometer with full open being 5V (or sensor's input value) and initial opening being some value between 0 and 0.5V. I built a <a href=""http://forums.adafruit.com/viewtopic.php?f=22&amp;t=44710"" rel=""nofollow noreferrer"">test bench with a PIC32 controller</a> to take a voltage measurement every 0.75Â° and the black line connects these measurements. </p>

<p>One of my products has a tendency to make localized, low amplitude variations away from (and under) the ideal line. <strong>This question is about my algorithm for quantifying these localized ""dips""; what is a good name or description</strong> for the process of measuring the dips? (full explanation follows) In the below picture, the dip occurs at the left third of the plot and is a marginal case whether I would pass or fail this part:</p>

<p><img src=""https://i.stack.imgur.com/gfx2w.jpg"" alt=""Print out of a suspect part""></p>

<p>So I built a dip detector (<a href=""https://stackoverflow.com/questions/19269638"">stackoverflow qa about the algorithm</a>) to quantify my gut feeling. I initially thought I was measuring ""area"". This graph is based on the printout above and my attempt to explain the algorithm graphically. There is a dip lasting for 13 samples between 17 and 31:</p>

<p><img src=""https://i.stack.imgur.com/W5oXL.png"" alt=""Sampled data shown with the &quot;dip&quot; magnified""></p>

<p>Test data goes in an array and I make another array for ""rise"" from one data point to the next, which I call $deltas$. I use a library to get the average and standard deviation for $deltas$. </p>

<p>Analyzing the $deltas$ array is represented in the graph below, where the slope is removed from the above graph. Originally, I thought of this as ""normalizing"" or ""unitizing"" the data as the x axis are equal steps and I'm now solely working with the rise between data points. When researching this question, I recalled this is the derivative, $\frac {dy}{dx}$ of the original data.</p>

<p><img src=""https://i.stack.imgur.com/FjzSS.png"" alt=""Analysis of the derivative...?""></p>

<p>I walk through $deltas$ to find sequences where there are 5 or more adjacent negative values. The blue bars are a series of data points who are below the average of all $deltas$. The values of the blue bars are:</p>

<p>$0.7 + 1.2 + 1.3 + 1.4 + 1.8 + 2.5 + 2.9 + 3.0 + 2.5 + 2.0 + 1.5 + 1.0 + 1.2$</p>

<p>They sum to $23$, which represents the area (or the integral). My first thought is ""I just integrated the derivative"" which should mean I get back the original data, though I'm certain there's a term for this.</p>

<p>The green line is the average of these ""below average values"" found via dividing the area by the length of the dip:  </p>

<p>$23 \div 13 = 1.77$</p>

<p>During the testing of 100+ parts, I came to decide that dips with my green line average less than $2.6$ are acceptable. Standard deviation calculated across the entire data set wasn't a strict enough test for these dips, as without enough total area, they still fell within the limit I established for good parts. I observationally chose standard deviation of $3.0$ to be the highest I would allow.</p>

<p>Setting a cutoff for standard deviation strict enough to fail this part would then be so strict as to fail parts which otherwise appear to have a great plot. I do also have a spike detector which fails the part if any $|deltas - avg| &gt; avg+std dev$. </p>

<p>It's been almost 20 years since Calc 1, so please go easy on me, but this <em>feels</em> a lot like when a professor used calculus and the displacement equation to explain how in racing, a competitor with less acceleration who maintains higher corner speed can beat another competitor having greater acceleration to the next turn: going through the previous turn faster, the higher initial speed means the area under his velocity (displacement) is greater.</p>

<p>To translate that to my question, I feel like my green line would be like acceleration, the 2nd derivative of the original data. </p>

<p>I visited wikipedia to re-read the fundamentals of calculus and the definitions of derivative and <a href=""http://en.wikipedia.org/wiki/Integral"" rel=""nofollow noreferrer"">integral</a>, learned the proper term for adding up the area under a curve via discreet measurements as <a href=""http://en.wikipedia.org/wiki/Numerical_integration"" rel=""nofollow noreferrer"">Numerical Integration</a>. Much more googling on <em>average of the integral</em> and I'm lead to the topic of nonlinearity and digital signal processing. <a href=""https://www.google.com/search?q=%22average%20of%20the%20integral%22%20quantify%20deviation&amp;num=20&amp;filter=0&amp;biw=1447&amp;bih=792"" rel=""nofollow noreferrer"">Averaging the integral seems to be a popular metric for quantifying data</a>.</p>

<p><strong>Is there a term for the Average of the Integral? ($1.77$, the green line)?</strong> <br>
... or for the process of using it to evaluate data?</p>
",2013-10-18 08:27:04.943
57756,22822.0,1,,,,Panel study is a quasi-experimental study? Quasi-experimental is the same as correlational?,<panel-data><observational-study>,CC BY-SA 3.0,"<p>I'm working with panel data. This panel dataset includes data from pupils of two kinds of schools: </p>

<ul>
<li><p>State schools (G1) where pupils change to secondary school when they are 12 years old.</p></li>
<li><p>Private schools (G2) where pupils remain in the same school. They can follow studying in the same school when they are 12 years old. </p></li>
</ul>

<p>I want to compare the change in a dependent variable between these two groups of pupils,  controlling for some other independent variables. </p>

<p>So, is this a quasi-experimental design? Maybe G1 can be seen as a treatment group and G2 as a control although there isn't a randomized assignment? Or is this an observational study?
And finally, what exactly is a correlational study? Is it quasi-experimental or observational?</p>
",2013-10-18 09:14:44.847
57757,22823.0,1,57844.0,,,Distance or Similarity metric for 2D frequency data maps,<distributions><distance-functions><information-theory><image-processing>,CC BY-SA 3.0,"<p>I want to compare the distance/similarity of 2D flood frequency data maps. The maps are square with YxY grid size and in each cell of the map is stored its flood frequency. For example in a 5x5 grid we may have this two flood frequency maps of the same area for the past 10 years, where we observe how many times the corresponding cell/place flooded:</p>

<p>0 0 0 0 0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 0 0 0 0<br> 
0 1 2 1 0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 2 3 1 0<br>
0 4 6 2 0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9 9 8 7 6<br>
0 1 2 1 0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 2 3 1 0<br>
0 4 6 2 0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9 9 8 7 6</p>

<p>I can easily transform these maps into a probability map that will add up to one. So the question now is what is the most meaningful way of comparing these kind of maps with each other to find their (dis)similarity. A distance metric taken from information theory field like JSD or L1 (and many others) or a similarity metric taken from the image processing field like the area under ROC (and many others)?</p>
",2013-10-18 09:16:24.410
57758,22824.0,1,57760.0,,,Question about $\epsilon' \epsilon$ in the linear regression model,<regression>,CC BY-SA 3.0,"<p>While studying the standard multivariate linear regression model, I came across the following:</p>

<p><img src=""https://i.stack.imgur.com/rQzKp.jpg"" alt=""enter image description here""></p>

<p>Could anyone please explain me why the last equality holds, and, why $Z(Z'Z)^{-1}Z'$ cannot simply be simplified to $I$ (isn't it true that $Z(Z'Z)^{-1}Z'=ZZ^{-1}Z'^{-1}Z'=II=I$?</p>
",2013-10-18 09:21:02.257
57759,14525.0,1,,,,Combining similarity scores,<statistical-significance><similarities><winsorizing><trimmed-mean>,CC BY-SA 3.0,"<p>I have a list of m x n similarity score matrix, something like </p>

<pre><code>           c1         c2         c3         c4         c5  
      d1  0.2159824  0.3528572  0.2390016  0.3673485  0.2849448
      d2  0.2849448  0.2669695  0.2441495  0.3829949  0.3511353
      d3  0.3281100  0.3251407  0.4328260  0.2895179  0.2814589  
</code></pre>

<p>these ""similarity scores"" lie in between 0-1. What I am trying to do here is to combine these scores into a single score, also in between 0-1.</p>

<p>My issue here is that I am not able to figure out a good approach to combine these scores into this single score. So far I have tried taking the average, max. value, calculating row, column averages and using the max.value out of them. The problem with these scores is that the matrices I have vary a lot in row and column lengths, and I cannot account for this variation using average because at the end of the day I have to sort these matrices based on this similarity score and select n top ranking ones, and from manually checking these observations, I realized that max.value in a matrix is not a suitable single score the similarity between these observations. Do you have any suggestions for an approach to combine these scores ? </p>

<p>Also is there any statistical tests that could be applied on this combined score ? I have tried random sampling approach, but the steps to calculate similarity scores for the observations take a long time to run and iterating 
these steps ~1,000 times or more is not feasible now.   </p>
",2013-10-18 09:23:40.167
57760,21638.0,2,,57758.0,,,,CC BY-SA 3.0,"<p>Your first question has been answered by Glen_b in his comment. Regarding your second question, $\matrix{Z}$ is not (generally) square, hence it does not have an inverse. $\matrix{Z}'\matrix{Z}$ on the other hand is square and can be inverted.</p>
",2013-10-18 09:47:38.870
57761,503.0,2,,57759.0,,,,CC BY-SA 3.0,"<p>You actually have two problems, not one.</p>

<p>The first problem seems to be to average the scores in a matrix. Here the mean, median, trimmed mean and winsorized mean all seem potentially sensible.</p>

<p>The other is to somehow ""account"" for the size of the matrix. Here the total size (rxc) seems to be the obvious solution. </p>

<p>If you need more than this, please clarify your question again, but I see no reason why the average (or any of the variations I listed) are poor choices simply because the matrices are different sizes. </p>
",2013-10-18 10:19:26.323
57762,2081.0,2,,57752.0,,,,CC BY-SA 3.0,"<p>[Forewarning: this answer appeared before the OP decided to reformulate the question, so it may have lost relevance. Originally the question was about <code>How to rank items according to their pairwise correlations</code>]</p>

<p>Because matrix of pairwise correlations isn't a unidimensional array it is not quite clear what ""ranking"" may look like. Especially as long as you haven't worked out your idea in detail, as it seems. But you mentioned PCA as suitable for you, and that immediately made me to think of <a href=""http://en.wikipedia.org/wiki/Cholesky_decomposition"" rel=""nofollow"">Cholesky root</a> as potentially even more suitable alternative.</p>

<p><strong>Cholesky root</strong> is like a matrix of loadings left by PCA, only it is triangular. I'll explain both with an example.</p>

<pre><code>R, correlation matrix
         V1       V2       V3       V4
V1   1.0000   -.5255   -.1487   -.2790
V2   -.5255   1.0000    .2134    .2624
V3   -.1487    .2134   1.0000    .1254
V4   -.2790    .2624    .1254   1.0000

A, PCA full loading matrix
          I       II      III       IV
V1   -.7933    .2385    .2944    .4767
V2    .8071   -.0971   -.3198    .4867
V3    .4413    .8918    .0721   -.0683
V4    .5916   -.2130    .7771    .0261

B, Cholesky root matrix
          I       II      III       IV
V1   1.0000    .0000    .0000    .0000
V2   -.5255    .8508    .0000    .0000
V3   -.1487    .1589    .9760    .0000
V4   -.2790    .1361    .0638    .9485

A*A' or B*B': both restore R
         V1       V2       V3       V4
V1   1.0000   -.5255   -.1487   -.2790
V2   -.5255   1.0000    .2134    .2624
V3   -.1487    .2134   1.0000    .1254
V4   -.2790    .2624    .1254   1.0000
</code></pre>

<p>PCA's loading matrix A is the matrix of correlations between the variables and the principal components. We may say it because row sums of squares are all 1 (the diagonal of R) while matrix sum of squares is the overall variance (trace of R). Cholesky root's elements of B are correlations too, because that matrix also has these two properties. Columns of B are not principal components of A, although they are ""components"", in a sense.</p>

<p>Both A and B can restore R and thus both can replace R, as its representation. B is triangular which clearly shows the fact that it captures the pairwise correlations of R sequentially, or hierarhically. Cholesky's component <code>I</code> correlates with all the variables and is the linear image of the first of them <code>V1</code>. Component <code>II</code> no more shares with <code>V1</code> but correlates with the last three... Finally <code>IV</code> is correlated only with the last, <code>V4</code>. I thought such sort of ""ranking"" is <em>perhaps what you seek for</em>?.</p>

<p>The problem with Cholesky decomposition, though, is that - unlike PCA - it depends on the order of items in the matrix R. Well, you might sort the items is descending or ascending order of the sum of squared elements (or, if you like, sum of absolute elements, or in order of multiple correlarion coefficient - see about it below). This order reflects the how much an item is gross correlated.</p>

<pre><code>R, rearranged
         V2       V1       V4       V3 
V2   1.0000   -.5255    .2624    .2134 
V1   -.5255   1.0000   -.2790   -.1487 
V4    .2624   -.2790   1.0000    .1254 
V3    .2134   -.1487    .1254   1.0000 

Column sum of squares (descending)
     1.3906   1.3761   1.1624   1.0833 

B 
          I       II      III       IV 
V2   1.0000    .0000    .0000    .0000 
V1   -.5255    .8508    .0000    .0000 
V4    .2624   -.1658    .9506    .0000 
V3    .2134   -.0430    .0655    .9738
</code></pre>

<p>From last B matrix we see that <code>V2</code>, most grossly correlated item, pawns all its correlations in <code>I</code>. Next grossly correlated item <code>V1</code> pawns all its correlatedness, except that with <code>V2</code>, in <code>II</code>; and so on.</p>

<hr>

<p>Another decision could be computing <strong>Multiple correlation coefficient</strong> for every item and ranking based on its magnitude. Multiple correlation between an item and all the other items grows as the item correlates more with all of them but them correlate less with each other. The squared multiple correlation coefficients form the diagonal of the so called <em>image covariance matrix</em> which is $\bf S R^{-1} S - 2S + R$, where $\bf S$ is the diagonal matrix of the reciprocals of the diagonals of $\bf R^{-1}$.</p>
",2013-10-18 10:21:33.683
57763,503.0,2,,57756.0,,,,CC BY-SA 3.0,"<p>Personally, I don't much care for the term ""quasi-experimental"" but it is used a lot. The <a href=""http://en.wikipedia.org/wiki/Quasi-experiment"" rel=""nofollow"">Wikipedia entry for quasi-experiment</a> seems to be good. Another way to think about it is that in a true experiment there is random selection and random assignment, but in an observational study there is neither. In a quasi-experiment there is one or the other but not both.</p>

<p>In your particular case, you seem to have no control over either selection or assignment, so I would call it an observational study.</p>

<p>As for ""correlational"" I've seen this used by many of my doctoral student clients. I think its frequent use comes from some book that seems to get recommended a lot. If terminology is sane, ""correlational"" should just mean ""involving correlations"", but I've seen it used for studies that involved only regressions. This terminological confusion is borne out by a Google search, which yields mostly results on sites such as ""about.com"". I'd avoid use of the term, myself; clearly correlations could be used in experimental designs, observational designs or pretty much any design you could come up with. </p>
",2013-10-18 10:28:30.710
57764,22827.0,1,67832.0,,,Why does Naive Bayes outperform Support Vector Machines?,<classification><svm><data-mining><text-mining><naive-bayes>,CC BY-SA 3.0,"<p>I have a dataset composed of about 36000 attributes and 550 samples, the dataset is generated from text communication between people in some chatrooms.</p>

<p>The questions is when I try to classify these samples, a Naive Bayes classifier always outperforms a support vector machine, both in speed and accuracy. But in literature it is always noted that SVM is better in text-mining classification tasks.</p>

<p>Can anyone please explain in which situations Naive Bayes is better and in which situations SVM? </p>

<p>For more information about the question:</p>

<p>I am using the RapidMiner tool, I'm using 10 fold cross validation with stratified sampling. for Naive Bayes, the Laplacian correction is applied and for SVM I use a dot kernel and other parameters are all in their defaults, but when I change the parameters and try again, I get same result; Naive Bayes still outperforms SVM. </p>
",2013-10-18 10:39:49.607
57765,22830.0,1,,,,Pearson correlation,<correlation><sample-size><missing-data>,CC BY-SA 3.0,"<p>Should the sample size n be equal when we are looking for simple correlation? I mean is it OK if variable 1 has a little more or fewer observations than variable 2? I am computing correlation between two variables...the <em>n</em> of one is a little higher than the n of other!</p>

<p>I am looking for correlation between 2 scales (psy tests). The n of one is a little higher than the n of another. I mean not ALL the respondents who filled up one form (scale) have filled up the other. There are some (very few though) missing.</p>
",2013-10-18 11:05:21.687
57766,22831.0,1,,,,How can I calculate utilities for attribute levels in conjoint analysis in R?,<r><multinomial-distribution><logit><conjoint-analysis>,CC BY-SA 3.0,"<p>I conducted a stated preference survey in which each respondent had to choose 1 set out of 3 choice sets (A, B and C), which are characterised by 4 attributes (let's say brand, color, size and keyboard yes/no) of either 2 or 3 levels.  </p>

<p>I tried to calculate the utilities/coefficients with the estimation of multinomial logit model using the mlogit package in R. It's working well to calculate the utilities for the attributes (I followed exactly the script of Yves Croissant in his paper on the mlogit package.)</p>

<p>But I want to calculate the coefficients not for the attributes, but for the levels. I want to know what is the utility e.g. for the color ""blue"".
How can I calculate this? Is it possible at all to calculate it with the mlogit package or R?</p>

<p>I am grateful for every advice!</p>
",2013-10-18 11:39:06.263
57780,10147.0,1,,,,Treatment of Ordinal Predicting Variable,<regression><generalized-linear-model><ordinal-data>,CC BY-SA 3.0,"<p>I am trying to perform an regression analysis where the response variable is ordinal and the 15 out of the 16 predicting variables are also ordinal. Besides treating all these ordinal predictors as factors, is there any other option? What is the best way to treat these ordinal predictors? Thank you.</p>
",2013-10-18 15:24:35.867
57781,,1,,,user31656,Structure of data and function call for recurrent event data with time-dependent variables,<r><survival><cox-model>,CC BY-SA 3.0,"<p>I'm attempting to estimate the effect of 2 drugs (<code>drug1</code>, <code>drug2</code>) on the likelihood of a patient falling (<code>event</code>).  The patients can fall more than once and can be put on or taken off of the the drugs at any point.  </p>

<p>My question is how the data should be structured with regard to the time period (days), specifically whether there needs to be overlap between the days.  There are two reasons why I think my structure is wrong, the first being a seemingly incorrect <code>N</code>.  I am also getting some errors where the time period is a single day (i.e. <code>time1=4</code>, <code>time2=4</code>) and am unsure how these should be coded.  Should the start time of subsequent entries be the stop time of the previous entry?  I've tried it both ways (with and without overlap), and while having overlap gets rid of the warning, the <code>N</code> is still incorrect. </p>

<pre><code>Warning message:
In Surv(time = c(0, 2, 7, 15, 20, 0, 18, 27, 32, 35, 39, 46, 53,  :
  Stop time must be &gt; start time, NA created
</code></pre>

<p>Right now I have the data set up where the beginning of the next entry is the next day.  Unique patients are identified by their <code>chart numbers</code>.  </p>

<pre><code>Time1    Time2    Drug1    Drug2   Event    ChartNo
    0        2        1        0       0        123
    3       10        1        1       1        123
   11       14        1        1       1        123
    0       11        0        1       0        345
    0       19        1        0       1        678
    0        4        0        1       0        900
    5       18        1        1       0        900
</code></pre>

<p>Patient 123 was on drug1 at the start to day 2, after which point they had drug2 added.  They went from day 3 to day 10 on both drugs before falling the first time, then fell a second time on day 14 while still on both drugs.  Patient 345 went 11 days on drug2 without falling (then was censored), etc.</p>

<p>The actual estimation looks like this:</p>

<pre><code>S &lt;- Srv(time=time1, time2=time2, event=event)
cox.rms &lt;- cph(S ~ Drug1 + Drug2 + cluster(ChartNo), surv=T)
</code></pre>

<p>My main concern is that the <code>n</code> for my analysis is reported to be <code>2017</code> (the number of rows in the data), when in actuality I only have <code>314</code> unique patients.  I am unsure if this is normal or the result of some error I've made along the way.</p>

<pre><code>&gt; cox.rms$n
Status
No Event    Event 
    1884      133 
</code></pre>

<p>The same is true when using <code>coxph()</code> from the survival package.</p>

<pre><code> n= 2017, number of events= 133
</code></pre>

<p>The number of events is correct however.  </p>

<p><a href=""https://stats.stackexchange.com/questions/58079/extended-cox-model-with-continuous-time-dependent-covariate-how-to-structure-d"">This Post</a> seems to have it set up with the 'overlap' I described, but I am unsure about the <code>N</code>, and they don't seem to be clustering by <code>ID</code>.  </p>
",2013-10-18 15:25:31.520
57936,22906.0,1,57961.0,,,Generating survival times for a piecewise constant hazard model with two change points,<r><distributions><survival><random-generation>,CC BY-SA 3.0,"<p>When there are two change points in  a piecewise constant hazard model then the density function becomes some triangle exponential distribution. In this situation I can't generate the survival time from the CDF using probability integral transformation. Can any one help me to generate the survival time from this model?</p>
",2013-10-21 14:57:14.690
57767,8629.0,1,,,,Using anomalies to calculate trends of seasonal data,<time-series><seasonality>,CC BY-SA 3.0,"<p>I commonly see people doing trend analysis of (monthly) timeseries data which show a strong inter-annual cycle following this scheme:</p>

<ol>
<li>compute climatological means (""mean January"", ""mean February"", ..., ""mean December"")</li>
<li>subtract climatological means from actual data, to yield an ""anomaly timeseries""</li>
<li>perform linear regression on this ""anomaly timeseries""</li>
</ol>

<p><em>Climatological</em> in this case means <em>multi-year average of individual months</em>, e.g., an average of the 10 Januaries from 2000 to 2009. As Nick Cox points out in his comment, <em>anomaly</em> just means deviation from a reference level; there is no implication of anything pathological or very unusual.</p>

<p>While user31264's answer makes sense for processes where the seasonal component is truly purely additive. However, in atmospheric science we often have processes where the amplitude of the seasonal variation depends on the base level, i.e., is somewhat multiplicative.</p>

<p>Even in these scenarios, people often use the approach I outlined above. However, I could nowhere find a rigorous statistical explanation of why this approach is actually valid. Why is the linear regression of these anomalies a reasonable solution to the regression of the original timeseries data?
Can you give me any justification why this could be reasonable to do? Those people I asked mostly say ""everyone's doing it"" ...</p>
",2013-10-18 12:07:29.190
57768,2765.0,2,,52449.0,,,,CC BY-SA 3.0,"<p>First, the appropriate definition of ""effective sample size"" is IMO linked to a quite specific question. If $X_1, X_2, \ldots$ are identically distributed with mean $\mu$ and variance 1 the empirical mean
$$\hat{\mu} = \frac{1}{n} \sum_{k=1}^n X_k$$
is an unbiased estimator of $\mu$. But what about its variance? For <em>independent</em> variables the variance is $n^{-1}$. For a weakly stationary time series, the variance of $\hat{\mu}$ is 
$$\frac{1}{n^2} \sum_{k, l=1}^n \text{cov}(X_k, X_l) = \frac{1}{n}\left(1 + 2\left(\frac{n-1}{n} \rho_1 + \frac{n-2}{n} \rho_2 + \ldots + \frac{1}{n} \rho_{n-1}\right) \right) \simeq \frac{\tau_a}{n}.$$
The approximation is valid for large enough $n$. If we define $n_{\text{eff}} = n/\tau_a$, the variance of the empirical mean for a weakly stationary time series is approximately $n_{\text{eff}}^{-1}$, which is the same variance as if we had $n_{\text{eff}}$ independent samples. Thus $n_{\text{eff}} = n/\tau_a$ is an appropriate definition if we ask for the variance of the empirical average. It might be inappropriate for other purposes. </p>

<p>With a negative correlation between observations it is certainly possible that the variance can become smaller than $n^{-1}$ ($n_{\text{eff}} &gt; n$). This is a well known variance reduction technique in Monto Carlo integration: If we introduce negative correlation between the variables instead of correlation 0, we can reduce the variance without increasing the sample size.</p>
",2013-10-18 12:23:37.040
57769,22507.0,2,,57767.0,,,,CC BY-SA 3.0,"<p>The underlying model is considered to be $T(t)=f(t)+g(t)+h(t)$, where $T$ is temperature, $t$ is time, $f(t)$ is a function without seasonality, $g(t)$ is a periodic function, $h(t)$ is non-autocorrelative noise. The underlying assumption is that the movement of Earth around the orbit, which is periodical and independent on anything else, determines $g(t)$.  There is also a small correction due to increase of concentration of greenhouse gases (and other things which are not seasonal in nature), which is .  Note that the concentration of greenhouse gases showls little seasonality (unlike its derivative).  In order to estimate $g(t)$, we calculate the mean temperature for each month.</p>

<p>In order to obtain the trends, the autoregression of $f(t)+h(t)$ is better than the autoregression of the whole $T(t)$, because $g(t)$, being periodic, has its own strong autocorrelation.</p>
",2013-10-18 12:47:46.180
57770,22833.0,1,57776.0,,,Clarification of log interpretation,<data-transformation><interpretation><logarithm>,CC BY-SA 3.0,"<p>Let us say we have this regression </p>

<p>$$\ln(y) = a + B_1(age) + B_2\ln(savings) + B_3\ln(income+1)$$</p>

<p>When carrying out the regression we obtain:</p>

<p>$$\ln(y) = 0.3445 + 0.5(age) + 0.4556 x_1 + 0.55566 x_2$$</p>

<p>How would one interpret the coefficients in each case? Of particular concern is the income coefficient.  An increase of income of 1% would lead to an increase of how much in $y$?</p>

<p>This is a hypothetical example to illustrate the problem I have.</p>
",2013-10-18 13:23:28.197
57771,21762.0,2,,4705.0,,,,CC BY-SA 3.0,"<p>Abraham Wald (1902-1950) for introducing the concept of Wald-tests and for his fundamental work on statistical decision theory.</p>
",2013-10-18 13:48:24.960
57772,21476.0,1,,,,Distribution of the sum of two independent Beta-Binomial variables,<density-function><beta-binomial-distribution>,CC BY-SA 3.0,"<p>Consider two independent discrete random variables $y_1$ and $y_2$, both distributed with a Beta-Binomial distribution, with different number of successes $n_1$ and $n_2$ but the same parameters $a$ and $b$</p>

<p>$ p(y_1|n_1,a,b) = {n_1 \choose y_1} \dfrac{B(y_1 + a,n_1 -y_1 +b)}{B(a,b)} $</p>

<p>$ p(y_2|n_2,a,b) = {n_2 \choose y_2} \dfrac{B(y_2 + a,n_2 -y_2 +b)}{B(a,b)} $</p>

<p>Consider a discrete variable $Z = y_1 + y_2$. Is  $Z$ distributed as well as a Beta-Binomial (with parameters $n_1+n_2$, $a'$ and $b'$?</p>

<p>I could not prove it in an analytic form so far, but I have been trying it out with some simulations, at least to check whether the assumption is wrong in some cases. Reparametrising $a$ and $b$ as $\mu = \dfrac{a}{a+b}$ and $\rho = \dfrac{1}{a+b+1}$, $y_1$ and $y_2$ have mean $\mu n_1$ and $\mu n_2$ respectively and  variance $\mu(1-\mu)n_1(1 + (n_1-1)\rho)$ and $\mu(1-\mu)n_2(1 + (n_2-1)\rho)$ respectively. </p>

<p>Based on independence, the mean of $Z$ is $\mu(n_1+n_2)$ and the variance of $Z$ is $\mu(1-\mu)(n_1(1 + (n_1-1)\rho) + n_2(1 + (n_2-1)\rho))$. If $Z$ was distributed according to a beta-binomial distribution, then it would have paramters $\mu'$ and $\rho'$, with $\mu'= \mu$ and </p>

<p>$\rho' = \dfrac{\dfrac{n_1(1 + (n_1-1)\rho) + n_2(1 + (n_2-1)\rho)}{n_1+n_2} - 1}{n_1+n_2-1} = \rho \dfrac{n_1(n_1-1)+n_2(n_2-1)}{(n_1+n_2)(n_1+n_2 -1)} $</p>

<p>Here is some code to generate $Z$ as a sum of two independent Beta-Binomials (sorry about the code, R is not my main language)</p>

<pre><code>n1 = 20
n2 = 50
mu = .6
k  = 20

p1  = rbeta(1e6,mu*k,(1-mu)*k)
y1  = rbinom(1e6,n1,p1)

p2  = rbeta(1e6,mu*k,(1-mu)*k)
y2  = rbinom(1e6,n2,p2)

z   = y1+y2

rho  = 1/(k+1)
rho1 = rho*(n1*(n1-1)+n2*(n2-1))/((n1+n2)*(n1+n2-1))
k1   = 1/ rho1 - 1
p3  = rbeta(1e6,mu*k1,(1-mu)*k1)
z1  = rbinom(1e6,n1+n2,p3)

print(c(var(z),var(z1)))
plot(density(z,width= 3))
lines(density(z1,width = 3))
</code></pre>

<p>I have been trying this code for different values of $n_1$, $n_2$, $\mu$ and $k$, but in all the cases the variances using the sum of two beta-binomials or an appropriately tuned beta-binomial are very similar (the densities look indistinguishable)</p>
",2013-10-18 13:51:46.780
57798,22843.0,1,57800.0,,,Why is Pearson's correlation coefficient defined the way it is?,<self-study><mathematical-statistics><variance><covariance><pearson-r>,CC BY-SA 3.0,"<p>$$
r = \frac{{\rm Cov}(X,Y)}{ \sigma_{X} \sigma_{Y}}
$$
I do not understand this equation at all. Where does it come from? </p>

<p>From my personal understanding ${\rm Cov}(X,Y)$ comes from that fact that $X$ and $Y$ are dependent random variables, that is, $E[XY]$ is not the same as $E[X]E[Y]$. Is this analogous to saying that $P(A \cap B) = P(A)P(B|A)$ if $A$ and $B$ are not independent? I'm just confused as to why we want the ratio of $E[XY]-E[X]E[Y]$ over the product of the standard deviations for $X$ and $Y$.  </p>
",2013-10-18 20:21:59.413
57808,22302.0,1,,,,Observational study vs experimental study?,<experiment-design><observational-study>,CC BY-SA 3.0,"<p>Say that a study conducted included 300 type A blood cancer patients, 300 type B blood cancer patients, 300 type C blood cancer patients and 300 healthy people, all selected randomly from clinics' lists. We are then given the blood pressure of each patient and healthy person to check if there's a difference between the groups. </p>

<p>Would that be an observational study or experimental study? I'm leaning towards observational because we do not change anything, just ""observe"".</p>

<p>Also, why is the healthy 300 people that were randomly selected important for the study?</p>
",2013-10-18 23:56:00.510
57773,10060.0,2,,57765.0,,,,CC BY-SA 3.0,"<p><strong>Yes, kind of...</strong></p>

<p>Yes, it is ""fine."" See this 10-case example, where case 5 and case 7 both have a missing:</p>

<p><img src=""https://i.stack.imgur.com/NkL6l.png"" alt=""enter image description here""></p>

<p>Now, look at their correlation outcome, there are only 8 cases participating.</p>

<p><img src=""https://i.stack.imgur.com/7jcaD.png"" alt=""enter image description here""></p>

<p>The reason is that Pearson's correlation requires the covariance between $x1$ and $x2$ to be calculated. If either one has a missing, there will be no covariance resulted, and the case is thrown out.</p>

<p>Now, to further illustrate, let us use select case to filter out the two cases:</p>

<p><img src=""https://i.stack.imgur.com/fQyvA.png"" alt=""enter image description here""></p>

<p>And rerun the correlation again, the results are identical. This exclusion does not just happen to system missing, if you have assigned a user-defined missing, the case with that user-defined missing will also be excluded.</p>

<p><img src=""https://i.stack.imgur.com/IE7JA.png"" alt=""enter image description here""></p>

<p><strong>But wait...!</strong></p>

<p>I said that it's ""fine"" because it's true that SPSS does screen out incomplete cases for you. But it is in no way solving the missing phenomenon for you. If there is any systematic reason that causes your participants to not answer a certain question, you correlation coefficient can be wrong. However, if you feel that they missed the answer in a random manner, then your correlation shouldn't be heavily affected, though you may lose some sample size and consequently power.</p>

<hr>

<p><strong>Q:</strong> <em>But - I ask you - please tell Tania about pairwise and listwise deletion of missings and under what button it is found in SPSS -- ttnphns</em></p>

<p><strong>A:</strong> Certainly. It would be necessary to illustrate with another example in which we have a new candidate, $x3$:</p>

<p><img src=""https://i.stack.imgur.com/TSc85.png"" alt=""enter image description here""></p>

<p>SPSS correlation analysis uses pairwise deletion by default, which means it'd always maximize the number of case in each of the pairwise comparisons. We have learned from above that the correlation between $x1$ and $x2$ has a sample size of 8 pairs. What about $x1$ and $x3$?</p>

<p><img src=""https://i.stack.imgur.com/vg59q.png"" alt=""enter image description here""></p>

<p>Turned out, it's 9 because maximally there are 9 pairs of data. Now, this can get inconvenient if you'd like to screen off the whole case and prevent it from being analyzed. In that case, you'll use list-wise deletion.</p>

<p>To call the option up, in the Correlation menu, press <code>Option</code> and then check <code>Exclude cases listwise</code>, then press <code>Continute</code> and <code>OK</code> to submit the test again:</p>

<p><img src=""https://i.stack.imgur.com/lGT0j.png"" alt=""enter image description here""></p>

<p>Now let's run the correlation matrix again, you'll notice that all sample sizes are unified to 8; only cases that provide data to all the three variables are retained. Visit this <a href=""http://www-01.ibm.com/support/docview.wss?uid=swg21475199"" rel=""nofollow noreferrer"">IBM FAQ</a> if you'd like to learn more about the two types of deletion.</p>

<p><img src=""https://i.stack.imgur.com/EmVTm.png"" alt=""enter image description here""></p>
",2013-10-18 13:57:49.450
57774,10772.0,2,,4705.0,,,,CC BY-SA 3.0,"<p><a href=""http://en.wikipedia.org/wiki/John_Kingman"" rel=""nofollow"" title=""John Kingman"">John Kingman</a> for Coalescent theory and his work on completely random measures</p>
",2013-10-18 14:11:35.507
57775,306.0,2,,57770.0,,,,CC BY-SA 3.0,"<p>Assuming everything else stays constant,</p>

<p>(change in y) / (y) = B3 * (change in income) / (1 + income)</p>

<p>LHS is your percentage change in y. Put the values in the RHS. So for a 1% increase in income, change in income is 0.01 and income is 100. Gives you a result straightaway as B3 * (1/101)% or 0.0055%.</p>
",2013-10-18 14:36:10.050
57776,21762.0,2,,57770.0,,,,CC BY-SA 3.0,"<p>If income is typically much larger than 1, you could ignore the $+1$ for interpretation and use the usual statement for linear log-log-models: ""A 1% increase in income is associated with a $100\% \cdot (1.01^{0.55566}âˆ’1)=0.5544\%$ increase in the geometric mean of $y$. Or, a bit less precise but better to understand: ""A 1% increase in income is associated with about a 0.56% increase in the typical value of $y$. </p>

<p>Edit:</p>

<ul>
<li>If you do not want to ignore the $+1$ for interpretation, just say ""A $1\%$ increase in $1 + \text{income}$ ...""</li>
<li>If you prefer to describe the effect on the arithmetic mean of $y$ instead of its geometric mean, try a (Gamma-)GLM with log link.</li>
</ul>
",2013-10-18 14:37:15.650
57777,17740.0,2,,57680.0,,,,CC BY-SA 3.0,"<p>Usually, the decision is whether to use linear or an RBF (aka Gaussian) kernel. There are two main factors to consider:</p>

<ol>
<li>Solving the optimisation problem for a linear kernel is <strong>much</strong> faster, see e.g. LIBLINEAR.</li>
<li>Typically, the best possible predictive performance is better for a nonlinear kernel (or at least as good as the linear one). </li>
</ol>

<p>It's been shown that the <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.880&amp;rep=rep1&amp;type=pdf"">linear kernel is a degenerate version of RBF</a>, hence the linear kernel is never more accurate than a properly tuned RBF kernel. Quoting the abstract from the paper I linked:</p>

<blockquote>
  <p>The analysis also indicates that if complete model selection using the  Gaussian kernel has been conducted, there is no need to consider linear SVM.</p>
</blockquote>

<p>A basic rule of thumb is briefly covered in NTU's <a href=""http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"">practical guide to support vector classification</a> (Appendix C).</p>

<blockquote>
  <p>If the number of features is large, one may not need to map data to a higher dimensional space. That is, the nonlinear mapping does not improve the performance.
  Using the linear kernel is good enough, and one only searches for the parameter C.</p>
</blockquote>

<p>Your conclusion is more or less right but you have the argument backwards. In practice, the linear kernel tends to perform very well when the number of features is large (e.g. there is no need to map to an even higher dimensional feature space). A typical example of this is document classification, with thousands of dimensions in input space.</p>

<p>In those cases, nonlinear kernels are not necessarily significantly more accurate than the linear one. This basically means nonlinear kernels lose their appeal: they require way more resources to train with little to no gain in predictive performance, so why bother.</p>

<h1>TL;DR</h1>

<p>Always try linear first since it is way faster to train (AND test). If the accuracy suffices, pat yourself on the back for a job well done and move on to the next problem. If not, try a nonlinear kernel.</p>
",2013-10-18 14:45:39.377
57778,10547.0,1,57901.0,,,"Calculation of an ""unconstrained"" normal distribution (starting from a censored one)",<distributions><normal-distribution><conditional-probability><density-function><truncation>,CC BY-SA 3.0,"<p>Assume that two r.v. $W$ and $Y|W=w$ with</p>

<p>(1) $W \sim \text{N}(\mu_w,\sigma_w^2)$ (iid)</p>

<p>(2) $Y|W=w \sim \text{N}(w,\sigma_y^2)$ (iid)</p>

<p>Further we only observe $Y$ if $Y$ is less then $W$, i.e., </p>

<p>(3) $Y|Y\le W$</p>

<p><strong>Goal:</strong> Find the pdf of the censored observations, i.e., of $Y|Y\le W$ and from that deduce the uncensored pdf and the first two moments (so i.m.h.o. we have to find$f_Y(y)$). The first two moments of this uncensored pdf are supposed to depend upon $E(Y|Y\le W)$ and $Var(Y|Y\le W)$.</p>

<hr>

<p>By definition of <em>conditional pdf</em> we have that:</p>

<p>(4) $f_{Y|W}(y|W = w)= \frac{f_{Y,W}(y,w)}{f_W(w)}$ </p>

<p>Next, the definition of a <em>truncated density</em> gives for a abitrary value of $W$:</p>

<p>(5) $ f_{Y|Y\le W}(y|y\le w) = \frac{f_Y(y)}{P(Y\le W)}$</p>

<hr>

<p>I would simply rewrite (4) to</p>

<p>$f_{Y|W}(y|W = w)f_W(w) = f_{Y,W}(y,w)$</p>

<p>then integration over $f_{Y,W}(y,w)$ w.r.t $w$ should yield $f_Y(y)$, i.e.,</p>

<p>(a) $\int_{-\infty}^{\infty} f_{Y,W}(y,w) dw = \int_{-\infty}^{\infty} f_Y(y|W = w)f_W(w) dw = f_Y(y)$</p>

<p>Plugin in $f_Y(y)$ into (5), ($P(Y\le W)$ will also be given by $f_Y(y)$) I will se how the moments of $f_{Y|Y\le W}(y|y\le w)$ will look and how the moments of $f_Y(y)$ depend upon them.</p>

<p>So (a) will look like</p>

<p>$f_Y(y) = \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi\sigma^2_y}}\text{exp}\big(-\frac{(y-w)^2}{2\sigma_y^2}\big)\frac{1}{\sqrt{2\pi\sigma^2_w}}\text{exp}\big(-\frac{(w-\mu_w)^2}{2\sigma_w^2}\big)dw$</p>

<p>Except for the $w$ in the first $\text{exp}$, this looks very easy but since there is a $w$ im a little bit stuck how to solve this... </p>
",2013-10-18 15:04:04.967
57779,,1,85831.0,,user30602,Verification of poisson approximation to hypergeometric distribution,<poisson-distribution><approximation><hypergeometric-distribution>,CC BY-SA 3.0,"<p>How can I verify that </p>

<p>$\lim_{N,M,K \to \infty, \frac{M}{N} \to 0, \frac{KM}{N} \to \lambda} \frac{\binom{M}{x}\binom{N-M}{K-x}}{\binom{N}{K}} = \frac{\lambda^x}{x!}e^{-\lambda}$, </p>

<p><strong>without</strong> using <strong>Stirling's formula</strong> or the <strong>Poisson approximation to the Binomial</strong>?</p>

<p>I have been stuck on this problem for a while, because I don't know how to divide up the terms  and factorials without using the help of prior results! </p>

<p>Any help would be appreciated. Thanks in advance.</p>
",2013-10-18 15:15:30.430
57782,17670.0,1,57964.0,,,Logistic regression: maximizing true positives - false positives,<r><regression><logistic><classification><glmnet>,CC BY-SA 3.0,"<p>I have a logistic regression model (fit via glmnet in R with elastic net regularization), and I would like to maximize the difference between true positives and false positives.  In order to do this, the following procedure came to mind:</p>

<ol>
<li>Fit standard logistic regression model</li>
<li>Using prediction threshold as 0.5, identify all positive predictions</li>
<li>Assign weight 1 for positively predicted observations, 0 for all others</li>
<li>Fit weighted logistic regression model</li>
</ol>

<p>What would be the flaws with this approach?  What would be the correct way to proceed with this problem?</p>

<p>The reason for wanting to maximize the difference between the number of true positives and false negatives is due to the design of my application.  As part of a class project, I am building a autonomous participant in an online marketplace - if my model predicts it can buy something and sell it later at a higher price, it places a bid.  I would like to stick to logistic regression and output binary outcomes (win, lose) based on fixed costs and unit price increments (I gain or lose the same amount on every transaction).  A false positive hurts me because it means that I buy something and am unable to sell it for a higher price.  However, a false negative doesn't hurt me (only in terms of opportunity cost) because it just means if I didn't buy, but if I had, I would have made money.  Similarly, a true positive benefits me because I buy and then sell for a higher price, but a true negative doesn't benefit me because I didn't take any action.</p>

<p>I agree that the 0.5 cut-off is completely arbitrary, and when I optimized the model from step 1 on the prediction threshold which yields the highest difference between true/false positives, it turns out to be closer to 0.4.  I think this is due to the skewed nature of my data - the ratio between negatives and positives is about 1:3.</p>

<p>Right now, I am following the following steps:</p>

<ol>
<li>Split data intto training/test</li>
<li>Fit model on training, make predictions in test set and compute difference between true/false positives</li>
<li>Fit model on full, make predictions in test set and compute difference between true/false positives</li>
</ol>

<p>The difference between true/false positives is smaller in step #3 than in step #2, despite the training set being a subset of the full set.  Since I don't care whether the model in #3 has more true negatives and less false negatives, is there anything I can do without altering the likelihood function itself?</p>
",2013-10-18 17:00:57.543
57783,2085.0,2,,57721.0,,,,CC BY-SA 3.0,"<p>If I understand, you are looking for <a href=""http://en.wikipedia.org/wiki/K-mer"" rel=""nofollow"">k-mers</a> which are patterns of size k found in sequences.</p>

<p>There is an R package for analyzing sequence data called <a href=""http://mephisto.unige.ch/traminer/"" rel=""nofollow"">TraMineR</a> which includes functions for plotting the sequences, finding the variance of state durations, compute within sequence entropy, extract frequent event subsequences, etc.</p>

<p>You could also compare two sequences to see how they align in time by using <a href=""http://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow"">Dynamic Time Warping</a></p>
",2013-10-18 17:29:17.817
57784,750.0,2,,57721.0,,,,CC BY-SA 3.0,"<p>A <a href=""http://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test"" rel=""nofollow"">runs test</a> seems appropriate, and the cited literature at the end develops the test statistic for multiple categories. Unfortunately the paper is paywalled but here is a quick run-down of the test statistic (screen shot of <a href=""https://dl.dropboxusercontent.com/u/3385251/MultRuns.PNG"" rel=""nofollow"">relevant page here</a>).</p>

<p>For each individual group, we can count;</p>

<ul>
<li>$n_s = \text{Number of successes}$</li>
<li>$r_s = \text{Number of success runs}$</li>
<li>$s_{s}^{2} = \text{Sample variance of success run lengths}$</li>
<li>$c_s = (r^2-1)(r+2)(r+3)/[2r(n-r-1)(n+1)]$</li>
<li>$v_s = cn(n - r)/[r(r + 1)]$</li>
</ul>

<p>Then you calculate this for each separate group, and the test statistic is the sum of the each $c_s \cdot s_{s}^{2}$ and is distributed as $\chi^{2}$ with $\sum{v_i}$ degrees of freedom.</p>

<p>So, lets say we have a table of run lengths for three different groups as follows;</p>

<pre><code>Data: 221331333121112112212112122

Length Group1  Group2  Group3
-----------------------------
     1   5       4       0 
     2   2       3       1
     3   1       0       1
-----------------------------
    n_s 12      10       5
    r_s  8       7       2
    s_s  0.6     0.3     0.5
    c_s 11.1    14.0     1.3
    v_s  7.4     7.5     3.1 
-----------------------------
x^2 = (0.6*11.1) + (0.3*14) + (0.5*1.3) = 11
DF  = 7.4 + 7.5 + 3.1 = 18
</code></pre>

<p>Evaluating the area to the right of the test statistic is .9, so in this circumstance we would either fail to reject the null hypothesis that the distribution of the runs are randomly distributed. It is fairly close to the other tail though, so it is borderline evidence the data is more dispersed than you would expect by chance (as this is one of those circumstances it makes sense to evaluate the left tail of the Chi-Square distribution).</p>

<hr>

<p>O'Brien, Peter C. &amp; Peter J. Dyck. 1985. A runs test based on run lengths. <a href=""http://dx.doi.org/10.2307/2530658"" rel=""nofollow""><em>Biometrics 41</em>(1):237-244.</a></p>

<p>I've posted a code snippet on estimating this in SPSS at <a href=""https://www.dropbox.com/sh/kr6qvukrw6xvue4/AABWSg-DAcoLoysqTKMyeRdNa"" rel=""nofollow"">this dropbox link</a>. It includes the made up example here, as well as a code example replicating the tables and statistics in the O'Brien &amp; Dyck paper (on a made up set of data that looks like theirs).</p>
",2013-10-18 18:23:56.543
57785,22703.0,1,,,,Order Statistics,<order-statistics>,CC BY-SA 3.0,"<p>What is the motivation behind the use of order statistics in parameter estimation. In a very general sense, the first order statistic is considered to be an initial estimate to the location parameter. I wonder, how this would be possible. A layman based explanation would help. </p>
",2013-10-18 18:36:56.647
57786,22703.0,2,,57053.0,,,,CC BY-SA 3.0,"<p>I guess, the frequency table summarizing the number of ones/zeros would be a good summary by itself. Mean seems to be the only reasonably explainable statistic here.</p>
",2013-10-18 18:40:50.570
57787,22703.0,1,,,,Three Parameter Gamma Distribution,<gamma-distribution>,CC BY-SA 3.0,"<p>What is the motivation for the three parameter gamma distribution and the resulting structure of its density?</p>

<p>What is the meaning of the location, scale and shape parameters here?</p>
",2013-10-18 18:54:21.117
57788,22703.0,2,,56859.0,,,,CC BY-SA 3.0,"<p>In one line, given the data, descriptive statistics try to summarize the content of your data with minimum loss of information ( depending on what measure do you use). You get to see the geography of the data.( Something like, see the performance graph of the class and say who is on top, the bottom and so on)</p>

<p>In one line, given the data, you try to estimate and infer to the properties of the hypothetical population from which the data comes from. ( Something like, understanding 7th grade students through the good sample from the class, assuming that the underlying population is large enough that you cannot take them into account in totality)  </p>
",2013-10-18 18:59:16.890
57789,22703.0,2,,56273.0,,,,CC BY-SA 3.0,"<p>Sturges formula is not from a clear-cut theorem to be proved. It is something like an opinion to fix the number of classes once you are unable to deduce it from data/study.</p>
",2013-10-18 19:02:50.657
57807,22792.0,1,57814.0,,,Why do Bayesian Networks use acyclicity assumption?,<probability><bayesian><bayesian-network><graph-theory>,CC BY-SA 4.0,"<p>Actually, this question is more or less a duplicate of the <a href=""https://math.stackexchange.com/questions/527798/why-do-bayesian-networks-use-acyclicity-assumption"">one</a> which I have asked on math.stackexchange two days ago. </p>

<p>I did not get any answer there but I think now here is a better place to ask this question since it is more about ""the philosophy"", not the calculations involved in the concept, which is what the ""math"" board likes more.</p>

<p>I am trying to gain a good understanding of Bayesian Networks and the first thing I want to understand exactly is how they are built. I see that these networks are built on conditional independence assumptions in order to simplify joint distributions and they are built commonly by using causal relationships since they imply conditional independence given the direct causes. What I still don't understand is why these networks assume a Directed Acyclic (DAG) structure? </p>

<p>There can be systems which contain circular causality relationships. For example, let's think of a hypothetical machine consisting of two parallel plates which rub together. We think of three variables ""Heat"", ""Plate Area Expansion"" and ""Friction"". Plate Area Expansion is the effect of the cause ""Heat"" and ""Friction"" is just the effect of the plate area expansion since larger area means larger amount of friction, in turn. But if we think of it, ""Friction"" also causes an increase in the heat level, so ""Friction"" is a direct cause of ""Heat"" as well. This circular causality ends up with the following diagram:</p>

<p><a href=""https://i.stack.imgur.com/sNNga.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sNNga.png"" alt=""Diagram""></a>
</p>

<p>This is a Directed Cyclic Graph and violates acyclicity (DAG) assumptions on which the whole Bayesian Network idea is founded. So, how can be such systems with circular causes, feedbacks and loops represented with Bayesian Networks? It is surely not possible that the DAG theory does not support such systems because this ""feedback"" mechanism is a quite common and basic thing. But I just cannot see how those kinds of systems are represented with acyclic graphs and I am sure that I am missing something here. I want to know what I am just missing.</p>
",2013-10-18 23:52:59.917
57790,22841.0,1,,,,"Logistic regression, SPSS ignores my reference category and assumes another one",<logistic><spss><categorical-data><stepwise-regression>,CC BY-SA 3.0,"<p>I am modelling logistic regressions in SPSS, the same model for different countries (well, with slight differences in the independent variables set due to collinearity diagnosis and stepwise results). The model seems to work fine for most countries. In two countries, I am having some issues with the same variable. The variable has three categories (hierarchical). In the output for those two beautiful countries, seems that SPSS take the reference category out, assumes other as reference category and gives exp(B) for the last category. </p>

<p>Is there something wrong going on? What should / can I do about it?</p>

<hr>

<p>Thanks a lot for your answers!
The problem cannot be in the syntax since I am using the same for all countries and works perfectly.</p>

<p>I am using ENTER. The STEPWISE was used as an exploratory method to identify the strongest predictors. According to the -2LL values my variable is not a 'strong' one for this two countries (0.12%). Is not the first time that I read that STEPWISE is a bad choice... I will that in account!</p>

<p>Nevertheless, why do SPSS does that? 
Is this a real problem, meaning something that I must solve?
Or can I use/report the odds for those two countries without that category adding a footnote? 
Should I run the model without that variable (theorectically the variable is not that important).</p>

<p>Thanks ;)</p>

<hr>

<p>Hi, All my variables are recoded to the reference category be 1 and most of my variables have three categories. I am using Enter with the following syntax. LOGISTIC REGRESSION VAR= ""DV""
/METHOD=ENTER ""IV's""
/CONTRAST (""IV"")=Indicator (1) [...]
/PRINT=GOODFIT CI(95) 
 /CRITERIA PIN(.05) POUT(.10) ITERATE(20) CUT(.5). 
Do you have idea what is going wrong?</p>
",2013-10-18 19:12:28.563
57791,5661.0,2,,28.0,,,,CC BY-SA 4.0,"<p><strong>Bayesian</strong>: &quot;Hello, Machine Learner!&quot;</p>
<p><strong>Frequentist</strong>:  &quot;Hello, Machine Learner!&quot;</p>
<p><strong>Machine Learning</strong>: &quot;I hear you guys are good at stuff.  Here's some data.&quot;</p>
<p><strong>F</strong>: &quot;Yes, let's write down a model and then calculate the MLE.&quot;</p>
<p><strong>B</strong>: &quot;Hey, F, that's not what you told me yesterday!  I had some univariate data and I wanted to estimate the variance, and I calculated the MLE.  Then you pounced on me and told me to <a href=""http://en.wikipedia.org/wiki/Bessel%27s_correction"" rel=""noreferrer"">divide by <span class=""math-container"">$n-1$</span> instead of by <span class=""math-container"">$n$</span></a>.&quot;</p>
<p><strong>F</strong>: &quot;Ah yes, thanks for reminding me.  I often think that I'm supposed to use the MLE for everything, but I'm interested in <a href=""http://en.wikipedia.org/wiki/Biased_estimator"" rel=""noreferrer"">unbiased estimators</a> and so on.&quot;</p>
<p><strong>ML</strong>: &quot;Eh, what's this philosophizing about?  Will it help me?&quot;</p>
<p><strong>F</strong>: &quot; OK, an <em>estimator</em> is a black box, you put data in and it gives you some numbers out.  We frequentists don't care about how the box was constructed, about what principles were used to design it.  For example, I don't know how to derive the <span class=""math-container"">$\div(n-1)$</span> rule.&quot;</p>
<p><strong>ML</strong>: &quot; So, what do you care about?&quot;</p>
<p><strong>F</strong>: &quot;Evaluation.&quot;</p>
<p><strong>ML</strong>: &quot;I like the sound of that.&quot;</p>
<p><strong>F</strong>: &quot;A black box is a black box.  If somebody claims a particular estimator is an unbiased estimator for <span class=""math-container"">$\theta$</span>, then we try many values of <span class=""math-container"">$\theta$</span> in turn, generate many samples from each based on some assumed model, push them through the estimator, and find the average <em>estimated <span class=""math-container"">$\theta$</span></em>.  If we can prove that the expected estimate equals the true value, for all values, then we say it's unbiased.&quot;</p>
<p><strong>ML</strong>: &quot;Sounds great!  It sounds like frequentists are pragmatic people.  You judge each black box by its results.  Evaluation is key.&quot;</p>
<p><strong>F</strong>: &quot;Indeed!  I understand you guys take a similar approach.  Cross-validation, or something?  But that sounds messy to me.&quot;</p>
<p><strong>ML</strong>: &quot;Messy?&quot;</p>
<p><strong>F</strong>: &quot;The idea of testing your estimator on real data seems dangerous to me.  The empirical data you use might have all sorts of problems with it, and might not behave according the model we agreed upon for evaluation.&quot;</p>
<p><strong>ML</strong>: &quot;What?  I thought you said you'd proved some results?  That your estimator would always be unbiased, for all <span class=""math-container"">$\theta$</span>.&quot;</p>
<p><strong>F</strong>: &quot;Yes.  While your method might have worked on one dataset (the dataset with train and test data) that you used in your evaluation, I can prove that mine will always work.&quot;</p>
<p><strong>ML</strong>: &quot;For all datasets?&quot;</p>
<p><strong>F</strong>: &quot;No.&quot;</p>
<p><strong>ML</strong>: &quot;So my method has been cross-validated on one dataset.  You haven't test yours on any real dataset?&quot;</p>
<p><strong>F</strong>: &quot;That's right.&quot;</p>
<p><strong>ML</strong>: &quot;That puts me in the lead then!  My method is better than yours.  It predicts cancer 90% of the time.  Your 'proof' is only valid if the entire dataset behaves according to the model you assumed.&quot;</p>
<p><strong>F</strong>: &quot;Emm, yeah, I suppose.&quot;</p>
<p><strong>ML</strong>: &quot;And that interval has 95% <em>coverage</em>.  But I shouldn't be surprised if it only contains the correct value of <span class=""math-container"">$\theta$</span> 20% of the time?&quot;</p>
<p><strong>F</strong>: &quot;That's right.  Unless the data is truly i.i.d Normal (or whatever), my proof is useless.&quot;</p>
<p><strong>ML</strong>: &quot;So my evaluation is more trustworthy and comprehensive? It only works on the datasets I've tried so far, but at least they're real datasets, warts and all.  There you were, trying to claim you were more 'conservative' and 'thorough' and that you were interested in model-checking and stuff.&quot;</p>
<p><strong>B</strong>: (interjects) &quot;Hey guys, Sorry to interrupt.  I'd love to step in and balance things up, perhaps demonstrating some other issues, but I really love watching my frequentist colleague squirm.&quot;</p>
<p><strong>F</strong>: &quot;Woah!&quot;</p>
<p><strong>ML</strong>: &quot;OK, children.  It was all about evaluation.  An estimator is a black box.  Data goes in, data comes out.  We approve, or disapprove, of an estimator based on how it performs under evaluation.  We don't care about the 'recipe' or 'design principles' that are used.&quot;</p>
<p><strong>F</strong>: &quot;Yes.  But we have very different ideas about which evaluations are important.  ML will do train-and-test on real data.  Whereas I will do an evaluation that is more general (because it involves a broadly-applicable proof) and also more limited (because I don't know if your dataset is actually drawn from the modelling assumptions I use while designing my evaluation.)&quot;</p>
<p><strong>ML</strong>: &quot;What evaluation do you use, B?&quot;</p>
<p><strong>F</strong>: (interjects) &quot;Hey. Don't make me laugh. He doesn't evaluate anything.  He just uses his subjective beliefs and runs with it.  Or something.&quot;</p>
<p><strong>B</strong>: &quot;That's the common interpretation.  But it's also possible to define Bayesianism by the evaluations preferred.  Then we can use the idea that none of us care what's in the black box, we care only about different ways to evaluate.&quot;</p>
<p><strong>B</strong> continues:   &quot;Classic example:  Medical test.  The result of the blood test is either Positive or Negative.  A frequentist will be interested in, of the Healthy people, what proportion get a Negative result.  And similarly, what proportion of Sick people will get a Positive. The frequentist will calculate these for each blood testing method that's under consideration and then recommend that we use the test that got the best pair of scores.&quot;</p>
<p><strong>F</strong>: &quot;Exactly. What more could you want?&quot;</p>
<p><strong>B</strong>: &quot;What about those individuals that got a Positive test result?  They will want to know 'of those that get a Positive result, how many will get Sick?' and 'of those that get a Negative result, how many are Healthy?' &quot;</p>
<p><strong>ML</strong>: &quot;Ah yes, that seems like a better pair of questions to ask.&quot;</p>
<p><strong>F</strong>: &quot;HERESY!&quot;</p>
<p><strong>B</strong>: &quot;Here we go again.  He doesn't like where this is going.&quot;</p>
<p><strong>ML</strong>: &quot;This is about 'priors', isn't it?&quot;</p>
<p><strong>F</strong>: &quot;EVIL&quot;.</p>
<p><strong>B</strong>: &quot;Anyway, yes, you're right ML.  In order to calculate the proportion of Positive-result people that are Sick you must do one of two things.  One option is to run the tests on lots of people and just observe the relevant proportions. How many of those people go on to die of the disease, for example.&quot;</p>
<p><strong>ML</strong>: &quot;That sounds like what I do.  Use train-and-test.&quot;</p>
<p><strong>B</strong>: &quot;But you can calculate these numbers in advance, if you are willing to make an assumption about the rate of Sickness in the population.  The frequentist also makes his calcuations in advance, but without using this population-level Sickness rate.&quot;</p>
<p><strong>F</strong>:  &quot;MORE UNFOUNDED ASSUMPTIONS.&quot;</p>
<p><strong>B</strong>: &quot;Oh shut up.  Earlier, you were found out.  ML discovered that you are just as fond of unfounded assumptions as anyone.  Your 'proven' coverage probabilities won't stack up in the real world unless all your assumptions stand up.  Why is my prior assumption so diffent?  You call me crazy, yet you pretend your assumptions are the work of a conservative, solid, assumption-free analysis.&quot;</p>
<p><strong>B</strong> (continues): &quot;Anyway, ML, as I was saying. Bayesians like a different kind of evaluation.  We are more interested in conditioning on the observed data, and calculating the accuracy of our estimator accordingly.  We cannot perform this <em>evaluation</em> without using a prior.  But the interesting thing is that, once we decide on this form of evaluation, and once we choose our prior, we have an automatic 'recipe' to create an appropriate estimator.  The frequentist has no such recipe.  If he wants an unbiased estimator for a complex model, he doesn't have any automated way to build a suitable estimator.&quot;</p>
<p><strong>ML</strong>: &quot;And you do? You can automatically build an estimator?&quot;</p>
<p><strong>B</strong>: &quot;Yes.  I don't have an automatic way to create an unbiased estimator, because I think bias is a bad way to evaluate an estimator.  But given the conditional-on-data estimation that I like, and the prior, I can connect the prior and the likelihood to give me the estimator.&quot;</p>
<p><strong>ML</strong>: &quot;So anyway, let's recap.  We all have different ways to evaluate our methods, and we'll probably never agree on which methods are best.&quot;</p>
<p><strong>B</strong>: &quot;Well, that's not fair.  We could mix and match them.  If any of us have good labelled training data, we should probably test against it.  And generally we all should test as many assumptions as we can.  And some 'frequentist' proofs might be fun too, predicting the performance under some presumed model of data generation.&quot;</p>
<p><strong>F</strong>: &quot;Yeah guys.  Let's be pragmatic about evaluation.  And actually, I'll stop obsessing over infinite-sample properties.  I've been asking the scientists to give me an infinite sample, but they still haven't done so.  It's time for me to focus again on finite samples.&quot;</p>
<p><strong>ML</strong>: &quot;So, we just have one last question.  We've argued a lot about how to <em>evaluate</em> our methods, but how do we <em>create</em> our methods.&quot;</p>
<p><strong>B</strong>: &quot;Ah. As I was getting at earlier, we Bayesians have the more powerful general method.  It might be complicated, but we can always write some sort of algorithm (maybe a naive form of MCMC) that will sample from our posterior.&quot;</p>
<p>F(interjects): &quot;But it might have bias.&quot;</p>
<p><strong>B</strong>: &quot;So might your methods.  Need I remind you that the MLE is often biased?  Sometimes, you have great difficulty finding unbiased estimators, and even when you do you have a stupid estimator (for some really complex model) that will say the variance is negative.  And you call that unbiased.  Unbiased, yes.  But useful, no!&quot;</p>
<p><strong>ML</strong>: &quot;OK guys. You're ranting again.  Let me ask you a question, F.  Have you ever compared the bias of your method with the bias of B's method, when you've both worked on the same problem?&quot;</p>
<p><strong>F</strong>: &quot;Yes.  In fact, I hate to admit it, but B's approach sometimes has lower bias and <a href=""http://en.wikipedia.org/wiki/Mean_squared_error"" rel=""noreferrer"">MSE</a> than my estimator!&quot;</p>
<p><strong>ML</strong>: &quot;The lesson here is that, while we disagree a little on evaluation, none of us has a monopoly on how to create estimator that have properties we want.&quot;</p>
<p><strong>B</strong>: &quot;Yes, we should read each other's work a bit more.  We can give each other inspiration for estimators.  We might find that other's estimators work great, out-of-the-box, on our own problems.&quot;</p>
<p><strong>F</strong>: &quot;And I should stop obsessing about bias.  An unbiased estimator might have ridiculous variance.  I suppose all of us have to 'take responsibility' for the choices we make in how we evaluate and the properties we wish to see in our estimators.  We can't hind behind a philosophy.  Try all the evaluations you can.  And I will keep sneaking a look at the Bayesian literature to get new ideas for estimators!&quot;</p>
<p><strong>B</strong>:&quot;In fact, a lot of people don't really know what their own philosophy is.  I'm not even sure myself.  If I use a Bayesian recipe, and then proof some nice theoretical result, doesn't that mean I'm a frequentist?  A frequentist cares about above proofs about performance, he doesn't care about recipes.  And if I do some train-and-test instead (or as well), does that mean I'm a machine-learner?&quot;</p>
<p><strong>ML</strong>: &quot;It seems we're all pretty similar then.&quot;</p>
",2013-10-18 19:17:41.230
57792,21884.0,1,,,,MADE and MSE pros and cons,<estimators><mse>,CC BY-SA 3.0,"<p>When assessing the performance of an estimator, in which scenarios should one prefer the use of the <strong>Mean Absolute Deviation Error</strong> (MADE) over the <strong>Mean Squared Error</strong> (MSE) and vice versa?</p>

<p>Edit / Clarification:</p>

<p>Assume that we have i.i.d data $(X_{1},Y_{1}),\cdots,(X_{n},Y_{n})$. Let $(X,Y)$ denote a generic member of the sample who's conditional mean is denoted by $m(x)=E(Y|X=x)$.</p>

<p>The performance of an estimator $\hat{m}(x)$ of $m(x)$ is often assessed either by:</p>

<p>$$MSE(x)=E\left[\left\{ \hat{m}(x)-m(x)\right\} ^{2}|\boldsymbol{X}\right]$$</p>

<p>or by</p>

<p>$$MADE(x)=E\left[\left|\hat{m}(x)-m(x)\right||\boldsymbol{X}\right]$$
 where $\mathbf{X}=(X_{1},\cdots,X_{n}).$</p>
",2013-10-18 19:19:20.950
57793,1985.0,1,,,,An equality for expectation of the non-negative random variable,<probability><mathematical-statistics><random-variable>,CC BY-SA 4.0,"<p>I once read the following inequality</p>
<p><img src=""https://i.stack.imgur.com/Wx8rr.png"" alt=""enter image description here"" /></p>
<p>Is there any specific name for this inequality? And, how to prove it?</p>
",2013-10-18 19:34:57.900
57794,22555.0,2,,37981.0,,,,CC BY-SA 3.0,"<p>With variance being defined as the second moment $\mu_{2}$, skewness being defined as the third moment $\mu_{3}$ and the kurtosis being defined as the fourth moment $\mu_{4}$, it is possible to describe the properties of a wide range of symmetric and non-symmetric distributions from the data.</p>

<p>This technique was originally described by <a href=""https://en.wikipedia.org/wiki/Pearson_distribution"" rel=""nofollow noreferrer"">Karl Pearson in 1895</a> for the so-called Pearson Distributions I to VII.  This has been extended by Egon S Pearson (date uncertain) as published in <a href=""https://en.wikipedia.org/wiki/Pearson_distribution"" rel=""nofollow noreferrer"">Hahn and Shapiro in 1966</a> to a wide range of symmetric, asymmetric and heavy tailed distributions that include Uniform, Normal, Students-t, Lognormal, Exponential, Gamma, Beta, Beta J and Beta U.  From the chart of p. 197 of Hahn and Shapiro, $B_{1}$ and $B_{2}$ can be used to establish descriptors for skewness and kurtosis as:</p>

<p>$\mu_{3} = \sqrt {B_{1}\ \mu_{2}^{3}}$<br>
$\mu_{4} = B_{2}\ \mu_{2}^{2}$  </p>

<p>If you just wanted simple relative descriptors then by applying a constant $\mu_{2} = 1$ the skewness is $\sqrt {B_{1}}$ and the kurtosis is $B_{2}$.</p>

<p>We have attempted to summarize this chart <a href=""https://stats.stackexchange.com/a/72434/31323"">here</a> so that it could be programmed, but it is better to review it in Hahn and Shapiro (pp 42-49,122-132,197).  In a sense we are suggesting a little bit of <em>reverse engineering</em> of the Pearson chart, but this could be a way to quantify what you are seeking. </p>
",2013-10-18 19:36:15.563
57795,20991.0,1,57796.0,,,Generating random variable from density function,<r><probability><estimation><density-function><random-generation>,CC BY-SA 3.0,"<p>How can I generate a random variable of size n= 2914 if I have the density function?.</p>

<p>So the problem is that I have the density f(x) (function well defined)</p>

<pre><code>P&lt;-function(a,e) { ( (1/6)*(1^3) )-((a/2)*(1^2)) +(((((a)^2)/2)+e)*1)}

D&lt;-function(u,mu,sigma) {dlogis(u,mu,sigma)}

K&lt;- function(u,a,e) {(((1/2)*(u^2))- (a*u) +(((a^2)/2)+e))}

H&lt;-function(u,mu,sigma){ plogis(u,mu,sigma, lower.tail = TRUE)}

Fprim&lt;- function(u,a,e,mu,sigma) (1/P(a,e))*(D(u,mu,sigma))*(K(H(u,mu,sigma),a,e))

Fprim(1,a,e,mu,sigma) 

df&lt;- function(u)  Fprim(u,a,e,mu,sigma)

#### Parameter n,a,e,mu,sigma 
n&lt;-2914
mu&lt;- -0.42155226
sigma&lt;- 0.60665552
a&lt;- 0.43218138
e&lt;- 0.02149706
</code></pre>

<p>I think I need to reverse and to use Monte Carlo, I don't know how to do?</p>
",2013-10-18 19:59:56.600
57796,5875.0,2,,57795.0,,,,CC BY-SA 3.0,"<p>I suppose you mean</p>

<pre><code>df &lt;- function(u)  Fprim(u,a = 0.43218138, e = 0.02149706, mu = -0.42155226, sigma = 0.60665552)
</code></pre>

<p>I propose</p>

<pre><code> x &lt;- seq(-20,20,length=10001)
 y &lt;- df(x)
 y1 &lt;- cumsum(y)*diff(x)[1]

 pf &lt;- approxfun(x,y1)
 qf &lt;- approxfun(y1,x)
 rf &lt;- function(n) qf(runif(n))
</code></pre>

<p>The functions <code>qf</code>, <code>pf</code> and <code>rf</code> are the quantile, cdf, and random generator for the density <code>df</code>. So you just end by</p>

<pre><code>rf(2914)
</code></pre>
",2013-10-18 20:19:48.297
57797,22555.0,2,,4705.0,,,,CC BY-SA 3.0,"<p>It's very difficult to add to the constellation of stars that are already listed, but for interest purposes I will throw in the <em>improbable</em> polymath <a href=""https://en.wikipedia.org/wiki/John_Maynard_Keynes"" rel=""noreferrer"">John Maynard Keynes</a> who many would not realize published <em>A Treatise on Probability</em> (1921) that can be downloaded <a href=""http://www.gutenberg.org/ebooks/32625"" rel=""noreferrer"">here</a>; and whose work was quoted frequently by Harold Jeffreys (1939).</p>

<p>Keynes by all accounts helped to bring forward Bayesian statistics and in his treatise considered the most important principle to be the <a href=""https://en.wikipedia.org/wiki/Principle_of_indifference"" rel=""noreferrer"">Principle of Indifference</a>.</p>

<p>According to Wikipedia, <em>The ""Principle of insufficient reason"" was renamed the ""Principle of Indifference"" by the economist John Maynard Keynes (1921), who was careful to note that it applies only when there is no knowledge indicating unequal probabilities.</em></p>
",2013-10-18 20:20:29.640
57799,5237.0,2,,57790.0,,,,CC BY-SA 3.0,"<p>I suspect your problem is you are using stepwise model selection.  Automatic model selection algorithms, such as stepwise selection, don't have any 'knowledge' about what the variables <em>mean</em> or how they <em>ought</em> to relate to each other.  For example, it is standard advice that if you include a squared term (e.g., $X_j^2$) in a model, you should include the lower level term ($X_j$) as well, even if it's not 'significant'.  However, the selection algorithm doesn't know there is any relationship between the two variables, and so you commonly have situations where it will throw out the lower level term but retain the squared term.  Likewise, <em>all</em> levels of your categorical variable should be retained or excluded together, but the stepwise algorithm doesn't necessarily 'know' that.  I believe this is your problem.  </p>

<p>Of course, even if you were to address this issue, stepwise selection algorithms are a truly horrible modeling strategy, and still should not be used.  For more on that topic, it may help you to read my answer here: <a href=""https://stats.stackexchange.com/questions/20836//20856#20856"">algorithms-for-automatic-model-selection</a>.  </p>

<p>As the above suggests, the way to rectify this problem is to stop using stepwise selection.  Aside from the problem described in the post, it does not do what people believe it does (i.e., tell them which variables are the 'real' or 'important' ones).  </p>

<hr>

<p>Based on your edit, I'm guessing there may be a different issue.  It is possible that you don't have any observations (cases) with those levels of the categorical variable for those countries.  You should check to see if that is the case.  If so, see if one of the other levels of the categorical variable has instances for every country, and use that one as your reference level instead.  </p>
",2013-10-18 20:33:00.640
57800,19752.0,2,,57798.0,,,,CC BY-SA 3.0,"<p>One nice thing you get from dividing by the product of standard deviations is that it guarantees that the correlation coefficient will be between -1 and +1.</p>

<p>If you want to determine if $X$ has a stronger linear relationship with $Y$ or with  $Z$ comparing $cov(X,Y)$ with $cov(X,Z)$ directly is not informative, since the scale of each of the covariances depends on the variance of $Y$ an $Z$, which could be very different.</p>

<p>Dividing by $\sigma_X \sigma_Y$ normalizes the covariance, so you can compare $cor(X,Y)$ with $cor(X,Z)$ in meaningful way.</p>
",2013-10-18 20:34:03.190
57801,22846.0,1,,,,Permutation test: Exactness,<statistical-significance><permutation-test>,CC BY-SA 3.0,"<p>I am currently reading about permutation/randomization tests and have some difficulties to understand why they are exact. More precisely, I consider two groups of random variables with means $\mu_1$ and $\mu_2$ and variances $\sigma^2_1$ and $\sigma^2_2$, which are assumed to be equal if  $ \mu_1 = \mu_2$ holds. 
To test the one-sided hypothesis $H_0: \mu_1 \leq \mu_2$ versus $H_1: \mu_1 &gt; \mu_2$, I apply a permutation test with  the same test statistic as for a two sample t-test with unequal variances and unequal sample sizes. 
It was no big deal to prove and to understand that the level of significance of the permutation test is equal to $\alpha$ for $\mu_1 = \mu_2$. However, I don't understand why the level of significance is less than $\alpha$ for $ \mu_1 \leq \mu_2$!?
Can someone give me a hint?</p>
",2013-10-18 21:05:16.017
57802,22848.0,1,57842.0,,,Recalculate log-likelihood from a simple R lm model,<r><generalized-linear-model><likelihood><lm>,CC BY-SA 3.0,"<p>I'm simply trying to recalculate with dnorm() the log-likelihood provided by the logLik function from a lm model (in R).</p>

<p>It works (almost perfectly) for high number of data (eg n=1000)  : </p>

<pre><code>&gt; n &lt;- 1000
&gt; x &lt;- 1:n
&gt; set.seed(1)
&gt; y &lt;- 10 + 2*x + rnorm(n, 0, 2)
&gt; mod &lt;- glm(y ~ x, family = gaussian)
&gt; logLik(mod)
'log Lik.' -2145.562 (df=3)
&gt; sigma &lt;- sqrt(summary(mod)$dispersion)
&gt; sum(log(dnorm(x = y, mean = predict(mod), sd = sigma)))
[1] -2145.563
&gt; sum(log(dnorm(x = resid(mod), mean = 0, sd = sigma)))
[1] -2145.563
</code></pre>

<p>but for small datasets there are clear differences : </p>

<pre><code>&gt; n &lt;- 5
&gt; x &lt;- 1:n
&gt; set.seed(1)
&gt; y &lt;- 10 + 2*x + rnorm(n, 0, 2)
&gt; 
&gt; mod &lt;- glm(y ~ x, family = gaussian)
&gt; logLik(mod)
'log Lik.' -8.915768 (df=3)
&gt; sigma &lt;- sqrt(summary(mod)$dispersion)
&gt; sum(log(dnorm(x = y, mean = predict(mod), sd = sigma)))
[1] -9.192832
&gt; sum(log(dnorm(x = resid(mod), mean = 0, sd = sigma)))
[1] -9.192832
</code></pre>

<p>Because of small dataset effect I thought it could be due to the differences in residual variance estimates between lm and glm but using lm provides the same result as glm : </p>

<pre><code>&gt; modlm &lt;- lm(y ~ x)
&gt; logLik(modlm)
'log Lik.' -8.915768 (df=3)
&gt; 
&gt; sigma &lt;- summary(modlm)$sigma
&gt; sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma)))
[1] -9.192832
&gt; sum(log(dnorm(x = resid(modlm), mean = 0, sd = sigma)))
[1] -9.192832
</code></pre>

<p>Where am I wrong ? </p>
",2013-10-18 22:27:08.737
57803,4779.0,2,,57790.0,,,,CC BY-SA 3.0,"<p>SPSS Statistics provides ridge, lasso, and elastic net in the CATREG command, but if you need to enter blocks of dummies or other groups together, that can be done in REGRESSION by using  blocks of terms via multiple METHOD subcommands.</p>
",2013-10-18 22:31:30.743
57804,21464.0,1,,,,What is the proper way to estimate the probability (proportion of time) a rare event occurs?,<estimation><binomial-distribution><rare-events><laplace-smoothing>,CC BY-SA 3.0,"<p>Often, I need to estimate the probability (proportion of time) a rare event occurs. The standard MLE estimate often gives me extreme estimates since the denominator is usually 1, and the numerator is either 0 or 1, giving me either 100% or 0%.</p>

<p>For example, I am trying to estimate the proportion of web referrals as a result of my email campaign for each of my users. Since the events are rare, most of my users usually have only 1 web referral, and they have either 0 email referral or 1 email referral. In such cases, the MLE estimate is quite unreliable.</p>

<p>Are there standard tricks to correct this over-under estimation? Perhaps something like the laplace smoothing? If yes, how should I go about it?</p>
",2013-10-18 23:11:44.410
57805,22850.0,2,,54574.0,,,,CC BY-SA 3.0,"<p>What definition of log-likelihood is that? I've seen $$r(a,b) = \log \frac{P(a|Mod)}{P(b|Mod)} = \log(P(a|Mod)) - \log(P(b|Mod)) ,$$ but here you're subtracting your two probabilities.</p>
",2013-10-18 23:23:26.417
57806,17538.0,2,,26070.0,,,,CC BY-SA 3.0,"<p>The paper <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.7409&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">On the surprising behavior of distance metrics in high dimensional space</a> discusses the behaviour of distance metrics in high dimensional spaces.</p>

<p>They take on the $L_k$ norm and propose the manhattan $L_1$ norm as the most effective in high dimensional spaces for clustering purposes. They also introduce a <em>fractional norm</em> $L_f$ similar to the $L_k$ norm but with $f \in (0..1)$.</p>

<p>In short, they show that for high dimensional spaces using the euclidean norm as a default is probably not a good idea; we have usually little intuition in such spaces, and the exponential blowup due to the number of dimensions is hard to take into account with the euclidean distance. </p>
",2013-10-18 23:51:54.773
57809,10060.0,2,,57808.0,,,,CC BY-SA 3.0,"<p>Strictly speaking there isn't enough information to decide what study it is because the outcome(s) and exposure(s) have not been explicitly declared. If the different blood cancers are outcomes and blood pressure is exposure, then it's a case-control study design. However, it's silly because the different blood pressures can be a manifestation of the cancers so the outcome/exposure relationship can be mushy. Hence it's a funny design. In a way, it feels just like a cross-sectional survey.</p>

<p>Anyway, it's observational so far, because <strong>the researchers did not allocate exposures</strong>. Using ""observing or not observing"" as a criterion is risky because no matter what the researchers actually always observe, experimental or observational.</p>

<p>Random sampling is important because if the sample is not random, then all the statistical inferences based on the analysis will be challenged. In other words, a statistical test's result can be used to infer what happens in the population level because the sample was randomly chosen.</p>
",2013-10-19 00:36:40.520
57810,22705.0,2,,57500.0,,,,CC BY-SA 3.0,"<p>I found all the above answers difficult to comprehend: that may be due to my limitations. But I found this link to help me understand the difference between parametric and non parametric: <br> <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-nonparametric-regression.pdf"" rel=""nofollow"">http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-nonparametric-regression.pdf</a></p>

<p>In parametric regression or the common $y = mx+c$ form, we specify the form of the relationship as a straight line. </p>

<p>In a non-parametric regression such as MARS or splining, we allow the technique to determine the form of the relationship. It could be a simple straight line, or a curved one, or a summation of multiple straight lines (through hinge functions etc.) to get a non-linear relationship.</p>
",2013-10-19 01:53:06.130
57811,22836.0,1,,,,"Bayesian, Fisher method: model very simple data to get discriminants",<bayesian><naive-bayes><discriminant-analysis>,CC BY-SA 3.0,"<p>I've just implemented a naive Bayesian classifier and found out about the Fisher method (<a href=""https://stats.stackexchange.com/questions/31366/linear-discriminant-analysis-and-bayes-rule"">Linear discriminant analysis and Bayes rule: classification</a>) while looking for ways to improve it. I'm very new to this field.</p>

<p>My raw data model is like the following (for sentiment analysis):</p>

<pre><code>{""I like the movie a lot"", ""positive"", 1000}
{""I hate the movie a lot"", ""negative"", 100}
...
</code></pre>

<p>As you see, I have only two classes, possibly more, and the third value is weight values to count when getting probabilities.</p>

<p>So when given this kind of data,  and to get the PDF like below,</p>

<p>$$PDF(x|k) = \frac {e^{-d/2}} {(2\pi)^{p/2}\sqrt{|S|})}$$</p>

<p>I have no idea how to approach this. What should I set as discriminants? Where should I start to matrix-ify my data to get the covariance? Once I model the data, the next step seems to be relatively easier with just calculation.</p>

<p>In short, what should I do to get the values of $p$ discriminants from the data <code>{""I like the movie a lot"", ""positive"", 1000}</code>, with the first value as training text, send as class value, third as weight value.</p>
",2013-10-19 02:19:52.437
57812,22705.0,1,,,,Estimation of a power function in regression $y = ax^k$,<regression><polynomial><power-law>,CC BY-SA 3.0,"<p>I'm performing a case of polynomial regression. I use a power $k$ for the regressors (e.g. marketing spend), which helps me determine the nature of the response curve.</p>

<p>I also need to estimate the coefficient for each regressor.</p>

<p>Consider the simplistic case: $y = ax^k + c$ ; $c$ constant, $a$ a coefficient.</p>

<p>The values of $k$ and $a$ need to be determined (if polynomial, $k$, $x$ and $a$ would be vectors). I vary $k$ between $-2$ and $2$ and find the value of $k$ for which <code>pow(x,k)</code> correlates best with $y$ using a SAS macro. I take the top three $k$ which help $x$ correlate with $y$.</p>

<p>I start regressing $y$ on <code>pow(x,k)</code> and vary $k$ between the top values in priority and observe model fit and error structure to decide. </p>

<p>This is a slightly approximate approach (depending on the intervals of $k$ which I choose to iterate over, 0.01/0.1 etc.), but has worked well in polynomial situations because it is a SAS macro and runs pretty fast.</p>

<p>Is there a better approach?</p>

<p>Editing to add some more context as suggested by @Nick-Cox. The dependent is the sales of a product. The regressors (x) are marketing spends. </p>

<p>There is a strong hypothesis backing interaction effects between the x's. </p>

<p>Another requirement is that not all marketing spends should be forced to have a diminishing impact on sales.</p>
",2013-10-19 02:43:28.907
57813,22851.0,1,,,,Modeling time in multilevel logistic regression,<time-series><logistic><multilevel-analysis><mixed-model>,CC BY-SA 3.0,"<p>I conducted an experiment in which participants listened to sentences while looking at pictures about the sentences on a computer screen. Whether at a given time point a participant looked at the left half of a picture or the right half of a picture was recorded via an eye-tracker. </p>

<p>The study had a 2 by 2 within subjects and within items design. If time was not an issue here, I could use the model below (using <code>R</code> code). (Here, I assume varying random intercepts and random slopes for both subject and item.)</p>

<pre><code>lmer(look ~ iv1 * iv2 + (1 + iv1*iv2 | subject) + (1 + iv1*iv2 | item), 
     family = ""binomial"", data=data)
</code></pre>

<p>However, given that participants listened to sentences each of which lasted a few seconds, where they looked at (left vs. right) would vary across time. So one way to model time might be to include <code>time</code> as a covariate (it may even be necessary to natural polynomials):</p>

<pre><code>lmer(look ~ iv1 * iv2 + time + (1 + iv1*iv2 | subject) + (1 + iv1*iv2 | item), 
     family = ""binomial"", data=data)
</code></pre>

<p>But things get complicated since (1) it is possible that <code>time</code> interacts <code>v1</code> and <code>v2</code> and (2) I probably need to somehow model <code>time</code> in the random effect terms. An additional complication is that eye-tracking data is fairly large. The particular set of data that I am working with currently has 5 million rows, so running even the simplest multilevel logistic regression can be fairly time consuming.</p>

<p>So my question is, given my design, what would be a good way to model time. </p>
",2013-10-19 02:44:35.453
57814,22705.0,2,,57807.0,,,,CC BY-SA 3.0,"<p>Just to add a little more clarity. this approach is sometimes called Temporal Bayesian Models. I have seen it being used in atleast one other situation of marketing mix models where today's marketing spend influences today's brand &amp; revenue. Today's brand also influences tomorrow's brand &amp; revenue and so on.</p>
",2013-10-19 02:47:17.707
57815,22705.0,2,,54622.0,,,,CC BY-SA 3.0,"<p>If we define levels differently, mixed models may not be able to do what Bayesian models can. The other alternative is structural equation modeling.</p>

<p>For e.g When variables A,B drive C. Variables A,B,C,D, drive E.</p>

<p>You have equation : E = oA + pB + qC + rD + z ;<br>
                    C = mA + nB + k ; <br> <br>where all small letters are parameter estimates/error terms k,z</p>

<p>Mixed models cant be used in this case since the errors z,k could be correlated. Thereby, we have to resort to hierarchical Bayesian models or structural equation models.</p>

<p>Thoughts?</p>
",2013-10-19 03:01:58.997
57816,19559.0,1,57821.0,,,Avoiding p-values and reporting t-values instead. References?,<hypothesis-testing><references><p-value><lme4-nlme>,CC BY-SA 3.0,"<p>I was wondering if I could get some opinions on an issue. I'm analyzing my data using mixed-effects modeling in R (lme4 package). My model has by-subject and by-item intercepts and slopes, and random correlation parameters between them. Since the current version of lmer() does not have MCMC sampling implemented, I cannot get a pvalue for the coefficients in the model. Therefore, I would like to report the <strong>t-value</strong> instead.</p>

<p>I have often seen papers in my field (psycholinguistics) just say something like ""In all models presented, |t| > 2 and |z| > 2 correspond to a significant effect at a significance level of .05"". I was wondering whether there is some reference I can provide from this type of sentences? I understand that this tends to be the case, but I wonder whether this is something that has been shown (and I should give references) or whether it is ok to just state it and assume everyone will be ok with it.</p>

<p>Suggestions welcome! </p>
",2013-10-19 03:15:22.560
57817,22785.0,1,,,,What's the value of information when we decrease the entropy of a probability distribution?,<estimation><entropy><value-of-information>,CC BY-SA 3.0,"<p>Suppose you have to choose between actions $A_1,\dots,A_n$. You have a probability distribution over each $U(A_i)$, i.e. over the utility of choosing each action. So you should choose the $A_i$ that maximizes $\mathbf E[U(A_i)]$.</p>

<p>But now you are given the opportunity to reduce your uncertainty about one of these $U(A_i)$. I think the value of this new evidence should be</p>

<p>$$\int_{e\in E}P(e)~\mathbf E[U'(A_i)]~\text{d}e$$</p>

<p>where $E$ contains every possible piece of evidence you could receive, $P(e)$ is how likely it is, and $U'(A_i)$ is the utility of action $A_i$ after you receive the evidence $e$.</p>

<p>And I know I'm pushing it, but: can you think of some way to <em>roughly estimate</em> the value of reducing entropy, given only the expectation and current entropy of each $U(A_i)$, and the amount by which you will reduce the entropy of one particular $U(A_i)$? For instance, if $U(A_1)$ has expectation 20 and entropy 10 bits, and you're given the same information about every other $A_i$, how much would you pay to reduce the entropy of $U(A_i)$ by 2 bits?</p>
",2013-10-19 03:50:43.880
57818,22524.0,1,,,,Anisotropy in kriging for non gridded data,<r><self-study><spatial>,CC BY-SA 3.0,"<p>I have to perform a mapping of a DVB-T field (the TV signal), per every location I consider the median in
 time of the measurements, there are some issues, e.g. the variance seems to be proportional 
to the intensity of the field because of the measuring instrumentation.</p>

<p>My first idea was to use kriging. </p>

<p>Following R's <a href=""http://cran.r-project.org/web/packages/gstat/vignettes/gstat.pdf"" rel=""nofollow noreferrer"">gstat package vignette</a> I have plotted the directional variograms and one of the variables that are measured shows a clear anisotropy. </p>

<ol>
<li><p>How do I deal with it? Is there a favourite framework? </p></li>
<li><p>Looking at literature a change in 
variable is suggested but 
since my data are not in a grid I do not know how to deal with it.</p>

<p>I have looked at this post 
<a href=""https://stats.stackexchange.com/questions/35316/problems-estimating-anisotropy-parameters-for-a-spatial-model/35318#35318"">Problems estimating anisotropy parameters for a spatial model</a></p></li>
<li><p>How do you know from the cloud variogram if there are enough points to consider aisotropy?</p>

<p>In a second dataset which is less numerous a low number of samples for a range of distance, that is lower than the range corresponds to 
a drop in the variogram. I am not sure if this is a phisiological drop due to the field or if it is due to the low-sampling. </p></li>
<li><p>Is there any way to find out?</p></li>
<li><p>For doing this kind of analysis I am using gstat in R. Is geoR a better framework? Or is there a better package in absolute?</p>

<p>There are two measurement campaigns, one with 110 locations, the other with 35, in a territory which is approximately 4 km * 4 km, but I have also simulated the field, so from the simulated field I can extract as many samples as I want.</p></li>
</ol>

<p>Any pointer to relavant literature is very welcome.</p>

<p>At the moment I am reading [Webster, Oliver] <em>Geostatistics for Environmental Scientists</em>.</p>
",2013-10-19 08:04:24.787
57819,15827.0,2,,57812.0,,,,CC BY-SA 3.0,"<p>Although you are sensibly keeping a careful eye on what fits, your approach can fairly be described as rather home-grown or <em>ad hoc</em>. Depending on your target audience or readership, the consequence may range from practitioner puzzlement to statistician flak. </p>

<p>Your general model I take to be a sum of power functions. With some change of notation that could be  </p>

<p>$y = b_0 + \sum_{j=1}^J b_j x_j^{k_j}$ </p>

<p>with additive error. As @Glen_b comments, the usual approach to fitting such a model would be to use nonlinear least squares, which I take to be well supported in SAS, although I can't advise on details. </p>

<p>In many ways a simpler model is a multiplicative power function </p>

<p>$y = B_0 \prod_{j=1}^J x_j^{b_j}$, from which </p>

<p>$\ln y = b_0 + \sum_{j=1}^J b_j \ln x_j$, where $\ln B_0 =: b_0$. </p>

<p>That model is easy to fit by least squares as it is just multiple regression on the logged variables. Error with this is taken to be multiplicative on the original scale and additive on a logarithmic scale, which is often about right. A virtue of this model, as with power functions taken singly, is that it can be consistent with the limiting behaviour that $y$ tends to 0 as all the $x$s tend to 0, often important economically (physically, biologically). (However, the assumption is that all data are positive. Your own approach appears consistent with occasional zeros but not with negative values.) </p>

<p>More generally, however, we have no sight of your data and only a hint of what the regressors or predictors are, so it is difficult to say much more except to guess that your response variable is probably something zero or positive. If so,  it is helpful to ensure that predictions are always positive for all data points. I can't see that is guaranteed by your present approach. </p>

<p>On a terminology question: I'd advise against calling any of these models a polynomial, even if  you spell out very clearly that the powers are in general not integers. Either people don't know what a polynomial is or they will expect the powers to be integers, at least as a mathematical default: there is some obscurity either way, better avoided. </p>

<p>Edit: </p>

<p>The model </p>

<p>$\ln y = b_0 + \sum_{j=1}^J b_j \ln x_j$</p>

<p>can naturally be complicated according to taste and need, e.g. </p>

<p>$\ln y = b_0 + \sum_{j=1}^J b_j \ln x_j + \text{extra terms}$,</p>

<p>where the extra terms could be in the $x_j$, the $\ln x_j$ or both. </p>
",2013-10-19 08:25:15.977
57820,15827.0,2,,55617.0,,,,CC BY-SA 3.0,"<p>In principle, you are right to worry that the response is bounded. In practice, with this kind of data, you are unlikely to get predictions beyond the observed range of the data. This won't be your fault, but just the effect of the high degree of unpredictability with firm-level data. </p>

<p>Put it this way: The worst you can get is that no predictors really help, in which case the model will predict the average rank for every firm, at least to a good first approximation. In practice, you hope you can do better, but there is little reason to expect that predictions will be outside the observed range. (Or is there?) </p>

<p>But why predict rank at all? Why not try to predict some performance measure, and then rank the predictions, and then compare with the expert's ranks? That sounds much less problematic. </p>
",2013-10-19 08:37:45.077
57840,10135.0,2,,57835.0,,,,CC BY-SA 3.0,"<p>You should not make any inference when the variance of your residuals are not constant. See for example page 243 of <a href=""http://books.google.ca/books?id=iCTlfWYOmzcC&amp;pg=PA243&amp;dq=non%20constant%20variance%20regression%20validity&amp;hl=en&amp;sa=X&amp;ei=rMtiUqTZEOmMyAGfrYHwAQ&amp;ved=0CDQQ6AEwAQ#v=onepage&amp;q=non%20constant%20variance%20regression%20validity&amp;f=false"" rel=""nofollow"">Practical data analysis</a>. This is mainly because the estimated standard errors of your coefficients are not reliable. And that makes your $t$-test (or $F$-ratio) invalid.</p>
",2013-10-19 18:19:31.100
57841,1412.0,2,,57832.0,,,,CC BY-SA 3.0,"<p>(Chain rule for differentiation:) You should post the function $\alpha()$. </p>

<p>If it were $(x_1+x_2 + ...x_n)^2$ , and $\theta$ were a $\beta$ coefficient in a regression, then $\frac{\partial \alpha}{\partial \theta}$ would be equal to that coefficient. The full derivative would be $2*\beta*(x_1+x_2 + ...x_n)$. The 2 term comes from differentiating $\alpha()$ w.r.t. $(x_1+x_2 + ...x_n)$</p>
",2013-10-19 18:27:56.793
57821,20120.0,2,,57816.0,,,,CC BY-SA 3.0,"<p>A reference can be found in footnote 1 of Baayen, R. H., Davidson, D. J., &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390â€“412. I'm quoting the relevant bits here:</p>

<blockquote>
  <p>For data sets characteristic for studies of memory and language, which typically comprise many hundreds or thousands of observations, the particular value of the number of degrees of freedom is not much of an issue. Whereas the difference between 12 and 15 degrees of freedom may have important consequences for the evaluation of significance associated with a t statistic obtained for a small data set, the difference between 612 and 615 degrees of freedom has no noticeable consequences. For such large numbers of degrees of freedom, the t distribution has converged, for all practical purposes, to the standard normal distribution. For large data sets, significance at the 5% level in a two-tailed test for the fixed effects coefficients can therefore be gauged informally by checking the summary for whether the absolute value of the t-statistic exceeds 2.</p>
</blockquote>

<p>As you see, he describes it as ""informal"".</p>

<p>Generally, I assume you will find many people encourage you to report more informative measures than the probability of the data given an effect of exactly zero; for example, confidence intervals/HPD intervals of standardised effect sizes.</p>
",2013-10-19 08:56:42.117
57822,22381.0,1,57823.0,,,What is a loss function in decision theory?,<bayesian><terminology><loss-functions><decision-theory>,CC BY-SA 3.0,"<p>My notes define a loss function as the 'cost' incurred when the true value of $\theta$ is estimated by $\hat\theta$. What kind of cost is it talking about? monetary cost? or is it something related to errors?</p>
",2013-10-19 11:19:33.860
57823,15827.0,2,,57822.0,,,,CC BY-SA 3.0,"<p>A <em>loss function</em> is a mathematical representation of anything bad or at least undesirable: the point is that it is therefore something you want to minimise. </p>

<p>Calling a loss function a <em>cost</em> is in general just terminology designed to be simple and evocative. The intention is to appeal to your sense that cost is something you want to avoid and (specifically and crucially) that (other things being equal) you prefer a smaller cost to a larger cost. </p>

<p>It doesn't necessarily imply a cost in any monetary, financial, economic or business sense. </p>

<p>Simple examples of a loss function arise when we consider the difference between some true or correct value $\theta$ and an estimate $\hat\theta$, which you would like to be as small as possible. Possible ways of taking that further are to work with $(\theta - \hat\theta)^2$ or $|\theta - \hat\theta|$, which are both loss functions. In either case there is a minimum loss of 0 when $\hat\theta = \theta$.  </p>
",2013-10-19 11:30:55.430
57824,13895.0,2,,34166.0,,,,CC BY-SA 3.0,"<p>I just re-tripped across this. I've refined some of my thoughts since that last post, and thought I might find a receptive audience for them here.</p>

<p>First off, on the philosophy of how to address such a controversy: Say arguments A and B exist. Each has a premise, a sequence of deductions, and a result; and the results differ.</p>

<p>The best way way to prove one argument is incorrect is to invalidate one of its deductions. If that were possible here, there wouldn't be a controversy. Another is to disprove the premise, but you can't do that directly. You can argue for why you donâ€™t believe one, but that won't resolve anything unless you can convince others to stop believing it.</p>

<p>To prove a premise wrong indirectly, you have to form an alternate sequence of deductions from it that leads to an absurdity or to a contradiction of the premise. The fallacious way is to argue that the opposing result violates your premise. That means that one is wrong, but it doesn't indicate which.</p>

<p>+++++</p>

<p>The halfer's premise is ""no new information."" Their sequence of deductions is empty - none are needed. Pr(Heads|Awake) = Pr(Heads)=1/2.</p>

<p>The thirders (specifically, Elga) have two premises - that Pr(H1|Awake and Monday) = Pr(T1|Awake and Monday), and Pr(T1|Awake and Tails) = Pr(T2|Awake and Tails). An incontrovertible sequence of deductions then leads to Pr(Heads|Awake) = 1/3.</p>

<p>Note that the thirders don't ever assume there is new information - their premises are based on whatever information exists - ""new"" or not - when SB is awake. And I've never seen anyone argue for why a thirder premise is wrong, except that it violates the halfer result. So the halfers have provided none of the valid arguments I've listed. Just the fallacious one.</p>

<p>But there are other deductions possible from ""no new information,"" with a sequence of deductions that start with Pr(Heads|Awake) = 1/2. One is that Pr(Heads|Awake and Monday) = 2/3 and Pr(Tails|Awake and Monday) = 1/3. This does contradict the thirder premise, but like I said, that doesnâ€™t help the halfer cause since it still could be their premise that is wrong. Ironically, this result does prove something - that the halfer premise contradicts itself. On Sunday, SB says Pr(Heads|Monday) = Pr(Tails|Monday), so adding the information ""Awake"" has allowed her to update these probabilities. It is new information.</p>

<p>So I have proven the halfer premise can't be right. That doesn't mean the thirders are right, but it does mean that halfers have not provided any contrary evidence.</p>

<p>+++++</p>

<p>There is another argument I find more convincing. It isn't completely original, but I'm not sure if the proper viewpoint has been emphasized enough. Consider a variation of the experiment: SB is always wakened on both days; usually it is in a room that is painted blue, but on Tuesday after Heads it is in a room that is painted red. What should she say the probability of Heads is, if she finds herself awake in a blue room?</p>

<p>I donâ€™t think anybody would seriously argue that it is anything but 1/3. There are three situations that could correspond to her current one, all are equally likely, and only one includes Heads.</p>

<p>The salient point is that there is no difference between this version, and the original. What she ""knows"" - her ""new information"" - is that it is not H2. It does not matter how, or <strong><em>IF</em></strong>, she would know it could be H2 if it could. Her capability to observe situations that she knows do not apply is irrelevant if she knows they do not apply.</p>

<p>I can not believe the halfer premise. It is based on a fact - that she can't observe H2 - that cannot matter since she can, and does, observe that it isn't H2.</p>

<p>So I hope that I have provided a convincing argument for why the halfer premise is invalid. Along the way, I know I have demonstrated that the thirder result must be correct.</p>
",2013-10-19 12:13:07.783
57825,2666.0,2,,55617.0,,,,CC BY-SA 3.0,"<p>Ordinal regression is ideal for this problem in my opinion.  There is no problem other than computational burden caused by having as many unique $Y$ as there are observations.  The R <code>rms</code> package's <code>orm</code> function solves the computational burden problem using a special sparse matrix representation.  For an example see <a href=""https://stats.stackexchange.com/questions/65548/which-model-should-i-use-to-fit-my-data-ordinal-and-non-ordinal-not-normal-an/65668#65668"">Which model should I use to fit my data ? ordinal and non-ordinal, not normal and not homoscedastic</a></p>
",2013-10-19 12:39:35.157
57826,22853.0,1,,,,Joint distribution of two multivariate normal distributions,<normal-distribution><multivariate-analysis><independence><joint-distribution>,CC BY-SA 3.0,"<p>If we define 2 independent variables $Y_1$ and $Y_2$ as follows:
\begin{align} 
Y_1 &amp;= (Y_{11},Y_{12},Y_{13})^T \sim\mathcal N_3(\mu_1,\Sigma_{11}),  \\
Y_2 &amp;= (Y_{21},Y_{22})^T        \sim\mathcal N_2(\mu_2,\Sigma_{22})
\end{align}
where,
\begin{align}
\mu_1 &amp;= (2, 2, 2)^T  &amp;\Sigma_{11} &amp;= 
\left[\begin{array}{ccc}  3 &amp;1 &amp;0 \\ 1 &amp;2 &amp;0 \\ 0 &amp;0 &amp;3  \end{array}\right]  \\
\mu_2 &amp;= (3, 4)^T     &amp;\Sigma_{22} &amp;= 
\left[\begin{array}{cc}  4 &amp;2 \\ 2 &amp;4  \end{array}\right]
\end{align}</p>

<p>Then how can I find the joint distribution of $Y_{11}-Y_{13}+Y_{22}$ and $Y_{21}-Y_{12}$?</p>

<p>I know its a simple question but I could find if it was asked for $Y_1-Y_2$ or something. How am I supposed to solve it when it is like that?</p>
",2013-10-19 13:26:36.807
57827,17635.0,2,,57287.0,,,,CC BY-SA 3.0,"<p>The recommended number of bootstrap replications may vary according to ""the test to be run on the data""(Mooney, C. Z., and R. D. Duval. Bootstrapping: A Nonparametric Approach to Statistical Inference. Newbury Park, CA: Sage. 1993:11), For standard errors, the same source recommends 50-200 bootstrap replications.</p>
",2013-10-19 13:39:09.630
57828,22856.0,2,,57816.0,,,,CC BY-SA 3.0,"<p><a href=""http://jnnp.bmj.com/content/early/2013/08/02/jnnp-2013-305114.abstract"" rel=""nofollow"">Here</a> is an example of the reporting to which you refer.<br>
We took the Baayen article seriously and report only AIC differences and t-values in a relatively large analysis of several variables. We emphasized effect sizes and no p-values appear!</p>
",2013-10-19 14:40:30.983
57829,4854.0,2,,57766.0,,,,CC BY-SA 3.0,"<p>Utility is an ordinal concept, so the utility of a particular level can only be interpreted relative to something else.  In your example, the utility of ""blue"" should be interpreted relative to, say, ""red.""  </p>

<p>Since most of your attributes are categorical rather than continuous, you've probably used dummy or effects coding in your model (and if you haven't, you should).  If you've used dummy coding, the utility of each design-coded parameter can be interpreted relative to the excluded reference level.  For example, if you used ""no keyboard"" as the reference level for the keyboard attribute, the coefficient on the ""with keyboard"" parameter represents the incremental utility associated with adding a keyboard.  The same applies to the colour attribute: the coefficient on the different dummy-coded parameters represents the incremental utility of a particular colour relative to the excluded colour.</p>

<p>Note that the interpretation of the coefficients on dummy-coded (0,1) and effects-coded (-1,1) parameters are slightly different.  The coefficient on an effects coded parameter represents the deviation of the â€˜level mean utilityâ€™ from â€˜overall mean utilityâ€™, which not necessarily the same as the difference from the reference level as with a dummy-coded parameter.  Dummy-coded parameters are a little easier to interpret, but you run into problems with confounding between the utility of the reference levels and the alternative-specific constant (i.e. the intercept).  This is a particular problem if you're using a labelled design, but arguably less critical in a generic design.  See this reference for a useful summary... <a href=""http://methodology.psu.edu/media/techreports/12-120.pdf"" rel=""nofollow"">http://methodology.psu.edu/media/techreports/12-120.pdf</a>.  There is also a good summary in <a href=""http://rads.stackoverflow.com/amzn/click/0521605776"" rel=""nofollow"">Hensher, Rose and Greene</a>.</p>
",2013-10-19 14:51:57.300
57892,503.0,2,,57887.0,,,,CC BY-SA 3.0,"<p>I'd say regular ordinary least square regression should be fine, although technically, your dependent variable is a count, so you should use Poisson or negative binomial regression.  But with counts this high, I would guess that OLS regression would give similar results</p>
",2013-10-20 19:59:25.313
57830,22092.0,1,,,,What is the test statistic in Kolmogorovâ€“Smirnov test?,<statistical-significance><kolmogorov-smirnov-test>,CC BY-SA 3.0,"<p>I am reading the Wikipedia page on the <a href=""http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"" rel=""nofollow noreferrer"">Kolmogorov-Smirnov test</a>, specifically the section titled <a href=""http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution"" rel=""nofollow noreferrer"">Kolmogorov_distribution</a>.</p>

<p>$x$ is $D_\text{max}$ in the CDF $Pr(k\leq x)$?</p>

<p>My question is the number of points matters too,right?</p>

<p>When $n$ is large, $x$ approximates $\sqrt n D_\text{max}$ ?</p>

<p>Here is a snapshot of the section from Wikipedia:</p>

<p><img src=""https://i.stack.imgur.com/4nNN2.png"" alt=""enter image description here""></p>

<hr>

<p><img src=""https://i.stack.imgur.com/JIyns.jpg"" alt=""enter image description here""></p>

<p>Q1<br/>
How do we obtain the distribution of $D_n$ when $n$ is fixed?</p>

<p>Q2<br/>
If I get the value of $D_\text{max}$ and the sample size is $n$, I have to calculate $Pr(K&lt;=x)$, right? Is $D_\text{max}$ the $x$ in the formula on the Wikipedia page?</p>

<p>By formula 14.3.9 of <em>Numerical Recipes</em>, we should calculate a value got from the expression in the brackets - should that be the $x$? The value approximates $\sqrt n D_\text{max}$ when $n$ is large.<br/> </p>

<p>I am totally confused here.</p>

<p>Q3<br/>
We make tests and get a distribution,right? Could you please explain your figure in a ""test"" way? Maybe that is easier to follow. </p>

<hr>

<p>I calculate several values(significance)and compare them with the table<a href=""http://www.cas.usf.edu/~cconnor/colima/Kolmogorov_Smirnov.htm"" rel=""nofollow noreferrer"">ks table</a></p>

<p>The $x$ in  $Pr(K&lt;=x)$ is not $D_{max}$ .If sample size is large,x is $(\sqrt{n}+0.12+0.11/\sqrt{n})*D_{max}$ .That means there is no error in 14.4.9 of NR . The significance we want to get is determined by sample size and $D_{max}$ .</p>

<p>Please let me explain my questions in a ""test"" way.</p>

<p>Suppose sample size is 30, we obtain a dataset of 30 points and we can get a $D_{max}$ from the comparison between the empirical distribution of the sample and the reference probability distribution. We do it 1000 times and we get 1000 $D_{max}$ . There is a distribution for $D_{max}$ ,right?</p>

<p>From your figure , there should be 1000*0.01 points with a $D_{max}$ larger than 0.29 and 1000*0.05 points with a $D_{max}$ larger than 0.24 ,more or less .  </p>

<hr>

<p>Like you said, I am very confused. Please let me make sure the several statements below are right or wrong first. </p>

<p>When sample size is large,my calculation method is right?<br>
we can take $(\sqrt{n}+0.12+0.11/\sqrt{n})*D_{max}$  as x to input to $Pr(K&lt;=x)$ and in this way we can get the significance when sample size is large,right?</p>

<p>Suppose sample size is 30, we obtain a dataset of 30 points and we can get a $D_{max}$ from the comparison between the empirical distribution of the sample and the reference probability distribution. We do it 1000 times and we get 1000 $D_{max}$ . There is a distribution for $D_{max}$ ,right?</p>

<p>From your figure , there should be 1000*0.01 points with a $D_{max}$ larger than 0.29 and 1000*0.05 points with a $D_{max}$ larger than 0.24 . Is that your figure tells us ?</p>
",2013-10-19 15:25:20.260
57831,21884.0,1,,,,Local linear regression: number of grid points?,<regression><nonparametric><local-statistics>,CC BY-SA 3.0,"<p>Local linear regression is a popular tool. How can one choose the number of grid points for which to estimate the unknown function on?</p>
",2013-10-19 15:56:12.137
57832,9456.0,1,,,,Confusion related to calculation of partial derivative,<gradient-descent>,CC BY-SA 3.0,"<p>I have this function $P = f(\alpha)$. $\alpha$ is a function $\alpha = f(\theta, x)=\theta x$. Now I have</p>

<p>$P = \alpha(x_1+x_2 + ...x_n)$</p>

<p>Now I need to calculate the partial derivative of P wrt $\theta$. Then</p>

<p>$\frac{\partial P}{\partial \theta} = \frac{\partial P}{\partial \alpha} * \frac{\partial \alpha}{\partial \theta} = (x_1+x_2+x_3)*\frac{\partial \alpha}{\partial \theta}$</p>

<p>Now what I am trying to do is use gradient descent to maximize P wrt $\theta$. So what I was expecting was the partial derivative wrt $\theta$ to be a single value. However, Here $\frac{\partial \alpha}{\partial \theta} = \frac{\partial \theta x}{\partial \theta}$ changes wrt to x. So I am a bit confused how to get a single value out of it?</p>
",2013-10-19 16:25:33.063
57833,22860.0,2,,541.0,,,,CC BY-SA 3.0,"<p>ANOVA you are testing whether there are significant difference between the population means assuming you are comparing more than two population means, then you are going to use an F test. </p>

<p>In regression analysis you build a model between independent variables and a dependent variable. If you have one independent variable with four levels you can use three dummy variables and run a regression model. The F-test for the regression model which is used to test for the significance of the regression model is the same as the F which you get when testing for the difference between the population means. If you run a stepwise regression then some of the dummy variables might be dropped from the model and your F-value will differ from that when you perform ANOVA test.</p>
",2013-10-19 16:30:46.430
57834,15860.0,2,,57756.0,,,,CC BY-SA 3.0,"<p>The use of terminology differs across disciplines. I'm answering as a political scientist.  </p>

<p>Your study would only be a (quasi-)experimental design if there were some aspect of randomization in assignment to treatment. Based on the information you've provided, you are dealing with an observational study because the units self-assigned to treatment. This means that there are measurable and unmeasurable features of the observations that differ systematically between units in G1 and G2. In a school setting, there are essential features of students who go to state schools and those who go to private schools. Ideally (but probably not practically) you could control for these differences. I am skeptical because there are probably unmeasurable differences between the populations, such as motivation, family support and upbringing. These are things you cannot control for.</p>

<p>The key feature of any experiment is random assignment to treatment. If units are selected at random to receive treatment and then all units adhere to their assignment, then in expectation all treatment units and control units will be similar in their pre-treatment characteristics. Then the researcher can credibly infer that any post-treatment differences are due to the effect of treatment.</p>

<p>An ""experiment"" involves an intervention by a researcher that randomly assigns treatment. In my line of work, random selection is only possible in a very limited range of research designs. As a result, I do not consider it a requirement for a ""good"" or ""true"" experiment in my field.</p>

<p>A ""natural experiment"" involves randomized assignment to treatment through an intervention executed by something (human or not) other than the researcher. </p>

<p>In my experience, ""quasi-experiment"" refers to a research design where there is some aspect of randomness to assignment to treatment but not to a degree where the researcher believes that treatment and control units are similar in expectation in terms of pre-treatment features. This term, however, is quite vague and subject to different meanings. In my field people advise against its use. I've heard some claim it's too vague to be useful, obscuring more than it reveals about the research design. I've also heard some claim that ""quasi-experiment"" is just a euphemism for ""bad experiment."" I agree with the former sentiment and tend to disagree with the latter. </p>

<p>An ""observational study"" involves no randomized assignment to treatment. In these settings a researcher must try to  establish ""conditional ignorability."" The term means that after controlling for a set of covariates $X$, a unit's potential outcomes are conditionally independent from assignment to treatment.  In other words, the treatment assignment and the potential outcomes are conditionally independent given $X$ if and only if, given knowledge that $X=x$, knowledge of whether $T=\{0,1\}$ provides no information on the likelihood of a particular outcome in $Y$.</p>

<p>That is, conditional on the covariates, the treatment is independent of the potential outcomes.</p>

<p>\begin{equation}
(Y(T=1),Y(T=0))âŠ¥T|X
\end{equation}</p>

<p>A ""correlational <em>study</em>"" would probably not be of much value. A ""correlational <em>analysis</em>,"" however, probably means that you present exploratory data analysis that shows the unadjusted relationship between variables. In other words, <em>without any claim of causality</em>, you are showing how variables tend to move together in your data set.</p>

<p>The formalization above is based on the Rubin potential outcomes framework for causal inference, a foundational work for this conversation. You need to read it if you're serious about moving forward with work like this.
Holland, Paul W. 1986. â€œStatistics and Causal Inference.â€ <em>Journal
of the American Statistical Association.</em> 81(396): 945-960.</p>

<p>Another approach to conditional ignorability is Judea Pearl's back-door criterion.</p>
",2013-10-19 16:31:02.250
57835,6384.0,1,,,,Nonconstancy of error variance,<regression><multiple-regression>,CC BY-SA 3.0,"<p>If the residuals show that the non constancy of the error variance is clearly present, does it mean that your regression results are completely invalid?</p>
",2013-10-19 16:35:27.303
57836,19559.0,1,57862.0,,,Alternatives to pvals.fnc to compute confidence intervals for fixed effects?,<r><confidence-interval><mixed-model><lme4-nlme>,CC BY-SA 3.0,"<p>Lately I keep encountering the same problem and I'm wondering whether other people have been able to get around it. I'm running a mixed effects model using <strong>lmer()</strong>. My model has by-subject and by-item intercepts and slopes, and random correlation parameters between them. Since the current version of lmer() does not have MCMC sampling implemented, I cannot use pvals.fnc(). I get this message:</p>

<pre><code>Error in pvals.fnc(m, withMCMC = T) : 
MCMC sampling is not implemented in recent versions of lme4
for models with random correlation parameters
</code></pre>

<p>pvals.fnc() is also the function I use to get confidence intervals (<strong>HPD95lower</strong> and  <strong>HPD95upper</strong> were two columns in the pvals.fnc output). Does anyone know of an alternative way of getting confidence intervals for the fixed effects estimates in the model? Or does using models with random correlations means that we can no longer get CIs from R? </p>

<p>Thanks!</p>

<p><strong>NOTE</strong>: I've seen this question asked in other forums in slightly different ways. However, the answers always seem to involve (1) calculating something different as an alternative to the confidence intervals, (2) some complicated solution that is unclear  (at least to me) how to implement. I would like to know if there is some alternative way of computing CIs that is both mainstream (so that other researchers can use it) and has a function to do it in R, since I am not a programmer and I feel that trying to create that function myself would be error prone. </p>
",2013-10-19 16:39:24.940
57837,20700.0,1,,,,Gibbs sampler for local linear trend model,<bayesian><sampling><gibbs><state-space-models>,CC BY-SA 3.0,"<p><strong>Question:</strong> Consider the local linear trend model given by:
\begin{align*}
y_t = \mu_t + \tau \varepsilon_t \ \cdots \ \text{Observation equation} \\
\mu_{t+1} = \phi \mu_t + \eta_t \  \cdots \ \text{State equation}
\end{align*}
for $t = 1, 2, \cdots, T$, where $(\varepsilon_t, \eta_t)'$ is independent of $\mu_k$ for $k \le t$ and where:
\begin{align*}
\begin{bmatrix} \varepsilon_t \\ \eta_t \end{bmatrix} \stackrel{i.i.d}{\sim} N\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}\right) 
\end{align*}
and
\begin{align*}
\mu_1 \sim N\left(0, \frac{1}{1-\phi^2} \right)
\end{align*}</p>

<p>(Note everything is a scalar quantity in this question).</p>

<p>Consider a Bayesian analysis of this model under the prior distribution given by:
\begin{align*}
p(\theta) \propto \frac{1}{\tau}  \ \text{for} \ -\infty &lt; \phi &lt; \infty \ \text{and} \ 0&lt;\tau&lt;\infty
\end{align*}
where $\theta = (\phi, \tau)$.</p>

<p>Devise a Gibbs sampler to sample from the joint posterior distribution, $p(\mu_{1:T}, \theta \mid y_{1:T})$ where the notation $\mu_{1:T}$ denotes $(\mu_1, \mu_2, \cdots, \mu_T)$ and similarly, $y_{1:T}$ denotes $(y_1, y_2, \cdots, y_T)$. </p>

<hr>

<p><strong>My Working So Far:</strong> </p>

<p>The joint posterior distribution is given by:
\begin{align*}
 p(\mu_{1:T}, \theta \mid y_{1:T}) &amp; \propto p(\mu_{1:T}, \theta, y_{1:T}) \\
 &amp; = \underbrace{p(y_{1:T} \mid \mu_{1:T}, \theta)}_{\text{'likelihood'}}\underbrace{p(\mu_{1:T} \mid \theta)p(\theta)}_{\text{prior}} \\
&amp;  = \left[\prod_{t=1}^{T} p(y_t \mid \mu_t, \theta)\right]\left[\prod_{t=1}^{T-1}p(\mu_{t+1} \mid \mu_t, \theta) \right]p(\mu_1 \mid \theta)p(\theta) \ \ \cdots \ \ (1)
\end{align*}
Since $y_t \mid \mu_t, \theta \sim N(\mu_t, \tau^2)$ for $t=1, 2, \cdots, T$, the pdf is given by:
\begin{gather*}
p(y_t \mid \mu_t, \theta) = \left(2\pi \tau^2\right)^{-\frac{1}{2}}\exp\left[-\frac{1}{2\tau^2}\left(y_t - \mu_t\right)^2  \right]
\end{gather*}
Similarly, $\mu_{t+1} \mid \mu_t, \theta \sim N\left(\phi \mu_t, 1 \right)$, so the pdf is given by:
\begin{gather*}
p(\mu_{t+1} \mid \mu_t, \theta) = \left(2\pi\right)^{-\frac{1}{2}} \exp\left[-\frac{1}{2}\left(\mu_{t+1} - \phi \mu_t \right)^2 \right]
\end{gather*}
We know that $\mu_1 \mid \theta \sim N\left(0, \frac{1}{1-\phi^2} \right)$, so the pdf is given by:
\begin{gather*}
p(\mu_1 \mid \theta) = \left(2\pi\left(\frac{1}{1-\phi^2} \right) \right)^{-\frac{1}{2}} \exp\left[-\frac{\mu_1^2}{2\left(\frac{1}{1-\phi^2}\right)} \right]
\end{gather*}
Finally, we are given that $p\left(\theta\right) \propto \frac{1}{\tau}$.</p>

<p>Substituting all of the above into Eqn. $(1)$, yields the joint posterior distribution:
\begin{align*}
p(\mu_{1:T}, \theta \mid y_{1:T}) &amp; \propto \left[\prod_{t=1}^{T} \left(2\pi \tau^2\right)^{-\frac{1}{2}}\exp\left[-\frac{1}{2\tau^2}\left(y_t - \mu_t\right)^2  \right]\right]\left[\prod_{t=1}^{T-1}\left(2\pi\right)^{-\frac{1}{2}} \exp\left[-\frac{1}{2}\left(\mu_{t+1} - \phi \mu_t \right)^2 \right] \right] \\
&amp; \times \left(2\pi\left(\frac{1}{1-\phi^2} \right) \right)^{-\frac{1}{2}} \exp\left[-\frac{\mu_1^2}{2\left(\frac{1}{1-\phi^2}\right)} \right] \left( \frac{1}{\tau}\right)
\end{align*}</p>

<p>I will implement a ""blocked"" Gibbs sampler sampling $\mu_{1:T}^{(i)}$ together, as follows:</p>

<p>For $i = 1, 2, \cdots, M$, sample:
\begin{align*}
\mu_{1:T}^{(i)} &amp; \sim \mu_{1:T} \mid \phi^{(i-1)}, \tau^{(i-1)}, y_{1:T} \ \ \cdots \ \ (2)\\
\phi^{(i)} &amp; \sim \phi \mid \mu_{1:T}^{(i)}, \tau^{(i-1)}, y_{1:T} \ \ \cdots \ \ (3) \\
\tau^{(i)} &amp; \sim \tau \mid \mu_{1:T}^{(i)}, \phi^{(i)}, y_{1:T} \ \ \cdots \ \ (4)
\end{align*}</p>

<p>Sampling from $(2)$ is straightforward by using the Forward Filter Backwards Sampling <a href=""http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm"" rel=""nofollow"">(FFBS)</a></p>

<p><strong>My Query:</strong> I am stuck on how to sample from $(3)$ and $(4)$, in order to use a Gibbs sampler on $(3)$ and $(4)$, we need to find the full conditional of $\phi \mid \mu_{1:T}, \tau, y_{1:T}$ and $\tau \mid \mu_{1:T}, \phi, y_{1:T}$, but how do you find these full conditionals? I do not see any obvious way by examining the joint posterior distribution. Perhaps a Metropolis Hastings subchain could work? But then what should I pick for my candidate density for the $\phi$ and $\tau$?</p>

<p>Thanks in advance.</p>
",2013-10-19 16:55:12.357
57838,2873.0,2,,57835.0,,,,CC BY-SA 3.0,"<p>It depends on which results and what you mean by valid/invalid.</p>

<p>The coefficients are still a measure of a line going through the center of the data, so the line itself is still meaningful.</p>

<p>The relationship between the mean square error and the variance of the residuals becomes more complicated since there is not a single value that is the variance.  But if you can model the variance then you can get a meaningful relationship using weighted regression.</p>

<p>Standard prediction intervals (based on ordinary least squares, not weighted least squares) will be too narrow in some areas and too wide in other areas, so would probably not be considered valid.</p>

<p>Tests and confidence intervals based on the standard assumptions are not going to be exact any more so p-values and confidence intervals will be approximate, whether that approximation is close enough to consider them valid, or potentially bad enough to consider them invalid will depend on the amount the variance varies and your personal preferences.  Proper use of weighted least squares (or other methodologies) will help here as well.</p>
",2013-10-19 17:08:28.540
57839,22862.0,1,,,,"Trace(AB)=Trace(BA)? even if A, B are vectors?",<normal-distribution><linear-algebra>,CC BY-SA 3.0,"<p>I have read this vector manipulation in standard books:</p>

<p>$$E[XX^T] =E[\mathrm{trace}(XX^T)]$$</p>

<p>where $X^T$ is the transpose of $X$, $X$ has a normal distribution and has dimension $n\times 1$, $XX^T$ has dimension $n\times n$. </p>

<p>How can they introduce trace into expectation?</p>
",2013-10-19 17:44:55.213
57842,6162.0,2,,57802.0,,,,CC BY-SA 3.0,"<p>The <code>logLik()</code> function provides the evaluation of the log-likelihood by <em>substituting the ML estimates of the parameters</em> for the values of the unknown parameters. Now, the maximum likelihood estimates of the regression parameters (the $\beta_j$'s in $X{\boldsymbol \beta}$) coincide with the least-squares estimates, but the ML estimate of $\sigma$ is $\sqrt{\frac{\sum \hat\epsilon_i^2}{n}}$, whereas you are using $\hat\sigma = \sqrt{\frac{\sum \hat\epsilon_i^2}{n-2}}$, that is the square root of the unbiased estimate of $\sigma^2$.</p>

<pre><code>&gt;  n &lt;- 5
&gt;  x &lt;- 1:n
&gt;  set.seed(1)
&gt;  y &lt;- 10 + 2*x + rnorm(n, 0, 2)
&gt;  modlm &lt;- lm(y ~ x)
&gt;  sigma &lt;- summary(modlm)$sigma
&gt; 
&gt;  # value of the likelihood with the ""classical"" sigma hat
&gt;  sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma)))
[1] -9.192832
&gt; 
&gt;  # value of the likelihood with the ML sigma hat
&gt;  sigma.ML &lt;- sigma*sqrt((n-dim(model.matrix(modlm))[2])/n) 
&gt;  sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma.ML)))
[1] -8.915768
&gt;  logLik(modlm)
'log Lik.' -8.915768 (df=3)
</code></pre>
",2013-10-19 18:45:53.120
57843,22793.0,2,,57839.0,,,,CC BY-SA 3.0,"<p>A vector is a matrix which has one column or one row.</p>

<p>The property holds, you can test it up. </p>
",2013-10-19 18:50:33.893
57844,7155.0,2,,57757.0,,,,CC BY-SA 3.0,"<p>Let's define a kernel that is sensitive to translation, rotation and scaling of the input grid.</p>

<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $f(x,y) ={(2\mu_x\mu_y + c_1)(2\sigma_{xy}+c_2)\over{(\mu_x^2+\mu_y^2+c_1)(\sigma_x^2+\sigma_y^2+c_2)}}$</p>

<p>Where $c_1$ and $c_2$ are some small constant times the range of the data.</p>

<p>The guts of this function is the covariance between data, $\sigma_{xy}$. The more absolutely similar our values are, the higher this function goes.</p>

<p>It's a valid mercer kernel, which means it's a good distance function for many purposes.</p>

<p>Plotting your data below, we see some vaguely similar pattern over the grid $(1,2)$ to $(2,4)$. </p>

<p><img src=""https://i.stack.imgur.com/4kJoL.png"" alt=""Data""></p>

<p>With data indexed in a grid pattern, we aren't interested in small translations of the inputs. One such way to overcome these translations is to average over the many small shifts in the grid. This calculation wrt to above distance function is known as Structural Similarity Index.</p>

<p>For observation $A$ here's 9 3x3 square shifts in the grid.</p>

<p><img src=""https://i.stack.imgur.com/qA0gF.png"" alt=""Window Shifts""></p>

<p>We do the same for $B$, then compute the SSIM. One way to look at the impact of different parts of the grid is to look at the local gradient of SSIM wrt inputs.</p>

<p>Shown below is the gradient of SSIM computed over windows and not.</p>

<p><img src=""https://i.stack.imgur.com/HBnAc.png"" alt=""Gradients of Data""></p>

<p>$SSIM(3) = 0.46375$</p>

<p>$SSIM = 0.30504$</p>

<p>If you want to discard the information about scaling, we can $x\over{\sum_i x_i}$, yielding the following gradient.</p>

<p><img src=""https://i.stack.imgur.com/UKXDQ.png"" alt=""Normed Gradient""></p>

<p>$SSIM_{normed}(3) = 0.72070$</p>

<p>$SSIM_{normed} = 0.74618$</p>

<p>See: <a href=""http://www.cns.nyu.edu/~zwang/files/papers/ssim.html"" rel=""nofollow noreferrer"">Image quality assessment: From error visibility to structural similarity.</a></p>

<p>The calculations were done with <a href=""http://scikit-image.org/docs/dev/api/skimage.measure.html#structural-similarity"" rel=""nofollow noreferrer"">Skimage.measure.structural_similarity</a></p>
",2013-10-19 19:09:57.850
57845,20700.0,1,,,,Determining the posterior distribution for an Autoregressive or order 1 model,<self-study><bayesian><autoregressive><posterior>,CC BY-SA 3.0,"<p><strong>Question:</strong> For this question, note that the notation $y_{1:T} = (y_1, y_2, \cdots, y_T)$, ie, a vector of random variables.</p>

<p>Consider the following AR(1) model:
\begin{align*}
y_{t+1} = \phi y_t + \sigma \eta_t  \ \ \cdots (a)
\end{align*}
for $t = 1, 2, \cdots, T$ where
\begin{align*}
\eta_t \stackrel{iid}{\sim} N(0,1) \ \ \cdots (b)
\end{align*}
with $\eta_1$ independent of $y_k$ for $k \le t$, and where
\begin{align*}
y_1 \sim N\left(0, \frac{\sigma^2}{1-\phi^2}\right) \ \ \cdots (c)
\end{align*}
Define $\theta = (\phi, \sigma)$ and consider a prior distribution given by:
\begin{align*}
p(\theta) \propto \frac{1}{\sigma} \ \text{for} \ \infty &lt; \phi &lt; \infty \ \text{and} \ 0&lt;\sigma&lt;\infty \ \ \cdots (1)
\end{align*}
Also define the conditional likelihood function to be:
\begin{align*}
p(y_{2:T} \mid y_1, \theta) = \left(2\pi \sigma^2 \right)^{-\frac{T-1}{2}}\exp\left[- \frac{1}{2\sigma^2}\sum_{t=2}^T \left(y_t - \phi y_{t-1} \right)^2\right] \ \ \cdots (2)
\end{align*}</p>

<p>Show that the conditional posterior distribution, corresponding to the conditional likelihood function in $(2)$ and the prior density in $(1)$, may be obtained analytically, whereas the full posterior distribution, corresponding to the model in $(a)-(c)$ and the prior density in $(1)$, requires an alternative computational approach.</p>

<hr>

<p><strong>My Working:</strong> So firstly, I worked out the likelihood function corresponding to the AR(1) model described by $(a)-(c)$. My working is as follows:</p>

<p>We are given $y_1 \sim N\left(0, \frac{\sigma^2}{1-\phi^2}\right)$, so the pdf is given by:
\begin{align*}
p(y_1 \mid \theta) = \left(2\pi\left( \frac{\sigma^2}{1-\phi^2}\right) \right)^{-\frac{1}{2}}\exp\left[ - \frac{y_1^2}{2\left(\frac{\sigma^2}{1-\phi^2}\right)} \right]
\end{align*}
Conditionally, we have $y_2 \mid y_1 \sim N\left( \phi y_1, \sigma^2\right)$, so the pdf is given by:
\begin{align*}
p(y_2 \mid y_1, \theta) = \left(2\pi \sigma^2\right)^{-\frac{1}{2}} \exp\left[-\frac{1}{2\sigma^2} \left(y_2 - \phi y_1\right)^2 \right]
\end{align*}
Similarly, $y_3 \mid y_2, y_1 \equiv y_3 \mid y_2 \sim N\left(\phi y_2, \sigma^2\right)$, so the pdf is given by:
\begin{align*}
p(y_3 \mid y_2, y_1, \theta) = \left(2\pi \sigma^2\right)^{-\frac{1}{2}} \exp\left[-\frac{1}{2\sigma^2} \left(y_3 - \phi y_2\right)^2 \right]
\end{align*}
So in general, $y_t \mid y_{t-1}, y_{t-2}, \cdots, y_1 \equiv y_t \mid y_{t-1} \sim N\left(\phi y_{t-1}, \sigma^2\right)$, with pdf:
\begin{align*}
p(y_t \mid y_{t-1}, y_{t-2}, \cdots, y_1, \theta) = \left(2\pi \sigma^2\right)^{-\frac{1}{2}} \exp\left[-\frac{1}{2\sigma^2} \left(y_t - \phi y_{t-1}\right)^2 \right]
\end{align*}
Using the method of composition, we have:
\begin{align*}
p(y_{1:T} \mid \theta) &amp; = p(y_1 \mid \theta)p(y_2 \mid y_1, \theta)p(y_3 \mid y_1, y_2, \theta) \cdots p(y_T \mid y_1, y_2, \cdots, y_{T-1}, \theta) \\
&amp; = p(y_1 \mid \theta) \prod_{t=2}^T p(y_t \mid y_{t-1}, \theta)
\end{align*}
Thus the likelihood function computed for a given value of $\theta = \left(\phi, \sigma^2\right)$, is given by:
\begin{align}
 L(\theta) &amp; = \left\{ \left(2\pi\left( \frac{\sigma^2}{1-\phi^2}\right) \right)^{-\frac{1}{2}}\exp\left[ - \frac{y_1^2}{2\left(\frac{\sigma^2}{1-\phi^2}\right)} \right]\right\} \prod_{t=2}^T \left(2\pi \sigma^2\right)^{-\frac{1}{2}} \exp\left[-\frac{1}{2\sigma^2} \left(y_t - \phi y_{t-1}\right)^2 \right] \\
&amp;  \propto (1-\phi^2)^{\frac{1}{2}}\sigma^{-T} \exp\left[-\frac{\sum_{t=2}^T\left(y_t - \phi y_{t-1}\right)^2+y_1^2\left(1-\phi^2\right)}{2\sigma^2} \right] \ \ \cdots (W1)
\end{align}</p>

<p>Deriving the conditional likelihood given in $(2)$ can be done as follows:
\begin{align*}
p(y_{2:T} \mid y_1, \theta) &amp; = \frac{p(y_{1:T} \mid \theta)}{p(y_1 \mid \theta)} \\
&amp; = \prod_{t=2}^T p(y_t \mid y_{t-1}, \theta) \\
&amp; = \left(2\pi \sigma^2 \right)^{-\frac{T-1}{2}}\exp\left[- \frac{1}{2\sigma^2}\sum_{t=2}^T \left(y_t - \phi y_{t-1} \right)^2  \right]
\end{align*}</p>

<hr>

<p><strong>My Query:</strong> I don't really get what the question is trying to ask me to do? What does it mean by conditional posterior? Full posterior? Any assistance will be appreciated!</p>

<hr>

<p><strong>EDIT 1 PROGRESS:</strong> Okay, so I've played around a bit more and made a bit of progress. I interpret 'conditional posterior' as follows:</p>

<p>Notice that when $T$ is large, then the factor $(1-\phi^2)$ in Eqn. $(W1)$ is small, so we can approximate the full likelihood with the conditional likelihood:
\begin{align*}
p(y_{2:T} \mid y_1, \theta) \propto \sigma^{-(T-1)}\exp\left[- \frac{1}{2\sigma^2}\sum_{t=2}^T \left(y_t - \phi y_{t-1} \right)^2  \right]
\end{align*}
So under the prior $p(\theta) \propto \frac{1}{\sigma}$, we have:
\begin{align*}
p(\theta \mid y_{1:T}) &amp; \propto \sigma^{-T}\exp\left[- \frac{1}{2\sigma^2}\sum_{t=2}^T \left(y_t - \phi y_{t-1} \right)^2  \right] \\
&amp; = \sigma^{-T}\exp\left[-\frac{1}{2\sigma^2}\sum_{t=2}^T \left(y_t^2 -2y_t \phi y_{t-1} + \phi^2 y_{t-1}^2 \right) \right] \\
&amp; = \sigma^{-T}\exp\left[-\frac{1}{2\sigma^2}\left(\underbrace{\sum_{t=2}^Ty_t^2}_{C} - 2\phi\underbrace{\sum_{t=2}^Ty_t y_{t-1}}_{B} + \phi^2 \underbrace{\sum_{t=2}^T y_{t-1}^2}_{A} \right) \right]
\end{align*}
First, note that:
\begin{align*}
p(\phi \mid \sigma, y_{1:T}) &amp; \propto \exp\left[-\frac{1}{2\sigma^2}\left(C-2\phi B + \phi^2A \right) \right] \\
&amp; = \exp\left[-\frac{A}{2\sigma^2} \left(\phi^2 - 2\phi \frac{B}{A} + \frac{C}{A} \right) \right] \\
&amp; = \exp\left[-\frac{A}{2\sigma^2}\left(\left(\phi - \frac{B}{A} \right)^2-\left(\frac{B}{A}\right)^2 + \frac{C}{A} \right) \right] \\
&amp; \propto \exp\left[-\frac{A}{2\sigma^2}\left(\phi-\frac{B}{A}\right)^2 \right] \\
&amp; = \exp\left[-\frac{1}{2\left(\frac{\sigma^2}{A} \right)}\left(\phi-\frac{B}{A}\right)^2 \right]
\end{align*}
So the distribution of $\phi \mid \sigma, y_{1:T}$ is given by:
\begin{gather}
 \phi \mid \sigma, y_{1:T} \sim N\left(\frac{B}{A}, \frac{\sigma^2}{A} \right) \\
 \implies \phi \mid \sigma, y_{1:T} \sim  N\left(\frac{\sum_{t=2}^T y_t y_{t-1}}{\sum_{t=2}^T y_{t-1}^2}, \frac{\sigma^2}{\sum_{t=2}^T y_{t-1}^2} \right)
\end{gather}
Next, note that:
\begin{align*}
p(\sigma \mid y_{1:T}) &amp; \propto \int_{\phi} \sigma^{-T} \exp\left[-\frac{1}{2\sigma^2} \sum_{t=2}^T \left(y_t - \phi y_{t-1}\right)^2 \right]d\phi \\
&amp; = \sigma^{-T} \int_{\phi} \exp\left[-\frac{A}{2\sigma^2} \left(\left(\phi - \frac{B}{A} \right)^2 - \left(\frac{B}{A}\right)^2 + \frac{C}{A} \right) \right]d\phi \\
&amp; = \sigma^{-T} \int_{\phi} \exp\left[-\frac{A}{2\sigma^2} \left(\phi - \frac{B}{A} \right)^2 + \left(\frac{A}{2\sigma^2}\right) \left(\frac{B}{A}\right)^2 - \left(\frac{A}{2\sigma^2}\right)\left(\frac{C}{A}\right) \right]d\phi \\
&amp; = \sigma^{-T} \exp\left[\frac{B^2/A - C}{2\sigma^2} \right] \int_{\phi} \exp\left[-\frac{A}{2\sigma^2} \left(\phi - \frac{B}{A} \right)^2\right] d\phi \\
&amp; = \sigma^{-T} \exp\left[\frac{B^2/A - C}{2\sigma^2} \right] \left(2\pi\left(\frac{\sigma^2}{A} \right) \right)^{\frac{1}{2}} \\
&amp; \propto \frac{1}{\sigma^{(T-2)+1}}\exp\left[\frac{B^2/A - C}{2\sigma^2} \right]
\end{align*}
Now define $b = \frac{B}{A}$, notice:
\begin{align*}
Q(y_{2:T}, b) &amp; = \sum_{t=2}^T \left(y_t - b y_{t-1}\right)^2 \\
&amp; = \sum_{t=2}^T \left(y_t^2 - 2y_t b y_{t-1} + b^2 y_{t-1}^2 \right) \\
&amp; = \sum_{t=2}^T y_t^2 - 2b\sum_{t=2}^T y_t y_{t-1} + b^2 \sum_{t=2}^T y_{t-1}^2 \\
&amp; = C - 2bB+b^2A
\end{align*}
Then clearly,
\begin{align*}
-Q(y_{2:T}, b) = B^2/A-C
\end{align*}
So the distribution of $\sigma \mid y_{1:T}$ is given by:
\begin{gather}
\sigma \mid y_{1:T} \sim IG\left(v = T-2, \widehat{\sigma^2} = \frac{1}{T-2} \left(B^2/A - C \right) \right) \\
 \implies \sigma \mid y_{1:T} \sim IG\left(v = T-2, \widehat{\sigma}^2 = \frac{1}{T-2} \left(\frac{\left(\sum_{t=2}^T y_t y_{t-1} \right)^2}{\sum_{t=2}^T y_{t-1}^2} - \sum_{t=2}^T y_t^2 \right) \right)
\end{gather}
Or equivalently:
\begin{gather*}
\sigma \mid y_{1:T} \sim IG\left(v = T-2, \widehat{\sigma^2} = -\frac{1}{T-2}Q(y_{2:T}, b) \right)
\end{gather*}
Thus, we can derive the conditional posterior distribution analytically as:
\begin{align*}
p(\phi, \sigma \mid y_{1:T}) = p(\phi \mid \sigma,  y_{1:T})p(\sigma \mid y_{1:T})
\end{align*}
where $p(\phi \mid \sigma,  y_{1:T})$ and $p(\sigma \mid y_{1:T})$ are derived above.</p>

<hr>

<p>However, if we use the full likelihood (which is what the second part of the question is asking), how can we derive the joint posterior? I do not see any obvious ways to find the appropriate integrating constants. I'm assuming I need to use Gibbs/M-H/or some other kind of MCMC sampling scheme?</p>
",2013-10-19 19:57:12.857
57846,22863.0,1,,,,Does relative Kullback-Leibler divergence exist?,<multivariate-analysis><references><joint-distribution><kullback-leibler>,CC BY-SA 3.0,"<p>Suppose I have two multivariate normal distributions. I have computed the KL divergence ($d_{KL}(N_1, N_2)$).  Is there a way to measure a relative divergence between these two distributions?</p>

<p>For instance in a deterministic case, there is absolute error and relative error.
Let's say if KL divergence is analogous to absolute error, is there an analogous case for relative error. </p>

<p>Any references to literature would be highly appreciated!    </p>
",2013-10-19 20:08:06.560
57847,22864.0,1,,,,Can I regress an index value with variables used to create the index?,<regression>,CC BY-SA 3.0,"<p>I developed an index value (vulnerability score scale of 0 to 1) using a series of variables. I would like to regress these variables with the index value to determine the relative predictive power of each variable. Can I do this? </p>

<p>I ran the regression and came up with standardized B coefficients. I then interpreted those as relative contribution of the variable towards predicting the index value (vulnerability score). </p>

<p>I know one cannot regress a variable against itself but I am essentially doing this but primarily am just looking at determining to what degree each variable predicts the indexed value.</p>

<p>Any insights would be helpful if this is a proper use of regression or if there is an alternative method to assess this. Thanks!</p>
",2013-10-19 20:29:44.800
57848,503.0,2,,57847.0,,,,CC BY-SA 3.0,"<p>No. You can't. In fact, since you created the index, you already know the contribution of each value to the index. You shouldn't do regression here and you don't need an alternate. </p>
",2013-10-19 20:35:21.537
57849,,1,57929.0,,user30490,Stationarity of Moving Average processes,<time-series><self-study>,CC BY-SA 3.0,"<p>Consider the infinite order MA process defined by
$$y_t=\epsilon_t+a(\epsilon_{t-1}+\epsilon_{t-2}+...),$$
where $a$ is a constant and the $\epsilon_t$s are i.i.d. $N(0,v)$ random variable.</p>

<p>What is the best way to show that $y_t$ is nonstationary?  I know that I need to look at the characteristic roots of the characteristics polynomial and then judge whether or not they are outside of the unit circle, but what is the best way to approach this problem?  Should I try rewriting the infinite order MA process as a finite order AR process or is it easier to work the MA process?</p>
",2013-10-19 21:11:46.583
57850,594.0,2,,57830.0,,,,CC BY-SA 4.0,"<p>Note that the Kolmogorov-Smirnov test statistic is very clearly defined in the immediately previous section:</p>

<p><span class=""math-container"">$$D_n=\sup_x|F_n(x)âˆ’F(x)|\,.$$</span> </p>

<p>The reason they discuss <span class=""math-container"">$\sqrt{n}D_n$</span> in the next section is that the standard deviation of the distribution of <span class=""math-container"">$D_n$</span> goes down as <span class=""math-container"">$1/\sqrt n$</span>, while <span class=""math-container"">$\sqrt{n}D_n$</span> converges in distribution as <span class=""math-container"">$n\to\infty$</span>. </p>

<p>Yes, the number of points, <span class=""math-container"">$n$</span>, matters to the distribution; for small <span class=""math-container"">$n$</span>, tables are given for each sample size, and for large <span class=""math-container"">$n$</span> the asymptotic distribution is given for <span class=""math-container"">$\sqrt{n}D_n$</span> <span class=""math-container"">$-$</span> the very same distribution discussed in the section you quote.</p>

<p>Without some result on asymptotic convergence in distribution, you'd have the problem that you'd have to keep producing tables at larger and larger <span class=""math-container"">$n$</span>, but since the distribution of <span class=""math-container"">$\sqrt{n}D_n$</span> pretty rapidly 'stabilizes', only a table with small values of <span class=""math-container"">$n$</span> is required, up to a point where approximating <span class=""math-container"">$\sqrt{n}D_n$</span> by the limiting Kolmogorov distribution is sufficiently good.</p>

<p>Below is a plot of exact 5% and 1% critical values for <span class=""math-container"">$D_n$</span>, and the corresponding asymptotic critical values, <span class=""math-container"">$K_\alpha/\sqrt n$</span>. </p>

<p><img src=""https://i.stack.imgur.com/vNF5T.png"" alt=""Kolmogorov-Smirnov critical values""></p>

<p>Most tables finish giving the exact critical values for <span class=""math-container"">$D_n$</span> and swap to giving the asymptotic values for <span class=""math-container"">$\sqrt n D_n$</span>, <span class=""math-container"">$K_\alpha$</span> (as a single table row) somewhere between <span class=""math-container"">$n=20$</span> and <span class=""math-container"">$n=40$</span>, from which the critical values of <span class=""math-container"">$D_n$</span> for any <span class=""math-container"">$n$</span> can readily be obtained.</p>

<hr>

<p><span class=""math-container"">$\text{Responses to followup questions:}$</span></p>

<p>1)</p>

<blockquote>
  <p><em>How do we obtain the distribution of <span class=""math-container"">$D_n$</span> when <span class=""math-container"">$n$</span> is fixed?</em></p>
</blockquote>

<p>There are a variety of methods for obtaining the distribution of the test statistic for small <span class=""math-container"">$n$</span>; for example, recursive methods build the distribution at some given sample size in terms of the distribution for smaller sample sizes.</p>

<p>There's discussion of various methods given <a href=""http://www.iro.umontreal.ca/~lecuyer/myftp/papers/ksdist.pdf"" rel=""nofollow noreferrer"">here</a>, for example.</p>

<p>2)</p>

<blockquote>
  <p><em>If I get the value of <span class=""math-container"">$D_\text{max}$</span> and the sample size is <span class=""math-container"">$n$</span>, I have to calculate <span class=""math-container"">$Pr(K&lt;=x)$</span>, right?</em></p>
</blockquote>

<p>Your test statistic is your observed sample value of the <span class=""math-container"">$D_n$</span> random variable, which will be some value, <span class=""math-container"">$d_n$</span> (what you're calling <span class=""math-container"">$D_\text{max}$</span>, but note the usual convention of upper case for random variables and lower case for observed values). You compare it with the null distribution of <span class=""math-container"">$D_n$</span>. Since the rejection rule would be ""<em>reject if the distance is 'too big'</em>."", if it is to have level <span class=""math-container"">$\alpha$</span>, that means rejecting when <span class=""math-container"">$d_n$</span> is bigger than the <span class=""math-container"">$1-\alpha$</span> quantile of the null distribution.</p>

<p>That is, you either take the p-value approach and compute <span class=""math-container"">$P(D_n&gt; d_n)=1-P(D_n\leq d_n)$</span> and reject when that's <span class=""math-container"">$\leq\alpha$</span> or you take the critical value approach and compute a critical value, <span class=""math-container"">$d_\alpha$</span>, which cuts off an upper tail area of <span class=""math-container"">$\alpha$</span> on the null distribution of <span class=""math-container"">$D_n$</span>, and reject when <span class=""math-container"">$d_n \geq d_\alpha$</span>. </p>

<blockquote>
  <p><em>By formula 14.3.9 of Numerical Recipes, we should calculate a value got from the expression in the brackets - should that be the x?</em></p>
</blockquote>

<p>14.3.9 looks like it has a typo (one of many in NR). It is trying to give an approximate formula for the p-value of ""observed"" (that is, my ""<span class=""math-container"">$d_n$</span>"", your <span class=""math-container"">$D_\text{max}$</span>), by adjusting the observed value so you can use the asymptotic distribution for even very small <span class=""math-container"">$n$</span> (in my diagram, that corresponds to changing the <span class=""math-container"">$y$</span>-value of the observed test statistic via a function of <span class=""math-container"">$n$</span>, equivalent to pushing the circles 'up' to lie very close the dotted lines) but then it (apparently by mistake) puts the random variable (rather than the observed value, as it should) into the RHS of the formula. The actual p-value must be a function of the <em>observed</em> statistic.</p>

<p>3) </p>

<blockquote>
  <p><em>We make tests and get a distribution, right?</em> </p>
</blockquote>

<p>I don't know what you mean to say there. </p>

<blockquote>
  <p><em>Could you please explain your figure in a ""test"" way</em>? </p>
</blockquote>

<p>My figure plots the 5% and 1% critical values of the null distribution of <span class=""math-container"">$D_n$</span> for sample sizes 1 to 40 (the circles) and also the value from the asymptotic approximation <span class=""math-container"">$K_\alpha/\sqrt n$</span> (the lines).</p>

<p>It looks to me like you have some basic issues with understanding hypothesis tests that's getting in the way of understanding what is happening here. I suggest you work on understanding the mechanics of hypothesis tests first.</p>

<blockquote>
  <p>That means there is no error in 14.4.9 of NR .</p>
</blockquote>

<p>(Presumably you mean 14.3.9, since that's what I was discussing.)</p>

<p>Yes there is an error. I think you may have misunderstood where the problem is. </p>

<p>The problem isn't with ""<span class=""math-container"">$(\sqrt{n}+0.12+0.11/\sqrt{n})$</span>"". It's with the meaning of the term they multiply it by. They appear to have used the wrong variable from the LHS in the RHS formula, putting the random variable where its observed value should be. </p>

<p>[When the thing you're reading is confused about that, it's not surprising you have a similar confusion.]</p>
",2013-10-19 21:31:24.453
57851,546.0,1,79542.0,,,Approximating the relative quantities of coins in Canada,<self-study><application>,CC BY-SA 3.0,"<p>Would it be possible to approximate accurately the relative quantities of <a href=""http://en.wikipedia.org/wiki/Loonie"" rel=""nofollow"">Loonies</a>, <a href=""http://en.wikipedia.org/wiki/Toonie"" rel=""nofollow"">Twoonies</a>, quarters, dimes, nickles (and perhaps the discontinued penny) in circulation from simply obtaining a large enough sample of coins through everyday use? By everyday use I refer to the coins you get back in change when you make a purchase in a grocery store for example.</p>

<p>I suppose this is a 2 part question: </p>

<ol>
<li>Is the method of sampling sufficient, or is there some kind of bias
introduced because you are collecting samples through a
deterministic process (of collecting change?) What size of samples
would you need?</li>
<li>If the sampling is sufficient for an accurate
approximation, can you use it to determine the relative quantities
of each coin type in circulation? Or, for example, is it that the
sample size necessary to accurately approximate the relative
quantities would itself change the relative quantities of each coin
type in circulation?</li>
</ol>
",2013-10-19 21:33:32.747
57893,22884.0,1,,,,Calculate the quantile for a mixed pdf,<sampling><density-function><mixed-model>,CC BY-SA 3.0,"<p>I am trying to draw random numbers that follow a two-part pdf</p>

<p>a) $|x|&lt; x_0: f(x)=\text{constant}\quad \to \quad F(p)=a+(b-a)p$</p>

<p>b) $|x|&gt; x_0: f(x)=\exp(-|x|)\quad \to \quad F(p)=-\ln(1-p)$</p>

<p>that is the probability is constant between $(-x_0,x_0)$ and falls exponentially outside it.</p>

<p>Knowing the quantile functions $F(p)$ ($0\leq p\leq 1$) of the pdf, how can I construct the quantile that describes the above pdf comprised of two parts?</p>
",2013-10-20 21:17:49.097
57937,22907.0,1,,,,How to prove that a t-distribution can be written as a ratio distribution?,<proof><t-distribution>,CC BY-SA 3.0,"<p>If $X \sim N(0,1)$ and $Y \sim \chi^2(n),$ then it's ""known"" that $Z = X/\sqrt{Y/n}$ is $t$ distributed.</p>

<p>Is there anywhere a proof for this? That in the end one can see the $t$ distribution?</p>
",2013-10-21 14:58:19.417
57852,22865.0,1,,,,A question about the multistart heuristic and pseudo convergence,<machine-learning><markov-chain-montecarlo>,CC BY-SA 3.0,"<p>I'm teaching myself MCMC methods and I encountered this passage in a book that I am not able to make head or tails of:</p>

<blockquote>
  <p>The phenomenon of pseudo-convergence has led many people to the idea of comparing 
  multiple runs of the sampler started at different points. If the multiple runs appear to 
  converge to the same distribution, thenâ€”according to the multistart heuristicâ€”all is well. 
  But this assumes that you can arrange to have at least one starting point in each part of the 
  state space to which the sampler can pseudo-converge. If you cannot do thatâ€”and in the 
  black box situation you never canâ€”then the multistart heuristic is worse than useless: it 
  can give you confidence that all is well when in fact your results are completely erroneous.</p>
</blockquote>

<p>Can anyone explain this a little better?</p>
",2013-10-19 21:50:57.833
57853,503.0,2,,57851.0,,,,CC BY-SA 3.0,"<p>The bigger problem is going to be part 1, not part 2.</p>

<p>It will be relatively easy to get a big sample of coins. But how do you know those coins are a random sample? Maybe people where you live use more of a particular coin than people in other parts of Canada.  You certainly use money in a way that is not the same as everyone else. </p>

<p>For example, some people will pay for nearly everything with credit or debit cards; some will make even large purchases with cash. If you only buy cheap stuff with cash, you are going to get smaller coins. If you tend to have a lot of small bills and coins in your wallet, you will get smaller coins. </p>

<p>Probably not possible to get a <em>truly</em> random sample, but I'd try to get samples from different people in different parts of the country (rural/urban; west, center, Atlantic, etc.) and different ages, incomes etc. </p>
",2013-10-19 21:53:18.657
57854,12683.0,2,,57785.0,,,,CC BY-SA 3.0,"<p>The motivations for the use of an order statistic as an estimator are no different from the <a href=""https://stats.stackexchange.com/questions/6655/how-do-i-know-which-method-of-parameter-estimation-to-choose"">motivations</a> for the use of any statistic as an estimator. For example, if you want to estimate the mean $\mu$ of a continuous uniform distribution from $0$ to $2\mu$, the sample maximum is the complete <a href=""https://stats.stackexchange.com/questions/44063/sufficient-statistics-for-layman"">sufficient</a> statistic for $\mu$, so both the <a href=""https://stats.stackexchange.com/tags/maximum-likelihood/info"">maximum-likelihood</a> &amp; the uniformly minimum-variance unbiased estimators are based on it. If you want to estimate the mean $\theta$ of a normal distribution, the sample mean is the complete sufficient statistic for $\theta$, so both the maximum-likelihood &amp; the uniformly minimum-variance unbiased estimators are based on it.</p>

<p>It's not true in general that the first order statistic is a good estimator of a  <a href=""https://stats.stackexchange.com/questions/72806/parameters-of-a-statistical-distribution"">location parameter</a> (in any reasonable sense of 'good'). Recall the mean of a normal distribution is a location parameter&mdash; in this case the sample minimum would not even be a <a href=""https://stats.stackexchange.com/questions/31036/what-is-the-difference-between-a-consistent-estimator-and-an-unbiased-estimator"">consistent</a> estimator. You've perhaps come across location parameters added to distributions like the Weibull, whose two-parameter version has a lower bound at zero. In cases like this the sample minimum is a consistent estimator of the population minimum, &amp; I daresay a decent starting point for fitting algorithms.</p>
",2013-10-19 22:31:32.813
57855,22865.0,1,57856.0,,,Why is the Dirichlet Process unsuitable for applications in Bayesian nonparametrics?,<machine-learning><markov-chain-montecarlo><dirichlet-process>,CC BY-SA 3.0,"<blockquote>
  <p>The discrete nature of the DP makes it unsuitable for general applications in Bayesian nonparametrics, but it is well suited for the problem of placing priors on mixture components in mixture modeling.</p>
</blockquote>

<p>This quote is from <a href=""http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/jasa2006.pdf"" rel=""noreferrer"">Hierarchical Dirichlet Processes</a> (Teh, et al, (2006)$^{[1]}$) and I was looking for an explanation about what it means. Bayesian nonparametrics seems to be too vague a term for me to understand what the author is referring to.</p>

<p>${[1]}$ Teh, Y. W., Jordan, M. I., Beal, M. J., Blei, D. M. (2006): ""Hierarchical Dirichlet Processes"". <em>Journal of the American Statistical Association</em>, 101, pp. 1566â€“1581.</p>
",2013-10-19 23:54:06.537
57856,7007.0,2,,57855.0,,,,CC BY-SA 4.0,"<p>With probability one, the realizations of a Dirichlet Process are discrete probability measures. A rigorous proof can be found in</p>
<p><a href=""https://projecteuclid.org/journals/annals-of-statistics/volume-1/issue-2/Discreteness-of-Ferguson-Selections/10.1214/aos/1176342373.full"" rel=""nofollow noreferrer"">Blackwell, D. (1973). &quot;Discreteness of Ferguson Selections&quot;, The Annals of Statistics, 1(2): 356â€“358.</a></p>
<p>The stick breaking representation of the Dirichlet Process makes this property transparent.</p>
<ol>
<li><p>Draw independent <span class=""math-container"">$B_i\sim\mathrm{Beta}(1,c)$</span>, for <span class=""math-container"">$i\geq 1$</span>.</p>
</li>
<li><p>Define <span class=""math-container"">$P_1=B_1$</span> and <span class=""math-container"">$P_i=B_i \prod_{j=1}^{i-1}(1-B_j)$</span>, for <span class=""math-container"">$i&gt;1$</span>.</p>
</li>
<li><p>Draw independent <span class=""math-container"">$Y_i\sim F$</span>, for <span class=""math-container"">$i\geq 1$</span>.</p>
</li>
<li><p>Sethuraman proved that the discrete distribution function
<span class=""math-container"">$$
  G(t,\omega)=\sum_{i=1}^\infty P_i(\omega) I_{[Y_i(\omega),\infty)}(t) 
$$</span>
is a realization of a Dirichlet Process with concentration parameter <span class=""math-container"">$c$</span> and centered at the distribution function <span class=""math-container"">$F$</span>.</p>
</li>
</ol>
<p>The <em>expectation</em> of this Dirichlet Processs is simply <span class=""math-container"">$F$</span>, and this may be the distribution function of a continuous random variable. But, if random variables <span class=""math-container"">$X_1,\dots,X_n$</span> form a random sample from this Dirichlet Process, the posterior expectation is a probability measure that puts positive mass on each sample point.</p>
<p>Regarding the original question, you can see that the plain Dirichlet Process may be unsuitable to model <strong>some</strong> problems of Bayesian nonparametrics, like the problem of Bayesian density estimation, but suitable extensions of the Dirichlet Process <a href=""https://projecteuclid.org/journals/annals-of-statistics/volume-12/issue-1/On-a-Class-of-Bayesian-Nonparametric-Estimates--I-Density/10.1214/aos/1176346412.full"" rel=""nofollow noreferrer"">are available</a> to handle these cases.</p>
",2013-10-20 00:31:34.960
57857,22867.0,1,,,,when is an estimator consistent?,<estimation><mixture-distribution><consistency>,CC BY-SA 3.0,"<p>Say there are parameters $\theta$ such that $\theta_i &gt; 0$ and $\sum_i \theta_i = 1$ and a model such as $p(x) = \sum_{i=1}^n \theta_i p_i(x)$ where $p_i(x)$ are fixed and defined over a domain of random variable $X$. Say $X$ is a random variable which is a categorical variable.</p>

<p>Now, say we partition $x$ into equivalence classes, such that we have a new random variable $Y$ which is a more coarse version of $X$. This means that $Y = f(X)$ and $f$ maps several values of $X$ to the same value of $Y$.</p>

<p>Under what condition a consistent estimator for $\theta$ under the $Y$ model will be a consistent estimator for $\theta$ under the $X$ model? (By the $Y$ model I am referring to $p(y) = \sum_{i=1}^n \theta_i p_i(y)$.) I have a feeling this largely depends on Rao-Blackwell, but not sure exactly how to apply it here.</p>

<p>Clearly there are cases where consistency is hopeless from the $Y$, for example, if $f$ maps all values of $X$ to just a single value. But say it is at the very least partitioning the space into two values? Can Rao-Blackwell help here? If it won't show consistency, what would it show?</p>

<p>EDIT: What would be a way to turn a consistent estimator under the $Y$ model to a consistent estimator for the $X$ model? Or at least a better estimator?</p>

<p>EDIT / refinement: Say we have multiple functions, $f_1$, $f_2$, etc and for each one of them there is a $f(Y_j)$ model. What would be assumptions on $f_j$ and a way to get a consistent estimator from $f(Y_j)$ for the $p(X)$ model?</p>
",2013-10-20 02:11:50.450
57858,10135.0,2,,56970.0,,,,CC BY-SA 3.0,"<p>You should plot your residuals vs. explanatory variables (i.e. $X_i$'s) and residuals vs. fitted values to see if there is anything wrong with the model. There are other diagnostic plots. In R you can use function glm.diag.plots in package <a href=""http://cran.r-project.org/web/packages/boot/boot.pdf"" rel=""nofollow noreferrer"">boot</a>. See the code below and also <a href=""https://stats.stackexchange.com/questions/23666/explanation-of-r-diagnostic-plot-for-logistic-regression"">this post</a>. Here I will also use package <a href=""http://cran.r-project.org/web/packages/MASS/MASS.pdf"" rel=""nofollow noreferrer"">Mass</a> to load a data.</p>

<pre><code>library(MASS)
data(menarche) 
plot(Menarche/Total ~ Age, data=menarche)
glm.out = glm(cbind(Menarche, Total-Menarche) ~ Age,family=binomial(logit), data=menarche)
library(boot)
glm.diag.plots(glm.out) 
</code></pre>

<p><img src=""https://i.stack.imgur.com/Zp5np.jpg"" alt=""enter image description here""></p>

<p>And to have some more fun with the fitted values:</p>

<pre><code>plot(Menarche/Total ~ Age, data=menarche)
lines(menarche$Age, glm.out$fitted, type=""l"", col=""red"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/XDzXb.jpg"" alt=""enter image description here""></p>
",2013-10-20 02:22:36.340
57859,22866.0,1,,,,Should I pick a constant sample size for regressions on data with different available n?,<multiple-regression><sample-size>,CC BY-SA 4.0,"<p>I am using OLS to estimate the effects of various factors on the sales of different items. The data are monthly, and vary somewhat in the number of monthly observations available (some of the items have only been sold for, say, 18 months while others have data going back for years).</p>

<p>Should I pick some n (e.g. 18) and stick with it for modeling all the products? This option is appealing because I'm more interested in more recent effects and because unmodeled and difficult-to-know-about factors might be present in the more distant data. Also, I'm getting good fits and high significance for the relevant variables even with the smaller sample sizes, so I don't ""need"" to increase the sample size from that perspective.</p>

<p>I will need to produce reporting on this periodically, so a desirable property would be that parameter estimates are fairly stable from report to report. This is a possible argument for using the larger sample sizes where available.   </p>
",2013-10-20 02:33:34.697
57860,5987.0,2,,57852.0,,,,CC BY-SA 3.0,"<p>It isn't clear to me whether you're asking what pseudo-convergence is, or whether you're asking how multistart can fail to detect that pseudo-convergence is happening, so I'll try to answer both briefly.  </p>

<p>Imagine that you're using MCMC to sample from a distribution where $X$ has a high probability (say 99.9%) of being uniformly distributed between 0 and 1, and a small probability (say 0.1%) of being uniformly distributed between 1,000,000 and 1,000,001.  This is a black box problem, so in constructing the proposal distribution for the MCMC sampler you might do something naive like considering jumps that are N(0,1) from the current point.  Under these circumstances, if you start at a point in [0,1], then it's virtually impossible that your MCMC sequence will ever reach the isolated interval [1,000,000, 1,000,001].  Your MCMC sampler will then converge very nicely to a U(0,1) distribution.  Conventional convergenece tests will look great, but you'll have converged to an incorrect distribution.  </p>

<p>You might argue that missing that other 0.1% of the distribution really doesn't matter.  Depending on what aspect of the distribution you're interested in, this could be a very serious problem.  For example, if you want to estimate $E[X]$, you'll get 0.5 as your estimate, when it should be around 1,000!</p>

<p>This is an example of false or pseudo convergence.  </p>

<p>Now, suppose you pick a bunch of starting points and run the MCMC sampler from each of those starting points.  As long as each of these starting points is close to [0,1], each MCMC run will eventually converge to a U(0,1) distribution, and you'll be led to believe that $X$ really does have a U(0,1) distribution.  That's the potential danger of relying on the multistart heuristic.</p>

<p>The example that I've used here is admittedly extreme.  However, pseudo convergence is often reported in practice, particularly on problems in which we're sampling in a very high dimensional space and there are lots of isolated regions with reasonably high probability.   </p>
",2013-10-20 02:45:44.317
57861,22868.0,1,,,,Can the t-distribution be defined as the distribution on the true mean of a sampled normal?,<t-distribution>,CC BY-SA 3.0,"<p>Wikipedia says now, here in the introduction:</p>

<p><a href=""http://en.wikipedia.org/wiki/Student%27s_t-distribution"" rel=""nofollow"">http://en.wikipedia.org/wiki/Student%27s_t-distribution</a></p>

<p>""... then the t-distribution (for n-1) can be defined as the distribution of the location of the true mean, relative to the sample mean and divided by the sample standard deviation... In this way the t-distribution can be used to estimate how likely it is that the true mean lies in any given range.""</p>

<p>Is this right?  It seems not right to me.  How can we have a distribution on the true mean after obtaining a sample, without some sort of Bayesian prior?  I understand we can get a confidence interval for the true mean.  But a distribution?</p>
",2013-10-20 03:31:09.140
57862,9049.0,2,,57836.0,,,,CC BY-SA 3.0,"<p>Most probably the packages <a href=""http://cran.r-project.org/web/packages/lmerTest/index.html"" rel=""nofollow"">lmerTest</a> and <a href=""http://cran.r-project.org/web/packages/lsmeans/index.html"" rel=""nofollow"">lsmeans</a> provide readily available routines for what you are looking for. Mind you, neither of them uses MCMC methodology. If you want to use something <em>resampling-based</em>, you can use <code>lme4</code>'s native <code>bootMer()</code> function to bootstrap your model and get parametric bootstrap estimates (ver. 1.0-4 or newer).</p>
",2013-10-20 04:35:19.480
57863,9456.0,1,,,,Issues with calculating gradient descent operation,<optimization><gradient-descent>,CC BY-SA 3.0,"<p>I have this issue when using gradient ascent. I have some synthetic data and after my first iteration the objective function decreases and from the second iteration it keeps on increasing. Is it possible for a convex function. I also have the learning rate very low 0.001. Any suggestions.</p>
",2013-10-20 05:50:06.650
57864,6204.0,2,,57861.0,,,,CC BY-SA 3.0,"<p>You don't have a distribution on the true mean, you have a distribution on the <em>difference</em> between the true mean and the sampled mean, and this difference is scaled by the sampled standard deviation (which is another separate random variable). The true mean is fixed.</p>

<p>Let $X \sim N(\mu,\sigma^2)$ such that $x_1...x_n$ constitutes an i.i.d. sample of size $N$ from $X$. Let $\bar{X}$ denote the sample mean and $S^2$ denote the sample variance. Then </p>

<p>$$\frac{\bar{X}-\mu}{\sqrt{S^2/N}} \sim t_{N-1}$$</p>

<p>The relevant section of the wikipedia article you linked is <a href=""http://en.wikipedia.org/wiki/Student%27s_t-distribution#Derivation"" rel=""nofollow"">http://en.wikipedia.org/wiki/Student%27s_t-distribution#Derivation</a></p>
",2013-10-20 06:02:07.550
57865,6204.0,2,,57863.0,,,,CC BY-SA 3.0,"<p>Sounds like something is probably wrong in your code or you have your step size (learning rate) set too large. If you coded it properly, the objective function at iteration t+1 should always be lower than at iteration t. This feature is what makes it an ""ascent"" algorithm. We might be able to give you more insight if you provide your code and, better yet, a reproducible example.</p>
",2013-10-20 06:21:55.823
57866,22860.0,2,,50739.0,,,,CC BY-SA 3.0,"<p>If I understood you correctly, since you have three categories in your data, you need to put two dummy variables in your regression model. If beta1 and beta2 are not significants then you can conclude there is no difference in the means between the two groups. How about the third group?</p>

<p>It is easier to conduct ANOVA and then post hoc multiple comparison.</p>
",2013-10-20 08:17:43.387
57977,22919.0,1,,,,Two independent groups and three dependent variables,<anova><multivariate-analysis><manova>,CC BY-SA 3.0,"<p>How do I test for differences between two groups on three dependent variables? Is that an ANOVA or multiple t-tests with an adjusted alpha level?</p>
",2013-10-22 00:32:26.990
57867,1501.0,1,58168.0,,,Bayesian MMSE estimators from a transformation of the observations,<bayesian><estimation><estimators><mse>,CC BY-SA 3.0,"<p>Consider a random variable X whose value we want to estimate using a Bayesian MMSE estimator. Let $O_1(X)$ be a set of observations which depend on $X$ in some complex way (captured by $P(O_1|X)$) then the MMSE estimator is the conditional mean $\hat{X}_1=\mathbb{E}[X|O_1]$. Now consider another set of observation which is a possibly complex transformation of the first $O_2(O_1)$ and the corresponding MMSE estimator $\hat{X}_2=\mathbb{E}[X|O_2]$. Now it is obvious that if the mapping $O_2(O_1)$ is deterministic then $MSE[\hat{X}_1]\le MSE[\hat{X}_2]$ from the minimality of the MSE of $\hat{X}_1$. </p>

<p>My question is if this is true also when the mapping $O_2(O_1)$ is probabilistic, that is defined by a conditional distribution $P(O_2|O_1)$. Intuitively, it should be as any other stochasticity in the mapping just seems to introduce additional noise as it does not depend on $X$. But I wonder if one can show this explicitly.  </p>

<p>EDITED: as the MMSE estimator is unbiased, indid $MSE[\hat{X}_i]=V[\hat{X}_i]$</p>
",2013-10-20 08:46:34.413
57868,221.0,2,,57648.0,,,,CC BY-SA 3.0,"<p><strong>Preface</strong></p>

<p>I work with recommender systems on daily basis and have also never heard of the application of such a model as recommender system. I can only speculate about the reasons though.</p>

<p>The main overall reason might be that recommender systems are often applied in a domain where the price/cost of an item is too small to force the customer
to invest time into making a nearly-optimal decision, maximizing his utility. This should be kept in mind in the following section. Such domains
include e-commerce or news portals (where articles are recommended) or sites like tastekid.com, where the decision at this step costs only a click, i.e. virtually nothing.</p>

<p><strong>Reasoning</strong></p>

<p>The described conditional multinomial model requires (or works best) with ...</p>

<ul>
<li>characteristics of the customer</li>
<li>characteristics of the items</li>
<li>assumed rationality when it comes to the decision</li>
</ul>

<p>Let's step through every point</p>

<p><strong>Characteristics of the customers</strong></p>

<p>Beside some basic demographic information like gender, address and (may be) age, little is known. The less the price of an item (see above),
the harder it is to request a survey before the selection process starts. Activity data (bought items, ratings etc.) on the other hand can be collected
without any work from the customer and can be used to describe the customer, following the motto ""you are what you are interested in"". The items 
the customer is interested in (the preferences) <em>implicitly capture</em> what is important to the customer.</p>

<p><strong>Characteristics of the items</strong></p>

<p>Building a model based on the characteristics of a item is already done, either via ""content based collaborative filtering"" or a model based approach.
These are e.g. used to solve the cold-start-problem, i.e. the fresh new recommender system has not (enough) preferences yet. </p>

<p>The drawback here however is that is hard to automatically collect the properties of an item. Imagine the case of fashion: Some are easy (color, brand),
some are very hard (how does the cloth feels like on skin, how does it look if my hip is broader than average). Sometime it is completely impossible because 
it entirely depends on the reception of the product, e.g. in case of movies. For certain items, such information can be collected by humans or by a very very sophisticated
system understanding semantics and language. It is not clear that the resulting improvement will outweigh the costs. </p>

<p>So instead of saying: ""Item A is similar to item B due to the properties p1,p2,...,"" it is easier to say ""a lot of people have both bought item A and item B. 
I don't know why, but they are similar enough for the purpose of a recommender system"". So the <em>preferences implicitly capture</em> how similar to items are.</p>

<p><strong>Assumed rationality when it comes to the decision</strong></p>

<p>We are humans and we pretend to be rational all the time. If e.g. the price or other circumstances forces us to think hard about it a decision, it might be the 
case that the rational part of a decision is higher than average. But when it comes to utilize advertising to sell people stuff (and yes, recommendations can be
seen as advertising), marketing will tell that rationality plays a lesser role.</p>

<p>Additionally, people are often do not know beforehand which properties are most important to them in order to maximize their personal utility function.
If this would be the case, all buying processes could be described via the usage of a search engine, where 
a) all relevant properties are listed
b) the customer selects all properties relevant to him and name the product of interest
and the search engine delivers exactly the right results.</p>

<p>Instead, people have a basic goal (e.g. buying a suit), but then are browsing around to see how products appeal to them and / or to get inspired.
Making a buy decision is still partially rational (budget, invested time) but often comes down to ""what feels right"". Of course, every domain has its
 own distribution of rationality and emotionality. The more technical, the more facts do play an important role. But
even than the customer might select a brand due to the curtain fire of advertising, which he would not have named as primary criterion beforehand.</p>

<p>So building a economic model here might be still be working and it surely correct, buth might be entirely over the top. Additionally, one might have
to build a separate model for each type of item a shop is selling.</p>

<p><strong>Summary</strong></p>

<p>Building a recommender system entirely based on preferences is often done because ...</p>

<ul>
<li>it is simple (=> cheap)</li>
<li>it can be done automatically, no extra work from the customer is required (=> cheap)</li>
<li>it works (good enough), so that a more complicated model might not outweigh the additional costs.</li>
</ul>

<p>But: There are domains, where such a economic model will be better. I do not doubt, that a good estate agent and hence a good expert system based
on a economic model will easily outperform a recommender system based on preferences. I have regularly observed that recommendations made by human experts are often
better than automatic ones. However, the automatics are still good and can produced en mass without too much costs, so that an expert can focus 
on more sophisticated tasks.</p>
",2013-10-20 11:41:33.910
57869,22262.0,1,60004.0,,,What can I read to give me a meta-view of statistics as a field,<references>,CC BY-SA 3.0,"<p>What can I read that will give me a meta-view of the diverse field of statistics and data science? With few exceptions much of what I get my hands of goes straight into formulae and methodologies. Preferably something sufficiently high level as to bring in diverse areas such as econometrics, psychometrics, machine learning, etc. </p>

<p>To be more concrete, preferably something that:</p>

<ul>
<li>Summarises and discusses the various branches of statistics/data science; what problems are encountered in each branch.</li>
<li>Talks about differences between the branches and their histories.</li>
<li>Contrasts methodological approach.</li>
</ul>
",2013-10-20 12:11:27.733
57870,22872.0,1,57873.0,,,Interpretation of PCA biplot?,<data-visualization><clustering><pca><biplot>,CC BY-SA 3.0,"<p>I just ran my first ever PCA, so please excuse any naivety on my part.</p>

<p>As input, I used five years worth of the following:</p>

<ul>
    <li><a href=""http://au.spindices.com/indices/equity/sp-asx-200-a-reit-sector"" rel=""nofollow noreferrer"">S&P/ASX 200 A-REIT</a></li>
    <li><a href=""http://au.spindices.com/indices/equity/sp-asx-200-consumer-discretionary-sector"" rel=""nofollow noreferrer"">S&P/ASX 200 Consumer Discretionary</a></li>
    <li>S&P/ASX 200 Consumer Staples</li>
    <li>S&P/ASX 200 Energy</li>
    <li>S&P/ASX 200 Financial-x-A-REIT</li>
    <li>S&P/ASX 200 Health Care</li>
    <li>S&P/ASX 200 Industrials</li>
    <li>S&P/ASX 200 Information Technology</li>
    <li>S&P/ASX 200 Materials</li>
    <li>S&P/ASX 200 Resources</li>
    <li>S&P/ASX 200 Telecommunication Services</li>
    <li>S&P/ASX 200 Utilities</li>
</ul>

<p>Using R, I simply ran the following commands:</p>

<pre>
arc.pca1 &lt;- princomp(sp_sector_data, scores=TRUE, cor=TRUE)
summary(arc.pca1)
plot(arc.pca1)
biplot(arc.pca1)
</pre>

<h3>Summary</h3>

<pre>
Importance of components:
                         Comp.1     Comp.2     Comp.3
Standard deviation     2.603067 1.05203261 0.88394057
Proportion of Variance 0.564663 0.09223105 0.06511258
Cumulative Proportion  0.564663 0.65689405 0.72200662

                           Comp.4     Comp.5     Comp.6
Standard deviation     0.84122312 0.76978259 0.73901015
Proportion of Variance 0.05897136 0.04938044 0.04551133
Cumulative Proportion  0.78097798 0.83035842 0.87586975

                           Comp.7     Comp.8     Comp.9
Standard deviation     0.66409102 0.62338449 0.52003850
Proportion of Variance 0.03675141 0.03238402 0.02253667
Cumulative Proportion  0.91262116 0.94500518 0.96754185

                          Comp.10    Comp.11      Comp.12
Standard deviation     0.45637805 0.42371864 0.0409804189
Proportion of Variance 0.01735674 0.01496146 0.0001399496
Cumulative Proportion  0.98489859 0.99986005 1.0000000000
</pre>

<h3>Loadings</h3>

<pre>
Loadings:
        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7
RE      -0.235         0.520 -0.533 -0.438  0.355 -0.150
disc    -0.332               -0.125                0.294
staples -0.295  0.226                      -0.211  0.554
energy  -0.332 -0.251         0.172  0.176        -0.130
fin_RE  -0.323               -0.118 -0.130         0.384
health  -0.224  0.465 -0.124 -0.193  0.603  0.537 -0.112
ind     -0.337                                          
IT      -0.224  0.145 -0.757        -0.461        -0.312
mat     -0.329 -0.351         0.295         0.126 -0.116
res     -0.335 -0.350         0.297         0.123 -0.133
telco   -0.161  0.609  0.327  0.609 -0.311        -0.113
util    -0.270  0.160  0.146 -0.256  0.234 -0.694 -0.509

        Comp.8 Comp.9 Comp.10 Comp.11 Comp.12
RE      -0.217                               
disc     0.309  0.567  0.596                 
staples -0.688        -0.141                 
energy         -0.215  0.240  -0.783  -0.165 
fin_RE   0.374 -0.724          0.207         
health                                       
ind      0.398  0.311 -0.743  -0.221         
IT      -0.183                               
mat     -0.127                 0.461  -0.638 
res     -0.123                 0.226   0.752 
telco    0.116                               
util                           0.115         

               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6
SS loadings     1.000  1.000  1.000  1.000  1.000  1.000
Proportion Var  0.083  0.083  0.083  0.083  0.083  0.083
Cumulative Var  0.083  0.167  0.250  0.333  0.417  0.500

               Comp.7 Comp.8 Comp.9 Comp.10 Comp.11
SS loadings     1.000  1.000  1.000   1.000   1.000
Proportion Var  0.083  0.083  0.083   0.083   0.083
Cumulative Var  0.583  0.667  0.750   0.833   0.917

               Comp.12
SS loadings      1.000
Proportion Var   0.083
</pre>

<h3>Scree Plot</h3>

<p><img src=""https://i.stack.imgur.com/6edCJ.png"" alt=""enter image description here""></p>

<h3>Biplot</h3>

<p><img src=""https://i.stack.imgur.com/QrCyS.png"" alt=""enter image description here""></p>

<h3>Is this useful?</h3>

<p>Am I right in assuming that these indices are correlated with each other?</p>

<p>Does the biplot show some sort of clustering?</p>

<p>What if anything, does any of this mean?</p>
",2013-10-20 13:28:51.057
57871,12808.0,2,,46384.0,,,,CC BY-SA 3.0,"<p>Suppose you have a d-dimensional observation vector. You may assume that your emission probabilities come from a single d-dimensional Gaussian density or a mixture of M Gaussians in which case the density is a linear combination of M component Gaussian densities. In the first case you estimate mean vector and cov. matrix for each state, in the second case you do this for all M component densiites together with their mixing weights.</p>
",2013-10-20 13:34:31.987
57872,20120.0,2,,57836.0,,,,CC BY-SA 3.0,"<p>This answer would have deserved comment status at best, but comments are too short and don't really lend themselves to extended mock code.
Also, it seems you already got a more sensible answer by @user11852, but I wanted to give a more general answer (though see the comments below!).</p>

<p>If all else fails, one may always (?) obtain CIs from bootstrapping. In your specific case, this may be computationally infeasible, since running the model 1000 or so times may take half a century, but it should be fairly fool proof. I don't know R well, so here is some mock code in fake matlab for the 95% CI for the output generated by some parameter estimation function, such as intercepts in lmer. As a special feature, it bootstraps individual subjects and for each subject generates bootstrapped samples of data points for this subject.</p>

<pre><code>a = data_set
s = (# of bootstrap iterations, e.g. 1000)
n = (# of subjects)

% main loop over bootstrap iterations
for x = 1:s

    % bootstrap over subjects

    % sample with replacement from your subjects (just collect n indices)
    z = random_sample_with_replacement(1:n)

    % loop over bootstrap selection of subjects
    % to bootstrap sample individual data points within subject
    for y = 1:length(z):
        % for each subject selected for resampling, draw from their data points with replacement
        for w = 1:length(a(z(y))) 
            within_subj_boot(w) = random_sample_with_replacement(a(z(y)))
        end
        bootsample(y) = within_subj_boot
    end

    % perform the respective calculation (e.g., lme4) for the bootstrap sample
    % and store the relevant parameter you want a CI for
    output(x) = parameter_estimation_function(bootsample)

% check the relevant percentiles of your bootstrapped parameter estimates
CI = percentile(output,[5,95])
</code></pre>

<p>Most languages will have some wrapper function for something equivalent to this (but a lot less inefficient), I think for R it might be the Coin package?</p>
",2013-10-20 13:49:19.927
57873,12808.0,2,,57870.0,,,,CC BY-SA 3.0,"<p>PCA tries to project your data onto a new set of dimensions where the variances in your data are captured such that you can classify/cluster them visually or by using a hopefully simple algorithm. </p>

<p>The variance plot tells you how much the new set of dimensions capture variances in decreasing order. Biplot is the projection of your data on the first two principal components (where the variances are the highest).</p>
",2013-10-20 13:59:09.630
57874,22752.0,1,57881.0,,,Calculate the error variance in a linear regression model,<regression><self-study><variance><error>,CC BY-SA 3.0,"<p>I am trying to calculate the error variance for the following question but I don't have clue where to start. Could anyone please help?</p>

<p><img src=""https://i.stack.imgur.com/yaxsG.jpg"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/gC1ha.jpg"" alt=""enter image description here""></p>
",2013-10-20 14:01:35.673
57875,22611.0,1,,,,Unpaired t-test with only summary means,<hypothesis-testing><t-test><mean><descriptive-statistics>,CC BY-SA 3.0,"<p>Is it appropriate to conduct an unpaired t-test with only summary means from both groups?  </p>

<p>For example, let's say you want to compare:</p>

<p>a. Professor A's scores received on a evaluation by <strong>his</strong> students from one semester; with<br>
b. The average evaluation scores among all professors in the university completed by <strong>all</strong> students from the same semester.</p>

<p>The teacher evaluation has 15 Likert questions (6-point scale). Constraints:</p>

<ol>
<li>You don't know how many students completed the evaluations in either group.  </li>
<li>All you have is 15 mean scores for Professor A, and 15 mean scores for the overall university sample.  </li>
</ol>

<p>It's safe to assume the evaluation is psychometrically sound.</p>

<p>An unpaired t-test is conducted to compare means from group 1 (15 means from Professor A's students) and group 2 (15 means for all professors in the University). So $N = 30$. </p>

<p>Is this appropriate?  </p>
",2013-10-20 14:08:17.733
57876,22874.0,1,,,,Statistical significance in yes/no poll question,<self-study><statistical-significance>,CC BY-SA 3.0,"<p>I have no statistics background, but an trying to complete my first quantitative research survey writeup for my researcher in education class and hope someone can direct me.</p>

<p>I created a poll of 62 teachers and am trying to analyze the following 4 questions </p>

<ol>
<li>Do they use social media personally (yes/no)</li>
<li>Do they use SM in the classroom (yes/no)</li>
<li>If they plan to use in future in classroom (yes/no)</li>
<li>What type of secondary school do you work in (public/private)</li>
</ol>

<p>I am running into a challenge with how to complete a ""test of statistical significance for this data""</p>

<p>The professor suggested using either a t-test or ANOVA and provided links, but the data I have do not seem to fit into the formulas.</p>

<p>Is anyone able to provide some guidance about how to test this type of survey? Is there a different way to analyze the data?</p>
",2013-10-20 14:40:07.613
58097,15658.0,1,58102.0,,,Quantile Regression - Interpretation of a significant quantile,<quantile-regression>,CC BY-SA 3.0,"<p>I want to perform a quantile regression on two continuous variables; Y (DV) and X (IV). I want to find out if there is an significant association between Y and X.</p>

<p>When doing this in R like:</p>

<p>fit2 &lt;- rq(Y ~ X,tau=c(.05, .25, .5, .75, .95))</p>

<p>If say, the 75% quantile of X is significant with a p-value &lt; 0.05 but rest is not, can I say that X is significant in total? If none of the quantiles are significant, is X not significant in total?</p>
",2013-10-23 14:51:06.183
57877,4656.0,2,,57826.0,,,,CC BY-SA 3.0,"<p>Assuming that the word <code>independent</code> in the opening statement is used
in the way that probabilists use the word and not in the sense of independent
versus dependent variable as is common in regression analysis, the
<em>joint</em> distribution of the five random variables $Y_{11}, Y_{12}, Y_{13}, Y_{21},Y_{22}$ is the product of the joint distributions of
$Y_{11}, Y_{12}, Y_{13}$, and $Y_{21},Y_{22}$, both of which are multivariate
normal. This $5$-variate joint distributions
is also a multivariate normal distribution in which the mean vector is
just the concatenation $(\mu_1, \mu_2)^T$ of the two mean vectors and
the covariance matrix is
$$\Sigma = \left[\begin{matrix}\Sigma_{11} &amp; 0\\0 &amp; \Sigma_{22}\end{matrix}\right].$$
Thus, the joint distribution of $Y_{11}-Y_{13}+Y_{22}$ and $Y_{21}-Y_{12}$
is a bivariate normal distribution which can be found by the standard
methods involving setting up a linear transformation mapping
$(Y_{11}, Y_{12}, Y_{13}, Y_{21},Y_{22})$ to 
$Y_{11}-Y_{13}+Y_{22},Y_{21}-Y_{12})$ and doing matrix calculations. More
simply, the means and variances of $Y_{11}-Y_{13}+Y_{22}$ and $Y_{21}-Y_{12}$
as well as their covariance can be computed more directly and used
in writing down the mean vector and covariance matrix of this bivariate
normal distribution.</p>
",2013-10-20 14:54:33.237
57878,17635.0,2,,57126.0,,,,CC BY-SA 3.0,"<p>Set aside the theoretical issues related to extreme observations, I would not advise ttest anyway, as you seem to have 3 groups of observations. Hence, you would be probably better with ANOVA and subsequent multiple comparisons. </p>

<p>Moreover, as far as your zero-spenders are concerned, I would try to understand how many of them are systematic zeros (that is, people who cannot afford to spend money because they cannot rely upon a disposable income) vs sample zeros (that is, people who are not interested in buying the goods considered in your research).</p>
",2013-10-20 14:55:11.057
57879,503.0,2,,57876.0,,,,CC BY-SA 3.0,"<p>The professor's advice seems odd, if this is all the data you have; also ""t-test of ANOVA"" is not a sensible phrase. Is it a typo of ""t-test or ANOVA""?  Even if it is a typo I think it strange advice. T-tests and ANOVAs are for comparing mean scores. In your question, there don't seem to be any means (or things to take the mean of).</p>

<p>Your title mentions ""statistical significance"" but that requires some hypothesis that you wish to test. What is your hypothesis? </p>

<p>What do you want to find out about the four questions that you asked? e.g.</p>

<p>What percent of teachers said ""yes"" to each?</p>

<p>(responding to your comment)</p>

<p>For public vs. private school you will have (for each of the four questions) a 2x2 table of results. Have you studied any statistical method that looks at that?</p>

<p>Similarly for personal vs. classroom use, you have a 2x2 table.</p>

<p>Unless you have yet more information....</p>

<p>How the four questions relate to each other?</p>

<p>Something else? (if so, what?)</p>

<p>Do you have any other information about the professors? If so, what do you have and how do you want to use it?</p>
",2013-10-20 15:11:34.663
57880,22643.0,1,,,,"Time to Event modeling, fixed but different durations",<survival><time-varying-covariate>,CC BY-SA 3.0,"<p>I am looking at probability of an event ($E$) for a number of customers.  Each customer qualifies for the analysis through a qualifying Action ($A$), and has a finite Duration ($D$) to complete Event.  There are a number of interim actions that should have an effect on reaching the Event.  Unlike the time-to-event modeling I've done before, the Duration is known ahead of time, but is different for every customer.  That is, customer 1 may have a duration of 3 months from their qualifying Action; whereas customer 2 may have a duration of 3 weeks.  The duration is always known at the time of the qualifying Action.</p>

<p>Consider an advance car rental booking.  Making the reservation is the qualifying Action.  The Event of interest is whether the customer completes a full online profile.  The Duration is the time period between contract &amp; pickup.  Interim actions might include receiving an email from the agency, calling into a help desk, starting the profile.</p>

<p>I think that incorporating duration is important because a customer booking 1 day in advance might (should) have a different propensity to complete an online profile than one who books 6 months in advance.</p>

<p>My instinct is to transform the duration to a proportion of the finite duration.  Concretely, qualifying Action is time 0 and everyone has a duration of 100.  But I've not seen that actually done in any of my literature review, and it feels like this may lose important information.</p>
",2013-10-20 16:35:43.337
57881,10135.0,2,,57874.0,,,,CC BY-SA 3.0,"<p>$(X'X)^{-1}=\dfrac{1}{150}.\left(
  \begin{array}{cc}
    5 &amp; -10 \\
    -10 &amp; 50 \\
  \end{array}
\right)$. $\hat{\beta}=(X'X)^{-1}X'Y=1/150.\left(
  \begin{array}{cc}
    5 &amp; -10 \\
    -10 &amp; 50 \\
  \end{array}
\right).\left(
  \begin{array}{cc}
    20 \\
    10 \\
  \end{array}
\right)=\left(
  \begin{array}{cc}
    0 \\
    2 \\
  \end{array}
\right)$<br>
$\hat{\beta}\sim N(\beta,\sigma^2.(X'X)^{-1})$. You can estimate $\sigma^2$ by $s^2=\dfrac{1}{n-p-1}(y-X\hat{\beta})'(y-X\hat{\beta})$. Now if you want to simultaneously test $H_0: \beta=\beta_0$ vs $H_1:\beta\neq \beta_0$, where $\beta_0$ is a $p$-dimensional constant, then you need to use the $F$ test as follow:     </p>

<p>$F=\dfrac{(\hat{\beta}-\beta_0)'(X'X)^{-1}(\hat{\beta}-\beta_0)}{_ps^2}\sim F_{p,n-p}$. Here $\hat{\beta}-\beta_0=\left(
  \begin{array}{c}
    -0.25 \\
    1.75 \\
  \end{array}
\right)
$.<br>
 And $(\hat{\beta}-\beta_0)'(X'X)^{-1}(\hat{\beta}-\beta_0)=162.1875
$. Here $p=1$ and $n=N$. If we let $_ps^2=\sigma^2=1$, then $F$ statistics is 162.1875 and we need to compare it with $F_{1,N-1}$. If $P_r(F_{\alpha,1,N-1}\geq 162.1875 )\geq (1-\alpha)$ then $H_0$ cannot be rejected otherwise accept $H_1$. See e.g. page 70 of <a href=""http://books.google.ca/books?id=MjNv6rGv8NIC&amp;pg=PA71&amp;dq=simultaneous%20confidence%20intervals%20regression&amp;hl=en&amp;sa=X&amp;ei=4AVkUuD0A4aTyQHLy4CoDw&amp;ved=0CC4Q6AEwAA#v=onepage&amp;q=simultaneous%20confidence%20intervals%20regression&amp;f=false"" rel=""nofollow"">Linear Regression Analysis: Theory and Computing</a>.</p>
",2013-10-20 17:00:29.333
57882,22877.0,1,,,,Clinical statistic problem,<confidence-interval><biostatistics>,CC BY-SA 3.0,"<p>You conduct a case-control study of elevated cholesterol and myocardial infarction (MI).  Of 20 MI cases, 10 had elevated cholesterol.  Of 30 healthy controls, 10 had elevated cholesterol.  These results give an odds ratio (OR) of 2.0, with a 95% confidence interval (CI) $ = [0.6-6.4]$.</p>

<p>Interpret the CI?</p>
",2013-10-20 17:09:25.137
57917,14799.0,2,,44772.0,,,,CC BY-SA 3.0,"<p>If each rater gave 9 ratings -- one for each pair of subjects -- at each time point, if the subjects were re-paired at each time point, and if it is not known who was paired with whom, then I don't think there is <em>any</em> proper way to analyze the data, because of the unknown and unestimatable correlations among the ratings from from one time to the next.</p>

<p>If you are willing to treat the data <em>as if</em> there were 18 different subjects at each time point (72 subjects total) then you could do a 1-between, 1-within analysis, where ""between"" and ""within"" are relative to the pairs, which would play the role that is usually played by subjects in such analyses: pairs are nested within time and crossed with raters; time is the between-pairs (grouping) factor -- to check for a time difference, you must look at different pairs -- and raters is the within-pair (repeated measures) factor -- you can check for a rater difference within each pair.</p>
",2013-10-21 07:19:51.677
57883,22878.0,1,,,,How to test association in contingency tables with very small numbers and proportions?,<statistical-significance><contingency-tables>,CC BY-SA 3.0,"<p>I have a sample of 197 responses. 8.6% (17)are from Group A, the remainder from Group B. (The groups are mutually exclusive, and not independent). (If it helps visualize the issue, Group A is under 10's, Group B respondents aged 11+.)</p>

<p>The responses are then sorted in to groups by type. Type 1 is ""head injuries"", and there are 3 members of this group. 2 are from Group B, 1 is from Group A.</p>

<p>I would expect the results for head injuries to be under 10 (Group A) &amp; head injury: 0.3 Under 10 &amp; no head injury: 16.7 Over 10 (Group B) &amp; head injury: 2.7 Over 10 &amp; no head injury: 179.3</p>

<p>I have asked a few friends how to test if the expected value being so much lower than the observed value is significant. So far responses seem to favour a Z test, or a chi squared test with a correction for <em>very small numbers</em>. I'm innumerate in the extreme, but can manage either well enough -- just not sure what is the appropriate test here? </p>
",2013-10-20 17:10:06.830
57884,21168.0,1,,,,Outlier treatment in Vector Autoregression (VAR) Model,<r><time-series><multivariate-analysis><outliers><vector-autoregression>,CC BY-SA 4.0,"<p>Data: Multivariate Time Series, Series </p>

<ol>
<li>Demand of a product </li>
<li>Rainfall data both available at monthly level from 2010-2013.</li>
</ol>

<p><strong>Approach:</strong> I am trying to estimate the effect of rainfall on demand of the product using VAR( Vector Autoregression) model. Demand data has some outliers, like a month of sudden high demand and  followed by zero values.</p>

<p><strong>Question:</strong> How to treat these outliers (I am working in R), since I already have few data and deleting them is not an option for me.</p>
",2013-10-20 17:48:42.767
57885,15183.0,2,,57884.0,,,,CC BY-SA 3.0,"<p>You can include dummy variables for the outliers, if they are caused by special events in the demand for the product. The dummy take the value of 1 on the outliers, and zero otherwise. </p>
",2013-10-20 18:03:09.000
57886,22880.0,1,,,,Propensity Score,<logistic><logit><propensity-scores>,CC BY-SA 3.0,"<p>What are the various methods used for binary classification other than logistic regression? </p>

<p>What are the advantages of logistic reg. model in developing Propensity score w.r.t. other methods?</p>

<p>Actually I have been asked why? I backed it by saying its binary classification, so logit is perfect. Then I was asked why specifically logistic regression when there are various other binary classification methods?</p>
",2013-10-20 19:29:08.897
57887,169.0,1,,,,Is chi square the best approach for looking at number of cases by year,<chi-squared-test><independence>,CC BY-SA 3.0,"<p>I have the number of people diagnosed with a condition in each quintile of deprivation over five different years. We are interested in whether numbers of diagnoses are going up faster in more deprived quintiles. The data looks like this:</p>

<p><img src=""https://i.stack.imgur.com/QZwzV.png"" alt=""enter image description here""></p>

<p>It looks to me very much like it rises much faster in the more deprived quintiles, on the left (Q1 and Q2).</p>

<p>I thought chi square was the best approach but:</p>

<pre><code>test = structure(list(Q1 = c(98L, 109L, 263L, 323L, 312L),
                      Q2 = c(90L, 113L, 199L, 237L, 247L),
                      Q3 = c(70L, 83L, 133L, 166L, 182L), 
               Q4 = c(20L, 39L, 60L, 87L, 90L),
                      Q5 = c(38L, 50L, 75L, 101L, 115L)),
                 .Names = c(""Q1"", ""Q2"", ""Q3"", ""Q4"", ""Q5""),
                 class = ""data.frame"", row.names =
                   c(""2008/09"", ""2009/10"", ""2010/11"", ""2011/12"", ""2012/13""))

chisq.test(test)

    Pearson's Chi-squared test

data:  test
X-squared = 17.285, df = 16, p-value = 0.3674
</code></pre>

<p>The thing that I can see is missing is that the years are really ordered, we are looking at an increase, but of course the chi-square just treats them as nominal.</p>

<p>I don't think I can use logistic regression because I'm just looking at a caseload- I don't have loads of people who don't have the disorder to compare with, just the increase over time.</p>

<p>Or is it the right approach and it really just isn't significant?</p>

<p>Thanks.</p>
",2013-10-20 19:34:51.553
57888,22882.0,1,,,,How to interpret results from an experiment where covariate is influenced by the experimental setup?,<experiment-design><interpretation><ancova><predictor>,CC BY-SA 3.0,"<p>I am doing study investigating the effect of company's heritage on consumer attitude towards the company/brand. For this purpose I have created two texts:</p>

<ul>
<li>one revealing the history of the company, </li>
<li>and the other one - the control text - revealing some general information about the firm. </li>
</ul>

<p>My results indicated that the two texts differ not only regarding heritage perception, but also regarding information credibility and believability. So the text with history details is also perceived as more credible. And this is also said to influence the attitude. </p>

<p>My questions are:</p>

<ol>
<li>Can I use the perceived text credibility as covariate in the ANCOVA, although it is measured after the treatment and caused by it?</li>
<li>How can I interpret the ANCOVA output in this case? Can I separate both effects - the one of the manipulation and the one of the covariate? </li>
<li>Would mediation analysis be one possible solution to check whether the effect of the treatment on my dependent variable is fully mediated by text credibility?</li>
</ol>
",2013-10-20 19:37:55.377
57889,21762.0,2,,57882.0,,,,CC BY-SA 3.0,"<p>The OR in your sample is 2. You don't know the exact odds ratio in the corresponding population, but at least you can be 95% confident that it is somewhere between 0.6 and 6.4. Since the interval contains the value 1 (no relation between MI and elevated cholesterol), you cannot claim that there is truely such relation.</p>
",2013-10-20 19:44:22.163
57890,22881.0,1,121372.0,,,How to compute the maximum a posteriori probability (MAP) estimate with / without a prior,<machine-learning><bayesian><estimation>,CC BY-SA 3.0,"<p>I am a newbie in this area so I hope someone could explain the following problem to me in plain English.</p>

<p>Assume I want to use MAP to estimate some parameters on the basis of some observations. I know the method of computing MAP is: 
$$
\theta(x) = {\rm argmax} \ f(X|\theta) g(\theta) 
$$</p>

<p>where $g$ is the prior. However, I cannot find any answers online on how to compute this using a real world example. So here is my proposed question:</p>

<p>Assume you asked 100 people of who they are going to vote for in an election (out of 2 candidates A and B), and assume the end result is 60% of them saying they will vote for A. How do you estimate the result of an election using MAP if:  </p>

<ol>
<li>candidate A is known to have a popularity of 40% and candidate B 60% (assume this to be the prior distribution)  </li>
<li>the popularity is unknown.</li>
</ol>

<p>I also looked at this answer but I'm still confused:
<a href=""https://stats.stackexchange.com/questions/65212/example-of-maximum-a-posteriori-estimation"">Example of maximum a posteriori estimation</a></p>
",2013-10-20 19:52:06.183
57891,14799.0,2,,55576.0,,,,CC BY-SA 3.0,"<p>Let $P$ = a 12 x 12 matrix with 1s on the diagonals and the negatives of the partial correlations on the offdiagonals, and let $Q = P^{-1}$. Then the original correlation between variables $i$ and $j$ is $r_{ij} = q_{ij}/\sqrt{q_{ii}q_{jj}}$.</p>
",2013-10-20 19:54:22.217
57894,22885.0,1,,,,PCA scores in a for portfolio replication task: stumble over mean-centering question,<pca><optimization><centering>,CC BY-SA 3.0,"<p>I'm trying to implement Principal Component Analysis (PCA) in a portfolio replication procedure.
(The replication procedure looks like regression: there is a vector representing payoffs of an asset under different ecocnomic scenarios,  I need to find a linear combination of vectors of another assets that fits the most closely to an initial asset).</p>

<p>Coefficients I get from regression should tell me how many assets I need to buy and sell to get approximately the same payoff as the asset I'm trying to replicate.</p>

<p>There is a problem with regression as the candidate asset matrix is ill-conditioned: assets show high correlation. There's a hope that orthogonal principal components could resolve this problem.</p>

<p>I need to do a PCA decomposition of candidate assets matrix, take only first $n$ components, do optimization and get components coefficients. Then I need to transform the coefficients back into an original basis.</p>

<p><strong>Now the problem</strong>: PCA usually works with mean-centered data, but if I subtract means from the original data, I don't know how to interpret resulting coefficients in my case and don't know how to reverse the operation.</p>

<p>So far I'm doing eigen-decomposition of a <em>covariance matrix</em>, then using eigenvectors to make an orthogonal transformation of the data that is <em>not mean-centered</em>. Then I'm running a regression (actually L1 norm optimization) to get coefficients and transform them back into an original basis. The results are not bad, but I can't stop thinking <strong>about the problem with mean-centering</strong>, if I'm doing it completely wrong.</p>

<p>I was hoping to find a detailed math reasoning for this problem, but unfortunately failed.
I'm very much a noob in this and my math skills are far from being good, so I would really appreciate your help if you can share some insights on the problem of mean-centering in the PCA.</p>
",2013-10-20 21:37:32.620
57895,20939.0,1,57912.0,,,How to visualize a fitted multiple regression model?,<regression><multiple-regression><data-visualization><reporting>,CC BY-SA 3.0,"<p>I am currently writing a paper with several multiple regression analyses. While visualizing univariate linear regression is easy via scatter plots, I was wondering whether there is any good way to visualize multiple linear regressions? </p>

<p>I am currently just plotting scatter plots like dependent variable vs. 1st independent variable, then vs. 2nd independent variable, etc. I would really appreciate any suggestions.</p>
",2013-10-20 21:46:37.727
57896,22843.0,1,57903.0,,,Explain Statistics: Matching formulas for linear regression,<regression><mathematical-statistics>,CC BY-SA 3.0,"<p>I am looking at my statistics book and an article online for linear regression and was wondering if anyone can verify that these two equations are entirely different. Consider the equation $\hat{y} = ax + b$</p>

<p>In my book, a and b are :</p>

<p>$a = \frac{r \cdot S_{y}}{S_{x}}$</p>

<p>$b = \bar{y} - a\bar{x}$ </p>

<p>$r = \sum \frac{(x_{i} - \bar{x})(y_{i} -\bar{y})}{S_{x}S_{y}(n-1)}$ </p>

<p>$\displaystyle S_{y} = \sqrt{ \frac{\sum (y_i - \bar{y})^{2}}{(n-1)} }$</p>

<p>From one online article, a and b are:</p>

<p>$\displaystyle a = \frac{n \sum x_{i}y_{i} - \sum x_{i} \sum y_{i}}{n \sum x^2_{i} - (\sum x_{i})^2}$ </p>

<p>$b = \bar{y} - a\bar{x}$.</p>

<p>The a from the online article vaguely looks like covariance in the numerator and the denominator looks like variance but for only one random variable, not two. Can someone explain the discrepancy (if there are any) and construct an argument for my book's choice? I can understand the second formulation mainly because it comes from setting partial derivatives to zero to minimize an objective function and then finding the coefficients a and b. </p>
",2013-10-20 22:32:47.917
57897,22887.0,2,,414.0,,,,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/jgYvQ.jpg"" alt=""pie chart example ""></p>

<p>an 'easy to digest' pie chart example for Rick Astley fans that my students seem to enjoy</p>
",2013-10-20 23:10:34.503
57898,19870.0,1,,,,When finding outliers from the Interquartile range why I have to multiply by 1.5?,<outliers>,CC BY-SA 3.0,"<p>I was looking at the outlier detection formula which uses the IQR and I wonder why it should be multiplied by 1.5? Can the constant be increased i.e 3 or 6 to be more ""acid"" if so under what criteria?</p>
",2013-10-20 23:16:57.080
57899,19750.0,1,57920.0,,,Understanding Feature Hashing,<feature-engineering>,CC BY-SA 3.0,"<p>Wikipedia provides the following example when describing <a href=""http://en.wikipedia.org/wiki/Feature_hashing"">feature hashing</a>; but the mapping does not seem consistent with the dictionary defined</p>

<p>For example, <code>to</code> should be converted to <code>3</code> according to the dictionary, but it is encoded as <code>1</code> instead.</p>

<p>Is there an error in the description? How does feature hashing work?</p>

<blockquote>
  <p>The texts:</p>

<pre><code>John likes to watch movies. Mary likes too.
John also likes to watch football games.
</code></pre>
  
  <p>can be converted, using the dictionary</p>

<pre><code>{""John"": 1, ""likes"": 2, ""to"": 3, ""watch"": 4, ""movies"": 5, ""also"": 6, 
""football"": 7, ""games"": 8, ""Mary"": 9, ""too"": 10}
</code></pre>
  
  <p>to the matrix</p>

<pre><code>[[1 2 1 1 1 0 0 0 1 1]
 [1 1 1 1 0 1 1 1 0 0]]
</code></pre>
</blockquote>
",2013-10-20 23:27:14.773
57900,503.0,2,,57898.0,,,,CC BY-SA 3.0,"<p>Certainly you can change the criterion.</p>

<p>The 1.5 multiplier is so that a certain proportion of the sample in a normal population will be outside it. But there is nothing sacred about it.</p>

<p>However, I would caution against <em>any</em> automatic method of selecting outliers. </p>
",2013-10-20 23:38:03.220
57901,20473.0,2,,57778.0,,,,CC BY-SA 3.0,"<p>Ok. Let's do this, for CV's shake.</p>

<p>First compact by setting $C=\frac{1}{\sqrt{2\pi\sigma^2_y}}\frac{1}{\sqrt{2\pi\sigma^2_w}} = \frac{1}{2\pi\sigma_y\sigma_w}$, so </p>

<p>$$f_Y(y) =C \int_{-\infty}^{\infty}\exp\left\{-\frac{(y-w)^2}{2\sigma_y^2}\right\}\exp\left\{-\frac{(w-\mu_w)^2}{2\sigma_w^2}\right\}dw$$</p>

<p>We have 
$$exp\left\{-\frac{(y-w)^2}{2\sigma_y^2}\right\}\exp\left\{-\frac{(w-\mu_w)^2}{2\sigma_w^2}\right\} = 
\exp\left\{-\frac{y^2-2yw+w^2}{2\sigma_y^2}\right\}\exp\left\{-\frac{w^2-2w\mu_w+\mu_w^2}{2\sigma_w^2}\right\} 
=\exp\left\{-\frac{y^2}{2\sigma_y^2}-\frac{\mu_w^2}{2\sigma_w^2}\right\} \exp\left\{-\frac{w^2}{2\sigma_y^2}-\frac{w^2}{2\sigma_w^2}\right\}\exp\left\{\frac{2yw}{2\sigma_y^2}+\frac{2w\mu_w}{2\sigma_w^2}\right\}$$</p>

<p>Setting $s^2\equiv \sigma_y^2+\sigma_w^2$ we arrive at</p>

<p>$$=\exp\left\{-\frac{y^2}{2\sigma_y^2}-\frac{\mu_w^2}{2\sigma_w^2}\right\} \exp\left\{-\frac{s^2}{2\sigma_y^2\sigma_w^2}w^2\right\}\exp\left\{\frac{\sigma_w^2y+\sigma_y^2\mu_w}{\sigma_y^2\sigma_w^2}w\right\}$$</p>

<p>Include the first $\exp$ in the constant, $C^*=C \exp\left\{-\frac{y^2}{2\sigma_y^2}-\frac{\mu_w^2}{2\sigma_w^2}\right\}$.
Set
$$\beta\equiv \frac{s^2}{2\sigma_y^2\sigma_w^2},\qquad \alpha\equiv \frac{\sigma_w^2y+\sigma_y^2\mu_w}{\sigma_y^2\sigma_w^2}$$ to obtain</p>

<p>$$f_Y(y) =C^* \int_{-\infty}^{\infty}e^{-\beta w^2+\alpha w}dw=C^*\left[ \int_{-\infty}^{0}e^{-\beta w^2+\alpha w}dw + \int_{0}^{\infty}e^{-\beta w^2+\alpha w}dw\right]$$</p>

<p>$$=C^* \int_{0}^{\infty}e^{-\beta w^2}\left[e^{-\alpha w}+e^{\alpha w}\right]dw =2C^* \int_{0}^{\infty}e^{-\beta w^2}\operatorname{cosh}(\alpha w)dw$$</p>

<p>where $\operatorname{cosh}$ is the hyperbolic cosine.</p>

<p>Using a formula provided in Gradshteyn &amp; Ryzhik (2007), ""Table of Integrals, Series and Products"", 7th ed., p. 384, eq. 3.546(2) we have</p>

<p>$$f_Y(y)=2C^*\frac 12 \sqrt {\frac {\pi}{\beta}} \exp\left\{\frac {\alpha^2}{4\beta}\right\}$$</p>

<p>Now $$\frac {\alpha^2}{4\beta} = \frac {\left(\frac{\sigma_w^2y+\sigma_y^2\mu_w}{\sigma_y^2\sigma_w^2}\right)^2}{4\frac{s^2}{2\sigma_y^2\sigma_w^2}} = \frac {(\sigma_w^2y+\sigma_y^2\mu_w)^2}{2\sigma_y^2\sigma_w^2s^2}$$ </p>

<p>and bringing back in $C^*$ (and $\beta$) in all its glory we have</p>

<p>$$f_Y(y)=\frac{1}{2\pi\sigma_y\sigma_w}\exp\left\{-\frac{y^2}{2\sigma_y^2}-\frac{2\mu_w^2}{\sigma_w^2}\right\}\sqrt{\pi} \left(\sqrt {\frac{s^2}{2\sigma_y^2\sigma_w^2}}\right)^{-1} \exp\left\{\frac {(\sigma_w^2y+\sigma_y^2\mu_w)^2}{2\sigma_y^2\sigma_w^2s^2}\right\} $$</p>

<p>The constant terms simplify to </p>

<p>$$\frac{1}{2\pi\sigma_y\sigma_w}\sqrt{\pi} \left(\sqrt {\frac{s^2}{2\sigma_y^2\sigma_w^2}}\right)^{-1} = \frac{1}{s\sqrt{2\pi}} $$</p>

<p>and, the exponentials end up in the normal exponential. So in the end</p>

<p>$$f_Y(y) = \frac{1}{s\sqrt{2\pi}}\exp\left\{-\frac{(y-\mu_w)^2}{2s^2}\right\}= N(\mu_w, s^2),\qquad s^2\equiv \sigma_y^2+\sigma_w^2$$</p>
",2013-10-20 23:39:58.167
57902,22843.0,1,57906.0,,,Expectation values as vectors?,<mathematical-statistics><expected-value>,CC BY-SA 3.0,"<p>I want to break down this statement:</p>

<p>$|E[(X - \bar{x})(Y - \bar{y})]|^2 = |&lt;X - \bar{x}, Y - \bar{y}&gt;|^2$ </p>

<p>I am not familiar with expectation values being broken down into vectors. I only know that by definition $\displaystyle E[(X - \bar{x})^2] = \sum_{i=1}^{n} \frac{(x_{i} - \bar{x})^2}{n}$ and I would like to know how expectation values can be viewed as vectors specifically in the context of inner products like $E[(X - \bar{x})^2] = &lt;X-\bar{x}, X-\bar{x}&gt;$  Also whatever happened to the n?</p>

<p>My other question is how do I view covariance as a vector? I know that covariance is $E[XY] - E[X]E[Y]$ so how do I rewrite that in vector form? </p>
",2013-10-21 00:25:41.560
57926,1406.0,2,,57921.0,,,,CC BY-SA 3.0,"<p>If you have looked at the package <a href=""http://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf"" rel=""nofollow"">vignette</a>, you would have seen that package <strong>rugarch</strong> estimates ARFIMA(p,d,q) model with $0&lt;d&lt;1$. So it is not possible to set the integration order higher than one. If you want to keep the value $d$ fixed set <code>fixed.pars=list(arfima=d)</code>. For that naturally you need to set <code>arfima=TRUE</code> in the argument <code>mean.model</code>.</p>
",2013-10-21 11:35:49.830
57903,10135.0,2,,57896.0,,,,CC BY-SA 3.0,"<p>They are mainly equivalent. First rewrite $r$ as $r=\dfrac{S_{xy}}{(n-1)S_xS_y}$, where $S_{xy}=\sum(x_i-\bar{x}).(y_i-\bar{y})$. Now plug in $r$ as above in  $a$ from your book, we have: $a=\dfrac{r.S_y}{S_x}=\dfrac{S_{xy}}{(n-1)S_xS_y}.\dfrac{S_y}{S_x}=\dfrac{S_{xy}}{S_x^2(n-1)}$. But from definition of $S_y$ from your book, we can have $S_x=\sqrt{\dfrac{\sum (x_i-\bar{x})^2}{n-1}}=\sqrt{\dfrac{S_{xx}}{n-1}}$. Now square both sides and use cross multiplication to have: $S_{xx}=S_x^2.(n-1)$. Now replace this last equality in the denominator of $a$ we found before to get: $a=\dfrac{S_{xy}}{S_{xx}}$. Now work on $a$ from online. Note that $S_{xy}=\sum x_iy_i-\bar{y}\sum x_i-\bar{x}\sum y_i+n\bar{y}\bar{x}=\sum x_iy_i-n\bar{y}\bar{x}$. Hence we have $nS_{xy}=n\sum x_iy_i-n^2\bar{y}\bar{x}=n\sum x_iy_i-\sum x_i \sum y_i.$ So what you have in the numerator of $a$ from online is actually $nS_{xy}$. Now work on the denominator of $a$ and factor out $n$ to get $n\sum x_i^2-(\sum x_i)^2=n\Big(\sum x_i^2-\dfrac{(\sum x_i)^2}{n}\Big )=n\Big(\sum x_i^2-\dfrac{n^2\bar{x}^2}{n}\Big )=n\Big(\sum x_i^2-n\bar{x}^2\Big)=nS_{xx}.$ So what you have in the denominator of $a$ is $nS_{xx}$. Therefore, $a$ from online is actually $a=\dfrac{nS_{xy}}{nSxx}=\dfrac{S_{xy}}{Sxx}$ that is equal to the $a$ from your book. Cheers :)</p>
",2013-10-21 00:35:39.223
57904,22889.0,1,,,,Probability that items chosen randomly are defective?,<probability><self-study>,CC BY-SA 3.0,"<p>6 light bulbs are chosen at random from 17 bulbs of which 6 are defective. </p>

<p>(a) What is the probability that exactly 2 are defective? </p>

<p>(b) What is the probability that at most 1 is defective?</p>
",2013-10-21 00:38:08.623
57905,3993.0,2,,57896.0,,,,CC BY-SA 3.0,"<p>I don't know what online article you looked it (maybe you could link it?), but as far as I can tell, the author there assumes that $x$ and $y$ are centered about their means. Under this assumption, the two formulae for $a$ that you posted do agree.</p>

<p>To see this, first note that your first $a$ formula can written as
$$
a = \frac{r \cdot S_y}{S_x} = \frac{{\rm cov}(y,x) \cdot S_y}{(S_y \cdot S_x) \cdot S_x} = \frac{{\rm cov}(y,x)}{S_x^2},
$$
with ${\rm cov}$ referring to the covariance.</p>

<p>Now if you take the second $a$ formula and assume the variables are centered, then the simple sum terms drop out (because then both variables separately sum to 0). So it just reduces to
$$
a = \frac{\sum x_{i}y_{i}}{\sum x^2_{i}} = \frac{\sum x_{i}y_{i} / (n-1)}{\sum x^2_{i} / (n-1)} = \frac{{\rm cov}(y,x)}{S_x^2},
$$
which matches the first formula. So they coincide if the variables are centered, but not in general. Maybe they mention this assumption in the online article you referred to.</p>

<p>Edit: I suggested that they were not equivalent in general, only in the special case where variables are centered. But @Stat has apparently shown that they are equivalent in general. So I will just leave this answer up in case you find considering this special case to be illuminating :)</p>
",2013-10-21 00:40:01.817
57906,20473.0,2,,57902.0,,,,CC BY-SA 3.0,"<p>The ""by definition"" equality you write does not hold.</p>

<p>$$\displaystyle E[(X - \bar{x})^2] = \int_{S_X}(x - \bar{x})^2f_X(x) dx $$
is the correct definition for continuous r.v.'s , with $S_X$ the support of $X$ and $f_X(x)$ the pdf of $X$.
For discrete random variables</p>

<p>$$E[(X - \bar{x})^2] = \sum_{S_X}(x - \bar{x})^2p_X(x) $$
Now <em>IF</em> the $x$'s can be viewed as realization of the same ergodic and stationary stochastic process, <em>THEN</em> $\frac {1}{n}\sum_{i=1}^{n} (x_{i} - \bar{x})^2$ is a consistent estimator of $E[(X - \bar{x})^2]$.</p>

<p>The expected value operator is applied to each element of any vector-matrix. If</p>

<p>$$A=\left[\begin{matrix}
a_{11} &amp;...&amp; a_{1n}\\
... &amp; ...&amp; ... \\
a_{k1} &amp;...&amp;a_{kn}
\end{matrix}\right]$$</p>

<p>then 
$$E(A) = \left[\begin{matrix}
E(a_{11}) &amp;...&amp; E(a_{1n})\\
... &amp; ...&amp; ... \\
E(a_{k1}) &amp;...&amp;E(a_{kn})
\end{matrix}\right]$$</p>

<p>If $\mathbf x$ and $\mathbf y$ are two $n\times 1$ column vectors, then (prime denoting the transpose)
$$ \operatorname{Cov}(\mathbf x,\mathbf y) = E(\mathbf x \mathbf y') - E(\mathbf x)\Big[E(\mathbf y)\Big]'$$</p>

<p>This is the expression for the covariance of two random vectors. If you want the covariance matrix of two samples, look up <a href=""https://math.stackexchange.com/questions/524403/covariance-matrix-for-2-vectors-with-elements-in-the-plane/524507#524507"">this answer in math.SE</a></p>
",2013-10-21 00:57:28.033
57907,22890.0,1,57909.0,,,"What does ""I"" represent in this context?",<point-estimation>,CC BY-SA 3.0,"<p>I'm trying to work on a problem which contains a symbol that I don't recall seeing before - <em>I</em>. I assume it has some special significance but I'm having a hard time looking it up. Relevant portion of the problem:</p>

<p>""Consider a random sample X<sub>1</sub>, ...X<sub>n</sub> from the pdf f(x; Î¸) = .5(1 + Î¸x) <em>I</em><sub>[âˆ’1,1]</sub>(x)""</p>

<p>If it helps, the context of the problem is point estimation. </p>
",2013-10-21 01:01:58.073
57908,15321.0,1,,,,Does adding a dataset in repeated measures change the type of test?,<anova><repeated-measures><t-test>,CC BY-SA 3.0,"<p>I am trying to design a test and am confused between the types of experiment that I should be conducting. Here is the description of what I am doing:</p>

<p>It is a repeated measures test. So, a participant is first given an uncolored node-link graph G1 and asked some questions about the graph. The dependent variable is the time it takes for the participant to answer the questions. Next the same participant is given a colored graph G2 and asked a set of questions. This is counterbalanced by reversing the order of the groups. Next I repeat this procedure except that this time, I provide colored G1 and uncolored G2. In all, I have 4 conditions/groups that perform the test in order as specified below:</p>

<ol>
<li>G1-uncolored G2-colored</li>
<li>G2-colored   G1-uncolored</li>
<li>G1-colored   G2-uncolored</li>
<li>G2-uncolored G1-colored</li>
</ol>

<p>I measure the time for each case. The two graphs are almost equal. The only reason I am using two different graphs is because I cannot use the same graph in both the conditions for a group as if I did that, then the participant would remember his answers from the previous condition.</p>

<p>Now, I am not sure what should I call this method? Should I consider one independent variable (i.e. presence/abscence of color in the graph) or whether I consider two different IV (color + which graph is used (G1/G2?)). In other words, is this just a t-dependent test or a two-way ANOVA (dunno which one)?</p>
",2013-10-21 01:04:11.403
57909,10135.0,2,,57907.0,,,,CC BY-SA 3.0,"<p>Probably that is an indicator function. It is defined as $I_{[-1,1]}(x)=x$ if $-1\leq x\leq1$ and $0$ otherwise. I am not 100% sure since you didn't write your pdf properly.</p>
",2013-10-21 01:09:19.913
57910,20222.0,2,,57904.0,,,,CC BY-SA 3.0,"<p>I believe the Hyper Geometric distribution can solve this problem.</p>

<p>My calculation for (a) is 0.399968</p>
",2013-10-21 01:52:08.647
57927,1406.0,2,,57925.0,,,,CC BY-SA 3.0,"<p>The convention to specify vectors or matrices with bold letters is much more frequently upheld than the convention of upper-case letters for random variables. In the articles I usually read (econometrics, time-series regression mostly) the latter convention is not used, i.e. the random variables are usually lower-case.</p>

<p>Look for the influential papers in your field and try to copy their conventions. Stating the notation somewhere in the beginning is a must usually. </p>
",2013-10-21 11:48:10.340
57911,22891.0,1,,,,Logistic Regression with time slices,<logistic><survival>,CC BY-SA 3.0,"<p>I'm using logistic regression to predict student retention in an online course.  </p>

<p>I have a data of student interactions within a web platform of an online course. The course spans 6 weeks, with new lecture resources and new assignments uploaded at the beginning of each week. The weekly assignments due at the end of each week. Students can watch lecture videos, view/write forum posts, and do assignments. Students can drop out of the course any time (i.e. no longer interacting within the course platform).</p>

<p>Each week, I want to predict the likelihood of a student staying within the course in the next week (<strong>stay in the next week=1, out in the next week=0</strong>). The predictors are the number of times the student watch the lecture videos (<strong>video_views</strong>), the number of posts the student read (<strong>posts_read</strong>), the number of posts the student wrote (<strong>post_written</strong>) and the student's score of this week assignment (<strong>score</strong>).</p>

<p>I'm thinking of building 6 models using logistic regression, for each week. But I also want to make a connection between, say, week 6's model and week 5's model. Can such connection be shown if I use percentage of cumulative values for each predictors (e.g. cumulative number of assignment score out of total assignment score for the whole course) instead of weekly values?</p>

<p>Should I just build a single model with the course week (<strong>course_week</strong>) as another predictor? This is my first time using logistic regression, and I'm not sure about putting a time variable in a logistic regression model.</p>
",2013-10-21 02:07:39.870
57912,5237.0,2,,57895.0,,,,CC BY-SA 4.0,"<p>There is nothing wrong with your current strategy.  If you have a multiple regression model with <em>only</em> two explanatory variables then you could try to make a 3D-ish plot that displays the predicted regression plane, but most software don't make this easy to do.  Another possibility is to use a <a href=""https://web.archive.org/web/20130808154531/http://answers.oreilly.com/topic/2428-when-and-how-to-use-conditional-plots-co-plots-for-data-analysis/"" rel=""nofollow noreferrer"">coplot</a> (see also: <a href=""https://stackoverflow.com/q/14568564/1217536"">coplot in R</a> or <a href=""http://cw.routledge.com/textbooks/9780805861853/R/ch07.pdf"" rel=""nofollow noreferrer"">this pdf</a>), which can represent three or even four variables, but many people don't know how to read them.  Essentially however, if you don't have any interactions, then the predicted <em>marginal</em> relationship between <span class=""math-container"">$x_j$</span> and <span class=""math-container"">$y$</span> will be the same as predicted <em>conditional</em> relationship (plus or minus some vertical shift) at any specific level of your other <span class=""math-container"">$x$</span> variables.  Thus, you can simply set all other <span class=""math-container"">$x$</span> variables at their means and find the predicted line <span class=""math-container"">$\hat y = \hat\beta_0 + \cdots + \hat\beta_j x_j + \cdots + \hat\beta_p \bar x_p$</span> and plot that line on a scatterplot of <span class=""math-container"">$(x_j, y)$</span> pairs.  Moreover, you will end up with <span class=""math-container"">$p$</span> such plots, although you might not include some of them if you think they are not important. (For example, it is common to have a multiple regression model with a single variable of interest and some control variables, and only present the first such plot).</p>
<p>On the other hand, if you <em>do</em> have interactions, then you should figure out which of the interacting variables you are most interested in and plot the predicted relationship between that variable and the response variable, but with several lines on the same plot.  The other interacting variable is set to different levels for each of those lines.  Typical values would be the mean and <span class=""math-container"">$\pm$</span> 1 SD of the interacting variable.  To make this clearer, imagine you have only two variables, <span class=""math-container"">$x_1$</span> and <span class=""math-container"">$x_2$</span>, and you have an interaction between them, and that <span class=""math-container"">$x_1$</span> is the focus of your study, then you might make a single plot with these three lines:<br />
<span class=""math-container"">\begin{align}
\hat y &amp;= \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 (\bar x_2 - s_{x_2})  + \hat\beta_3 x_1(\bar x_2 - s_{x_2}) \\
\hat y &amp;= \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 \bar x_2 \quad\quad\quad\  + \hat\beta_3 x_1\bar x_2 \\
\hat y &amp;= \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 (\bar x_2 + s_{x_2}) + \hat\beta_3 x_1(\bar x_2 + s_{x_2}) 
\end{align}</span></p>
<p>An example plot that's similar (albeit with a binary moderator) can be seen in my answer to <a href=""https://stats.stackexchange.com/a/135349/7290"">Plot regression with interaction in R</a>.</p>
",2013-10-21 02:44:59.367
57913,449.0,2,,57908.0,,,,CC BY-SA 3.0,"<p>It sounds like you counterbalanced the pictures and order only to assess colour so colour presence would be your primary variable of interest.  However, regardless of how well you controlled for picture similarity, colour could have impacted them differentially. In that case you might want to look at a colour x picture interaction because it could compromise your conclusions, point to new directions for research, or both. A picture main effect wouldn't really mean anything.</p>

<p>Often times, in these cases one might report the main effect of colour but not fully report the interaction unless there was something meaningful in it because it's only run to assess the design, not as an outcome you want to generalize.</p>
",2013-10-21 03:29:05.027
57914,22893.0,1,,,,Probability with fair dice,<probability><combinatorics>,CC BY-SA 3.0,"<p>If you roll 5 standard 6 sided dies, what's the probability that you will get at least three 2s?</p>

<p>I imagine it would be 1 - P(0 twos) - P(1 two) - P(2 twos), but I don't know how to calculate the probability of these.</p>
",2013-10-21 03:43:45.820
57915,14799.0,2,,57778.0,,,,CC BY-SA 3.0,"<p>$Y = W + Z$, where $Z$ is normal with mean 0 and variance $\sigma_z^2$ and is independent of $W$. (Note that I am using $\sigma_z^2$ where the OP used $\sigma_y^2$, which I reserve for the marginal variance of $Y$.) Then the unconstrained joint distribution of $(W,Y)$ is bivariate normal with $\mu_y = \mu_w$, $\sigma_y^2 = \sigma_w^2 + \sigma_z^2$, and $\sigma_{wy} = \sigma_w^2$.</p>

<p>Letting $\phi$ denote the standard normal pdf, integrating over the halfplane $Z&lt;0$ gets the following marginal moments of $Y\,|\,(Y&lt;W)$:</p>

<p>Mean $= \mu_y = 2 \int_{-\infty}^\infty \int_{-\infty}^0 (w \sigma_w + \mu_w + z \sigma_z)\,\phi(z)\mathrm{d}z\,\phi(w)\mathrm{d}w = \mu_w - \sigma_z \sqrt{2/\pi}$. </p>

<p>Variance $=\sigma_y^2 = 2 \int_{-\infty}^\infty \int_{-\infty}^0 (w \sigma_w + \mu_w + z \sigma_z - \mu_y)^2\,\phi(z)\mathrm{d}z\,\phi(w)\mathrm{d}w =  \sigma_w^2 + \sigma_z^2 (1-2/\pi)$.</p>

<p>Third central moment $=2 \int_{-\infty}^\infty \int_{-\infty}^0 (w \sigma_w + \mu_w + z \sigma_z - \mu_y)^3\,\phi(z)\mathrm{d}z\,\phi(w)\mathrm{d}w = \sqrt{2}(\pi - 4) \sigma_z^3 / \pi^{3/2}$.</p>

<p>Those can be solved in reverse order to get $\sigma_z^2$, then $\sigma_w^2$, then $\mu_w$, which are necessary and sufficient to specify the unconstrained joint distribution.</p>
",2013-10-21 04:19:30.463
57916,18914.0,1,,,,How to deal with collinearity in lme with categorical IV with > 2 levels,<r><mixed-model><lme4-nlme><multicollinearity>,CC BY-SA 3.0,"<p>I'm analysing data from our experiment. We had participants in 4 groups, each participant was measured 4 times. We measured cortisol in saliva, so it leads us to the linear mixed models, because the individual cortisol levels have different slopes.
I have fitted following model:</p>

<pre><code>lmer1 &lt;- lmer(Cortisol ~ group*measurement + (1|id), data=df)
</code></pre>

<p>I used treatment codig for both categorical variables, because we are interested in differences between 1st measurement in first group with other measurements. </p>

<p>My problem is, that I get strong correlations between factor levels and I'm not sure, how to solve it. Contrast coding would be one solution, but it would answer different question (as I said, we want to compare differences between 1st group,1st measurement and all the others).</p>

<p>This is my correlation matrix for fixed effect from lmer method (lme4 package):</p>

<pre><code>          (Intr) group2 group3 groupP msrmn2 msrmn3 msrmn4 grp2:2 grp3:2 grpP:2 grp2:3 grp3:3 grpP:3 grp2:4 grp3:4
group2      -0.770                                                                                    
group3      -0.650  0.500                                                                             
groupP      -0.557  0.429  0.362                                                                      
measuremnt2 -0.602  0.464  0.391  0.335                                                               
measuremnt3 -0.598  0.460  0.388  0.333  0.521                                                        
measuremnt4 -0.602  0.464  0.391  0.335  0.524  0.521                                                 
grp2:msrmn2  0.461 -0.600 -0.299 -0.257 -0.765 -0.398 -0.401                                          
grp3:msrmn2  0.390 -0.300 -0.589 -0.217 -0.647 -0.337 -0.339  0.495                                   
grpP:msrmn2  0.329 -0.253 -0.214 -0.578 -0.546 -0.284 -0.287  0.418  0.353                            
grp2:msrmn3  0.461 -0.599 -0.300 -0.257 -0.402 -0.772 -0.402  0.519  0.260  0.220                     
grp3:msrmn3  0.383 -0.295 -0.579 -0.213 -0.333 -0.641 -0.333  0.255  0.501  0.182  0.495              
grpP:msrmn3  0.333 -0.256 -0.216 -0.585 -0.290 -0.557 -0.290  0.222  0.188  0.499  0.430  0.357       
grp2:msrmn4  0.462 -0.598 -0.300 -0.257 -0.402 -0.399 -0.767  0.518  0.260  0.220  0.518  0.256  0.223  
grp3:msrmn4  0.390 -0.300 -0.589 -0.217 -0.339 -0.337 -0.647  0.260  0.510  0.185  0.260  0.501  0.188  0.496
grpP:msrmn4  0.329 -0.253 -0.214 -0.578 -0.287 -0.284 -0.546  0.219  0.185  0.493  0.220  0.182  0.499  0.419  0.353
</code></pre>

<p>Do you have suggestions about how to solve this (reduce collinearity/ignore it)?</p>
",2013-10-21 07:14:43.380
58159,22959.0,1,58161.0,,,how many years will it take to achieve six-sigma quality?,<variance><mean>,CC BY-SA 3.0,"<p>Suppose a business is operating at the three-sigma quality level. If projects have an average improvement rate of 50% annually, how many years will it take to achieve six-sigma quality?</p>
",2013-10-24 12:05:49.620
57918,14470.0,1,,,,"Map a normal distribution N(x,s) to an ordinal response variable",<normal-distribution><ordinal-data><psychometrics><latent-variable>,CC BY-SA 3.0,"<p>I am working on a simulation. I am going to extract a series of normally distributed values from a distribution $N(x,s)$ whose mean $x$ and variance $s$ is known. I want to pretend that such distribution $N(x,s)$ is a latent causal variable that determinates a response ordinal variable $O$ which is in a known range (i.e. $[1,10]$). All other characteristics of $O$ (value distribution) are to be determined on the basis of $N$. What function could I use to map $N$ values to $O$? Since this is just a demonstrative simulation of an algorithm working on ordinal data (this mapping is not the core of the simulation, but it is part of the generative data process) and I am not oriented toward any particular psychometric theory, I am looking for some advice for a simple mapping function, though it would be better if based on some psychometric theory.  </p>
",2013-10-21 09:05:56.440
57919,22752.0,1,,,,Derivation of the Bivariate normal distribution using change-of-variable technique,<self-study><normal-distribution><multivariate-analysis>,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/kRbNV.jpg"" alt=""enter image description here"" /></p>
<p>I am not familiar with the change-of-variable technique that the question refers to. Does anyone have an idea what is meant and how one should go about doing it?</p>
",2013-10-21 09:11:38.317
57920,221.0,2,,57899.0,,,,CC BY-SA 3.0,"<p>The matrix is constructed in the following way:</p>

<ul>
<li>rows represent lines</li>
<li>columns represent features</li>
</ul>

<p>and every entry matrix(i,j)=k means: </p>

<p>In line i, the word with index j appears k times.</p>

<p>So <code>to</code> is mapped to index 3. It appears exactly one time in line 1. So m(1,3)=1.</p>

<p>More examples</p>

<ul>
<li><code>likes</code> is mapped to index 2. It appears exactly two times in the first line. So m(1,2)=2</li>
<li><code>also</code> is mapped to index 6. It does not appear in line 1, but one time in line 2. So m(1,6)=0 and m(2,6)=1.</li>
</ul>
",2013-10-21 09:31:36.713
57921,22677.0,1,57926.0,,,"How does one specify arima (p,d,q) in ugarchspec for ugarchfit in rugarch?",<r><time-series><garch>,CC BY-SA 3.0,"<p>Basically I'm trying to fit garch(1,1) model with arima order from auto.arima</p>

<pre><code>&gt; assign(paste(""spec.ret.fin."",colnames(base.name[1]),sep=""""),    
+ ugarchspec(variance.model = list(model = ""fGARCH"", garchOrder = c(1, 1), 
+ submodel = ""GARCH"", external.regressors = NULL, variance.targeting = FALSE), 
+ mean.model = list(armaOrder = c(2,3,4), include.mean = TRUE, archm = FALSE, 
+ archpow = 1, arfima = FALSE, external.regressors = NULL, archex = FALSE), 
+ distribution.model = ""norm"", start.pars = list(), fixed.pars = list()))
</code></pre>

<p>This gives the following result:</p>

<blockquote>
  <p>spec.ret.fin.chn</p>
</blockquote>

<pre><code>*---------------------------------*
*       GARCH Model Spec          *
*---------------------------------*

Conditional Variance Dynamics   
------------------------------------
GARCH Model     : fGARCH(1,1)
fGARCH Sub-Model    : GARCH
Variance Targeting  : FALSE 

Conditional Mean Dynamics
------------------------------------
Mean Model      : ARFIMA(2,0,3)
Include Mean        : TRUE 
GARCH-in-Mean       : FALSE 

Conditional Distribution
------------------------------------
Distribution    :  norm 
Includes Skew   :  FALSE 
Includes Shape  :  FALSE 
Includes Lambda :  FALSE 
</code></pre>

<p>But the same code with <code>arfima=TRUE</code> gives</p>

<blockquote>
  <p>spec.ret.fin.chn</p>
</blockquote>

<pre><code>*---------------------------------*
*       GARCH Model Spec          *
*---------------------------------*

Conditional Variance Dynamics   
------------------------------------
GARCH Model     : fGARCH(1,1)
fGARCH Sub-Model    : GARCH
Variance Targeting  : FALSE 

Conditional Mean Dynamics
------------------------------------
Mean Model      : ARFIMA(2,d,3)
Include Mean        : TRUE 
GARCH-in-Mean       : FALSE 

Conditional Distribution
------------------------------------
Distribution    :  norm 
Includes Skew   :  FALSE 
Includes Shape  :  FALSE 
Includes Lambda :  FALSE 
</code></pre>

<p>How does one replace that <code>d</code> with the integration order (d) of the arima?</p>
",2013-10-21 09:39:42.080
57922,21638.0,2,,57826.0,,,,CC BY-SA 3.0,"<p>Given that $Y_{1}$ and $Y_{2}$ are independent, we have that</p>

<p>$$
\left[\array{Y_{11} \\ Y_{12} \\ Y_{13} \\ Y_{21} \\ Y_{22}}\right] \sim MVN\left(\left[\array{2\\2\\2\\3\\4}\right],\left[\array{3 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\1 &amp; 2 &amp; 0 &amp; 0 &amp;0\\0&amp;0&amp;3&amp;0&amp;0\\0&amp;0&amp;0&amp;4&amp;2\\0&amp;0&amp;0&amp;2&amp;4}\right]\right)
$$</p>

<p>Let</p>

<p>$$
\begin{array}{rcl}X_1 &amp; = &amp; Y_{11}-Y_{13}+Y_{22}\\
X_2 &amp; = &amp; Y_{21}-Y_{12}\end{array}
$$</p>

<p>As $Y_{11},Y_{12},Y_{13},Y_{21},Y_{22}$ are jointly normal, the linear combinations $Y_{11}-Y_{13}+Y_{22}$ and $Y_{21}-Y_{12}$ are normally distributed. It also follows that as any linear combination of $X_{1}$ and $X_{2}$ is a linear combination of $Y_{11},Y_{12},Y_{13},Y_{21},Y_{22}$ so must $X_{1}$ and $X_{2}$ be jointly normal.</p>

<p>All that remains is to determine the mean and covariance of $X_{1}$ and $X_{2}$. Given the linearity of expectations, the mean is trivial to calculate:</p>

<p>$$
\begin{array}{rcl}
E[X_1] &amp;=&amp; E[Y_{11} - Y_{13} + Y_{22}]\\ &amp;=&amp; E[Y_{11}] - E[Y_{13}] + E[Y_{22}]\\
E[X_2] &amp;=&amp; E[Y_{21} - Y_{12}]\\ &amp;=&amp; E[Y_{21}] - E[Y_{12}]
\end{array}
$$</p>

<p>The covariance is equally straightforward yet tedious:</p>

<p>$$
\begin{array}{rcl}
Cov[X_1,X_1] &amp;=&amp; Cov[Y_{11},Y_{11}] + 2 \times Cov[Y_{11},-Y_{13}+Y_{22}] + Cov[-Y_{13}+Y_{22},-Y_{13}+Y_{22}]\\
&amp;=&amp; Cov[Y_{11},Y_{11}] - 2 \times Cov[Y_{11},Y_{13}] + 2 \times Cov[Y_{11},Y_{22}] + Cov[Y_{13},Y_{13}] - 2 \times Cov[Y_{13},Y_{22}] + Cov[Y_{22},Y_{22}]\\\\
Cov[X_2,X_2] &amp;=&amp; Cov[Y_{21},Y_{21}] - 2 \times Cov[Y_{12},Y_{21}] + Cov[Y_{12},Y_{12}]\\\\
Cov[X_1,X_2] &amp;=&amp; Cov[Y_{11},Y_{21}-Y_{12}] + Cov[-Y_{13}+Y_{22},Y_{21}-Y_{12}]\\
&amp;=&amp; Cov[Y_{11},Y_{21}] - Cov[Y_{11},Y_{12}] - Cov[Y_{13},Y_{21}] + Cov[Y_{13},Y_{12}] + Cov[Y_{22},Y_{21}] - Cov[Y_{22},Y_{12}]
\end{array}
$$</p>

<p>Fortunately many of these terms are zero.</p>

<p>Given the tedious nature of the calculations you can do a simple Monte Carlo simulation to check your answers. Here is some <code>R</code> code for achieving that:</p>

<pre><code># Include MASS library for mvrnorm for generating multivariate normally distributed samples
library(MASS)

generateSamples &lt;- function(N)
{
  # Generate N samples from Y1 and Y2 with the given mean vectors and covariance matrices
  Y1 &lt;- mvrnorm(mu=rep(2,3),Sigma=matrix(c(3,1,0,1,2,0,0,0,3),nrow=3,ncol=3),n=N)
  Y2 &lt;- mvrnorm(mu=c(3,4),Sigma=matrix(c(4,2,2,4),nrow=2,ncol=2),n=N)

  # Calculate X1 and X2
  X1 &lt;- Y1[,1] - Y1[,3] + Y2[,2]
  X2 &lt;- Y2[,1] - Y1[,2]

  cbind(X1,X2)
}

# Generate 100000 samples from X1 and X2
mySample &lt;- generateSamples(100000)

# Empirical mean vector
mu &lt;- colMeans(mySample)

# Empirical covariance matrix
Sigma &lt;- cov(mySample,mySample)
</code></pre>
",2013-10-21 09:52:26.270
57923,14470.0,2,,57899.0,,,,CC BY-SA 3.0,"<p>As Steffen pointed out, the example matrix encodes the number of times a word appears in a text. The position of the encoding into the matrix is given by the word (column position on the matrix) and by the text (row position on the matrix). </p>

<p>Now, The hashing trick works the same way, though you don't have to initially define the dictionary containing the column position for each word. </p>

<p>In fact it is the hashing function that will give you the range of possible column positions (the hashing function will give you a minimum and maximum value possible) and the exact position of the word you want to encode into the matrix. So for example, let's imagine that the word ""likes"" is hashed by our hashing function into the number 5674, then the column 5674 will contain the encodings relative to the word ""likes"".</p>

<p>In such a fashion you won't need to build a dictionary before analyzing the text. If you will use a sparse matrix as your text matrix you won't even have to define exactly what the matrix size will have to be. Just by scanning the text, on the fly, you will convert words into column positions by the hashing function and your text matrix will be populated of data (frequencies, i.e.) accordingly to what document you are progressively analyzing (row position).  </p>
",2013-10-21 11:02:59.820
57924,22899.0,1,,,,Duration analysis of unemployment,<survival><survey><random-effects-model><clustered-standard-errors>,CC BY-SA 3.0,"<p>I am trying to run a discrete duration model for analyzing (monthly) unemployment using survey data. I have household-level data, and as such I would like to control for the household effects in my model. I thought to do this by either allowing for cluster effects in the estimation of the standard errors or by random effects (for households) - i.e., I think that fixed effects would not work because there are a lot of households and because of the incidental parameter problem. </p>

<p>My model will include both individual characteristics (e.g., age, school, occupation, since when the person has been unemployed - as they were asked retrospectively), and some other variables) as well as household characteristics (e.g. size, number of people unemployed).</p>

<p>Can anyone provide some comments on my proposed methodology? Are there any things I should be mindful of or are there any better ways of doing this?</p>

<p>Also I would highly appreciate any relevant references. </p>
",2013-10-21 11:16:10.237
57925,22900.0,1,57927.0,,,Notation for random vectors,<random-variable><matrix><notation>,CC BY-SA 3.0,"<p>Random variables are usually denoted with upper-case letters. For example, there could be a random variable $X$. Now, because vectors are usually denoted with a bold lower-case letter (e.g. $\mathbf{z} = (z_0, \dots, z_{n})^{\mathsf{T}}$ and matrices with a bold upper-case letter (e.g. $\mathbf{Y}$), how should I denote a vector of random variables? I think $\mathbf{x} = (X_0, \dots, X_n)^\mathsf{T}$ looks a bit odd. On the other hand if I see $\mathbf{X}$ I would first think it is a matrix. What is the usual way to do this? Of course, I think it would be best to state my notation somewhere in the beginning of paper.</p>
",2013-10-21 11:27:29.240
57928,22901.0,1,,,,Use matrix feature for machine learning or cluster analysis,<machine-learning><clustering><matrix><feature-engineering>,CC BY-SA 3.0,"<p>I have a bunch of features that I would like to use for classification/machine learning and cluster analysis. Normally I use single point values or transformations of values for features and everything is fine</p>

<p>Now I would like to use a matrix as a feature. The matrix is probably going to be a fairly big (say 50x50) but will only be filled with 1's and 0's. It is pretty much an 'image' matrix. It is the shape/pattern of the matrix entries which is important. </p>

<p>Is there anyway I can <strong>easily</strong> use the matrix as a feature for machine learning? I know I could use each matrix entry, say Row1Column1 as a feature and then give it a value, but then I would have 2500 features from my 50x50 matrix, which is what I am trying to get away from. </p>

<p>Any ideas would be greatly appreciated.</p>
",2013-10-21 12:00:09.227
57929,1406.0,2,,57849.0,,,,CC BY-SA 3.0,"<p>When dealing with infinite series of random variables it helps to know when they actually exist. One simple result dealing with the stationarity is the following. The series $\sum \psi_j X_{t-j}$ converges absolutely, almost surely and in mean if $\sum |\psi_j|&lt;\infty$ and $\sup_tE|X_t|&lt;\infty$. In particular if $X_t$ is stationary so is $\sum \psi_jX_{t-j}$. </p>

<p>In your case $\sum |\psi_j|=1+a+a+...$, so the series does not converge and the sum is not defined.</p>

<p>A more general result about existence of the series of the sum is the following. For any collection of random variables $X_t$, if $\sum E|X_t|&lt;\infty$ then $\sum X_t$ converges almost surely, i.e. is defined. Again in your case this condition is violated since $E|X_t|=aE|N(0,1)|$ and the resulting series are not summable.</p>
",2013-10-21 12:01:39.650
57930,4499.0,1,,,,Is this a valid test?,<r><sampling><p-value><validity>,CC BY-SA 3.0,"<p>I have 5000 cases vs 5000 controls with positive/negative outcome. <code>chi.test</code> shows no significance for this data. I get significance when subsetting cases on <em>VarX</em>, 1000 cases vs 5000 controls.</p>

<p>I suspect that doing 1000 cases vs 5000 controls makes up the significance. To eliminate this I am randomly picking 1000 controls and running <code>chi.test</code> 1000 times vs 1000 cases.</p>

<p>Here is the histogram of pvalues and the red line is <code>Pvalue</code> for 1000 cases vs 5000 controls:
<img src=""https://i.stack.imgur.com/Nc0qA.jpg"" alt=""enter image description here""></p>

<p>From this can I conclude that there is no significance?
If there is a better way of doing this please advise.</p>
",2013-10-21 12:10:33.237
57931,2666.0,2,,57782.0,,,,CC BY-SA 3.0,"<p>There are several things wrong with that approach, including:</p>

<ul>
<li>Seeking a cutoff for a continuous probability</li>
<li>Using an arbitrary cutoff of 0.5</li>
<li>Assuming that the cost of a ""false positive"" and a ""false negative"" are the same for all subjects</li>
<li>Using weights that are not fractional</li>
<li>Using weights that are estimated</li>
<li>Overriding maximum likelihood estimation</li>
<li>Not utilizing optimum Bayes decision theory, which dictates that optimum decisions are based on full information (not on whether something exceeds something else) and utility/loss/cost functions</li>
</ul>
",2013-10-21 12:33:40.250
57932,20498.0,1,57933.0,,,Combining instance-based learning with regression analysis to improve predictions,<regression><prediction><rapidminer>,CC BY-SA 3.0,"<p>I have a table that contains items. Each day its possible that an incident can occur to some items but equally no incident may happen. I wish to see if its possible to create a prediction for the next day based on the information below</p>

<pre><code>**Date      Item 1    Incident       Location**
20130701    40          12           ES
20130702    50           6           ES
20120701    60          10           ES
20120702    20           8           ES 
</code></pre>

<p>This could be a regression problem (at the moment I'm only interested in seeing the next days prediction) but I have an idea that taking the same time period for the previous year and combining it with this years data based on week number (e.g. week 37 this year with week 37 last year) would enrich the prediction because the weather could be similar or the conditions for the incident could be similar. This might make it possible to get a better prediction.</p>

<p>I'm using rapidminer for this but would know if anyone would know how to implement this correctly or even point me to some research papers that may have covered this. I can get up to five years of data so in essence week 37 five times but for the different years.</p>
",2013-10-21 13:04:41.890
57933,16043.0,2,,57932.0,,,,CC BY-SA 3.0,"<p>Attention conservation notice: this is a long comment.</p>

<p>Sure -- this approach is called instance-based learning and matches current weather patterns to the ""best matches"" among previously-observed weather conditions; an introduction appears in <em>Data Mining: Practical Machine Learning Tools and Techniques.</em> </p>

<p>Your proposal merges matching and an explicit generalization (regression). Some things to consider:
How do you measure similarity to previous instances? I don't know anything about your data, but I suspect there are patterns that have resulted in <em>different</em> patterns at subsequent intervals.</p>

<p>A second part of building the model will be considering how to combine/weight the previous instances with the regression forecast to make a prediction. This can be done by simple averaging or more complicated methods like weighting the instances according to <em>how</em> similar they are, according to some metric.</p>

<p>I'm not familiar with <code>rapidminer</code> so I'm afraid my answer is not complete with respect to your tool-specific advice.</p>
",2013-10-21 13:17:50.113
57934,12683.0,2,,57869.0,,,,CC BY-SA 3.0,"<p>There's <a href=""http://books.google.co.uk/books?id=F78QmEmce_gC"" rel=""nofollow noreferrer"">Barnett (2009), <em>Comparative Statistical Inference</em></a>, which does a good job of contrasting different methodologies, with only as much maths as necessary. Nothing on Machine Learning though&mdash;some of the references <a href=""https://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning"">here</a> may be useful (&amp; indeed the answers).</p>

<p>A survey of different areas of application (psychometrics, econometrics, &amp;c.) would be interesting, &amp; I hope someone can suggest one.</p>
",2013-10-21 13:48:28.970
57935,10409.0,1,57944.0,,,What type of regression to use with negative values?,<r><regression>,CC BY-SA 3.0,"<p>If possible, please explain things like I'm 5.  I know very little about this subject, but would like to learn more.</p>

<p>I have a data frame (in R) containing <code>player_id</code>, <code>points</code>, <code>away</code>, <code>opponent_fact_1</code>, <code>opponent_fact_2</code>.  <code>points</code> can be negative.  <code>away</code> lets us know if the game was at home or away (0 or 1).  <code>opponent_fact_1</code> and <code>opponent_fact_2</code> gives us a stat about the opponent.  <code>opponent_fact_1</code> is on a scale of about 0.0-5.5.  <code>opponent_fact_2</code> is on a scale of about 70.0-95.0.  The issue with the two facts is that there are fewer opponents as you reach the upper and lower bounds, so fewer data points exist at those levels.</p>

<p>How can I determine how much of an influence <code>away</code>, <code>opponent_fact_1</code>, and <code>opponent_fact_2</code> has on a player's <code>points</code>?</p>

<p>I asked someone online how to do this and he said use poisson regression, but didn't go into detail.  Why would regression be helpful here?  What is it?  And I read that you can't use poisson regression with negative values?  Also, how do I deal with the fewer data points around the upper and lower bounds?</p>

<p>I'm using R, so any examples in R would be awesome.  Explaining the output would be even better.</p>

<p>I hope this isn't asking for too much.</p>

<p><strong>EDIT: Added sample data</strong></p>

<pre><code>  player_id opponent_team_id away  points opponent_fact_1 opponent_fact_2 
1       695               22    0     0.0        2.888889           81.58 
2       695               30    1     1.2        2.750000           81.58 
3       695                4    1     3.0        3.714286           69.57 
4       695               20    0    -3.0        3.000000           84.09 
5       695               14    0     0.0        2.444444           72.97 
</code></pre>
",2013-10-21 13:55:49.620
57938,21756.0,2,,57686.0,,,,CC BY-SA 3.0,"<p>Sklar's Theorem is very powerful and reads quite easy, but in practice one will have to know the (analytical) distributions before any ""inverse"" can be applied. Without knowing the exact joint and marginal distributions, one can still plot the empirical copula of a data set. At times, these plots reveal e.g. asymmetric patterns that can not be captured with correlation coefficients.</p>

<p>Typically, the scaled ranks of a data set (scaled to the interval (0,1)) are a good estimate of a non-parametric inverse of the marginal distributions. An empirical copula's density can then be represented as a scatter plot of the data's scaled ranks. As the spread of (many) points is at times hard to grasp, a smoothed scatter plot provides a visual proxy of the empirical copula's density (see e.g. the function ""smoothScatter"" from the (base) graphics package or ""dependencePlot"" in the package spcopula available from r-forge). In case one does know the marginal distributions, one can of'course replace the scaled ranks by the marginally ""inverted"" data.</p>

<p>In case one does know the copula's family and corresponding parameter(s) (sometimes a 1-1 relationship with Kendall's tau), 3D-plots of the copula can be obtained using the function ""persp"" with a copula and PDF/CDF function:</p>

<pre><code>library(copula)
persp(claytonCopula(2), dCopula) # plotting the PDF
persp(claytonCopula(2), pCopula) # plotting the CDF
</code></pre>

<p>The package copula provides as well the function ""fitCopula"" that helps to estimate a copula's parameter(s) using different estimators.</p>
",2013-10-21 15:15:26.320
57939,8888.0,1,92891.0,,,How to correctly apply the Nemenyi post-hoc test after the Friedman test,<nonparametric><multiple-comparisons><post-hoc>,CC BY-SA 4.0,"<p>I'm comparing the performance of multiple algorithms on multiple data sets. Since those performance measurements are not guaranteed to be normally distributed, I chose the Friedman Test with the Nemenyi post-hoc test based on <a href=""http://www.jmlr.org/papers/v7/demsar06a.html"" rel=""noreferrer"">DemÅ¡ar (2006)</a>.</p>

<p>I then found another paper that, aside from suggesting other methods like the Quade test with subsequent Shaffer post-hoc test, they apply the Nemenyi test differently.</p>

<h1>How do I apply the Nemenyi post-hoc test correctly?</h1>

<p><strong>1. Using the Studentized range statistic?</strong></p>

<p>In DemÅ¡ar's paper it says to reject the null hypothesis (no performance difference of two algorithms) if the average rank difference is greater than the critical distance CD with
<span class=""math-container"">$$
CD = q_{\alpha}\sqrt{{k(k+1)}\over{6N}}
$$</span></p>

<blockquote>
  <p>""where critical values qÎ± are based on the Studentized range statistic divided by <span class=""math-container"">$\sqrt{2}.$</span>""</p>
</blockquote>

<p>After some digging I've found that you those ""critical values"" can be looked up for certain alphas, for example in a <a href=""http://cse.niaes.affrc.go.jp/miwa/probcalc/s-range/srng_tbl.html#fivepercent"" rel=""noreferrer"">table for <span class=""math-container"">$\alpha = 0.05$</span></a>, for infinite degrees of freedom (at the bottom of each table).</p>

<p><strong>2. or using the normal distribution?</strong></p>

<p>Just when I thought I knew what to do, I found another paper that confused me again, because they were only using the normal distribution. DemÅ¡ar is stating a similar thing at page 12:</p>

<blockquote>
  <p>The test statistics for comparing the i-th and j-th classiï¬er using these methods is
  <span class=""math-container"">$$
z = {{(R_i âˆ’ R_j)}\over{\sqrt{{k(k +1)}\over{6N}}}}
$$</span>
  The z value is used to ï¬nd the corresponding probability from the table of normal distribution, which is then compared with an appropriate <span class=""math-container"">$\alpha$</span>. The tests differ in the way they adjust the value of <span class=""math-container"">$\alpha$</span> to compensate for multiple comparisons.</p>
</blockquote>

<p>At this paragraph he was talking about comparing all algorithms to a control algorithm, but the remark ""differ in the way they adjust ... to compensate for multiple comparisons"" suggests that this should also hold for the Nemenyi test.</p>

<p>So what seems logical to me is to calculate the <em>p</em>-value based on the test statistic <span class=""math-container"">$z$</span>, which is normally distributed, and correct that one by dividing through <span class=""math-container"">$k(k-1)/2$</span>.</p>

<p>However, that yields completely different rank differences at which to reject the null hypothesis. And now I'm stuck and don't know which method to apply. I'm strongly leaning towards the one using the <em>normal distribution</em>, because it is simpler and more logical to me. I also don't need to look up values in tables and I'm not bound to certain significance values.</p>

<p>Then again, I've never worked with the <em>studentized range statistic</em> and I don't understand it.</p>
",2013-10-21 15:35:51.460
57940,8414.0,1,,,,Three open philosophical problems in statistics,<probability>,CC BY-SA 3.0,"<p>I recently finished reading <a href=""http://rads.stackoverflow.com/amzn/click/0805071342"">The Lady Tasting Tea</a>, a fun book about the history of statistics.  At the end of the book, the author, <a href=""http://en.wikipedia.org/wiki/David_Salsburg"">David Salsburg</a>, proposes three open philosophical problems in statistics, the solutions to which he argues would have larger implications for the application of statistical theory to science.  I had never heard of these problems before, so I am interested in other people's reactions to them.  I am venturing into territory about which I have little knowledge, so I'm just going to describe Salsburg's portrayal of these problems and pose two general questions about these problems below.</p>

<p>Salsburg's philosophical problems are:</p>

<blockquote>
  <ol>
  <li>Can statistical models be used to make decisions?</li>
  <li>What is the meaning of probability when applied to real life?</li>
  <li>Do people really understand probability?</li>
  </ol>
</blockquote>

<p><strong>Statistics and decision making</strong></p>

<p>As an illustration of the problem presented in question 1, Salsburg presents the following paradox.  Suppose we organize a lottery with 10000 unnumbered tickets.  If we use probability to make a decision about whether any given ticket will win the lottery by rejecting this hypothesis for tickets with probabilities below, say, .001, we will reject the hypothesis of a winning ticket for all the tickets in the lottery!</p>

<p>Salsburg uses this example to argue that logic is inconsistent with probability theory as probability theory is currently understood, and that, therefore, we currently do not have a good means of integrating statistics (which, in its modern form, is based in large part on probability theory) with a logical means of decision-making.</p>

<p><strong>The meaning of probability</strong></p>

<p>As a mathematical abstraction, Salsburg argues that probability works well, but when we attempt to apply the results to real life, we run into the problem that probability has no concrete meaning in real life.  More specifically, when we say that there is a 95% chance of rain tomorrow, it is unclear to what entities that 95% applies.  Does it apply to the set of possible experiments that we could conduct to obtain knowledge about rain?  Does it apply to the set of people who might go outside and get wet?  Salsburg argues that the lack of a means to interpret probabilities creates problems for any statistical model based on probability (i.e., most of them).</p>

<p><strong>Do people understand probability?</strong></p>

<p>Salsburg argues that one attempt to resolve the issues with the lack of a concrete means of interpreting probability is through the concept of ""<a href=""http://en.wikipedia.org/wiki/Subjective_expected_utility"">personal probability</a>"", proposed by <a href=""http://en.wikipedia.org/wiki/Jimmie_Savage"">Jimmie Savage</a> and <a href=""http://en.wikipedia.org/wiki/Bruno_de_Finetti"">Bruno de Finetti</a>, which understands probability as personal beliefs about the likelihood of future events.  However, in order for personal probability to provide a coherent basis for probability, people need to have a common understanding of what probability is and a common means of using evidence to draw conclusions about probability.  Unfortunately, evidence such as that produce by Kahneman and Tversky suggests that personal beliefs might be a difficult basis on which to create a coherent basis for probability.  Salsburg suggests that statistical methods that model probabilities as beliefs (perhaps such as Bayesian methods? I'm stretching my knowledge here) will need to deal with this problem.</p>

<p><strong>My questions</strong></p>

<ol>
<li>To what extent are Salsburg's problems really problems for modern statistics?</li>
<li>Have we made any progress towards finding resolutions to these problems?</li>
</ol>
",2013-10-21 15:38:25.327
57941,,1,57950.0,,user30490,Reciprocal roots and eigenvalues relationship in time series,<time-series><self-study>,CC BY-SA 3.0,"<p>I came across a result in a time series textbook the other day and have not been able to understand why it is true (the authors don't give a proof but just state it as true).  I want to show that the eigenvalues of the matrix $\mathbf{G}$ given by</p>

<p>$$G=
\begin{pmatrix}
\phi_1&amp;\phi_2 &amp;\phi_3 &amp;...&amp;\phi_{p-1} &amp; \phi_p\\
1 &amp; 0 &amp;0 &amp;...&amp; 0 &amp;0\\
0 &amp; 1 &amp; 0 &amp;... &amp;0 &amp;0\\
\vdots &amp; &amp; &amp; \ddots&amp;0&amp;0\\
0 &amp; 0 &amp;...&amp;...&amp;1 &amp;0
\end{pmatrix}
$$</p>

<p>correspond to the reciprocal roots of the $AR(p)$ characteristic polynomial</p>

<p>$$\Phi(u)=1-\phi_1u-\phi_2u^2-...-\phi_pu^p$$</p>

<p>The one thing i was able to deduce is that the eigenvalues of $\mathbf{G}$ must satisfy
$$\lambda^p-\phi_1\lambda^{p-1}-\phi_2\lambda^{p-2}-...-\phi_{p-1}-\phi_p=0$$</p>
",2013-10-21 15:39:32.187
57942,22909.0,1,,,,P value calculation,<p-value>,CC BY-SA 3.0,"<p>I have three groups of experiments. For each experiment I am looking for the percentage of occurrence of case x.</p>

<p>In the first group I have 15 experiments. The case x was seen 10.191% of the total time for 15 experiments.</p>

<p>In the second group I have 6 experiments. The percentage of x is 1.564%.</p>

<p>In the third group I have 3 experiments. The percentage of x is 0%.</p>

<p>I want to show that occurrence of case x  significantly decreased from group one to two and to three. Thus I want to calculate the p-values.  Can anyone tell me how to do it?</p>

<p>Edit:</p>

<p>The number of measurements for the first group is 22568 and 10.191% of these measurements are case x.</p>

<p>The second group has 1854 measurements (1.564% are x) and the third group has 1164 measurements (0% x)</p>
",2013-10-21 15:42:32.237
57943,2081.0,2,,57894.0,,,,CC BY-SA 3.0,"<p>Unstandardized principal component values (PC scores) are given by $\bf XU$, where $\bf X$ is the data matrix and $\bf U$ is the matrix of eigenvectors. If covariance matrix decomposed to $\bf U$, - then $\bf X$ must be centered to give proper PC scores. But you said that you used raw data to compute the PC scores despite that eigenvectors came from covariance matrix. What happens then?</p>

<p>Example is below. Variables are V1, V2. PC1 and PC2 are the scores computed in the usual way; these are centered PC scores. dcPC1 and dcPC2 are the scores computed by ""your"" way; these are decentered PC scores.</p>

<pre><code>      V1       V2      PC1      PC2    dcPC1    dcPC2

  6.7662   8.6155   2.8843    .3930  10.9224    .8427
  5.9534   6.9533   1.0506    .6401   9.0887   1.0897
  5.1772   4.6352  -1.3083   1.2819   6.7298   1.7315
  5.3906   3.5785  -2.0685   2.0461   5.9696   2.4958
  3.0136   6.5524   -.9154  -1.5821   7.1227  -1.1325
  1.4195   3.8332  -4.0620  -1.3978   3.9761   -.9481
  4.9248   6.0971   -.2327    .2602   7.8054    .7098
  4.5031   8.5152   1.5441  -1.4333   9.5822   -.9837
  6.0504   6.8867   1.0491    .7578   9.0872   1.2074
  1.7513   2.2287  -5.2121   -.2308   2.8260    .2188
  4.4432   7.1862    .4056   -.7451   8.4437   -.2955
  2.8280   5.4160  -1.9635  -1.1054   6.0746   -.6558
  6.8661   3.4229  -1.3786   3.3597   6.6595   3.8093
  3.6724   3.9823  -2.6869    .3930   5.3512    .8426
  5.8395   6.4047    .5311    .8500   8.5692   1.2996
  6.7118  11.4956   5.2492  -1.2516  13.2873   -.8020
  4.7179   8.8247   1.9208  -1.4266   9.9589   -.9770
  1.0230   3.2331  -4.7813  -1.3943   3.2568   -.9446
  7.2815  10.1138   4.4165   -.0105  12.4546    .4391
  8.4197  10.7265   5.5581    .5958  13.5962   1.0454
</code></pre>

<p>The pic displays the principal components drawn in the space of the variables. In both computational cases, the PCs are the same - as axes. They orthogonally cross each other in the centre of the cloud. This is because covariance matrix was analyzed (it implies centering). However, the scores (shown as markers on the PC axes) are different. ""Decentered PC1"" = ""Centered PC1"" + 8.04. ""Decentered PC2"" = ""Centered PC2"" + 0.45. It is unclear what use could be of such decentered scores, since they do not share their means with each other and with the data cloud.</p>

<p><img src=""https://i.stack.imgur.com/i1QN1.jpg"" alt=""enter image description here""></p>
",2013-10-21 15:55:45.527
57944,1805.0,2,,57935.0,,,,CC BY-SA 3.0,"<p>Regular linear regression (e.g. the <code>lm</code> or <code>glm</code> functions in R) handles negative values just fine.</p>

<p>One model you could try would be:</p>

<pre><code>model1 &lt;- lm(points ~ away + opponent_fact_1 + opponent_fact_2, data=my_data_frame)
summary(model1)
</code></pre>

<p>If you've got a lot of data (and several rows per player and per opponent), you could also try this model:</p>

<pre><code>model2 &lt;- lm(points ~ away + factor(player_id) + factor(opponent_team_id), data=my_data_frame)
summary(model2)
</code></pre>

<p>This will give you a model that includes a coefficient for each player, and for each opponent_team_id.  These coefficients will represent the average points expected for a player, as well as the average points expected against a given opponent.</p>

<p>Have you every run a regression model before?  What's the goal of this analysis?</p>
",2013-10-21 16:16:55.103
57945,651.0,2,,57940.0,,,,CC BY-SA 3.0,"<p>Can we use statistics/probability to make decisions?  Of course we can, the way in which we should go about this is by choosing the course of action that minimises our expected loss.  In this case, all lottery numbers are equally likely to come up; if all provide the same prize, then the expected loss is the same for any number, so it doesn't matter which we choose.  If we also have the option not to play the lottery, that would probably be the course of action we should take as it will minimise our expected loss assuming that the lottery makes a profit for somebody (or at least covers the cost of running the lottery).  Of course this is just common sense and is consistent with logic, and could be expressed in purely probabilistic terms.</p>

<p>It seems to me that the question arises from a rather limited view of how statistics can be used to make decisions, it doesn't have to be done with quasi-Fisherian hypothesis tests.</p>

<p>I would suggest that Jaynes book on Probability theory goes a fair way to addressing points (2) and (3), probabilities can represent objective measures of plausibility without them being ""personal probabilities"", but I expect @probabilityislogic can explain that better than I can.</p>
",2013-10-21 16:18:15.443
57946,10469.0,2,,57940.0,,,,CC BY-SA 3.0,"<p>I don't think these really are questions which can be answered conclusively.
(IOW, they are, indeed, philosophical).
That said...</p>

<h1>Statistics and decision making</h1>

<p>Yes, we can use statistics in decision making.</p>

<p>However, there are limits to its applicability; IOW, one has to understand what one is doing.</p>

<p>This is fully applicable to <em>any</em> theory.</p>

<h1>The meaning of probability</h1>

<p>95% probability of rain tomorrow means that if <em>your</em> cost of preparing for a rain (e.g., taking the umbrella) is <code>A</code> and <em>your</em> cost of being caught in the rain unprepared (e.g., wet suit) is <code>B</code>, then you should take the umbrella with you <em>iff</em> <code>A &lt; 0.95 * B</code>.</p>

<h1>Do people understand probability?</h1>

<p>No, people do not understand much, least of all probability.</p>

<p>Kahneman and Tversky have shown that human intuition is flawed on many levels, but <em>intuition</em> and <em>understanding</em> are not identical, and I would argue that people understand even less than they intuit.</p>

<h1>To what extent are Salsburg's problems really problems for modern statistics?</h1>

<p>Nil. I don't think anyone cares about these issues except for philosophers and those in a philosophical mood.</p>

<h1>Have we made any progress towards finding resolutions to these problems?</h1>

<p>Everyone who cares has a resolution.
My personal resolution is above.</p>
",2013-10-21 16:55:13.357
57947,15209.0,1,,,,Maximizing incomplete likelihood,<bayesian><maximum-likelihood><naive-bayes><hierarchical-bayesian><hyperparameter>,CC BY-SA 3.0,"<p>Given the conditional distribution $p(x|y)$ and the prior of the hidden variables $p(y|\theta)$ with unknown hyper-parameter $\theta$. Now we have observed i.i.d. samples of $x$. </p>

<p>Besides the Bayes and empirical Bayes approach, is it possible to estimate both the hidden variables $y$ and the hyper-parameter $\theta$ directly by maximizing the incomplete likelihood $p(x,y|\theta)$?</p>

<p>From the Bayesian perspective, it is equivalent to finding the joint mode of $p(y,\theta|x)$ given an uninformative flat prior of $\theta$. </p>
",2013-10-21 16:58:59.500
57948,22910.0,1,,,,Comparison of cell counts with a right censoring,<nonparametric><normality-assumption><censoring>,CC BY-SA 3.0,"<p>I have cell counts related to the action of different microorganisms and I want to compare their distribution. It's supposed they follow a normal distribution after a log transformation, but I can't register all the data due to the limit of measuring instrument, so I obtain a very similar normal curve but it's cut in the point in which I can't register the data, and if I run a K-S test, it doesn't support the normality hypothesis (but n is large). </p>

<p>My questions:</p>

<p>I find this can be an usual problem, so how would you demonstrate normality in these cases? or is it impossible?</p>

<p>I think a Kruskal-Wallis test could works in my case, but I don't know if I have to take into account something else. Someone has suggested me to use an ANOVA due to my n is very high (n>2000) in each microorganism, despite the lack of normality, is this right?</p>

<p>After reading comments I think I have a right censoring. I have seen in survival analysis is used a Kaplan-Maier estimator, but this case is different. Would it be better just remove censored data and run a non-parametric test? </p>
",2013-10-21 17:02:30.750
57949,22752.0,1,57974.0,,,$Cov(\hat{\epsilon})$ in a linear regression model,<regression>,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/VIoz3.jpg"" alt=""enter image description here""></p>

<p>I proved the result myself too, but my proof was a lot lengthier. Apparently however, it can be done in just these two lines. However, I don't really get the first step here. Could anyone please explain me why the first equality holds?</p>
",2013-10-21 17:24:54.920
57950,668.0,2,,57941.0,,,,CC BY-SA 3.0,"<p>An eigenvalue of any matrix $\mathbb{G}$ must be a root of its characteristic polynomial $p_G(\lambda) = \det(\lambda - \mathbb{G}).$  By row-reducing the latter we readily find that</p>

<p>$$p_G(\lambda) = \lambda^p-\phi_1\lambda^{p-1}-\phi_2\lambda^{p-2}-...-\phi_{p-1}\lambda-\phi_p.$$</p>

<p>If $u = 1/\lambda$ is the reciprocal of an eigenvalue, then $1/u = \lambda,$ whence</p>

<p>$$0 = p_G(\lambda) = p_G\left(\frac{1}{u}\right) = \left(\frac{1}{u}\right)^p-\phi_1\left(\frac{1}{u}\right)^{p-1}-\phi_2\left(\frac{1}{u}\right)^{p-2}-...-\phi_{p-1}\left(\frac{1}{u}\right)-\phi_p \\
= u^{-p}\left(1-\phi_1u^{1}-\phi_2u^{2}-...-\phi_{p-1}u^{p-1}-\phi_pu^{p}\right) = u^{-p}\Phi(u).$$</p>

<p>Since $u$ must be nonzero (it's the reciprocal of a number), multiplying both sides by $u^p$ does not change the roots: $u$ must therefore be a root of $\Phi$.</p>
",2013-10-21 17:35:58.853
57951,22911.0,1,,,,Finding the most relevant factors that lead to an event,<statistical-significance><conditional-probability><excel><java>,CC BY-SA 3.0,"<p>I have been tasked with looking at a set of data with about 6000 records that each have 60 or so qualities associated with them (Let these be X1, X2, ... ) and determining what are the top 8 factors that determine whether a record will have a certain designation (there are three, Let's say, A, B, and C). Most of these X's only have two possible values so they are easy to deal with. However there is an X that has 6 values and an X that has 8 values which I have determined to be fairly important. For now, I am just interested in finding the factors amongst the data that lead a record to be event A. My approach so far has been to calculate all of the probabilities of each individual factor (A, B, C and all X's), calculate the probabilities of all of the X's given factor A, and to then use Bayes' theorem to calculate the probability of event A given the X. This gives me a feel for which factors seem to be contributing to event A but I am not sure how I should continue my analysis.</p>

<p>I also delved into looking at intersections of events but I feel like that is a big time sink that yields little to no insight because that requires counting all of the intersections and when you want to look at 5+ factors, the amount of counting you have to do is ridiculous (For example, if you want to look at 5 factors, you need to find 2^5 different events because of the possible ways the events can line up. This causes problems especially when I am looking at the factors that have 6 or 8 different possibilities). I wrote some simple java classes to read in a tab delimited text file of the records and to do all of the counting and calculating for me for individual events and some intersections and I also have an excel spreadsheet that I set up first but that seemed to be fairly slow for performing the calculations. Besides adding specific code (or specific formulas in excel) to pick up each individual event, I am not sure how I can efficiently calculate these probabilities. And I'm not sure if calculating these conditional probabilities of the intersections will help me answer the overall question of what are the top 8 factors that contribute to event A. Any thoughts would be greatly appreciated.</p>
",2013-10-21 17:36:12.943
57952,10964.0,1,,,,Standard Error vs. Standard Deviation of Sample Mean,<standard-error>,CC BY-SA 3.0,"<p>I'm having difficulty figuring out the difference between (typically referred to as S.E):</p>

<p>A. The Standard Error</p>

<p>And</p>

<p>B. The Standard Deviation of the Sample Mean (typically referred to as s)</p>

<p>Are they the same thing?</p>

<p>OR</p>

<ol>
<li>S.E is the standard deviation of the the mean of the sampling distribution similar formula as the standard deviation except you use n-1 instead of n in the denominator</li>
<li>s is the standard deviation of a single random sample -- same formula as the standard deviation </li>
</ol>
",2013-10-21 17:37:59.103
57953,22912.0,2,,57941.0,,,,CC BY-SA 3.0,"<p>For any polynomial $p(x)$, we can define a reciprocal polynomial of the form $x^n p(1/x)$ where the roots of this reciprocal function are the reciprocal roots of the original polynomial.</p>

<p>In the case of $\phi(u)$, the reciprocal polynomial would look like:<br>
\begin{align}
u^p p(1/u) &amp;= u^p - \phi_1 u^p/u - \phi_2 u^p/u^2  - \ldots - \phi_p u^p/u^p   \\
             &amp;= u^p - \phi_1 u^{p-1}  - \phi_2 u^{p-2}  - \ldots - \phi_p.
\end{align}<br>
A reordering of this equation reveals the exact characteristic eigenvalue equation that you have found above, only now the $u$ values have become constant eigenvalues.  Thus, solutions to the eigenvalue equation will be reciprocal roots of the ${\rm AR}(p)$ characteristic equation.</p>
",2013-10-21 17:45:28.917
57954,20473.0,2,,57952.0,,,,CC BY-SA 3.0,"<p>The official term for the <em>dispersion measure</em> (of a distribution, of a sample etc) is ""standard deviation"" - the square root of the variance. </p>

<p>The tern ""standard error"" is more often used in the context of a <em>regression</em> model, and you can find it as ""the standard error of regression"". It is the square root of the sum of squared <em>residuals from the regression</em> - divided sometimes by sample size $n$ (and then it is the maximum likelihood estimator of the standard deviation of the error term), or by $n-k$ ($k$ being the number of regressors), and then it is the ordinary least squares (OLS) estimator of the standard deviation of the error term.</p>

<p>So you see that they are closely related, but not the same thing.</p>
",2013-10-21 17:56:34.387
57955,,2,,57952.0,user31668,,,CC BY-SA 3.0,"<p>For normally distributed data, the SE = s, as the mean is an explicit parameter of the normal distribution. In general, standard error arises in Likelihood theory, where you are forming inferences from a likelihood function as opposed to the true sampling distribution. For example, if you are modeling some data as iid Exponential then you would form the likelihood function for your data $L(X|\lambda)= \prod L_{exp}(x_i|\lambda)$, with unknown $\lambda$ and then optimize L(X|$\lambda$) for $\lambda^*$ (i.e, maximum likelihood estimator). The standard error is defined as the curvature of the quadratic approximation to log(L(X|$\lambda^*$))at the MLE, which will equal the standard deviation for normal data. the only difficulty is that for non-normal data, you will need to do a second step to transform the actual parameters of your distribution (e.g., $\lambda$) into an estimate of the sample mean. Here, you would map $\frac{1}{\lambda} \rightarrow\mu$, so the likelihood of the latter equals that of the former, then take the log of that likelihood and get a standard error of that transformed likelihood function. Sorry for the long answer, but its not super clear cut in all cases. Sometime, its even used loosely, so yo need to read the documentation to really know.</p>
",2013-10-21 17:56:53.347
57956,16644.0,2,,57952.0,,,,CC BY-SA 3.0,"<p>The standard deviation of the mean is usually unknown. We would write it as $$  \sigma_{\bar x } ={\sigma \over \sqrt n} $$</p>

<p>The standard error of the mean is an estimate of the standard deviation of the mean. $$ \hat \sigma_{\bar x}  = {s \over \sqrt n}.  $$ </p>
",2013-10-21 17:57:03.800
57957,22555.0,1,,,,Sequential Inference And Evidence (Jaynes 2003): Is it valid? Is it used?,<probability><self-study><bayesian><inference><sequential-analysis>,CC BY-SA 3.0,"<p>Exploring the work of ET Jaynes, <em>Probability Theory</em> (11th Printing 2013) has led to consideration of the technique he identifies as <strong>Sequential Inference</strong> (p. 96); where the <em>evidence</em>, in decibels, accumulates until the investigator either (1) stops with acceptance, (2) stops with rejection or (3) continues with another test.  Jaynes' technique appears to be a special case of, and perhaps a more Bayesian approach than, sequential analysis as proposed by Abraham Wald.  But I am certainly not an expert here.</p>

<p>This looks to be an incredibly powerful technique that can be used to investigate a claim of compliance/non-compliance in a cost effective and time efficient manner.  Yet when searching the internet and also checking other Bayesian texts (such as Silvia and Skilling) there is little depth on it, if it is mentioned at all.</p>

<p>So these questions are posed:</p>

<p>(1) Is it used in practice?  </p>

<p>(2) If so, in what way, if not, why not?</p>

<p>(3) Are there critical issues/pitfalls with it's use in practice?</p>

<p>(4) Is (are) there any in-depth reference(s) with case studies of application?</p>

<p><strong>Addendum</strong>  </p>

<p>Since posting this earlier, we have found an excellent collection of information <a href=""http://bayes.wustl.edu/"" rel=""nofollow"">here</a>. </p>

<p>Coming from an engineering background, by nature we tend to be data driven, and this would put Engineers solidly in the Frequentist camp.  However, time and again we have seen that these analyses should start from a position of logic, and this is what a <em>proper</em> Bayesian approach will accomplish, it is clear.  So there is no doubt in our mind that the Bayesian method <em>trumps</em> the Frequentist one.</p>

<p>However, what we are specifically interested in is the concept of <em>evidence</em> in <em>decibels</em> (it seems that <em>bels</em> or maybe the <em>Jaynes Scale</em> (0,10] analogous to the <em>Richter Scale</em> might have been a more interesting way to consider it).</p>

<p>But is this approach used for more routine issues found in practice, and are there any case studies of such?</p>
",2013-10-21 18:09:49.567
57958,22.0,2,,57952.0,,,,CC BY-SA 3.0,"<p>A standard error can be computed for almost any parameter you compute from data, not just the mean. The phrase ""the standard error"" is therefore ambiguous. I assume you are asking about the standard error of the mean.</p>

<p>Here are the key differences between the standard deviation (SD) and the standard error of the mean (SEM)</p>

<ul>
<li><p>The SD quantifies scatter â€” how much the values vary from one
another.</p></li>
<li><p>The SEM quantifies how precisely you have determined the true mean of the
population. It takes into account both the value of the SD and the
sample size.</p></li>
<li><p>Both SD and SEM are in the same units -- the units of the data.</p></li>
<li><p>The SEM, by definition, is always smaller than the SD.</p></li>
<li><p>The SEM gets smaller as your samples get larger. This makes sense,
because the mean of a large sample is likely to be closer to the true
population mean than is the mean of a small sample. With a huge
sample, you'll know the value of the mean with a lot of precision
even if the data are very scattered.</p></li>
<li><p>The SD does not change predictably as you acquire more data. The SD
you compute from a sample is the best possible estimate of the SD of
the overall population. As you collect more data, you'll assess the
SD of the population with more precision. But you can't predict
whether the SD from a larger sample will be bigger or smaller than
the SD from a small sample. (This is not strictly true. It is the
variance -- the SD squared -- that doesn't change predictably, but
the change in SD is trivial and much much smaller than the change in
the SEM.)</p></li>
<li>The SEM is hard to define conceptually. The only real ""purpose"" of an SEM is as an ""ingredient"" to compute the confidence interval of the mean.</li>
<li>The SEM is computed from the SD and sample size (n) as $$SEM ={SD \over \sqrt n}.  $$</li>
</ul>

<p>(From the <a href=""http://www.graphpad.com/guides/prism/7/statistics/index.htm?stat_semandsdnotsame.htm"" rel=""nofollow"">GraphPad statistics guide</a> that I wrote.)</p>
",2013-10-21 18:27:55.197
57959,21985.0,1,,,,Ratio of lengths of two confidence intervalls,<self-study><normal-distribution><asymptotics><t-distribution>,CC BY-SA 3.0,"<p>I have two random variables:</p>

<p>(1) With standard normal distribution. Confidence interval $I_1$, which is centered and has probability of $(1-\alpha)$</p>

<p>(2) With T-distribution. Confidence intervall intervall $I_2$, also centered and has prob. $(1-\alpha)$.</p>

<p>First I had to compute the length of both:
I got $\mathcal{L}(I_1) = 2 \cdot z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}}$ and $\mathcal{L}(I_2) = 2 \cdot t_{1-\alpha/2;n-1} \frac{S}{\sqrt{n}}$. Hope this is correct.</p>

<p>Now I have to calculate the asymptotic distribution of the ratio: $\Lambda = \frac{\mathcal{L}(I_2)^2}{\mathcal{L}(I_1)^2}$. As a hint I got $(n-1)\frac{S^2}{\sigma^2} \sim \chi_{n-1}^2$.</p>

<p>So far I got: $\Lambda = \frac{z_{1-\alpha/2}^2}{t_{1-\alpha/2;n-1}^2}\cdot\frac{S^2}{\sigma^2}$.</p>

<p>How do I express the ratio of the two distributions? I know that t-Distribution gets to a standard normal if n goes to infinity, but does that help? And do I get the $(n-1)$ from the hint somehow, or is this not really relevant?</p>
",2013-10-21 18:31:53.557
57960,5821.0,2,,57782.0,,,,CC BY-SA 3.0,"<p>The best approach to achieving what you're trying to describe is probably to directly optimize the logistic regression parameters with an AUC loss function. The textbook ""Statistical Methods in Diagnostic Medicine"" by Zhou describes this method.</p>

<p>The AUC (area under the receiver operating characteristic curve-- or ROC) is roughly interpreted as the probability that a randomly sampled ""case"" has a higher marker value than a ""control"". This is a measure of model discrimination, or its ability to correctly classify the outcome. The ROC is a curve in the unit plane which shows the sensitivity versus 1 - specificity for all possible marker values (fitted outcomes) in a regression model.</p>

<p>By using the traditional formulation of the logistic regression model, </p>

<p>$$ \mbox{logit Pr}(Y = 1 | X) = \alpha  + \beta X$$</p>

<p>with log odds ratios for model parameters, you can roughly define an AUC based loss function to obtain optimal parameters. Unlike likelihood based logistic regression, AUC regression is not regular and can converge to local maxima in the parameter space.</p>
",2013-10-21 18:41:31.170
57961,232.0,2,,57936.0,,,,CC BY-SA 3.0,"<p>There are two basic approaches to generating data with piecewise constant hazard: inversion of the cumulative hazard and the composition method.</p>

<ol>
<li><strong>Inversion of the cumulative hazard</strong> - essentially the inverse CDF method. Since $F(t) = 1-\exp(-H(t))$. If $U \sim Unif(0,1)$, then $F(X) = U$ is equivalent to $1-\exp(-H(X)) = U$, so $X=H^{-1}(-\log(1-U))$. You can also note that $-\log(1-U) \sim Exp(1)$, so you can apply the inverse cumulative hazard to an exponential random variable.
The cumulative hazard is piecewise linear for your case, and should be easy to invert.</li>
</ol>

<p><em>Edit</em> (more detail): with two change-points, the hazard is:
$$h(t) = \left\{ \begin{matrix} f_1 , &amp; 0\leq t\leq t_1\\ 
                                f_2, &amp; t_1 &lt; t \leq t_2\\
                                f_3, &amp; t &gt; t_2 \end{matrix}\right.$$
The cumulative hazard is:
$$H(t) = \left\{ \begin{matrix} f_1 t , &amp; 0\leq t\leq t_1\\ 
                                f_1 t_1 + f_2(t-t_1), &amp; t_1 &lt; t \leq t_2\\
                                f_1t_1 + f_2(t_2-t_1) + f_3(t-t_2), &amp; t &gt; t_2 \end{matrix}\right.$$
The inverse of the cumulative hazard is:
$$H^{-1}(x) =  \left\{ \begin{matrix} x/f_1 , &amp; 0\leq x\leq f_1t_1\\ 
                                t_1 + (x-f_1t_1)/f_2, &amp; f_1t_1 &lt; x \leq f_1t_1 + f_2(t_2-t_1)\\
                                t_2 + (x-f_1t_1-f_2(t_2-t_1))/f_3, &amp; x &gt; f_1t_1 + f_2(t_2-t_1)\end{matrix}\right.$$</p>

<p>Now generate an exponentially distributed random variable, and plug it in into $H^{-1}$.
<em>End edit</em></p>

<ol start=""2"">
<li>The <strong>Composition method</strong> uses the fact that if $X_1$ has hazard $h_1$, and $X_2$ has hazard $h_2$, then $X=\min(X_1,X_2)$ has hazard  $h=h_1+h_2$. You can represent your piecewise constant hazard as a sum of hazards that are constant on an interval and 0 outside. Generate a value $X_i$ for each interval (it could be $\infty$, since the resulting distributions are not necessarily proper), and take their minimum.</li>
</ol>

<p><em>Edit</em> (more detail): with the above notation, the composition hazards are
$$h_1(t) = \left\{ \begin{matrix} f_1 , &amp; 0\leq t\leq t_1\\ 
                                0, &amp; t &gt; t_1 \end{matrix}\right.$$
$$h_2(t) = \left\{ \begin{matrix} f_2 , &amp; t_1 &lt; t\leq t_2\\ 
                                0, &amp; \text{otherwise} \end{matrix}\right.$$
$$h_3(t) = \left\{ \begin{matrix} f_3 , &amp; t_2&lt; t\\ 
                                0, &amp; \text{otherwise} \end{matrix}\right.$$
You can easily calculate the CDF or the cumulative hazard for each of these hazards.</p>

<p><a href=""http://www.demogr.mpg.de/papers/technicalreports/tr-2010-003.pdf"" rel=""nofollow"">One resource with R-Code</a></p>
",2013-10-21 18:48:02.733
57962,10594.0,1,,,,What's the likelihood function in this case?,<regression><logistic><maximum-likelihood><poisson-distribution><poisson-regression>,CC BY-SA 3.0,"<p>I have a data set with a binary response variable as non-zero survival cells (y=0) or zero survival cells (y=1), and I would like to build a model with explanatory variable X. I know that the most straightforward method is to use logistic regression. However, in my case, it is more biologically sound to use a Poisson model. Thus, the number of survived cells follows a Poisson distribution $f(k,\lambda)$, and the response (probability of having zero survival cell) is the Poisson distribution at $k=0$.</p>

<p>Therefore, my model can be written as $y=f(0,\lambda)=e^{-\lambda}$, where $\lambda=g(\beta,x)$. $\lambda$ is a function of $x$ with parameter $\beta$. I am trying to estimate $\beta$ using MLE. </p>

<p>Now I am bit confused about which likelihood function I should use. Shall I assume that $y$ follows a binomial distribution: $y\sim B(1,e^{-\lambda})$, so that I could conduct MLE on the likelihood function from this binomial distribution? What about the Poisson distribution? It is not used in the estimation? Suppose $\beta^{*}$ is the estimated parameter, which gives $\lambda^{*}$. This means that the probability of having zero survival cells follows a $B(1,e^{-\lambda^{*}})$ distribution and the survival cells follows a Poisson $f(k,\lambda^{*})$ distribution?</p>
",2013-10-21 18:55:31.027
57963,20222.0,2,,57951.0,,,,CC BY-SA 3.0,"<p>If you believe there is little if any interaction between the independent variables a simple approach is the linear-additive model.</p>

<p>Y= B0 +B1X1 +B2X2 + . . . B60X60 +e</p>

<p>Then by examining the coefficients you can identify the top 8 factors.</p>

<p>This <a href=""http://www.apec.umn.edu/grad/jdiaz/IntroductiontoRegression.pdf"" rel=""nofollow"">paper</a> may be of some use. If the above model is too simple, there are many other approaches such as neural networks, symbolic regression, splines and others.</p>
",2013-10-21 19:12:41.107
57964,17573.0,2,,57782.0,,,,CC BY-SA 3.0,"<p>You don't seem to want logistic regression at all.  What you say is ""I would like to maximize the difference between true positives and false positives.""  That is a fine objective function, but it is not logistic regression.  Let's see what it is.  </p>

<p>First, some notation.  The dependent variable is going to be $Y_i$:<br>
\begin{align}
Y_i &amp;= \left\{ \begin{array}{l}
               1 \qquad \textrm{Purchase $i$ was profitable}\\
               0 \qquad \textrm{Purchase $i$ was un-profitable}
               \end{array}
       \right.
\end{align}</p>

<p>The independent variables (the stuff you use to try to predict whether you should buy) are going to be $X_i$ (a vector).  The parameter you are trying to estimate is going to be $\beta$ (a vector).  You will predict buy when $X_i\beta&gt;0$.  For observation $i$, you predict buy when $X_i\beta&gt;0$ or when the indicator function $\mathbf{1}_{X_i\beta&gt;0}=1$.</p>

<p>A true positive happens on observation $i$ when both $Y_i=1$ and $\mathbf{1}_{X_i\beta&gt;0}=1$.  A false positive on observation $i$ happens when $Y_i=0$ and $\mathbf{1}_{X_i\beta&gt;0}=1$.  You wish to find the $\beta$ which maximizes true positives minus false positives, or:
\begin{equation}
max_\beta \; \sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0} - \sum_{i=1}^N (1-Y_i)\cdot\mathbf{1}_{X_i\beta&gt;0}
\end{equation}</p>

<p>This is not an especially familiar objective function for estimating a discrete response model, but bear with me while I do a little algebra on the objective function:
\begin{align}
 &amp;\sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0} - \sum_{i=1}^N (1-Y_i)\cdot\mathbf{1}_{X_i\beta&gt;0}\\
= &amp;\sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0} - \sum_{i=1}^N \mathbf{1}_{X_i\beta&gt;0}
+ \sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0}\\
= &amp;\sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0} - \sum_{i=1}^N \mathbf{1}_{X_i\beta&gt;0}
+ \sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0} \\
 &amp; \qquad + \sum_{i=1}^N 1 - \sum_{i=1}^N 1 + \sum_{i=1}^N Y_i - \sum_{i=1}^N Y_i\\
= &amp;\sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0} + \sum_{i=1}^N (1-Y_i)(1-\mathbf{1}_{X_i\beta&gt;0}) - \sum_{i=1}^N 1 + \sum_{i=1}^N Y_i \\
\end{align}</p>

<p>OK, now notice that the last two terms in that sum are not functions of $\beta$, so we can ignore them in the maximization.  Finally, we have just shown that the problem you want to solve, ""maximize the difference between true positives and false positives"" is the same as this problem:
\begin{equation}
max_\beta \; \sum_{i=1}^N Y_i\cdot\mathbf{1}_{X_i\beta&gt;0} + \sum_{i=1}^N (1-Y_i)(1-\mathbf{1}_{X_i\beta&gt;0})
\end{equation}</p>

<p>Now, that estimator has a name!  It is named the maximum score estimator.  It is a very intuitive way to estimate the parameter of a discrete response model.  The parameter is chosen so as to maximize the number of correct predictions.  The first term is the number of true positives, and the second term is the number of true negatives.</p>

<p>This is a pretty good way to estimate a (binary) discrete response model.  The estimator is consistent, for example. (Manski, 1985, J of Econometrics)  There are some oddities to this estimator, though.  First, it is not unique in small samples.  Once you have found one $\beta$ which solves the maximization, then any other $\beta$ which makes the exact same predictions in your dataset will solve the maximization---so, infinitely many $\beta$s close to the one you found.  Also, the estimator is not asymptotically normal, and it converges slower than typical maximum likelihood estimators---cube root $N$ instead of root $N$ convergence. (Kim and Pollard, 1990, Ann of Stat)  Finally, you can't use bootstrapping to do inference on it.  (Abrevaya &amp; Huang, 2005, Econometrica)  There are some papers using this estimator though---there is a fun one about predicting results in the NCAA basketball tournament by Caudill, International Journal of Forecasting, April 2003, v. 19, iss. 2, pp. 313-17.</p>

<p>An estimator that overcomes most of these problems is Horowitz's smoothed maximum score estimator (Horowitz, 1992, Econometrica and Horowitz, 2002, J of Econometrics).  It gives a root-$N$ consistent, asymptotically normal, unique estimator which is amenable to bootstrapping.  Horowitz provides example code to implement his estimator on <a href=""http://faculty.wcas.northwestern.edu/~jlh951/software.html"">his webpage.</a></p>
",2013-10-21 19:18:00.883
57965,22914.0,1,58047.0,,,Drawing with repetition,<probability><combinatorics>,CC BY-SA 3.0,"<p>I thought I was fairly good at combinatorics, but this puzzle is giving me some trouble! </p>

<p>I have a bag with $N$ balls. I pick one at random, mark it and put it back inside the bag. I repeat this operation $D$ times. What is the probability that I will end up with exactly $C$ clean balls?</p>
",2013-10-21 19:35:50.170
57966,22885.0,2,,57894.0,,,,CC BY-SA 3.0,"<p>@ttnphns Thank you for your answer. You're right, in common case, without mean-centering PCA would lose a lot of sense.</p>

<p>But I think, if I use it to regularize regression, it could work.</p>

<p>Here I wrote a bit of algebra for myself. I hope it's correct, but I would appreciate some comments, cause I'm not completely sure.</p>

<ol>
<li><p>Initial problem (doesn't work because A'A is nearly collinear and numerically uninvertible)
$$
y = Ax
$$</p></li>
<li><p>Decompose covariance matrix of data (standardized data). To get correct (from the PCA perspective) eigen-everything.
$$
cov(A) = VDV'
$$</p></li>
<li><p>Transform raw-data (without mean centering) into an orthogonal basis
$$
AV
$$
(just a linear transformation).</p></li>
<li>Coefficients $x^*$ for transformed problem are founded because matrix $AV$ is well-conditioned.
$$
y = AVx^*
$$</li>
<li>As (1) and (4) are approximately equal
$$
Ax = AVx^*
$$</li>
<li>so...
$$
x = (A'A)^{-1}A'AVx^*
$$</li>
<li>A'A seems to have a full rank, so theoretically is invertible analiticaly
$$
x = Vx^*
$$</li>
</ol>
",2013-10-21 20:16:20.140
57967,20604.0,1,,,,When is it appropriate to model count data as continuous?,<r><generalized-linear-model><maximum-likelihood><poisson-distribution><count-data>,CC BY-SA 4.0,"<p>I have time series of several variables of 60 or so rows of count data. I want to do a regression model <code>y ~ x</code>. I've chosen to use a Quasipoisson &amp; Negative Binomial GLMs as there's overdispersion etc. </p>

<pre><code>x
Min.   : 24000  
1st Qu.: 72000  
Median :117095  
Mean   :197607  
3rd Qu.:291388  
Max.   :607492  

y
Min.   : 136345
1st Qu.: 405239
Median : 468296
Mean   : 515937
3rd Qu.: 633089
Max.   :1218937
</code></pre>

<p>The data itself are very high and so it may be best to model these as count data (this is what I'm trying to investigate - at which point I can model count data as continuous). It seems to be very common practice, what I want to know is the motivation for this? </p>

<p>Are there any texts that actually show the problem of modelling high count data with Poisson distribution? Perhaps something that shows the factorial in the distribution makes things difficult. </p>
",2013-10-21 20:30:43.837
57968,22916.0,1,,,,Joint pdf of a continuous and a discrete rv,<density-function><joint-distribution><censoring>,CC BY-SA 3.0,"<p>Let us consider a manufacturing system. It involves 2 independent components. If one of these components fails then the entire system fails. Let $Y_j$ be distributed $\exp(Q_j)$ where $j=1, 2$.</p>

<p>If component 1 fails first, then $Y_1$ is observed but $Y_2$ is not ($Y_2$ is censored). If component 2 fails first, then $Y_2$ is observed but Y_1 is not ($Y_1$ is censored). Therefore, if the system fails, we can only observe $u = \min(Y_1, Y_2)$ and the binary random variable $V$, which is $1$ if $Y_1 &lt; Y_2$ and $0$ otherwise.</p>

<p>How can I derive the joint pdf of a continuous variable $u = \min(Y_1, Y_2)$ and a discrete variable $V = 1$ if $Y_1 &lt; Y_2$ and $0$ otherwise?</p>
",2013-10-21 20:53:00.177
58160,22200.0,1,58174.0,,,Interpretation of the final cluster centers (cluster analysis),<clustering>,CC BY-SA 3.0,"<p>I have a question concerning the interpretation of the final cluster centers. I performed a cluster analysis based on a pca (the variables are based on a five point Likert-scale). I got the following result for one factor:</p>

<pre><code>        cluster_1        cluster_2       cluster_3      cluster_4       cluster_5
          0,31            0,39               -0,82          0,63            0,35
</code></pre>

<p>Is this factor also interesting for the description of cluster 1, 2 and 5? Or should I only mention its influence for the clusters 3 and 4?</p>

<p>Thanks a lot!</p>
",2013-10-24 12:10:37.597
57969,8869.0,1,,,,A distributional test based on entropy and self-information,<entropy>,CC BY-SA 3.0,"<p>Say that I have a real-valued discrete distribution $p(x)$ and $N$ samples, $x_1, \ldots, x_N$, and I want to test whether the samples came from the distribution without making any further assumptions whatsoever. Note that there are very few samples; in the application that motivated me to make this post, we have $N = 5$. Thus Kolmogorov-Smirnov and Chi-squared tests are not expected to have much power.</p>

<p>I had a simple idea for doing this under the assumption that one can efficiently sample from $p(x)$. Being a bit statistically naive, I'm having difficulty figuring out whether it exists in the literature or not, and hoping someone can point me to the right resource.</p>

<p>The idea in a nutshell is to compare the self-information of the sample, $\hat{I}_N$, to the distribution of the self-information $I_N$ of $N$ random samples from $p(x)$. Formally, recall that the self-information of a random variable $X$ having distribution $p$ is given by </p>

<p>$$ I = - \log p(X), $$</p>

<p>and the self-information of $N$ iid random variables $X_1,\ldots X_N$ each having distribution $p(x)$ is given by</p>

<p>$$ I_N = -\sum_{i=1}^N \log p(X_k). $$</p>

<p>The self-information of the sample data we have, $x_1,\ldots,x_N$ is denoted</p>

<p>$$ \hat{I}_N = -\sum_{k=1}^N \log p(x_k). $$</p>

<p>As a test statistic one might consider $\hat{C} = C(\hat{I}_N)$, where $C(s) = \mathbb{P}(I_N &lt; s)$ is the cumulative distribution function. If $\hat{C}$ takes a value very close to 0 or 1, then it has an extreme value compared to the distribution of $I_N$, and is unlikely to come from $p(x)$. For concreteness, we could use the standard thresholds for extreme value tests, such as 0.95 and 0.05 for the high and low ends respectively.</p>

<p>As an example application of this test, say that $p(x)$ is some strange multimodal distribution with multiple humps, and the samples $x_1,\ldots,x_N$ lie in the valleys between the humps. It is not clear that there is any test suitable from the literature for such a problem, but intuitively we can see that the samples are unlikely to have come from $p(x)$ because the values of $p(x_i)$ are so small, or equivalently, the self-information $-\log p(x_i)$ is much larger than typical. In terms of the above discussion, $\hat{C}$ will be very close to 1, and the hypothesis will be rejected. </p>

<p>The main problem I see with this test is that the distribution of $I_N$ may be difficult to compute. That may be, but in many cases I would imagine that a few million (or billion) Monte Carlo samples would suffice to get a good approximation of the distribution. Analytical/asymptotic approximations could be used to speed things up, get theoretical results, etc. For example, the first moment of $I_N$ is $N$ times the Shannon entropy, and higher moments could be computed without great difficulty.</p>
",2013-10-21 21:03:05.447
57970,21119.0,1,,,,normalising constant on exponential of exponential,<normal-distribution><normalization><exponential-family>,CC BY-SA 3.0,"<p>I have a distribution of the form $\exp(-\exp(-x^2))$. Is this a known family of distributions. Otherwise how would you find/approximate the normalising constant. The domain is $x \in (-\infty,\infty) $</p>

<p>Note: I want to do this without sampling. </p>
",2013-10-21 21:23:25.110
57971,3580.0,1,,,,"Approximately sampling $(X, Y)$ when sampling $X$ is easy",<simulation><monte-carlo><gibbs>,CC BY-SA 3.0,"<p>Suppose I am interested in sampling many pairs $(\mathbf X, Y)$ from some distribution $f(\mathbf x, y)$ where $\mathbf x \in \mathbb R^p$, $p$ large ; I am interested in both exact and approximate simulations. $f(\mathbf x)$ is easy to sample from, but $f(y \mid \mathbf x)$ is not. </p>

<p>For motivation, I could do Gibbs sampling if the distributions $f(\mathbf x\mid y)$  and $f(y \mid \mathbf x)$ were both easy to simulate from by initializing $(\mathbf X_0, Y_0)$ and drawing $\mathbf X_t \sim f(x \mid Y_{t-1})$ and $Y_t \sim f(y \mid \mathbf X_t)$. If $f(\mathbf x \mid y)$ is easy to sample from, then I am in really good shape because, worst case scenario, I can replace $f(y \mid \mathbf x)$ with any update that leaves this distribution invariant. </p>

<p>In my situation, $f(\mathbf x \mid y)$ is difficult to sample from, and substantial work would have to go into constructing a suitable transition kernel given the dimension of $\mathbf x$. However, I can draw from the marginal $f(\mathbf x)$ exactly! It seems like this should buy me something like it does with the Gibbs sampler, however if I do something like the following:</p>

<ol>
<li>Draw $\mathbf X_t \sim f(\mathbf x)$;</li>
<li>Draw $Y_t \sim K(y \mid \mathbf X_t, Y_{t-1})$ where $K(\cdot \mid \mathbf x, y)$ leaves $f(y \mid \mathbf x)$ invariant,</li>
</ol>

<p>I believe I will not get the correct stationary distribution. </p>

<p>If it helps, I might be willing to evaluate the density $f(y \mid \mathbf x)$. I know $f(y \mid \mathbf x)$ up-to a normalizing constant, but I can afford to do one numerical integration to get the constant (I would really prefer not to, though). However, $f(y \mid \mathbf x)$ is expensive to compute.</p>

<p>One thought I've had is to generate $f(\mathbf x)$ and then do a small number of MH random walk steps, but I would very much prefer getting an answer that will at least converge to the correct thing as $t \to \infty$ (this one will always have some error if I do a small number of steps for each $Y_t$). </p>

<p><strong>EDIT:</strong> Maybe this all seems obvious - just use generic sampling techniques for each $f(y \mid \mathbf x)$ and, in general, I shouldn't be able to do better. I guess what I have in mind is that I should somehow be able to get the MCMC to work across $t$, and <em>not</em> have to do my approximate sampling within $t$. That is, I want my approximation to get better and better as $t \to \infty$; I don't want my approximation to have the same amount of error within $t$. </p>

<p>In general, though, $f(y \mid \mathbf x)$ might be very different for different values of $\mathbf x$ so I shouldn't hope for a solution that will always work. I just want something that has a good chance of working and addresses the above issues. It is strange to me that $f(\mathbf x)$ would be less useful than $f(\mathbf x \mid y)$. </p>
",2013-10-21 21:23:55.850
57972,22587.0,1,58018.0,,,Hoops in a Field,<sampling><proportion><continuous-data>,CC BY-SA 3.0,"<p>I have a simple question, but I haven't been able to find the answer I'm looking for anywhere. I'm sure there is some simple theorem or distribution that tackles this problem, but it evades me. Perhaps I haven't googled the right terms or looked in the right textbooks!</p>

<p>My problem is as follows. Suppose a proportion ($p$) of a field is covered in water. The water is randomly dispersed in the field. The field has an area $A$. Now we fly over the field in a helicopter and randomly drop a hoop of size $a$ (where $aN = A$) into the field. The hoop lands at a random spot within the field borders. What is the probability that there will be water inside the hoop?</p>

<p>My intuition about the problem is that for $a &gt; A(1-p)$, the probability is 1 (because the area excluded by the hoop is smaller than the total area of the water), and as $a \rightarrow 0$, the probability approaches $p$. But what about for values of $a$ in the middle? Can the question be answered?</p>

<p>I have a sneaking suspicion that the answer is embarrassingly simple.</p>

<p>Thanks for your help! </p>
",2013-10-21 21:49:46.393
57973,14715.0,1,,,,"In R, how do I fit a student-t distribution to a set of empirical data?",<r><data-visualization><density-function><kernel-smoothing><ggplot2>,CC BY-SA 3.0,"<p>In R, I can fit a Probability Density Function to some empirical data using the following code:</p>

<pre><code>energy &lt;- rnorm(30) * 20
dens &lt;- density(energy)
sum(dens$y)*diff(dens$x[1:2])
hist(energy,probability=TRUE)
lines(density(energy),col=""red"")
</code></pre>

<p>This produces the following graph of the Probability Density Function (PDF):</p>

<p><img src=""https://i.stack.imgur.com/roJ99.png"" alt=""enter image description here""></p>

<p>Howevever, I would like to fit a student-t distribution to this data instead. I'm wondering if its possible to do this and if its possible to plot the result like in the diagram above?</p>
",2013-10-21 22:48:43.223
57974,21638.0,2,,57949.0,,,,CC BY-SA 3.0,"<p>Let</p>

<p>$$
\hat{\epsilon} = y - Z\hat{\beta}
$$</p>

<p>the usual estimator of $\beta$ is</p>

<p>$$
\hat{\beta} = (Z^{T}Z)^{-1}Z^{T}y
$$</p>

<p>which gives</p>

<p>$$
\begin{array}{rcl}\hat{\epsilon} &amp;=&amp; y - Z(Z^{T}Z)^{-1}Z^{T}y\\&amp;=&amp;\left[I - Z(Z^{T}Z)^{-1}Z^{T}\right]y\end{array}
$$</p>

<p>and so</p>

<p>$$
Cov[\hat{\epsilon}] = Cov\left[\left[I - Z(Z^{T}Z)^{-1}Z^{T}\right]y\right]
$$</p>

<p>then simply use the property</p>

<p>$$
Cov[Ax] = ACov[x]A^{T}
$$</p>
",2013-10-21 22:52:55.497
57975,10135.0,2,,57973.0,,,,CC BY-SA 3.0,"<p>Have a look at function <code>fit.st</code> in package <a href=""http://cran.r-project.org/web/packages/QRM/QRM.pdf"" rel=""nofollow"">QRM</a>.    </p>

<pre><code>library(QRM)
fit.st(energy )
</code></pre>
",2013-10-21 23:20:11.733
57976,21119.0,1,,,,Normalising Constant for exponentiated function,<normal-distribution><normalization><bounds><exponential-family><gumbel-distribution>,CC BY-SA 3.0,"<p>What would the normalising  constant be of the following, or atleast an approximation? I would like to avoid sampling.</p>

<p>$$f(\theta)=\exp(-k_1e^{-k_2\theta^2}-\theta^2)\qquad\theta\in(-\infty,\infty), \quad k_1,k_2&gt;0$$</p>

<p>In the event that its not possible, what would be a valid <strong>polynomial</strong> lower bound on $\log(f(\theta))$, that would take into account $k_1$</p>

<p>For anyone, who is interested, I end up with this expresssion because I place a Gaussian prior over (a squared exponential) kernel parameters, and trying to use variational bayes (hence, a lower bound).</p>
",2013-10-21 23:52:46.893
57978,5237.0,2,,57977.0,,,,CC BY-SA 3.0,"<p>You need to look into the <em>Multivariate</em>-ANOVA (called MANOVA).  I'm not sure about good internet-based resources; the Wikipedia page is <a href=""http://en.wikipedia.org/wiki/Multivariate_analysis_of_variance"" rel=""nofollow"">here</a>, and the entry in StatSoft's online textbook is <a href=""http://www.statsoft.com/Textbook/ANOVA-MANOVA#multivariate"" rel=""nofollow"">here</a>, but neither seem great.  For a 'real' textbook that is more or less math-free, <a href=""http://rads.stackoverflow.com/amzn/click/0205849571"" rel=""nofollow"">Tabachnick &amp; Fidell</a> is the most popular.  If you use SPSS, SAS, or Stata, the excellent UCLA statistics help site has some quick tutorials; they can be found <a href=""http://www.ats.ucla.edu/stat/dae/"" rel=""nofollow"">here</a>.  </p>
",2013-10-22 01:36:51.040
57979,20179.0,1,,,,Is p-value involved in Neyman-Pearson Testing procedure?,<hypothesis-testing><p-value>,CC BY-SA 3.0,"<p>It seems to me that, after having a predetermined level of significance, the Neyman-Pearson approach only requires to see if the observed x lies in the rejection region or not.
In this case, how to relate it to p-value? 
Or is it an alternative way to judge if x is rejected or not by using p-values?
After reading some references about NP and Fisher's treatment, I am confused.</p>

<p>Thank you for any suggestion.</p>
",2013-10-22 01:44:10.797
57980,22328.0,1,,,,Growing Self-Organizing Map for mixed-type data,<machine-learning><self-organizing-maps>,CC BY-SA 3.0,"<p>I am trying to write code to build a growing SOM for mixed-type data. I came across a paper Growing Self-Organizing Map with cross insert for mixed-type data (<a href=""http://www.sciencedirect.com/science/article/pii/S1568494612001731"" rel=""nofollow"">http://www.sciencedirect.com/science/article/pii/S1568494612001731</a>). It was very interesting and handled both categorical and numeric data in a unified way. However, my dataset has variables/attributes that can have multiple values (for example: attribute ""interests"" can have more than one value - movies, sports, and so on...). I got stuck at handling such attributes. Any inputs how to handle attributes with sets of values in a mixed-type dataset? References to material that talk about this problem would be greatly appreciated.</p>
",2013-10-22 02:20:10.527
57981,20981.0,2,,57942.0,,,,CC BY-SA 4.0,"<p>You can use a Wald approximation for confidence intervals for the first set of experiments, and probably the second.  For the third, you can't calculate confidence intervals at all, but you can test whether any of the other experiments were significantly different from zero.</p>
<p><a href=""https://en.wikipedia.org/wiki/Binomial_distribution#Confidence_intervals"" rel=""nofollow noreferrer"" title=""Wikipedia"">Wikipedia</a> has a formula for the Wald and other approximations and this paper: <a href=""https://projecteuclid.org/journals/statistical-science/volume-16/issue-2/Interval-Estimation-for-a-Binomial-Proportion/10.1214/ss/1009213286.full"" rel=""nofollow noreferrer"">Interval Estimation for a Binomial Proportion</a>, describes the various approximations in more detail.</p>
<p>You should probably test for differences between the samples in each experiment group (which is what I assume Aniko is implying.  If there are big differences between the 15 experiments in the first group for instance, it would call into question these simple confidence intervals, and you might want to consider an <a href=""https://en.wikipedia.org/wiki/Mixed_model"" rel=""nofollow noreferrer"">effects model</a> of some sort.</p>
",2013-10-22 02:24:09.137
57982,4656.0,2,,57968.0,,,,CC BY-SA 3.0,"<p>In simplistic terms, there is no such thing as a joint <strong>density</strong> of a continuous random variable and a discrete random variable because all the probability mass lies
on two straight lines ($v=0$ and $v=1$) and on these lines, the <strong>joint</strong> 
density, being the probability mass per unit <em>area,</em> is infinite. On the other
hand, the <strong>line</strong> density of the mass on the two lines
is a (univariate) exponential density (measured in probability mass per unit
<em>length</em>).  More specifically, the line density on the line $v=0$ is the density
of $U_2$ and the line density on the line $v=1$ is the density of $U_1$.</p>
",2013-10-22 03:17:53.623
57983,22923.0,2,,51577.0,,,,CC BY-SA 3.0,"<p>Standard cross validation (when you randomly split data into k blocks) is a wrong thing to do for time-series because in time-series you often have serial dependence, data are not iid. </p>

<p>For example, draw a random walk path, then cut-off one point in the middle, fit a regression on the rest and predict the value for the cut-off point - your prediction error will be small because cut-off point's value if very similar to neighbor values, it's not independent from them so cross-validation in this case will give you very optimistic estimate of prediction error.</p>

<p>Better thing to do to test a time-series regression is to run a moving 1-step-ahead prediction with fitting a regression on every step. </p>

<p>But even then, just calculating mean squared error is not good enough if your series aren't stationary - it's not very indicative. You may need to compare 1-step-ahead prediction error of your model and some simple reference model like ""next step predicted value is the same as previous step realized value"". This will indicate how good is you model out-of-sample compared to a reference.</p>
",2013-10-22 04:14:44.413
57984,18268.0,1,,,,Approach to a Poisson question,<probability><self-study><poisson-distribution>,CC BY-SA 3.0,"<p>This is a question from an old test, taken completely out of context:</p>

<p>""The customers arrive at a shop according to a Poisson distribution with mean $\lambda$ per hour. 
Each customer takes $1/k$ hours of service. What is the expected value and the variance of the service time for customers arriving within an hour? What is the probability that the service time exceeds $t$ hours?</p>

<p>I am not entirely certain how to approach this question. This is supposed to be a simple Poisson-distribution question, with no Poisson processes or queuing.</p>

<p>Are there any ways to look at this question in a simpler way? What is the general approach here? </p>

<p>Apologies if I am completely misunderstanding this.</p>
",2013-10-22 05:17:37.467
57985,6204.0,2,,57968.0,,,,CC BY-SA 3.0,"<p>What you have here is a mixture model, specifically a mixture of exponentials. If I understand your problem setup correctly, I believe what you're looking for looks something like this:</p>

<p>$$
u \sim f(x) =
\begin{cases}
f_{Y_1}(x), &amp; V=1 \\
f_{Y_2}(x), &amp; V=0
\end{cases}
$$</p>

<p>or alternatively</p>

<p>$$u \sim f(x) = \theta f_{Y_1}(x) + (1-\theta)f_{Y_2}(x)$$</p>

<p>Where $\theta$ is the expected proportion of samples generated by $Y_1$ (or using your formulation, $\theta = E[V]$).</p>

<p>You can confirm this experimentally. Here's a mixture model with arbitrarily selected parameters <code>r1</code>, <code>r2</code> and <code>theta</code>:</p>

<pre><code>n=1e5
theta=.2
v=rbinom(n,1,theta)
r1=5; r2=1
sample=v*rexp(n,r1) + (1-v)*rexp(n,r2)

f=function(x){theta*dexp(x,r1) + (1-theta)*dexp(x,r2)}

plot(density(sample), xlim=c(0,6))
xv=seq(from=0,to=6, length.out=1e4)
lines(xv,f(xv), col='red')
</code></pre>

<p><img src=""https://i.stack.imgur.com/1Un92.png"" alt=""enter image description here""></p>
",2013-10-22 05:27:22.733
57986,5045.0,1,,,,Constructing the OLS standard error by hand to avoid regression,<regression><variance><standard-error><treatment-effect>,CC BY-SA 3.0,"<p>I am having trouble deriving the standard error of a simple regression estimator by hand. Stata code and output for a toy example using the cars dataset is below.</p>

<p>The basic idea is that I have a binary treatment that interacts with a binary covariate. All observations are independent. I'm probably overlooking something simple.</p>

<p>My outcome is a weighted average of the effects in the two groups:
\begin{equation}
TE= \frac{N_1 \cdot \delta_1 + N_2 \cdot \delta_2}{N_1+N_2}=\frac{N_1 \cdot (\bar X_{T_1}-\bar X_{C_1}) + N_2 \cdot (\bar X_{T_2}-\bar X_{C_2})}{N_1+N_2},
\end{equation}
where $N_i=T_i+C_i$ is the number of people in group $i$ and $\delta_i$ is the effect of treatment in group $i$. I will estimate the $\delta$s with the group-specific difference in sample means. My manual calculation agrees with the regression output below.</p>

<p>Using the standard sum of weighed random variables, I get
\begin{equation}
Var[TE]= \left(\frac{N_1}{N_1+N_2}\right)^2 \cdot \left[ \frac{\sigma^2_{T_1}}{N_{T_1}}+\frac{\sigma^2_{C_1}}{N_{C_1}}\right]
+\left(\frac{N_2}{N_1+N_2}\right)^2 \cdot\left[ \frac{\sigma^2_{T_2}}{N_{T_2}}+\frac{\sigma^2_{C_2}}{N_{C_2}}\right],
\end{equation}</p>

<p>This is the quantity that I am having trouble scaling to get the standard error. My intuition is that the delta method used by Stata is using the covariance terms, whereas I am dropping them by assumption. <strong>Is there a way to get the OLS standard error without using OLS?</strong> I am trying to avoid running a regression.</p>

<p>Here's my code and output:</p>

<pre><code>. /* Make Fake Data */
. sysuse auto, clear
(1978 Automobile Data)

. rename foreign treat

. label define treat 0 ""C"" 1 ""T""

. lab val treat treat

. gen group=cond(weight&gt;3000,1,0)

. sum group, meanonly

. local mean = r(mean)

. /* Get Summary Stats */
. table treat group, c(mean price sd price freq) format(%9.3fc) // use 3 SDs to match the margins output

--------------------------------
          |        group        
 Car type |         0          1
----------+---------------------
        C | 4,183.800  6,838.081
          |   743.072  3,359.359
          |    15.000     37.000
          | 
        T | 5,773.900  12492.500
          | 1,803.450    703.571
          |    20.000      2.000
--------------------------------

. /* Regression Treatment Effects */
. reg price ib(0).treat##ib(0).group

      Source |       SS       df       MS              Number of obs =      74
-------------+------------------------------           F(  3,    70) =    7.78
       Model |   158773405     3  52924468.2           Prob &gt; F      =  0.0001
    Residual |   476291991    70  6804171.31           R-squared     =  0.2500
-------------+------------------------------           Adj R-squared =  0.2179
       Total |   635065396    73  8699525.97           Root MSE      =  2608.5

------------------------------------------------------------------------------
       price |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       treat |
          T  |     1590.1   890.9658     1.78   0.079    -186.8752    3367.075
     1.group |   2654.281   798.4409     3.32   0.001     1061.841    4246.721
             |
 treat#group |
        T#1  |   4064.319   2092.798     1.94   0.056    -109.6345    8238.272
             |
       _cons |     4183.8   673.5068     6.21   0.000     2840.533    5527.067
------------------------------------------------------------------------------

. lincom 1.treat + 1.treat#1.group*`mean'

 ( 1)  1.treat + .527027*1.treat#1.group = 0

------------------------------------------------------------------------------
       price |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         (1) |   3732.106   1083.335     3.45   0.001     1571.463    5892.748
------------------------------------------------------------------------------

. margins, dydx(treat)

Average marginal effects                          Number of obs   =         74
Model VCE    : OLS

Expression   : Linear prediction, predict()
dy/dx w.r.t. : 1.treat

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       treat |
          T  |   3732.106   1083.335     3.45   0.001     1571.463    5892.748
------------------------------------------------------------------------------
Note: dy/dx for factor levels is the discrete change from the base level.

. /* By Hand */
. di ""E[TE] =  ""((15+20)*(5773.9 - 4183.8)  + (37+2)*(12492.5 - 6838.081))/(15 + 37 + 20 + 2)
E[TE] =  3732.106

. di ""SE[TE] = ""sqrt( (15+20)^2*((743.072^2)/15 + (1803.45^2)/20 )/(15+37+20+2)^2 + (37+2)^2*((3359.359^2)/37+(703.571^2)/2)/(15+37+20+2)^2)/sqrt(4) 
SE[TE] = 222.53007
</code></pre>

<p>Here's just the code in case someone wants to cut n' paste:</p>

<pre><code>/* Make Fake Data */
sysuse auto, clear
rename foreign treat
label define treat 0 ""C"" 1 ""T""
lab val treat treat

gen group=cond(weight&gt;3000,1,0)
sum group, meanonly
local mean = r(mean)

/* Get Summary Stats */
table treat group, c(mean price sd price freq) format(%9.3fc) // use 3 SDs to match the margins output

/* Regression Treatment Effects */
reg price ib(0).treat##ib(0).group
lincom 1.treat + 1.treat#1.group*`mean'
margins, dydx(treat)

/* By Hand */
di ""E[TE] =  ""((15+20)*(5773.9 - 4183.8)  + (37+2)*(12492.5 - 6838.081))/(15 + 37 + 20 + 2)
di ""SE[TE] = ""sqrt( (15+20)^2*((743.072^2)/15 + (1803.45^2)/20 )/(15+37+20+2)^2 + (37+2)^2*((3359.359^2)/37+(703.571^2)/2)/(15+37+20+2)^2)/sqrt(4) 
</code></pre>
",2013-10-22 05:30:39.013
57987,1717.0,1,,,,Forecasting in ARCH(1) models,<time-series><heteroscedasticity><garch><arch>,CC BY-SA 3.0,"<p>I read this definition of an ARCH(1) model:</p>

<p>$$r_{Å§}=\sigma_{t|t-1}\epsilon_{t}$$
$$\sigma^{2}_{t|t-1} = \omega + \alpha r_{t-1}^{2}$$</p>

<p>However, when it comes to forecasting the h-step-ahead variance, I don't understand why is defined in this way. This is the h-step-ahead conditional variance:</p>

<p>$$\sigma^{2}_{t+h|t}=E(r^{2}_{Å§+h}|r_{t},r_{t-1},...)$$</p>

<p>How can I see that the equation above is correct?</p>

<p>Another question. I'd like to know what is going on in the first steps of the derivation of the recursive conditional variance:</p>

<p>$$\sigma^{2}_{t+h|t} = E(r^{2}_{t+h}|r_{t},r_{t-1},...) = E[E(\sigma^{2}_{t+h|t+h-1}\epsilon^{2}_{t+h}|r_{t+h-1},r_{t+h-2},...)|r_{t},r_{t-1},...] = \omega + \alpha \sigma^{2}_{t+h-1|t}$$</p>

<p>I skipped the final steps because they are easier to follow. I guess this is an instance of $E[E(X|Y)]=E(X)$ but I don't know why those conditional variables are chosen in this case.</p>

<p><strong>UPDATE</strong></p>

<p>A couple of additional questions.</p>

<ol>
<li><p>Based on the expression for $E(r^{2}_{t+h}|r_{t}, r_{t-1},...)$, does it mean that, for instance, $E(r^{2}_{t+h+1}|r_{t},r_{t-1}) = \sigma^{2}_{t+h+1|t}$  but since $\sigma^{2}_{t+h+1|t} = \omega + \alpha r_{t}$, then any other expectation for additional steps $t+ h + 2$, $t+h+3$, etc are going to produce the same result? </p></li>
<li><p>When we take the expectation $E(r^{2}_{t+h}|r_{t}, r_{t-1},...)$, why don't we modify the conditional subindex from $t$ to $t+h-1$ in $\sigma^{2}_{t+h|t}$ and we do change it when we write:</p></li>
</ol>

<p>$$E(r^{2}_{t+h}|r_{t},r_{t-1},...) = E[E(\sigma^{2}_{t+h|t+h-1}\epsilon^{2}_{t+h}|r_{t+h-1},r_{t+h-2},...)|r_{t},r_{t-1},...]$$</p>
",2013-10-22 06:12:42.927
57988,21119.0,1,,,,Quadratic lower bound on Gaussian,<normal-distribution><bounds>,CC BY-SA 3.0,"<p>Suppose I have a multivariate Gaussian such that $p(y)=\mathcal{N}(\mathbf{0},\Sigma)$. What would be a quadratic lower bound, $f(y)$ on on $p(y)$.</p>

<p>i.e. for what values of k and $\Omega$ will $f(y)=-y^T\Omega y+k\le p(y)$ s.t. $p(0)=f(0)=\frac{1}{|2\pi\Sigma|^{1/2}}$</p>

<p>As pointed out below $k=\frac{1}{|2\pi\Sigma|^{1/2}}$</p>
",2013-10-22 06:28:52.470
57989,22423.0,2,,57972.0,,,,CC BY-SA 3.0,"<p>A simple answer to your question would be (but not as beautiful as thought):</p>

<p>If it is the case of randomly scattered infinitely small water droplets in a continuous 2-D field, then probability of water in the hoop would be $p_w \approx 1$ for all sizes of hoop. </p>

<p>Why? We can imagine the hoop consisting of N number of equally sized 'pockets'. Probability of water found in hoop = $P(\text{water found any of N pockets}) = 1-(P(\text{water not found in a single pocket}))^N$</p>

<p>As long as size of pocket $\ne 0$, $P(\text{water not found in a single pocket}) =1-p \ne 1$ and thus $(P(\text{water not found in a single pocket}))^N \approx 0$ as $N \to \infty$.</p>

<p>**</p>

<p>If though, we have a more specific and realistic definition of water:</p>

<p>This can be an example:</p>

<ol>
<li>The body of water can be made up of <strong>finite</strong> number of water patches.</li>
<li>Each patch of water has to be closed and bounded.</li>
</ol>

<p>Even with this question, the answer relies very much on the form of the water patches as noted by others here. Zig-zag lines of water would mean $p_w =1$. Big circle pools of water would yield different probability(and still no easy answer). Because of the infinite number of arrangement of the water and the possible complexity of solution with each, this question is probably impossible to answer. </p>

<p>Unless the we know the exact water arrangement already, I believe we wouldn't be able to quantify the chance of having water in the hoop.</p>
",2013-10-22 06:34:22.423
57990,21918.0,1,58014.0,,,Comparing maximum likelihood estimation (MLE) and Bayes' Theorem,<bayesian><maximum-likelihood>,CC BY-SA 3.0,"<p>In Bayesian theorem, $$p(y|x) = \frac{p(x|y)p(y)}{p(x)}$$, and from the book I'm reading, $p(x|y)$ is called the <em>likelihood</em>, but I assume it's just the <em>conditional probability</em> of $x$ given $y$, right?</p>

<p>The <em>maximum likelihood estimation</em> tries to maximize $p(x|y)$, right? If so, I'm badly confused, because $x,y$ are both random variables, right? To maximize $p(x|y)$ is just to find out <em>the</em> $\hat y$? One more problem, if these 2 random variables are independent, then $p(x|y)$ is just $p(x)$, right? Then maximizing $p(x|y)$ is to maximize $p(x)$.</p>

<p>Or maybe, $p(x|y)$ is a function of some parameters $\theta$, that is $p(x|y; \theta)$, and MLE tries to find the $\theta$ which can maximize $p(x|y)$? Or even that $y$ is actually the parameters of the model, not random variable, maximizing the likelihood is to find the $\hat y$?</p>

<p><strong>UPDATE</strong></p>

<p>I'm a novice in machine learning, and this problem is a confusion from the stuff I read from a machine learning tutorial. Here it is, given an observed dataset $\{x_1,x_2,...,x_n\}$, the target values are $\{y_1,y_2,...,y_n\}$, and I try to fit a model over this dataset, so I assume that, given $x$, $y$ has a form of distribution named $W$ parameterized by $\theta$, that is $p(y|x; \theta)$, and I assume this is the <em>posterior probability</em>, right?</p>

<p>Now to estimate the value of $\theta$, I use MLE. OK, here comes my problem, I think the likelihood is $p(x|y;\theta)$, right? Maximizing the likelihood means I should pick the right $\theta$ and $y$?</p>

<p>If my understanding of likelihood is wrong, please show me the right way.</p>
",2013-10-22 06:38:49.637
57991,6204.0,2,,57990.0,,,,CC BY-SA 3.0,"<p>Normally $p(x|y)$ is a function of the parameter $y$. Consider the following reformulation of Bayes theorem:</p>

<p>$$p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}$$</p>

<p>Or even more explicitly (with regards to the notion of the likelihood):</p>

<p>$$p(\theta|x) = \frac{L(\theta;x)p(\theta)}{p(x)}$$</p>

<p>For a concrete example, consider the model</p>

<p>$$
X|\theta \sim Binomial(\theta)   \\
\theta \sim Beta(\alpha,\beta)
$$</p>
",2013-10-22 06:46:00.550
57992,22925.0,1,,,,How to generate random data that conforms to a given mean and upper / lower endpoints?,<mean><random-generation><minitab>,CC BY-SA 3.0,"<p>I'm in need of generating a set of random numbers (between 100 and 300 would suffice), which conform to a given mean, and also fall within certain upper and lower endpoints. Using the statistics software that I have (Minitab), I can find options to </p>

<ol>
<li>generate random data with a given mean but no endpoints, and </li>
<li>generate random data with endpoints and a given mode (not mean). </li>
</ol>

<p>I think the problem I'm having is that it doesn't seem to be a normal distribution; for instance, one of the data sets should give a mean of 2.90, but range between 0 and 30.66. </p>

<p>It is for a paper I'm writing, wherein the concern is with being able to run statistical analyses on the data in question; in other words, once I generate the data set, I have to do ANOVA and other tests on it. (I'm fine with doing all that; I just can't figure out how to generate the data.)</p>
",2013-10-22 06:51:41.413
58202,22480.0,1,58233.0,,,How can I get the prior of a random variable that's a function of a random variable in Bayesian data analysis?,<bayesian><prior>,CC BY-SA 3.0,"<p>I have a model which includes the following priors:</p>

<p>$\lambda_C \rightarrow \dfrac{1}{\sigma_C^2}$</p>

<p>and</p>

<p>$\sigma \sim \text{uniform}(0,500)$</p>

<p>Where $\sigma$ is the standard deviation and $\lambda_C$ is the precision of a normal distribution.</p>

<p>Being a beginner, I have problems conceptualizing the prior of $\lambda_C$. How would I even calculate the single values for the prior of $\lambda_C$?</p>
",2013-10-25 08:44:45.667
57994,5045.0,5,,,,,,CC BY-SA 3.0,"<p>This tag is ambiguous. Please do not use it as is, and use the specific tags <a href=""/questions/tagged/generalized-moments"" class=""post-tag"" title=""show questions tagged &#39;generalized-moments&#39;"" rel=""tag"">generalized-moments</a> (econometric), <a href=""/questions/tagged/gaussian-mixture"" class=""post-tag"" title=""show questions tagged &#39;gaussian-mixture&#39;"" rel=""tag"">gaussian-mixture</a> for Gaussian Mixture Models (machine learning), or <a href=""/questions/tagged/growth-mixture-model"" class=""post-tag"" title=""show questions tagged &#39;growth-mixture-model&#39;"" rel=""tag"">growth-mixture-model</a> for (psychometric).</p>

<p>One entry is retained with <a href=""/questions/tagged/gmm"" class=""post-tag"" title=""show questions tagged &#39;gmm&#39;"" rel=""tag"">gmm</a> so that the tag itself does not disappear: <a href=""http://stats.stackexchange.com/questions/27736/when-should-one-consider-using-gmm"">When should one consider using GMM?</a>. Please do not remove the tag from that entry.</p>
",2013-10-22 06:57:19.783
57993,5045.0,4,,,,,,CC BY-SA 3.0,"This tag is ambiguous. Please do not use it as is, and use the specific tags `generalized-moments` for econometric GMM, `gaussian-mixture` for machine learning GMM, and `growth-mixture-model` for psychometric GMM.",2013-10-22 06:57:19.783
57995,594.0,2,,57992.0,,,,CC BY-SA 3.0,"<p>If you want the <em>distribution</em> on the range min to max and with a given population mean:</p>

<p>One common solution when trying to generate a distribution with specified mean and endpoints is to use a location-scale family beta distribution.</p>

<p>The usual <a href=""http://en.wikipedia.org/wiki/Beta_distribution"" rel=""nofollow noreferrer"">beta</a> is on the range 0-1 and has two parameters, $\alpha$ and $\beta$. The mean of that distribution is $\frac{\alpha}{\alpha+\beta}$. If you multiply by $\text{max}-\text{min}$ and add $\text{min}$, you have something between $\text{max}$ and $\text{min}$ with mean $\text{min}+\frac{\alpha}{\alpha+\beta}(\text{max}-\text{min})$.</p>

<p>This suggests you should take</p>

<p>$\beta/\alpha = \frac{\text{max} - \text{mean}}{\text{mean}-\text{min}}$</p>

<p>Or </p>

<p>$\alpha/\beta = \frac{\text{mean} - \text{min}}{\text{max}-\text{mean}}$</p>

<p>This leaves you with a free parameter (you can choose $\alpha$ or $\beta$ freely and the other is determined). You could choose the smaller of them to be ""1"". Or you could choose it to satisfy some other condition, if you have one (such as a specified standard deviation). Larger $\alpha$ and $\beta$ will look more 'bell-shaped'.</p>

<p>In Minitab, <code>Calc</code> $\to$ <code>Random Data</code> $\to$ <code>Beta</code>.</p>

<p>--</p>

<p>Alternatively, you could generate from a <a href=""http://en.wikipedia.org/wiki/Triangular_distribution"" rel=""nofollow noreferrer"">triangular distribution</a> rather than a beta distribution. (Or any number of other choices!)</p>

<p>The triangular distribution is usually defined in terms of its min, max and mode, and its mean is the average of the min, max and mode. The triangular distribution is reasonably easy to generate from even if you don't have specialized routines for it.  To get the mode from a given mean, use mode = 3$\,\times\,$mean - min - max. However, the mean is restricted to lie in the middle third of the range (which is easy to see from the fact that the mean is the average of the mode and the two endpoints).</p>

<p>Below is a plot of the density functions for a beta (specifically, $\text{beta}(2,3)$) and a triangular distribution, both with mean 40% of the way between the min and the max:</p>

<p><img src=""https://i.stack.imgur.com/MG4vx.png"" alt=""enter image description here""></p>

<hr>

<p>One the other hand, if you want the <em>sample</em> to have a smallest value of min and a largest value of max and a given sample mean, that's quite a different exercise. There are easy ways to do that, though some of them may look a bit odd.</p>

<p>One simple method is as follows. Let $p=\frac{\text{mean} - \text{min}}{\text{max}-\text{min}}$. Place $b=\lfloor p(n-1)\rfloor$ points at 1, and $n-1-b$ points at 0, giving an average of $b/(n-1)$ and a sum of $b$. To get the right average, we need the sum to be $np$, so we place the remaining point at $np-b$, and then multiply all the observations by ${\text{max}-\text{min}}$ and add $\text{min}$.</p>

<p>e.g. consider $n$ = 12, min = 10, max = 60, mean = 30, so $p$ = 0.4, and $b$ = 4. With seven (12-1-4) points at 0 and four at 1, the sum is 4. If we place the remaining point at 12$\,\times\,$0.4$\,$-$\,$4 = 0.8, the average is 0.4 ($p$). We then multiply all the values by ${\text{max}-\text{min}}$ (50) and add $\text{min}$ (10) giving a mean of 30. Then randomly sample the whole set of $n$ without replacement, (or equivalently, just randomly order them). You now have a random sample with the required mean and extremes, albeit one from a discrete distribution.</p>
",2013-10-22 07:44:24.763
57996,22899.0,1,,,,Average partial effects,<econometrics><probit><communication>,CC BY-SA 3.0,"<p>I need to explain what average partial effects (APEs) are to a very general non-statistical audience (i.e. the APEs derived from a probit model). I have tried to define APEs using layman's terms but I find it hard, can somebody please help me with this?</p>
",2013-10-22 08:45:29.923
57997,22928.0,1,58003.0,,,Large sample size and partial F-test for multiple regression makes adding a variable always significant,<multiple-regression><f-test>,CC BY-SA 3.0,"<p>I am developing a two variable multiple regression model.
ie.
$$ Y = b0 - b1 * X1 + b2 * X2  $$</p>

<p>I am using the following formula for partial F-test from <a href=""http://luna.cas.usf.edu/~mbrannic/files/regression/Reg2IV.html"" rel=""nofollow"">http://luna.cas.usf.edu/~mbrannic/files/regression/Reg2IV.html</a> under the section Testing Incremental R2.  The F-statistics calculated is supposed to tell me if adding the second variable is significant (more details in that link).</p>

<p>$$ F= {\frac{(R_L^2 - R_S^2)/(k_L-k_s)}{(1-R_L^2)/(N-k_L-1)}}$$</p>

<p>My first variable has a strong correlation:
regression_coeff_string:    b1 =      0.664, b0 =      0.035
R2_val: 0.564</p>

<p>My second variable has a weak correlation:
regression_coeff_string:    b1 =    -25.026, b0 =      0.469, 
R2_val: 0.027</p>

<p>Adding my seond variable only marginally improves the R2 value
regression_coeff_string:    b0 =     0.0559, b1 =     0.6633, b2 =    -5.2222, 
R2_val: 0.565</p>

<p>However, because I have a sample size 2949, that
With $$ R_L^2 = 0.565, R_S^2 = 0.564$$
$k_L$ the number of predictors in the full set being 2, 
$k_S$ the number of predictors in the subset being 1
$$ F= {\frac{(0.565 - 0.564)/(2-1)}{(1-0.565)/(2949-2-1)}} = 6.77$$</p>

<p>With F(1,2946) at 0.05 confidence having a F_stat of 4.182, the result is significant.  But it seems that it is only because the sample size is large.  If I sort the second variable X2 in ascending order in Excel and leave the order of the Y and X1 variables unchanged, I would still get a significant F score.</p>

<p><strong>Question:</strong> How can I do a fair incremental R2 test for the addition of a new variable in multiple regression when the sample size becomes large?  </p>

<p>Simply looking at the R2 of each X variable individually does not take into account that that they may be cross-correlated, that is why I turned to the incremental R2 test to see how the overall R2 improves relative to adding a new variable.</p>

<p><strong>EDIT1:</strong></p>

<p>The context of my example is predicting solar radiation.  The first variable is a solar radiation variable from NWP (numerical weather prediction) software (ie. high correlation).  The other variables are other NWP output variables and we are trying to improve our prediction.  </p>
",2013-10-22 08:58:05.767
57998,12282.0,2,,57990.0,,,,CC BY-SA 3.0,"<ul>
<li><em>""...$p(x|y)$ is called the likelihood...""</em></li>
</ul>

<p>$p(x|y)$ is <strong>the likelihood of y given x</strong>. Saying what it is the likelihood of is important. And yes, it is just the conditional probability of $x$ given $y$.</p>

<ul>
<li><em>""...if these 2 random variables are independent, then $p(x|y)$ is just $p(x)$, right? Then maximizing $p(x|y)$ is to maximize $p(x)$...""</em></li>
</ul>

<p>If they are independent, i.e. $p(x|y) = p(x)$, the $p(x)$ is constant with respect to $y$. Be careful here, as you don't specify what you are maximising with respect to - from what you wrote earlier, I would assume you are maximising with respect to $y$.</p>

<ul>
<li><em>...Or maybe, $p(x|y)$ is a function of some parameters $\theta$, that is $p(x|y;\theta)$, and MLE tries to find the $\theta$ which can maximize $p(x|y)$? Or even that y is actually the parameters of the model, not random variable, maximizing the likelihood is to find the $\hat{y}$?...</em></li>
</ul>

<p>Introducing $\theta$ makes this an entirely new problem. In general, the answer to most of this question here seems to be 'it depends'. We could denote parameters as $y$ if we wanted, and maximise with respect to them. Equally, we could have a situation where we maximise $p(x|y;\theta)$ with respect to parameters $\theta$ if that was a sensible way of approaching the problem at hand.</p>
",2013-10-22 09:14:44.040
57999,22930.0,1,,,,"How to interpret Hidden Markov Model parameters (transition matrix, emission matrix, and pi values)?",<hidden-markov-model><likelihood><expectation-maximization>,CC BY-SA 3.0,"<p>I am working on channel modeling for cognitive radio using HMM. I've written a MATLAB program for forward, backward and Baum-Welch algorithm for multiple sequences. After given some random input and running the program for 1000 to 4000 iterations I'm getting some results. But I'm not getting how to interpret my results.</p>

<p>Will be glad if anyone can talk about inputting matrix/log-likelihood/transition matrix/emission matrix.</p>
",2013-10-22 09:58:26.890
58000,22899.0,1,,,,Asymptotic Theory in Economics,<econometrics><asymptotics>,CC BY-SA 3.0,"<p>I am interested in deepening my Asymptotic Theory understanding. My current knowledge is that of a typical PhD student (from a decent University), say at the level of Green's textbook. Are there any good book(s) that you would recommend?</p>

<p>Much appreciated. </p>
",2013-10-22 10:29:40.320
58001,20470.0,2,,57999.0,,,,CC BY-SA 3.0,"<p>For MatLab, I would recommend using the <a href=""http://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html"" rel=""nofollow"">HMM toolbox</a>. It allows you to do pretty much all you would need from an HMM model. </p>

<p>If you feel strongly about using your own code, before running on a real dataset, you should probably validate your Baum Welch implementation by checking whether it actually returns sensible results. You can use an experimental setup similar to below. Please note that I am using the HMM toolbox functions, but it is more the order of steps that I am trying to draw your attention to.</p>

<pre><code>M = 3; % number of observation levels
N  = 2; % number of states

% A - ""true"" parameters (of your validation model)
prior0 = normalise(rand(N ,1));
transmat0 = mk_stochastic(rand(N ,N ));
obsmat0 = mk_stochastic(rand(N ,M));

% B- using the real parameters in step A, simulate a sequence of states and corresponding observations
n_seq = 5;  % you want to generate 5 multiple sequences
seq_len= 100; % you want each sequence to be of length 100
obs_seq, state_seq = dhmm_sample(prior0, transmat0, obsmat0, n_seq, seq_len);

% C- like you say you do, generate some initial guesses of the real parameters (from step A) that you want to learn
prior1 = normalise(rand(N ,1));
transmat1 = mk_stochastic(rand(N ,N ));
obsmat1 = mk_stochastic(rand(N ,M));

% D - train based on your guesstimates using EM (Baum-Welch)
[LL, prior2, transmat2, obsmat2] = dhmm_em(data, prior1, transmat1, obsmat1, 'max_iter', 5);

% E- Finally, compare whether your trained values in step D are actually similar to the real values (that generated your data) from Step A. 
% The simplest way to do that is to print them side by side or look at the absolute differences...
obsmat0
obsmat2

transmat0
transmat2
</code></pre>

<p>The interpretations of the transition matrix, observation (emission) matrix and loglikelihoods is a broad topic. I assume you already have a fair understanding since you could implement your own Baum-Welch. The easiest and best read on HMMs (in my opinion) is <a href=""http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf"" rel=""nofollow"">Rabiner's paper.</a> I would recommend having a look at this if you haven't yet.</p>
",2013-10-22 10:33:21.310
58002,10135.0,2,,57987.0,,,,CC BY-SA 3.0,"<p>$E(r_t^2|r_{t-j},j=1,2,...)=E(\sigma^2_{t|t-1}\epsilon_t^2|r_{t-j},j=1,2,...)$. Now given $r_{t-j},j=1,2,...$, you know the values of $\sigma_{t|t-1}$ because $\sigma^2_{t|t-1}=\omega+\alpha r_{t-1}^2$. So you can take it out of expectation to have: $E(r_t^2|r_{t-j},j=1,2,...)=\sigma^2_{t|t-1}E(\epsilon_t^2|r_{t-j},j=1,2,...)$. Also $\epsilon_t$ is independent of $r_{t-j},j=1,2,...$, so (roughly speaking) any function of that is also independent of $r_{t-j},j=1,2,...$. In particular, $f(x)=x^2$. So, $\epsilon_t^2$ is independent of $r_{t-j},j=1,2,...$. Hence, $E(r_t^2|r_{t-j},j=1,2,...)=\sigma^2_{t|t-1}E(\epsilon_t^2)$. Finally note that $\epsilon_t\sim N(0,1)$. So $Var(\epsilon_t)=E(\epsilon_t^2)=1$. Therefore, $E(r_t^2|r_{t-j},j=1,2,...)=\sigma^2_{t|t-1}.$ Now we can change $t$ to $t+h$ to get $h$-step ahead forecasts, i.e. $E(r_{t+h}^2|r_{t-j},j=1,2,...)=\sigma^2_{t+h|t-1}.$ I think you can now answer your 2nd question. Hint: you condition on previous returns $r_t$ that makes your expectation easy to compute.</p>
",2013-10-22 11:06:12.883
58003,503.0,2,,57997.0,,,,CC BY-SA 3.0,"<p>The test you are doing is ""fair"", it's just that p-values don't answer the question you want to ask (they often don't). The way to proceed is to figure out what change in effect size is substantively meaningful and base decisions on that.</p>

<p>This is entirely dependent on your field and, indeed, on your question. To illustrate: If 1 in 1000 children misunderstand a question on a test, that is a very small proportion, and won't affect the validity of the test much. But if 1 in 1000 airplane trips end in a crash, that is a very large proportion and would end aviation.</p>

<p>Is there <em>any</em> context in which a change of $R^2$ from 0.564 to 0.565 is important? I can't think of one, offhand, but I haven't had all my coffee :-). Perhaps some variation on the plane crash scenario. </p>
",2013-10-22 11:15:23.507
58004,1406.0,2,,58000.0,,,,CC BY-SA 3.0,"<p>Since you mention <a href=""http://people.stern.nyu.edu/wgreene/Text/econometricanalysis.htm"" rel=""nofollow"">Greene's book</a>, I assume you are interested in more in-depth understanding of asymptotic statistics. Then I can recommend A. van der Vaart's ""Asymptotic statistics"" and H. White's ""Asymptotic theory for econometricians"". Also J. Wooldridge's ""Econometric Analysis of Cross Section and Panel Data"" has nice chapters on asymptotic theory.</p>
",2013-10-22 11:16:17.970
58005,11262.0,1,,,,Questions about posterior linear regression,<regression><bayesian>,CC BY-SA 3.0,"<p>Just 2 simple questions IÂ´m struggling with. Hope you can help.</p>

<p>Suppose that the model $y=\beta X +\epsilon$ with $\epsilon \sim \text{ Normal}(o,\sigma^2I_n)$ has a prior $\beta \sim \text{ Normal}( \beta_0, k(X^TX)^{-1})$</p>

<p>I want two things:</p>

<p>1.) I want to show that for the density of $\beta$ that</p>

<p>$$p(\beta) \propto \exp \{-\frac{1}{2} k^{-1} (\beta^TX^TX\beta-2\beta^TX^TX\beta_0) \} $$</p>

<p>2.) I want to show that 
$$p(\beta | y,X,\sigma^2) \propto p(y|X,\beta, \sigma^2)\cdotp(\beta) $$</p>

<p>It would be very nice if somebody can say anything about this. I would also know why $$p(\beta|y,X,\sigma^2) \propto \exp\{-\frac{1}{2}(k^{-1}+\sigma^{-2})\beta^TX^TX\beta -2\beta^TX^TX\beta_0 \}$$</p>
",2013-10-22 11:23:40.697
58006,9095.0,1,58009.0,,,Finding mean and SD of 2 parts of a whole,<standard-deviation><mean><compositional-data>,CC BY-SA 3.0,"<p>I am putting together a review/meta-analysis of body composition in children. The data I will analyze consists of measures of fat-mass (FM) and fat-free-mass (FFM), which when summed  equal total mass. While most papers report the means and SDs for FM and FFM, which is what I want, every so often I get a paper that reports the mean and SD of total mass, and a mean and SD of %FM (i.e. FM/(FM+FFM)). </p>

<p>My question: Can I calcuate the mean and SD of FM and FFM, given I have the mean and SD of total mass, and the mean and SD of FM/(FM+FFM). </p>
",2013-10-22 11:36:50.420
58007,10135.0,2,,58000.0,,,,CC BY-SA 3.0,"<p>""Asymptotic theory for econometricians"" by Halbert White. ""Asymptotic Theory of Statistics and Probability"", by Anirban DasGupta.</p>
",2013-10-22 11:38:09.667
58008,22821.0,1,58016.0,,,What does the inverse of covariance matrix say about data? (Intuitively),<bayesian><maximum-likelihood><covariance><matrix>,CC BY-SA 3.0,"<p>I'm curious about the nature of $\Sigma^{-1}$. Can anybody tell something intuitive about ""What does $\Sigma^{-1}$ say about data?""</p>

<p>Edit:</p>

<p>Thanks for replies</p>

<p>After taking some great courses, I'd like to add some points:</p>

<ol>
<li>It is measure of information, i.e., $x^T\Sigma^{-1}x$ is amount of info along the direction $x$.</li>
<li><strong>Duality:</strong> Since $\Sigma$ is positive definite, so is $\Sigma^{-1}$, so they are dot-product norms, more precisely they are dual norms of each other, so we can derive Fenchel dual for the regularized least squares problem, and do maximization w.r.t dual problem. We can choose either of them, depending on their conditioning.</li>
<li><strong>Hilbert space:</strong> Columns (and rows) of $\Sigma^{-1}$ and $\Sigma$ span the same space. So there is not any advantage (other that when one of these matrices is ill-conditioned) between representation with $\Sigma^{-1}$ or $\Sigma$</li>
<li><strong>Bayesian Statistics:</strong> norm of $\Sigma^{-1}$ plays an important role in the Bayesian statistics. I.e. it determined how much information we have in prior, e.g., when covariance of the prior density is like $\|\Sigma^{-1}\|\rightarrow 0 $  we have non-informative (or probably Jeffreys prior)</li>
<li><strong>Frequentist Statistics:</strong> It is closely related to Fisher information, using the CramÃ©râ€“Rao bound. In fact, fisher information matrix (outer product of gradient of log-likelihood with itself) is CramÃ©râ€“Rao bound it, i.e. $\Sigma^{-1}\preceq \mathcal{F}$ (w.r.t positive semi-definite cone, i.e. w.r.t. concentration ellipsoids). So when $\Sigma^{-1}=\mathcal{F}$ the maximum likelihood estimator is efficient, i.e. maximum information exist in the data, so frequentist regime is optimal. In simpler words, for some likelihood functions (note that functional form of the likelihood purely depend on the probablistic model which supposedly generated data, aka generative model), maximum likelihood is efficient and consistent estimator, rules like a boss. (sorry for overkilling it)</li>
</ol>
",2013-10-22 12:00:53.823
58009,7949.0,2,,58006.0,,,,CC BY-SA 3.0,"<p>Just a partial answer, adressing one of your questions.</p>

<p>Generally, for probability distributions you have:</p>

<p>$$
E(X+Y) = E(X)+E(Y)
$$
and
V
$$
V(X+Y) = V(X)+V(Y) + 2\text{Cov}(X,Y)
$$
with E and V being the expectation and variance respectively ($SD = \sqrt{V}$).   </p>

<p>For you, this means the mean of the sum is just the sums of the means (so you can get it). For the SD of the sum you also need the covariance or correlation between the FM and FFM. One might use sensible guesses to get realistic lower and upper bounds. Your question regarding FM/(FM+FFM) is much more complicated. </p>

<p>I am not even sure that it is possible without knowing the exact distributions. There is an approximate solution using the delta method:
<a href=""http://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables"" rel=""nofollow"">http://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables</a>. But you also require the covariance for that.</p>
",2013-10-22 12:07:08.897
58035,6204.0,2,,58029.0,,,,CC BY-SA 3.0,"<p>Further to Eupraxis1981's discussion of informative priors, you can think of the ""information"" in a prior as inversely proportional to its variance. Consider a prior with near zero variance: you're basically saying ""before looking at the data, I'm almost positive I already know the location of the true value of the statistic."" Conversely, if you set a really wide variance, you're saying ""without looking at the data, I have really no assumptions about the true value of the parameter. It could be pretty much anywhere, and I won't be that surprised. I've got hunch it's probably near the mode of my prior, but if it turns out to be far from the mode I won't actually be surprised.""</p>

<p>Uninformative priors are attempts to bring no prior assumptions into your analysis (how successful they are is open to debate). But it's entirely possible and sometimes useful for a prior to be only ""weakly"" informative.</p>
",2013-10-22 16:59:02.657
58010,3922.0,2,,58000.0,,,,CC BY-SA 3.0,"<p>Ferguson's <a href=""http://rads.stackoverflow.com/amzn/click/0412043718"" rel=""nofollow""><em>A Course in Large Sample Theory</em></a> is the best concise introduction to the topic, and it is written in a nice didactic way of having an equivalent of a week's lecture course material in a chapter followed by a strong set of exercises. (Ferguson introduced GMM in 1968 under the name of the minimum $\chi^2$, and it is tucked in as one of the exercises in that book). Van der Vaart's <a href=""http://rads.stackoverflow.com/amzn/click/0521784506"" rel=""nofollow""><em>Asymptotic Statistics</em></a>, recommended by others, is great, too, but it's going off in weird directions (for an economist). Another relatively easy introduction to the first-order asymptotics is Lehmann's <a href=""http://rads.stackoverflow.com/amzn/click/0387985956"" rel=""nofollow""><em>Elements of Large Sample Theory</em></a>. I would argue though that you would get a better mileage out of a book like Smith &amp; Young's <a href=""http://rads.stackoverflow.com/amzn/click/0521548667"" rel=""nofollow""><em>Essentials of Statistical Inference</em></a>, as it will teach you about how statisticians think (sufficiency, UMPT, Cramer-Rao bound, etc.).</p>

<p>Of course you won't find the odd econometric asymptotics such as <a href=""http://biomet.oxfordjournals.org/content/75/2/335.short"" rel=""nofollow"">unit roots</a> or <a href=""http://www.jstor.org/stable/2171753"" rel=""nofollow"">weak instruments</a>. Few statisticians have heard of them, and these are wa-a-ay too exotic for them. However, you would definitely want to revisit these unusual papers to shake off the wrong belief that everything asymptotic is asymptotically normal at $\sqrt{n}$ rate (you can find disturbing counterexamples <a href=""http://www.citeulike.org/user/ctacmo/article/1318051"" rel=""nofollow"">here</a> and <a href=""http://www.citeulike.org/user/ctacmo/article/575126"" rel=""nofollow"">there</a>, too).</p>
",2013-10-22 12:09:32.127
58011,10409.0,1,,,,"Simple way to categorize: terrible, poor, average, good, excellent",<r><clustering><binning>,CC BY-SA 3.0,"<p>I have a data frame with the following:</p>

<pre><code>&gt; summary(d5$points)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -4.200   0.000   1.000   2.579   5.000  23.400 

&gt; sd(d5$points)
[1] 3.736616
</code></pre>

<p>What's a simple, but statistically sound way to categorize this data into <code>terrible</code>, <code>poor</code>, <code>average</code>, <code>good</code>, <code>excellent</code>.</p>

<p>I'm using R.</p>

<p><strong>Edit:</strong></p>

<p>Higher is points is better.  Negative points is <code>terrible</code>.  A good game would be a player scoring 6+ points, but that's just from my observations.</p>

<p>As requested, are are the histograms.</p>

<p>All Players</p>

<p><img src=""https://i.stack.imgur.com/GHRNR.png"" alt=""Histogram of all players""></p>

<p>Top 100 Players (based on their avg points)</p>

<p><img src=""https://i.stack.imgur.com/7shPU.png"" alt=""Histogram of the best players""></p>
",2013-10-22 12:24:58.413
58012,17573.0,2,,57924.0,,,,CC BY-SA 3.0,"<p>First, the incidental parameter problem is pretty easy to solve in a discrete duration model.  As long as you are willing to assume a logistic form for your model, you can eliminate the incidental parameters via a clever conditioning argument.  The usual cite in economics is Chamberlain (1980, Rev Econ Stud).  If you prefer a textbook, there is Greene's <em>Econometric Analysis</em> (any of the recent editions) --- look up ""fixed effects model binary choice"" or ""Chamberlain"" in the index.  In the seventh edition, the discussion runs from pg 721 through 725.   The resulting estimator is usually called ""fixed-effects logit"" or ""Chamberlain's estimator.""  </p>

<p>To be clear, you DO NOT just run a logistic regression with a bunch of dummy variables for households.  If you are a <a href=""http://www.stata.com/help.cgi?xtlogit"" rel=""nofollow noreferrer"">Stata</a> user, the <code>xtlogit</code> command with the <code>fe</code> option runs Chamberlain's fixed effects logit model.  In R, I don't know how to do it.  There are a couple of questions on this here at cross validated (<a href=""https://stats.stackexchange.com/questions/2134/r-package-for-fixed-effect-logistic-regression"">one</a>, <a href=""https://stats.stackexchange.com/questions/10141/discrete-choice-panel-models-in-r?lq=1"">two</a>), and <a href=""https://stackoverflow.com/questions/2804001/panel-data-with-binary-dependent-variable-in-r"">one</a> over at stack overflow.  The answers  in those threads seem mostly to misunderstand what the Chamberlain estimator is, and I think the right conclusion from them is that Chamberlain's estimator is not currently implemented in R. (I would love to be corrected if I am wrong)</p>

<p>Looking over your question again, I wonder whether you really want a fixed effects estimator.  As with any fixed effects estimator, you are not going to be able to directly estimate the effects of any household characteristic which does not change over time.  Generally, schooling, occupation, household size are fixed or almost fixed in a short panel.  If you include time dummies (and why wouldn't you?), then any characteristic which changes regularly with time within each household cannot be included.  Age, for example, since, once you control for time, it is just birth date, a fixed characteristic of household head.  Similarly, even the effect of the duration of the unemployment spell cannot be measured once you have time dummies and household dummies, unless some households have multiple unemployment spells.</p>
",2013-10-22 12:56:29.680
58013,22705.0,1,,,,Handling stationarity issues in proc ucm/state space time series models,<time-series><stationarity><sas><cointegration><state-space-models>,CC BY-SA 3.0,"<p>Hope I'm able to find someone who can answer this question. The previous one didn't get answered!</p>

<p>Proc ucm is the SAS implementation (using state space concepts) to isolate the unobserved trend, seasonality &amp; estimate the coefficients of regressors simultaneously. </p>

<p>Documentation/research on proc ucm is sparse. There are two questions I'm trying to find answers for:
a. Does the dependent variable have to be treated for weak stationarity (differenced) before running proc UCM?
Ans. <a href=""http://www.iasri.res.in/sscnars/socialsci/17-Structural%20Time%20Series%20Models%20for%20Describing%20Trend%20in%20All%20India%20Sunflower%20Yield%20Using%20SAS.pdf"" rel=""nofollow"">http://www.iasri.res.in/sscnars/socialsci/17-Structural%20Time%20Series%20Models%20for%20Describing%20Trend%20in%20All%20India%20Sunflower%20Yield%20Using%20SAS.pdf</a></p>

<p>The attached paper seems to suggest that I don't need to worry about stationarity while using proc ucm. </p>

<p>However, I have found a contrary situation in my data. Consider my dependent is y. And, t &amp; s are the smoothed trend &amp; season components isolated by proc ucm. I'd have expected the series (y-t-s) to be stationary. But, it was not.</p>

<p>Thereby, I conclude that proc ucm is not capable of handling non-stationary time series, until I explicitly difference/stationarize the dependent.</p>

<p>Is this right?</p>

<p>b. I also have regressors which exhibit co-integrated relationships with y. From Granger's research paper, it is evident that spurious regression results when co-integrated series exists. But, I hear that's only if we use OLS-based proc reg. Proc UCM is based on maximum likelihood estimation. Is co-integration not a problem with maximum likelihood estimation based regression? </p>

<p>I could be ambiguous in my problem statement above and can clarify if need be. </p>
",2013-10-22 13:12:30.567
58026,8958.0,2,,57156.0,,,,CC BY-SA 3.0,"<p>I doubt that unsupervised will work better but it could be a cool exercise to try out. Unsupervised learning with random forest is done by constructing a joint distribution based on your independent variables that roughly describes your data. Then simulate a certain number of observations using this distribution. For example if you have 1000 observations you could simulate 1000 more. Then you label them, e.g. 1:= real observation, 0:=simulated observation. After this, you run a usual random forest classifier trying to distinguish the real observations from the simulated ones. Note that you must have the calculate the proximity option turned on. The real useful output is exactly this, a description of proximity between your observations based on what Random Forest does when trying to assign these labels. You now have a description of how ""close"" or ""similar"" your observations are from each other and you could even cluster them based on many techniques. A straightforward one would be to select thresholds for these ""distances"". I mean stick together observations that are closer than a certain threshold. Another easy option is to do hierarchical clustering but using this particular distance matrix. If you can work with R, most hierarchical clustering packages allow you to feed the functions custom distance matrices. You then select a cutoff point, you may visualize it as a dendrogram and so on and so forth. </p>

<p><a href=""http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering.htm"" rel=""noreferrer"">This</a> used to be a very good tutorial on Random Forest clustering and they shared some useful R functions which they wrote for this purpose but the link seems to be dead now. Maybe it will come back up later. They also wrote a very neat random glm R package (which is analogous to random forest but based on duh...glms) if you want to check that out. You could always write to the authors and ask for the material for Random Forest classification which used to be available on the dead link. I have the R code but it's too large to paste here, I can send it to you if you send me a private message. </p>
",2013-10-22 15:43:34.490
58014,16043.0,2,,57990.0,,,,CC BY-SA 3.0,"<p>I think the core misunderstanding stems from questions you ask in the first half of your question. I approach this answer as contrasting MLE and Bayesian inferential paradigms. A very approachable discussion of MLE can be found in chapter 1 of Gary King, <em>Unifying Political Methodology.</em> Gelman's <em>Bayesian Data Analysis</em> can provide details on the Bayesian side.</p>

<blockquote>
  <p>In Bayes' theorem, $$p(y|x)=\frac{p(x|y)p(y)}{p(x)}$$
   and from the book I'm reading, $p(x|y)$ is called the likelihood, but I assume it's just the conditional probability of $x$ given $y$, right?</p>
</blockquote>

<p>The likelihood <em>is</em> a conditional probability. To a Bayesian, this formula describes the distribution of the parameter $y$ given data $x$ and prior $p(y)$. But since this notation doesn't reflect your intention, <strong>henceforth I will use ($\theta$,$y$) for parameters and $x$ for your data.</strong></p>

<p>But your update indicates that $x$ are observed from some distribution $p(x|\theta,y)$. If we place our data and parameters in the appropriate places in Bayes' rule, we find that these additional parameters pose no problems for Bayesians:
$$p(\theta|x,y)=\frac{p(x,y|\theta)p(\theta)}{p(x,y)}$$</p>

<p>I believe this expression is what you are after in your update.</p>

<blockquote>
  <p>The maximum likelihood estimation tries to maximize $p(x,y|\theta)$, right? </p>
</blockquote>

<p>Yes. MLE posits that $$p(x,y|\theta) \propto p(\theta|x,y)$$
That is, it treats the term $\frac{p(\theta,y)}{p(x)}$ as an unknown (and unknowable) constant. By contrast, Bayesian inference treats $p(x)$ as a normalizing constant (so that probabilities sum/integrate to unity) and $p(\theta,y)$ as a key piece of information: the prior. We can think of $p(\theta,y)$ as a way of incurring a penalty on the optimization procedure for ""wandering too far away"" from the region we think is most plausible. </p>

<blockquote>
  <p>If so, I'm badly confused, because $x,y,\theta$ are random variables, right? To maximize $p(x,y|\theta)$ is just to find out the $\hat{\theta}$? </p>
</blockquote>

<p>In MLE, $\hat{\theta}$ is assumed to be a <em>fixed</em> quantity that is unknown but able to be inferred, <em>not</em> a random variable. Bayesian inference treats $\theta$ as a random variable. Bayesian inference puts probability density functions <em>in</em> and gets probability density functions <em>out</em>, rather than point summaries of the model, as in MLE. That is, Bayesian inference looks at the full range of parameter values and the probability of each. MLE posits that $\hat{\theta}$ is an adequate summary of the data given the model.</p>
",2013-10-22 13:33:40.650
58015,22933.0,2,,16209.0,,,,CC BY-SA 3.0,"<pre><code>as.numeric(factor(c(""d"", ""a"", ""b"", ""b"", ""c"")))

[1] 4 1 2 2 3
</code></pre>
",2013-10-22 13:36:16.093
58016,22399.0,2,,58008.0,,,,CC BY-SA 3.0,"<p>It is a measure of precision just as $\Sigma$ is a measure of dispersion. </p>

<p>More elaborately, $\Sigma$ is a measure of how the variables are dispersed around the mean (the diagonal elements) and how they co-vary with other variables (the off-diagonal) elements. The more the dispersion the farther apart they are from the mean and the more they co-vary (in absolute value) with the other variables the stronger is the tendency for them to 'move together' (in the same or opposite direction depending on the sign of the covariance). </p>

<p>Similarly,  $\Sigma^{-1}$ is a measure of how tightly clustered the variables are around the mean (the diagonal elements) and the extent to which they do not co-vary with the other variables (the off-diagonal elements). Thus, the higher the diagonal element, the tighter the variable is clustered around the mean. The interpretation of the off-diagonal elements is more subtle and I refer you to the other answers for that interpretation.</p>
",2013-10-22 13:42:11.060
58017,306.0,2,,58011.0,,,,CC BY-SA 3.0,"<p>Apply a Box-Cox transformation. Then use the following :</p>

<ol>
<li>x &lt; mean - 2 * sigma : terrible</li>
<li>mean - 2 * sigma &lt;= x &lt; mean - sigma : poor</li>
<li>mean - sigma &lt;= x &lt; mean + sigma : average</li>
<li>mean + sigma &lt;= x &lt; mean + 2 * sigma : good</li>
<li>mean + 2 * sigma &lt;= x : excellent.</li>
</ol>

<p>in absence of any other information, i would go this away. however at least a histogram could have been given.</p>
",2013-10-22 13:45:33.197
58018,668.0,2,,57972.0,,,,CC BY-SA 3.0,"<p>This question comes up in analyses of spatial relationships, especially in studying edge effects in spatial statistics.  <strong>The answer depends intimately on the shape of the field, the shape of the water within the field, and the size of the hoop.</strong></p>

<p>A hoop of radius $r$ ""contains water"" if and only if its center lies within distance $r$ of water, which we may view as a point set $W$ in the plane.  To solve this, construct the $r$ <em>buffer</em> of the water: this is the locus of all points within distance $r$ of water.  Let's call it $B_r(W).$</p>

<p>Assume now that the field $F$ is also modeled as a set of points in the plane, and suppose it is <em>bounded</em> and <em>measurable.</em>  Regardless of the shape of $W$ (it might not be measurable), this implies (via standard compactness arguments) that $B_r(W) \cap F$ is (Borel) measurable for all positive buffer distances $r$.  Accordingly, it is meaningful to talk about the probability that a uniformly distributed hoop center lies within $B_r(W) \cap F$.  By the very definition of a uniform distribution, <strong>that probability is precisely the buffer area divided by the area of the field.</strong></p>

<h3>Examples</h3>

<p><img src=""https://i.stack.imgur.com/hggr2.png"" alt=""Figure""></p>

<ol>
<li><p>(A single puddle, at left.)  The field is a circle of diameter $R$ and the water is represented by a single interior point of distance at least $r$ from the boundary of the field.  The buffer $B_r(W)$ then is a circle of radius $r$ and its area relative to that of the field is $(r/R)^2$.</p></li>
<li><p>(Multiple puddles, center.)  As before, but now the water is a finite set of $n$ points all separated by at least $2r$ from each other and all lying at least $r$ from the field's boundary.  The probability is now $n(r/R)^2.$</p></li>
<li><p>(Circular pond, at right.)  Suppose the water is a circular pond of radius $w$ and, as before, lies no closer than $r$ to the field's boundary.  Its buffer is a circle of radius $w+r,$ whence the probability is $\left((w+r)/R\right)^2.$</p></li>
<li><p>(Canal system.) Let the field be a rectangle and suppose the water forms a gridded irrigation system covering the field.  The spacing between the edge of one irrigation canal and the edge of the next is no greater than $2r$.  The buffer $B_r(W)$ now covers the entire field (by design!) and the probability is $1$.</p>

<p><img src=""https://i.stack.imgur.com/v6xWS.png"" width=""300"" height=""300""></p>

<p><em>This figure (taken from an answer at <a href=""https://gis.stackexchange.com/a/17390"">https://gis.stackexchange.com/a/17390</a>) shows a small-radius buffer of an irregular set of line segments (the irrigation ditches, shown in black) within a rectangular field.  As the radius grows, eventually the buffer--the gray area--will merge into the entirety of the field.  (The pink areas are not relevant to the present discussion.)</em></p></li>
</ol>

<p>Note that in many of these cases the water may have practically no area--it would be represented on a map as a collection of points or line segments--while the probability could end up being anything between $0$ and $1$.  When the water forms a connected, simply-connected domain that is not too tortuous and the buffer distance $r$ is very small (smaller than the width of the water at its narrowest point), then the buffer area is approximately </p>

<p>$$|W| + rP + \pi r^2$$</p>

<p>where $|W|$ is the area of the water and $P$ is the length of the <em>perimeter</em> of the water (assuming it is finite).  (This formula is exact for piecewise linear convex domains: that is, convex <em>polygons</em>.)  To first order in $r,$ then, the probability is approximately</p>

<p>$$\frac{|W| + rP}{|F|}$$</p>

<p>where $|F|$ is the area of the field.  This shows that <strong>initially, for very small $r$, the probability grows linearly with $r$ and the slope (constant of proportionality) is the ratio of the perimeter of $W$ to the area of the field, $P/|F|.$</strong></p>
",2013-10-22 13:51:13.770
58019,10147.0,1,,,,Ordinal Regression,<correlation><generalized-linear-model><logit><ordered-logit>,CC BY-SA 3.0,"<p>In my data, the response variable is ordinal and there are 16 predicting variables which are also ordinal. I plan to perform an ordinal regression. But before doing that I want to graphically show the relationship between the response and predictor variables. In regular regression where the variables are numerical, we can produce a scatterplot matrix for that purpose. I am wondering whether for ordinal regression with ordinal predictors there is any graph that can be used to visually display the relationship before the formal analysis. Another possibility is to calculate the association between the response and the ordinal predictors before the analysis. Any recommendation on that part?
Also, I plan to perform a proportional odds model using R. Can anyone recommend a R package or functions which can be used to test the proportionality and also used for the analysis?</p>
",2013-10-22 14:56:25.547
58020,4735.0,1,58034.0,,,"kurtosis, positive skewed and negative skewed for probability distribution",<probability><distributions><normal-distribution><mathematical-statistics><descriptive-statistics>,CC BY-SA 3.0,"<p>When discussing probability distribution, I always read something such as excess kurtosis, positive kurtosis, positive skewed and negative skewed. What exactly do these concepts indicate? In practical applications, such as market return or something else, what can these characteristics tell us?</p>
",2013-10-22 15:06:37.997
58021,12358.0,1,58022.0,,,What is the name for this distribution defined on a circle,<distributions><density-function><terminology><circular-statistics>,CC BY-SA 3.0,"<p>One can define the probability distribution:
$$
p(\theta; \alpha, \theta_0) = \frac{ e^{ \alpha \cos( \theta-\theta_0) }}{ 2 \pi I_0(\alpha)}
$$
over an angular variable $\theta \in [0,2 \pi]$.  By what name(s) is this distribution called?</p>

<p>($I_0$ is the modified Bessel function and serves to normalize the distribution)</p>
",2013-10-22 15:15:20.203
58022,3922.0,2,,58021.0,,,,CC BY-SA 3.0,"<p>It's <a href=""http://en.wikipedia.org/wiki/Von_Mises_distribution"">von Mises distribution</a>, aka Tikhonov distribution, and plays the role similar to the normal distribution in 1D statistics.</p>

<p>For reference, $I_0(z)$ is the <a href=""http://en.wikipedia.org/wiki/Bessel_function"">modified Bessel function</a> of order 0.</p>
",2013-10-22 15:17:50.823
58024,3922.0,4,,,,,,CC BY-SA 3.0,"Directional statistics deals with distributions of angles (on a circle, on a sphere, or another kind of hypersphere appropriate for dimension at hand)",2013-10-22 15:20:37.460
58023,0.0,5,,,,,,CC BY-SA 3.0,,2013-10-22 15:20:37.460
58025,22935.0,1,58030.0,,,Are MRFs with edges to all observed data possible?,<machine-learning><graphical-model>,CC BY-SA 3.0,"<p>I have been discussing the following issue with a colleague of mine and I can't seem to wrap my head around it. I have a computer vision background, so I'm mostly familiar with 2D MRFs/CRFs for image restoration and segmentation. </p>

<p>What is clear to me is that MRFs typically have a simple 2D grid structure for p(x) and each underlying variable x_i has an edge to the corresponding pixel y_i in the graph for the posterior p(y|x). In CRFs you have an edge to each pixel y in the image. I can fully understand that CRFs are trained in a discriminative way and thus they are tractable and that it wouldn't be possible to really learn a generative model based on such a CRF graph. However in all the literature it seems that by definition the MRF model cannot have edges from a x_i to all pixels y, as it then would no longer be a MRF. For example the <a href=""http://www.sanjivk.com/DRF_bookchapter_ICVSS.pdf"" rel=""nofollow"">following</a> states: </p>

<blockquote>
  <p>If the conditional independence assumption is not used, the posterior will usually not be a MRF making the inference difficult.</p>
</blockquote>

<p>Now that it would not be tractable to actually learn such a model I can understand, but why would it no longer be an MRF and also what would it be then? Because I don't see why it would by definition become a CRF, as we do not NEED to condition it on the data. </p>

<p>I really hope someone can tell me what I am missing here, or where my mistake is!</p>
",2013-10-22 15:42:00.327
58027,22923.0,1,58028.0,,,Non-stationary series keep close to each other but correlation between growth rates is ~0 - how is this possible?,<regression><time-series><correlation><econometrics><cointegration>,CC BY-SA 3.0,"<p>I have 2 (monthly) time-series that look like this:</p>

<p><img src=""https://i.stack.imgur.com/oewFd.png"" alt=""cross plot of 2 series""></p>

<p>Economical intuition suggests that they are positively related and I can see this on the plot but if I compute correlation between their log-returns $\ln x_t/x_{t-1}$ and $\ln y_t/y_{t-1}$ this correlation is -0.04 this is basically zero and not statistically significant for my data size (~60 points).</p>

<p>How can it be?</p>

<p>One may say that series are cointegrated $y_t = a x_t +\varepsilon_t$, but then returns should follow $\Delta y_t = a \Delta x_t +\Delta \varepsilon_t$ and correlation between returns would also be significant. So if I see zero correlation between returns, there is no cointegration between levels as well - right? </p>

<p>So does this zero correlation means that there is no relation between series? 
If yes - why do they follow each other so closely..
If no - how to quantify this relation if correlation between diff'ed series is ~0 and cointegration tests for original series are inconclusive.</p>

<p>EDITS:</p>

<p>-- added  cointegration -> correlation link to address @AlecosPapadopoulos question.</p>
",2013-10-22 15:50:53.610
58028,12358.0,2,,58027.0,,,,CC BY-SA 3.0,"<p>Your measure is a short-time-scale measurement; note that you are only looking at the (log) differences between successive time stamps.  There is enough short-time-scale noise that is is masking the longer-term, $O(1 year)$, timescale correlations in the data.</p>
",2013-10-22 16:04:04.133
58029,6630.0,1,58033.0,,,What exactly is weakly informative prior?,<bayesian><prior>,CC BY-SA 3.0,"<p>Is there a precise definition of weakly informative prior?</p>

<p>How is it different from a subjective prior with broad support?</p>
",2013-10-22 16:22:35.387
58030,3183.0,2,,58025.0,,,,CC BY-SA 3.0,"<p>Restricted Boltzmann Machines are Markov random fields that usually have complete connectivity between the hidden and visible variables.  If we drop the ""restricted"" part and look at <a href=""http://www.scholarpedia.org/article/Boltzmann_machine"" rel=""nofollow"">Boltzmann Machines</a> more generally, there can also be complete connectivity among the visible variables (pixels).</p>

<p>So yes, they're possible.</p>

<p>I haven't read the whole paper you linked to, but here is my assessment of what they mean.</p>

<ul>
<li><p>If $p(x)$ is an MRF, and all the elements of $y$ are conditionally independent of one another given $x$, then $p(x, y)$ is also an MRF.</p></li>
<li><p>If the $y$ variables are <em>not</em> conditionally independent of one another, then this will not <em>necessarily</em> be true (although it still could be).  For instance, if there were directed connections among the $y$ variables, then you wouldn't have an MRF anymore.</p></li>
</ul>

<p>However, the case you seem to be thinking about, where there are undirected connections between the variables (like the Boltzmann Machines I linked to) are still considered MRFs because all their pieces are MRFs.</p>
",2013-10-22 16:34:44.577
58031,22938.0,1,,,,Coding for Regression Analysis,<regression><categorical-data><linear-algebra>,CC BY-SA 3.0,"<p>Q: What is the mathematical theory behind coding categorical variables for regression analysis?  </p>

<p>The situation is this:</p>

<p>For numeric variables, we can code observations via the transformation</p>

<p>$\textrm{coded value} = \frac{\textrm{uncoded value} - \frac{(high+low)}{2}}{\frac{(high-low)}{2}}$.</p>

<p>For categorical variables, the above clearly will not work.  I have been given the following codings without justification.  For two variables, such as material type A and type B, we code them as </p>

<p>$
Type A = -1 \\
Type B = 1.
$  </p>

<p>If the there are three variables, we code them as</p>

<p>$
Type A = \{1,0\} \\
Type B = \{0,1\} \\
Type C = \{-1,-1\}.
$</p>

<p>Once coded, we can then enter the data into SAS and run PROC REG to determine a regression model.</p>

<p>What is the mathematical basis for this coding?  My textbook, Montgomery Design and Analysis of Experiments, 8th Edition, provides little more than a page on coding in general.  My instructors cannot provide me with anything more than that it has something to do with orthogonality of vectors. </p>

<p>For the three variable case, we can see that any two vectors are linearly independent, but the three together are dependent.  As such if we arrange them in a matrix, the matrix must then be singular.  </p>

<p>Just as there is a justification for the numeric case, I want to understand the categorical.  I'm not afraid of matrices or linear algebra, although it seems like there could be a simple geometric explanation.  If it is too lengthy to explain, I would be happy with a textbook or online reference.</p>
",2013-10-22 16:42:07.447
58032,40516.0,1,,,user2551700,Comparison of two curves,<probability><p-value>,CC BY-SA 3.0,"<p>I have two measurements of something. You can think of them both as their own curve with known error on each point. They look quite similar, and if I calculate the R2 value it comes out to >0.9. But what I want to be able to calculate is a P value comparing these two curves (i.e., what is the probability that the difference i'm looking at is just due to noise?). Now I could easily do a student t-test at each point and come up with a P-value at each point, sure. But is there some way to come up with an over-all P value that uses all the points and not just one? Thanks very much for any help.</p>
",2013-10-22 16:47:18.103
58033,,2,,58029.0,user31668,,,CC BY-SA 3.0,"<p>The above comment is accurate. For a quantitive discussion, there are a number of ""uninformative"" priors in the literature. See for example Jeffreys' prior; see earlier post <a href=""https://stats.stackexchange.com/questions/20520/what-is-an-uninformative-prior-can-we-ever-have-one-with-truly-no-information/20751#20751"">What is an &quot;uninformative prior&quot;? Can we ever have one with truly no information?</a></p>

<p>They are defined in different ways, but the key is that they do not place too much probability in any particular interval (and hence favor those values) with the uniform distribution being a canonical example. The idea is to let the data determine where the mode is.</p>
",2013-10-22 16:48:53.887
58034,,2,,58020.0,user31668,,,CC BY-SA 3.0,"<p>Excess kurtosis $=$ kurtosis $-$ 3, since the normal distribution has kurtosis $=$ 3 (that is what the ""excess"" refers to). Also, kurtosis is always positive, so any reference to signs suggests they are saying that a distribution has more kurtosis than the normal. Skew indicates how asymmetrical the distribution is, with more skew indicating that one of the tails ""stretches"" out from the mode farther than the other does.</p>

<p>Practically: High kurtosis indicates a high propensity of a distribution to give you ""outliers"", in the sense that you will tend to get a lot of rather closely spaced outcomes, followed by a few, rare, way-out-there values. In the markets, this type of distribution can lull you into a sense of complacency, with well localized values most of the time, only to ruin your day with a crazy loss. For skewed distributions, a right-skew to a financial product indicates that its positive returns tend to be higher than its losses, for a simple example, which all other things being equal, is good.</p>
",2013-10-22 16:57:16.083
58036,22399.0,2,,58031.0,,,,CC BY-SA 3.0,"<p>Consider your two types situation (say Gender to be concrete). Suppose, that you decide to code as follows:</p>

<p>If respondent is male: Set M = 1 or 0 otherwise</p>

<p>If respondent is female: Set F = 1 or 0 otherwise</p>

<p>Then the data set will look like so:</p>

<pre><code>M F Y
1 0 y1
0 1 y2
1 0 y3
1 0 y4
0 1 y5
0 1 y6
</code></pre>

<p>Obviously, including both these variables as part of your $x$ matrix would lead to a singular design matrix which cannot be inverted. In other words, you cannot estimate the effect of both 'male' and 'female' respondents on your dependent variable independently of each other. Thus, you decide to drop one of these variables (say 'F'). Then assuming a model that looks like so:</p>

<p>$y=a + b M $</p>

<p>the OLS estimate of the intercept will give us effect of the average 'male' respondent on $y$ and $a+b$ the effect of the 'female' respondent. In other words, $b$ gives us the difference between female and male respondents on $y$.</p>

<p>What happens if we code M as $1, -1$?</p>

<p>Then the equation for male respondents and female respondents are:</p>

<p>$y_m = a + b$ and</p>

<p>$y_f = a - b$</p>

<p>Thus, $b$ is now $\frac{y_m-y_f}{2}$ which is bit complex to interpret. Thus, a simpler way to obtain interpretability is to choose a coding scheme such as: $0.5, -0.5$ instead of $1, -1$ which then results in:</p>

<p>$y_m = a + 0.5 b$ and</p>

<p>$y_f = a - 0.5 b$</p>

<p>Thus, $b$ is $y_m-y_f$ which is identical to the interpretation we obtained when we chose to code them as $1, 0$.</p>

<p>The above logic can be extended to the context of more than two categorical variables. If you want to know more you should probably look up 'Contrast Coding' as the different ways to code categorical variables are different ways to estimate contrasts between the categorical variables (e.g., do we want to estimate the impact of male - female in which case we pick the 'M' column or do we want to estimate the impact of female - male in which we case we pick the 'F' column).</p>
",2013-10-22 17:27:14.333
58037,22637.0,1,58060.0,,,Value that increases the Standard Deviation,<standard-deviation>,CC BY-SA 3.0,"<p>I am puzzled by the following statement: </p>

<p>"" In order to increase the standard deviation of a set of numbers, you must add a value that is more than one standard deviation away from the mean""</p>

<p>What is <strong>the proof</strong> of that? I know of course how we define the standard deviation but that part I seem to miss somehow. Any comments? </p>
",2013-10-22 17:40:55.053
58038,14799.0,2,,58008.0,,,,CC BY-SA 3.0,"<p>Using superscripts to denote the elements of the inverse, $1/\sigma^{ii}$ is the variance of the component of variable $i$ that is uncorrelated with the $p-1$ other variables, and $-\sigma^{ij}/\sqrt{\sigma^{ii}\sigma^{jj}}$ is the partial correlation of variables $i$ and $j$, controlling for the $p-2$ other variables.</p>
",2013-10-22 17:57:30.763
58039,503.0,2,,58037.0,,,,CC BY-SA 3.0,"<p>Leaving aside the algebra (which also works) think about it this way: The standard deviation is square root of the variance. The variance is the average of the squared distances from the mean. If we add a value that is closer to the mean than this, the variance will shrink. If we add a value that is farther from the mean than this, it will grow. </p>

<p>This is true of any average of values that are non-negative. If you add a value that is higher than the mean, the mean increases. If you add a value that is less, it decreases.</p>
",2013-10-22 17:57:56.327
58040,7927.0,2,,58037.0,,,,CC BY-SA 3.0,"<p>I'll get you started on the algebra, but won't take it quite all of the way. First, standardize your data by subtracting the mean and dividing by the standard deviation: $$ Z = \frac{x-\mu}{\sigma} .$$ Note that if $x$ is within one standard deviation of the mean, $Z$ is between -1 and 1. Z would be 1 if $x$ were exactly one sd away from the mean. Then look at your equation for standard deviation: $$\sigma = \sqrt{\frac{\sum_{i=1}^{N}Z_i^2}{N-1}}$$ What happens to $\sigma$ if $Z_N$ is between -1 and 1?</p>
",2013-10-22 18:02:10.477
58041,9030.0,1,58054.0,,,conditional mutual information and how to deal with zero probabilities,<probability><conditional-probability><information-theory><mutual-information>,CC BY-SA 3.0,"<p>the conditional mutual between three sets of mutually exclusive variables, X, Y, and Z, is defined as follows.</p>

<p>$I(X,Y|Z) = \sum_{xyz} P(x,y,z) \log \frac{P(z)P(x,y,z)}{P(x,z)P(y,z)}$</p>

<p>my questions concern the $\log$ of the ratio of the probability products. </p>

<ol>
<li>if $P(z)$ or $P(x,y,z)$ is 0, then $\log(0)$ is undefined.</li>
<li>if $P(x,z)$ or $P(y,z)$ is 0, then $\log(\infty)$ is undefined.</li>
</ol>

<p>how do i deal with these 2 situations? the approach can be very flexible. for example, i thought about ignoring the inner sums where such conditions occur, but is this correct or reasonable?</p>

<p>any help is appreciated.</p>
",2013-10-22 18:08:36.503
58042,22941.0,2,,57997.0,,,,CC BY-SA 3.0,"<p>You may want to consider other metrics such as <a href=""http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2"" rel=""nofollow"">adjusted R square</a>, or <a href=""http://en.wikipedia.org/wiki/Mallows%27s_Cp"" rel=""nofollow"">Mallow's Cp</a>. </p>
",2013-10-22 18:10:17.457
58043,22942.0,1,99851.0,,,Problem with factorial design in Minitab,<minitab><tables>,CC BY-SA 3.0,"<p>I'm having a problem with a factorial design in Minitab, as it is not as clear how to proceed as all problems I've had thus far.</p>

<p>The problem is to create a factorial design with two factors where the first factor, Environment, which has two levels - H2O and Salt H2O whose values overlap (i.e. they cannot be differentiated by simply stating a value for low which is only met for H2O and one value for high which is only met by Salt H2O). </p>

<p>The second factor is Frequency and it is 10 for four measurements and 1 for four others.</p>

<p>The data is structured as a table such as:</p>

<pre><code>                         Environment
                   H2O                  Salt H2O

            10    value                  value 
Frequency 
</code></pre>

<p>The problem is that I don't know how to create this in Minitab. I'm aware that it will be a 2$\times$2 factorial design with 4 replicates, but as mentioned earlier I don't know how to define the Environment factor. I also don't understand how to put in the response variable. </p>

<p>I guess it will be the values in the table, but to what do these entries in the table correspond to in Minitab?</p>
",2013-10-22 18:40:24.197
58044,11359.0,2,,54637.0,,,,CC BY-SA 3.0,"<p><strong>Yes</strong>, it is possible and, yes, there are <code>R</code> functions that do it. Instead of computing the p-values of the repeated analyses by hand, you can use the package <code>Zelig</code>, which is also referred to in the <a href=""http://cran.r-project.org/web/packages/Amelia/vignettes/amelia.pdf"" rel=""nofollow noreferrer"">vignette</a> of the <code>Amelia</code>-package (<em>for a more informative method see my update below</em>). I'll use an example from the <code>Amelia</code>-vignette to demonstrate this:</p>

<pre><code>library(""Amelia"")
data(freetrade)
amelia.out &lt;- amelia(freetrade, m = 15, ts = ""year"", cs = ""country"")

library(""Zelig"")
zelig.fit &lt;- zelig(tariff ~ pop + gdp.pc + year + polity, data = amelia.out$imputations, model = ""ls"", cite = FALSE)
summary(zelig.fit)
</code></pre>

<p>This is the corresponding output including $p$-values:</p>

<pre><code>  Model: ls
  Number of multiply imputed data sets: 15 

Combined results:

Call:
lm(formula = formula, weights = weights, model = F, data = data)

Coefficients:
                Value Std. Error t-stat  p-value
(Intercept)  3.18e+03   7.22e+02   4.41 6.20e-05
pop          3.13e-08   5.59e-09   5.59 4.21e-08
gdp.pc      -2.11e-03   5.53e-04  -3.81 1.64e-04
year        -1.58e+00   3.63e-01  -4.37 7.11e-05
polity       5.52e-01   3.16e-01   1.75 8.41e-02

For combined results from datasets i to j, use summary(x, subset = i:j).
For separate results, use print(summary(x), subset = i:j).
</code></pre>

<p><code>zelig</code> can fit a <a href=""http://projects.iq.harvard.edu/zelig/models"" rel=""nofollow noreferrer"">host of models</a> other than least squares. </p>

<p>To get confidence intervals and degrees of freedom for your estimates you can use <code>mitools</code>:</p>

<pre><code>library(""mitools"")
imp.data &lt;- imputationList(amelia.out$imputations)
mitools.fit &lt;- MIcombine(with(imp.data, lm(tariff ~ polity + pop + gdp.pc + year)))
mitools.res &lt;- summary(mitools.fit)
mitools.res &lt;- cbind(mitools.res, df = mitools.fit$df)
mitools.res
</code></pre>

<p>This will give you confidence intervals and proportion of the total variance that is attributable to the missing data:</p>

<pre><code>              results       se    (lower    upper) missInfo    df
(Intercept)  3.18e+03 7.22e+02  1.73e+03  4.63e+03     57 %  45.9
pop          3.13e-08 5.59e-09  2.03e-08  4.23e-08     19 % 392.1
gdp.pc      -2.11e-03 5.53e-04 -3.20e-03 -1.02e-03     21 % 329.4
year        -1.58e+00 3.63e-01 -2.31e+00 -8.54e-01     57 %  45.9
polity       5.52e-01 3.16e-01 -7.58e-02  1.18e+00     41 %  90.8
</code></pre>

<p>Of course you can just combine the interesting results into one object:</p>

<pre><code>combined.results &lt;- merge(mitools.res, zelig.res$coefficients[, c(""t-stat"", ""p-value"")], by = ""row.names"", all.x = TRUE)
</code></pre>

<h1>Update</h1>

<p>After some playing around, I have found a more flexible way to get all necessary information using the <code>mice</code>-package. For this to work, you'll need to modify the package's <code>as.mids()</code>-function. Use Gerko's version posted in my <a href=""https://stats.stackexchange.com/questions/73562/analyzing-multiply-imupted-data-from-amelia-in-r-why-do-results-from-zelig-and"">follow-up question</a>:</p>

<pre><code>as.mids2 &lt;- function(data2, .imp=1, .id=2){
  ini &lt;- mice(data2[data2[, .imp] == 0, -c(.imp, .id)], m = max(as.numeric(data2[, .imp])), maxit=0)
  names  &lt;- names(ini$imp)
  if (!is.null(.id)){
    rownames(ini$data) &lt;- data2[data2[, .imp] == 0, .id]
  }
  for (i in 1:length(names)){
    for(m in 1:(max(as.numeric(data2[, .imp])))){
      if(!is.null(ini$imp[[i]])){
        indic &lt;- data2[, .imp] == m &amp; is.na(data2[data2[, .imp]==0, names[i]])
        ini$imp[[names[i]]][m] &lt;- data2[indic, names[i]]
      }
    } 
  }
  return(ini)
}
</code></pre>

<p>With this defined, you can go on to analyze the imputed data sets:</p>

<pre><code>library(""mice"")
imp.data &lt;- do.call(""rbind"", amelia.out$imputations)
imp.data &lt;- rbind(freetrade, imp.data)
imp.data$.imp &lt;- as.numeric(rep(c(0:15), each = nrow(freetrade)))
mice.data &lt;- as.mids2(imp.data, .imp = ncol(imp.data), .id = NULL)

mice.fit &lt;- with(mice.data, lm(tariff ~ polity + pop + gdp.pc + year))
mice.res &lt;- summary(pool(mice.fit, method = ""rubin1987""))
</code></pre>

<p>This will give you all results you get using <code>Zelig</code> and <code>mitools</code> and more:</p>

<pre><code>                  est       se     t    df Pr(&gt;|t|)     lo 95     hi 95 nmis   fmi lambda
(Intercept)  3.18e+03 7.22e+02  4.41  45.9 6.20e-05  1.73e+03  4.63e+03   NA 0.571  0.552
pop          3.13e-08 5.59e-09  5.59 392.1 4.21e-08  2.03e-08  4.23e-08    0 0.193  0.189
gdp.pc      -2.11e-03 5.53e-04 -3.81 329.4 1.64e-04 -3.20e-03 -1.02e-03    0 0.211  0.206
year        -1.58e+00 3.63e-01 -4.37  45.9 7.11e-05 -2.31e+00 -8.54e-01    0 0.570  0.552
polity       5.52e-01 3.16e-01  1.75  90.8 8.41e-02 -7.58e-02  1.18e+00    2 0.406  0.393
</code></pre>

<p>Note, using <code>pool()</code> you can also calculate $p$-values with $df$ adjusted for small samples by omitting the <code>method</code>-parameter. What is even better, you can now also calculate $R^2$ and compare nested models:</p>

<pre><code>pool.r.squared(mice.fit)

mice.fit2 &lt;- with(mice.data, lm(tariff ~ polity + pop + gdp.pc))
pool.compare(mice.fit, mice.fit2, method = ""Wald"")$pvalue
</code></pre>
",2013-10-22 18:47:52.037
58045,14799.0,2,,58037.0,,,,CC BY-SA 3.0,"<p>The puzzling statement gives a necessary but insufficient condition for the standard deviation to increase. If the old sample size is $n$, the old mean is $m$, the old standard deviation is $s$, and a new point $x$ is added to the data, then the new standard deviation will be less than, equal to, or greater than $s$ according as $|x-m|$ is less than, equal to, or greater than $s\sqrt{1+1/n}$.</p>
",2013-10-22 18:49:23.243
58046,22944.0,1,,,,Comparing two survey items with a third variable,<logistic><categorical-data><survey><binary-data>,CC BY-SA 3.0,"<p>I want to compare the results of two items by age category and do not really know what test to use. I tried a <a href=""https://en.wikipedia.org/wiki/Cochran%E2%80%93Mantel%E2%80%93Haenszel_statistics"" rel=""nofollow"">Cochranâ€“Mantelâ€“Haenzelâ€“test</a> at first, but I don't think it is testing what I really want to know.</p>

<p>We have two items (both dichotomous yes/no). We are interested in the proportion who said yes to item 2 if they said yes to item 1 and if this differs by age category. </p>

<p>Anyone have any ideas? I thought about doing a logistic regression with the ""events"" as those who chose item 2 and the ""trials"" as the number who chose item 1, with the age category as the dependent variable. Does that make sense? </p>
",2013-10-22 19:18:45.377
58086,2666.0,2,,58085.0,,,,CC BY-SA 3.0,"<p>The ""significance"" of the effect has no effect at all on the interpretation, given that you pay attention to confidence intervals.  Even better might be to compute simultaneous confidence intervals as made easy by the R <code>rms</code> package <code>Predict</code>, <code>plot.Predict</code>, and <code>lrm</code> functions using the R <code>multcomp</code> package.</p>
",2013-10-23 13:07:07.897
58047,22914.0,2,,57965.0,,,,CC BY-SA 3.0,"<p>I found this beautiful web page with precisely the information that I was looking for: <a href=""http://www.randomservices.org/random/urn/Birthday.html#General"" rel=""nofollow"">Birthday Problem at the university of Alabama in Huntsville</a>.</p>

<p>Following the derivation in Eq (13) therein, the number of possible $D$-samples from a population of $N$ balls, that exclude exactly $C$ of them is
$$
\#\{\# excluded=C\} = {N\choose C}\sum_{k=0}^{N-C}(-1)^k{N-C\choose k}(N-C-k)^D.
$$</p>

<p>Below, we evaluate the sum analytically. So the probability of ending up with exactly $C$ clean (unmarked) balls is the ratio of $\#\{\# excluded=C\}$ to the total number of possible samples:</p>

<p>$$
P_C = \frac{\#\{\# excluded=C\}}{N^D}=N^{-D} (N-C)! \binom{N}{C}\mathcal{S}_D^{(N-C)},
$$
where $\mathcal{S}_D^{(N-C)}$ is the Stirling number of the second kind.</p>
",2013-10-22 19:35:19.693
58048,21762.0,2,,58046.0,,,,CC BY-SA 3.0,"<p>Edited after clarification</p>

<p>Your logistic regression looks just fine. Maybe easier to sell, but similar, would be to use a chi squared test of independence between age and item 2 for people with positive item 1. An alternative to this test would be a trend test (e.g. test for non-zero Spearman rank correlation) that would consider the ordinal nature of age categories (also only for persons with positive answer to item 1).</p>
",2013-10-22 19:35:57.857
58049,19089.0,2,,55209.0,,,,CC BY-SA 3.0,"<p>Your reasoning here is not incorrect, but I understand why it is shaky. In essence, the K-S test looks for sufficient evidence that the true distribution, $F$, is different from your assumed distribution $F_0$ by looking at the maximum absolute deviation $K = \sup_x \hat F(x) - F_0(x)$. But first we must note two things:</p>

<ol>
<li><p>Your ""reliability"" functions here are also called <em>survival</em> functions, often denoted $\bar F$, and defined as $$\bar F(x) = 1 - F(x)$$ where $F$ is the cdf. So it is easily seen that $$K = \sup_x \hat F(x) - F_0(x) = \sup_x \hat {\bar F}(x) - \bar F_0(x).$$</p></li>
<li><p>The statistic $\sqrt nK$ has a Kolmogorov distribution <em>asymptotically</em>, but it does not at $n=20$. The military handbook you site above is taking a shortcut around this requirement to provide you with your critical value of 0.264.</p></li>
</ol>

<p>As for interpretation of your plot, I think you need to recognize the following: the <em><a href=""http://en.wikipedia.org/wiki/Statistical_power"" rel=""nofollow"">power</a></em> of the K-S test is very low at this sample size. Power of course is the probability of rejecting the null when the null is not true. You'll notice that, for example, at $t=4$, you would have to have <em>over 30% failure</em> in order to reject your distribution assumption, a far cry from the 5% the client wanted.</p>

<p>How to remedy this? That's the tough question for professional statisticians. My thought is that the reason the K-S test has such low power is that it is so <em>general</em>. Do you really need the Weibull fit? How about setting up tests for both 4-year and 8-year marks, and adjusting your level according to the fact that you have two tests. In fact, since these would be binary results (failed vs. didn't fail), you can possibly use <a href=""http://en.wikipedia.org/wiki/Fisher%27s_exact_test"" rel=""nofollow"">Fisher's Exact Test</a> to obtain both the level <em>and</em> the power of your test. Then you can be sure about what your saying in response to the requirement of ""90% confidence"".</p>
",2013-10-22 19:37:42.740
58050,8374.0,1,,,,Generate data from a t-distribution with specified mean and standard deviation,<r><distributions><random-generation><t-distribution><non-central>,CC BY-SA 3.0,"<p>How does one randomly sample from a T-distribution in R. From what I've found, the function <code>rt</code> in R doesn't let you specify the mean and standard deviation. For a normal distribution it is simply <code>rnorm(x,mu,sd)</code>.</p>

<p>EDIT:</p>

<p>For the t-distribution there is a <em>central</em> and a <em>non-central</em> version. I want to know the difference between the two.  In addition, if I want to specify the mean and standard deviation, does that automatically mean I am dealing with the non-central version of the t-distribution?</p>

<p>I chose to use the t-distribution because the data I am using are rather fat tailed and a t-distribution with a low degrees of freedom seems like a good idea. What other distributions are there for handling fat tailed data? Also it would be great if you can specify the function in R too.</p>
",2013-10-22 20:10:35.257
58051,22880.0,1,,,,Missing value treatment,<missing-data><data-imputation>,CC BY-SA 3.0,"<p>I have a data set with 18% of AGE variable missing which is an important variable for analysis. </p>

<ol>
<li><p>Should I try regression imputation or should I drop those observations? </p></li>
<li><p>Does even regression imputation make sense in this case (for age)??</p></li>
<li><p>I also have income variable but the correlation between age and income is negative and strength is .1 </p></li>
</ol>

<p>What should I do?</p>
",2013-10-22 20:14:21.513
58052,594.0,2,,57463.0,,,,CC BY-SA 3.0,"<p>While it's possible to do it recursively for fixed degrees of freedom (write the cdf for a given d.f. in terms of the cdf for lower degrees of freedom, and the integrals foir the two lowest-integer df may be done directly), I've never seen anyone try to implement it that way.</p>

<p>Some algorithms for the cdf of the $t$ are based on the <em>incomplete beta function</em> (which is a commonly used function in various parts of mathematics or physics). </p>

<p>There are some for the inverse cdf (quantile function) based on ratios of polynomials.</p>

<p>Plain googling on <em>algorithm cdf|""distribution function"" student t</em> turns up plenty of references within the pages linked (e.g. <a href=""http://devdoc.madlib.net/v0.2beta/student_8cpp.html"" rel=""nofollow"">here</a>), such as Abramowitz and Stegun's <em>Handbook of Mathematical Functions</em> (which gives some small-d.f.-exact and approximate calculations), and various other books and papers. </p>

<p>If you want the noncentral t (e.g. for power calculations) a standard reference is Lenth, R. V. 1989. ""Algorithm AS 243: Cumulative distribution function of the noncentral t distribution"". <em>Applied Statistics</em>, <strong>38</strong>, 185-189.</p>

<p>However, if you're doing many of these, hypothesis tests may not suit your purposes. Something more like a measure of effect size might be better.</p>
",2013-10-22 20:23:11.970
58053,13037.0,1,58058.0,,,Simulating new x's in regression simulation study,<self-study><least-squares><simulation><ridge-regression>,CC BY-SA 3.0,"<p>One of my homework problems is a simulation that compares three estimators (least squares, ridge regression with known parameters, and ridge regression with estimated parameters) for the following model $$Y_i = \beta X_i + \epsilon_i,\quad \epsilon_i\sim N(0,\sigma^2)$$</p>

<p>I am supposed to do 1000 replications with $X_i\sim N(0,2)$. Initially I generated my $X$ vector of data and used the same $X$ vector for each of the 1000 repetitions (so only thing different between repetitions is what random error gets added on). </p>

<p>Then I thought that might be wrong and that I should generate new $X$ data between each repetition. </p>

<p>What is the correct thing to do? </p>

<p>I can provide code if need be, but not really necessary to answer my question. </p>
",2013-10-22 20:28:41.463
58054,4320.0,2,,58041.0,,,,CC BY-SA 3.0,"<p>Ignoring the terms where this happens is the correct thing to do. You can justify this by noting that in each case you've outlined, no matter what happens inside the $\log$ you will have $P(x,y,z) = 0$. You can see this by applying the <a href=""http://en.wikipedia.org/wiki/Frechet_inequalities"" rel=""nofollow"">Frechet inequalities</a>, namely that $P(A,B) \le \min\{P(A), P(B)\}$.</p>
",2013-10-22 20:35:35.647
58055,22507.0,2,,57965.0,,,,CC BY-SA 3.0,"<p>You may use the recurrent formula:</p>

<p>$p(D+1,C) = p(D,C){N-C \over N} + p(D,C+1){C+1 \over N}$</p>

<p>$p(0,C) = \cases{1,&amp;if C=N\\0,&amp;otherwise}$</p>
",2013-10-22 20:45:17.047
58087,2666.0,2,,58067.0,,,,CC BY-SA 3.0,"<p>Simpler than BIR is the logarithmic or quadratic (Brier) scoring rules.  These are proper scores that, unlike the proportion classified correctly, will not give rise to a bogus model upon optimization. </p>
",2013-10-23 13:09:47.363
58056,16464.0,1,,,,How to think about iid observations,<distributions><iid>,CC BY-SA 3.0,"<p>I'm trying to make sure my understanding of iid observations is rock solid and so I have two write-ups in which I've attempted to accurately explain the concept. Can anyone comment on the accuracy of each of these write-ups?</p>

<pre><code>(1) One way to represent a population is through a random variable X, which would have
a pdf/pmf that modeled the relative frequencies of values with the population. If you
were to sample from the population such that each subject is equally likely to be
selected and so that selecting a particular subject has no bearing on the
likelihood of any other subject being selected, then each observation can be thought of
as a realization of the random variable X. As such, we denote the observations x_i.
Under these conditions the observations can be thought of as independent draws from the
random variable X, and so the observations are said to be iid (independently and
identically distributed) â€“ i.e. the observations can be thought of as n realizations of
the random variable X.
</code></pre>

<p>Or</p>

<pre><code>(2) One way to represent a population is through a random variable X, which would have
a pdf/pmf that modeled the relative frequencies of values within the population. If you
were to sample from the population such that each subject is equally likely to be
selected and so that selecting a particular subject has no bearing on the
likelihood of any other subject being selected, then each observation can be thought of
as a realization of the random variable X. As such, we denote the observations x_i.
Under these conditions the observations can be thought of as independent draws from
identically distributed random variables, X_1,â€¦,X_n, where x_i is the realization of
the random variable X_i for all i. Hence, the observations are said to be iid
(independently and identically distributed).
</code></pre>

<p>There are a couple of portions that I want to get answers about, specifically.</p>

<ul>
<li>Could the statement ""such that each subject is equally likely to be selected and so that"" in the second sentence of each write up be removed without affecting the accuracy of the statement?</li>
<li>Are both of these ways of thinking about iid draws correct?</li>
<li>If both statements are correct, is it conventional to interpret the draws as being realizations from one RV, or each observation as a realization from separate but identical RV's?</li>
</ul>
",2013-10-22 20:49:49.513
58057,503.0,2,,58051.0,,,,CC BY-SA 3.0,"<p>In any missing data situation the first thing to ask is why the data are missing.  There are three types;</p>

<p>Missing completely at random (MCAR) - this means that the missing data are a totally random set of the data. This rarely happens unless there is some sort of mechanical glitch</p>

<p>Missing at random (MAR) - this means that the missing data could be a non-random subset of the data, but that the non-randomness can be completely explained by variables that are in the data.</p>

<p>Not missing at random aka nonignorable non-response (NMAR) - neither of the first two.</p>

<p>If it's MCAR, then the only thing lost by deleting the data is statistical power.  If MAR, then the usual approach is multiple imputation, to account for the variance in single imputation regression imputation</p>

<p>If the data are NMAR then, technically, nothing will really work. However, multiple imputation may still be a good choice. Joe Schafer said (informally; he gave a talk at my old workplace and this was in the Q and A) that MI works well unless the data are ""really NMAR"". </p>
",2013-10-22 21:15:00.103
58058,7189.0,2,,58053.0,,,,CC BY-SA 3.0,"<p>I think it makes sense to change X between iterations. But within an iteration you should have the same X to compare the three estimators. </p>

<p>You want to compare the three estimators in general. If you keep X constant, it could happen that an estimator that is not better in general performs better, because that particular X you chose turned out to have some special property.</p>
",2013-10-22 21:33:39.603
58059,8671.0,1,68114.0,,,boosting with linear svm,<machine-learning><svm><boosting><libsvm>,CC BY-SA 3.0,"<p>I am working on boosting classifier. I am planning to use linear svm as the weak classifier. I am using liblinear for it.</p>

<p>My question is how can I weight each instance of liblinear based on the boosting weights?</p>
",2013-10-22 22:07:51.183
58060,4656.0,2,,58037.0,,,,CC BY-SA 3.0,"<p>For <em>any</em> $N$ numbers $y_1,y_2, \ldots, y_N$ with mean 
$\displaystyle \bar{y} = \frac{1}{N}\sum_{i=1}^N y_i$, the variance is given by
$$\begin{align}
\sigma^2 &amp;= \frac{1}{N-1}\sum_{i=1}^N (y_i-\bar{y})^2\\ 
&amp;= \frac{1}{N-1}\sum_{i=1}^N \left(y_i^2 - 2y_i\bar{y} + \bar{y}^2\right)\\
&amp;= \frac{1}{N-1}\left[\left(\sum_{i=1}^Ny_i^2\right) - 2N(\bar{y})^2
+ N(\bar{y})^2 \right] \\
\sigma^2 &amp;=\frac{1}{N-1}\sum_{i=1}^N \left(y_i^2 - (\bar{y})^2\right) \tag{1}
\end{align}$$
Applying $(1)$ to the given set of $n$ numbers $x_1, x_2, \ldots x_n$
which we take for convenience in exposition to have mean $\bar{x} = 0$,
we have that
$$\sigma^2 = \frac{1}{n-1}\sum_{i=1}^n \left(x_i^2-(\bar{x})^2\right)
= \frac{1}{n-1}\sum_{i=1}^n x_i^2$$
If we now add in a new observation $x_{n+1}$ to this data set, then the new mean of
the data set is 
$$\frac{1}{n+1}\sum_{i=1}^{n+1}x_i 
= \frac{n\bar{x} + x_{n+1}}{n+1} = \frac{x_{n+1}}{n+1}$$ 
while the new variance is
$$\begin{align}
\hat{\sigma}^2 &amp;= \frac{1}{n}\sum_{i=1}^{n+1} \left(x_i^2-\frac{x_{n+1}^2}{(n+1)^2}\right)\\ 
&amp;= \frac{1}{n}\left[\left((n-1)\sigma^2 + x_{n+1}^2\right)
- \frac{x_{n+1}^2}{n+1}\right]\\
&amp;= \left.\left.\frac{1}{n}\right[(n-1)\sigma^2 + \frac{n}{n+1}x_{n+1}^2\right]\\
&amp;&gt; \sigma^2 ~ \text{only if}~ x_{n+1}^2 &gt; \frac{n+1}{n}\sigma^2.
\end{align}$$
So $|x_{n+1}|$ needs to be larger than $\displaystyle\sigma\sqrt{1+\frac{1}{n}}$
or, more generally, $x_{n+1}$ needs to
differ from the mean $\bar{x}$ of the original data
set by more than $\displaystyle\sigma\sqrt{1+\frac{1}{n}}$, in order for
the augmented data set to have larger variance than the original data set.
See also Ray Koopman's answer which points out that the new variance is larger
than, equal to, or smaller than, the original variance according as $x_{n+1}$
differs from the mean by more than, exactly, or less than $\displaystyle\sigma\sqrt{1+\frac{1}{n}}$.</p>
",2013-10-22 22:16:54.387
58061,22949.0,1,,,,Difference between Gaussian Distribution and Cauchy Distribution,<machine-learning><distributions><normal-distribution><cauchy-distribution>,CC BY-SA 3.0,"<p>I have searched for the above topic but did not have an answer. Can someone please tell me the detailed difference between the multivariate Gaussian Distribution and multivariate Cauchy Distribution? How do they differ in sampling is Estimation of Distribution algorithm for optimisation.</p>
",2013-10-22 22:37:54.810
58062,22372.0,1,,,,How can find the slope of a GLM logit model?,<r><logit><model>,CC BY-SA 3.0,"<p>I've created a logit model using GLM and IÂ´d like to get the slope of the independent variables.</p>

<p>I've read that using <code>coef()</code> in R itÂ´s possible to do it, but the only thing I have back is the coefficients that I already have using <code>summary()</code>.</p>

<p>I'm trying to get the slope at mean, like the one that is given when we use Gretl.</p>
",2013-10-22 22:41:03.607
58063,23157.0,1,58473.0,,Eric,Gradient and Hessian of a likelihood function where y is defined implicitly,<maximum-likelihood><optimization>,CC BY-SA 3.0,"<p>$\newcommand{\implicit}{\mathrm{implicit}}$</p>

<p>I'm attempting to optimize model parameters $\theta$ by maximizing the likelihood function
$$
f(y)  = \ln \Bigl(\frac {n!}{k!(n-k)!}y^k(1-y)^{n-k}\Bigl) $$
where $y$ must be calculated iteratively, as it is defined implicitly by $g(\theta)=0$, as follows:
$$
g(\theta) = -1 + \frac{a\cdot \theta_2}{\ln \bigl(\frac{-y}{y - 1} \bigl) - \theta_1} + \frac{b\cdot \theta_4}{\ln \bigl(\frac{-y}{y - 1} \bigl) - \theta_3}
$$</p>

<p>($n$, $k$, $a$, and $b$ are constants).</p>

<p>As far as I can tell, using partial implicit differentiation, the gradient (first partial derivative with respect to $\theta_i$) would be defined by $$
\frac{\partial \operatorname{f}}{\partial \operatorname{\theta_i}} = \frac{\frac{\partial \operatorname{f}}{\partial \operatorname{y}}\cdot \frac{-\partial \operatorname{g}}{\partial \operatorname{\theta_i}}}{\frac{\partial \operatorname{g}}{\partial \operatorname{y}}}
$$
This seems to match numerical approximations computed by R, but when I try to calculate the Hessian by taking the partial derivative of the gradient with respect to $\theta_i$, I get strange results that are nowhere close to numerical approximations.</p>

<p>This is the formula I came up as my attempt at calculating the Hessian (second-order mixed partial derivative of $f$ with respect to $\theta_i$):</p>

<p>$$
\frac{\partial}{\partial \operatorname{\theta_i}} \Bigl(\frac{\partial \operatorname{f}}{\partial \operatorname{\theta_j}}\Bigl) = \frac{\frac{\partial}{\partial \operatorname{\theta_i}} \Bigl(\frac{\partial \operatorname{f}}{\partial \operatorname{y}}\Bigl) \cdot \frac{-\partial \operatorname{g}}{\partial \operatorname{\theta_i}}}{\frac{\partial \operatorname{g}}{\partial \operatorname{y}}}+
\frac{\frac{\partial \operatorname{f}}{\partial \operatorname{y}} \cdot \frac{\partial}{\partial \operatorname{\theta_i}} \Bigl(\frac{-\partial \operatorname{g}}{\partial \operatorname{\theta_i}}\Bigl)}{\frac{\partial \operatorname{g}}{\partial \operatorname{y}}}+
\frac{\frac{\partial \operatorname{f}}{\partial \operatorname{y}} \cdot  \frac{-\partial \operatorname{g}}{\partial \operatorname{\theta_i}}\cdot\frac{\partial}{\partial \operatorname{\theta_i}}\Bigl(\frac{\partial \operatorname{g}}{\partial \operatorname{y}}\Bigl)}{\Bigl(\frac{-\partial \operatorname{g}}{\partial \operatorname{y}}\Bigl)^2}
$$</p>

<p>Considering that the values I get from these calculations don't match (not even close) the automatically generated numerical approximations returned by R, I suspect that I've done something wrong here. Can anyone spot an error with the Hessian formula? Is there a property of higher-order mixed partial implicit differentiation that requires a different approach?</p>

<p>Thanks!</p>

<p>UPDATE: Alecos Papadopoulos posted a solution that eliminates the need for iteration by solving directly for $g(\theta)$, thus providing an exact value for $y$. This works perfectly for this problem, as the calculation of the gradient and Hessian does not require implicit differentiation in this case!</p>

<p>For proof of concept (and in case I come across any $g(\theta)$ functions that can't be solved directly), I'm still interested in figuring out what went wrong with my attempt at the Hessian. If anyone has any insight into a general solution for the Hessian using implicit differentiation, it is certainly welcome.</p>
",2013-10-23 00:00:39.590
58064,20667.0,1,58066.0,,,Why do zero differences not enter computation in the Wilcoxon signed ranked test?,<hypothesis-testing><wilcoxon-signed-rank><paired-comparisons>,CC BY-SA 3.0,"<p>The <strong>Wilcoxon signed ranked test</strong> tells us if the median difference between paired data can be zero. The test is executed by computing a statistic, then a z-score and comparing it to a critical value. </p>

<p>The thing that I find shocking is that we </p>

<p><strong>discard all the pairs with same values from the process of computing the statistic</strong>. </p>

<p>From <a href=""http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test#Test_procedure"">Wikipedia</a> we have in step2:</p>

<blockquote>
  <p>Exclude pairs with $|x_{2,i} - x_{1,i}| = 0$. Let $N_r$ be the reduced
  sample size.</p>
</blockquote>

<p>And only $N_r$ is used in the rest of the computation.</p>

<p>One of the <a href=""http://vassarstats.net/textbook/ch12a.html"">sources</a> cited says:</p>

<blockquote>
  <p>In most applications of the Wilcoxon procedure, the cases in which
  there is zero difference between $X_A$ and $X_B$ are at this point
  eliminated from consideration, since they provide no useful
  information, and the remaining absolute differences are then ranked
  from lowest to highest, with tied ranks included where appropriate.</p>
</blockquote>

<p>The author then proceeds to compute in the same manner as in the Wikipedia article.</p>

<p>I tried to look at the original <a href=""http://sci2s.ugr.es/keel/pdf/algorithm/articulo/wilcoxon1945.pdf"">Wilcoxon's article</a>, but he does not seem to mention same value pairs.</p>

<p>The reason why I think this is madness is:</p>

<p>Ok, same value pairs do not change the value of the statistic, but they <strong>change the z-score</strong>. Imagine having a sample of $10^{1000}$ pairs while in $10$ pairs, the second value is higher and in all the remaining pairs, the values are the same. According to the above mentioned articles, we should discard these $10^{1000}-10$ pairs since they ""provide no useful information"" and consider only the remaining $10$ pairs. But those $10^{1000} - 10$ pairs <strong>do provide useful information</strong>. They scream in favor of the null hypothesis.</p>

<p>Please, could you explain how to do the test right?</p>
",2013-10-23 00:35:45.353
58065,3993.0,2,,51644.0,,,,CC BY-SA 3.0,"<p><em>""I have always been taught that random effects only influence the variance (error), and that fixed effects only influence the mean.""</em></p>

<p>As you have discovered, this is only true for balanced, complete (i.e., no missing data) datasets with no continuous predictors. In other words, for the kinds of data/models discussed in classical ANOVA texts. Under these ideal circumstances, the fixed effects and random effects can be estimated independent of one another.</p>

<p>When these conditions do not hold (as they very very often do not in the ""real world""), the fixed and random effects are not independent. As an interesting aside, this is why ""modern"" mixed models are estimated using iterative optimization methods, rather than being exactly solved with a bit of matrix algebra as in the classical mixed ANOVA case: in order to estimate the fixed effects, we have to know the random effects, but in order to estimate the random effects, we have to know the fixed effects! More relevant to the present question, this also means that when data are unbalanced/incomplete and/or there are continuous predictors in the model, then adjusting the random-effects structure of the mixed model can alter the estimates of the fixed part of the model, and vice versa. </p>

<p>Edit 2016-07-05. From the comments: ""<em>Could you elaborate or provide a citation on why continuous predictors will influence the estimates of the fixed part of the model?</em>""</p>

<p>The estimates for the fixed part of the model will depend on the estimates for the random part of the model -- that is, the estimated variance components -- if (but not only if) the variance of the <em>predictors</em> differs across clusters. Which will almost certainly be true if any of the predictors are continuous (at least in ""real world"" data -- in theory it would be possible for this to not be true, e.g. in a constructed dataset).</p>
",2013-10-23 00:51:08.400
58066,594.0,2,,58064.0,,,,CC BY-SA 3.0,"<p>It has to do with the assumptions of the test for which the distribution of the test statistic under the null is derived. </p>

<p>The variables are assumed to be continuous. </p>

<p>The probability of a tie is therefore 0 ... and this makes it possible to compute the permutation distribution of the test statistic under the null for given sample size. </p>

<p>Without that assumption being true, you could still do a test, but if you're going to get the null distribution of the test statistic, you'll have to try to compute it conditional on the pattern of tied values (or more easily, simulate). </p>

<p>The easier alternative is to only consider untied values.</p>

<p>Note further that observing ties is not 'evidence in favor of the null', it only contains a lack of evidence against it. With discrete distributions, a range of non-null alternatives are likely to produce ties, not just the null itself.</p>

<p>The 'correct' thing to do is not use a test that assumes continuous distributions on data that don't satisfy the assumptions. If you don't have that, you have to do something to deal with that failure.</p>

<p>I believe that conditioning on the untied data preserves the required properties for the significance level in a way that including ties in some way would not. We might check by simulation.</p>
",2013-10-23 01:06:41.770
58067,22950.0,1,58077.0,,,"Choosing a classification performance metric for model selection, feature selection, and publication",<svm><cross-validation><model-selection><auc>,CC BY-SA 3.0,"<p>I have a small, unbalanced data set (70 positive, 30 negative), and I have been playing around with model selection for SVM parameters using BAC (balanced accuracy) and AUC (area under the curve). I used different class-weights for the C parameter in libSVM to offset the unbalanced data following the advice here (<a href=""https://stats.stackexchange.com/questions/28029/training-a-decision-tree-against-unbalanced-data"">Training a decision tree against unbalanced data</a>).</p>

<ol>
<li><p>It seems that k-fold cross-validation error is very sensitive to the type of performance measure. It also has an error in itself because the training and validation sets are chosen randomly. For example, if I repeat BAC twice with different random seeds, I will get different errors, and subsequently different optimal parameter values. If I average repeated BAC scores, averaging 1000 times will give me different optimal parameter values than averaging 10000 times. Moreover, changing the number of folds gives me different optimal parameter values.</p></li>
<li><p>Accuracy metrics for cross validation may be overly optimistic. Usually anything over a 2-fold cross-validation gives me 100% accuracy. Also, the error rate is discretized due to small sample size. Model selection will often give me the same error rate across all or most parameter values.</p></li>
<li><p>When writing a report, how would I know that a classification is 'good' or 'acceptable'? In the field, it seems like we don't have something like a goodness of fit or p-value threshold that is commonly accepted. Since I am adding to the data iteratively, I would like to know when to stop- what is a good N where the model does not significantly improve? </p></li>
</ol>

<p>Given the issues described above, it seems like accuracy can't be easily compared between publications while AUC has been described as a poor indicator for performance (see <a href=""http://arxiv.org/abs/1202.2564"" rel=""noreferrer"">here</a>, or <a href=""http://cs.unm.edu/~terran/academic_blog/?p=88"" rel=""noreferrer"">here</a>, for example).  </p>

<p>Any advice on how to tackle any of these 3 problems? </p>
",2013-10-23 02:27:10.847
58068,22953.0,1,58079.0,,,Interpreting a negative confidence interval,<confidence-interval>,CC BY-SA 3.0,"<p>How do I interpret a negative confidence interval when comparing two population means?</p>

<p>For example, a confidence interval is $(-23.11, -1.02)$, what is the significance of these values being negative? Is it strictly signifying that $\bar{x}_1 &lt; \bar{x}_2$ ?</p>
",2013-10-23 02:48:42.997
58069,19750.0,1,69580.0,,,What is the relationship between the mean squared error and the residual sum of squares function?,<residuals><mse>,CC BY-SA 4.0,"<p>Looking at the Wikipedia definitions of:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Mean_squared_error"" rel=""noreferrer"">Mean Squared Error</a> (MSE)</li>
<li><a href=""http://en.wikipedia.org/wiki/Residual_sum_of_squares"" rel=""noreferrer"">Residual Sum of Squares</a> (RSS)</li>
</ul>

<p>It looks to me that </p>

<p><span class=""math-container"">$$\text{MSE} = \frac{1}{N} \text{RSS} = \frac{1}{N} \sum (f_i -y_i)^2$$</span></p>

<p>where <span class=""math-container"">$N$</span> is he number of samples and <span class=""math-container"">$f_i$</span> is our estimation of <span class=""math-container"">$y_i$</span>.</p>

<p>However, none of the Wikipedia articles mention this relationship. Why? Am I missing something?</p>
",2013-10-23 02:55:28.270
58070,22955.0,1,58666.0,,,Multiple Linear Regression Models Comparison Based on R-squared And Residual Errors,<multiple-regression><model-selection><r-squared><model-comparison>,CC BY-SA 3.0,"<p>am currently working on a problem where I have to calibrate weather parameters at a ground location using Satellite data available (1979-2012) over rectangular grid points and surface observatory data for a location (1980-2000). I have built two models - One for January months over 1980-2000 period and other taking data from November to April over 1980-2000.</p>

<p>Now the problem arises when I compare adjusted R-squared for both the models. Nov-Apr model gives me higher R-squared (0.7 vs. 0.2) whereas residual errors tell me the opposite story, giving poor results in the case of Nov-Apr model.</p>

<p>One option would be to calculate different components of R-squared and see if it can explain the differences. So how do I calculate and explain these components in R? or is there any better and efficient way?</p>

<p>Thanks in advance.</p>
",2013-10-23 04:37:43.580
58071,15766.0,1,58074.0,,,Basic binomial question,<binomial-distribution><poisson-distribution><proportion>,CC BY-SA 3.0,"<p>The following appeared on an assignment of mine (already turned in). I contend that not enough information is given to provide an answer.... it seems pretty cut and clear to me. However, instructor insisted it's solvable in minitab. Can you help me figure out what I'm not understanding?  </p>

<p>How do you solve this without a model of distribution of weekly demand, or at least an average value to use as constant approximation. I must be missing something simple.  </p>

<p>The problem:  </p>

<p>Consider a service company.  </p>

<p>10% of the weekly demand is for a service category named ""X"" [Assume service categories are mutually exclusive].  </p>

<p>The company must revise their resource plan iff there are too few customer orders(less than one/week) or too many customer orders (more than five/week) of service category ""X"".  </p>

<p>For the next 12 weeks, what is the probability that the company will <strong>not</strong> need to revise the resource plan?  </p>

<p>Thanks</p>
",2013-10-23 05:34:47.950
58072,22956.0,1,,,,Probability Calculation,<probability><self-study>,CC BY-SA 3.0,"<p>Assume that a typical computer manufactured by a company lasts 10 Months and that the standard deviation is 50 days. Computer life follows a normal distribution. What is the probability that a computer made by this company will last at most 1 Year?</p>

<p>Assumption is that one month has 30 days.</p>

<p>Can you please explain how this is calculated ?</p>

<p>Thanks</p>
",2013-10-23 05:54:28.690
58073,15766.0,2,,58072.0,,,,CC BY-SA 3.0,"<p>Normal distribution has 2 parameters: mean and variance (or standard deviation which is the square root of variance).  </p>

<p>Mean=10months*30days/month= 300 days  </p>

<p>Therefore, lifetime, T,  is distributed as Normal(mean=300 days, std. dev=50 days).  </p>

<p>You want to find P(T&lt; 365 days)<br>
Calculate the z-score for T=365--> z = (365-300)/50=1.3<br>
And use a table or software to find the appropriate cumulative (""lower tail"") probability corresponding to z=1.3</p>
",2013-10-23 06:12:59.573
58074,7949.0,2,,58071.0,,,,CC BY-SA 3.0,"<p>This depends on the total number of customer orders; consider the situation if you have just one order per week. Then you are almost certainly have less than one in any given week. OTH if you have 1000 customers, you will have about 100 ordering ""X"" each week which is too much.</p>

<p>It's also not clear if the 10% is an average or a fixed number; the same is the case for the missing number of orders per week. The most likely way to interpret this question is to assume that each customer order has a chance of 10% of belonging to category ""X"" - but then we will still need the number of customer orders. If the number of orders is fixed then X the number of orders of ""X"" per week would be binomially distributed and the question would be solvable.</p>

<p>I think it is really questionable to claim that it is ""solvable in Minitab"" and give no theoretical background. There may be a button in Minisab that takes these numbers and gives an answer, but is it the answer to the question as it is stated here?  </p>

<p>Short version, I agree with you.</p>
",2013-10-23 06:26:43.607
58075,20470.0,2,,58067.0,,,,CC BY-SA 3.0,"<p>As you point out, predictive accuracy and AUC are limited in certain aspects. I would give the <a href=""http://www.csse.monash.edu.au/~korb/pubs/ai02.pdf"" rel=""nofollow noreferrer"">Bayesian Information Reward</a> (BIR) a go, which should give a more sensitive assessment of how well or badly your classifier is doing and how that changes as you tweak your parameters (number of validation folds, etc.).</p>

<p>The intuition of BIR is as follows: a bettor is rewarded not just for identifying the ultimate winners and losers (0's and 1's), but more importantly for identifying the appropriate odds. Furthermore, it goes a step ahead and compares all predictions with the prior probabilities. </p>

<p>Let's say you have a list of 10 Arsenal (football team in England) games with possible outcomes: $Win$ or $Lose$. The formula for binary classification rewarding per game is:</p>

<p><img src=""https://i.stack.imgur.com/gI3No.png"" alt=""enter image description here""></p>

<p>where, $p$ is your model's prediction for a particular Arsenal game, and $p'$ is the prior probability of Arsenal winning a game. The catch-point is: if I know beforehand that $p'=0.6$, and my predictor model produced $p =0.6$,even if its prediction was correct it is rewarded 0 since it is not conveying any new information. As a note, you treat the correct and incorrect classifications differently as shown in the equations. As a result, based on whether the prediction is correct or incorrect, the BIR for a single prediction can take a value between $(-inf, 1]$.</p>

<p>BIR is not limited to binary classifications but is generalised for multinomial classification problems as well. </p>
",2013-10-23 07:47:35.447
58076,22960.0,2,,51895.0,,,,CC BY-SA 3.0,"<p>I have encountered a similar problem, and I solved it by transferring the class values (""status"" in your case) into factor type. After using <code>data$status=factor(data$status)</code>, <code>newData</code> prints as follows:</p>

<pre><code>     looking risk every status
7          0    0     0      1
2          0    0     0      1
7.1        0    0     0      1
12         0    0     0      1
4          0    0     0      1
12.1       0    0     0      1
11         0    0     0      3
8         NA   NA    NA      3
9         NA   NA    NA      3
10        NA   NA    NA      3
111       NA   NA    NA      3
121       NA   NA    NA      3
13        NA   NA    NA      3
</code></pre>

<p>No errors!</p>
",2013-10-23 08:25:37.997
58077,2958.0,2,,58067.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>It seems that k-fold cross-validation error is very sensitive to the type of performance measure. It also has an error in itself because the training and validation sets are chosen randomly. </p>
</blockquote>

<p>I think you've discovered the high variance of performance measures that are proportions of case counts such as $\frac{\text{# correct predictions}}{\text{# test cases}}$. You try to estimate e.g. the probability that your classifier returns a correct answer. From a statistics point of view, that is described as a Bernoulli trial, leading to a binomial distribution. You can calculate confidence intervals for binomial distributions and will find that they are very wide. This of course limits your ability to do model comparison.</p>

<p>With resampling validation schemes such as cross validation you have an additional source of variation: the instability of your models (as you build $k$ surrogate models during each CV run)</p>

<blockquote>
  <p>Moreover, changing the number of folds gives me different optimal parameter values.</p>
</blockquote>

<p>That is to be expected due to the variance. You may have an additional effect here: libSVM splits the data only once if you use their built-in cross validation for tuning. Due to the nature of SVMs, if you built the SVM with identical training data and slowly vary the parameters, you'll find that support vectors (and consequently accuracy) jumps: as long as the SVM parameters are not too different, it will still choose the same support vectors. Only when the paraters are changed enough, suddenly different support vectors will result. So evaluating the SVM parameter grid with exactly the same cross validation splits may hide variability, which you see between different runs.</p>

<p>IMHO the basic problem is that you do a grid search, which is an optimization that relies on a reasonably smooth behaviour of your target functional (accuracy or whatever else you use). Due to the high variance of your performance measurements, this assumption is violated. The ""jumpy"" dependence of the SVM model also violates this assumption. </p>

<blockquote>
  <p>Accuracy metrics for cross validation may be overly optimistic. Usually anything over a 2-fold cross-validation gives me 100% accuracy. Also, the error rate is discretized due to small sample size. Model selection will often give me the same error rate across all or most parameter values.</p>
</blockquote>

<p>That is to be expected given the general problems of the approach.</p>

<p>However, usually it is possible to choose really extreme parameter values where the classifier breaks down. IMHO the parameter ranges where the SVMs work well is important information. </p>

<p>In any case you absolutely need an external (double/nested) validation of the performance of the model you choose as 'best'. </p>

<p>I'd probably do a number of runs/repetitions/iterations of an outer cross validation or an outer out-of-bootstrap validation and give the distribution of </p>

<ul>
<li>hyperparameters for the ""best"" model</li>
<li>reported performance of the tuning</li>
<li>observed performance of outer validation</li>
</ul>

<p>The difference between the last two is an indicator of overfitting (e.g. due to ""skimming"" the variance).</p>

<blockquote>
  <p>When writing a report, how would I know that a classification is 'good' or 'acceptable'? In the field, it seems like we don't have something like a goodness of fit or p-value threshold that is commonly accepted. Since I am adding to the data iteratively, I would like to know when to stop- what is a good N where the model does not significantly improve?</p>
</blockquote>

<p>(What are you adding? Cases or variates/features?)</p>

<p>First of all, if you do an iterative modeling, you either need to report that due to your fitting procedure your performance is not to be taken seriously as it is subject to an optimistic bias. The better alternative is to do a validation of the final model. However, the test data of that must be independent of all data that ever went into training or your decision process for the modeling (so you may not have any such data left).</p>
",2013-10-23 08:35:20.010
58096,803.0,2,,58091.0,,,,CC BY-SA 3.0,"<p>There's a few things here.</p>

<ol>
<li>For binomial data, the variance is directly determined by the mean, and isn't an additional parameter, so there's no need to do a t-test... a normal z-test is slightly more efficient.</li>
<li>For binomial data, the Normal approximation (i.e. a Wald test) often fails.  See Agresti and Coull, 1998, for some more detailed discussion and simulation studies.  <a href=""http://www.stat.ufl.edu/~aa/articles/agresti_coull_1998.pdf"" rel=""nofollow"">http://www.stat.ufl.edu/~aa/articles/agresti_coull_1998.pdf</a></li>
</ol>

<p>They give some recommendations about when it's okay to use or not use the normality assumption (as do others)... generally the closer you get to p=.5, and the larger your data set, the better it is, the further away from .5 you get (towards p=0 or p=1), or the smaller the data, it's worse.</p>

<p>But the Wilcoxon sign rank test is popular for this kind of data.</p>
",2013-10-23 14:40:40.563
58078,22961.0,1,,,,"Quantitative ,longitudinal, observational study:Population, sample and instrument",<sample><population>,CC BY-SA 4.0,"<p>I am studying US public company financial performance for the last 2 decades. I am searching for evidence of ""sustainable superior performance"", thus to isolate those companies that display such characteristics. I will use the Compustat database.</p>

<p>Questions:</p>

<ol>
<li>Is the total population the universe of US public company with
their past, present and future performance <strong>or</strong> the population is
the subset of those displaying superior performance (subject of the
study)? </li>
<li>Because all the companies will be observed to spot those
with certain characteristics (i.e. superior performance), what is
the sample? Can I consider the company present in the Compustat
database (independently if they have the traits I am seeking or not)
to be the sample?</li>
<li>What is the Instrument I need to ensure validity and reliability?
-The Compustat database with its screening capabilities?
-The criteria I will use to assess ""sustained, superior performance""? (like sales growth above industry average and so on)</li>
<li>The statistical technique used to find such outlier? (like Cluster Analysis for instance)  </li>
</ol>
",2013-10-23 08:39:29.317
58079,22959.0,2,,58068.0,,,,CC BY-SA 3.0,"<p>You should write your hypothesis first :)</p>

<p>But I guess your hyp was x_1 >= x_2 ? Then we can say that we are (1-alfa)% confident that the difference between the true mean of x_1 and x_2 is between (âˆ’23.11,âˆ’1.02) </p>
",2013-10-23 08:57:00.940
58080,20470.0,1,,,,What is an intuitive explanation of the Bayesian score?,<bayesian-network><scoring-rules>,CC BY-SA 3.0,"<p>I am familiar with the <a href=""http://kdbio.inesc-id.pt/~asmc/pub/talks/09-TA/ta_pres.pdf"" rel=""nofollow"">Bayesian score</a>, which is used to compare competing structures of a BN. However, I have difficulty in understanding how the Bayesian score formula below is derived.</p>

<p><strong>1)</strong> What is an intuitive explanation of the Bayesian score? (e.g. how do the fractions within the product operators equate to $P(D|B_S)$)</p>

<p><strong>2)</strong> What type of structures does it favour? (e.g. more/less connected, higher/lower number of parents, etc.)</p>

<p><strong>Background:</strong> The Bayesian score of a BN structure, $B_S$, for a data set, $D$, is used to calculate the joint probability of $P(B_S,D)$. For a discrete network (after some assumptions*),  it is given as:</p>

<p>$$P(B_S,D) = P(B_S)\prod_{i=0}^n\prod_{j=1}^{qi} \frac{\Gamma(N'_{ij})}{\Gamma(N'_{ij} + N_{ij})} \prod_{k=1}^{ri} \frac{\Gamma(N'_{ijk} + N_{ijk})}{\Gamma(N'_{ijk})} $$ </p>

<ul>
<li><p>$\Gamma()$ is the gamma function. Above, since all counts are positive integers, $\Gamma(int) = (int-1)!$</p></li>
<li><p>$P(B_S)$ is the prior on the network structure ( taken to be uniform/uninformative by many versions of the Bayesian score)</p></li>
<li><p>$xi$ is a node in the network with $n$ nodes; </p></li>
<li><p>$ri (1 â‰¤ i â‰¤ n)$ is the cardinality of $xi$; </p></li>
<li><p>$qi$ denotes the cardinality of the parent set of $xi$ , that is, the number of different values to which the parents of xi, $pa(xi)$, can be instantiated.</p></li>
<li><p>$N_{ij} (1 â‰¤ i â‰¤ n, 1 â‰¤ j â‰¤ qi)$ denotes the number of records in $D$ for which $pa(xi)$ takes its $j$th value. </p></li>
<li><p>$N_{ijk} (1 â‰¤ i â‰¤ n, 1 â‰¤ j â‰¤ qi, 1 â‰¤ k â‰¤ ri)$ denotes the number of records in $D$ for which $pa(xi)$ takes its $j$th value and for which $xi$ takes its $k$th value. So, $N_{ij} = \sum_{k=1}^{ri} N_{ijk}$.</p></li>
<li><p>$Nâ€²_{ij}$ and $Nâ€²_{ijk}$ represent choices of priors on counts restricted by $N'_{ij} = \sum_{k=1}^{ri} N'_{ijk}$.</p></li>
</ul>

<p>*List of asssumptions: 1 - Multinomial Sample; 2- Parameter Independence; 3- Parameter Modularity; 4 - Dirichlet; 5- Complete Data</p>
",2013-10-23 10:42:29.047
58081,11359.0,1,58115.0,,,Analyzing multiply imupted data from Amelia in R: Why do results from zelig and mice differ?,<r><multiple-imputation><mice>,CC BY-SA 3.0,"<p>Motivated by my answer to <a href=""https://stats.stackexchange.com/questions/69130/how-to-get-pooled-p-values-on-tests-done-in-multiple-imputed-datasets"">this question</a>, I played around with analyzing mulitply imputed data from the <code>Amelia</code> package in R. As I have explained in my answer, the multiply imputed datasets can be analyzed using the combined <code>Zelig</code> and <code>mitools</code> packages or using a combination of <code>Zelig</code> and <code>mice</code>.</p>

<p>Now, to me it seemed rather inconvenient to fit the linear model using <code>zelig()</code> as <code>mice</code>, too, provides the <code>with.mids()</code>-function to fit linear models to multiply imputed datasets. However, I found that the results differ depending on the function used for fitting. For the analysis using <code>with.mids</code>, I first had to circumvent a bug in the current <code>mice</code>-package by defining the following function, as has been explained in <a href=""https://stats.stackexchange.com/a/71908/14646"">another question</a>:</p>

<pre><code>as.mids2 &lt;- function(data2, .imp=1, .id=2){
  ini &lt;- mice(data2[data2[, .imp] == 0, -c(.imp, .id)], maxit=0)
  names  &lt;- names(ini$imp)
  if (!is.null(.id)){
    rownames(ini$data) &lt;- data2[data2[, .imp] == 0, .id]
  }
  for (i in 1:length(names)){
    for(m in 1:(max(as.numeric(data2[, .imp])) - 1)){
      if(!is.null(ini$imp[[i]])){
        indic &lt;- data2[, .imp] == m &amp; is.na(data2[data2[, .imp]==0, names[i]])
        ini$imp[[names[i]]][m] &lt;- data2[indic, names[i]]
      }
    }
  }
  return(ini)
}
</code></pre>

<p>Once I had done this, I used <code>Zelig</code>:</p>

<pre><code>library(""Amelia"")
data(freetrade)
amelia.out &lt;- amelia(freetrade, m = 15, ts = ""year"", cs = ""country"")

library(""Zelig"")
zelig.fit &lt;- zelig(tariff ~ pop + gdp.pc + year + polity, data = amelia.out$imputations, model = ""ls"", cite = FALSE)
zelig.results &lt;- lapply(zelig.fit, function(x) x$result)

library(""mice"")
zelig4mice &lt;- as.mira(zelig.results)
zelig.mice.res &lt;- summary(pool(zelig4mice, method = ""rubin1987""))
</code></pre>

<p>Then I tried the same thing using only <code>mice</code>:</p>

<pre><code>imp.data &lt;- do.call(""rbind"", amelia.out$imputations)
imp.data &lt;- rbind(freetrade, imp.data)
imp.data$.imp &lt;- as.numeric(rep(c(0:15), each = nrow(freetrade)))
mice.data &lt;- as.mids2(imp.data, .imp = ncol(imp.data), .id = NULL)

mice.fit &lt;- with(mice.data, lm(tariff ~ polity + pop + gdp.pc + year))
mice.res &lt;- summary(pool(mice.res2, method = ""rubin1987""))
</code></pre>

<p>These are the results:</p>

<pre><code>&gt; zelig.mice.res
                  est       se     t    df Pr(&gt;|t|)     lo 95     hi 95 nmis   fmi lambda
(Intercept)  3.18e+03 7.22e+02  4.41  45.9 6.20e-05  1.73e+03  4.63e+03   NA 0.571  0.552
pop          3.13e-08 5.59e-09  5.59 392.1 4.21e-08  2.03e-08  4.23e-08   NA 0.193  0.189
gdp.pc      -2.11e-03 5.53e-04 -3.81 329.4 1.64e-04 -3.20e-03 -1.02e-03   NA 0.211  0.206
year        -1.58e+00 3.63e-01 -4.37  45.9 7.11e-05 -2.31e+00 -8.54e-01   NA 0.570  0.552
polity       5.52e-01 3.16e-01  1.75  90.8 8.41e-02 -7.58e-02  1.18e+00   NA 0.406  0.393

&gt; mice.res
                  est       se     t     df Pr(&gt;|t|)     lo 95     hi 95 nmis    fmi lambda
(Intercept)  3.42e+03 8.87e+02  3.86   8.01 4.80e-03  1.38e+03  5.47e+03   NA 0.7599 0.7066
pop          3.20e-08 5.25e-09  6.10 504.30 2.10e-09  2.17e-08  4.24e-08    0 0.0927 0.0891
gdp.pc      -2.09e-03 5.31e-04 -3.93 189.23 1.19e-04 -3.13e-03 -1.04e-03    0 0.1543 0.1454
year        -1.70e+00 4.46e-01 -3.83   8.02 5.02e-03 -2.73e+00 -6.78e-01    0 0.7594 0.7061
polity       5.74e-01 3.60e-01  1.59  13.93 1.34e-01 -2.00e-01  1.35e+00    2 0.5907 0.5358
</code></pre>

<p>From these data it is apparent, that the linear models fit by the two methods differ and so do the determined degrees of freedom.</p>

<p><strong>Why do these results differ? What is the correct analysis procedure?</strong></p>
",2013-10-23 10:48:16.873
58082,22967.0,1,327191.0,,,Extreme value theory for count data,<poisson-distribution><count-data><extreme-value>,CC BY-SA 3.0,"<p>I am aware of extreme value theory for continuous distributions. I need to fit an extreme value distribution to the maximum observation of number of events on a day, per month. This seems to be the block maxima problem, which is approximated by the GEV family of distributions for continuous distributions.  How do I do this for count data? </p>

<p>As a secondary question, let's assume the basic count process is ~ Poisson. Then does this lead to a different answer to the original question?   </p>
",2013-10-23 11:35:49.187
58083,17573.0,2,,57996.0,,,,CC BY-SA 3.0,"<p>It helps to make things concrete, so I will assume we are analyzing the effect of credit score on loan default, controlling for other stuff, like income.  You will have to search and replace to put in your example.  I recommend against trying to explain it in general.  Laymen are pretty much never interested in that.  How about (maybe too long?):</p>

<p>The partial effect of credit score on default probability is the amount that default probability goes up (or down) when credit score rises by one point and all other factors stay the same.  Think about two people, one with a credit score of 650 and one with a credit score of 651.  In all other respects (income, time on job, loan-to-value ratio, etc), they are identical.  The one with the higher credit score will have a lower probability of default.  The probability may only be a tiny, tiny bit lower since this is such a small difference in credit scores, but, because everything else is the same, it will be lower.  This difference in default probabilities between a person with a credit score of 650 and a credit score of 651 but with everything else identical is the partial effect of credit score on default probability.</p>

<p>There are two complications, though.  First, the difference in default probability between a person with a 650 score and a 651 credit score will not be the same as the difference in default probability between someone with a 750 and 751 credit score.   Second, the 650 vs 651 difference in default probability will depend on their other characteristics.  Two low income people, one with a 650 and one with a 651, may have a larger difference in default probability than two high income people, one with a 650 score and one with a 651.</p>

<p>To deal with these complications, we first calculate a personalized partial effect, the difference in default probabilities due to a one point increase in credit score, for each person in the sample.  Then, we average over these personalized partial effects to give the average partial effect.  This is called the ""average partial effect"" of credit scores on default probabilities.</p>
",2013-10-23 12:18:00.803
58084,22968.0,1,58093.0,,,Comparing data files,<time-series>,CC BY-SA 3.0,"<p>We have several weather files for a year's data sampled hourly. In each we have several variables (up to ten), temperature, wind speed, solar intensity etc.</p>

<p>I would like to try and develop a system that could determine if these files are statistically different. If I was using a single variable, I could use a KS-test, but with a collection of variables how should I approach this problem?</p>
",2013-10-23 12:34:44.323
58085,22970.0,1,58086.0,,,Prediction with not significant covariate in logistic regression,<r><regression><logistic><prediction>,CC BY-SA 3.0,"<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
&gt; 
&gt; fit2 &lt;- mod$family$linkinv(fit)
&gt; upr2 &lt;- mod$family$linkinv(upr)
&gt; lwr2 &lt;- mod$family$linkinv(lwr)
&gt; 
&gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
&gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""https://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
",2013-10-23 12:50:26.157
58088,11359.0,2,,25072.0,,,,CC BY-SA 3.0,"<p>I think this article will provide answers for your questions 1 and 4:</p>
<p>Fabrigar, L. R., Wegener, D. T., MacCallum, R. C., &amp; Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. <em>Psychological Methods</em>, 4, 272â€“299. doi:10.1037//1082-989X.4.3.272 (<a href=""http://www.statpower.net/Content/312/Handout/Fabrigar1999.pdf"" rel=""nofollow noreferrer"">PDF</a>)</p>
<p>From page 275:</p>
<blockquote>
<p>[...] it is important to recognize that the goal of identifying latent constructs (i.e., understanding the structure of correlations among measured variables) is different from that of data reduction. Data reduction involves taking scores on a large set of measured variables and reducing them to scores on a smaller set of composite variables that retain as much information from the original variables as possible. Data reduction does not attempt to model the structure of correlations among the original variables.</p>
<p>[...]</p>
<p>If the goal is to arrive at a parsimonious representation of the associations among measured variables, EFA can be an appropriate form of analysis. If the goal is data reduction, principal components analysis (PCA) is more appropriate.</p>
</blockquote>
<p>The author than continues to elaborate on the conceptual differences differences, which will probably help to clarify things.</p>
",2013-10-23 13:11:02.900
58089,13427.0,2,,54637.0,,,,CC BY-SA 3.0,"<p>Normally you would take the p-value by applying Rubin's rules on conventional statistical parameters like regression weights. Thus, there is often no need to pool p-values directly. Also, the likelihood ratio statistic can be pooled to compare models. Pooling procedures for other statistics can be found in my book Flexible Imputation of Missing Data, chapter 6.</p>

<p>In cases where there is no known distribution or method, there is an unpublished procedure by Licht and Rubin for one-sided tests. I used this procedure to pool p-values from the <code>wilcoxon()</code> procedure, but it is general and straightforward to adapt to other uses. </p>

<p>Use procedure below ONLY if all else fails, as for now, we know little about its statistical properties.</p>

<pre><code>lichtrubin &lt;- function(fit){
    ## pools the p-values of a one-sided test according to the Licht-Rubin method
    ## this method pools p-values in the z-score scale, and then transforms back 
    ## the result to the 0-1 scale
    ## Licht C, Rubin DB (2011) unpublished
    if (!is.mira(fit)) stop(""Argument 'fit' is not an object of class 'mira'."")
    fitlist &lt;- fit$analyses
        if (!inherits(fitlist[[1]], ""htest"")) stop(""Object fit$analyses[[1]] is not an object of class 'htest'."")
    m &lt;- length(fitlist)
    p &lt;- rep(NA, length = m)
    for (i in 1:m) p[i] &lt;- fitlist[[i]]$p.value
    z &lt;- qnorm(p)  # transform to z-scale
    num &lt;- mean(z)
    den &lt;- sqrt(1 + var(z))
    pnorm( num / den) # average and transform back
}
</code></pre>
",2013-10-23 13:51:33.063
58090,1809.0,1,,,,Comparing category distributions,<categorical-data><proportion><count-data><frequency>,CC BY-SA 3.0,"<p>I have categorical count data (for categories C1 to C3, but potentially several more categories) for two datasets:</p>

<pre><code>        | --- Dataset 1 --- | --- Dataset 2 --- |
        |  C1    C2    C3   |  C1    C2    C3   |
Item 1  |  0     200   300  |  0     2      3   |
Item 2  |  0     200   300  |  5     0      0   |
</code></pre>

<p>The total number of data-points in each dataset is different (500 and 5 in this example).</p>

<p>What statistical test should I use to determine if the distribution of counts for each item across the categories is the same between the two datasets?</p>

<p>For example, the distribution of Item 1 is the same across the two datasets, but the distribution of Item 2 is not. I will test each item separately.</p>
",2013-10-23 14:05:15.060
58091,22972.0,1,,,,Paired t-test for binary data,<t-test><normality-assumption><wilcoxon-signed-rank><paired-comparisons><paired-data>,CC BY-SA 3.0,"<p>I have one sample with n=170 and two binary variables (A,B) that can take as a value 1 or 0, where 1 counts as a success and 0 counts as a failure. What I want to know is whether the means of these two variables are equal.</p>

<p>To find this out I generate a new variable that takes the difference between these two variables called C, so C = B-A. I then compute the p-value for the hypothesis that C is normally distributed with the Shapiro-Wilk test and I find a p-value of .96, so I choose not to reject this hypothesis. Apart from that the difference is normally distributed, I am not worried about the other assumptions required for a paired t-test. </p>

<p><strong>Question:</strong> <em>Can I use the paired t-test in this circumstance or is it a mistake to use the Shapiro-Wilk test for binary data to check for normality and should I use the Wilcoxon sign rank test instead?</em> </p>

<p>I would much prefer to use the t-test, because I believe it has a higher power than the Wilcoxon sign rank test, but that higher power pretty much does not matter if the test used is the wrong one. </p>

<p>Cheers,</p>

<p>Martin</p>
",2013-10-23 14:10:58.907
58092,22729.0,1,,,,Test incorrect functional form when residuals have non-normal distribution,<regression><hypothesis-testing><misspecification>,CC BY-SA 3.0,"<p>J. B. Ramsey (in <em>""Tests for specification errors in classical linear least-squares regression analysis."" Journal of the Royal Statistical Society. 1969</em>) says that the <a href=""http://en.wikipedia.org/wiki/Ramsey_RESET_test"" rel=""nofollow"">RESET test</a> assumes that the residuals are normally distributed. </p>

<p>If one wants to test the incorrect functional form of a model but the residuals have a <strong>non-normal</strong> distribution, how can it be done?</p>

<p>Ramsey also says that ""<em>the cases where mis-specification leads to a non-normal distribution of Ã» [residuals] are to be discussed in a later paper</em>"". Does any one know which paper is this?</p>
",2013-10-23 14:11:37.900
58093,10469.0,2,,58084.0,,,,CC BY-SA 3.0,"<p>Use Kolmogorov-Smirnov for each variable - this will tell you the p-value for each one being different.</p>

<p>Now you have 10 binary values (telling you whether the original variable failed the KS), and you need to test the hypothesis that the ""yes"" values occurred at random with probability at most <code>p</code>.</p>
",2013-10-23 14:16:01.433
58094,503.0,2,,58090.0,,,,CC BY-SA 3.0,"<p>If your reshape your data so that it is datasetXC, then you can use chi square or similar. E.g.</p>

<pre><code>            C1       C2     C3
1            0       200    300
2            0         2     3
</code></pre>

<p>Here you would need an exact test. </p>
",2013-10-23 14:31:50.700
58095,22399.0,2,,58091.0,,,,CC BY-SA 3.0,"<p>You are using the term 'mean' but actually you are comparing 'proportions' as your variables are categorical. I would ignore any issues with normality as the sampling distribution of the proportions will be normal (ignoring some pathological situations such as low sample size which is not an issue here or proportions close to $0$ or $100$).</p>

<p>I recommend looking at the two-proportion z-test at the wiki: <a href=""http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Common_test_statistics"" rel=""nofollow"">Common Statistical Tests</a>. Search for ""two-proportion z-test"" in the table for the relevant test and conditions under which it is valid.</p>
",2013-10-23 14:33:18.997
58098,9095.0,2,,45279.0,,,,CC BY-SA 3.0,"<p>So far, the best options I've found, thanks to your suggestions, are these:</p>

<pre><code>  library (igraph)
  library (ggparallel)

# Generate random data

  x1 &lt;- sample(1:1, 1000, replace=T)
  x2 &lt;- sample(2:3, 1000, replace=T)
  x3 &lt;- sample(4:6, 1000, replace=T)
  x4 &lt;- sample(7:10, 1000, replace=T)
  x5 &lt;- sample(11:15, 1000, replace=T)
  results &lt;- cbind (x1, x2, x3, x4, x5)
  results &lt;-as.data.frame(results)

# Make a data frame for the edges and counts

  g1           &lt;- count (results, c(""x1"", ""x2""))

  g2           &lt;- count (results, c(""x2"", ""x3""))
  colnames(g2) &lt;- c     (""x1"", ""x2"", ""freq"")

  g3           &lt;- count (results, c(""x3"", ""x4""))
  colnames(g3) &lt;- c     (""x1"", ""x2"", ""freq"")

  g4           &lt;- count (results, c(""x4"", ""x5""))
  colnames(g4) &lt;- c     (""x1"", ""x2"", ""freq"")

  edges        &lt;- rbind (g1, g2, g3, g4)

# Make a data frame for the class sizes

  h1            &lt;- count (results, c(""x1""))

  h2            &lt;- count (results, c(""x2""))
  colnames (h2) &lt;- c     (""x1"", ""freq"")

  h3            &lt;- count (results, c(""x3""))
  colnames (h3) &lt;- c     (""x1"", ""freq"")

  h4            &lt;- count (results, c(""x4""))
  colnames (h4) &lt;- c     (""x1"", ""freq"")

  h5            &lt;- count (results, c(""x5""))
  colnames (h5) &lt;- c     (""x1"", ""freq"")

  cSizes        &lt;- rbind (h1, h2, h3, h4, h5)

# Graph with igraph

  gph    &lt;- graph.data.frame (edges, directed=TRUE)

  layout &lt;- layout.reingold.tilford (gph, root = 1)
  plot (gph,
        layout           = layout,
        edge.label       = edges$freq, 
        edge.curved      = FALSE,
        edge.label.cex   = .8,
        edge.label.color = ""black"",
        edge.color       = ""grey"",
        edge.arrow.mode  = 0,
        vertex.label     = cSizes$x1 , 
        vertex.shape     = ""square"",
        vertex.size      = cSizes$freq/20)

# The same idea, using ggparallel

  a &lt;- c(""x1"", ""x2"", ""x3"", ""x4"", ""x5"")

  ggparallel (list (a), 
              data        = results, 
              method      = ""hammock"", 
              asp         = .7, 
              alpha       = .5, 
              width       = .5, 
              text.angle = 0)
</code></pre>

<p>Done with igraph</p>

<p><img src=""https://i.stack.imgur.com/ROG07.png"" alt=""With Igraph""></p>

<p>Done with ggparallel</p>

<p><img src=""https://i.stack.imgur.com/NNXlM.png"" alt=""With ggparallel""></p>

<p>Still too rough to share in a journal, but I've certainly found having a quick look at these very useful. </p>

<p>There is also a possible option from <a href=""https://stackoverflow.com/questions/16112162/graph-flow-chart-of-transition-from-states"">this question on stack overflow</a>, but I haven't had a chance to implement it yet; and another possibility <a href=""https://stackoverflow.com/questions/19730604/visualize-in-r-flow-from-one-set-of-objects-to-another"">here</a>. </p>
",2013-10-23 14:51:25.430
58099,8926.0,2,,4187.0,,,,CC BY-SA 3.0,"<p>Temptation to use advanced statistical methods without understanding them, just because they sound impressive or because they happen to better support researcher's initial hypothesis. </p>

<p>When one uses an advanced method he or she should have solid reasons as to why the method is appropriate.</p>
",2013-10-23 15:02:20.477
58100,22974.0,1,,,,Netflix Challenge - some help with SVD/SoftImpute,<r><cross-validation><model-selection><large-data><svd>,CC BY-SA 3.0,"<p>I'm currently working on the Netflix Challenge with the original huge dataset and have run into some problems. I don't have access to any servers or computing clusters so I've been running everything (slowly) on my personal machine.</p>

<p>I'm trying to implement the softImpute function in R and the algorithm converges in a reasonable amount of time. However, I can't feasibly do cross validation (SV) to optimize the best ""rank.max"" and ""lambda"" values in order to get the result.</p>

<p>I'm wondering if there is some sort of rule of thumb for these collaborative filtering problems in terms of what rank to choose based on the dimensions of your data matrix and perhaps an estimate on what lambda to then select. I understand this will never be as accurate as CV and parameter optimising but perhaps some basic rule of thumb can help me get a closer answer.</p>
",2013-10-23 15:38:02.280
58101,22.0,2,,58091.0,,,,CC BY-SA 3.0,"<p>If I understand the context correctly, then <a href=""http://en.wikipedia.org/wiki/McNemar%27s_test"">McNemar's test</a> is exactly what you want. It compares two binomial variables measured in each subject, sort of a paired chi-square test. The key point is that your data are paired -- you've measured two different binomial outcomes in each subject, so need a test that accounts for that.</p>
",2013-10-23 15:48:34.750
58102,21762.0,2,,58097.0,,,,CC BY-SA 3.0,"<p>As long as you correct for multiplicity (e.g. Bonferroni-Holm), this is one of many possible ways to test for association. Of course it can capture only linear aspects of the association. A non-significant result can thus be due to low power, lack of linear aspects in the true association or due to conservativeness of the correction for multiplicity.</p>
",2013-10-23 16:02:04.757
58103,13526.0,1,58108.0,,,Fuzzy regression discontinuity design in Stata,<stata><instrumental-variables><regression-discontinuity>,CC BY-SA 3.0,"<p>I am currently running computations through a ""Fuzzy"" Regression discontinuity Design. Suppose my data are in the following form:</p>

<ul>
<li>$Z$: <strong>assignment variable</strong>; if $Z &gt; Z_0$ then the person is assigned to the treatment with a certain probability $p_D$ (since we are in the ""fuzzy"" RDD framework, $p_D&lt;1$).</li>
<li>$D$: <strong>treatment status</strong>; $D=1$ if the person is treated, 0 otherwise.</li>
<li>$X$: set of exogenous variables.</li>
<li>$Y$: <strong>Binary outcome</strong> variable. </li>
</ul>

<p>To my knowledge - see e.g. [1] - running a fuzzy RDD is equivalent to apply Instrumental Variables using $Z$ as instrument (hence at the first stage we should have $D$ regressed on $Z$ and $X$).</p>

<p>In order to estimate the model through Stata I used the following code:</p>

<pre><code>biprobit (Y = X D) (D = X Z)
</code></pre>

<p>According to some research I have done - see Nichols' pdf at [2] - the <code>-biprobit-</code> package should be required because of the binary nature of the endogenous variable ($D$). </p>

<p>Do you find the above codes correct? Is it also possible to use a simple linear probability model like this? </p>

<pre><code>ivregress 2sls Y X (D=Z)
</code></pre>

<p>Thanks fo any help,</p>

<p>Stefano</p>

<blockquote>
  <p>[1] Angrist, J. D., Pischke, J. (2008). <em>Mostly Harmless Econometrics: An Empiricist's Companion</em>. Princeton University Press.</p>
  
  <p>[2]: <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDQQFjAA&amp;url=http://www.stata.com/meeting/chicago11/materials/chi11_nichols.pdf&amp;ei=GvVnUvKOFIPv4gT-moH4DQ&amp;usg=AFQjCNGv9pmEIOIvhVsmmMq38q05pRbFbg&amp;bvm=bv.55123115,d.bGE"" rel=""nofollow"">http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDQQFjAA&amp;url=http://www.stata.com/meeting/chicago11/materials/chi11_nichols.pdf&amp;ei=GvVnUvKOFIPv4gT-moH4DQ&amp;usg=AFQjCNGv9pmEIOIvhVsmmMq38q05pRbFbg&amp;bvm=bv.55123115,d.bGE</a></p>
</blockquote>
",2013-10-23 16:05:58.830
58104,22976.0,1,58113.0,,,Learning probability bad reasoning. Conditional and unconditional,<probability><conditional-probability>,CC BY-SA 3.0,"<p>I have a problem, I'm learning probability at the moment (I'm a programmer) and starting I have this:</p>
<blockquote>
<p>(Source: Minka.) My neighbor has two children. Assuming that the gender of a child is like a coin flip, it is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other possibilitiesâ€”two boys or two girlsâ€”have probabilities 1/4 and 1/4.</p>
<ul>
<li><p>a. Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is a girl?</p>
</li>
<li><p>b. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability that the other child is a girl?</p>
</li>
</ul>
</blockquote>
<p>Now my reasoning is:</p>
<pre><code>BB = 1/4 = 0.25
BG = 1/4 = 0.25
GB = 1/4 = 0.25
GG = 1/4 = 0.25
</code></pre>
<p>So for a., the probability of G I get it just by summing p(B,G) + p(G,B) = 0.5</p>
<p>And for b. p(G|B) = p(G,B)/p(B) = 0.5/0.5 = 1 that is wrong but I'm not getting why.</p>
",2013-10-23 16:31:13.440
58105,17628.0,1,58118.0,,,Parameter confidence intervals which include errors in data,<confidence-interval><fitting><uncertainty>,CC BY-SA 3.0,"<p>My question seems to be very basic one but my search has not given any similar question. I have small dataset of 8 $(x,y)$ values with uncertainties for $y$ (dependent variable) and the theory predicts quadratic dependence $y=a x^2 + b x + c$. I wish to fit this dataset to the quadratic equation and to calculate confidence intervals for the parameters $a$, $b$, $c$ based both on the residuals (they are very small, i.e. the quadratic model describes the experimental data with very small residuals) and on declared uncertainty for every measurement (the uncertainties are much bigger than the residuals). What is the most correct way to do this?</p>
",2013-10-23 16:37:54.700
58218,10060.0,2,,58210.0,,,,CC BY-SA 3.0,"<p>Sorry, can't wrap my head around it... <img src=""https://i.stack.imgur.com/ImR4p.png"" alt=""enter image description here""></p>

<p>From the plot above (the red line is when x = y)... I can see your first condition where when x increases, y can only increase. But I don't understand given the first condition, how can ""X values are anything for all Y values.""</p>

<p>Please post your scatter plot, while I'll go prepare some popcorn.</p>
",2013-10-25 13:24:51.600
58106,21576.0,2,,56875.0,,,,CC BY-SA 3.0,"<p>An interesting and topical issue in risk modelling. In my experience, risk models for credit which are developed on shorter periods of data tend to produce unstable coefficients - cross validation and/or out-of-time testing for model performance have usually shown this to be the case.</p>

<p>For stress testing of credit risk models, we are concerned with estimating probability of default (PD, probability of customer not repaying a loan) and loss given default (LGD, proportion of loan lost in event of default) during downturn macroeconomic conditions.</p>

<p>In terms of regulatory perspectives for stress testing, the <a href=""http://www.bis.org/bcbs/about.htm"" rel=""nofollow"">Basel Committee on Banking Supervision (BCBS)</a>, which can be considered the central bank of central banks, indicates a minimum of 5 to 7 years of data (dependent on model and portfolio type) for model development, unless strong evidence is shown that more recent data is more predictive. Banking regulators typically adhere to these time periods as part of Basel II and III standards for calculating regulatory capital.</p>

<p>Additionally, the time period of 2008 to 2009 would seem to be somewhat short as the financial crisis has persisted for a longer period in some countries, e.g. UK where macro economic conditions worsened in 2007. It could be argued that the crisis is still ongoing in some parts of the world.</p>
",2013-10-23 16:46:43.820
58107,22978.0,1,,,,What is the best convergence measure of MCMC,<markov-chain-montecarlo><convergence>,CC BY-SA 3.0,"<p>I use measures of Gelman and Rubin or Geweke. However, they are not applicable to sampling from multi-modal distribution, say p(x), because a chain can be stuck in a local mode. In such cases, the chain seems to be stationary and the two convergence measures will give a bad result that it is safe to terminate sampling, actually it is not. When I handled toy problems, I set first or second moment of target distributions is known. The distance between the value and its approximation using the corresponding chain was used as a convergence measure, but it is practically not applicable.</p>

<p>Is there any MCMC convergence measure that makes use of target distributions?</p>
",2013-10-23 16:53:02.503
58108,5045.0,2,,58103.0,,,,CC BY-SA 3.0,"<p>This is partial answer. I think you should probably use both the <code>biprobit</code> and the <code>ivreg/ivreg2</code> commands to check how robust your effects are. I like the <code>biprobit</code> approach given your data, but it does make some strong assumptions (no heteroskedasticity, no hetrogenous effects, normality of errors).* However, there's also a dedicated RD command in Stata called <code>rdrobust</code>. It can handle the fuzzy design and may be installed with:</p>

<pre><code>net install rdrobust, from(http://www-personal.umich.edu/~cattaneo/rdrobust) replace
</code></pre>

<p>You can find an intro to the command in Cattaneo, Calonico, and Titiunik's <em>Stata Journal</em> paper <a href=""http://www-personal.umich.edu/~cattaneo/papers/Calonico-Cattaneo-Titiunik_2013_STATA.pdf"" rel=""noreferrer"">Robust Data-Driven Inference in the Regression-Discontinuity Design</a>.</p>

<hr>

<p>*Austin Nichols' <a href=""http://www.stata.com/meeting/chicago11/materials/chi11_nichols.pdf"" rel=""noreferrer"">simulation results</a> indicate that the marginal effects may be less sensitive than the latent index function parameters to biprobit assumption violations. The LPM model is also <a href=""https://www.google.com/search?q=LPM%20site%3adavegiles.blogspot.com&amp;oq=LPM%20site%3adavegiles.blogspot.com&amp;aqs=chrome..69i57.11781j0j4&amp;sourceid=chrome&amp;espv=210&amp;es_sm=91&amp;ie=UTF-8"" rel=""noreferrer"">not always</a> the model of steel that A&amp;P make it out.</p>
",2013-10-23 16:54:44.293
58109,22399.0,1,,,,Interpretation of regression coefficients in the presence of modest correlations,<regression><multicollinearity>,CC BY-SA 3.0,"<p>I have a multiple regression model where I have nearly 20 independent variables. These variables are modestly correlated with each other (e.g., the maximum VIF is around 4 with most of them in the 2s).</p>

<p>One of the coefficients is statistically significant and is negative when I expected that it would be positive. I know that 'wrong signs' can be because of several reasons such as multi-collinearity, missing data, omitted variables etc but I am wondering if there is a simpler explanation for the 'wrong sign'.</p>

<p>The usual interpretation of the coefficients is that it represents the impact on the dependent variable when we change the independent variable by 1 unit holding everything else constant. </p>

<p>However, the above interpretation is accurate only if the independent variables are completely uncorrelated with one another. In the presence of modest correlations among the independent variables, when we increase one of them by 1 unit the others are also bound to go up/down by a modest amount (depending on the sign of the correlation) and hence the only way to predict the impact of a unit change of an independent variable is to evaluate its impact on the other independent variables and then assess the overall impact on the dependent variable. When we do such an analysis we may well discover that the 'wrong sign' is a non-issue as increasing that variable by 1 unit may result in an increase in the dependent variable via the changes in the other independent variables in the model.</p>

<p>Does the above explanation make sense or am I missing something?</p>
",2013-10-23 17:09:32.380
58110,22977.0,1,,,,Parameter space exploration,<data-mining><pattern-recognition><multivariate-regression>,CC BY-SA 3.0,"<p><em>I do realise this question is quite specific and practical, but I seek for some general help which helps me progress further in my analysis.</em></p>

<p>Let $y(\boldsymbol{x})\in\mathbb{R}$ be the function I'd like to regress, whereas $\boldsymbol{x}\in\mathbb{R}^6$. My design matrix $\boldsymbol{X}\in\mathbb{R}^{11\times6}$ (therefore, at the moment, $y(\boldsymbol{X})\in\mathbb{R}^{11}$, but I can run further experiments, if needed).<br>
My aim is maximising $y(\boldsymbol{x})$ over $\boldsymbol{x}$, $x_{2k+1}=\{5,7,9\}$, $x_{2k}=\{2,4\}$.</p>

<p>If I would run a <em>dense</em> experimental analysis, I would to test $3^3\times2^3=216$ cases, and each of those are very time consuming.</p>

<p>The collected data is the following (yes, I know, they do not respect the constrain given above, which it allows me to run a reasonably lower total number of experiments if the <em>dense experimental analysis</em> will be considered as only feasible solution):</p>

<pre><code>X = [
     4     2     3     2     3     2
     7     2     5     2     5     4
    10     2     7     4     5     2
    10     2     9     2     8     4
    10     4     6     2     4     2
     7     2     5     4     5     2
     7     4     5     2     5     2
     7     2     5     4     5     4
     7     4     5     2     5     4
     7     4     5     4     5     2
     7     4     5     4     5     4
     ];

y = [
     64.7
     68.0
     69.4
     68.5
     68.5
     71.9
     71.6
     71.7
     71.0
     71.6
     68.6
     ];
</code></pre>

<p>I did scatter plot $(x_i, y(x_i)),i=\{1,2,3,4,5,6\}$, but the output is not that informative.</p>

<pre><code>name = {
     'ks1'
     'ss1'
     'ks2'
     'ss2'
     'ks3'
     'ss3'
     };

[m, n] = size(X); % # of experiments, # of parameters = 3 conv kernel size + 3 subsamp

for j = 1:2:n
    subplot(2,3,(j+1)/2)
    h = plot(X(:,j),y,'x','MarkerSize',10,'LineWidth',2);
    xlabel(name{j})
    ylabel('Testing accuracy [%]')
    axis([2 10 60 75])
    grid on
end

for j = 2:2:n
    subplot(2,3,3+j/2)
    h = plot(X(:,j),y,'x','MarkerSize',10,'LineWidth',2);
    xlabel(name{j})
    ylabel('Testing accuracy [%]')
    axis([1 5 60 75])
    grid on
end
</code></pre>

<p><img src=""https://i.stack.imgur.com/R7WP6.png"" alt=""Scatter plots""></p>

<p>So, my question is, <strong>how can I understand</strong> (and/or visualise) <strong>what is going on in this multivariate problem?</strong><br>
To my mind a PCA keeps coming up as possible useful tool, even though I know it is used for visualising main orientations rather than helping me finding the maxima of my scalar fields..</p>

<p><strong>Update</strong><br>
I am currently running the 216 cases batch experiment, limited to 30 epochs training. Still, I would like to <em>visualise</em> my results. Actually, I do have a testing (and training) accuracy function per case, whereas above I just picked its maximum.</p>

<p>This is an overview of the testing accuracy functions
<img src=""https://i.stack.imgur.com/0mYvb.png"" alt=""Testing accuracies""></p>

<hr>

<p>What I am trying to accomplish is determining the best configuration for a convolutional artificial neural network. The parameters I am considering here are <em>kernel size</em> (<code>ks</code>) and <em>subsamplig</em> (<code>ss</code>) factor for multiple layers (<code>1</code>, <code>2</code> and <code>3</code>), whereas $y(\cdot)$ is the testing (or cross-validation) accuracy.</p>
",2013-10-23 17:19:28.557
58111,7927.0,1,58132.0,,,Why use bayesglm?,<bayesian><generalized-linear-model>,CC BY-SA 3.0,"<p>My overall question is: why use <code>bayesglm</code> instead of other classification methods? </p>

<p>Note:</p>

<ol>
<li>I'm only interested in prediction.</li>
<li>I have a decent amount of data (~ 100,000 obs). </li>
</ol>

<p>I feel like the sample size is large enough the parameters of a regular logistic regression are going to be normally distributed (CLT). What would I gain by specifying priors? My hunch is that it will only matter for a small dataset, but I don't have any theoretical or applied evidence.</p>
",2013-10-23 17:25:23.237
58112,14172.0,1,,,,Ratio data with categorical events,<categorical-data><data-transformation><dataset>,CC BY-SA 3.0,"<p>I have a question about treating qualitative (categorical) events within otherwise quantitative (ratio scale) data. Without going into too much detain, the experiment is on throwing. I measure the distance between the target and the result of the throw for each attempt. Some attempts are failed because the participant hits an obstacle on the way to the target. In the end the dependent variable may look something like this:
[.2 .1 .3 .2 -1 .2 .2 .8 -1] where the values may range from 0 to anything, but -1 represent the obstacle hits. What are some ways to deal with such categorical data within quantitative variable? I cannot just throw them out because they are meaningful from the performance perspective. Substituting with some high error distance seems arbitrary. Is there anything else? In the end I'm interested in how people get better at throwing as a result of practice. </p>
",2013-10-23 17:39:16.837
58241,23046.0,1,,,,Slope of a Regression,<regression><conditional-expectation>,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/UQ88F.jpg"" alt=""enter image description here""></p>

<p>Given this chart, how do you determine the slope of the regression Y on F? </p>
",2013-10-25 17:58:52.327
58113,19331.0,2,,58104.0,,,,CC BY-SA 3.0,"<p>For (a), a simple way to look at is that you've reduced your probability space to only the combinations that have at least one boy:</p>

<pre><code>BB = 1/3
BG = 1/3
GB = 1/3
</code></pre>

<p>GG is no longer a possibility based on the fact that your neighbor said he had at least one boy.  Of the possibilities remaining, you're left with a 2/3 probability that he has a girl.  The information he gave you reduced the probability of him having a girl from 3/4 to 2/3. Formally, this can be shown as follows:  $$P(At\ least\ one\ girl|At\ least\ one\ boy) = \frac{P(At\ least\ one\ girl\ \cap At\ least\ one\ boy)}{P(At\ least\ one\ boy)}  $$
From your original box, we can see the probability of having at least one boy and at least one girl is BG + GB = 0.25 + 0.25 = 0.5, but we need to divide by the probability of at least one boy, which is BB + BG + GB = 0.25 + 0.25 + 0.25 = 0.75, so we get $\frac{\tfrac{1}{2}}{\tfrac{3}{4}} = \frac{2}{3}$.</p>

<p>For (b), now that we've seen a boy, the only uncertainty remaining is the gender of the other child, and given no other information, the probability of the other child being female is 1/2, which is the answer.</p>
",2013-10-23 17:41:54.690
58114,5045.0,2,,58109.0,,,,CC BY-SA 4.0,"<p>This is not an answer, but it is too long for a comment.</p>

<p>I would say the interpretation is accurate even with multicollinearity, but the <em>ceteris paribus</em> coefficient is not the quantity you care about. If you believe that the multicollinearity arises from an approximate linear relationship among some of the regressors, that relationship could be formalized either through some constraint on the parameters (such as dropping a variable or something more) or with a simultaneous equation approach. Without more details about the nature of your problem, it's hard to be more specific. There are some examples (28, 29 and 5) in Peter Kennedy's paper <a href=""http://www.montana.edu/cstoddard/2013-562/documents/oh%20no.pdf"" rel=""nofollow noreferrer"">Oh No! I Got the Wrong Sign! What Should I Do?</a>.</p>
",2013-10-23 17:44:29.413
58115,22355.0,2,,58081.0,,,,CC BY-SA 3.0,"<p>Stef is right. An initial mice object is created in <code>as.mids2()</code> with <code>m = 5</code> as a default. Even though the <code>mids</code> object is completely filled in, the <code>with()</code> function only uses the first five. This is because the number of imputations - corresponding to the largest value in <code>.imp</code> - is not changed in the returned object. The following code yields the correct <code>mids</code> object</p>

<pre><code>as.mids2 &lt;- function(data2, .imp=1, .id=2){
  ini &lt;- mice(data2[data2[, .imp] == 0, -c(.imp, .id)], m = max(as.numeric(data2[, .imp])), maxit=0)
  names  &lt;- names(ini$imp)
  if (!is.null(.id)){
    rownames(ini$data) &lt;- data2[data2[, .imp] == 0, .id]
  }
  for (i in 1:length(names)){
    for(m in 1:(max(as.numeric(data2[, .imp])))){
      if(!is.null(ini$imp[[i]])){
        indic &lt;- data2[, .imp] == m &amp; is.na(data2[data2[, .imp]==0, names[i]])
        ini$imp[[names[i]]][m] &lt;- data2[indic, names[i]]
      }
    } 
  }
  return(ini)
}
</code></pre>

<p>and yields the same results as <code>Zelig</code> given the active random seed.</p>

<pre><code>&gt; zelig.mice.res
                      est           se         t        df     Pr(&gt;|t|)         lo 95
(Intercept)  3.118344e+03 7.031673e+02  4.434711  55.20608 4.437533e-05  1.709283e+03
pop          3.074658e-08 5.993667e-09  5.129844 211.31179 6.550041e-07  1.893154e-08
gdp.pc      -2.185839e-03 5.968324e-04 -3.662400 174.91116 3.308584e-04 -3.363759e-03
year        -1.551702e+00 3.535625e-01 -4.388762  55.26609 5.183456e-05 -2.260180e+00
polity       4.649357e-01 3.102389e-01  1.498638 125.11682 1.364865e-01 -1.490600e-01
                    hi 95 nmis       fmi    lambda
(Intercept)  4.527404e+03   NA 0.5206397 0.5035824
pop          4.256162e-08   NA 0.2643263 0.2573962
gdp.pc      -1.007919e-03   NA 0.2909757 0.2829145
year        -8.432231e-01   NA 0.5203580 0.5033089
polity       1.078931e+00   NA 0.3448966 0.3345077

&gt; mice.res
                      est           se         t        df     Pr(&gt;|t|)         lo 95
(Intercept)  3.118344e+03 7.031673e+02  4.434711  55.20608 4.437533e-05  1.709283e+03
pop          3.074658e-08 5.993667e-09  5.129844 211.31179 6.550041e-07  1.893154e-08
gdp.pc      -2.185839e-03 5.968324e-04 -3.662400 174.91116 3.308584e-04 -3.363759e-03
year        -1.551702e+00 3.535625e-01 -4.388762  55.26609 5.183456e-05 -2.260180e+00
polity       4.649357e-01 3.102389e-01  1.498638 125.11682 1.364865e-01 -1.490600e-01
                    hi 95 nmis       fmi    lambda
(Intercept)  4.527404e+03   NA 0.5206397 0.5035824
pop          4.256162e-08    0 0.2643263 0.2573962
gdp.pc      -1.007919e-03    0 0.2909757 0.2829145
year        -8.432231e-01    0 0.5203580 0.5033089
polity       1.078931e+00    2 0.3448966 0.3345077 
</code></pre>
",2013-10-23 17:48:43.877
58116,,2,,58112.0,user31668,,,CC BY-SA 3.0,"<p>In statistical terminology, the -1 data represent <strong>censored</strong> data. Two general approaches for sensoring are <a href=""http://sites.stat.psu.edu/~jls/mifaq.html"" rel=""nofollow"">multiple imputation</a> and <a href=""http://www.public.iastate.edu/~stat415/meeker/ml_estimation_chapter.pdf"" rel=""nofollow"">censored likelihood</a></p>

<p>One of those two should be helpful.</p>
",2013-10-23 17:54:04.683
58117,22982.0,1,,,,Is Cronbach's alpha interchangeable with $\rho$?,<correlation><reliability><cronbachs-alpha>,CC BY-SA 3.0,"<p>I am trying to calculate the reliability of my change scores using the following formula.  Is $r$ ($\rho$) in this instance interchangeable with Cronbach's alpha? 
$$
\frac{.5(r_{xx}+ r_{yy}) - r_{xy}}{1 - r_{xy}}
$$
Where reliability of difference scores is a function of the score reliability of each measure ($r_{xx}, r_{yy}$) and the correlation between the measures ($r_{xy}$).</p>
",2013-10-23 17:56:19.437
58118,,2,,58105.0,user31668,,,CC BY-SA 3.0,"<p>Based on the OP's response to my questions, I'd suggest <a href=""http://www.sagepub.com/upm-data/21122_Chapter_21.pdf"" rel=""nofollow"">boostrapping</a>, which will take into account the possibility that your small sample size results in overfitting the model.
I would look into the body of theory called Error in Variables Regression. See <a href=""http://econ.lse.ac.uk/staff/spischke/ec524/Merr_new.pdf"" rel=""nofollow"">link</a> and <a href=""http://en.wikipedia.org/wiki/Errors-in-variables_models"" rel=""nofollow"">link</a></p>
",2013-10-23 17:57:03.863
58119,20622.0,1,58496.0,,,Does the presence of an outlier increase the probability that another outlier will also be present on the same observation?,<outliers><mathematical-statistics>,CC BY-SA 3.0,"<p>**<strong>Edit:</strong> (10/26/13) More clear (hopefully) mini-rewrites added at the bottom**</p>
<p><em><strong>I'm asking this from a theoretical/general standpoint - not one that applies to a specific use case.</strong></em></p>
<p>I was thinking about this today:</p>
<p><strong>Assuming the data does not contain any measurement errors, if you're looking at a specific observation in your data and one of the measurements you recorded contains what could be considered an outlier, does this increase the probability (above that of the rest of the observations that do not contained measured outliers) that the same observation will contain another outlier in another measurement?</strong></p>
<p><em><strong>For my answer I'm looking for some sort of theorem, principle, etc. that states what I'm trying to communicate here much more elegantly.  For clearer explanations see Gino and Behacad's answers.</strong></em></p>
<p>Example:</p>
<p>Let's say you're measuring the height and circumference of a certain type of plant.  Each observation corresponds to 1 plant (you're only doing this once).</p>
<p>For height, you measure:</p>
<pre>
Obs 1  |   10 cm
Obs 2  |   9 cm
Obs 3  |   11 cm
Obs 4  |   22 cm
Obs 5  |   10 cm
Obs 6  |   9 cm
Obs 7  |   11 cm
Obs 8  |   10 cm
Obs 9  |   11 cm
Obs 10  |   9 cm
Obs 11  |   11 cm
Obs 12  |   10 cm
Obs 13  |   9 cm
Obs 14  |   10 cm
</pre>
<p>Since observation 4 contains what could be considered an outlier from the rest of the data, would the probability increase that measured circumference also contains an outlier for observation #4?</p>
<p>I understand my example may be too idealistic but I think it gets the point across...just change the measurements to anything.</p>
<h1><em><strong>Edited in attempts to make more clear:</strong></em></h1>
<p>(10/26/13)</p>
<p><strong>Version 1 Attempt of Abbreviated Question:</strong></p>
<blockquote>
<p>In nature and in general, is there a tendency (even a weak one) that
the <em><strong>probability is greater</strong></em> that the &quot;degree of variance from the
mean in any attribute(s) of an observation&quot; will be <em>similar</em> to the
&quot;degree of variance from the mean in any other* specific attribute of
that same observation&quot; <em><strong>in comparison to the probability</strong></em> that it
will INSTEAD be <em>more similar</em> to the &quot;degree of variance from the
mean in that same* specific attribute of <em>any other observation</em>.&quot;</p>
</blockquote>
<p><em>* next to a word means I was pairing what they reference.<br>
&quot;Quotes&quot; used above mean nothing and are used simply to help section parts together/off for clarity.</em></p>
<p><strong>Version 2 Attempt of Abbreviated Question:</strong></p>
<blockquote>
<p>In nature and in general, is variance from the mean across
observations for one attributeÂ¹ correlated (even with <em>extremely</em>
loose correlation) to the variance from the mean across observations for all
attributesÂ¹?</p>
</blockquote>
<p><em>Â¹Attribute meaning measurement, quality, presence-of-either of these, and/or nearly anything else that the word &quot;attribute&quot; could even slightly represent as a word.  Include all synonyms of the word &quot;attribute&quot; as well.</em></p>
",2013-10-23 18:17:15.587
58161,9074.0,2,,58159.0,,,,CC BY-SA 3.0,"<p>What do you mean by ""average improvement rate""? If you mean that the number of faulty products are halved every year, then it is simply an exponential model that you are looking for. </p>

<p>$3\sigma=66,807$ faults per $1,000,000$</p>

<p>$6\sigma = 3.4 $ faults per $1,000,000$</p>

<p>An exponential model of faults is then given by
$f(t)=f(0)(1+r)^t$, with $r = -0.5$, which can be solved as follows:
$3.4=66,807*0.5^t \leftrightarrow {3.4 \over 66,807} = 0.5^t \leftrightarrow \log({3.4 \over 66,807})=t\log(0.5) \leftrightarrow  \\ {\log({3.4 \over 66,807}) \over log(0.5)} = t$</p>

<p>$t=-9.8859/-0.693 = 14.265$ years. Or 14 years and 97 days.</p>
",2013-10-24 12:29:36.073
58120,,2,,58082.0,user31668,,,CC BY-SA 3.0,"<p>I don't know a definitive answer for your primary question. Although I found the following two references:</p>

<p><a href=""http://www.jstor.org/stable/3212152"" rel=""nofollow"">Anderson, C. W., â€œExtreme value theory for a class of discrete distributions with applications to some stochastic processesâ€, Journal of Applied Probability, vol 7, 1970, pp. 99â€“113.</a></p>

<p><a href=""http://journals.cambridge.org/action/displayAbstract;jsessionid=DC999372089497C149031565F99F7683.journals?fromPage=online&amp;aid=2082124"" rel=""nofollow"">Anderson, C. W., â€œLocal limit theorems for the maxima of discrete random variablesâ€,
Mathematical Proceedings of the Cambridge Philosophical Society, vol 88, 1980, pp. 161â€“
165.</a></p>

<p>For your secondary question, the CDF of the Poisson is $\frac{\Gamma(\lfloor k+1\rfloor,\lambda)}{\lfloor k\rfloor!}$ so $P(\max\limits_N X_n \leq M) = (\frac{\Gamma(\lfloor k+1\rfloor,\lambda)}{\lfloor k\rfloor!})^N$. Apply the difference operator (lag1) and you get the PMF of the max.</p>
",2013-10-23 18:20:32.227
58121,2069.0,2,,58119.0,,,,CC BY-SA 3.0,"<p>I'm going to say yes. Of course something being an outlier on one measure does not necessarily mean it will be an outlier on other measures, but I think it certainly increases the odds. In essence, I think anything being unusual in one way is more likely to be unusual in other ways. This is perhaps the result of different variables being correlated in all kinds of ways. So if one is ""extreme"", this will also correlate with other factors that might also be in the extreme range.</p>

<p>I hope someone can provide a better answer with some empirical evidence. Perhaps we can look at this from the multivariate outlier perspective. In my experience, individuals who are outliers on one variable are often outliers on many other variables as well, contributing to multivariate outlierness. </p>

<p>Take this answer with a grain of salt since it depends on how we define an outlier and such, but I think in general it makes sense. </p>
",2013-10-23 18:36:40.280
58122,22985.0,1,58151.0,,,Correlation of parts of two variables that miminize correlation with another variable,<correlation>,CC BY-SA 3.0,"<p>I have three data vectors $A$, $B$ and $C$ that are all more or less correlated.</p>

<p>What I want is a meaningful definition for a measure of the degree of correlation between those parts of $A$ and $B$ that are not correlated with $C$.</p>

<p>I do apologize if this is not fully understandable. I struggle to come to an easier formulation of the problem. However I can give an example:</p>

<p>Example: Say $A$, $B$, $C$ could all be binary, i.e. only contain the values 0 and 1. I then would want to know the number of positions where $A$ and $B$ are 1 and $C$ is 0.</p>

<p>As a side note: How does one ""uncorrelate"" variables, i.e. given two variables $A$ and $B$ how to split $A$ into a part that is maximally correlated with B and another that is maximally uncorrelated?</p>
",2013-10-23 18:50:22.557
58123,22987.0,1,,,,Standard error from correlation coefficient,<distributions><correlation><normal-distribution><variance>,CC BY-SA 3.0,"<p>Many studies only report the relationship between two variables (e.g. linear or logistic equation), $n$, and $r^2$. I want to use these reported statistics to reproduce this relationship with its variation.  Most statistical software will generate a parameter distribution from a mean and standard error. Assuming a normal distribution, can the standard error of the parameter estimates be calculated with just these three statistics? Essentially, can I get a standard error from $r^2$? </p>

<p>Or will I need to do some kind of bootstrapping procedure to generate a distribution that has the same $r^2$ and then calculate the standard error? if so are there better ones for linear vs. nonlinear equations?</p>
",2013-10-23 18:53:42.190
58124,21762.0,2,,57968.0,,,,CC BY-SA 3.0,"<p>Sheldon, Sheldon. How comes that you have to ask a question about math to people like <em>us</em>?</p>

<p>In survival analysis, your setting is called ""competing risk"". The joint distribution of the earliest failure time and the type of failure is fully described by the so called ""cumulative incidence function"" (it even allows for censoring, i.e. no failure until end of time horizon). I am quite sure that you will find relevant information in the literature stated in
<a href=""https://stats.stackexchange.com/questions/13935/assumptions-and-pitfalls-in-competing-risks-model"">Assumptions and pitfalls in competing risks model</a> </p>
",2013-10-23 19:04:02.220
58125,211.0,1,58128.0,,,"""Kernel density estimation"" is a convolution of what?",<r><kernel-smoothing><convolution>,CC BY-SA 3.0,"<p>I am trying to get a better understanding of kernel density estimation.</p>

<p>Using the definition from Wikipedia: <a href=""https://en.wikipedia.org/wiki/Kernel_density_estimation#Definition"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Kernel_density_estimation#Definition</a></p>

<p>$
 \hat{f_h}(x) = \frac{1}{n}\sum_{i=1}^n K_h (x - x_i) \quad = \frac{1}{nh} \sum_{i=1}^n K\Big(\frac{x-x_i}{h}\Big)
$</p>

<p>Let's take $K()$ to be a rectangular function which gives $1$ if $x$ is between $-0.5$ and $0.5$ and $0$ otherwise, and $h$ (window size) to be 1.</p>

<p>I understand that the density is a convolution of two functions, but I am not sure I know how to define these two functions. One of them should (probably) be a function of the data which, for every point in R, tells us how many data points we have in that location (mostly $0$). And the other function should probably be some modification of the kernel function, combined with the window size. But I am not sure how to define it. </p>

<p>Any suggestions?</p>

<p>Bellow is an example R code which (I suspect) replicates the settings I defined above (with a mixture of two Gaussians and $n=100$), on which I hope to see a ""proof"" that the functions to be convoluted are as we suspect.</p>

<pre><code># example code:
set.seed(2346639)
x &lt;- c(rnorm(50), rnorm(50,2))
plot(density(x, kernel='rectangular', width=1, n = 10**4))
rug(x)
</code></pre>

<p><img src=""https://i.stack.imgur.com/pEjMp.png"" alt=""enter image description here""></p>
",2013-10-23 19:36:10.130
58126,19750.0,1,,,,Mean absolute deviation,<regression>,CC BY-SA 3.0,"<p>Wikipedia states:</p>

<blockquote>
  <p>The mean absolute error (MAE) is a common measure of forecast error in time
  series analysis, where the terms ""mean absolute deviation"" is
  sometimes used in confusion with the more standard definition of mean
  absolute deviation. The same confusion exists more generally.</p>
</blockquote>

<p>What does that mean? What exactly is the confusion?</p>

<p>Also, why is MAE is used in time series analysis specifically? (as opposed to more general measures of error such as MSE)?</p>
",2013-10-23 19:47:38.237
58127,22961.0,2,,58119.0,,,,CC BY-SA 3.0,"<p>I will try to reply using empirical evidence. Let's assume you are measuring the heights in a men sample. In this case, the outlier will be represented by a very tall men (a giant). It is very likely this men will represent an outlier also for other variables like for instance shoe size or arms lengths and so on.
Other case, you are measuring financial performance of US Public company. An outlier will be a very successful company with a sales growth twice the industry average. Very likely the same company will be an outlier in respect of any measure of profitability or stock price appreciation. In a nut shell, I am incline to think something behaving exceptionally out of the norm will tend to conserve this property across different manifestations. Is there a theorem that disprove this theory? </p>
",2013-10-23 19:56:26.190
58128,668.0,2,,58125.0,,,,CC BY-SA 4.0,"<p>Corresponding to any batch of data <span class=""math-container"">$X = (x_1, x_2, \ldots, x_n)$</span> is its ""empirical density function""</p>

<p><span class=""math-container"">$$f_X(x) = \frac{1}{n}\sum_{i=1}^{n} \delta(x-x_i).$$</span></p>

<p>Here, <span class=""math-container"">$\delta$</span> is a ""generalized function.""  Despite that name, it isn't a function at all: it's a new mathematical object that can be used only within integrals.  Its defining property is that for any function <span class=""math-container"">$g$</span> of compact support that is continuous in a neighborhood of <span class=""math-container"">$0$</span>,</p>

<p><span class=""math-container"">$$\int_{\mathbb{R}}\delta(x) g(x) dx = g(0).$$</span></p>

<p>(Names for <span class=""math-container"">$\delta$</span> include ""atomic"" or ""point"" measure and ""<a href=""https://en.wikipedia.org/wiki/Dirac_delta_function"" rel=""noreferrer"">Dirac delta function</a>.""  In the following calculation this concept is extended to include functions <span class=""math-container"">$g$</span> which are continuous from one side only.)</p>

<p>Justifying this characterization of <span class=""math-container"">$f_X$</span> is the observation that</p>

<p><span class=""math-container"">$$\eqalign{
\int_{-\infty}^{x} f_X(y) dy 
&amp;= \int_{-\infty}^{x} \frac{1}{n}\sum_{i=1}^{n} \delta(y-x_i)dy \\
&amp;= \frac{1}{n}\sum_{i=1}^{n} \int_{-\infty}^{x}  \delta(y-x_i)dy \\
&amp;= \frac{1}{n}\sum_{i=1}^{n} \int_{\mathbb{R}} I(y\le x) \delta(y-x_i)dy \\
&amp;= \frac{1}{n}\sum_{i=1}^{n} I(x_i \le x) \\
&amp;= F_X(x)
}$$</span></p>

<p>where <span class=""math-container"">$F_X$</span> is the usual empirical CDF and <span class=""math-container"">$I$</span> is the usual characteristic function (equal to <span class=""math-container"">$1$</span> where its argument is true and <span class=""math-container"">$0$</span> otherwise).  (I skip an elementary limiting argument needed to move from functions of compact support to functions defined over <span class=""math-container"">$\mathbb{R}$</span>; because <span class=""math-container"">$I$</span> only needs to be defined for values within the range of <span class=""math-container"">$X$</span>, which is compact, this is no problem.)</p>

<p>The convolution of <span class=""math-container"">$f_X(x)$</span> with any other function <span class=""math-container"">$k$</span> is given, by definition, as</p>

<p><span class=""math-container"">$$\eqalign{
(f_X * k)(x) &amp;= \int_{\mathbb{R}} f_X(x - y) k(y) dy \\
             &amp;=\int_{\mathbb{R}}  \frac{1}{n}\sum_{i=1}^{n} \delta(x-y-x_i) k(y) dy \\
             &amp;= \frac{1}{n}\sum_{i=1}^{n}\int_{\mathbb{R}}  \delta(x-y-x_i) k(y) dy \\
             &amp;=\frac{1}{n}\sum_{i=1}^{n} k(x_i-x).
}$$</span></p>

<p>Letting <span class=""math-container"">$k(x) = K_h(-x)$</span> (which is the same as <span class=""math-container"">$K_h(x)$</span> for symmetric kernels--and most kernels are symmetric) we obtain the claimed result: the Wikipedia formula is a convolution.</p>
",2013-10-23 19:57:19.317
58129,4582.0,2,,31575.0,,,,CC BY-SA 3.0,"<p>Here is my implementation in R</p>

<pre><code>x &lt;- c(1,2,1,1,3,4,4,1,2,4,1,4,3,4,4,4,3,1,3,2,3,3,3,4,2,2,3)
xChar&lt;-as.character(x)
library(markovchain)
mcX&lt;-markovchainFit(xChar)$estimate
mcX
</code></pre>
",2013-10-23 20:06:30.957
58130,5237.0,2,,58123.0,,,,CC BY-SA 3.0,"<p>If you look at the Wikipedia page for the Pearson product-moment correlation, you will find <a href=""http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Inference"" rel=""noreferrer"">sections</a> that describe how confidence intervals can be calculated.  Typically, people will use Fisher's $z$-transformation (arctan) to turn the $r$ into a variable that is approximately normally distributed:<br>
$$
z_r = \frac 1 2 \ln \frac{1 + r}{1 - r}
$$
Having applied this transformation, the standard error will be approximately $^1/_{\sqrt{(N-3)}}$.  With this you can form whatever length confidence interval you like.  Once you've found the confidence limits you want, you can back-transform them to the original $r$ scale (i.e., $[-1, 1]$) like so:<br>
$$
\text{CI limit}_r = \frac{\exp(2z) - 1}{\exp(2z) + 1}
$$
In other words, you <em>can</em> form a confidence interval for $r$ without the original data, so long as you have the original $N$.  </p>

<p><sub><em>Notes</em>: This approach is an approximation, there are exact formulae listed on the Wikipedia page, but they are harder to use.  Although it doesn't say on the Wikipedia page, there are several conditions you want to meet in order for this approximation to be reasonable.  The $N$ should be at least $30$ (IIRC), and the marginal distributions (i.e., the univariate distributions of the two variables being correlated) should be normal.  For example, I'm not sure that this will be accurate if the correlation were composed of two vectors of $1$s and $0$s.  However, higher $N$ should allow you to compensate for minor non-normality.</sub></p>
",2013-10-23 20:15:29.683
58131,16046.0,1,,,,How to sample using MCMC from a posterior distribution in general?,<bayesian><sampling><simulation><markov-chain-montecarlo><metropolis-hastings>,CC BY-SA 3.0,"<p>Assume one has the posterior distribution of a parameter, $p(\theta|y)$ and what I mean by having it is that for each point of $\theta$, one can use Monte Carlo method+MCMC to calculate the $p(\theta|y)$. Now my question is if I want to sample from $p(\theta|y)$, them basically I have to do one Gibbs sampling(for example) to sample from distribution and at any point I have to run Monte Carlo method on the point to calculate $p(\theta|y)$'s value right? i.e. it needs two loops, one inside of the other. Is this correct?</p>

<p>As I got an answer to this question and I thought maybe my question is vague I will try to clarify it a bit more:</p>

<p>From what I know by reading for a week the whole time about Monte Carlo method and MCMC, I understood(correct me if I am wrong) that:
$$p(\theta|y)=\frac{p(y|\theta)p(\theta)}{\int_{\Theta}{p(y|\theta)p(\theta)}\text{d}\theta}.$$</p>

<p>Now if you consider that we only have a sampling algorithm for $\theta$ and we can only calculate $p(y|\theta)$ explicitly(and not the other functions!), therefore to get values from $p(\theta|y)$ one needs to numerically integrate the denominator. And for each value of this posterior one needs to apply a sampling scheme like Gibbs sampling to generate a sample of $p(\theta|y)$; each new transition in the parameter space should then sample from the distribution which is $p(\theta|y)$ here and to calculate that the above proportion should be computed. </p>
",2013-10-23 20:15:39.310
58132,15766.0,2,,58111.0,,,,CC BY-SA 3.0,"<p>In engineering, as well as supply chain risk management, ""engineering knowledge"" --eg an educated persons best guess-- may be the best data you have. For example, the likelihood of a tsunami occurring and disrupting the supply chain, without additional data, can be estimated by an expert in the subject (there are better methods for constructing priors). As time passes, tsunamis occur and, as a result, we gain more data, and can update our priors (engineering knowledge) with posteriors (priors adjusted for new data). At some point, there will be so much data that the initial prior is irrelevant, and no matter whom made the prediction, you will have equal predictions of likelihood.  </p>

<p>It is my belief that if you have that much data, a ""traditional"" Frequentist approach is (typically) preferable to the Bayesian approach (of course others will disagree, especially with choosing between statistical philosophies rather than sticking to one and selecting an appropriate method). Note that it is entirely possible (and occurs often) that the Frequentist approach yields similar/identical results to the Bayesian.  </p>

<p>That said, when the difference in methods is a line of code, why not implement multiple methods and compare the results yourself?</p>
",2013-10-23 20:34:04.577
58133,2666.0,2,,45280.0,,,,CC BY-SA 3.0,"<p>Had you had a larger sample size, Hoeffding's D test is the most general one and is nonparametric.  It will even detect non one-to-one relationships (e.g., a circle).  This test is implemented in the R <code>Hmisc</code> package <code>hoeffd</code> function.</p>
",2013-10-23 21:40:29.270
58134,16644.0,2,,58126.0,,,,CC BY-SA 3.0,"<p>One of the reasons MAE is used in time series or forecasting is that non-scientists find it easy to understand. So if you tell your client the MAE is 1.5 units, for example, he/she can interpret that as the average amount that the forecast is in error (in absolute units). But if you tell them the MSE you may well get a blank look because it has no such interpretation. </p>

<p>I'm not sure what causes the confusion between MAE and mean absolute deviation, but I'd attribute it to a lack of clear definitions or explanations in the specific context where it is used. </p>
",2013-10-23 22:08:16.910
58135,15025.0,1,,,,Is there a notion of the product of confidence bands?,<confidence-interval>,CC BY-SA 3.0,"<p>Given two confidence bands, it seems natural to consider the confidence band of the product of these two distributions. Does this have a name? Are there any reference that might be helpful regarding this notion?</p>

<p>Edit: Assume that the errors of the first, say, regression curve are not correlated with the errors of the second. </p>
",2013-10-23 22:17:47.940
58136,594.0,2,,57893.0,,,,CC BY-SA 3.0,"<p>Here's the actual pdf required:</p>

<p>$$f(x)=\begin{cases}
  c\quad &amp;|x|\leq x_0\\
   c_1 \exp(-|x|)\quad &amp;|x|&gt; x_0
\end{cases}$$
that is the probability is constant between $(-x_0,x_0)$ and falls exponentially outside it, and where $c_1$ and $c$ are such that the pdf is continuous and integrates to 1.</p>

<p>That is, we have the conditions that $c_1\exp(-x_0) = c$; i.e. $c_1 = c\exp(x_0)$ and that the $\int_{-\infty}^\infty f(x) dx = 1$.</p>

<p>\begin{eqnarray}
\int_{-\infty}^\infty f(x) dx 
&amp;=&amp; 2c\left( \int_{0}^{x_0} 1 dx  +  \exp(x_0)\int_{x_0}^\infty \exp(-x) dx \right)\\
&amp;=&amp; 2c\left( x|_{0}^{x_0} - \exp(x_0)\exp(-x)|_{x_0}^\infty \right)\\
&amp;=&amp; 2c\left( x_0 - \exp(x_0)[0-\exp(-x_0)] \right)\\
&amp;=&amp; 2c( x_0 +1 )
\end{eqnarray}</p>

<p>Implying $c=\frac{1}{2( x_0 +1 )}$. Here's $f$ for $x_0=1$ (black) and $x_0=2$ (green):</p>

<p><img src=""https://i.stack.imgur.com/BUVts.png"" alt=""enter image description here""></p>

<p>As a quick check on our algebra, via approximate numerical integration the area under both curves seems to be 1 to about the right number of figures.</p>

<p>Now we have the pdf right, we can write the cdf:</p>

<p>$$F(x)=
\begin{cases}
c\exp(x+x_0)&amp;x\leq -x_0\\
1/2+cx&amp; -x_0&lt;x&lt;x_0\\
1/2+cx_0+c(1-\exp(x_0-x))&amp; x\geq x_0
\end{cases}$$</p>

<p>(though there are perhaps better ways to write that last term; I'm not going to investigate that here)</p>

<p>Here are the corresponding cdfs for the above two cases:</p>

<p><img src=""https://i.stack.imgur.com/xTd6T.png"" alt=""enter image description here""></p>

<p>Here's the quantile function, $Q(p) = F^{-1}(p)$:</p>

<p>$$Q(p) = 
\begin{cases}
\ln(p/c)-x_0  &amp; 0&lt;p\leq 1/2 -cx_0\\
(p-.5)/c      &amp; 1/2-cx_0&lt;p&lt;1/2+cx_0\\
x_0-\ln(1-[(p - 0.5)/c - x_0]) &amp; 1/2+cx_0\leq p&lt;1
\end{cases}$$</p>

<p>and a drawing of the inverse cdf for the same cases as above:</p>

<p><img src=""https://i.stack.imgur.com/5bqtn.png"" alt=""inverse cdf""></p>

<p>I wouldn't have got these right without drawing diagrams along the way, especially of $f$ and $F$.</p>
",2013-10-23 22:22:06.500
58137,19374.0,1,58184.0,,,Diagnosing collinearity in a Cox proportional hazards model,<r><prediction><multicollinearity>,CC BY-SA 3.0,"<p>I am building a Cox Proportional Hazards Model to predict the survival outcome of seabird faced with predation pressure. I have 6 factor variables with two or three levels each that I have predicted to affect survival. Three of which are management relevant (they can be manipulated by wildlife managers to increase or decrease survival if significant). The ultimate goal of the model is prediction but I would like to include the management relevant variables as well even if not significant. How can I check for multicollinearity among my variables. I am using program R for the analysis.</p>
",2013-10-23 22:39:16.600
58138,1506.0,1,58244.0,,,Matching X'X with Wishart Samples in R,<wishart-distribution>,CC BY-SA 3.0,"<p>$X'X \sim Wishart(\Sigma,n)$, however I'm having a tough time producing this in R.</p>

<p>Example:</p>

<pre><code>data=cbind(rnorm(100,10,5),rnorm(100,5,2),rnorm(100,-4,3))

X=cbind(rnorm(1,10,5),rnorm(1,5,2),rnorm(1,-4,3))
t(X)%*%X
rWishart(10,99,cov(data))
</code></pre>

<p>The data generated from rWishart is not close to $X'X$.  What am I doing wrong?  The help documentation mentions $\Sigma$ should be a scaled matrix, however I'm unsure what this is.</p>
",2013-10-23 22:55:44.577
58139,8374.0,1,,,,R: Multivariate T distribution Sampling,<distributions><simulation><t-distribution>,CC BY-SA 3.0,"<p>In the question <a href=""https://stackoverflow.com/questions/18153450/generating-random-variables-from-the-multivariate-t-distribution"">https://stackoverflow.com/questions/18153450/generating-random-variables-from-the-multivariate-t-distribution</a>, I am confused by why the answer requires we modify the <code>Sigma</code> in such a way that we need to multiply is by <code>(D-2)/D</code>. Here sigma is the covariance matrix for me. The answer also mentions that the correlation matrix is defined when df > 2, shouldn't it be df>= 2? This is because the correlation coefficient can't be calculated when the data is continue itself one, there must be more than 1 series. Am I interpreting this correctly??</p>
",2013-10-23 23:15:32.967
58140,22449.0,1,,,,How do you derive the Success-Run Theorem from the traditional form of Bayes Theorem?,<reliability><bayes><confidence-interval>,CC BY-SA 3.0,"<p>In my industry it is common to test a sample of 20-30 and then use that data to draw conclusions about the reliability of the product with a certain confidence.  We have tables for such things but it appears that for the case of 0 failures in the sample, the ""Success Run Theorem"" is used.  In my references this appears as:
$$R_c = (1-C)^{\frac{1}{(n+1)}}$$
where
$C$ = confidence level,
$R$ = reliability at confidence level $C$, and
$n$ = sample size.</p>

<p>However, I cannot find an explanation of how to get to the above equation from Bayes' theorem: </p>

<p><img src=""https://i.stack.imgur.com/KH5f9.png"" alt=""enter image description here""></p>

<p>Every attempt to talk myself through Bayes theorem to arrive at the Success Run Theorem gets me confused.  Even more confusing is when I try to extend my understanding to cases where some failures are observed in the sampling.  Then I know to use this formula: </p>

<p><img src=""https://i.stack.imgur.com/22Fvx.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/9rogV.jpg"" alt=""enter image description here""></p>

<p>But again I don't understand where it comes from (binomial?) or how it relates to the above two other formulas, if at all.  </p>

<p><strong>My specific question would be how you go from Bayes Theorem (written as probabilities) to the Success Run Theorem (written as confidence, reliability, sample size)?</strong></p>

<p>Thank you for helping a poor engineer lost in the world of stats.</p>
",2013-10-23 23:21:54.180
58141,10469.0,1,58146.0,,,How do I test that two continuous variables are independent?,<hypothesis-testing><references><independence>,CC BY-SA 3.0,"<p>Suppose I have a sample $(X_n,Y_n), n=1..N$ from the joint distribution of $X$ and $Y$. How do I test the hypothesis that $X$ and $Y$ are <strong>independent</strong>? </p>

<p>No assumption is made on the joint or marginal distribution laws of $X$ and $Y$ (least of all joint normality, since in that case independence is identical to correlation being $0$).</p>

<p>No assumption is made on the nature of a possible relationship between $X$ and $Y$; it may be non-linear, so the variables are <em>uncorrelated</em> ($r=0$) but <em>highly co-dependent</em> ($I=H$).</p>

<p>I can see two approaches:</p>

<ol>
<li><p>Bin both variables and use <a href=""http://en.wikipedia.org/wiki/Fisher%27s_exact_test"">Fisher's exact test</a> or <a href=""http://en.wikipedia.org/wiki/G-test"">G-test</a>.</p>

<ul>
<li>Pro: use well-established statistical tests</li>
<li>Con: depends on binning</li>
</ul></li>
<li><p>Estimate the <em>dependency</em> of $X$ and $Y$: <a href=""http://en.wikipedia.org/wiki/Mutual_information"">$\frac{I(X;Y)}{H(X,Y)}$</a> (this is $0$ for independent $X$ and $Y$ and $1$ when they completely determine each other).</p>

<ul>
<li>Pro: produces a number with a clear theoretical meaning</li>
<li>Con: depends on the approximate entropy computation (i.e., binning again)</li>
</ul></li>
</ol>

<p>Do these approaches make sense?</p>

<p>What other methods people use?</p>
",2013-10-23 23:54:16.613
58142,14748.0,1,,,,Machine-learning input data distribution,<machine-learning><distributions><dataset>,CC BY-SA 3.0,"<p>I'm trying to build a binary 1/0 ML classification algorithm, and was thinking about how to set up the input dataset. If the event I want to predict (the 1's) occur relatively less frequently in the total data than the 0's, does it makes sense to pare the dataset in such a way to get a more equal distribution of 1's and 0's? Would that be falsely representing the data to the algorithm? What are the disadvantages to doing so?</p>
",2013-10-24 00:15:29.007
58162,15658.0,1,58164.0,,,When to use residual plots?,<regression><residuals>,CC BY-SA 3.0,"<p>I have performed a simple regression analysis between one dependent variable (DV) and one explanatory variable (IV).</p>

<p>If the p-value from the regression analysis for the IV is not significant, should I still use residual plots to verify that the regression model used was correct (and the statement of non-significant IV is correct)?</p>

<p>Or should one use residual plots only for models that include significant IVs?</p>
",2013-10-24 13:13:57.760
58143,5448.0,2,,58131.0,,,,CC BY-SA 3.0,"<p>We don't use MCMC to calculate the $p(\theta | y)$ for each value (or many values) of $\theta$.  What MCMC (or the special case of Gibbs sampling) does is generate a (large) random sample from $p(\theta | y)$.  Note that $p(\theta | y)$ is not being calculated; you have to do something with that vector (or matrix) of random numbers to estimate $p(\theta)$.  Since you're not calculating $p(\theta)$ for lots of values of $\theta$, you don't need a Gibbs (or MCMC) loop inside a $\theta$ loop - just one (long) Gibbs (or MCMC) loop.</p>

<p>EDIT in response to an update to the question:  We do not need to integrate the distribution to get the constant of integration (CoI)!  The whole value of MCMC is is found in situations where we can't calculate the CoI.  Using MCMC, we can still generate random numbers from the distribution.  If we could calculate the CoI, we could just calculate the probabilities directly, without the need to resort to simulation.</p>

<p>Once again, we are NOT calculating $p(\theta|y)$ using MCMC, we are generating random numbers from $p(\theta|y)$ using MCMC.  A very different thing.</p>

<p>Here's an example from a simple case: the posterior distribution for the scale parameter from an Exponential distribution with a uniform prior.  The data is in <code>x</code>, and we generate <code>N &lt;- 10000</code> samples from the posterior distribution.  Observe that we are only calculating $p(x|\theta)$ in the program.</p>

<pre class=""lang-r prettyprint-override""><code>x &lt;- rexp(100)

N &lt;- 10000
theta &lt;- rep(0,N)
theta[1] &lt;- cur_theta &lt;- 1  # Starting value
for (i in 1:N) {
   prop_theta &lt;- runif(1,0,5)  # ""Independence"" sampler
   alpha &lt;- exp(sum(dexp(x,prop_theta,log=TRUE)) - sum(dexp(x,cur_theta,log=TRUE)))
   if (runif(1) &lt; alpha) cur_theta &lt;- prop_theta
   theta[i] &lt;- cur_theta
}

hist(theta)
</code></pre>

<p>And the histogram:</p>

<p><img src=""https://i.stack.imgur.com/4ZNFl.png"" alt=""Posterior distribution of $\theta$""></p>

<p>Note that the logic is simplified by our choice of sampler (the <code>prop_theta</code> line), as a couple of other terms in the next line (<code>alpha &lt;- ...</code>) cancel out, so don't need to be calculated at all.  It's also simplified by our choice of a uniform prior.  Obviously we can improve this code a lot, but this is for expository rather than functional purposes.</p>

<p>Here's a <a href=""https://stats.stackexchange.com/questions/5885/mcmc-method-good-sources"">link</a> to a question with several answers giving sources for learning more about MCMC.</p>
",2013-10-24 00:41:29.560
58144,22997.0,2,,32317.0,,,,CC BY-SA 3.0,"<p><a href=""http://jwork.org/scavis"" rel=""nofollow"">SCaVis</a> data analysis framework has a built-in clustering program with a nice GUI to show clusters and their centers. There is a number of cluster algorithms available (k-means, fuzzy etc.)</p>
",2013-10-24 01:25:36.453
58145,449.0,2,,58141.0,,,,CC BY-SA 3.0,"<p>Rarely (never?) in statistics can you demonstrate that your sample statistic = a point value. You can test against point values and either exclude them or not exclude them. But the nature of statistics is that it is about examining variable data. Because there is always variance then there will necessarily be no way to know that something is exactly not related, normal, gaussian, etc. You can only know a range of values for it. You could know if a value is excluded from the range of plausible values. For example, it's easy to exclude no relationship and give range of values for how big the relationship is.</p>

<p>Therefore, trying to demonstrate no relationship, essentially the point value of <code>relationship = 0</code> is not going to meet with success. If you have a range of measures of relationship that are acceptable as approximately 0. Then it would be possible to devise a test.</p>

<p>Assuming that you can accept that limitation it would be helpful to people trying to assist you to provide a scatterplot with a lowess curve. Since you're looking for R solutions try:</p>

<pre><code>scatter.smooth(x, y)
</code></pre>

<p>Based on the limited information you've given so far I think a generalized additive model might be the best thing for testing non-independence. If you plot that with CI's around the predicted values you may be able to make statements about a belief of independence. Check out <code>gam</code> in the mgcv package. The help is quite good and there is assistance here regarding the <a href=""https://stats.stackexchange.com/q/33327/601"">CI</a>.</p>
",2013-10-24 01:53:45.937
58146,7483.0,2,,58141.0,,,,CC BY-SA 4.0,"<p><em>(Answer partially updated in 2023.)</em></p>
<p>This is a very hard problem in general, though your variables are apparently only 1d so that helps. Of course, the first step (when possible) should be to plot the data and see if anything pops out at you; you're in 2d so this should be easy.</p>
<p>Here are a few approaches that work in <span class=""math-container"">$\mathbb{R}^d$</span> or even more general settings, to match the general title of the question.</p>
<p>One general category is, related to the suggestion here, to estimate the mutual information:</p>
<ul>
<li>Estimate mutual information via entropies, as mentioned. In low dimensions with sufficient samples, histograms / KDE / nearest-neighbour estimators should work okay, but expect them to behave very poorly as the dimension increases. In particular, the following simple estimator has finite-sample bounds (compared to most approaches' asymptotic-only properties):</li>
</ul>
<blockquote>
<p>Sricharan, Raich, and Hero. <a href=""https://arxiv.org/abs/1012.4188"" rel=""nofollow noreferrer""><em>Empirical estimation of entropy functionals with confidence.</em></a> arXiv:1012.4188 [math.ST]</p>
</blockquote>
<ul>
<li>Similar direct estimators of mutual information, e.g. the following based on nearest neighbours:</li>
</ul>
<blockquote>
<p>PÃ¡l, PÃ³czos, and SvepesÃ¡ri. <a href=""https://arxiv.org/abs/1003.1954"" rel=""nofollow noreferrer""><em>Estimation of RÃ©nyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</em></a>, NeurIPS 2010.</p>
</blockquote>
<ul>
<li>Variational estimators of mutual information, based on optimizing some function parameterized typically as a neural network; this is probably the &quot;default&quot; modern approach in high dimensions. The following paper gives a nice overview of the relationship between various estimators. Be aware, however, that these approaches are highly dependent on the neural network class and optimization scheme, and can have <a href=""https://arxiv.org/abs/1910.06222"" rel=""nofollow noreferrer"">particularly surprising behaviour in their bias/variance tradeoffs</a>.</li>
</ul>
<blockquote>
<p>Poole, Ozair, van den Oord, Alemi, and Tucker. <a href=""https://arxiv.org/abs/1905.06922"" rel=""nofollow noreferrer""><em>On Variational Bounds of Mutual Information</em></a>, ICML 2019.</p>
</blockquote>
<p>There are also other approaches, based on measures other than the mutual information.</p>
<ul>
<li>The Schweizer-Wolff approach is a classic one based on copula transformations, and so is invariant to monotone increasing transformations. I'm not very familiar with this one, but I think it's computationally simpler but also maybe less powerful than most of the other approaches here. (I vaguely expect it can be framed as a special case of some of the other approaches but haven't really thought about it.)</li>
</ul>
<blockquote>
<p>Schweizer and Wolff, <a href=""https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-4/On-Nonparametric-Measures-of-Dependence-for-Random-Variables/10.1214/aos/1176345528.full"" rel=""nofollow noreferrer""><em>On Nonparametric Measures of Dependence for Random Variables</em></a>, Annals of Statistics 1981.</p>
</blockquote>
<ul>
<li>The Hilbert-Schmidt independence criterion (HSIC): a kernel (in the sense of RKHS, not KDE)-based approach, based on measuring the norm of <span class=""math-container"">$\operatorname{Cov}(\phi(X), \psi(Y))$</span> for kernel features <span class=""math-container"">$\phi$</span> and <span class=""math-container"">$\psi$</span>.  In fact, the HSIC with kernels defined by a deep network is related to one of the more common variational estimators, InfoNCE; see <a href=""https://arxiv.org/abs/2106.08320"" rel=""nofollow noreferrer"">discussion here</a>.</li>
</ul>
<blockquote>
<p>Gretton, Bousqet, Smola, and SchÃ¶lkopf, <a href=""https://link.springer.com/chapter/10.1007/11564089_7"" rel=""nofollow noreferrer""><em>Measuring Statistical Independence with Hilbert-Schmidt Norms</em></a>, Algorithmic Learning Theory 2005.</p>
</blockquote>
<ul>
<li>Statisticians are probably more familiar with the distance covariance/correlation as mentioned here previously; this is in fact <a href=""http://arxiv.org/abs/1207.6076"" rel=""nofollow noreferrer"">a special case</a> of the HSIC with a particular choice of kernel, but that choice is maybe often a better kernel choice than the default Gaussian kernel typically used for HSIC.</li>
</ul>
<blockquote>
<p>SzÃ©kely, Rizzo, and Bakirov, <a href=""https://arxiv.org/abs/0803.4101v1"" rel=""nofollow noreferrer""><em>Measuring and testing dependence by correlation of distances</em></a>, Annals of Statistics 2007.</p>
</blockquote>
",2013-10-24 02:16:41.853
58147,22998.0,1,,,,Detrending Discrete Data,<regression><discrete-data><bias>,CC BY-SA 3.0,"<p>I am trying to detrend some discrete data and I am having difficulty finding a model to describe the trend. There is a number of discrete data points and there is a linear error being introduced with time. I created some representative data in excel:</p>

<p>Unbiased:</p>

<p><img src=""https://i.stack.imgur.com/4fv72.png"" alt=""Unbiased""></p>

<p>Biased data:</p>

<p><img src=""https://i.stack.imgur.com/02Qit.png"" alt=""Biased""></p>

<p>The discrete data here is 1,2,3 with a positive linear bias over time. Traditional least squares does a poor job, orthogonal least squares does no better. I tried a few other models with no success. I have been able to visually inspect the trend and select some of the ~2s, perform a linear regression, and then subtract the bias. I would like to programmatically do this across many data sets, so a  generic way to do this reliability is essential.  Any ideas on how to detrend the data and remove the bias?</p>

<p>Actual data: <a href=""https://github.com/bkief/python-hipster/blob/master/data.txt"" rel=""nofollow noreferrer"">data</a></p>
",2013-10-24 02:45:42.290
58148,7155.0,2,,58142.0,,,,CC BY-SA 3.0,"<p>Define an accuracy metric that reasonably models how you want your algorithm to perform. </p>

<p>Once you have a metric in hand you can cross-validate this question and see if it is an improves the performance.</p>

<p>Some common accuracy metrics that model the problem different: Normalized mutual information, Gini on the labels argsorted by the probabilities, Precision, Recall, AUC.</p>

<p>If the classes are extremely unbalanced and FN are crucial, you'll see an improvement.</p>
",2013-10-24 04:00:00.463
58149,22564.0,1,58150.0,,,Possible Paradox: Calculating a confidence interval with within-experiment error,<confidence-interval><mixed-model><error-propagation><paradox>,CC BY-SA 3.0,"<p>This is a spinoff of </p>

<p><a href=""https://stats.stackexchange.com/questions/12002/how-to-calculate-the-confidence-interval-of-the-mean-of-means?lq=1"">How to calculate the confidence interval of the mean of means?</a></p>

<p>and related to</p>

<p><a href=""https://stats.stackexchange.com/questions/72573/when-making-inferences-about-group-means-are-credible-intervals-sensitive-to-wi?lq=1"">When making inferences about group means, are credible Intervals sensitive to within-subject variance while confidence intervals are not?</a></p>

<p>Dataset 1 here is taken from the first link above. Dataset 2 has the approximately the same experimental means but different within experiment variance. My first question is:</p>

<p>1) How do I calculate a confidence interval for the overall mean for each of these data sets?</p>

<p>If I understand @StÃ©phane Laurent's answer in the two linked questions they should be the same.If that is true this goes strongly against all my scientific intuition and also appears to be a paradox. </p>

<p>2) How can it be that the confidence interval is apparently both sensitive to and not sensitive to within experiment error?</p>

<p><img src=""https://i.stack.imgur.com/zvYXu.png"" alt=""enter image description here""></p>

<p>dataset 1:</p>

<pre><code>  Experiment Value
          1    34
          1    41
          1    39
          2    45
          2    51
          2    52
          3    29
          3    31
          3    35

structure(list(Experiment = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 
3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor""), Value = c(34, 
41, 39, 45, 51, 52, 29, 31, 35)), .Names = c(""Experiment"", ""Value""
), row.names = c(NA, -9L), class = ""data.frame"")
</code></pre>

<p>dataset2:</p>

<pre><code>  Experiment    Value
          1 38.20744
          1 37.99410
          1 37.96299
          2 49.27085
          2 49.40519
          2 49.24894
          3 31.81259
          3 31.73708
          3 31.73834

structure(list(Experiment = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 
3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor""), Value = c(38.2074373061779, 
37.9941025108851, 37.9629896019425, 49.2708491636015, 49.4051867974062, 
49.2489418702291, 31.8125943239769, 31.7370826901692, 31.7383364604132
)), .Names = c(""Experiment"", ""Value""), row.names = c(NA, -9L), class = ""data.frame"")
</code></pre>
",2013-10-24 04:23:26.413
58150,449.0,2,,58149.0,,,,CC BY-SA 3.0,"<p>I think you're confusing things with a manufactured example. Yes, you could have a specific case where the two CI's matched and you could have ones where the Dataset 1 CI was lower but on average the Dataset 1 CI will be higher. Furthermore, if these really are supposed to be multiple experiments tackling the same problem (within each Dataset) then there's something seriously wrong with Dataset 2. Lower within experiment variability should be leading to lower between experiment variability.</p>
",2013-10-24 04:46:24.827
58163,23014.0,2,,33598.0,,,,CC BY-SA 3.0,"<p>Ho: Parameter is  structurally stable when your probability is less than 5% as in your case it is 0.0016 then do reject Ho (i.e., the null hypothesis). It means there is a structural break in your data. The best thing you can do is check each variable one by one by using the chow test.</p>
",2013-10-24 13:43:49.420
58151,668.0,2,,58122.0,,,,CC BY-SA 3.0,"<p>Correlation, according to <a href=""http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Geometric_interpretation"" rel=""nofollow"">its usual definition,</a> is the cosine of the angle between vectors.  Least squares regression decomposes a vector into a component within the linear span of a given set of vectors and an orthogonal component (the ""residuals"" or ""errors"").  Those two components, being at a right angle to each other, have zero correlation.  Therefore,</p>

<ul>
<li><p>A meaningful measure of the ""degree of correlation between those parts of $A$ and $B$ that are not correlated with $C$"" is the correlation of the residuals of $A$ and $B$ with each other when regressed separately against $C$.</p></li>
<li><p>The part of $A$ that is maximally correlated with $B$ is the projection of $A$ onto $B$ (the least-squares fit) and the part that is ""maximally uncorrelated"" is the residual of the regression of $A$ against $B$.</p></li>
</ul>

<hr>

<p>The example in the question suggests that ""correlation"" may be used there in a less conventional but undefined sense.  Note that the residuals of the regressions of $A$ and $B$ on $C$ when all are binary will usually not be binary.</p>
",2013-10-24 05:29:43.127
58152,22507.0,2,,58105.0,,,,CC BY-SA 3.0,"<p>Let $z_i$ be true value of $y_i$.  If $z_i \sim N(y_i, \Delta y_i)$, and the theory predicts $z_i=a x_i^2$ then you should minimize $$\sum_i \left( {a x_i^2 + b x_i + c - y_i \over \Delta y_i} \right)^2$$</p>

<p>This is a weighted linear regression problem.</p>
",2013-10-24 07:24:14.877
58153,22957.0,1,58155.0,,,How to specify/restrict the sign of coefficients in a GLM or similar model in R,<r><bayesian><generalized-linear-model><predictive-models>,CC BY-SA 3.0,"<p><strong>The situation:</strong> I'm struggling with a predictive analysis of food sales prices using a generalized linear model. My dataset contains different kinds of food (cheeses, vegetables, meats, spices etc.) and hence I am splitting the dataset completely by these kinds when doing the analysis, because they are very different by nature.</p>

<p><strong>The current model:</strong> The dataset/model contains both factors such as ""country of production"" and numeric variables such as ""transport distance"" which is all used in the gamma based GLM i R.</p>

<p><strong>The problem:</strong> Now in general my model fits pretty well, however sometimes in rare cases some of the metric variables gets the opposite sign (+/-) than you would expect it to have, because the model somehow catches other influences.</p>

<p><strong>An example:</strong> An example would be spices. All spices have a relative long ""transport distance"" and a relative long shelf life and hence a pretty small impact on the sales price compared to e.g. meat. So in this case the model might by accident end up giving the ""transport distance"" variable a small but negative value - which is of cause wrong because it would mean that the longer the distance the food was transported the lower the price would be.</p>

<p><strong>My question:</strong> What kind of model should I use in R if I wan't something similar to a GLM model but I want to be able to specify restrictions on some of the variables/coefficients? E.g. if I want to say that an increased ""transport distance"" should ALWAYS have a positive impact on the sales price?</p>

<p><strong>Ideas:</strong> I have heard something about both ""Bayesian GLM"" models or using a so called ""prior distribution"" but I have no idea which one, if any, would be the best to use..?</p>

<p><strong>UPDATE</strong>
The answer below by @ACD is not, exactly what I'm looking for. I don't need an explanation of WHY this occurs, I need a solution to restricting the coefficient signs :-)</p>
",2013-10-24 07:41:50.433
58154,22339.0,1,,,,Graphical Multiple Linear Regression in Stata,<data-visualization><multiple-regression><stata>,CC BY-SA 3.0,"<p>Consider this standard linear regression model:</p>

<p>$Y = \beta_0+\beta_1X_1+\cdots+\beta_pX_p+\epsilon$</p>

<p>I've loaded such a dataset into Stata 12.0, so I have some variables $y,x_1,x_2,\dots,x_p$. How do I produce a plot, like I would with <code>. scatter y x</code> for a simple linear regression model? </p>
",2013-10-24 08:04:22.503
58155,12787.0,2,,58153.0,,,,CC BY-SA 3.0,"<p>The negative estimated coefficient on something that you KNOW is positive comes from omitted variable bias and/or colinearity between your regressors.  </p>

<p>For prediction, this isn't so problematic, so long as you are sampling new data to predict the outcome (price?) of from the same population as your sample.  The negative coefficient comes because the variable is highly correlated with something else, making the coefficient estimate highly variable, OR because it is correlated with something important that is omitted from your model, and the negative sign is picking up the effect of that omitted factor.  </p>

<p>But it sounds like you are also trying to do inference -- how much does an exogenous change in $X$ change $Y$.  Causal inferential statistics uses different methods and has different priorities than predictive statistics.  It is particularly well developed in econometrics.  Basically you need to find strategies such that you can convince yourself that $E(\hat\beta|X,whatever)=\beta$, which generally involves making sure that the regressor of interest is not correlated with the error term, which is generally accomplished by controlling for observables (or unobservables in certain cases).  Even if you get to that point however, colinearity will still give you highly variable coefficients, but negative signs on something that you KNOW is positive will generally come with huge standard errors (assuming no omitted variable bias).</p>

<p>Edit:  if your model is </p>

<p>$$
price = g^{-1}\left(\alpha + country'\beta + \gamma distance + whatever + \epsilon\right)
$$</p>

<p>then country will be correlated with distance.  hence, if you are in Tajikistan and you are getting a spice from Vanuatu, then the coefficient on Vanuatu will be really high.  After controlling for all of these country effects, the additional effect of distance may well not be positive.  In this case, if you want to do inference and not prediction (and think that you can specify and estimate a model that gives a causal interpretation), then you may with to take out the country variables.</p>
",2013-10-24 08:10:26.823
58156,,1,,,user31790,How to compute whether the mean of a non-random sub-sample is statistically different from the mean of the sample?,<regression><categorical-data><mean><sample>,CC BY-SA 3.0,"<p>I have a variable called ""obs"" and from this variable I generated a new variable called ""obs_sub"" by excluding all observations for which a dummy variable is equal to one. Now what I want to know is if I remove these observations whether the mean of the sub-sample is equal to the mean of the sample. This is my null hypothesis. </p>

<p>I could run a regression of $E[obs|d] = \alpha + \beta \cdot d$, but the regression with the dummy $(d)$ will only tell me whether the coefficient on the dummy is statistically significant from zero. It will not tell me whether the data are unlikely given $E[obs|d=1] = E[obs]$ or is the null hypothesis $\beta=0$ the same as the null hypothesis as $E[obs|d=1] = E[obs]$?</p>

<p>Cheers,</p>

<p>Martin</p>
",2013-10-24 08:43:06.080
58157,12683.0,2,,58119.0,,,,CC BY-SA 3.0,"<p>It might be true for your example (or it might not&mdash;plant No. 4 could be an etiolated seedling elongating its stem in a hunt for light without expanding its girth), but it's a matter of prior botanical knowledge rather than any general statistical law. It's not hard to construct counter-examples: someone who's an outlier on hours per week spent exercising is less likely to be an outlier on blood pressure. Any type of dependence is possible in principle&mdash;Chaos Theory doesn't say the butterfly that flaps its wings hardest causes the biggest hurricane (I don't know what Zen Buddhism says). A general theory that ranges over all measurements anyone might want to make on all kinds of things is going to be elusive.</p>

<p>If you consider a world with a well-defined set of objects, $a_1, a_2, \ldots$, &amp; a well-defined set of measurements we can make on each object, $x_{11}, x_{12}, \ldots, x_{21}, x_{22}\ldots$, then your question could perhaps be framed in an answerable way (""Pick $n$ objects at random from the $a$s, pick two measurements at random from the $x$s, ...""); for our world I don't see how it can be.</p>
",2013-10-24 10:04:05.427
58158,503.0,2,,58156.0,,,,CC BY-SA 3.0,"<p>A better null hypothesis (and substantively equivalent) is to see whether the subsample's mean is equal to the mean of all the <em>other</em> cases. That is, divide the sample into two mutually exclusive subsamples. Then you can run a t-test between them (or some nonparametric variation, if need be). </p>

<p>If this variation of the null isn't workable, could you say why not?</p>

<p>And, if it isn't, you could do a permutation test</p>
",2013-10-24 10:54:18.750
58164,750.0,2,,58162.0,,,,CC BY-SA 3.0,"<p>They are still useful in assessing whether the relationship between the explanatory variables and the dependent variable is linear (or modeled properly given the equation). For an extreme example, I generated some data with a quadratic relationship and fit a linear regression of the form $Y = \alpha + \beta(X) + e$. (Because the parabola is approximately centered on zero $\beta$ is insignificant in the equation). </p>

<p><img src=""https://i.stack.imgur.com/Bkgwy.png"" alt=""enter image description here""></p>

<p>If you plot $X$ versus the residuals though the quadratic relationship is still very clear. (Imagine just detilting the first plot.)</p>

<p><img src=""https://i.stack.imgur.com/Kfkt7.png"" alt=""enter image description here""></p>

<p>I'm sure you can dream up other scenarios in which regression coefficients are insignificant but examining the residuals will show how the model is inadequate.</p>
",2013-10-24 13:44:19.557
58165,22479.0,1,,,,Combining posterior probabilities from multiple classifiers,<bayesian><classification><conditional-probability>,CC BY-SA 3.0,"<p>I am new to machine learning and can't get my head around this problem. I have two patient datasets, the first ($D_1$) contains $Y,Z,X$ that convey blood-sample information and the second ($D_2$) contains  $W,T,X$ that convey x-ray information. In both datasets, $X$ is the common diagnostic output. </p>

<p>Since there are two distinct datasets, I modelled a solution with two Naive Bayes models as below, where $X$ is the common dependent variable. ( I am aware I can use other techniques than Naive Bayes but that is not the aim of my question.)</p>

<ul>
<li>$P(X|Y,Z,D_1) \propto P(Y|X,D_1) \cdot P(Z|X,D_1) \cdot P(X,D_1)$ </li>
<li>$P(X|W,T,D_2) \propto P(W|X,D_2) \cdot P(T|X,D_2) \cdot P(X,D_2)$ </li>
</ul>

<p>I want to combine the posterior probabilities of X in these models: $P(X|Y,Z)$ and $P(X|W,T)$ to determine an overall outcome probability $P(X|Y,Z,W,T)$ (diagnostic outcome) of a new patient. </p>

<p><strong>How can I combine these probabilities?</strong> </p>

<p>Can it be done as below?</p>

<p>$P(X|Y,Z,W,T) \propto \frac{P(X|Y,Z) * P(X|W,T)}{P(X,D_{combined})}$ </p>

<p>If so, what is the relationship between the priors $P(X,D_1)$, $P(X,D_2)$ and $P(X,D_{combined})$?</p>
",2013-10-24 14:06:01.187
58166,15827.0,2,,58162.0,,,,CC BY-SA 3.0,"<p>Assume for simplicity that you have fitted some line $\hat y = b_0 + b_1 x$ given a dependent or response variable $y$ and a predictor or independent variable $x$. This specific assumption can be relaxed, which we will get to in good time. </p>

<p>With one variable on each side, a residual plot (meaning, a plot of residual $y - \hat y =: e$ versus fitted or predicted $\hat y$) in principle shows just the same information as a scatter plot with regression line superimposed. On the latter, the residuals are just the vertical differences between the data points and the line and the fitted are the corresponding values on the line, i.e. for the same value of $x$. </p>

<p>In practice, a residual plot can make structure in the residuals more evident: </p>

<ul>
<li><p>The regression line is rotated to the horizontal. Seeing structure in anything is easiest when the reference indicating no structure is a horizontal straight line, here the line $e = 0$. </p></li>
<li><p>There is better use of space. </p></li>
</ul>

<p>In this easy example, some structure in the residuals is discernible in the scatter plot </p>

<p><img src=""https://i.stack.imgur.com/FZOo0.png"" alt=""enter image description here""></p>

<p>but even easier to see in the residual plot: </p>

<p><img src=""https://i.stack.imgur.com/HN2BZ.png"" alt=""enter image description here""></p>

<p>The recipe here was simple. The data were fabricated as a quadratic plus Gaussian noise, but the quadratic is only roughly captured by the naive linear fit. </p>

<p>But it is still generally true that structure is easier to see on a residual plot. Some caution is needed in not over-interpreting residual plots, especially with very small sample sizes. As usual, what you spot should make scientific or practical sense too. </p>

<p>What if the fitted is more complicated than $b_0 + b_1 x$? There are two cases: </p>

<ul>
<li><p>Everything can still be shown on a scatter plot, e.g. the right-hand side is a polynomial or something in trigonometric functions of $x$. Here, if anything, the residual plot is even more valuable in mapping everything so that zero residual is a reference. </p></li>
<li><p>The model uses two or more predictors. Here also the residual plot can be invaluable as a kind of health check showing how well you did and what you missed. </p></li>
</ul>

<p>The health check analogy is a fair one more generally: Residual plots can help you spot if something is wrong. If nothing is evidently wrong, no news is good news, but there is no absolute guarantee: something important may have been missed. </p>

<p>On whether the predictor had a significant effect, I know of no rule whatever for drawing or not drawing a residual plot. In the concocted example here, significance levels and figures of merit such as $R^2$ are extremely good, but the straight line model still misses a key part of the real structure. Conversely, a residual plot often illuminates why a model failed to work: either the pattern really is all noise, so far as can be seen, or your model misses something really important, such as some nonlinearity. </p>

<p>Footnote: for many statistical people IV means instrumental variable, not independent variable. </p>
",2013-10-24 14:12:26.123
58167,1406.0,2,,58109.0,,,,CC BY-SA 3.0,"<p>If you hold everything else constant, you assume that it is constant. So it does not matter that the independent variables are correlated and they might change when you change your variable of interest. The assumption was that <strong>everything else is constant</strong>. It is perfectly ok to question this assumption, but the interpretation holds nevertheless. Whether you should care about it is another matter. </p>
",2013-10-24 14:23:10.210
58168,5556.0,2,,57867.0,,,,CC BY-SA 3.0,"<p>This fact is called the data processing inequality. If the variables $X$, $O_1$ and $O_2$ form a Markov chain ($X \rightarrow O_1\rightarrow O_2$), then $Var [X|O_1] \leq Var[X|O_2] $, with equality iff  $E[X|O_1] = E[X|O_2] $.</p>

<p>You can prove it by using the Markov property and the fact that conditioning decreases variance (which is itself the consequence of the law of total variance).</p>

<p>See Theorem 11 in ""Functional Properties of Minimum Mean-Square Error and Mutual Information"",
Y. Wu, S. VerdÃº, IEEE Trans. Info. Theory, March 2012</p>

<p><a href=""http://www.princeton.edu/~verdu/reprints/WuVerIT2012f.pdf"" rel=""nofollow"">http://www.princeton.edu/~verdu/reprints/WuVerIT2012f.pdf</a></p>
",2013-10-24 15:26:10.970
58169,20304.0,1,,,,Machine Learning problem - identifying fake fraudulent names,<machine-learning><fraud-detection>,CC BY-SA 3.0,"<p>I have a dataset of fraudulent orders from some business.  Each order has a bunch of features such as order_amount, address, state, city, phone_number, and name.  Obviously a criminal would not be using his/her real name when making a fraudulent order.  So I was wondering if there was any sort of machine learning strategy to identify fake names.  I assume there must be some sort of underlying structure to how fake names are selected - so understanding this structure could allow me to identify them.  Unless the fake names are completely randomly selected.  Any thoughts on how to do this?</p>
",2013-10-24 15:55:46.143
58170,23017.0,2,,20667.0,,,,CC BY-SA 3.0,"<p>This <a href=""http://www.mathworks.com/matlabcentral/fileexchange/37435-generate-data-for-clustering/content/generateData.m"" rel=""nofollow"">Matlab script</a> generates 2D data for clustering. It accepts several parameters so that the generated data is within user requirements.</p>
",2013-10-24 15:59:38.697
58171,20304.0,2,,58131.0,,,,CC BY-SA 3.0,"<p>MCMC is a family of sampling methods (Gibbs, MH, etc.).  The point of MCMC is that you cannot sample directly from the posterior distribution that you mentioned.  The way MCMC works is a Markov Chain (the first MC in MCMC) is identified whose stationary distribution is the posterior that you are interested in.  You can sample from this Markov Chain and when it converges to its equilibrium distribution, you are essentially sampling from the posterior distribution that you are interested in. </p>
",2013-10-24 16:02:48.610
58172,23019.0,1,,,,ANOVA versus nonlinear fit,<regression><anova>,CC BY-SA 3.0,"<p>I have a data set which looks something like this (not real data):</p>

<pre><code>conc   Resp
0      5
0.1    18
0.2    20
0.3    23
0.4    24
0.5    24.5
0      5
0.1    17
..     ..
</code></pre>

<p>which happens to fit perfectly to the Michaelis-Menten equation:</p>

<blockquote>
  <p>Resp = max_value  *  conc / (conc_value_at_half_max + conc)</p>
</blockquote>

<p>Even though it is something else entirely importantly, the response increases quickly with ""conc"" and then reaches a ceiling or max value of sorts. </p>

<p>Anyway, I want to know how low I can go in ""conc"" before the value of ""Resp"" is not significantly lower than the max value. </p>

<p>Using a simple ANOVA accomplices this nicely, but I was thinking: ""should I not be exploiting the fact that the structure of the data is so nicely explained by a known equation?"" Is there such a way?</p>

<p>I am using minitab for this because it is easier, but would work in <code>R</code> all the time.</p>
",2013-10-24 17:03:46.683
58173,1575.0,1,,,,Overlap in time series training sets,<time-series><machine-learning><forecasting>,CC BY-SA 3.0,"<p>I have a time series prediction problem where the aim is to forecast the average value of $y_t$ over the next $T$ periods, given all the information available up to point $t$. For example, I want to forecast</p>

<p>$$\bar{y}_t = \frac{1}{T}\sum_{k=1}^T y_{t+k}$$</p>

<p>as a function of a bunch of other variables $x_t$ which are available at time $t$.</p>

<p>When building a training set from the data, I could ensure that I have no overlapping responses by considering</p>

<p>$$t = 0, T, 2T, 3T, \dots$$</p>

<p>However, I feel that this may not be making best use of the available data, and result in models with a lot of variance. An alternative is to use overlapping responses, for example </p>

<p>$$t=0, \tfrac{1}{2}T, T, \tfrac{3}{2}T, \dots$$</p>

<p>but I worry that this may create a lot of bias in the trained model.</p>

<p>Are there known results about how using overlapping data affects the bias/variance tradeoff? Is there a ""best"" level of overlap to use?</p>
",2013-10-24 17:08:18.863
58174,21804.0,2,,58160.0,,,,CC BY-SA 3.0,"<p>Each cluster center is a point in the dimensional space with as many dimensions as extracted factors from your PCA. The row you report is the value of one dimension for each of the cluster centers. If you want to report on the position of the cluster centers, then you need the full coordinates. It is misleading though, to say that a factor ""influences"" a cluster, as the clusters are built based on factor values, in other words there would be no clusters if you did not start out with those factors. </p>

<p>Whether 0 is a special position for a cluster center depends on the nature of the input variables, but even if you assume that 0 represents mean and median for the values on this dimension it is informative. To give an intutiive demosntration why a value of 0 is informative: imagine a cluster analysis based on a one-dimensional variablwe and you learn that the cluster centers are located at -3,0, and 5. Then you know something about the relative position of the cluster centers. If you ignored the factor you could not even describe the middle cluster. </p>

<p>I think the question might be motivated by the practice of dropping small factor laodings when reporting the results of PCAs, which is offered by some statistical packages (and of dubious value in many practical applications).   </p>
",2013-10-24 17:14:04.797
58175,19089.0,2,,57670.0,,,,CC BY-SA 3.0,"<p>As far as I can tell the fact that these are networks or graphs have no bearing on the problem you had in mind. Instead, it sounds like you have three binomial trials, with $n$ values $n_1, n_2, n_3$, respectively, and estimated proportions $\hat p_1 = \frac{a}{n_1}$, $\hat p_2 = \frac{d}{n_1}$, $\hat p_1 = \frac{g}{n_1}$, respectively, for this first criteria you're interested in. These you should be able to analyze with the usual $t$-test.</p>

<p>Beyond this, if you want to test all three graphs at once, recognize that you basically have a two-way anova. The blocks are which graph you're in, and the parameters are the treatment. The responses are of course 0 and 1, which you might consider remapping to -1 and 1. I recommend researching two-level factorial designs, as in Chapter 29 of <a href=""http://rads.stackoverflow.com/amzn/click/007310874X"" rel=""nofollow"">Applied Linear Statistical Models</a></p>
",2013-10-24 18:07:45.343
58176,12756.0,1,58178.0,,,Calculation of probability of selling a Poisson number of products,<self-study><poisson-distribution>,CC BY-SA 3.0,"<p>You sell 10 product on average every day. The model is based on a Poisson distribution.
To be enable to cover the indirect cost, the manager needs to sell 8 products per day.</p>

<p>Two questions:</p>

<blockquote>
  <ol>
  <li><p>What is the probability to sell between 0 - 5 products in a day?</p></li>
  <li><p>What is the probability to have at least 6 products in a single day?</p></li>
  </ol>
</blockquote>
",2013-10-24 19:04:53.360
58177,23018.0,1,,,,Sample size analysis for a study,<sample-size><medicine>,CC BY-SA 3.0,"<p>I am a clinical researcher and orthopedic surgeon and one of my PhD student is starting on the last part of her thesis. Her work is centered around the potential benefits of interprofessional teaching at an orthopedic ward. The study is a retrospective cohort study on register data for hip fracture patients. Our hypothesis is that an interprofessional ward (with medical students and nurse students guided by real doctors and nurses) does not put patient at higher risk for adverse events than a control ward. As the primary outcome variable we have a proxy variable for adverse events; readmission rate at 3 months. The number of patients treated at the interprofessional ward is about 1:4 compared to the control ward.</p>

<p>My question: Assuming a 3-month readmission rate of about 20% (about normal for this patient group) in the control ward, how large a sample size do I need when I aim at detecting a 5% difference in readmission rates? This is for 80% power and $p \le 0.05$. </p>

<p>When performing this calculation as a superiority calculation for proportions in for instance SamplePower 3,  I wind up at approximately 500 and 2000 in the interprofessional and control ward, respectively. Is this calculation correct, or should I do a non-inferiority analysis for proportions instead where the minimum clinical difference I am interested in is 5%? I cant find any such non-inferiority calculators where the number of subjects in the two groups are of different numbers, as in this study.</p>
",2013-10-24 19:08:11.140
58178,21182.0,2,,58176.0,,,,CC BY-SA 3.0,"<p>For a random variable $X \sim \textrm{Poisson}(\lambda)$, the probability of seeing a value of $k$ is:
$$
P_k = \frac{e^{-\lambda} \lambda^k}{k!}
$$
So the probability of selling less than or equal to $5$ products in a given day, where the long-term average is $8$ is:
$$
P_{0\ldots5}=\sum_{i=0}^5\frac{e^{-8} \lambda^i}{i!}
$$</p>

<p>The probability of at least $6$ is everything other than $0$ to $5$ or $(1-P_{0\ldots5})$ for which you were asked in the first part.</p>
",2013-10-24 19:18:01.523
58179,22564.0,1,,,,What is the best way to analyze data if your experimental design changes while running the experiment?,<hypothesis-testing><multiple-comparisons><experiment-design><p-value>,CC BY-SA 3.0,"<p>I imagine this is a somewhat common situation in practice. I am thinking mainly in terms of preclinical drug trials.</p>

<p>1) During the course of the study a new technique is learned or some time/money is freed up to be able to use a new technique as a secondary outcome. </p>

<p>2) Some key piece of equipment breaks, the lab member with the skill to perform a technique leaves, or a planned technique is looking like it is yielding inconsistent results for unknown reasons so that outcome is dropped for financial reasons. </p>

<p>3) When time comes to analyze data it comes to the knowledge of the researcher that the planned methods are inadequate/inappropriate and they wish to use a different approach.</p>

<p>4) Experiment 1 (e.g. test drug A) does not look like it will pan out, so a financial decision is made to stop that experiment and instead test drug B. Alternatively some other part of the experiment may be modified such as cell culture or animal strain.</p>

<p>5) Study is stopped prematurely or additional funds are freed up to increase sample size.</p>

<p>I can think of more, but you get the idea. Are p values of any use under these circumstances? Should corrections for multiple comparisons be made in the case of #4? At what point does deviation from the plan invalidate the hypothesis testing procedure?</p>

<p>The best way to me would be to simply plot the data and describe it in the hopes it may be useful for someone. However this behaviour is discouraged in favor of performing significance/hypothesis tests and the data may not be published if these are not included.</p>

<p><strong>EDIT:</strong>
I was thinking about it and the actual best thing to do is to take the data and come up with a model to explain it that makes a precise prediction that can then be tested. It seems like this should always be the best thing to do, though.</p>
",2013-10-24 19:26:11.043
58180,1809.0,1,58182.0,,,Chi Square Test for Independence in R and Python,<r><chi-squared-test><python>,CC BY-SA 3.0,"<p>Consider the following R code and output:</p>

<pre><code>row1 = c(0,23,0,0)
row2 = c(0,1797,0,0)
data.table = rbind(row1, row2)
chisq.test(data.table)

    Pearson's Chi-squared test

data:  data.table
X-squared = NaN, df = 3, p-value = NA
</code></pre>

<p>Now consider the same in Python:</p>

<pre><code>import scipy.stats
scipy.stats.chi2_contingency([[0,23,0,0], [0,1797,0,0]])

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python2.7/dist-packages/scipy/stats/contingency.py"", line 236, in
     chi2_contingency
    ""frequencies has a zero element at %s."" % zeropos)
ValueError: The internally computed table of expected frequencies has a zero element at [0, 0, 0, 1, 1, 1].
</code></pre>

<p>Is this expected behaviour? Should I just trap for the error in Python. A search for the message ""The internally computed table of expected frequencies has a zero element at"" did not reveal anything useful.</p>
",2013-10-24 19:42:29.797
58181,22.0,2,,58172.0,,,,CC BY-SA 3.0,"<p>I think you should rethink the question. </p>

<p>Why would it possibly matter to find the highest concentration where the response is far enough below the maximum plateau to give a P value less than some arbitrary threshold? </p>

<p>If you added more concentrations, or collected replicate values, or obtained cleaner (smaller experimental error) data, then you'd reach a different conclusion about the largest concentration that gives a response ""significantly"" less than the plateau. So the answer to that question is partly determined by details of the experimental design that you can change. The answer tells you nothing fundamental about the system. </p>

<p>As with most questions in statistics, I urge you to set aside the word ""significant"" and try to articulate the question you want answered in scientific terms. </p>
",2013-10-24 20:15:11.560
58182,449.0,2,,58180.0,,,,CC BY-SA 3.0,"<p>They're both errors but in R it just reported NaN.</p>

<p>The reason they are errors likely has to do with divide by 0 issues. You must have some kind of count in each cell, typically at least 4-7 is preferred. See any online article on the assumptions and requirements of a chi-square test.  It tests independence but it can't do so with no data in either cell in a 2 by X design.</p>

<p>If the problem is just that python will exit then, by all means, trap the error.</p>
",2013-10-24 20:17:03.853
58183,23021.0,1,58185.0,,,Maximum Likelihood for shifted Geometric Distribution,<self-study><maximum-likelihood><order-statistics><geometric-distribution>,CC BY-SA 3.0,"<p>Really struggling with this please help.</p>

<p>Find MLE for p and c</p>

<p>\begin{equation}
\ {f}(x,p,c) = (1-p)^{x-c}p
\end{equation}</p>

<p>x=c,c+1,c+2,.....</p>

<p>p is between 0 and 1</p>

<p>c is element of the integers</p>

<p>I am more interested in the mle for c</p>

<p>For the mle of p is it fine to just take the derivative of the log likelihood function.</p>
",2013-10-24 20:21:40.683
58184,21762.0,2,,58137.0,,,,CC BY-SA 3.0,"<p>(Almost) perfect multicollinearity (MC) will lead to large standard errors of the estimates and/or non-convergence of the optimization routines. Any other MC is no issue since your goal is prediction (and not interpretation of effects). </p>
",2013-10-24 20:26:39.817
58185,22399.0,2,,58183.0,,,,CC BY-SA 3.0,"<p>This should get you started:</p>

<p>$L = \prod_i (1-p)^{x_i-c} p$</p>

<p>To maximize the above we work with the log-likelihood:</p>

<p>$LL = \sum_i \Big( (x_i-c) \log(1-p) + \log(p) \Big)$</p>
",2013-10-24 20:34:35.450
58186,19388.0,1,,,,Regression estimate of a non-negative variable,<regression><bayesian><multiple-regression><hierarchical-bayesian><two-step-estimation>,CC BY-SA 3.0,"<p>I have to estimate linear weight $\beta$ for regression $Y \sim \mathbf{X}$, where $Y$ are non-negative samples. If I perform vanilla regression (lets assume ridge regression) it will find $\beta$ such that most of estimated $\hat{Y}$ are non-negative. But some of them will be negative. But I need them to be invariably non-negative. In Bayesian framework, it implies I need a prior $\beta$ that is non-negative (ridge assume normal priors). Is there simple way to modify ridge regression to obtain this? Or putting non-negative priors and estimating via MCMC is only solution to it. </p>

<p>For further description of problem (why I need estimated $\hat{Y}$ to be positive). My full model is a hierarchical model where there is a variable $Z$ that is a linear model of latent variable $Y$ i.e $Z \sim N(\Theta^T \mathbf{Y}, \sigma^2)$.  Observable variable are $Z$ and $X$. I need estimates of $\beta$ and $\theta$.  I am currently estimating $\beta$ given Y. Given $\hat{Y}$ I am estimating $\theta$. I am iterating between this 2 steps. If $\hat{Y}$ is negative the problem become unidentifiable.    </p>
",2013-10-24 21:16:19.347
58187,23021.0,1,,,,Challenging Likelihood Ratio Test,<hypothesis-testing><self-study><maximum-likelihood><likelihood-ratio>,CC BY-SA 3.0,"<p>Derive Likelihood Ratio Test of size $\alpha$. H$_0$: $\theta=\theta_0$  H$_1$:$\theta \neq \theta_0$</p>

<p>\begin{equation}
{f}(x,\theta , c) = \theta(x-c)^{\theta-1}  \ {c&lt;x&lt;c+1}
\end{equation}</p>

<p>I am having trouble with the mle for c. We have to use order stats and I am not sure which is the correct one to use</p>
",2013-10-24 21:26:54.730
58188,1930.0,1,58195.0,,,regarding conditional independence and its graphical representation,<machine-learning><bayesian><conditional-probability><covariance><graphical-model>,CC BY-SA 3.0,"<p>When studying covariance selection, I once read the following example. With respect to the following model:</p>

<p><img src=""https://i.stack.imgur.com/EftcN.png"" alt=""enter image description here""></p>

<p>Its covariance matrix and inverse covariance matrix are given as follows,</p>

<p><img src=""https://i.stack.imgur.com/NJ4gb.png"" alt=""enter image description here""></p>

<p>I do not understand why the independence of $x$ and $y$ is decided by the inverse covariance here? </p>

<p>What is the mathematical logic underlying this relationship? </p>

<p>Also, the left graph in the following figure is claimed to capture the independence relationship between $x$ and $y$; why?</p>

<p><img src=""https://i.stack.imgur.com/kOtw8.png"" alt=""enter image description here""></p>
",2013-10-24 21:32:11.820
58189,5213.0,1,,,,error of the mean in presence of background,<mixture-distribution><measurement-error>,CC BY-SA 3.0,"<p>Suppose I have a normal distribution, $N[\mu,\sigma]$, and I have a sample of size $n$. It is well know that the error (std deviation) of the mean is $\sigma/\sqrt{n}$.</p>

<p>Now suppose that my distribution is a mixture of signal (normal distributed) and a flat background 
$$\text{pdf}[x|\mu, \sigma, s, b] = \frac{sN[x|\mu,\sigma] + b U}{s+b} $$</p>

<p>suppose that the uniform distribution is much wider than the normal but in the signal window the background is important. Question: what is the error of $\mu$ now?</p>

<p>($s$ and $b$ are random variables, they can be assumed to be Poisson distributed)</p>
",2013-10-24 21:35:19.270
58190,23022.0,1,58206.0,,,1-d extension of 2-d variance?,<multivariate-analysis>,CC BY-SA 3.0,"<p>A random variable X={X1..Xm} and Var(X) is the mXm variance-covariance matrix.  Is there an accepted 1-d statistic like variance that may be extracted from the sample variance of data representing a 2-d random variable?  </p>

<p>(explanation of the 2d equivalent of a sample variance)
<a href=""https://stats.stackexchange.com/questions/49521/how-to-find-variance-between-multidimensional-points/73717#73717"">How to find variance between multidimensional points?</a></p>
",2013-10-24 21:38:34.937
58191,5671.0,2,,58160.0,,,,CC BY-SA 3.0,"<p>Be really careful when using PCA on discrete values such as likert scales. <strong>PCA is designed for continuous variables, not for discrete values</strong>.</p>

<p>There is a high chance that you will discover <strong>artifacts from the discrete scale</strong>.</p>

<p>In fact, the vector looks susceptible much like the frequencies of the 5 answers, or something like this...</p>

<p>If you would share more of what you've been doing, it would be easier to help you.</p>
",2013-10-24 21:40:44.713
58192,22936.0,1,,,,Entropy of Inverse-Wishart distribution,<entropy><wishart-distribution>,CC BY-SA 3.0,"<p>What is the entropy of the <a href=""http://en.wikipedia.org/wiki/Inverse-Wishart_distribution"" rel=""nofollow"">Inverse-Wishart distribution</a>?  I need just a reference, but derivation (e.g. using inverse property) would be interesting too.</p>
",2013-10-24 22:01:38.600
58193,503.0,2,,58180.0,,,,CC BY-SA 3.0,"<p>The problem is not that there are 0 cells, the problem is that only one column has any data. E.g</p>

<pre><code>row1 = c(100,23,0,100)
row2 = c(0,1797,100,0)
data.table = rbind(row1, row2)
chisq.test(data.table)
</code></pre>

<p>works fine</p>

<p>and </p>

<pre><code>row1 = c(10,0,0,100)
row2 = c(0,1797,100,0)
data.table = rbind(row1, row2)
chisq.test(data.table)
</code></pre>

<p>gives only a notice that the approximation may be incorrect - here an exact test should be used.</p>

<p>Even</p>

<pre><code>row1 = c(23,0,0,0)
row2 = c(0,1797,0,0)
data.table = rbind(row1, row2)
chisq.test(data.table)
</code></pre>

<p>gives only that same warning.</p>
",2013-10-24 22:49:10.423
58194,2352.0,1,,,,How should I approach modeling these subjective probability estimates?,<mixed-model>,CC BY-SA 3.0,"<p>In my data about 1000 people have made estimates of the probability of 100 unique events.  On average people forecast on about 50 events, but some forecast on all of events and some on only a few.  About 500 of these people are working in teams of 15.  They see each others forecasts and communicate with each other, but each individual still offers their own estimates).  One question I'd like to answer is how much of the variance in these forecasts is explained by which team people were (randomly) assigned to.  But I also suspect that the share of the variance explained by team will vary by event.  So I'd like to examine that variation as well.  I have a bit of experience with mixed models with lmer, but its not immediately obvious to me how to approach this.  </p>
",2013-10-24 22:51:47.620
58195,1889.0,2,,58188.0,,,,CC BY-SA 3.0,"<p>The inverse covariance matrix can be used to work out conditional variances and covariances for multivariate Gaussian distributions. <a href=""https://stats.stackexchange.com/questions/10795/how-to-interpret-an-inverse-covariance-or-precision-matrix"">An earlier question gives some references</a>   </p>

<p>For example to find the conditional covariance of $Y$ and $Z$ given the value $X=x$, you would take the bottom right corner of the inverse covariance matrix </p>

<p>$$\left( \begin{array}{rr}
1 &amp; -1  \\
-1 &amp; 3  \end{array} \right) \text{ and re-invert it to }\left( \begin{array}{rr}
\tfrac32 &amp; \tfrac12  \\
\tfrac12 &amp; \tfrac12  \end{array} \right)$$ </p>

<p>which does indeed give the covariance matrix of $Y$ and $Z$ conditioned on the the value for $X=x$.</p>

<p>So similarly to find the conditional covariance matrix of $X$ and $Y$ given the value for $Z=z$, you would take the top left corner of the inverse covariance matrix </p>

<p>$$\left( \begin{array}{cc}
1 &amp; 0  \\
0 &amp; 1  \end{array} \right) \text{ and re-invert it to }\left( \begin{array}{cc}
1 &amp; 0  \\
0 &amp; 1 \end{array} \right)$$ </p>

<p>telling you that the conditional covariance between $X$ and $Y$ given $Z=z$ is $0$ (and that each of their conditional variances is $1$).  </p>

<p>To conclude that this zero conditional covariance implies conditional independence, you also have to use the fact this is a multivariate Gaussian (as in general zero covariance does not necessarily imply independence).  You know this from the construction. </p>

<p>Arguably you also know about the conditional independence from the construction, since you are told that $\epsilon_1$ and $\epsilon_2$ are iid, so conditioned on a particular value for $Z=z$, $X=z+\epsilon_1$ and $Y=z+\epsilon_2$ are also iid. If you know $Z=z$, there is no additional information from $X$ that helps you say anything about possible values of $Y$.</p>
",2013-10-24 23:16:29.650
58196,2806.0,1,58197.0,,,What is the intuition behind (M)ANCOVA and when/why should one use it?,<ancova><manova><intuition><teaching><mancova>,CC BY-SA 3.0,"<p>As per my understanding here's the what/why/when of the following hypotheses tests in a crude sense:</p>

<ul>
<li><strong>t-test</strong>: Used when comparing means between two samples</li>
<li><strong>ANOVA (one way)</strong>: Used when you have one dependent variable and one independent  (i.e., categorical) variable and you wish to analyze the 'means' (i.e., effects) across multiple groups. Simply stated, multi-way t-tests in essence.</li>
<li><strong>ANOVA (two way)</strong>: Similar to one-way except you have two independent (i.e., categorical) variables</li>
<li><strong>MANOVA</strong>: ANOVA with multiple dependent variables</li>
<li><strong>ANCOVA</strong>: ??</li>
<li><strong>MANCOVA</strong>: ??</li>
</ul>

<p>Intuitively, the concepts/intuition behind (M)ANOVA makes sense and I understand when/how to apply it and why is it necessary. I've just overly simplified my understanding about them above. However, I lack the similar intuition behind (M)ANCOVA.</p>
",2013-10-25 00:54:47.617
58197,5237.0,2,,58196.0,,,,CC BY-SA 3.0,"<p>To complete your scheme:  </p>

<ul>
<li><strong>ANCOVA</strong>: ANOVA conducted to compare multiple (possibly only two) conditions on at least one independent variable while controlling for a set of continuous nuisance variables (possibly only one).  </li>
<li><strong>MANCOVA</strong>: MANOVA conducted to compare multiple (possibly only two) conditions on at least one independent variable while controlling for a set of continuous nuisance variables (possibly only one).  </li>
</ul>
",2013-10-25 01:23:54.390
58198,82.0,2,,58172.0,,,,CC BY-SA 3.0,"<p>You could model it using nonlinear least squares regression, then use the modeled values and SE of the fit to determine the conc level that is ""different"" than the max. Once you fit the model, you could brute force search progressively lower thresholds until you reach a conc where Resp differs by a pre-specified alpha value from the max value and the estimated error around that.</p>

<p>So, fit a nls, then write a for loop to calculate the p values for, say, a t-test comparison, and search that for the critical conc value you're looking for.</p>
",2013-10-25 01:54:05.503
58199,23027.0,1,58200.0,,,Translating R lme comand to mathematical equation,<random-effects-model><computational-statistics><lme4-nlme><mixed-model>,CC BY-SA 3.0,"<p>I would appreciate if someone could help me in translating the following R command into a mathematical equation:</p>

<pre><code>lme(score ~ factor(timeslot), random=~1|subjectid, data=a)
</code></pre>

<p>For each subject there are observations at 6 different times (timeslots) during a day.</p>
",2013-10-25 06:49:54.090
58200,21638.0,2,,58199.0,,,,CC BY-SA 3.0,"<p>$$
y = X\beta + Z\gamma + \epsilon
$$</p>

<p>$y$ is an $N$ length vector of all observations across all subjects</p>

<p>$X$ is an $N \times 6$ design matrix. The first column is all $1$s for the intercept term which is implicit in the call to <code>lme</code> - i.e. your call is equivalent to</p>

<p><code>lme(score  ~ 1+factor(timeslot),random=~1|subjectid,data=a)</code></p>

<p>The remaining $5$ columns of $X$ map the observations to the timeslots at which they were observed. Note that there are $5$ rather than $6$ of these columns as the effects of each timeslot are taken <em>relative</em> to the first one.</p>

<p>$\beta$ is a $6$ length vector of fixed-effects where the first element is the intercept term and the remaining $5$ elements correspond to the relative effect of the last $5$ timeslots compared to the first.</p>

<p>$Z$ is an $N \times n$ design matrix mapping the observations to the subject on which they were measured. $n$ is the total number of subjects in your data set (which you have not specified).</p>

<p>$\gamma$ is an $n$ length vector of random intercept terms, one for each subject.</p>

<p>$\epsilon$ is an $N$ length vector of error terms.</p>
",2013-10-25 07:40:15.233
58201,22637.0,2,,58176.0,,,,CC BY-SA 3.0,"<p>You can calculate the probabilities manually or you can do it with R. The code for the cumulative distr. of Poisson is ppois(k,m), k being the limit $P(X\leq k) $ and m the parameter of the distribution.</p>
",2013-10-25 08:11:17.770
58203,17321.0,1,66211.0,,,"When doing systematic sampling, what should be done if the sampling interval (i.e. the skip) is not an integer?",<sampling>,CC BY-SA 3.0,"<p>Let:
population size $=N$;
sample size $=n$;
sampling interval $=\frac{N}{n} = k$, which can be non-integer; and
$r=$ random starting point, which can be non-integer, $0 &lt; r &lt; k$.</p>

<p><a href=""http://en.wikipedia.org/wiki/Systematic_sampling"" rel=""nofollow"">http://en.wikipedia.org/wiki/Systematic_sampling</a> says we round up $r + mk$ (where $m$ is an integer between $0$ and $n-1$, both inclusive) although the values given (11, 26, 41, 56, 71, 86, 101, and 116) show some rounded-down values. </p>

<p>ocw.jhsph.edu/courses/statmethodsforsamplesurveys/PDFs/Lecture2.pdf says:</p>

<p>1) ""if $k=5$ is considered, stop the selection of samples when $n=175$ achieved.""</p>

<p>But this means the last few members would not have any chance of being picked.</p>

<p>2) ""if $k=6$ is considered, treat the sampling frame as a circular list and continue the selection of samples from the beginning of the list after exhausting the list during the first cycle.""</p>

<p>This doesn't give equal chance to each member of being picked, does it?</p>

<p>3) ""An alternative procedure is to keep $k$ non-integer and continue the sample selection as follows:
Let us consider, $k=5.71$, and $r=4$.
So, the first sample is 4th in the list. The second $=(4+5.71) =9.71$ ~ 9th in the list, the third $=(4+2\times5.71) =15.42$ ~ 15th in the list, and so on. (The last sample is: $4+5.71\times(175-1) = 997.54$ ~ 997th in the list).""</p>

<p>This uses rounding down of $r + mk$ (different from the rounding up suggested by the Wikipedia page mentioned above).</p>

<p>Shouldn't we be rounding off instead to give equal chance to each member of being picked?</p>

<p>An even better way is to let random starting point be $R$, randomly selected from the integers 1 to $N$, both inclusive, and use $r + mk$, rounded off and modulo $N$?</p>
",2013-10-25 08:50:00.713
58204,9554.0,1,,,,Leave-out data approach to get intervals for predictive performance of a regression model,<regression><prediction><prediction-interval>,CC BY-SA 3.0,"<p>When measuring predictive performance of a regression model, I am thinking about using repeated data splitting (or leave-out data at random) instead of using bootstrapping.</p>

<p>By ""repeated data splitting"" (not sure if that's even term) I understand:</p>

<pre><code>FOR N=10000
Sample 70% of the data points and fit the model
Predict for the remaining 30%
Store performance
END FOR
Calculate quantiles from stored performance data
</code></pre>

<p>I am currently using a GLM in order to rank my data according to a ratio scaled response variable Y (which my model is supposed to predict). Later I am evaluating the quality of the ranking using a specific metric. </p>

<p>I would like to calculate the prediction (or at least) confidence intervals for this metric -- that's the thing I care about (and not, say, the variance of the $\beta_3$ estimate)</p>

<p><strong>Would the repeated data-splitting provide useful intervals for the performance of my ranking? i.e. Can I consider them as prediction intervals?</strong></p>

<p>I would argue that this approach makes more sense than non-parametric bootstrap, as long as the data set is large enough, since we are using only actual data (that really occurred at one point in history, as opposed to a bootstrapped point the quality of which completely depends on the quality of the empirical approximation of the underlying distribution).</p>

<p>Since my data set is fairly large ($&gt;20,000$) but also extremely skewed and long tailed, I would prefer using the data-splitting but I am not sure if this is a plain heuristic, or whether the intervals are actual prediction intervals.</p>
",2013-10-25 09:08:56.807
58205,23028.0,1,,,,How to calculate spatial correlation between two variables?,<correlation><spatial>,CC BY-SA 3.0,"<p>I have a dataset of point coordinates of individuals and different variables of these individuals. I want to calculate if the spatial distribution of a certain variable is correlated to the spatial distribution of a different variable. For example: do patches of large plants have higher predation rates, even small plants within a patch of large plants.
Is it possible to calculate a correlation or a level of significance for these two variables and what software should I use?</p>
",2013-10-25 09:28:08.600
58206,21029.0,2,,58190.0,,,,CC BY-SA 3.0,"<p>I don't believe there is an answer to your question. You can't scale down a variance-covariance matrix. If you did (say by taking the determinant) you will lose all the usefull information -- it would no longer be explaining the variance.</p>

<p>As suggested in the comments, you can run a Principal Component Analysis to reduce the dimension of your data to one. Then the inertia (<strong>not the variance</strong>) of the first component can provide a variance-like summary of the full data. </p>

<p>However, this is different from the variance because you will be calculating the variation with respect to a new calculated principal component and not the original variables. It is therefore no longer the variance, but the inertia.</p>
",2013-10-25 09:56:19.230
58207,9554.0,2,,58202.0,,,,CC BY-SA 3.0,"<p>I am not sure if I really get what is troubling you, but perhaps a couple of hints will help you:</p>

<ul>
<li>How can I conceptualize the prior of a deterministic variable in Bayesian data analysis? </li>
</ul>

<p>Formally, deterministic variables don't have a probability distribution (yes you guessed it, since they are not random variables!). No random variable => no prior.</p>

<ul>
<li>Now, as far as I understand the first is a deterministic variable and
the second is stochastic.</li>
</ul>

<p>If by first you mean $\lambda_C$, since it is a function of r.v. $\sigma$, IT IS a random variable.
(just a side note: usually you reserve the term ""stochastic"" for random quantities involving time, such as a stochastic process)</p>

<ul>
<li>However, I have problems conceptualizing the prior of the deterministic variable.</li>
</ul>

<p>I think I answered this in the first comment. But let me elaborate. In Bayesian statistics there are multiple ways how you can treat a parameter, depending on your previous knowledge. In general you assume the parameter to have an uncertain value, and consider it random. Depending on your beliefs and prior knowledge about its values you can either, use a uniform distribution, or a distribution that favors a certain value, e.g. if you know the precision is most likely centered around a certain value, you can use a Gamma($\alpha$,$\beta$) distributed prior with appropriate values $\alpha$ and $\beta$. Now here is the point that might be tripping you up. Priors are there since sometimes you have a strong belief, but usually you will not be a 100% certain about the value of a certain parameter. 
Technically, that case can be considered a random variable, with a point mass. But that is completely beyond the point. Priors help model uncertainty of parameters.
Deterministic parameters in the model are something completely different. They are unknowns, that you decide to model as single values without any uncertainty.</p>

<ul>
<li>How would you, for example, plot the graph of the prior?</li>
</ul>

<p>By plotting its probability distribution.</p>

<ul>
<li>How would I even calculate the single values for the prior of $\lambda_C$?</li>
</ul>

<p>Not sure what you mean here. I hope by now you can see this question makes no sense.</p>

<p>HTH</p>
",2013-10-25 10:03:45.003
58216,,2,,58210.0,anon,,,CC BY-SA 3.0,"<p>The question is rather vague. From your description, the only case where I can imagine the scatterplot to look like a triangle (and fit the description), is a case where the variance of the Y increases with the X values. The scatter plot would appear something like:</p>

<pre><code>|           .
|         ...  
|       ..... 
|    ........
|  ......     
|.... 
+----------------
</code></pre>

<p>What is the aim of your analysis? If you're trying to predict the Y from the X, you might want to take the increasing variance into account during the analysis using something like generalized least squares or other such methods. If you want to find other such cases from a larger data set that contains several X variables, as crude solution you might consider calculating, e.g., a range or variance of the Y variable along the X variable. Generating all the possible plots, and eyeballing them for interesting patterns might be a viable option, also.</p>
",2013-10-25 12:59:48.207
58208,23030.0,1,,,,Relation between autocorrelation function and periodogram in time series analysis,<time-series><autocorrelation>,CC BY-SA 3.0,"<p>I was wondering if anyone could give me some insight on the relation between the ACF and the periodogram of a time series.</p>

<p>I have a bunch of timeseries and their ACF's and periodograms are typically much like the examples below.</p>

<p>For my analysis, I'm mostly interested in periodicity at lag 8 and 16 (for theoretical reasons)</p>

<p>The frequencies 'B' and 'HB' correspond to lag 16 and lag 8 respectively. The time series actually concerns interresponse intervals in musical performance of a piece that consists solely of eighth notes (16 of them in a 4:4 bar so 'B' stand for bar and 'HB' for half bar).</p>

<p>The thing I actually wanted to ask: in my periodograms, I consistently get very large peaks at frequency 0.25 (which corresponds to lag 4). However, the ACF peak at lag 4 is much smaller than those at lag 8 or 16. I was wondering how to interpret this finding. A lot of time series variance can be explained at this frequency even though the lag 4 autocorrelation is quite low?</p>

<p>I hope I was sufficiently clear in my question. If not, don't hesitate to ask me.</p>

<p><img src=""https://i.stack.imgur.com/K6bsc.jpg"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/RKYMl.jpg"" alt=""enter image description here""></p>
",2013-10-25 10:10:59.607
58209,23031.0,2,,54836.0,,,,CC BY-SA 3.0,"<p>I would suggest you look at page 8 of ""<a href=""http://faculty.washington.edu/jwilker/559/SteyversGriffiths.pdf"" rel=""nofollow"">Probabilistic Topic Models</a>"" by Mark Steyvers and Tom Griffiths. I found their explanation of the Gibbs algorithm quite clear and easy to implement.</p>

<p>To answer your questions:</p>

<ul>
<li>i seems to range over (indexes for) all the words in all the documents, and d indeed seems to refer to the document of the word under consideration.</li>
<li>There are some algorithms for estimating alpha and beta but I haven't really understood any of them myself. Just tuning the parameters manually should be ok in most cases.</li>
<li>W seems to be the size of the vocabulary, i.e. the number of unique words.</li>
<li>I'm confused about the beta with / without subscript as well. If they use a different beta per word, then maybe in the denominator it should say the sum of all beta_w, instead of beta * W?</li>
</ul>
",2013-10-25 10:17:58.963
58210,23032.0,1,,,,Are there statistical techniques that investigate such relationships ...?,<correlation>,CC BY-SA 3.0,"<p>If we have data set, X and Y variable. Say, we do correlation analysis and get some correlation coefficient. Besides, we find an important fact after observing their relationship: That is, the scatter plot of X and Y has a triangular shape. Which means that (for example) when X values are increasing Y values are increasing for all X values, whereas vice versa is not true; when the Y value is increasing X values are anything for all Y values.</p>

<p>What kind of analysis should I do to investigate this?  </p>

<hr>

<p>(<em>Update in response to</em> @Penguin_Knight)</p>

<p>Your graph is exactly what I explained. Just take away the negative values with an imaginary y=0 line. As you can see there are many data points the X of which is either 0 or very small, and Y has pretty high value. However there is no data point that have y=0 and x is quite high value. And as you can see it makes the data scatter shape a right angle triangle. Thus we could say that the high X values necessitates high Y values but high Y values does not always have high X values. I find it very interesting. In practice for example I measure Complexity and Size of given entities. And my analysis show that all complex entities have big size but not all big-size entities are complex. Then I conclude that the certain amount of complexity requires defined amount of size. You cannot put more complexity in a given size. This is a bit abstract but you see my point? It is very interesting and I would like to get some help on how this kind of relationships are discussed or described in statistics formally.</p>
",2013-10-25 10:55:34.690
58211,18514.0,2,,58208.0,,,,CC BY-SA 3.0,"<p>The relation between the autocovariance (or autocorrelation) and the spectral density (for which the periodogram is an estimator) is given by the Fourier transform. The two form a so-called Fourier-transform pair meaning the two are time(or space)-domain vs. frequency-domain representations of the same thing. Specifically, if time series $\{X_t\}$ has autovariance function $\gamma(\tau)$ at time lag $\tau$, then the spectral density is defined by
\begin{equation}
f(\nu)=\sum\limits_{\tau=-\infty}^{\infty} \gamma(\tau)e^{-2i\pi\nu\tau}.
\end{equation}
In words, the spectral density partitions the autocovariance as energy-per-hertz of a signal. For example, if you have a deterministic signal with period $t=12$, then the series lagged with itself (ACF) at lag 12 will be perfectly correlated (autocorrelation=1). Subsequently, all power in the spectral density will be concentrated at frequency $1/t$.  </p>
",2013-10-25 11:08:27.573
58212,9554.0,2,,58190.0,,,,CC BY-SA 3.0,"<p>I guess it really depends on what you are doing. If your are investigating the relationship between the two variables, the answer would be the covariance. (off-diagonal element in your 2x2 matrix.)</p>

<p>In case you are trying to track the uncertainty of estimates from 2 different systems using their covariance matrices, you could use:</p>

<p>$det(\Sigma)$, or $tr(\Sigma)$, where $\Sigma$ is the covariance matrix.</p>

<p>i.e. the determinant of the covariance matrix is a 1D measure to track uncertainty, and there is <a href=""http://en.wikipedia.org/wiki/Optimal_design"" rel=""nofollow"">some theory</a> about how and why it might make sense. Intuitively, the det tells you by how much you would the cov matrix scale a space if applied as a linear transformation.</p>
",2013-10-25 11:26:16.530
58213,9554.0,2,,58210.0,,,,CC BY-SA 3.0,"<p>Typically, to say a scatterplot has triangular shape, it would have to look like this: /\. What you are describing sounds like this: /.</p>

<p>The part about Y~X being positively correlated in the plot, and X~Y not, is IMPOSSIBLE. Please check your code, or the description. </p>

<p>In general you can use a regression model to model a relationship between a ratio scaled X and Y. Look up the assumptions for different regression models in a book or on the <a href=""http://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow"">Wiki</a>.</p>
",2013-10-25 11:45:34.933
58214,20388.0,1,,,,Sample from Wishart distribution with inverse Scale matrix,<matlab><covariance><random-generation><numerics><wishart-distribution>,CC BY-SA 4.0,"<p>I tried to model precision matrix in a hierarchical Bayesian setup with Wishart prior given d.f. and inverse scale matrix, and matrix normal likelihood, since it's a conjugate prior, my posterior on the precision matrix $K$ ends up in the form:
$$
K \sim \text{Wishart}(\text{df}, \Lambda^{-1})
$$
Since the dimension of $\Lambda$ is quite big, I do not wish to take the inverse of the matrix and use the built-in sampler. I looked at the source code for Matlab's <code>wishrnd</code>, they used <a href=""http://en.wikipedia.org/wiki/Wishart_distribution#Bartlett_decomposition"" rel=""nofollow noreferrer"">Bartlett decomposition</a> for large dimensions (81+n), which should also work for smaller dimensions albeit its inefficiency. </p>

<p>snips of the code:</p>

<pre><code>[n,m] = size(Lambda);
[d,p] = cholcov(Lambda,1);

% Otherwise use the Smith &amp; Hocking procedure
d = eye(size(d)) / d; 
% Load diagonal elements with square root of chi-square variates
a = diag(sqrt(chi2rnd(df-(0:n-1))));

% Load upper triangle with independent normal (0, 1) variates
a(itriu(n)) = randn(n*(n-1)/2,1);
% Desired matrix is D'(A'A)D
x = a(:,1:size(d,1))*d;

a = x' * x;

% --------- get indices of upper triangle of p-by-p matrix
function d=itriu(p)

d=ones(p*(p-1)/2,1);
d(1+cumsum(0:p-2))=p+1:-1:3;
d = cumsum(d);
</code></pre>

<p>In order to accommodate my needs,i.e. inputing degrees of freedom $df$ and the inverse scale matrix $\Lambda$ I added </p>

<pre><code>d = d \ eye(size(d))
</code></pre>

<p>after the second line, so that I would have the inverse of the cholesky decomp of the inverse scale matrix, in others words, the cholesky decomp of the scale matrix. Then everything should be okay from there ( I hoped).</p>

<p>I tested this by firstly generating a 3 by 3 positive definite matrix:</p>

<pre><code>&gt;&gt; test = rand(190, 3); Lambda = test'*test;
&gt;&gt; Lambda

Lambda =

  62.7642   46.4970   45.6662
  46.4970   61.9178   45.4114
  45.6662   45.4114   59.1070b
</code></pre>

<p>Setting $df = 6$, 100,000 randoms samples were generated and the empirical mean is compare with the first moment of the distribution</p>

<pre><code>&gt;&gt; df = 6;
&gt;&gt; sam = 0; for loop = 1:100000 sam = sam + mywishrnd(Lambda,df); end
&gt;&gt; sam/100000
ans =

 0.0956   -0.1069   -0.0689
-0.1069    0.3380   -0.0286
-0.0689   -0.0286    0.3872

&gt;&gt; inv(Lambda)*df

ans =

 0.2647   -0.1118   -0.1187
-0.1118    0.2692   -0.1205
-0.1187   -0.1205    0.2857
</code></pre>

<p>There is quite a big differences in some entries of the results, but I don't see anything wrong with this theoretically. Is it possibly due to numerical error? Could someone enlighten me on this problem? </p>
",2013-10-25 12:07:31.487
58215,3868.0,1,,,,"need name, reference, and/or study for the following variable reduction procedure in regression",<regression><model-selection><predictor>,CC BY-SA 3.0,"<p>I have seen the following commonly used: 1. fit a model with all variables, 2. in a single reduction step, remove from the model all variables at once that do not fit some criteria (p-value, whatever), 3. calibrate the reduced model, in my case to a new data set, 4. check model results and hopefully everything went well and you can stop.</p>

<p>I'm betting this will perform better than stepwise, especially when used with data splitting, but would appreciate a name for this procedure if it exists and perhaps a reference related to it so that I can learn more.  Maybe it is so simple/bad/obvious that no one has bothered? -WVG</p>
",2013-10-25 12:15:34.753
58217,10547.0,1,,,,Confusion about the kind of offset-value for this non homogenous poisson-process,<regression><generalized-linear-model><poisson-distribution><poisson-process><offset>,CC BY-SA 3.0,"<p>Suppose that I observe $M$-times this kind of table:</p>

<pre><code>     t X
 1:  1 0
 2:  2 0
 3:  3 0
 4:  4 0
 5:  5 0
 6:  6 1
 7:  7 0
 8:  8 0
 9:  9 0
10: 10 0
11: 11 0
12: 12 0
13: 13 0
14: 14 0
15: 15 0
16: 16 5
17: 17 1
18: 18 0
19: 19 0
20: 20 0
.   . .
.   . .
T-1:T-1 0
T:  T 0
</code></pre>

<p>So, what I get is like: $x_1 = 0, x_2 = 0, x_3 = 0, x_4 = 0, x_5 = 0, x_6 = 1....$, i.e., $X_t \in \{0,1,2,3,4,5,...\}$ where $t = 0,1,2,...,T-1,T$. As we can see, I observe jumps greater then one for particular $t$'s (like $t = 16, \rightarrow x_{16} = 5$).</p>

<p><strong>I want to investigate if more counts occur if $t$ moves towards $T$ and what the impacts of different covariates, captured in $\boldsymbol{X}$, are.</strong></p>

<p>To do that, I could assume that $N(t) = \sum_{i=1}^t X_t$ is a poisson-process, i.e.,</p>

<p>(1) $P(N(t) = k) = \frac{\lambda(t)t^{k}}{k!}\text{exp}\{-\lambda(t)t\}$</p>

<p>The regression model will be:</p>

<p>(2) $\text{log}\{E(N(t)|t)\} = \boldsymbol{X\beta + \epsilon}$</p>

<p>$\Leftrightarrow \text{log}\{\lambda(t)t\} = \boldsymbol{X\beta + \epsilon}$</p>

<p>$\Leftrightarrow \text{log}\{\lambda(t)\} = -\text{log}\{t\} + \boldsymbol{X\beta + \epsilon}$</p>

<p>In this case, I would model $-\text{log}\{t\}$ as the <em>offset-value</em> and take $N(t)$ as the dependend variable - which is just the cumsum of $X_i$, where $i$ goes from $1$ to $T$. </p>

<p><strong>Question 1:</strong>
<em>I think here I'll get a problem because, as far as I remember, for particular $t$'s, the process is not supposed to jump by more then one?</em></p>

<p>Another way I could think of, is, that I see my data as a big cross-table. If I assume that each cell count is a realization of an independent poisson-process and the total count $n$ is fixed, then I'm able to say that I observe a set of counts $n_1,...,n_T$ which are each poissonian with rate $\lambda_t$. </p>

<p>If these assumptions hold, it can be shown that the cell counts are Multinomial$(\frac{\lambda_t}{n_1},...,\frac{\lambda_t}{n_T},n)$.</p>

<p>Hence, if I see the observations as a multinomial respones, for the cell counts $X_t$, I'll get the model:</p>

<p>(3) $\text{log}\{E(X_t)\}) = \boldsymbol{X\beta + \epsilon}$</p>

<p>$\Leftrightarrow \text{log}\{\lambda_t\} = \boldsymbol{X\beta + \epsilon}$</p>

<p>$\Leftrightarrow \text{log}\{np_t\} = \boldsymbol{X\beta + \epsilon}$</p>

<p>$\Leftrightarrow \text{log}\{p_t\} = -\text{log}\{n\} + \boldsymbol{X\beta + \epsilon}$</p>

<p><strong>Question 2:</strong>
<em>Now, I would choose $-\text{log}\{n\}$ as my offset-value. If the sequence of $\lambda_t, \ \ t = 1,...,T$ is increasing in $t$, this would answer my question. Since my $n$ is actually fixed and with this approach I'm able to deal with the multiple jumps at some $t$ it seems that the model (3) would be the way to go?</em></p>

<p><strong>Question 3:</strong>
<em>If I use a <code>glm()</code> what would my dependend variable be? I thought about defining a new r.v. $Y_t$ which is 1 if $X_t&gt;0$ and zero otherwise. If $X_t&gt;1$ then I would duplicate the corresponding row $x_t$-times. If I do that, in my optinion, things are not supposed to change since I look at a ratio as a dependend variable anyway?</em></p>

<p><strong>Question 4:</strong>
<em>Another issue I've is, that in model (2) I'm not sure how (3) catches that moving towards $T$ the cumsum of $X_t$ inceases steadily, since (3) looks at each cell separately.</em></p>
",2013-10-25 13:17:25.907
58219,750.0,1,58900.0,,,How to test for randomness in bins with small N?,<hypothesis-testing><chi-squared-test>,CC BY-SA 3.0,"<p>I observe a series of crime incidents linked by <em>modus operandi</em> or some other peculiar characteristic of the crime (e.g. cutting catalytic converters from underneath vehicles). I would like to know if the observed days of the week that the crimes occur on (ignoring the uncertainty that sometimes occur in crime incidents - e.g. it happens overnight) are random. I typically have very few linked crime events, say between 5-15. </p>

<p>So question 1, there is a lot of knowledge about using Pearson's $\chi^2$ on <a href=""https://stats.stackexchange.com/a/14230/1036"">small n contingency tables</a>, can I use all of that same advice for $2\times 2$ contingency tables that come with it for just 7 day of week bins? (In particular can I use the $N - 1$ correction given expected cell frequencies are at least 1 and still expect similar coverage rates - which would mean I need at least 7 observed events?)</p>

<p>Or alternatively question 2, are there any other obvious approaches I can take to test the hypothesis of the events being random with respect to the day of the week? (Permutation approaches perhaps given the limited nature of the potential permutations?)</p>
",2013-10-25 13:26:16.757
58220,16174.0,1,58223.0,,,Does stand-alone dummy variables in linear regression models make sense?,<regression><categorical-data><linear-model>,CC BY-SA 3.0,"<p>Dummy (or binary) variables ($X_2$) can be used in linear regression models to help explaining a possible group effect that a continuous predictor variable ($X_1$) might present in explaining the response variable ($Y$).</p>

<p>Now, I am wondering if it makes sense to have only dummy variables as predictors in a linear regression model with a continuous response variable. Does it?</p>

<p>Example: </p>

<p>% of population with instruction = influenced by politics A + influenced by politics B.</p>

<p>Both politics A and B can assume values 1 or 0.</p>
",2013-10-25 13:42:12.593
58221,23037.0,2,,57939.0,,,,CC BY-SA 3.0,"<p>As far as I know, when comparing only 2 algorithms, DemÅ¡ar suggests the Wilcoxon signed rank test rather than Friedman + posthoc. I am, sadly, just as confuzed as you when it comes to decyphering what demÅ¡ar's dividing by k-1 is supposed to mean.</p>
",2013-10-25 13:42:22.577
58222,22775.0,1,,,,overall effects of categorical variables,<stata>,CC BY-SA 3.0,"<p>I'm doing a Poisson regression in Stata, so the dependent variable is a count variable and I have some categorical predictors. If A is a categorical variable with, for example, 4 levels, in the parameter estimates table I get results for the 3 levels of the variable compared to the level I have set as the reference category. Similarly for an interaction term. However, I would also like to have an estimate of the overall effect of variable A and also an estimate of the overall effect of the interaction. How do I do this in Stata?</p>

<p>Ie. I need the test of the effect of A in the model on the dependent variable D, which tests the joint hypothesis that every one of the four levels of A has the same effect on D, and therefore A does not explain any of the variation in D, and so it is not a significant predictor of D in the model. 
I do not need a test which tests the hypothesis that the effects on D of e.g. A2 , A3, or A4, respectively, are all equal to each other in this model, but not necessarily equal to the effect of A1.</p>

<p>Thank you</p>

<p>UPDATE:  Well, prompted by the comments from all of you, and after hours of trying out different things, I found that what I was looking for was 
""constrast A"", 
described in <a href=""http://www.stata.com/manuals13/u25.pdf"" rel=""nofollow"">http://www.stata.com/manuals13/u25.pdf</a>
However, I am now wondering why ""contrast A"" does not give me the same results as
""test A4=A3=A2=A1"". I'm told I shouldn't make stata-specific questions, however I'm trying to see how stata corresponds to other packages and this is what prompted my question. I also think that if someone can answer this question for me i.e. why constrast is not the same as a joint test on whether all levels have the same effect, then this would be of general interest as it obviously translates to a general statistical question. </p>
",2013-10-25 13:50:09.057
58223,10060.0,2,,58220.0,,,,CC BY-SA 3.0,"<p>Because linear regression does not assume any distribution of predictors, as long as</p>

<ol>
<li>They are not perfectly collinear, and </li>
<li>None of them is a constant, it should be fine.</li>
</ol>

<p>Your example is just like using regression as an ANOVA sans interaction (aka, not a full-factorial design.) If additional effect due to co-influence by A &amp; B is of your interest, compute an interaction term (by multiplying your two dummy variables) and include it as a predictor as well.</p>
",2013-10-25 13:57:50.543
58224,23039.0,2,,58141.0,,,,CC BY-SA 3.0,"<p>How about this paper: </p>

<p><a href=""http://arxiv.org/pdf/0803.4101.pdf"" rel=""noreferrer"">http://arxiv.org/pdf/0803.4101.pdf</a></p>

<p>""Measuring and testing dependence by correlation of distances"". SzÃ©kely and Bakirov always have interesting stuff. </p>

<p>There is matlab code for the implementation:</p>

<p><a href=""http://www.mathworks.com/matlabcentral/fileexchange/39905-distance-correlation"" rel=""noreferrer"">http://www.mathworks.com/matlabcentral/fileexchange/39905-distance-correlation</a></p>

<p>If you find any other (simple to implement) test for independence let us know. </p>
",2013-10-25 14:22:34.343
58225,16474.0,2,,58222.0,,,,CC BY-SA 3.0,"<p>Given your comments I will assume that you do not want an estimate of the size of the effect but instead a statistical test whether the expected (possibly adjusted) count for each of the categories are the same. This may or may not be wise depending on your circumstances, but this is an example of how you do it in Stata:</p>

<pre><code>webuse dollhill3
poisson deaths smokes i.agecat, exposure(pyears)
testparm i.agecat
</code></pre>

<p>If you want something like a single effect size you could look into sheaf coefficients. In case of interaction terms this generalizes to a model with parametrically weighted covariates. A brief discussion on how to do those in Stata can be found <a href=""http://www.maartenbuis.nl/wp/prop.html"" rel=""nofollow"">here</a>.</p>
",2013-10-25 14:30:54.353
58226,23038.0,1,58238.0,,,Similarity of new element x with the training set X,<regression><modeling><nonparametric>,CC BY-SA 3.0,"<p>Suppose we have trained a model (function, algorithm) $M$ which gives prediction to a new sample $x$ not observed in the training set, $M(x)$. It is natural to assume that the quality of prediction $M(x)$ depends on similarity of $x$ with the training set $X$. I wonder if there are any indices/methods which would evaluate similarity of $x$ with training set $X$, or any indices which would evaluate the quality of prediction $M(x)$. To be more specific, I don't want an overall performance of the model (which could be evaluated by RMSE, R-squared or deviance function), but I would like to estimate the quality of the prediction $M(x)$ for this particular $x$. It would be great if anyone could point me to the literature on this problem.</p>
",2013-10-25 14:36:12.433
58242,23046.0,1,,,,Directed Graph to Regression Help,<regression>,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/YwgCn.jpg"" alt=""enter image description here""></p>

<p>So I have this directed graph (above)</p>

<p>Each arrow represents a causal link. Is it possible to calculate the affect of X on Z, where all variables are observed except U and W. If so, what would the regression equation(s) look like. I believe it has to do with 2 step least squares, but I haven't been able to figure it out. Any help/tips would be much appreciated. Thanks.</p>
",2013-10-25 18:39:17.683
58227,23041.0,1,,,,Does it matter if I use correlations or regression coefficients to suggest areas to focus on to improve overall customer satisfaction?,<correlation><multiple-regression>,CC BY-SA 3.0,"<p>I am working with customer satisfaction data where the dependent variable is ""Overall satisfaction"" and the independent variables are satisfaction with various areas such as customer support, delivery etc.</p>

<p>I want to suggest areas where the company should focus on in order to improve overall satisfaction. </p>

<p>Option 1: I could look at correlations between the 'Overall satisfaction"" and the independent variables and suggest that the company focus on the top 3 positive correlations as areas for improvement.</p>

<p>Option 2: I can use a linear regression and suggest that the company should focus on the areas associated with the 3 highest regression coefficients.</p>

<p>Are the two options equivalent? If not, which one is the better approach? </p>
",2013-10-25 15:15:10.080
58228,23042.0,1,,,,Sample size and correct choice of test in g*power,<anova><effect-size><statistical-power>,CC BY-SA 3.0,"<p>I am new to g*power and have a question about which test I have to choose and how to interpret the given sample size.</p>

<p>I have 2 measurements (pre / post), one control-group and one intervention-group. In a reference study, I was able to find an effect size of .7 for the most important parameter. In g*power I chose F-test -> ANOVA RM within-between interaction, a power of 0.8 and alpha .05.</p>

<p>Here are my questions:  </p>

<ol>
<li>Did I choose the correct test?</li>
<li>Is the resulting sample size referring to each group or total?</li>
<li>Regarding the effect size of .7 I found in a reference study, am I allowed to fill it in g*power -> effect size f, or should I leave it as it was at .25?</li>
</ol>
",2013-10-25 15:56:13.647
58229,22637.0,1,58235.0,,,Cumulative Distribution Function Inequality (Discrete Distributions),<probability><discrete-data>,CC BY-SA 3.0,"<p>Let a discrete Random Variable $T$ have CDF $F_T(T)$. Could you please help me understand why $$ P \left[ F_T (T) \leq a_1 \right] \leq a_1 $$</p>

<p>I know that the result holds with equality for the continuous case, it is known as Probability Integral Transform, but I am having trouble understanding it for the Discrete one since the inverse is not defined. Thank you.</p>
",2013-10-25 16:11:47.767
58230,306.0,2,,55722.0,,,,CC BY-SA 3.0,"<p>If you want to read probability as a story, read the best book ever by <a href=""http://rads.stackoverflow.com/amzn/click/0471257087"" rel=""noreferrer"">Feller</a>. I am also guessing that you do not want to go to the level of measure theoretic definition of probabilities which has specialized books. another beginner level book is from <a href=""http://rads.stackoverflow.com/amzn/click/032179477X"" rel=""noreferrer"">Ross</a>. Other specialized applications have specialized books. so more information will gather better suggestions.</p>
",2013-10-25 16:14:42.940
58231,20473.0,2,,58227.0,,,,CC BY-SA 3.0,"<p>If I understand correctly, customers rate the company in various aspects of the transaction, and then, customers again give an overall score. This is the real-world structure. Making an assumption that customers are reasonably rational (i.e. consistent in their opinions), it means that somehow, they, in their minds, construct some sort of ""weighted average"" in order to go from the partial scores to the overall score.  </p>

<p>Then you should use the regression approach, which reflects the above situation. Using partial correlation coefficients does not capture how one reasonably believes that the customers thought and acted when scoring the company.  </p>

<p>This regression is in the spirit of ""hedonic index regression"", if we view ""overall satisfaction"" as the ""price"" of the ""product"" named ""transacting with company"", and the regressors as ""features"" of the product (that are provided in different levels for each customer, and hence their variability).</p>

<p>If the rankings are consistently coded (say, a higher number means a higher level of satisfaction for the partial scores and for the overall score), then a higher estimated regression coefficient on a partial score will indicate that this aspect of the transaction ""bears more heavily"" (has a higher marginal effect) on ""overall satisfaction"", and so indeed, focusing and improving on the areas with the higher regression coefficients, should yield larger benefits in overall satisfaction.  </p>

<p>But also, in order to finally decide on the prioritization, one should also look how the various areas compare in average score. Say ""customer support"" has a higher regression coefficient than ""delivery"", but also, ""customer support"" is on average rated already very high by customers, compared to ""delivery"". Then the efforts to further improve ""customer support"" may be more costly and difficult, compared to improving ""delivery"". So while one unit of increase in customer support satisfaction may yield higher overall satisfaction increase compared to one unit increase in ""delivery"", this one unit increase may be more costly to achieve in customer satisfaction than in delivery, offsetting partially, or fully, the economic gains from the increase in ""overall satisfaction"".   </p>

<p>Of course this last issue is not a statistical question, but I mentioned it so that any prioritization suggestion based on statistical analysis, at least mentions this aspect that must be taken into account for the final decisions.</p>
",2013-10-25 16:25:37.833
58232,,2,,58194.0,user31668,,,CC BY-SA 3.0,"<p>Unless the forecasts (or your metric) for each event are commensurate, you will need to make 100 separate evaluations, one per event. For each event $i$ and associated forecaster $j$, you could try a linear mixed model:</p>

<p>$y_{ij} = \mu_i + \sum\limits_{k=0}^{\#Teams}a_{ik}T_{ijk}+\epsilon_j$</p>

<p>Where $\mu_i$ is the mean forecast across all members,$T_{ijk}$ is a binary variable indicating whether person j was assigned to team k for question i, with $a_{ik}$ being the team effect for question i. $\epsilon_j$ is the individual effect.</p>

<p>Now, if you make ""completed"" datasets via imputation (and the forecasts/metrics are commensurate) then you can do an analysis across all questions at once, so each person will have 100 y's associated with them.</p>

<p>I'm not familiar with how to work directly with unbalanced designs, so someone else may have a more sophisticated approach, but hopefully this give you ideas or gets you started.</p>
",2013-10-25 16:29:40.270
58233,12358.0,2,,58202.0,,,,CC BY-SA 3.0,"<p>You can think of the relationship between $\lambda$ and $\sigma$ as just
a change of variables, a.k.a. a reparameterization, of the probability distribution that was initially specified in terms of $\sigma$.</p>

<p>We have the random variable $\sigma$ distributed as $p(\sigma)=1/500$ for $0&lt;\sigma&lt;500$.  We want the distribution for $\lambda=\sigma^{-2}$:</p>

<p>\begin{equation}
\begin{aligned}
p(\sigma) d\sigma &amp;= p(\lambda)d \lambda \\
\frac{\lambda^{-3/2}}{1000} d\lambda &amp;= p(\lambda) d\lambda \\
\frac{1}{500^2} &lt;&amp; \lambda &lt; \infty
\end{aligned}
\end{equation}</p>

<p>(Deriving this is involves equating the CDFs, $\Phi( \sigma) =\Phi(\lambda)$, reversing the  sense of $\sigma$, i.e. remapping $\sigma \rightarrow 500-\sigma$, and then taking the derivative of the resulting CDF)</p>
",2013-10-25 16:32:57.587
58234,6204.0,1,58237.0,,,"Why is standard error sometimes used for ""error bands"" in plots?",<statistical-significance><data-visualization><confidence-interval><interpretation><standard-error>,CC BY-SA 3.0,"<p>It seems that often what someone really wants to plot is a confidence interval of some kind, but using SE for this purpose I think only ends up comprising something like a 68% confidence band. Therefore, plotting SE for error bars instead of a wider band more representative of the significance level of your analysis visually suggests significance in your data that may not actually be there.</p>

<p>Consider the following concrete example:</p>

<pre><code>set.seed(123)
X &lt;- rnorm(100, 0, 1)
Y &lt;- rnorm(100,1.7,5)
df = data.frame(X,Y)

boxplot(df)

se.x = sd(X)/sqrt(length(X))
se.y = sd(Y)/sqrt(length(Y))

X.err.CI = 1.96*se.x
Y.err.CI = 1.96*se.y


plot(1:2, colMeans(df), ylim=c(-1,3), xlim = c(0.5,4.5), col=""dark green""
     , main=""Comparison of SE bars vs 95% CI"")
lines(c(1,1), c(mean(X) + X.err.CI, mean(X) - X.err.CI), col=""dark green"")
lines(c(2,2), c(mean(Y) + Y.err.CI, mean(Y) - Y.err.CI), col=""dark green"")
text(1:2 + .2, colMeans(df), c(""X"",""Y""))

points(3:4, colMeans(df), col=""blue"")
lines(c(3,3), c(mean(X) + se.x, mean(X) - se.x), col=""blue"")
lines(c(4,4), c(mean(Y) + se.y, mean(Y) - se.y), col=""blue"")
text(3:4 + .2, colMeans(df), c(""X"",""Y""))

abline(v=2.5, lty=2)

legend(""topright""
       ,c(""95% CI"", ""+/- SE"")
       ,lty=c(1,1)
       ,pch=c(1,1)
       ,col=c(""dark green"", ""blue"")
       )
</code></pre>

<p><img src=""https://i.stack.imgur.com/IyUoQ.jpg"" alt=""enter image description here""></p>

<p>If we just base our analysis on SE (the image on the right), visually it appears that there is significance between the means of X and Y because we don't have overlap in our error bars. But if we're testing at a 5% significance level, plotting the 95% confidence bands shows that this is clearly not the case. </p>

<p>Since we can expect that a test at the 32% level will never be appropriate, why even show the SE bars since they will probably be interpreted as though they represent a confidence interval? Do people use SE bars instead of more meaningful CIs because it's moderately easier to calculate (e.g. using a built-in function in Excel)? It seems that we're paying a pretty high cost in terms of the interpretability of our graphic in exchange for a few minutes' less work. Is there some value/utility in SE bars that I'm missing?</p>

<p>For context, I was prompted to write this after skimming <a href=""http://qbox.wharton.upenn.edu/documents/mktg/research/Losing_and_Winning.pdf"" rel=""noreferrer"">this article</a>. I was frustrated by the lack of confidence intervals in the plots provided by the authors, and then when they did finally provide them, it turned out they were just SE bars. </p>
",2013-10-25 16:33:44.443
58243,22555.0,2,,58234.0,,,,CC BY-SA 3.0,"<p>Whether from convention or otherwise, it is <em>honest</em> in the sense that it is easy for the reader to develop their own idea of significance, ie mentally the reader can consider a multiple of, say 2 or 3 times larger to get their own idea of significance.  In a sense you are letting the data <em>speak for itself</em> rather than <em>speaking for the data</em>.</p>

<p>From that perspective it is logical to provide SE as the basis for banding.  In my view, however, the caption of the chart should clearly state that the basis of banding is, in fact, one SE.  Similarly these should not be identified in any way as <em>confidence intervals</em> but simply as <em>properties</em> of the data set.</p>
",2013-10-25 18:49:00.690
58342,1406.0,2,,58027.0,,,,CC BY-SA 3.0,"<p>This is a textbook example of spurious time series regression. The levels are highly correlated, but the differences are not. This happens when  we have two independent random walk processes. To make sure that this is really the case, check that the residuals from the level regression have unit-root and that the residuals from the difference regression do not have it. </p>
",2013-10-28 08:16:41.137
58235,10135.0,2,,58229.0,,,,CC BY-SA 3.0,"<p>Try to draw CDF of a Discrete random variable like the (upper) one you have <a href=""http://en.wikipedia.org/wiki/File:Discrete_probability_distribution_illustration.svg"" rel=""nofollow noreferrer"">here</a>. Now draw a horizontal line to indicate the level of $a_1$. All you need to do is to find the values of $T$ such that your CDF i.e. $F_T$ satisfies $F_T(T)\leq a_1$. You can move your $a_1$ vertically. Now depending on the level of $a_1$ sometimes you get $P[F_T(T)\leq a_1]&lt;a_1$ and sometimes $P[F_T(T)\leq a_1]= a_1$. The equality happens when your $a_1$ is equal to one of those horizontal (red) lines in the plot of your CDF. OK, see $a_1$ in the graph below. <img src=""https://i.stack.imgur.com/VQedD.jpg"" alt=""enter image description here""><br>
For what values of $T$ you have $F_T(T)\leq a_1$? Obviously for $t&lt;t_1$. For $t&lt;t_1$, $P_T(t)=P(T&lt; t_1)=0$. And as the plot shows you have $0&lt;a_1$. So the condition you want to prove is correct in this case. Now look at $a_1$ below. <img src=""https://i.stack.imgur.com/A1Nli.jpg"" alt=""enter image description here""><br>
Again for what values of $T$ you have $F_T(T)\leq a_1$? Obviously for all $t&lt; t_2$. Now if $t_1\leq t&lt; t_2$ we have $P_T(t)=P(T\leq t)=a_1$. In this case you will end up with equality i.e. $P[F_T(T)\leq a_1]= a_1$.  And if $t&lt;t_1$ then $P_T(t)=P(T\leq t)=0$. Here again as the graph shows the condition holds i.e. $0\leq a_1$. You can do exactly the same argument if you move $a_1$ vertically.</p>
",2013-10-25 16:58:25.353
58236,23045.0,1,58240.0,,,Pearson's correlation for time series requires normally distributed data?,<time-series><normal-distribution><pearson-r>,CC BY-SA 3.0,"<p>In order to use Pearson's correlation to measure the similarity  of two time series, is  normal distribution of both time series a necessary condition? </p>
",2013-10-25 16:59:53.883
58237,12358.0,2,,58234.0,,,,CC BY-SA 3.0,"<p>Mostly its that ""it's been done that way in the past"", but in some domains it is precisely because the authors are not drawing statistical inferences directly from the reported standard errors (even though, for the example paper 
it might be reasonable to do so).</p>

<p>As an example, physics research papers often depict the standard errors related to (estimated) statistical errors in the data collection.  These are usually estimated from
running (as much a possible) the same experimental  multiple times using the same setup and estimating the variance.  However, these statistical errors
are only very rarely used in a direct confidence interval/degree of significance
type of assessment.  This is due to the fact that in most experiments systematic
errors of various type are likely to be larger than the statistical errors, and these types of errors are not amenable to statistical analysis.  Thus, representing the 95% confidence interval based on just the statistical errors could be deceiving.  Experimental particle physicists in particular go to great pains to identify statistical uncertainties, systematic uncertainties and then combine them (in physics community approved ways) into confidence intervals (the preprints on the discovery of the Higgs boson are probably easily found examples of this).</p>
",2013-10-25 17:03:36.480
58238,7155.0,2,,58226.0,,,,CC BY-SA 3.0,"<p>To evaluate the probability of observing some $x \in R^n$ in a data set defined by $D \in R^{i \times n}$, where $i$ is observations and $n$ is the cardinality of your features you can use density estimation/one-class classification.</p>

<p>While it stands to reason that there exists some relationship between the $p(x \in D$) and the quality of your prediction, you haven't observed this relationship.</p>

<p>What I'd recommend instead is exploring the literature on nonparametric confidence intervals. In particular, Gaussian Processes. They produce standard error estimates of your prediction and confer all of the advantages of other kernel machines at adapting to many different kinds of features.</p>

<p>The drawback is that they don't scale well with data size, without some hacks I've yet to master, and they are only good for regression and structured learning problems.</p>

<p>Gradient boosting algorithms can be adapted to produce confidence intervals, are good at classification and scale well to large datasets.</p>
",2013-10-25 17:21:53.337
58239,668.0,2,,58229.0,,,,CC BY-SA 3.0,"<p>Consider a box $\Omega$ filled with tickets.  On each ticket $\omega$ is written a number called $X(\omega)$.  For any number $x$, whether or not it appears among the tickets, $F_X(x)$ is (defined to be) the proportion of tickets for which $X \le x.$</p>

<p>Let's add some new information to each ticket $\omega$: next to the value of $X$ written on it, we will also write the value of $F_X(X(\omega))$: it is the proportion of all tickets with values of $X$ less than or equal to <em>this</em> value, $X(\omega).$  (It's the same concept as a percentile or quantile: the tickets with the smallest values of $X$ get the smallest proportions and the tickets with the largest values of $X$ get the largest proportions.)  These new values, being proportions, lie between $0$ and $1$ inclusive.  But, when $X$ is discrete, <em>they will not include all possible numbers,</em> but only the proportions that actually occur in the box.</p>

<p>Consider drawing a single ticket from this box at random. Fixing a number $a$ in advance, what is the chance that the <em>new</em> value (the ""quantile"") written on the ticket will not exceed $a$?  Of course it's the proportion of tickets with values of $a$ or lower.  But all such tickets, by construction, have values of $X$ that lie within the lower $100a\%$ of all the values.  Therefore this chance cannot exceed $a$.</p>

<p>The chance might be strictly less than $a$ when $a$ is not one of the actual proportions in the box.  Because it cannot be greater than $a$ and now cannot be equal to $a$ it <em>has</em> to be less than $a$!</p>

<p><strong>A simple example</strong> is afforded by a box with two tickets: on one of these $X$ equals $0$ and on the other it equals $1$.  When we write the proportions on the tickets, then, we will write $1/2$ (or $50\%$) on the first ticket (because half the tickets have values of $0$ or less) and $1$ (or $100\%$) on the second ticket (because all the tickets have values of $1$ or less).</p>

<p>What is the chance that this new value on a randomly drawn ticket will be less than or equal to $a=3/4$ (or $75\%$)?  Because the new values are only $50\%$ and $100\%$, and half of them are less than $75\%$, the answer obviously is $1/2$. This is strictly less than $a$ <em>because there are no proportions in the box between $50\%$ and $75\%$.</em>  The issue is just that trivial and simple.</p>

<hr>

<p>The preceding used a <a href=""https://stats.stackexchange.com/questions/50/what-is-meant-by-a-random-variable/54894#54894"">tickets in a box metaphor</a> for reasoning about random variables.  If we replace $\Omega$ by a probability space, insist that $X$ be a measurable function, and understand ""proportion"" as the value of the probability measure, then we will have a rigorous proof.  And it's still just as trivial.</p>
",2013-10-25 17:37:09.560
58240,15827.0,2,,58236.0,,,,CC BY-SA 3.0,"<p>Several questions are bundled together here: </p>

<ul>
<li><p>Pearson correlation measures linearity of relationship, not similarity of values. $y$ and $a + by$ have correlation $+1$ for $b &gt; 0$: make $a$ as different from zero or $b$ as large as you like, and the similarity is at best one of shape, not size. (Concordance correlation does measure agreement in the sense that $y = x$ is required for perfect positive correlation.) </p></li>
<li><p>Normality of distribution is not a requirement to measure correlation; correlation is perfectly well defined as a descriptive statistic (or even as a non-statistical property; it's just a cosine from one point of view) so long as both variables are genuinely variable. Marginal distribution is of concern if you wish to test for significance, e.g. produce a P-value. </p></li>
<li><p>But you have time series and can expect dependence in time and (quite likely) other kinds of structure. The standard machinery for Pearson correlation is for independent data, and no inference for Pearson correlation for time series can be taken seriously without adjustment for dependence structure. </p></li>
</ul>

<p>What is the real problem? For assessing similarity of two time series, I would always start with plotting the series and examining (as appropriate) the difference or ratio between them. The next step is harder and entails modelling the series to see if they have the same structure; others will predictably make positive suggestions here. Also, here as elsewhere, a single summary measure will rarely do justice to the fine structure of interesting data. </p>
",2013-10-25 17:37:25.393
58278,15473.0,4,,,,,,CC BY-SA 3.0,"Refers to Toeplitz matrix or diagonal-constant matrix, which has constant entries on the diagonals, i.e. $A_{ij}=a_{i-j}$.",2013-10-26 19:31:28.250
58244,20388.0,2,,58138.0,,,,CC BY-SA 3.0,"<p>a 'scale' matrix can be any matrix that is positive definite. Wishart distribution is often used in Bayesian hierarchical model to capture characteristics of a inverse covariance matrix.</p>

<p>Back to your problem, if you read the help page of <code>rWishart</code> carefully, it says:</p>

<blockquote>
  <p>If X1,...,Xm, Xi in R^p is a sample of m independent multivariate Gaussians with <strong>mean (vector) 0</strong>, and covariance matrix Î£, the distribution of M = X'X is W_p(Î£, m).</p>
</blockquote>

<p>However in your toy example, you chose to sample $X_i$'s with different means and different variance, and the degree of freedom $p$ is predetermined by the size of your sample $X_i$, not randomly chosen.</p>

<p>A better example can be constructed as such:</p>

<pre><code>require(MASS)
data = cbind(rnorm(100,0,5),rnorm(100,0,2),rnorm(100,0,3))
Sigma = cov(data) % this is a 3 by 3 matrix
eigen(Sigma) %check positiv definite

%construct X
X = mvtnorm(100,rep(0,3),Sigma)

%define d.f.
df = dim(X)[1]

%generate random wishart sample with df and Sigma
rWishart(10,df,Sigma)

% compute X'X
t(X)%*% X
</code></pre>

<p>You'll find that these random samples are roughly in the range of X'X, a more rigorous check can be done by looping said X'X a few times(~100,000) and take the empirical mean. In theory this should agree with the first moment of Wishart distribution m*Î£ by law of large numbers.</p>

<p>You can certainly generate non-central Wishart distribution, a good reference on this topic (or in fact any matrix variate distribution) is to look at <a href=""http://books.google.co.uk/books/about/Matrix_Variate_Distributions.html?id=PQOYnT7P1loC"" rel=""nofollow"">Matrix Variate Distributions</a> by Gupta and Nagar.</p>

<p>Hope this helps :)</p>
",2013-10-25 18:58:47.143
58245,21905.0,1,,,,"Expected value of $Ye^X$ where $X \sim U(0,1)$ and $Y \sim U(0,1)$",<self-study><mean><expected-value><uniform-distribution><joint-distribution>,CC BY-SA 3.0,"<p>I am trying to find the expected value of $Z$ where $Z = Y\cdot e^X$ where $Y \sim U(0,1)$ and $X \sim U(0,1)$.</p>

<p>My attempt so far:</p>

<p>$$F_Z(z) = P(Ye^X \le z) = \int \int_{Ye^X \le z} f(x,y)\, dxdy$$</p>

<p>Where $f_{X,Y}(x,y) = f_y\cdot f_{e^x}$</p>

<p>$$f_Y(y) = \frac{1}{1-0}\,, \quad  y \in (0,1)$$</p>

<p>I am stuck trying to find $f_{e^X}$ but I cannot remember how to find that pdf.</p>
",2013-10-25 19:06:11.117
58246,23048.0,1,,,,Finding the expected value of two normal random variables,<self-study><normal-distribution><mean><expected-value>,CC BY-SA 3.0,"<p>Suppose $a_1 = b + c_1$ and $a_2 = 2b + c_2$ where $b, c_1, c_2$ are all $N(0,1)$</p>

<p>Find $E[b|a_1,a_2]$</p>

<p>My attempt:
As $E[b] = 0$, I assume $E[b|a_1, a_2] = 0$. Is this a logical assumption?</p>
",2013-10-25 19:19:09.190
58247,23046.0,2,,58241.0,,,,CC BY-SA 3.0,"<p>@AsymLabs In R, you can just use the command lm(c(32,40,46) ~ c(1,2,3), weights=1/c(6,8,40)). To get slope = 7.4359 and intercept = 24.7179. That gives the fit whuber described. It's a regression weighted according to the inverse of the variance.</p>
",2013-10-25 20:13:45.023
58248,12900.0,1,60256.0,,,Machine learning applications in number theory,<machine-learning><modeling><references><mathematical-statistics>,CC BY-SA 4.0,"<p>Is there any research into or applications of machine learning in number theory?</p>
<p>I am also looking for (leading examples of) statistical/empirical analysis of number theory questions. Also wondering if genetic algorithms in particular have ever been used in these areas.</p>
<ul>
<li><p>roughly related question on other site: <a href=""https://cstheory.stackexchange.com/questions/15039/why-can-machine-learning-not-recognize-prime-numbers"">Why can machine learning not recognize prime numbers?</a></p>
</li>
<li><p>an area in number theory that seems to have had some statistical analysis, the <a href=""http://en.wikipedia.org/wiki/Collatz_conjecture"" rel=""nofollow noreferrer"">Collatz conjecture</a>.</p>
</li>
<li><p>possibly somewhat related, <a href=""http://en.wikipedia.org/wiki/Automated_theorem_proving"" rel=""nofollow noreferrer"">automated theorem proving</a>.</p>
</li>
</ul>
",2013-10-25 20:58:02.690
58249,13385.0,2,,58245.0,,,,CC BY-SA 3.0,"<p>There is definitely an easier approach to this problem (hints were given in the comments), but since you asked about a specific step, I'll go from there.</p>

<p>You want to compute the pdf $f_{e^X}(x)$.  Let's start with:</p>

<p>$$ F_{e^X}(x) = P(e^X &lt; x)
              = P(X &lt; \log x)
              = (F_X \circ \log)(x)
$$</p>

<p>Recall that we can compute $f_{e^X}$ by differentiating $F_{e^X}$.  In this case, you can use the chain rule.</p>

<p>This kind of transformation generalizes to the multivariate case.</p>
",2013-10-25 23:49:46.850
58250,23052.0,1,,,,Correlation coefficient between two arrays of 2D points?,<correlation>,CC BY-SA 3.0,"<p>I have two arrays of 2D points and I need to estimate their correlation. What formula should I use?</p>

<p>Example of arrays:</p>

<p>$$X: ((1,5),(2,5),(1,7),(4,1)),$$</p>

<p>$$Y: ((3,4),(1,6),(4,6),(4,3)).$$</p>
",2013-10-26 00:01:06.250
58251,23053.0,1,,,,Are $\mathbb{F}_2$-linear combinations of random variables in an i.i.d. Bernoulli process again an i.i.d. Bernoulli process?,<mathematical-statistics><binomial-distribution><bernoulli-distribution>,CC BY-SA 3.0,"<p>I'm having trouble understanding how certain combinations of random variables can correlate. The problem is as follows:</p>

<p>I have a binary $m \times n$ matrix $A$ of full rank (over the finite field $\mathbb{F}_2$ with two elements), where each row has exactly $w$ $1$'s (i.e., their weights are all $w$).</p>

<p>I randomly pick an $n$-dimensional binary vector $\boldsymbol{v} = (v_0,\dots,v_{n-1}) \in \mathbb{F}_2^n$, where each $v_i$ is $1$ with probability $p$ and $0$ with probability $1-p$. The probability $p$ is assumed to be strictly smaller than $\frac{1}{2}$. Here, $v_i$ are all independent, so that picking $\boldsymbol{v}$ is seen as an i.i.d. Bernoulli process of probability $p$.</p>

<p>I am going to hash this vector $\boldsymbol{v}$ into an $m$-dimensional vector $\boldsymbol{v}' = (v'_0,\dots,v'_{m-1})$ by taking $\boldsymbol{v}' = A\boldsymbol{v}^T$ over $\mathbb{F}_2$. So, if I write the $i$th row of $A$ as $\boldsymbol{r}_i$, each $v'_i$ is the product $v'_i = \boldsymbol{r}_i\cdot\boldsymbol{v}^T$ between the $i$th row of $A$ and $\boldsymbol{v}$. In other words, I'm just taking the mod $2$ sum of some bits of $\boldsymbol{v}$.</p>

<p>Because the weight of $\boldsymbol{r}_i$ is assumed to be $w$, the probability $P(v'_i = 1)$ that the $i$th bit $v_i'$ of the hashed vector $\boldsymbol{v}'$ is $1$ is</p>

<p>$$P(v'_i = 1) = \sum_{x: \text{odd}}^{w}\binom{w}{x}p^x(1-p)^{w-x},$$</p>

<p>which is a constant if we fix $p$ and $w$, and is also uniform across all $v_i'$.</p>

<p>Now, if $A$ had linearly dependent rows, the hashed bits $v_i'$ are clearly not independent. My question is:</p>

<p><em><strong>What if all rows of $A$ are linearly independent? Is $\boldsymbol{v}'$ an i.i.d. Bernoulli process with probability $P(v'_i = 1)$? If not, can I approximate it as one if $\boldsymbol{r}_i$ and $\boldsymbol{r}_j$ have only few $1$'s at the same columns for all $i \not=j$?</em></strong></p>

<p>I remember I read in some research paper in electrical engineering that $\boldsymbol{v}'$ is i.i.d. Bernoulli if rows of $A$ are linearly independent, <strike>although I can't seem to remember where it was.</strike> (I found one recent paper that says this is the case: <a href=""http://dx.doi.org/10.1109/LCOMM.2011.122810.102182"" rel=""nofollow"">V. Toto-Zarasoa, A. Roumy, C. Guillemot, <em>Maximum likelihood BSC parameter estimation for the Slepian-Wolf problem,</em> IEEE Comm. Lett. <strong>15</strong> (2011) 232â€“234.</a> It's Lemma 1 on the second page.) But now I think about it, this is counterintuitive (to me) because if two rows of $A$ have $1$ at the same column, that means that I took at least one same $v_i$ when hashing $\boldsymbol{v}$ for those two rows, so the corresponding $v_i'$ look correlated.</p>

<p>If my reasoning and Alecos Papadopoulos's answer are correct, the last part of the above question in boldface becomes essential. For instance, are there standard methods for evaluating how close or similar a given set of random variables like $\boldsymbol{v}'$ is to i.i.d. Bernoulli?</p>
",2013-10-26 00:26:12.477
58252,3185.0,1,58392.0,,,Sign of the unnormalized log likelihood in Ising model,<self-study><graphical-model>,CC BY-SA 3.0,"<p>Here is a section of <em><a href=""http://www.cs.ubc.ca/~murphyk/MLbook/pml-print3-ch19.pdf"" rel=""nofollow noreferrer"">Machine Learning: a Probabilistic Perspective</a></em> by Kevin Patrick Murphy</p>

<p><img src=""https://i.stack.imgur.com/jatxV.png"" alt=""enter image description here""></p>

<p>I don't understand in (19.18) why there is a negative sign. For me, $\log \tilde{p}(\mathbf{y})=\sum_{s\sim t}\log\psi_{st}(y_s,y_t)$ holds. When $y_s$ and $y_t$ agree, $\log\psi_{st}(y_s,y_t)=w_{st}$, otherwise $\log\psi_{st}(y_s,y_t)=-w_{st}$. So shouldn't it be $\log \tilde{p}(\mathbf{y})=\sum_{s\sim t} y_sw_{st}y_t$? Also, when all entries of $\mathbf{y}$ agree, $\sum_{s\sim t} y_sw_{st}y_t$ is maximized because all summands are positive. So where is the problem? Do I miss something? Thank you.</p>
",2013-10-26 02:02:38.987
58253,7007.0,2,,58246.0,,,,CC BY-SA 3.0,"<p>Let $B,C_1,C_2$ be independent $\mathrm{N}(0,1)$ random variables. Define $A_1=B+C_1$ and $A_2=2B+C_2$. Since we are conditioning on the same information, and $C_1$ and $C_2$ have the same distribution, by symmetry we have
$$
  \mathrm{E}[C_1\mid A_1,A_2] = \mathrm{E}[C_2\mid A_1,A_2]
$$
almost surely (we haven't used the independence assumption yet). Hence,
$$
  \mathrm{E}[B\mid A_1,A_2] = \mathrm{E}[B\mid A_1,A_2] + \mathrm{E}[C_2\mid A_1,A_2] - \mathrm{E}[C_1\mid A_1,A_2]
$$
$$
  = \mathrm{E}[B+C_2-C_1\mid A_1,A_2] = \mathrm{E}[A_2-A_1\mid A_1,A_2]
$$
$$
  = \mathrm{E}[A_2\mid A_1,A_2] - \mathrm{E}[A_1\mid A_1,A_2] = A_2 - A_1 = B+C_2-C_1
$$
almost surely. Therefore (Why? Remember the independence assumption and use <a href=""http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables"" rel=""nofollow noreferrer"">this</a>. What is the distribution of $-C_1$?),
$$
  \mathrm{E}[B\mid A_1,A_2]\sim \mathrm{N}(0,3) \, .
$$</p>

<p>(If you have any doubts about $\mathrm{E}[B\mid A_1,A_2]$ being a random variable, <a href=""https://stats.stackexchange.com/questions/38700/conditional-expectation-of-multivariate-distributions/38707#38707"">check this answer</a>.)</p>
",2013-10-26 02:32:12.797
58254,13385.0,2,,55722.0,,,,CC BY-SA 3.0,"<p>Finding a single, comprehensive book will be very difficult.  If you're asking because you want to do some self-study, get a couple of used texts instead of a single new one.  You can get classics for $3-10 dollars if you look around online.</p>

<p>Feller's ""Introduction to Probability"" is great for its completeness and expository style, but I don't like the exercises much.  And the exposition would not make it so good for a reference.  He tends to have a lot of long examples, which is great for fostering understanding, and not so great for looking things up.</p>

<p>I enjoyed Allan Gut's ""An Intermediate Course in Probability"".  There is some overlap with Feller, but it goes into greater depth on those topics.  He covers the various transformations, order statistics (which, if I recall, Feller only does by example).</p>

<p>Ross' Introduction to Probability Models is pretty comprehensive, but it is very example oriented.  Again, that is not my favorite style (I'd rather they saved those examples for exercises with hints, and kept them out of the main flow), but if it works for you, I can recommend it.</p>

<p>You might as well consider Cacoullos' ""Exercises in Probability"" and Mosteller's ""50 Challenging Exercises in Probability"".</p>
",2013-10-26 02:37:43.937
58255,22728.0,1,,,,Best subset selection,<feature-selection><model>,CC BY-SA 3.0,"<p>My statistical learning text claims that for best subset selection, 2^p total models must be fit through regression if for p covariates, we fit p choose k models at each k, k = 1,...,p.  I interpret this mathematically as 2^p = p choose 1 + ... + p choose k.  Why 2^p models?</p>
",2013-10-26 02:45:12.413
58256,594.0,2,,58255.0,,,,CC BY-SA 3.0,"<p>1) For each of p covariates, think of something indicating whether it's either included as a predictor (1) or it isn't (0). There are two possibilities of the indicator for each of $p$ predictors.</p>

<p>That is, there are 2 x 2 x 2 ... x 2 models (where there are p terms)</p>

<p>2) You're almost correct, in fact it's:</p>

<p>$$\sum_{i=0}^p {p\choose i} = 2^p$$</p>

<p>e.g. ${2\choose 0} + {2\choose 1} + {2\choose 2} = 1 + 2 + 1 = 2^2$</p>

<p>As for <em>why</em> that's true, think of the <a href=""http://en.wikipedia.org/wiki/Binomial_theorem#Statement_of_the_theorem"" rel=""noreferrer"">binomial expansion</a> of $(1+1)^p$.</p>
",2013-10-26 03:05:53.883
58257,17660.0,2,,55150.0,,,,CC BY-SA 3.0,"<h2>A Counterexample</h2>

<p>The problem doesn't seem to be that mean independence (the condition where $E[Y|X] = E[Y]$) implies that $Y$ and $X$ are uncorrelated. If $X$ and $Y$ are not correlated, it is not generally true that they are mean independent. So this doesn't seem problematic so far. </p>

<p>However, suppose you had a relationship (we can call it causal) defined as $Y = WX$, where $X$ is distributed with a standard normal distribution and $W$ is distributed with a Rademacher distribution so that $W = 1$ or $-1$, each with probability $1/2$ (<a href=""https://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent"" rel=""nofollow"">see this Wikipedia article</a>). Then notice that $E[Y|X] = E[Y]$. Under your definition, this relationship would not be causa even though $Y$ clearly depends on $X$.</p>

<h2>An Example of a Formal Way of Thinking About Causality</h2>

<p>To give you maybe a clearer and more mathematical way to look at causality, take the following example. (I borrow this example from the book ""Mostly Harmless Econometrics."") Suppose you want to analyze the effect of hospitalization on health. Define $Y_i$ as some health measure of individual $i$ and $D_i \in \{0,1\}$ to indicate whether or not that individual was hospitalized. In our first attempt, suppose we look at the average difference in health of the two kinds of individuals:
$$
E[Y_i | D_i=1] - E[Y_i|D_i=0].
$$
On first look at the data, you might notice, counter intuitively, that individuals that have been hospitalized actually have worse health than those that have not. However, going to the hospital certainly does not make people sicker. Rather, there is a selection bias. People who go to the hospital are those people that are in worse health. So this first measure does not work. Why? Because we are not interested in just the <em>observed</em> differences, but rather in the potential differences (we want to know what would happen in the counter-factual world).</p>

<p>Define the potential outcome of any individual as follows:
$$
\text{Potential Outcome} = \left \{
\begin{array}{ll}
Y_{1,i} &amp; \text{if } D_i = 1 \\
Y_{0,i} &amp; \text{if } D_i = 0.
\end{array}
\right .
$$
$Y_{0,i}$ is the health of individual $i$ if he had not gone to the hospital, regardless of whether he actually went or not (we want to think about counterfactuals) and in the same way, $Y_{1,i}$ is the health of the individual is he did go. Now, write the actual observed outcome in terms of the potentials,
$$
Y_i = \left \{
\begin{array}{ll}
Y_{1,i} &amp; \text{if } D_i = 1 \\
Y_{0,i} &amp; \text{if } D_i = 0.
\end{array}
\right.
$$
Thus, $Y_i = Y_{0,i} + (Y_{1,i} - Y_{0,i}) D_i$. Now, we can define the causal effect as $Y_{1,i} - Y_{0,i}$. This works because it is in terms of potentials. Now, suppose we again look at the observed differences in average health:
\begin{align*}
E[Y_i | D_i=1] - E[Y_i|D_i=0] &amp;= E[Y_{1,i}|D_i = 1] - E[Y_{0,i}|D_i = 1] \\
    &amp; \qquad + E[Y_{0,i}|D_i=1] - E[Y_{0,i}|D_i=0].
\end{align*}
Notice that the term $E[Y_{1,i}|D_i = 1] - E[Y_{0,i}|D_i = 1]$ can be interpreted as the average treatment effect on the treated and $E[Y_{0,i}|D_i=1] - E[Y_{0,i}|D_i=0]$ as the bias in selection. Now, if the treatment $D_i$ is assigned randomly, then we have
\begin{align*}
E[Y_i | D_i=1] - E[Y_i|D_i=0] &amp;= E[Y_{1,i}|D_i] - E[Y_{0,i}|D_i=0] \\
    &amp;= E[Y_{1,i}|D_i] - E[Y_{0,i}|D_i=1] \\
    &amp;= E[Y_{1,i} - Y_{0,i}|D_i=1] \\
    &amp;= E[Y_{1,i} - Y_{0,i}],
\end{align*}
where we see that $E[Y_{1,i} - Y_{0,i}]$ is the average causal effect that we are interested in. This is a basic way of thinking about causality.</p>
",2013-10-26 07:05:51.210
58258,16737.0,1,,,,Overlapping time series: is there any better way to visualize them?,<data-visualization><data-transformation>,CC BY-SA 3.0,"<p>I have this time series dataset: </p>

<p><img src=""https://i.stack.imgur.com/eXXhd.png"" alt=""enter image description here""></p>

<p>The graph shows trend lines for 7 stock prices. They are very close and overlapping, but you will be able to get an idea that trend lines are layered (i.e. brown on the top and red/orange at the bottom, though far from conspicuous).</p>

<p>Is there any way to better visualize this data? like transforming $y$-axis to another scaling, mapping the whole thing onto cylinder/cone etc.? I tried with moving average, but the improvement is not so good.</p>

<p>NOTE: This is not an ML/DM problem. I am looking for a better/alternative/suitable visualization technique, that's all.</p>
",2013-10-26 07:20:29.463
58259,15806.0,1,58274.0,,,Reporting operative effect in paired t test,<t-test><effect-size><paired-comparisons>,CC BY-SA 3.0,"<p>I just want to make sure I have something clear in my head. When I calculate the effect size for a paired samples t-test after obtaining a significant result, I simply take the mean of the differences divided by the standard deviation of the differences to get an effect size d. Do I then need to take d and divide it by the square root of 1-r, where r is the correlation between pairs and r is estimated from the sample pairs? I am confused because dividing by the square root of 1-r supposedly gives me an ""operative effect size"" and I'm not really sure if the operative effect size is what I should be reporting in my analyses. For example, in this report I am working on, I need to know if there was an effect size of 2 SD. So when I calculate my effect size, should I be dividing by square root 1-r? I don't think so, I think I need to report the actual detected effect size and not the operative effect, but I would love a second opinion. Thanks!</p>
",2013-10-26 07:42:44.340
58260,15827.0,2,,58258.0,,,,CC BY-SA 3.0,"<p>Graphical comparison of time series is in principle straightforward: plot two or more series against time and look at the graph. Your example is one of many showing that it may not be so easy in practice. </p>

<p>This is pitched fairly generally. <strong>For stock prices, some of the strategies may not be especially relevant or successful, but they may have value for other kinds of series.</strong>  </p>

<p>Some solutions, direct or indirect, include </p>

<ul>
<li><p>Graphical multiples, as already suggested by @Glen_b. Each series could be plotted separately. An extension to the idea of showing a reference series is this: For each series, plot the other series as backdrop in a subdued colour (e.g. a light gray) and then plot the series of interest on top in a more prominent colour. </p></li>
<li><p>Smoothing the series first. Even if you are also interested in fine structure, smoothing can help establish general patterns of change and thus aid understanding. </p></li>
<li><p>Looking at differences or ratios. One series of interest, or an average or other reference series, can be used to look at differences, or as appropriate ratios, of series rather than the series themselves. So, for example, plot (this series $-$ IBM) or (this series / IBM). If using ratios, then consider logarithmic scale too. (Ratios depend on all values being positive, or at least  having the same sign, to work.)  </p></li>
<li><p>Changing the aspect ratio. Erratic series with numerous changes of direction are often best plotted with an aspect ratio yielding short, long graphs, which you may need to split into different sections. The ideal is that typical segments are at about $45^\circ$. (That is a counsel of perfection for very long series.) </p></li>
<li><p>Sampling. Do you need every value? Would plotting every $k$th value be as informative <em>visually</em>? In some cases, sampling should include local maxima and minima to show important details. The principle here is that short-term changes are often noise and lacking in interest or intelligibility.  </p></li>
</ul>
",2013-10-26 09:40:20.397
58261,20473.0,2,,58251.0,,,,CC BY-SA 3.0,"<p><strong>1) Meta-issue:</strong><br>
I believe the OP should include in the title of the question a signal that something special is going on here - for example instead of ""Are linear combinations of..."" the title should read ""Are $\mathbb{F}_2-$linear combinations of..."", so that the reader gets the message that concepts may have special meanings in this specific question.  </p>

<p><strong>2) Bernoulli or not Bernoulli?</strong><br>
By $\mathbb{F}_2-$arithmetic, the sum of i.i.d Bernoulli rv's is not a Binomial, since all probabilities are condensed on the values $0$ and $1$. We first derive the binomial distribution and then add the probabilities of all even values of the support to the probability of $0$, and the probabilities of all odd values of the support to the probability of $1$ -and thus we obtain again a Bernoulli r.v. This is just to validate the probability mass function included in the OP's question. It is  by construction a Bernoulli random variable, irrespective of how $P(v'_i = 1)$ is derived. Moreover if the number of $1$'s is the same in each row of the matrix $A$, then each $v'_i$ has an identical Bernoulli marginal distribution.</p>

<p><strong>3) $\mathbb{F}_2-$ linear independence.</strong><br>
$\mathbb{F}_2-$ linear dependence does not look much like the ""usual"" concept of linear independence. To be able to obtain a square matrix of full row/column rank under $\mathbb{F}_2-$arithmetic, I conjecture that $w$, the number of $1$'s in each row, should be an odd number. Consider a square matrix $n\times n$ matrix $A$, and the first element of one of its rows, say $a_{i1}$. I reason as follows:<br>
<strong>a)</strong> Assume $w$ is an even number.<br>
<em>a1)</em> Assume $a_{i1} = 0$. Then in the remaining $n-1$ elements of the row, there exists an even number of $1$'s, which by $\mathbb{F}_2-$arithmetic will give us $0$. The rest of the elements of the row all also zero, so overall the sum of the $n-1$ elements will give $0$, i.e equal to the value of the first element.<br>
<em>a2)</em> Assume $a_{i1} = 1$. Then in the remaining $n-1$ elements of the row, there exists an odd number of $1$'s, which by $\mathbb{F}_2-$arithmetic will give us $1$. The rest of the elements of the row are all zero, so overall the sum of the $n-1$ elements will give $1$, i.e again equal to the value of the first element.  </p>

<p>So if $w=even$, the  $\mathbb{F}_2-$sum of the $n-1$ columns will always equal the $n$-th column, depriving us of full rank.</p>

<p><strong>b)</strong> Assume $w$ is an odd number.<br>
<em>b1)</em> Assume $a_{i1} = 0$. Then in the remaining $n-1$ elements of the row, there exists an odd number of $1$'s, which by $\mathbb{F}_2-$arithmetic will give us $1$. The rest of the elements of the row all zero, so overall the sum of the $n-1$ elements will give $1$, i.e different to the value of the first element.<br>
<em>b2)</em> Assume $a_{i1} = 1$. Then in the remaining $n-1$ elements of the row, there exists an even number of $1$'s, which by $\mathbb{F}_2-$arithmetic will give us $0$. The rest of the elements of the row all also zero, so overall the sum of the $n-1$ elements will give $0$, i.e again different than the value of the first element.  </p>

<p>So it appears that $w= odd$, is at least a necessary condition to have a matrix $A$ of full column/row rank. </p>

<p><strong>4) Stochastic (in)dependence in the $\mathbb{F}_2-$world.</strong><br>
Does the characteristics of $\mathbb{F}_2-$field affect the <em>concept</em> of stochastic (in)dependence? No. Two r.v.'s are independent if and only if their joint distribution is the product of their marginals. Meaning, the conditional distributions must equal the unconditional ones. Maybe the way operations work in the $\mathbb{F}_2-$field produces some unexpected results?
Let's see: Assume that we have a square $A$ matrix that has $\mathbb{F}_2-$ linearly independent rows. The <em>column</em> vector process $\boldsymbol{v}'$, say of dimension $5\times 1$ is written</p>

<p>$$\boldsymbol{v}' = A\boldsymbol{v} = \left[\begin{matrix}
v_1'(v_0,...)\\
v_2'(v_0,...)\\
v_3'(v_0,...)\\
v_4'(v_0,...)\\
v_5'(v_0,...)\\
\end{matrix}\right]$$</p>

<p>Now assume that $w=3$ and, say, that the $1$'s in $A$ as dispersed such that we have</p>

<p>$$ v_2' = v_0+v_1+v_5,\qquad v_4' = v_2+v_3+v_5$$</p>

<p>Consider the conditional probability (under $\mathbb{F}_2-$ arithmetic)</p>

<p>$$P_{\mathbb{F}_2}(v_2' =1\mid v_4'=0) = P_{\mathbb{F}_2}(v_0+v_1+v_5 =1\mid v_2+v_3+v_5=0)$$</p>

<p>If the conditioning statement is to affect the probabilities of $v_2'$, it will do so through $v_5$: the fact that $v_2+v_3+v_5=0$ must affect the probabilities related to $v_5$. Under the ""usual"" arithmetic this would be obvious: it would mean that $v_5$ should equal zero. What happens under $\mathbb{F}_2-$ arithmetic?
We can examine
$$P_{\mathbb{F}_2}(v_5 =1\mid v_2+v_3+v_5=0) = \frac{P_{\mathbb{F}_2}(\{v_5 =1\}\land \{v_2+v_3+v_5=0\})}{P_{\mathbb{F}_2}(v_2+v_3+v_5=0)}$$</p>

<p>The possible values of $v_2+v_3+v_5$ under $\mathbb{F}_2-$ arithmetic are 
<img src=""https://i.stack.imgur.com/mZGnm.jpg"" alt=""enter image description here""></p>

<p>from which we get the following contingency table</p>

<p><img src=""https://i.stack.imgur.com/zaeyX.jpg"" alt=""enter image description here""></p>

<p>Therefore 
$$P_{\mathbb{F}_2}(v_5 =1\mid v_2+v_3+v_5=0) = \frac{2p^2(1-p)}{(1-p)^3+3p^2(1-p)}=\frac{2p^2}{1-2p+4p^2} \neq p$$
<em>except</em> when $p=1/2$ - but the set up explicitly specifies that $p&lt;1/2$ . So we conclude that 
$$P_{\mathbb{F}_2}(v_5 =1\mid v_2+v_3+v_5=0) \neq P_{\mathbb{F}_2}(v_5 =1)$$</p>

<p>and so 
$$P_{\mathbb{F}_2}(v_2' =1\mid v_4'=0) \neq P_{\mathbb{F}_2}(v_2' =1)$$</p>

<p>I have shown that, in general, the elements of the vector process $\boldsymbol{v}'$ are not independent (although they have identical marginals), even under $\mathbb{F}_2$-arithmetic. I have found and read the lemma in the paper the OP mentions. There is no actual proof, just a verbal assertion that they are independent, due to the $\mathbb{F}_2$-linear independence of the rows of $A$. Hopefully, I am mistaken, but for the moment it appears the assertion is not valid. </p>

<p><strong>5) Now what?</strong><br>
Not much. The joint distribution of the random vector will depend on how the $1$'s are allocated in matrix $A$. Without a specific form, the various measures of distance between distributions become vacuous. Intuitively, if $w$ is small relative to the length of the rows of $A$, then one can expect/hope, that the dependence will be relatively weak, and so pretending that the vector is i.i.d. won't hurt much... but without knowing the joint distribution, one cannot really tell... Copulas for discrete r.v.'s suffer from identification issues... I am still thinking about it, but I am not optimistic.</p>
",2013-10-26 10:08:41.883
58262,23058.0,1,,,,Simple OLS with two samples,<least-squares><unbiased-estimator>,CC BY-SA 3.0,"<p>I want to obtain an unbiased estimator for $b_1$ in a simple regression like that: $Y_i = B_0 + B_1X_i + u_i$ when I have two samples, always same size for Y and X, but once the sample size is l and once the sample size is m. The respective sample means $\bar{Y_l},\bar{X_l}$ and $\bar{Y_m},\bar{X_m}$ are given. Now I wonder how I cans tart to get an unbiased estimator?</p>

<p>My idea was to use the 'normal/one-sample' formula and just put weights (correcting for different sample size between the two independent sets of data) in front. </p>

<p>An estimator for $b_1$ would be: (X'X)$^{-1}$X'Y without matrices: $\frac{\sum X_iY_i - N \bar{Y}\bar{X}}{\sum X_i^2 -N \bar{X}^2}$</p>

<p>which I wanted to modify to $\frac{l}{m+l} \frac{\sum X_iY_i - L \bar{Y_l}\bar{X_l}}{\sum X_i^2 - L \bar{X_l}^2} + \frac{m}{m+l} \frac{\sum X_iY_i - M \bar{Y_m}\bar{X_m}}{\sum X_i^2 - M \bar{X_m}^2}$</p>

<p>The capital M and L denoting the respective sample size.</p>

<p>Now I am not sure if my result is right, as I cannot show if it is unbiased, to be honest. </p>

<p>Is it unbiased in probabilistic terms? Or is it just a wrong estimator?</p>
",2013-10-26 10:25:30.220
58279,22843.0,1,58284.0,,,What is a numerical example of $Var(X_1 + X_2) = Var(X_1) + Var(X_2)$,<variance><covariance>,CC BY-SA 3.0,"<p>I need a numerical example to illustrate cases where $Cov(X_1, X_2) = 0$. Can you think of examples involving functions or matrices? </p>
",2013-10-26 21:55:27.983
58263,10450.0,2,,58258.0,,,,CC BY-SA 3.0,"<p>I am not certain, what exactly you are trying to capture, but as they are financial time series I've assembled some possible alternate methods to visualize the information.</p>

<ul>
<li>As they are stock time series, and I assume returns or price differences, I would recommend integrating (cumsum or cumprod) the series. The cumulative price
series would be a better way to visually discern differences between the series.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/0XZlY.jpg"" alt=""enter image description here""></p>

<ul>
<li>If you are trying to visually get a feel for difference of the series
in the current    form, I would consider breaking up the series into
smaller time ranges (using       something like panels or trellis plots), as
the data looks too compressed to  discern much. Here, one can see some correlation of daily series on monthly time intervals.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/tJC4s.jpg"" alt=""enter image description here""></p>

<ul>
<li>You could also run overlapping density plots of each of the individual series to quickly ascertain differences in sample  statistics (mean, variance, higher moments).
In your case, I would expect to see some separation between the distributions, indicating differences in mean (drift), as well as differences in variance (volatility) between series.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/43wWj.jpg"" alt=""enter image description here""></p>

<p>The plots were generated via R.</p>
",2013-10-26 10:40:43.203
58264,22381.0,1,58271.0,,,How to interpret negative ACF (autocorrelation function)?,<time-series>,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/vvOCA.jpg"" alt=""Returns""></p>

<p>So I plotted the ACF/<a href=""http://en.wikipedia.org/wiki/Partial_autocorrelation_function"" rel=""noreferrer"">PACF</a> of oil returns and was expecting to see some positive autocorrelation but to my surprise I only get negative significant autocorrelation. How should I interpret the above graph?  They seem to indicate that there is a tendency for oil returns to increase when it decreased previously and vice-versa, thus the oscillating behaviour. Please correct me if I'm wrong.</p>
",2013-10-26 11:45:24.910
58265,1809.0,1,58266.0,,,Chi Squared Results in R and Python,<r><chi-squared-test><python>,CC BY-SA 3.0,"<p>Based on <a href=""https://stackoverflow.com/a/9330332/580010"">this</a> answer, Python requires expected values in a chi square test to be absolute frequencies. Consider the following in Python:</p>

<pre><code>import numpy
import scipy.stats
# chisquare function requires (observed, expected)
scipy.stats.chisquare(numpy.array([0,0,23,0]), numpy.array([1,1,1794,1]))
(1751.2948717948718, 0.0)
</code></pre>

<p>results in a p-value of 0 (whatever that means).</p>

<p>The same calculation in R, which requires that the expected values be proprotions:</p>

<pre><code>chisq.test(c(0, 0, 23, 0), p=c(1/1797,1/1797,1794/1797, 1/1797))

        Chi-squared test for given probabilities

data:  c(0, 0, 23, 0)
X-squared = 0.0385, df = 3, p-value = 0.998
</code></pre>

<p>resulting in a p-value of 0.998.</p>

<p>Which is correct?</p>
",2013-10-26 12:31:31.527
58266,503.0,2,,58265.0,,,,CC BY-SA 3.0,"<p>These two seem to be testing different things. The <code>Python</code> code looks like it is a two way chi square test (but a p value of 0 makes no sense here), while the <code>R</code> code is one way. I am not sure which you want.</p>

<p>To do the two way test in <code>R</code> use</p>

<pre><code>x1 &lt;- c(0, 0, 23, 0)
x2 &lt;- c(1, 1, 1794, 1)
chisq.test(x1, x2)
</code></pre>

<p>Which gives a p value of 0.5.</p>

<p>However, since a lot of the expected values are less than 5, <code>R</code>correctly gives a warning. You can simulate using </p>

<pre><code>chisq.test(x1, x2, simulate = TRUE)
</code></pre>

<p>which gives a p of 0.25</p>

<p>Your code also gives a warning, but this</p>

<pre><code>chisq.test(c(0, 0, 23, 0),
           p=c(1/1797,1/1797,1794/1797, 1/1797),
           simulate = TRUE)
</code></pre>

<p>gives a p of 1.</p>

<p>This certainly makes sense.  </p>

<p>I don't have Python so I can't say for sure what is going on there.</p>

<p>A two way chi square tests whether two categorical variables are associated with each other; a one way tests whether one categorical variable is distributed equal to a certain set of proportions.</p>
",2013-10-26 12:47:35.687
58267,20473.0,2,,58262.0,,,,CC BY-SA 3.0,"<p>The unbiasedness poperty of the OLS estimator in the linear regression model is a finite-sample property, and it is based on a specific assumption of the model being correct -that the regressors are ""strictly exogenous to the error term"", namely $E(u_i|\mathbf X)=0$.</p>

<p>So if you accept that this assumption holds, as you indicate in a comment, and so the OLS estimator for each sample has the unbiasedness property, then <strong>a combination of the two will be unbiased if it is a linear combination with weights adding up to unity (but not necessarily a convex combination)</strong>. Namely, let $\hat B_{1l}$ and $\hat B_{1m}$ be the two single sample estimators. Consider an estimator that it is some function of the two:</p>

<p>$$\hat B^* = h\left(\hat B_{1l},\hat B_{1m}\right) $$
Its expected value is</p>

<p>$$E\left[\hat B^*\right] = E\left[h\left(\hat B_{1l},\hat B_{1m}\right)\right] $$</p>

<p>If $h()$ is <em>not</em> an affine function, then by Jensen's inequality 
$$E\left[h\left(B_{1l},\hat B_{1m}\right)\right] \neq h\left(E\hat B_{1l},E\hat B_{1m}\right)$$
and in general $\hat B^*$ won't be unbiased.  </p>

<p>Assume now that $h()$ is affine namely</p>

<p>$$\hat B^* = a_0 +a_1\hat B_{1l}+a_2\hat B_{1m} $$</p>

<p>with $a$'s being constants. Then </p>

<p>$$E\left[\hat B^*\right] = a_0 +a_1E\hat B_{1l}+a_2E\hat B_{1m} =a_0 + (a_1+a_2)B_{1}$$</p>

<p>For
$$E\left[\hat B^*\right] = B_{1} \Rightarrow a_0 = (1-a_1-a_2)B_{1} $$</p>

<p>This condition depends on the unknown coefficient $B_1$ <em>except</em> if we set $a_0=0,\; a_1=1-a_2$, in which case it will hold always. In principle, these conditions do not exclude the possibility that $a_2 &gt;1, a_1&lt;0$, in which case we have no longer a convex combination. But interpreting negative weights is difficult (although in forecasting literature negative weights have been found to increase efficiency occasionally), so usually we take the convex combination, i.e. $0&lt;a_1&lt;1,\; 0&lt;a_2&lt;1, \; a_1+a_2=1$.</p>
",2013-10-26 13:05:55.553
58268,1809.0,1,,,,Test to compare large and small datasets,<categorical-data><contingency-tables>,CC BY-SA 3.0,"<p>I send Alice and Bob out to record people's eye colour (blue, brown, green, other). Alice does a great job and writes down the eye colour of 2000 people. Bob only records the eye colour of 20 people. Alice found all 2000 people had brown eyes. Bob found 19 people with brown eyes and one with green eyes. How important is the one green result? How can I compare the results? Is the green result significant?</p>

<p><strong>Update</strong>
I now send Alice and Bob out to a different area on another day. Alice is thorough and finds 2000 people again, while Bob is still lazy and only finds 20 people. Their results would look something like this:</p>

<pre><code>| Day | Dataset | Blue | Brown | Green | Other |
|  1  | Alice   | 0    | 2000  | 0     | 0     |
|  1  | Bob     | 0    | 19    | 1     | 0     |
|  2  | Alice   | 10   | 1900  | 45    | 45    |
|  2  | Bob     | 2    | 10    | 6     | 6     |
</code></pre>

<p>I repeat this process, sending them out to different areas on different days. Alice always finds 2000 people and Bob always finds 20.</p>

<p>Considering each day separately, how can I determine if Alice or Bob's results are more representative of the population they sampled on a given day? How can I compare their results for a given day?</p>
",2013-10-26 13:23:28.290
58280,6204.0,2,,58279.0,,,,CC BY-SA 3.0,"<p>This will be (approximately) true of any two independent variables. If you're ok with cov(x,y) being nearly but not actually 0, generating an example should be trivial:</p>

<pre><code>set.seed(123)
N=1000
x = rnorm(N)
y = rnorm(N)
cov(x,y)

0.0865909
</code></pre>

<p>As N approaches infinity, your covariance will approach zero.</p>
",2013-10-26 22:05:46.537
58269,21985.0,1,58299.0,,,Asymptotic normal distribution via the central limit theorem,<self-study><binomial-distribution><central-limit-theorem><estimators>,CC BY-SA 3.0,"<p>I have a sample $n = 100$ with two ""successes"" (Two kids having a disease among 100). So we obviously have a binomial distribution.</p>

<p>First I had to compute the maximum likelihood (ML) estimator $\hat{p}$. I got $\hat{p} = \frac{k}{n}$.</p>

<p>Now, I have to derive asymptotic normal distribution for $\hat{p}$ via the central limit theorem (CLT).</p>

<p>I know that the expected value of $\hat{p}$ is not infinite and also variance is not infinite, so I know it will be normally distributed.</p>

<p>I have to know expected value and variance of $\hat{p}$ to get the asymptotic normal distribution, right? </p>

<p>I know that expected value is $\frac{k}{n}$. But what is variance?</p>
",2013-10-26 13:27:28.340
58270,22410.0,1,58286.0,,,What is the meaning of this formula in R?,<r><self-study><data-mining><naive-bayes>,CC BY-SA 3.0,"<p>I have rows of data with columns  <code>age</code>, <code>sex</code>, <code>education</code> and <code>income</code>. </p>

<p>I am doing homework that asks me to predict income with naive Bayes in R with the formula<br>
<code>income ~ age + sex + educ</code> </p>

<p>I know formulas tell R the shape of the model to apply in the method. </p>

<p>I also know that formulas don't refer to any specific variables in a dataset -- they only give the shape of the data. </p>

<p>So with all that said, what does the formula <code>income ~ age + sex + educ</code> mean? I am guessing that it means</p>

<pre><code>p(income|age, sex, income) = P(y|x1,x2,x3) = P(x1|y) * P(x2|y) * P(x3|y) * P(y)
</code></pre>
",2013-10-26 14:37:56.087
58271,10135.0,2,,58264.0,,,,CC BY-SA 3.0,"<p>Negative ACF means that a positive oil return for one observation increases the probability of having a negative oil return for another observation (depending on the lag) and vice-versa. Or you can say (for a stationary time series) if one observation is above the average the other one (depending on the lag) is below average and vice-versa. Have a look at ""<a href=""http://www.pmean.com/09/NegativeAutocorrelation.html"">Interpreting a negative autocorrelation</a>"".</p>
",2013-10-26 14:48:01.100
58272,34640.0,1,,,Lingxiang Cheng,Use hierarchical clustering in R to cluster items into fixed size clusters,<r><machine-learning>,CC BY-SA 3.0,"<p>I am trying to use R to do Kmeans clustering and as most people I ran into the challenge of determining when to finish. I have 10,000 items and potentially 10 times of that down the road. My goal is to create a series of clusters with minimal size (e.g. 50 items per cluster) OR reasonably similar items. In other words, I don't want any of my output clusters to be too small (even if the items are quite different from each other), but I also don't mind if the clusters are too big as long as the items are similar enough.</p>

<p>I imagine I can use some kind of divisive hierarchical approach. I can start by building a small number of clusters and examine each cluster to determine if it needs to be split into more clusters. I can keep doing this till all clusters meet my stopping criteria.</p>

<p>I wonder if anyone knows good information on how other people do this? </p>
",2013-10-26 16:19:39.950
58273,20603.0,2,,58272.0,lejlot,,,CC BY-SA 3.0,"<p>There is a whole family of <strong>hierarchical clustering</strong> which should suit your needs, as it creates a tree, where each level represents the bigger (more general) clusters. Analysis of this structure and some custom cutting will bring you to described solution.</p>

<p>In R you can check out this source <a href=""http://cran.r-project.org/web/views/Cluster.html"" rel=""nofollow"">http://cran.r-project.org/web/views/Cluster.html</a> , where you will find some hierarchical clustering implementations.</p>

<p>The easiest approach would be to:</p>

<ul>
<li>run hierarchical clustering (any) and analyze the tree and select clusters generality which fits your constraints</li>
<li>cluster with any existing method, and then prune the small clusters (remove them iteratively and assign each point to the nearest of the remaining clusters).</li>
</ul>
",2013-10-26 16:24:26.117
58274,3993.0,2,,58259.0,,,,CC BY-SA 3.0,"<p>If you are constructing $d$ as the mean difference divided by the standard deviation <em>of the difference scores</em> -- rather than by the pooled standard deviation of scores in each group, as $d$ is conventionally defined! -- then that is <em>already</em> what I referred to (following Cohen, 1988) as the ""operative effect size."" So further dividing this operative effect size by $\sqrt{1-r}$ would not make sense, because that correction is already ""built in"" to that instantiation of $d$. </p>

<p>I think @John has a good brief discussion of different ways of computing $d$ at the bottom of his answer <a href=""https://stats.stackexchange.com/questions/30261/cohens-d-for-paired-vs-independent-samples-and-which-pooled-sd-to-use-for-coh"">HERE</a>. John mentions that some people firmly believe that $d$ should always be computed using the classical, independent-groups specificaton. I am one of these people. (Cohen was also one of these people. That's why he used the separate term ""operative effect size"" to talk about other ways of computing $d$.) I think it is a very bad idea to give $d$ different definitions in different contexts. Aside from the important problem this creates of killing any possible comparison of $d$ sizes between experimental paradigms that tend to use different designs, this inconsistent definition of $d$ also fosters confusion about what any given person means when they speak of $d$, unless they explicitly say which $d$ they mean! I believe this latter confusion is exactly what we have experienced here.</p>

<p>The ""operative effect size"" language convention is an attempt to allow us to talk sensibly and unambiguously about these nonstandard, but still useful, definitions of $d$ (nonstandard in the sense that they deviate from Cohen's definition). In case you are wondering, I believe Cohen calls these ""operative"" effect sizes because they are the effect sizes that are relevant for conducting a power analysis, which is what makes them useful. But let's keep in mind that this is only <em>one</em> of the uses of an effect size measure.</p>

<ul>
<li>Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd edition). Routledge.</li>
</ul>
",2013-10-26 18:22:10.630
58275,3993.0,2,,57916.0,,,,CC BY-SA 3.0,"<p>This is not a ""problem"" and does not need to be ""solved."" As you already noted yourself, this apparent multicollinearity is a natural consequence of using dummy codes. If you use non-orthogonal codes, you get non-orthogonal parameter estimates. My advice: ignore it.</p>
",2013-10-26 18:27:01.227
58276,16046.0,1,,,,Numerical sampling in hierarchical Bayesian models (HBM),<sampling><posterior><hierarchical-bayesian><numerical-integration>,CC BY-SA 3.0,"<p>I am reading chapter 5 of $\textit{Bayesian Data Analysis}$ by Gelman $\textit{et al.}$. There it explains the few steps of data analysis for hierarchical models and if I quote from the book it will be:</p>

<p>""We first perform the following three steps analytically.</p>

<ol>
<li><p>Write the joint posterior density, $p( \theta, \phi|y)$, in unnormalized
form as a product of the hyperprior distribution $p(\phi)$, the
population distribution $p(\theta|\phi)$, and the likelihood $p(y|\theta)$. </p></li>
<li><p>Determine analytically the conditional posterior density of $\theta$ given
the hyperparameters $\phi$; for fixed observed $y$, this is a function
of $\phi$, $p(\theta|\phi, y)$.</p></li>
<li><p>Estimate $\phi$ using the Bayesian paradigm; that is, obtain its
marginal posterior distribution, $p(\phi|y)$.</p></li>
</ol>

<p>The first step is immediate, and the second step is easy for conjugate models
because, conditional on $t/J$, the population distribution for $\theta$ is just the iid
model (5.1), so that the conditional posterior density is a product of conjugate
posterior densities for the components $\theta_j$.
The third step can be performed by brute force by integrating the joint
posterior distribution over $\theta$:
$$p(\phi|y) = \int{p(\phi,\theta|y)d\theta} \ \ \ \ \ \ \ \ \ \ \ \text{(5.4)}""$$</p>

<p>Now my question is whether we can (in a numerical approach) sample from $p(\phi,\theta|y)$ and ignore the $\theta$s to generate a sample of $p(\phi|y)$?</p>
",2013-10-26 18:52:30.817
58277,0.0,5,,,,,,CC BY-SA 3.0,,2013-10-26 19:31:28.250
58284,7007.0,2,,58279.0,,,,CC BY-SA 3.0,"<p>As a very simple example (maybe too simple?), consider $X,Y\in\{0,1\}$ with joint distribution defined by the table</p>

<pre><code>  Y \ X   0    1    
  0     1/4  1/4 1/2
  1     1/4  1/4 1/2
        1/2  1/2   1
</code></pre>

<p>This table also displays the marginal distributions of $X$ and $Y$. First, check that $X$ and $Y$ are independent. For example,
$$
  \mathrm{Pr}(X=0,Y=0) = 1/4 = 1/2 \times 1/2 = \mathrm{Pr}(X=0)\,\mathrm{Pr}(Y=0) \, ,
$$
and so on. Now, compute the distribution of $Z=X+Y\in\{0,1,2\}$. For example,
$$
\mathrm{Pr}(Z=1) = \mathrm{Pr}(X=1,Y=0) + \mathrm{Pr}(X=0,Y=1) = 1/2 \, .
$$
Using these distributions, compute $\mathrm{Var}(X),\mathrm{Var}(Y)$, and $\mathrm{Var}(Z)$.</p>
",2013-10-26 22:18:23.570
58285,8671.0,1,,,,approximation to maximum and minimum function : soft-min and soft-max,<machine-learning><mathematical-statistics>,CC BY-SA 3.0,"<p>The approximation to the function $max(x)$ can be written as a NOISY-OR as given below: $$ max_k(x) = 1-\prod_k(1-x) $$</p>

<p>Are there any way to approximate $min(x)$ ?</p>
",2013-10-26 23:02:44.790
58286,594.0,2,,58270.0,,,,CC BY-SA 3.0,"<p>Unless you're using terms differently that what I understand you to mean, you're mistaken when you assert that ""formulas don't refer to any specific variables in a dataset"". They certainly do refer to specific variables, explicitly by name.</p>

<p>See <a href=""https://stackoverflow.com/a/16313110/330679"">this stackoverflow answer</a> for some background information and where R formulas originate.</p>

<p>Formulas are used for many purposes in R, and a specific component of a formula (such as a variable name or an operator) may have a somewhat different meaning in a different context. </p>

<p>The meaning of the formula in <code>plot(y ~ x1 + x2, data=mydata)</code> and in <code>lm(y ~ x1 + x2, data=mydata)</code> and in <code>glm(y ~ x1 + x2, family=binomial, data=mydata)</code> are all somewhat different ... and as you go further afield, meanings can change even more, even between packages doing very similar things.</p>

<p>So what that formula might mean <em>in R</em> is very context dependent -- and we don't have sufficient context.</p>

<p>(You don't even mention whether you're using a package in R or building something yourself.)</p>

<p>Given this is a naive Bayes classifier, your interpretation certainly makes sense (think in terms of logs, for example), and likely that's what I'd have anticipated it to mean, but that's not really anything to do with R unless you're using some particular package... whose specific interpretation of formulas we might then be able to explain.</p>
",2013-10-26 23:04:56.650
58287,6728.0,1,,,,Optimal orthogonal polynomial chaos basis functions for log-normally distributed random variables,<stochastic-processes><polynomial>,CC BY-SA 4.0,"<p>I hope this is the appropriate venue for this type of question.  If not, please feel free to migrate! :)</p>

<p>I'm trying to solve a stochastic partial differential equation of the form <span class=""math-container"">$$\alpha(\omega)\nabla^2u=f$$</span>
where <span class=""math-container"">$\alpha(\omega)$</span> represents a random field that is log-normally distributed, i.e. it has a probability density function <span class=""math-container"">$$f(x)=\frac{1}{x\sqrt{2\pi\sigma^2}}e^{-\frac{(\log(x)-\mu)^2}{2\sigma^2}}.$$</span></p>

<p>I want to represent the solution of this problem as a polynomial chaos expansion <span class=""math-container"">$$u=\sum_{i=0}^p u_i(x)\Psi_i(\xi)$$</span> where <span class=""math-container"">$u_i(x)$</span> is a deterministic coefficient and <span class=""math-container"">$\Psi_i(\xi)$</span> are orthogonal polynomials in terms of a random variable <span class=""math-container"">$\xi$</span> with the same log-normal probability density function.</p>

<p>According to <a href=""http://www.dam.brown.edu/scicomp/media/report_files/BrownSC-2003-07.pdf"" rel=""nofollow noreferrer"">Xiu &amp; Karniadakis (2002)</a>, certain orthogonal polynomial bases give optimal (exponential) convergence of finite expansions to the true solution <span class=""math-container"">$u$</span>.  For instance, Hermitte polynomials are optimal for Gaussian distributions, Legendre polynomials for uniform distributions, Laguerre for gamma distributions etc (see the above paper, bottom of page 8).</p>

<p>What is the corresponding optimal polynomial basis for log-normal distributions?</p>
",2013-10-26 23:41:14.957
58288,,1,58750.0,,user31766,What's the difference between observable fixed effect and control variable?,<least-squares>,CC BY-SA 4.0,"<p>I am confused about the exact definitions here.</p>

<p>Assuming I have a cross-sectional regression, let's say, Wage on Education and I additionally control for observable characteristics with a set of dummies or variables like intelligence level, age, parent's education level, urbanization area, gender, race, work experience etc. </p>

<ol>
<li><p>Does this mean I used dummies/variables to ""<strong>control</strong> for observable fixed effects"" that I obtained through my data collection? (Is, for example, parent's education level thus an <strong>observable</strong> fixed effect?)</p></li>
<li><p>Do <strong>unobservable</strong> fixed effects like ability (as often quoted in the literature) are then said to be ""controlled"" by proxies through my dummies like intelligence level, experience etc.?</p></li>
</ol>

<p>In a way I'd like to know the exact difference between controlling for a variable, observable and unobservable characteristics, observable and unobservable fixed effects. Thanks.</p>
",2013-10-27 00:01:04.910
58289,22843.0,1,,,,Explain Statistics: Matching formulas for chi square tests,<mathematical-statistics><chi-squared-test>,CC BY-SA 3.0,"<p>The chi square formula given in my book is:</p>

<p>$\chi^2 = \frac{(n-1)s^2}{\sigma^2} $ </p>

<p>At first I admitted to feeling some kind of strangeness to this formula, but after a few hours I realized that $\chi^2$ was $\displaystyle \sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 - \left( \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}} \right)^2$ in disguise. However, there's a formula in my book that says that </p>

<p>$\chi^2 = \displaystyle \frac{\sum_{i=1}^n (O_i - E_i)^2}{E_i}$ where $O_i$ is the observed value and $E_i$ is the expected value. I want to show that this expression is equivalent to the one above: $$\displaystyle \sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 - \left( \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}} \right)^2$$</p>

<p>This equation comes from the fact that $(n-1)s^2 = \displaystyle \sum_{i=1}^n ((X_i - \mu)+(\mu - \bar{x}))^2$</p>
",2013-10-27 01:28:50.803
58297,23064.0,1,58298.0,,,Combining discrete and continuous variables,<density-function><cumulative-distribution-function>,CC BY-SA 3.0,"<p>I need to find the pdf of a random variable which is a mixture of discrete and continuous random variables. I have seen on this website but it does not exist in the general case, but maybe in this one it does.</p>

<p>In any case, I have $X \sim Bern(p)$ where $p$ is known, and I have $Y = XW+(1-X)Z$ where $W,Z$ are both continuous with pdf also known. For the moment, I've tried to 
\begin{align*}
\text{cdf}_Y (y) &amp; = P( Y \leq y) = P( XW+(1-X)Z \leq y) \\
&amp; = P( ... \leq y \mid X=0 ) + P( ... \leq y \mid X =1) \\
&amp; = \text{cdf}_W(y) + \text{cdf}_Z(y)
\end{align*}
I am just not sure I am allowed to go from the first line to the second...is this correct ? Does anyone have any suggestion on this problem ? </p>

<p>Thank you very much !</p>
",2013-10-27 13:06:52.080
58298,1889.0,2,,58297.0,,,,CC BY-SA 3.0,"<p>$P( Y \leq y) $</p>

<p>$= P( Y \leq y|X=1)P(X=1) + P( Y \leq y|X=0)P(X=0)$</p>

<p>$=P( Y \leq y|X=1)p + P( Y \leq y|X=0)(1-p)$</p>

<p>$=pP( W\leq y) + (1-p)P( Z \leq y)$</p>

<p>So $F_Y(y)=pF_W(y)+(1-p)F_Z(y)$ and thus $f_Y(y)=pf_W(y)+(1-p)f_Z(y)$</p>
",2013-10-27 13:19:42.593
58290,10957.0,1,58316.0,,,ROC curves and AUC in simulations to compare models,<roc><auc>,CC BY-SA 3.0,"<p>I am using ROC curves to compare different methods but not sure if I need to re-simulate datasets using different seeds in R in order to reduce the ""by-chance"" issue for a particular output. Here is a brief outline of my simulation:</p>

<ol>
<li><p>The function <code>generate.data</code> is used to simulate data of some distribution, and by simulation, I know which data are true positives. <strong>The random number generator is controlled by fixing the <code>seed</code> in R</strong>.</p></li>
<li><p>The function <code>check.models</code> is used to test a total of 5 methods, and return the quantities used to draw a ROC curve for each method. Also for each curve (method), the AUC is reported.</p></li>
<li><p>The function <code>plot.roc</code> is used for plotting.</p></li>
</ol>

<p>In step #1, there are some other factors to change so that the data are under different ""alternatives"". When I run steps #1 and #2 above using <code>seed=123</code> and pick up the method with the highest AUC, I got one set of results. However, when I re-run using a different seed (say <code>seed=456</code>), I got another set of results <strong>not identical to the first run</strong>. Therefore, I think rigorously I should run my simulation across different <code>seed</code>'s in R to generate data in step #1, so that the ""by-chance"" issue of using a particular dataset is reduced. </p>

<p>Am I correct? If so, then I should report the average of the AUC's for each method across (say, 1000) simulations, and pick up the highest among the methods compared? Thanks!</p>
",2013-10-27 02:05:28.200
58291,7275.0,1,,,,Joint PMF for two Geometric distribution variables,<mathematical-statistics><density-function><joint-distribution>,CC BY-SA 4.0,"<p>I am interested to know how to calculate the joint probability mass function for two independent geometric random variables.</p>
<p>Suppose two variables X1 and X2 are independent, such that <code>Xiâˆ¼Geometric(theta)</code>, how to find the joint pmf distribution of X1 and X2. I am not sure but I think it should be the product of pmf of both mass function.</p>
<p>Also, how should I calculate the probability of the event where kth trial being the the first success/failure for both the variables or k1th trial for X1 and k2th trial for X2?</p>
",2013-10-27 02:37:25.640
58292,23066.0,1,59638.0,,,"How do I use the â€œsurvivalâ€ package and ""Surv"" function in R with left-truncated data?",<r><survival><truncation><kaplan-meier>,CC BY-SA 3.0,"<p>I am trying to run survival analysis using the <code>Surv</code> and <code>survfit</code> functions from the <code>survival</code> package in R. Most of my data is left truncated, and I'm not sure if I'm entering it into the <code>Surv</code> function correctly. My response variable is time (measured in years) beginning from when a bridge is classified as deficient, and ending when it collapses. I can track each bridge's deficiency status from 2012 back to 1992, but no further. The censoring occurs because many bridges were classified as deficient from the time of their collapse back to 1992, and thus I don't know exactly when they became deficient, and therefore I don't know their true ""lifetime"" (number of years from deficient classification to collapse). Say for example a bridge collapsed in 1995, and was classified as being deficient in 1995, 1994, 1993, and 1992. It is possible that it was first classified as being deficient in 1992, it is also possible that it has been classified as deficient since 1984. Thus I believe my censoring is considered to be left truncated.</p>

<p>Some example data:</p>

<pre><code>Year0 = c(1992, 1992, 1999, 1992, 1993, 2007, 2005, 1992) # The years when each bridge     was first observed as being deficient.
Year1 = c(1993, 1994, 2002, 1996, 2004, 2012, 2011, 2000) # The years in which each bridge collapsed
Defyears = Year1 - Year0 + 1 # The number of years for wich I can observe each bridge being deficient
time1 = Year0 - 1992 # Since I want the time scale to be from 0 to 21 instead of 1992 - 2012, I subtract 1992 from each time observation.
                     # This now becomes the beginning point for the lifetime of each bridge.
time2 = Defyears + time1 # This is the ending point of the lifetime of each bridge.
n = length(time2)
</code></pre>

<p>Notice that four out of the eight bridges are left truncated, bridge 1, 2, 4, and 8. I cannot observe exactly when they were first classified as being deficient. For bridges 3, 5, 6, and 7 I know their exact lifetimes since they became deficient after 1992, hence these observations are not censored.</p>

<p>I then fit the model:</p>

<pre><code>bridges = survfit(Surv(time = time1, time2 = time2, event = rep(1,n)) ~ 1) # I do ""event = rep(1,n)"" because each bridge collapsed.
</code></pre>

<p>I'm just not sure that this model is correct. For one thing, in the documentation it says that <code>time</code> is for right censored data or the starting time for interval censored data. For another, I don't see how this model accounts for the observations that aren't censored. Can anyone tell me if this is right, and if not, what I need to change and why. Any help is greatly appreciated. Thanks so much!</p>
",2013-10-27 04:18:19.037
58293,22419.0,1,58294.0,,,Intuition for consequences of multicollinearity,<self-study><multiple-regression><multicollinearity><intuition>,CC BY-SA 3.0,"<p>So we have a regression equation with one explained variable and 10 explanatory variables.</p>

<p>What I have read so far:</p>

<ol>
<li><p>Multicollinearity doesnt affect the regression of the model as a whole.</p></li>
<li><p>But if we start looking at the effect of individual predictor variable Xs on the explained variable, then we are going to have inaccurate estimates. </p></li>
</ol>

<p>I have tried to start thinking intuitively about it like follows:</p>

<ul>
<li>High Multicollinearity means that in a matrix, two or more rows/columns are linearly dependent on each other. In other words, in a 3-dimensional space there are 2 vectors which have (almost) the same direction only different magnitudes (is this right?)</li>
</ul>

<p>I'd appreciate it if someone could explain how this translates into ""multicollinearity not affecting the regression as a whole but only individual variable's coefficient estimates"".</p>

<p>Also, could someone explain the statement in bold? I cant make sense out of it:</p>

<blockquote>
  <p>One other thing to keep in mind is that the tests on the individual
  coefficients each assume that all of the other predictors are in the
  model. <strong>In other words each predictor is not significant as long as all
  of the other predictors are in the model. There must be some
  interaction or interdependence between two or more of your predictors.</strong></p>
</blockquote>

<p>which was an answer to this question: <a href=""https://stats.stackexchange.com/questions/14500/how-can-a-regression-be-significant-yet-all-predictors-be-non-significant/14528#14528"">How can a regression be significant but all predictors insignificant?</a></p>
",2013-10-27 06:02:56.510
58294,21762.0,2,,58293.0,,,,CC BY-SA 3.0,"<p>Let us first distinguish between <em>perfect</em> multi-collinearity (model matrix not of full rank, so that usual matrix inversions fail. Usually due to misspecification of the predictors) and <em>non-perfect</em> multi-collinearity (some of the predictors are correlated without leading to computational problems). This answer is about the second type, which occurs in almost any multivariable linear model since the predictors have no reason to be uncorrelated. </p>

<p>A simple example with strong multi-collinearity is a quadratic regression. So the only predictors are $X_1 = X$ and $X_2=X^2$:</p>

<pre><code>set.seed(60)

X1 &lt;- abs(rnorm(60))
X2 &lt;- X1^2
cor(X1,X2)   # Result: 0.967
</code></pre>

<p>This example illustrates your questions/claims:</p>

<p><em>1. Multicollinearity doesnt affect the regression of the model as a whole.</em></p>

<p>Let's have a look at an example model:</p>

<pre><code>Y &lt;- 0.5*X1 + X2 + rnorm(60)
fit &lt;- lm(Y~X1+X2)
summary(fit)

#Result
[...]

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -0.3439     0.3214  -1.070    0.289
X1            1.3235     0.8323   1.590    0.117
X2            0.5861     0.3931   1.491    0.141

Residual standard error: 1.014 on 57 degrees of freedom
Multiple R-squared:  0.7147,    Adjusted R-squared:  0.7047 
F-statistic: 71.39 on 2 and 57 DF,  p-value: 2.996e-16
</code></pre>

<p><em>Global</em> statements about the model are just fine: </p>

<ul>
<li>R-Squared: $X$ explains about 71% of the variability of $Y$</li>
<li>Global F-test: At the 5% level, there is really an association between $X$ and $Y$</li>
<li>Predictions: For persons with $X$-value 2, a best guess for his $Y$-value is 
$$
-0.3439 + 1.3235\cdot 2 + 0.5861 \cdot 2^2 = 4.6475
$$</li>
</ul>

<p><em>2. But if we start looking at the effect of individual variable Xs on the explained variable, then we are going to have inaccurate estimates.</em></p>

<p>The estimates are accurate, this is not the problem. The problem with the standard interpretation of isolated effects is that we hold all other predictors fixed, which is strange if there are strong correlations to those other predictors. In our example it is even wrong to say ""the average $Y$ value increases by 1.3235 if we increase $X_1$ by 1 and hold $X_2$ fixed, because $X_2 = X_1^2$. Since we cannot interpret isolated effects descriptively, also all inductive statements about them are not useful: Look at the t-tests in the output. Both are above the 5% level, although the global test of association gives us a p-value below 5%. The null hypothesis of such a t-test is ""the effect of the predictor is zero"" or, in other words, ""the inclusion of this predictor does not increase the true R-squared in the population"". Because $X_1$ and $X_2$ are almost perfectly correlated, the model has almost the same R-squared if we drop one of the two variables:</p>

<pre><code>summary(lm(Y~X1))

# Gives

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -0.7033     0.2148  -3.274  0.00179 ** 
X1            2.5232     0.2151  11.733  &lt; 2e-16 ***

Residual standard error: 1.025 on 58 degrees of freedom
Multiple R-squared:  0.7036,    Adjusted R-squared:  0.6985 
F-statistic: 137.7 on 1 and 58 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>This already illustrates the first part of the statement:</p>

<p><em>One other thing to keep in mind is that the tests on the individual coefficients each assume that all of the other predictors are in the model. In other words each predictor is not significant as long as all of the other predictors are in the model. There must be some interaction or interdependence between two or more of your predictors.</em></p>

<p>The last statement here is plainly wrong.</p>
",2013-10-27 07:54:54.227
58295,503.0,2,,58293.0,,,,CC BY-SA 3.0,"<p>Another problem, in addition to those @Michael gave, is that when there is strong near-colinearity, small changes in the input data can lead to large changes in the output. </p>

<p>I made up some data (taking wild guesses at the average lengths of legs and torso (in inches) and weight (in pounds) for adult humans).</p>

<pre><code>set.seed(1230101)
lengthleg &lt;- rnorm(100, 30, 5)
lengthtorso &lt;- lengthleg + rnorm(100, 0, 1)
weight &lt;- 1.2*lengthleg + 1.8*lengthtorso + rnorm(100, 0, 10)

m1 &lt;- lm(weight~lengthleg + lengthtorso)
coef(m1)
</code></pre>

<p>the first time through, I got coefficients of -5.93, 0.43 and 2.73. Rerunning everything except <code>set.seed</code> gave me -9.91, 1.12 and 2.18.</p>
",2013-10-27 11:57:25.867
58296,2081.0,1,58301.0,,,Suppression effect in regression: definition and visual explanation/depiction,<multiple-regression><data-visualization><geometry><suppressor>,CC BY-SA 3.0,"<p>What is a suppressor variable in multiple regression and what might be the ways to display suppression effect visually (its mechanics or its evidence in results)? I'd like to invite everybody who has a thought, to share.</p>
",2013-10-27 12:08:33.367
58343,1406.0,2,,37819.0,,,,CC BY-SA 3.0,"<p>This is an expected result. The matrix $\Pi$ has full rank, when the process is stationary. Of course Johansen procedure usually requires that the time series should be checked for unit roots first. The null hypothesis is that time series are unit-roots and they are cointegrated. If your variables are $I(0)$ then the first step should eliminate the need to use Johansen's test. I would hesitate to use Johansen's test for testing whether the processes are $I(0)$, since it was not designed to be used as such.</p>
",2013-10-28 08:29:21.737
58299,20473.0,2,,58269.0,,,,CC BY-SA 3.0,"<p>Each child can be modeled as a Bernoulli r.v. $X_i$ with probability of having the disease equal to $p_i$, $X_i \sim B(p_i)$, $i=1,\dots ,n$. If you assume that a) $p_1 =p_2=\dots=p_n=p$ and b) that these are independent rv's then their joint density is </p>

<p>$$f(X_1,\dots,X_n) = \prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}$$
and their log-likelihood function, viewed as a function of $p$ is</p>

<p>$$\ln L =\sum_{i=1}^{n}\left\{x_i\ln p+(1-x_i)\ln (1-p)\right\}$$</p>

<p>which leads to the MLE for $p$
$$\hat p =\frac 1n\sum_{i=1}^{n}x_i$$
which is unbiased since $$E\hat p =\frac 1n\sum_{i=1}^{n}Ex_i = \frac 1n np =p$$</p>

<p>Consider now the variable
$$U_i = X_i - E(X_i) = X_i -p \Rightarrow X_i = U_i + p$$
We have
$$EU_i = 0,\qquad Var(u_i) = Var(X_i) = p(1-p) $$
so it is covariance-stationary.</p>

<p>Subsitute for the $x$'s in the estimator</p>

<p>$$\hat p =\frac 1n\sum_{i=1}^{n}(u_i+p) = \frac 1n\sum_{i=1}^{n}u_i +p$$
and consider the quantity
$$\sqrt n (\hat p-p) =\sqrt n\frac 1n\sum_{i=1}^{n}u_i= \frac {1}{\sqrt n}\sum_{i=1}^{n}u_i$$</p>

<p>Since the $U$'s are covariance stationary, (and evidently i..i.d) then the CLT certainly applies and so </p>

<p>$$\sqrt n (\hat p-p) \rightarrow_d N\left (0, p(1-p)\right) $$ </p>

<p>For approximate statistical inference, we manipulate this expression through 
$$ \sqrt n (\hat p-p) = Z \Rightarrow \hat p = \frac {1} {\sqrt n}Z +p$$</p>

<p>and write that, for ""large samples""</p>

<p>$$\hat p \sim_{approx} N\left (p, \frac {p(1-p)}{n}\right)$$</p>

<p>(but not when $n$ truly goes to infinity, since then $\hat p$ does not have a distribution, but collapses to a constant, the true value $p$ since $\hat p$ is a consistent estimator).</p>
",2013-10-27 14:34:09.020
58300,23073.0,1,,,,Confidence Intervals Intuition,<confidence-interval>,CC BY-SA 3.0,"<p>I am new to statistics and have run into some trouble understanding computing confidence intervals and am seeking some help. I will outline the motivating example in my textbook and hopefully someone can offer some guidance. </p>

<p>Example </p>

<p>There is a population of mean values and your goal is to figure out the true mean (as best you can). In order to accomplish this, a number of samples are taken, each of which has a mean value. </p>

<p>Next, because we know by the central limit theorem that as the number of samples increase, the sampling distribution will be normally distributed, we use the equation $z = \frac{X - \bar{X}}{s}$  (noting that in this case s = standard error) to compute a lower and upper bound taking each sample mean as the mean for the z-score equation and z-scores of -1.96 and +1.96, for example,  to compute a 95% confidence interval. </p>

<p>Iâ€™ve included a graph from my textbook in attempt to add clarity.</p>

<p><img src=""https://i.stack.imgur.com/VrE41.png"" alt=""enter image description here""></p>

<p>So I do not understand how it is you can use each sample mean as the mean value in our z equation to compute intervals. We know that the sample distribution is normally distributed so isnâ€™t it the case that only the mean of all the samples can be used? How can we compute an interval around each mean value that contributes to the sampling distribution?</p>

<p>Any help with this would be much appreciated  </p>

<p>Note: I'm reading ""Discovering Statistics Using IBM SPSS Statistics 3rd Edition"" by Andy Field and this example is from pg 43-45  </p>
",2013-10-27 14:38:22.093
58301,2081.0,2,,58296.0,,,,CC BY-SA 4.0,"<p>There exist a number of frequenly mentioned regressional effects which conceptually are different but share much in common when seen purely statistically (see e.g. <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2819361/pdf/nihms-173346.pdf"" rel=""noreferrer"">this paper</a> &quot;Equivalence of the Mediation, Confounding and Suppression
Effect&quot; by David MacKinnon et al., or Wikipedia articles):</p>
<ul>
<li>Mediator: IV which conveys effect (totally of partly) of another IV
to the DV.</li>
<li>Confounder: IV which constitutes or precludes, totally or
partly, effect of another IV to the DV.</li>
<li>Moderator: IV which, varying,
manages the strength of the effect of another IV on the DV.
Statistically, it is known as interaction between the two IVs.</li>
<li>Suppressor: IV (a mediator or a moderator conceptually) which inclusion
strengthens the effect of another IV on the DV.</li>
</ul>
<p>I'm not going to discuss to what extent some or all of them are technically similar (for that, read the paper linked above). My aim is to try to show graphically what <strong>suppressor</strong> is. The above definition that &quot;suppressor is a variable which inclusion strengthens the effect of another IV on the DV&quot; seems to me <em>potentially</em> broad because it does not tell anything about mechanisms of such enhancement. Below I'm discussing one mechanism - the only one I consider to be suppression. If there are <em>other</em> mechanisms as well (as for right now, I haven't tried to meditate of any such other) then either the above &quot;broad&quot; definition should be considered imprecise or my definition of suppression should be considered too narrow.</p>
<h3>Definition (in my understanding)</h3>
<p>Suppressor is the independent variable which, when added to the model, raises observed R-square <strong>mostly due to its accounting for the residuals</strong> left by the model without it, and not due to its own association with the DV (which is comparatively weak). We know that the increase in R-square in response to adding a IV is the squared part correlation of that IV in that new model. This way, if the part correlation of the IV with the DV <strong>is greater</strong> (by absolute value) than the zero-order <span class=""math-container"">$r$</span> between them, that IV is a suppressor.</p>
<p>So, a suppressor mostly &quot;suppresses&quot; the error of the reduced model, being weak as a predictor itself. The error term is the complement to the prediction. The prediction is &quot;projected on&quot; or &quot;shared between&quot; the IVs (regression coefficients), and so is the error term (&quot;complements&quot; to the coefficients). The suppressor suppresses such error components unevenly: greater for some IVs, lesser for other IVs. For those IVs &quot;whose&quot; such components it suppresses greatly it lends considerable facilitating aid by actually <strong>raising their regression coefficients</strong>.</p>
<p>Not strong suppressing effects occurs often and wildly (an <a href=""https://stats.stackexchange.com/q/35091/3277"">example</a> on this site). Strong suppression is typically introduced consciously. A researcher seeks for a characteristic which must correlate with the DV as weak as possible and at the same time would correlate with something in the IV of interest which is considered irrelevant, prediction-void, in respect to the DV. He enters it to the model and gets considerable increase in that IV's predictive power. The suppressor's coefficient is typically not interpreted.</p>
<p>I could summarize my <strong>definition</strong> as follows [up on @Jake's answer and @gung's comments]:</p>
<ul>
<li>Formal (statistical) definition: suppressor is IV with part
correlation larger than zero-order correlation (with the dependent).</li>
<li>Conceptual (practical) definition: the above formal definition + the zero-order
correlation is small, so that the suppressor is not a sound predictor
itself.</li>
</ul>
<p>&quot;Suppessor&quot; is a role of a IV in a specific <em>model</em> only, not the characteristic of the separate variable. When other IVs are added or removed, the suppressor can suddenly stop suppressing or resume suppressing or change the focus of its suppressing activity.</p>
<h3>Normal regression situation</h3>
<p>The first picture below shows a typical regression with two predictors (we'll speak of linear regression). The picture is copied from <a href=""https://stats.stackexchange.com/a/65817/3277"">here</a> where it is explained in more details. In short, moderately correlated (= having acute angle between them) predictors <span class=""math-container"">$X_1$</span> and <span class=""math-container"">$X_2$</span> span 2-dimesional space &quot;plane X&quot;. The dependent variable <span class=""math-container"">$Y$</span> is projected onto it orthogonally, leaving the predicted variable <span class=""math-container"">$Y'$</span> and the residuals with st. deviation equal to the length of <span class=""math-container"">$e$</span>. R-square of the regression is the angle between <span class=""math-container"">$Y$</span> and <span class=""math-container"">$Y'$</span>, and the two regression coefficients are directly related to the skew coordinates <span class=""math-container"">$b_1$</span> and <span class=""math-container"">$b_2$</span>, respectively. This situation I've called normal or typical because both <span class=""math-container"">$X_1$</span> and <span class=""math-container"">$X_2$</span> correlate with <span class=""math-container"">$Y$</span> (oblique angle exists between each of the independents and the dependent) and the predictors compete for the prediction because they are correlated.</p>
<p><img src=""https://i.stack.imgur.com/jI8z6.jpg"" alt=""enter image description here"" /></p>
<h3>Suppression situation</h3>
<p>It is shown on the next picture. This one is like the previous; however <span class=""math-container"">$Y$</span> vector now directs somewhat away from the viewer and <span class=""math-container"">$X_2$</span> changed its direction considerably. <span class=""math-container"">$X_2$</span> acts as a suppressor. Note first of all that it hardly correlates with <span class=""math-container"">$Y$</span>. Hence it cannot be a valuable <em>predictor</em> itself. Second. Imagine <span class=""math-container"">$X_2$</span> is absent and you predict only by <span class=""math-container"">$X_1$</span>; the prediction of this one-variable regression is depicted as <span class=""math-container"">$Y^*$</span> red vector, the error as <span class=""math-container"">$e^*$</span> vector, and the coefficient is given by <span class=""math-container"">$b^*$</span> coordinate (which is the endpoint of <span class=""math-container"">$Y^*$</span>).</p>
<p><img src=""https://i.stack.imgur.com/3eMTT.jpg"" alt=""enter image description here"" /></p>
<p>Now bring yourself back to the full model and notice that <span class=""math-container"">$X_2$</span> is fairly correlated with <span class=""math-container"">$e^*$</span>. Thus, <span class=""math-container"">$X_2$</span> when introduced in the model, can explain a considerable portion of that error of the reduced model, cutting down <span class=""math-container"">$e^*$</span> to <span class=""math-container"">$e$</span>. This constellation: (1) <span class=""math-container"">$X_2$</span> is not a rival to <span class=""math-container"">$X_1$</span> as a <em>predictor</em>; and (2) <span class=""math-container"">$X_2$</span> is a dustman to pick up <em>unpredictedness</em> left by <span class=""math-container"">$X_1$</span>, - makes <span class=""math-container"">$X_2$</span> a <strong>suppressor</strong>. As a result of its effect, predictive strength of <span class=""math-container"">$X_1$</span> has grown to some extent: <span class=""math-container"">$b_1$</span> is larger than <span class=""math-container"">$b^*$</span>.</p>
<p>Well, why is <span class=""math-container"">$X_2$</span> called a suppressor to <span class=""math-container"">$X_1$</span> and how can it reinforce it when &quot;suppressing&quot; it? Look at the next picture.</p>
<p><img src=""https://i.stack.imgur.com/XisTU.jpg"" alt=""enter image description here"" /></p>
<p>It is exactly the same as the previous. Think again of the model with the single predictor <span class=""math-container"">$X_1$</span>. This predictor could of course be decomposed in two parts or components (shown in grey): the part which is &quot;responsible&quot; for prediction of <span class=""math-container"">$Y$</span> (and thus coinciding with that vector) and the part which is &quot;responsible&quot; for the unpredictedness (and thus parallel to <span class=""math-container"">$e^*$</span>). It is <strong>this</strong> second part of <span class=""math-container"">$X_1$</span> - the part irrelevant to <span class=""math-container"">$Y$</span> - is suppressed by <span class=""math-container"">$X_2$</span> when that suppressor is added to the model. The irrelevant part is suppressed and thus, given that the suppressor doesn't itself predict <span class=""math-container"">$Y$</span> any much, the relevant part looks stronger. A suppressor is not a predictor but rather a facilitator for another/other predictor/s. Because it competes with what impedes them to predict.</p>
<h3>Sign of the suppressor's regression coefficient</h3>
<p>It is the sign of the correlation between the suppressor and the error variable <span class=""math-container"">$e^*$</span> left by the reduced (without-the-suppressor) model. In the depiction above, it is positive. In other settings (for example, revert the direction of <span class=""math-container"">$X_2$</span>) it could be negative.</p>
<h3>Suppression example</h3>
<p>Example data:</p>
<pre><code>         y         x1         x2

1.64454000  .35118800 1.06384500
1.78520400  .20000000 -1.2031500
-1.3635700 -.96106900 -.46651400
 .31454900  .80000000 1.17505400
 .31795500  .85859700 -.10061200
 .97009700 1.00000000 1.43890400
 .66438800  .29267000 1.20404800
-.87025200 -1.8901800 -.99385700
1.96219200 -.27535200 -.58754000
1.03638100 -.24644800 -.11083400
 .00741500 1.44742200 -.06923400
1.63435300  .46709500  .96537000
 .21981300  .34809500  .55326800
-.28577400  .16670800  .35862100
1.49875800 -1.1375700 -2.8797100
1.67153800  .39603400 -.81070800
1.46203600 1.40152200 -.05767700
-.56326600 -.74452200  .90471600
 .29787400 -.92970900  .56189800
-1.5489800 -.83829500 -1.2610800
</code></pre>
<p>Linear regression results:</p>
<p><a href=""https://i.stack.imgur.com/MBLhi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MBLhi.png"" alt=""enter image description here"" /></a></p>
<p>Observe that <span class=""math-container"">$X_2$</span> served as suppressor. Its zero-order correlation with <span class=""math-container"">$Y$</span> is practically zero but its part correlation is much larger by magnitude, <span class=""math-container"">$-.224$</span>. It strengthened to some extent the predictive force of <span class=""math-container"">$X_1$</span> (from r <span class=""math-container"">$.419$</span>, a would-be beta in simple regression with it, to beta <span class=""math-container"">$.538$</span> in the multiple regression).</p>
<p>According to the <em>formal</em> definition, <span class=""math-container"">$X_1$</span> appeared a suppressor too, because its part correlation is greater than its zero-order correlation. But that is because we have only two IV in the simple example. Conceptually, <span class=""math-container"">$X_1$</span> isn't a suppressor because its <span class=""math-container"">$r$</span> with <span class=""math-container"">$Y$</span> is not about <span class=""math-container"">$0$</span>.</p>
<p>By way, sum of squared part correlations exceeded R-square: <code>.4750^2+(-.2241)^2 = .2758 &gt; .2256</code>, which would not occur in normal regressional situation (see the <strong>Venn diagram</strong> below).</p>
<h3>Suppression and coefficient's sign change</h3>
<p>Adding a variable that will serve a supressor may as well as may not change the sign of some other variables' coefficients. &quot;Suppression&quot; and &quot;change sign&quot; effects are not the same thing. Moreover, I believe that a suppressor can never change sign of <em>those</em> predictors whom they serve suppressor. (It would be a shocking discovery to add the suppressor on purpose to facilitate a variable and then to find it having become indeed stronger but in the opposite direction! I'd be thankful if somebody could show me it is possible.)</p>
<h3>Suppression and coefficient strengthening</h3>
<p>To cite an earlier passage: &quot;For those IVs &quot;whose&quot; such components [error components] it suppresses greatly the suppressor lends considerable facilitating aid by actually <strong>raising their regression coefficients</strong>&quot;. Indeed, in our Example above, <span class=""math-container"">$X_2$</span>, the suppressor, raised the coefficient for <span class=""math-container"">$X_1$</span>. Such enhancement of the unique predictive power of another regressor is often the <em>aim</em> of a suppressor to a model but it is not the <em>definition</em> of suppressor or of suppression effect. For, the aforementioned enhancement of another predictor's capacity via adding more regressors can easily occure in a normal regressional situation without those regressors being suppressors. Here is an example.</p>
<pre><code>   y       x1       x2       x3

   1        1        1        1
   3        2        2        6
   2        3        3        5
   3        2        4        2
   4        3        5        9
   3        4        4        2
   2        5        3        3
   3        6        4        4
   4        7        5        5
   5        6        6        6
   4        5        7        5
   3        4        5        5
   4        5        3        5
   5        6        4        6
   6        7        5        4
   5        8        6        6
   4        2        7        7
   5        3        8        8
   6        4        9        4
   5        5        3        3
   4        6        4        2
   3        2        1        1
   4        3        5        4
   5        4        6        5
   6        9        5        4
   5        8        3        3
   3        5        5        2
   2        6        6        1
   3        7        7        5
   5        8        8        8
</code></pre>
<p>Regressions results without and with <span class=""math-container"">$X_3$</span>:</p>
<p><a href=""https://i.stack.imgur.com/SnT9g.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SnT9g.png"" alt=""enter image description here"" /></a></p>
<p>Inclusion of <span class=""math-container"">$X_3$</span> in the model raised the beta of <span class=""math-container"">$X_1$</span> from <span class=""math-container"">$.381$</span> to <span class=""math-container"">$.399$</span> (and its corresponding partial correlation with <span class=""math-container"">$Y$</span> from <span class=""math-container"">$.420$</span> to <span class=""math-container"">$.451$</span>). Still, we find no suppressor in the model. <span class=""math-container"">$X_3$</span>'s part correlation (<span class=""math-container"">$.229$</span>) is not greater than its zero-order correlation (<span class=""math-container"">$.427$</span>). Same is for the other regressors. &quot;Facilitation&quot; effect was there, but not due to &quot;suppression&quot; effect. Definition of a suppessor is different from just strenghtening/facilitation; and it is about picking up mostly errors, due to which the part correlation exceeds the zero-order one.</p>
<h3>Suppression and Venn diagram</h3>
<p>Normal regressional situation is often explained with the help of Venn diagram.</p>
<p><img src=""https://i.stack.imgur.com/5zSa6.jpg"" alt=""enter image description here"" /></p>
<p><strong>A+B+C+D</strong> = 1, all <span class=""math-container"">$Y$</span> variability. <strong>B+C+D</strong> area is the variability accounted by the two IV (<span class=""math-container"">$X_1$</span> and <span class=""math-container"">$X_2$</span>), the R-square; the remaining area <strong>A</strong> is the error variability. <strong>B+C</strong> = <span class=""math-container"">$r_{YX_1}^2$</span>; <strong>D+C</strong> = <span class=""math-container"">$r_{YX_2}^2$</span>, Pearson zero-order correlations. <strong>B</strong> and <strong>D</strong> are the squared part (semipartial) correlations: <strong>B</strong> = <span class=""math-container"">$r_{Y(X_1.X_2)}^2$</span>; <strong>D</strong> = <span class=""math-container"">$r_{Y(X_2.X_1)}^2$</span>. <strong>B/(A+B)</strong> = <span class=""math-container"">$r_{YX_1.X_2}^2$</span> and <strong>D/(A+D)</strong> = <span class=""math-container"">$r_{YX_2.X_1}^2$</span> are the squared partial correlations which have the <a href=""https://stats.stackexchange.com/a/76819/3277"">same basic meaning</a> as the standardized regression coefficients betas.</p>
<p>According to the above definition (which I stick to) that a suppressor is the IV with part correlation greater than zero-order correlation, <span class=""math-container"">$X_2$</span> is the suppressor if <strong>D</strong> area &gt; <strong>D+C</strong> area. That <strong>cannot</strong> be displayed on Venn diagram. (It would imply that <strong>C</strong> from the view of <span class=""math-container"">$X_2$</span> is not &quot;here&quot; and is not the same entity than <strong>C</strong> from the view of <span class=""math-container"">$X_1$</span>. One must invent perhaps something like multilayered Venn diagram to wriggle oneself to show it.)</p>
<hr />
<p><strong>P.S.</strong> Upon finishing my answer I found <a href=""https://stats.stackexchange.com/a/34016/3277"">this</a> answer (by @gung) with a nice simple (schematic) diagram, which seems to be in agreement with what I showed above by vectors.</p>
",2013-10-27 16:31:46.460
58302,23075.0,1,,,,Would the group means of PC scores differ from the PC scores of group means?,<pca><multivariate-analysis><covariance><covariance-matrix>,CC BY-SA 3.0,"<p>I have $2$ $n\times p$ matrices, where $n$ are the rows (samples), and $p$ the columns (measurements).  Each matrix has samples and measurements from different groups. I call these the ""raw"" data. I've conducted a principal components analyses of the complete raw data, and computed the mean of each PC score by group. The latter I call the mean of the PC scores by group. </p>

<p>My question is whether the means of the PC scores by group (raw-data $\rightarrow$ PCA $\rightarrow$ mean PCs by group) would differ from the PC scores derived from a PCA conducted on the ""raw"" group means (raw data $\rightarrow$ mean by group $\rightarrow$ PCA)?</p>

<hr>

<h3>Example analysis of simulated data</h3>

<pre class=""lang-r prettyprint-override""><code>set.seed(123) 
a &lt;- matrix(rnorm(900),ncol=3,byrow=F) 
a[1:100,] &lt;- 4 + a[1:100,] 
a[101:200,] &lt;- -4 + a[101:200,]
# compute PCA and extract PC scores
pc &lt;- prcomp(a)$x 
    plot(pc[,1:2],col=rep(c(""red"",""blue"",""green""),each=100))
    # compute PC means and plot
    m &lt;-rbind(colMeans(pc[1:100,1:2]),colMeans(pc[101:200,1:2]),colMeans(pc[201:300,1â€Œâ€‹â€Œâ€‹:2]))
    points(m,col=""black"", pch=19,cex=1)
    # compute means of raw data by group
    b &lt;- rbind(colMeans(a[1:100,]),colMeans(a[101:200,]),colMeans(a[201:300,]))
    # conduct PCA on ""raw means"" and plot 
    pc2 &lt;- prcomp(b)$x
points(pc2[,1:2],col=""black"", pch=17,cex=1)
</code></pre>
",2013-10-27 17:00:57.470
58303,,1,,,user31966,Probability questions for statistics,<probability><self-study>,CC BY-SA 3.0,"<p>A set of final examination grades in a course is normally distributed with a mean of 73 and a standard deviation of 8.  </p>

<ol>
<li>What is the probability of getting a grade below 91 on the exam?  </li>
<li>What is the probability that a student scored between 65 and 89?  </li>
<li>If the professor grades on a curve (gives Aâ€™s to the top 10% of the class, regardless of the score), are you better off with a grade of 81 on this exam or a grade of 68 on a different exam, where the mean is 62 and the standard deviation is 3?  Explain why.   </li>
</ol>
",2013-10-27 18:05:36.960
58304,22637.0,2,,58303.0,,,,CC BY-SA 3.0,"<p>What you need to do is standardize those grades so you can use the standard Normal Distribution which is extensively tabulated. Try that first and should you have any problems let us know.</p>
",2013-10-27 18:16:09.723
58305,19750.0,1,236322.0,,,When is the differential entropy negative?,<entropy><information-theory>,CC BY-SA 4.0,"<p>The definition of entropy for a continuous signal is:</p>
<p><span class=""math-container"">$$h[f] = \operatorname{E}[-\ln (f(X))] = -\int\limits_{-\infty}^{\infty} f(x) \ln (f(x))\, dx$$</span></p>
<p><a href=""http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Extending_discrete_entropy_to_the_continuous_case:_differential_entropy"" rel=""nofollow noreferrer"">According to Wikipedia</a>, it can be negative. When would that happen? As far as I understand, <span class=""math-container"">$f(x)$</span> is always <span class=""math-container"">$\in[0,1]$</span> so <span class=""math-container"">$f(x)\cdot ln(f(x))$</span> can only be negative. What am I missing ?</p>
",2013-10-27 18:53:20.880
58306,2806.0,1,,,,What is the most appropriate test for a multi-year different group/yr experiment?,<anova><statistical-significance><t-test><experiment-design><ancova>,CC BY-SA 3.0,"<p>I've been working on a research project for close to five years now. For my thesis I have to show how ""well"" did my approach improve things. </p>

<p><strong>Setup</strong>: Every year we use a tool A to brainstorm and negotiate software requirements. The tool was wiki-based and had very low participation from technical and non-technical (client) stakeholders. (Yes we have real living and breathing clients for our class :). I looked at the state of affairs and saw that perhaps social networking based/influenced technologies could help increase participation. So I created tool B to replace tool A. However, tool A was used for the first 2 years and tool B for the latter 3 years. </p>

<p><strong>Environment</strong>: The students changed every year but the overall composition of the class was relatively same (i.e., we had the similar amount of awesome, average and underperforming teams/students). We strive for selecting client projects which are similar level of difficulty and are doable in the duration of the class. They can all be considered projects of the same 'class' (class as in category - same level of complexity etc).</p>

<p>Here's my <strong>hypothesis</strong>: Tool B will increase stakeholder participation as compared to Tool A (it was initially in past tense, then changed to present and now to future. Not sure what's right. Keep getting corrections for tense from advisors.)</p>

<p>Here are my <strong>measurements</strong>:</p>

<ol>
<li>Client (non-technical) participation using Tool B vs Tool A - via access logs</li>
<li>Team (student) participation using Tool B vs Tool A - via access logs + observational data</li>
<li>Number of requirements captured/negotiated (new/updated) in Tool A vs Tool B</li>
<li>Client surveys (for ascertaining the usefulness of tool A for capturing/negotiating requirements. Already have for Tool B.)</li>
</ol>

<p>One of my advisors suggests I use MANCOVA with 1-3 above as DVs and covariates capturing ""sense of complexity of projects"", ""average number of use-cases per project"" and ""some metric for team composition/makeup"" (which are pretty much the same across the years). Another advisor thinks that simple t-tests would work just fine: That is, I compare the average performance of groups across the years (Group 1 = Tool A; Group 2 = Tool B) and it should be sufficient or maybe an ANOVA at most. Another advior says not to do anything since the data itself is highly skewed i.e., using Tool B has really increased each of the above! He said that doing a statistical test is only to increase the perceived success of the tool and just makes a pompous show of the rigor in analysis.</p>

<p>I'm not really sure what would be a good approach here? I'm familiar with t-tests but have never done a MANCOVA ever and am afraid that I may just crunch the numbers and falsify underlying assumptions. What would be an appropriate test for such an experimental design, which is done across multiple years, with different groups, keeping the environment relatively constant? I have many such hypotheses w.r.t. tool B since there are many things that it enables from a process standpoint than what was doable before. It's really confusing with 3 advisors giving different advice and I not being a statistician to be able to decide.</p>
",2013-10-27 19:04:49.590
58450,503.0,2,,58444.0,,,,CC BY-SA 3.0,"<p>Given your further comment, I am not surprised at this result. BIC is a penalized log likelihood. It is useful for comparing models on one data set (here, each participant), but not for comparing across data sets. </p>

<p>What this result is telling you, in essence, is that the model fits very differently for different people, but that the amount of improvement in the fit by adding two parameters is about the same for each person. </p>
",2013-10-29 18:42:43.290
58307,16046.0,1,,,,MCMC for an explicitly uncomputable prior?,<bayesian><sampling><markov-chain-montecarlo><monte-carlo><prior>,CC BY-SA 3.0,"<p>I am trying to sample from a posterior distribution and I only have an explicit formula for likelihood but I can sample from the prior distribution. How can I sample from the posterior distribution with such a restriction. Is there any specific method?</p>

<p>After seeing the answers I've decided to write my exact question to clarify stuff:
Its about learning hyper-parameters $\alpha$ and $\beta$ and parameters $\theta_i$ in the following case:</p>

<p>$\alpha$ and $\beta$ are uniformly chosen from the perimeter of a square by following vertices: $(0,0),(0,1),(1,0),(1,1)$. Now $\theta_i$ is uniformly chosen from this line. $\theta_i$ it self is the parameter for data $y_i\sim\text{Bin}(n_i,\theta_i)$.</p>

<p>In my first attempt, and maybe being foolish I wrote a neat vectored algorithm which would sample from $p(\alpha,\beta,\theta)$ where $\theta=(\theta_1,\theta_2,...)$. But afterwards I realised that it is hardly related to sampling from $p(y|\alpha,\beta,\theta)$ maybe as a result of the answers here.</p>

<p>So what I am doing now is that I ignored the whole sampling algorithm I had for the joint priors. To solve the problem is to make a MC random-walk on parameter space $(\alpha,\beta)$ and sub-sampling from it (according to discussion on <a href=""https://stats.stackexchange.com/questions/73885/mcmc-on-a-bounded-parameter-space?noredirect=1#comment143867_73885"">another question of mine</a> in each step), then sampling from $p(\theta|\alpha,\beta)$ and then calculating the likelihood and then test the new sample according to Metropolis Hasting! I am not even sure this is correct but after my studies, this is the what I can think of!</p>
",2013-10-27 20:00:57.170
58308,16046.0,1,58319.0,,,MCMC on a bounded parameter space?,<sampling><markov-chain-montecarlo><monte-carlo><random-walk>,CC BY-SA 3.0,"<p>I am trying to apply MCMC on a problem, but my priors(in my case they are $\alpha\in[0,1],\beta\in[0,1]$)) are restricted to an area? Can I use normal MCMC and ignore the samples that fall outside of the restricted zone(which in my case is [0,1]^2), i.e. reuse transition function when the new transition falls out of restricted(constrained) area?</p>
",2013-10-27 20:07:02.930
58309,9554.0,2,,58306.0,,,,CC BY-SA 3.0,"<p>Guess what, all 3 of them might have a point. The issues with ""A causes B"" are tricky. :)</p>

<p>But first things first. If your hypothesis is:</p>

<blockquote>
  <p>Tool B will increase stakeholder participation as compared to Tool A.</p>
</blockquote>

<p>Stick with it. That's probably the most straightforward and honest thing to measure (honest, in my opinion, since you don't use any covariates such as ""sense of complexity of projects"" that are unreliable in terms of how well can you measure that etc.). As you correctly pointed out, and so did Prof. 2, a two sample t-test is the right tool to measure mean differences between two groups.</p>

<p>However, then you seem to be wanting to analyze other four things you are measuring, out of which three weren't measured for both tools according to your description: 1:B, 2:B, 3:{A,B}, 4:A. Statistically, I don't see how you want to determine differences between groups for anything other than measurement 3. Which leaves you with a t-test again.</p>

<p>Your main problem is that a t-test will allow you to say that there is a significant difference. You always need a controlled experiment to claim causality, which in your case is tricky, as you didn't deploy both tools simultaneously using a control group, but rather you first test one, than the other. The obvious problem with this approach is that in the meantime, the usage pattern might have changed due to a number of factors so diverse such as better smartphones, more savvy users, you don't know. </p>

<ul>
<li>But I would still claim that what you have is more than observational
data.  </li>
<li>I would roll with a t-test for the things measured for both
tools.  </li>
<li>I would avoid metrics such as ""perceived complexity of XY"". </li>
<li>If the densities of the ""Number of requirements"" of A and B are
visibly different, plot them.</li>
</ul>

<p>You can also run <a href=""http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"" rel=""nofollow"">one</a> of the plethora of tests for testing whether your two empirical distributions are the same. (I agree with Prof. 3 that if you can clearly see a different distribution shape, centered around a different value, formal tests are a bit of a showmanship, but he is a professor and can get away with saying that, you probably won't).</p>

<p>Best of luck!</p>
",2013-10-27 20:09:36.460
58310,23078.0,1,,,,Does JMP aggregate data in graph-building?,<data-visualization><jmp>,CC BY-SA 3.0,"<p>I am using JMP to find a relationship between drought index values and yearly corn yields for a 30 year period.  I have drought data for seven different indeces and each index ranges from -6 (severe drought) to +6 (extremely high precipitation).</p>

<p>The drought data contains monthly drought index values for March-September for every year from 1981 to 2011.  The corn yield data contains one number (bushels/acre) for every year from 1981 to 2011.  I built two side-by-side graphs in JMP, with one showing corn yield data over the 30 year period and the other showing drought data from one particular index over the same time period.  Both are smooth curves and visually line up and seem to have a relationship.</p>

<p>My problem is that I have no idea how JMP is processing the drought data. There are eight separate drought index values per year, every year, for each drought index, and only one value per year for the corn yield data. I assumed that JMP was smoothing out the drought curve by taking an average of the eight index values for every year, then using that number in the graph construction but I haven't been able to find any literature that can tell me this for certain.</p>
",2013-10-27 20:16:54.170
58311,9554.0,2,,58305.0,,,,CC BY-SA 3.0,"<p>You are just confusing $f(x)$ and $F(X)$. The density function $f(x)$ can be greater 1. It just integrates to 1. It is $F(X) \in [0, 1]$.</p>

<p>Best </p>
",2013-10-27 20:30:28.140
58313,503.0,4,,,,,,CC BY-SA 3.0,ggplot is an R package for creating graphics. It was developed by Hadley Wickham and based on the Grammar of Graphics by Leland Wilkinson.,2013-10-27 21:12:43.780
58312,503.0,5,,,,,,CC BY-SA 3.0,,2013-10-27 21:12:43.780
58314,19750.0,1,58315.0,,,Why does entropy increase with dispersion for continuous but not for discrete distributions?,<distributions><entropy><intuition>,CC BY-SA 3.0,"<p>For a pdf $f(x)$ (i.e. <strong>continuous</strong> distribution), Entropy (differential entropy) <a href=""http://en.wikipedia.org/wiki/Differential_entropy"" rel=""nofollow"">is defined</a> as:</p>

<p>$H_C(X) = -\int_\mathbb{X} f(x)\log f(x)\,dx.$</p>

<p>For a <strong>discrete</strong> distribution with p.m.f $F(x)$, Entropy is defined as:</p>

<p>$H_D(X) = -\sum_{i=1}^n {F(x_i) \log F(x_i)}.$</p>

<p>The definitions look analogous to each other. However, entropy increases with dispersion for continuous but not for discrete distributions. Why?</p>
",2013-10-27 21:15:30.070
58324,855.0,2,,58310.0,,,,CC BY-SA 3.0,"<p>Just to make sure I understand you, I think you're plotting the index value by the year value, and there are several index values per year value. And you're in Graph Builder using the Smoother element. Here's a quick mock-up:</p>

<p><img src=""https://i.stack.imgur.com/h1mTS.png"" alt=""JMP Graph Builder smoother with duplicate X values""></p>

<p>Actually in my mock-up, I also have the Points element turned on to emphasize the multiple Y values by X.</p>

<p>From the <a href=""http://www.jmp.com/support/help/Additional_Examples_Using_Graph_Builder.shtml"" rel=""nofollow noreferrer"">JMP 11 doc</a>:</p>

<blockquote>
  <p>The smoother is a cubic spline with a default lambda of 0.05 and
  standardized X values. You can change the value of lambda using the
  slider. You can obtain the same spline in the Bivariate platform...</p>
</blockquote>

<p>Cubic splines are technically only defined for data sets with unique X values. In case of duplicate Xs, JMP first takes the weighted mean of the corresponding Y values. Use the Freq drop zone in Graph Builder if you want to control the weighting, otherwise each Y is weighted equally.</p>
",2013-10-27 23:50:31.847
58315,633.0,2,,58314.0,,,,CC BY-SA 3.0,"<p>It seems you might be asking why spreading discrete data has no effect on entropy.  Because entropy is a measure of expected surprise, the various labels or values that a thing can take is immaterial.  So, the discrete values $x_i$ don't matter, merely their masses and spreading the $x_i$s has no effect.</p>

<p>In the continuous case, spreading things out by scaling inevitably reduces the densities, which affects the entropy as defined in your question.  The definition is consistent with our intuition of entropy, Shannon explains, because we typically compare two entropies, and since both are scaled, this effect cancels out.  Differential entropy is also consistent with discrete entropy in the sense that it approximates what would happen if the entropy of the quantized distribution were measured.</p>

<p>Note that in the continuous case, spreading things out by other methods can leave entropy unchanged.  For example, a uniform distribution over $[-\frac12,\frac12]$ has entropy zero.  ""Spreading it out"" so that it is uniform over $[-10, -9.5], [9.5,10]$ still has entropy zero.  Spreading never matters; only the expected surprisal does.</p>

<blockquote>
  <p>There is one important difference between the continuous and discrete
  entropies. In the discrete case the entropy measures in an absolute
  way the randomness of the chance variable. In the continuous case the
  measurement is relative to the coordinate system [and] the entropy can
  be considered a measure of randomness relative to an assumed standard,
  namely the coordinate system chosen with each small volume element $dx_1, â€¦, dx_n$
  given equal weight. When we change the coordinate system to
  $y_1, â€¦, y_n$, the entropy in the new system measures the randomness
  when equal volume elements $dx_1, â€¦, dx_n$ in the new system are given
  equal weight.</p>
  
  <p>In spite of this dependence on the coordinate system the
  entropy concept is as important in the continuous case as the discrete
  case. This is due to the fact that the derived concepts of information
  rate and channel capacity depend on the difference of two entropies
  and this difference does not depend on the coordinate frame, each of
  the two terms being changed by the same amount.</p>
  
  <p>The entropy of a
  continuous distribution can be negative. The scale of measurements
  sets an arbitrary zero corresponding to a uniform distribution over a
  unit volume. A distribution which is more confined than this has less
  entropy and will be negative. The rates and capacities will, however,
  always be nonnegative.
  â€” Shannon 1948</p>
</blockquote>
",2013-10-27 21:32:09.783
58316,9554.0,2,,58290.0,,,,CC BY-SA 3.0,"<p>Since you are using the ROC, I presume that you are running 5 classifiers.
Frank is right about the ROC, that's not the way people compare models.
For the linear, and generalized linear models you can apply the likelihood ratio test.</p>

<p>However, in case you are after the best prediction performance, and particularly in case you are not using a parametric model, but say a random forest classifier, I would do the following:</p>

<ul>
<li>generate data</li>
<li>split it randomly into a training and testing set</li>
<li>train all your 5 models and test their performance</li>
<li>repeat the entire procedure for as many time as the run time permits and store all 5 ROC curves (I would pick a 1000, or 10000 as a minimum, depending on the convergence of the mean predictions)</li>
<li>report the means of the 5 ROC curves together with a 90% pointwise confidence interval around them</li>
</ul>

<p>The idea is of course that you pick a model that seems like the best combination of high AUC and low variance (narrow intervals around the mean) of the estimates.</p>

<p>Best</p>
",2013-10-27 21:32:43.350
58317,7007.0,2,,58307.0,,,,CC BY-SA 3.0,"<p>Can you sample from the conditional distribution of $X\mid\Theta$? If you can, try using <a href=""http://en.wikipedia.org/wiki/Approximate_Bayesian_computation"" rel=""nofollow"">ABC</a> to sample (approximately) from the posterior. The ABC rejection algorithm does not use the value of the prior density at each candidate point.</p>
",2013-10-27 21:43:48.200
58318,22843.0,1,58321.0,,,Using expectation to detect bias,<mathematical-statistics><bias>,CC BY-SA 3.0,"<p>I was going through Penn State's online notes and noticed this expression:</p>

<p>$ v^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2$ </p>

<p>In the line below it they stated that the $E[v^2] = (1 - \frac{1}{n})\sigma^2$. I was wondering how would you get that? </p>

<p>Would it be wrong for me to say that since $\sum_{i=1}^n (y_i - \bar{y})^2 = (n-1)s^2$ the
$E[v^2] = E[\frac{(n-1)s^2}{n}] = (1 - \frac{1}{n})\sigma^2$ ? But this line of reasoning forces me to assume that $E[s^2] = \sigma^2$ and I don't even know why that's true.</p>
",2013-10-27 22:41:43.817
58319,5448.0,2,,58308.0,,,,CC BY-SA 4.0,"<p>You have several nice, more-or-less simple, options.  Your uniform prior helps make them simpler.</p>
<p>Option 1: Independence sampler.  You can just set your proposal distribution equal to a uniform distribution over the unit square, which ensures that samples won't fall outside the restricted zone, as you call it.  Potential downside: if the posterior is concentrated in a very small region of the unit square, you may have a very low acceptance rate.  OTOH, it's hard to generate random numbers faster than from a U(0,1) distribution.  Potential upside: less work for you.</p>
<p>Option 2: Transform your parameters to something that isn't bounded, make proposals for the transformed parameters, then transform the parameters back for use in the likelihood functions.  Note that in this case the prior is going to be on the transformed parameters, because that's what you're making proposals for, so you'll have to mess with the Jacobian of the transform to get the new prior.  For your analysis, of course, you'll transform the MCMC-generated parameter random numbers back to the original parameters.  Potential downside: more initial work for you.  Potential upside: better acceptance rate for your proposals.</p>
<p>Option 3: Construct a proposal distribution other than an independence sampler that is on the unit square.  This allows you to keep your uniform prior, but at the cost of greater complexity when calculating the proposal probabilities.  An example of this, letting <span class=""math-container"">$x$</span> be the current value of one of your parameters, would be a Beta distribution with parameters <span class=""math-container"">$(nx, n(1-x))$</span>.  The larger <span class=""math-container"">$n$</span> is, the more concentrated your proposal will be around the current value.  Potential downside: more initial work for you.  Potential upside: better acceptance rate for your proposals - but if you make <span class=""math-container"">$n$</span> too large, and move near to a corner, you might wind up making lots of small moves in the corner before getting out.</p>
<p>Option 4: Just reject any proposals that fall outside the unit square (Xian's half-hearted suggestion).  Note that this is not the same as just generating another proposal; in this case you are rejecting the proposal, which means your next value for the parameter is the same as the current value for the parameter. This works because it's what would happen if you had a zero prior probability for some region of your parameter space and generated a random number that fell in that region.  Potential downside: if you get near a corner, you may have a low acceptance probability and get stuck for a while.  Potential upside: less work for you.</p>
<p>Option 5: Create an extended problem on the plane which, on the unit square, is the same as the actual problem you face, do everything right, then, when post-processing the results of the MCMC sampling, throw out all the samples outside of the unit square.  Potential upside:  If it's very easy to create that extended problem, it may be less work for you.  Potential downside: if the Markov chain wanders off somewhere outside the unit square for a while, you may have, in effect, horrible acceptance probabilities, as you will throw out most of your samples.</p>
<p>No doubt there are other options, I'd be interested to see what other people suggest!</p>
<p>The difference between 2 and 3 is to some extent conceptual, although with real implications for what you actually do.  I'd probably go with 3, as I'd just let R  tell me what the proposal probabilities are (if I'm programming in R) and the amount of extra effort, aside from some tuning of the proposal distribution parameter <span class=""math-container"">$n$</span>, looks small to me.  If I was using JAGS or BUGS, of course, that would be a whole different matter, since those tools handle their own proposals.</p>
",2013-10-27 23:05:50.693
58320,3894.0,1,,,,Computing non-central moments and normalizer of a quartic exponential distribution,<moments><exponential-family><numerical-integration>,CC BY-SA 3.0,"<p>Consider a random variable $X$ which has quartic exponential distribution: $$X \sim P(x)=\frac{1}{Z}e^{ax + bx^2 + cx^3 + dx^4}$$ How can one compute $Z$ or non-central moments $E X^k$ given that they exist? As far as I understand, there are no closed-form formulas for these quantities, but is there a good numerical procedure for estimating them?</p>

<p>Since I'm very far from being any kind of expert in numerical integration, I'm open to any suggestions that will get the job done with reasonable precision.</p>
",2013-10-27 23:19:58.153
58321,9554.0,2,,58318.0,,,,CC BY-SA 3.0,"<p>Ok, though I'm not sure about what is the course, or what is your $s^2$ exactly, the course notes seem to use $v^2$ to denote sample variance.</p>

<p>The <a href=""http://en.wikipedia.org/wiki/Variance"" rel=""nofollow"">proof</a> (see bit on sample variance) of the line is not hard, but I guess ""hard"" is relative. I remember, a lot of people struggling with this one in the undergrad. The key is not to forget, after writing out the second power of the term in brackets and taking the expected value operator inside the sum, you have to use
$E[X^2] = V[X^2] + E[X]^2$ to substitute all the terms $E[y_i^2]$, the rest is re-arranging the sums.</p>
",2013-10-27 23:27:18.270
58322,23081.0,1,,,,How to interpret the results of a t-test?,<statistical-significance>,CC BY-SA 3.0,"<p>I recently performed an experiment observing ants. For five days I measured the amount of food they ate, 25 ants in the fruits group and 25 ants in the vegetables group. Each ant was kept isolated in its own container. </p>

<p>My hypothesis was that the ant (3 mg in body weight) would be able to eat 20 times more food than what it weighed; I came to that assumption by relating it to the amount of weight they can lift. </p>

<p>The average for the fruits was 357 mg and the average for the vegetables was 358 mg. Keep in mind that I didn't take into account all the weight lost from the food by loss of water evaporating; I just assumed it was all eaten by the ants. </p>

<p>For my results I get a t-value of -0.0981 ; df=223 ; standard error of difference=0.015 ; p-value of 0.9220. </p>

<p>So since this is my first experiment dealing with t-test. I don't know how to interpret and present the results. Can someone please explain what they mean?</p>
",2013-10-27 23:36:55.623
58323,22637.0,2,,58322.0,,,,CC BY-SA 3.0,"<p>The p-value indicates that the null hypothesis cannot be rejected. It seems that the ant  is indeed able to eat 20 times more food than its weight. While I can only speak for the statistical significance of the test, I would advise you to incorporate as many parameters as possible in your research, including water evaporation.</p>
",2013-10-27 23:47:12.423
58340,10450.0,2,,58337.0,,,,CC BY-SA 3.0,"<p>1) The error, $err_{m}$, in step 4. captures the relative ratio of weighted prediction errors for each iterated pass, $m$, of the sequential learning algorithm ( $ 0 &lt; err_{m} &lt; 1$) .  The errors are equivalent in both equations. You can see the weight distribution $D_{t}(i) = w_{i,t} / {\sum_{i=1}^m w_{i,t}}$. Only the chosen variable names are reversed ($m=t$, the iteration step in one case, and $m=N$ the number of observations in the other).</p>

<p>$\text{err}_m = \frac{\sum_{i=1}^N w_{i,m} \mathbb{I}(\hat{y} \neq \phi(\mathbf{x_i)}}{\sum_{i=1}^N w_{i,m}} = $</p>

<p>$\epsilon_{t} = \sum_{i=1}^{m} D_{t}(i)I(y_i \ne h_{t}(x_{i}))$</p>

<p>2) While the stopping rule is not shown in Kevin's algorithm, one of the theoretical requirements of the adaboost learner is to have weak learners slightly greater than chance. He does mention it in the text.</p>

<p>3) The unclosed parenthesis does appear to be a typo.</p>

<p>4) see 2) </p>
",2013-10-28 06:22:23.923
58325,15430.0,1,,,,Hypothesis testing with exponential family,<hypothesis-testing><distributions><gamma-distribution><exponential-family><convolution>,CC BY-SA 3.0,"<p>I'm interested in running hypothesis tests for a variety of members of the exponential family with continuous support, for different values of the parameter/s, for a sample of n i.i.d random variables (distributed according to a particular member and parametrization of the exponential family).</p>

<p>In general, it seems the sufficient statistic (at least for a single parameter) is a sum of the variables in the sample.  I know this can be obtained at least theoretically from n-fold convolution, but in general, there does not seem to be a nice form for the distribution of this sum, which I will need to compute p-values.</p>

<p>so I'm wondering what distributions have known distributions for the sum of n i.i.d variables, and if there is a good reference for doing hypothesis testing with exponential family.</p>

<p>I know that:</p>

<ul>
<li>sums of iid exponentials are distributed Erlang</li>
<li>sums of iid normals are distributed normal</li>
<li>sums of iid gamma are distributed gamma (but what about product of gamma, which is the sufficient statistic for the shape parameter?)</li>
</ul>

<p>So what about beta, pareto, log normal, and so on?</p>

<p>Is the difficulty of this the reason why no one does hypothesis tests anymore?</p>
",2013-10-27 23:57:23.037
58326,15430.0,2,,58314.0,,,,CC BY-SA 3.0,"<p>In the continuous case, because they are continuous, spreading out the $x$ requires that you dampen the densities, and so this effects the entropy, since the density is tied in an explicit way to the values of $x$.</p>

<p>In the discrete case, the values of $x_i$ are more like indices, and there is no explicit connection between the density and the $x_i$ (unlike for continuous, where there is a function connecting them), so spreading out the $x_i$ doesn't affect the densities, and hence doesn't affect the entropy.</p>

<p>All that being said, it was my understanding that the entropy of a continuous distribution (with unbounded support) tends to diverges...</p>
",2013-10-28 00:04:20.103
58327,8926.0,2,,58322.0,,,,CC BY-SA 3.0,"<p>Commonly used level of statistical significance at which null hypothesis can be considered rejected is 5% or p-value = 0.05, though this number differs depending on a problem or discipline, 0.1%, 1% or 10% are used. In your case you can reject the null hypothesis at 92.2% significance level or p-value = 0.922. Therefore, you fail to reject the null hypothesis for any meaningful significance level.</p>

<p>t-stat and p-value are inversely related, large t-stat corresponds to small p-value and vice-versa, but p-value is easier to interpret. </p>
",2013-10-28 00:08:25.780
58328,21932.0,1,58363.0,,,hypothesis testing using poisson distribution,<hypothesis-testing><poisson-distribution>,CC BY-SA 3.0,"<p>At a nuclear plant great care is taken to measure the employees health.These are the number of visits made by each of the 10 employees to the doctor during a calender year. 3,6,5,7,4,2,3,5,1,4</p>

<p>Assuming the number of visits made by employee has a poisson distribution ,test the hypothesis that the annual mean per employee is greater than 3.</p>

<p>I am using the graphical method and I am not sure of which p[X=x] should i consider.</p>

<p>X: no.of visits by each employee to the doctor.</p>

<pre><code>H0:lambda=3
H1:lambda&gt;3
X follows a Poisson(3)
</code></pre>

<p>Then what is the probability that I should check?</p>

<p>What I did was as the average of sample data is 4.73636. Therfore calculated p[X>=4] and checked if it was in the critical region. Is this the correct probability to calculate?  In a poisson distribution the expected value is calculated as x*p[X=x] right?Not as (sigma x*f(x))/(sigma x)</p>
",2013-10-28 01:17:32.970
58329,22591.0,1,,,,Search in TF-IDF,<machine-learning><python><text-mining>,CC BY-SA 3.0,"<p>I want to find the similarity between a document with documents coded as TF-IDF in a pickle file (Python). TF-IDF is done as offline so there is no problem, but when I send a new document for similarity check it takes around 2 minute while I need something real-time (&lt; 2 seconds). For this purpose I used the following code:</p>

<pre><code>for p_tf in p_tfidf:
    temp_similarity = 0
    for item in p_tf:
        (score,word) = item
        if word in input_text:
            temp_similarity += score

    similarity_score.append([temp_similarity, id])
</code></pre>

<p>Any clue how to improve system?</p>
",2013-10-28 01:38:35.207
58330,23085.0,1,59733.0,,,Prove/counter example: A minimax decision rule is always Bayes wrt some proper prior,<bayes><decision-theory><point-estimation>,CC BY-SA 3.0,"<p>Not sure whether the claim is true or false.</p>

<p>If claim is true, intuitively, it might have something to do with ""least favorable priors"", but am not able to figure out the connection.</p>

<p>If claim is false, one example is when $X_i|\theta \sim $ Poisson$(\theta)$, then $\bar{X}$ is minimax. But a Gamma$(\alpha, \beta)$ prior fails since, that would indicate $\beta = 0$, which is improper. But, how do we know there is no other prior that gives $\bar{X}$ as a Bayes rule?</p>
",2013-10-28 02:20:34.733
58331,855.0,2,,40121.0,,,,CC BY-SA 3.0,"<p>There is a discussion of the circle construction in the JMP help/manuals. See <a href=""http://www.jmp.com/support/help/Statistical_Details_for_the_Oneway_Platform.shtml"" rel=""nofollow"">Statistical Details for Comparison Circles</a>.</p>
",2013-10-28 02:39:50.397
58332,8926.0,2,,58329.0,,,,CC BY-SA 3.0,"<p>You can make use of <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">sklearn.feature_extraction.text.TfidfVectorizer</a></p>

<p>A simple example:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=1)

my_phrases = [""boring answer phrase"",
              ""exciting phrase"",
              ""phrase on stackoverflow"",
              ""answer on stackoverflow""]

my_features = vectorizer.fit_transform(my_phrases)
</code></pre>

<p>Result:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.set_printoptions(precision=4)
&gt;&gt;&gt; my_features.A
array([[ 0.5535,  0.702 ,  0.    ,  0.    ,  0.4481,  0.    ],
       [ 0.    ,  0.    ,  0.8429,  0.    ,  0.538 ,  0.    ],
       [ 0.    ,  0.    ,  0.    ,  0.6137,  0.4968,  0.6137],
       [ 0.5774,  0.    ,  0.    ,  0.5774,  0.    ,  0.5774]])
&gt;&gt;&gt; vectorizer.get_feature_names()
[u'answer', u'boring', u'exciting', u'on', u'phrase', u'stackoverflow']
</code></pre>

<p>As a side note, you can remove ""stop words"" like ""on"", by passing <code>stop_words='english'</code> parameter:</p>

<pre><code>vectorizer = TfidfVectorizer(min_df=1, stop_words='english')
</code></pre>

<p><strong>Edit:</strong></p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# each phrase here could be document in your list 
# of documents
my_phrases = [""boring answer phrase"",
              ""exciting phrase"",
              ""phrase on stackoverflow"",
              ""answer on stackoverflow""]

#  and you want to find the most similar document
#  to this document             
phrase = [""stackoverflow answer""]

# You could do it like this:
vectorizer = TfidfVectorizer(min_df=1, stop_words='english')
all_phrases = phrase + my_phrases
my_features = vectorizer.fit_transform(all_phrases)
scores = (my_features[0, :] * my_features[1:, :].T).A[0]
best_score = np.argmax(scores)
answer = my_phrases[best_score]
</code></pre>

<p>Result:</p>

<pre><code>&gt;&gt;&gt; answer
'answer on stackoverflow'
</code></pre>
",2013-10-28 02:52:52.830
58341,21985.0,1,,,,How to get exact distribution of estimated p for binomial distribution?,<distributions><self-study><estimation><binomial-distribution>,CC BY-SA 3.0,"<p>This question is kind of a follow up of another question I had: <a href=""https://stats.stackexchange.com/questions/73830/asymptotic-normal-distribution-via-the-central-limit-theorem"">Asymptotic normal distribution via the central limit theorem</a></p>

<p>There I had to calculate the estimator for $p$ (meaning $p$ for success) and approximate it's distribution by approximation with a normal distribution.</p>

<p>Now I would like to get the exact distribution of $p$.</p>

<p>I got already the following hint: ""You have the functional form of $\hat{p}$. Look up how we derive the distribution of a function of a discrete random variable."" Unfortunately that did not lead me to a solution...</p>
",2013-10-28 07:21:43.657
58333,23086.0,1,58335.0,,,R- Analysis of homogeneity of slopes,<r><lm>,CC BY-SA 3.0,"<p>IÂ´d like to analyse the effect of a treatment (<code>treatment    : Factor w/ 2 levels ""ambient"",""elevated""</code>) in tree diameter increment. Tree diameter is influenced by tree size. To do so, I performed the following lm:</p>

<pre><code>Call:
lm(formula = BAI2013 ~ diameterJul12 * treatment, data = bandNA)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.6493  -3.1740  -0.3767   3.3631  22.7267 

Coefficients:
                                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                     -20.49357    2.12883  -9.627  &lt; 2e-16 ***
diameterJul12                     1.24194    0.08876  13.992  &lt; 2e-16 ***
treatmentelevated                10.72336    3.45783   3.101 0.002295 ** 
diameterJul12:treatmentelevated  -0.54953    0.14795  -3.714 0.000285 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.461 on 153 degrees of freedom
Multiple R-squared:  0.6035,    Adjusted R-squared:  0.5958 
F-statistic: 77.63 on 3 and 153 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>How can I interpret the results? I need to figure out whether the slope of BAI~diameter is steeper for elevated trees than for ambient.
Thanks</p>
",2013-10-28 03:04:51.660
58334,5448.0,2,,58307.0,,,,CC BY-SA 3.0,"<p>One specific method is <a href=""http://streaming.stat.iastate.edu/~stat444x_B/Notes/7-ImportanceSampling.pdf"" rel=""nofollow"">importance sampling</a>.  The key slide in the link is slide 3.  </p>

<p>In this case, you'd:</p>

<ol>
<li><p>Generate a large random sample from your prior, let us denote it $\theta_i, i = 1, \dots, N$.  </p></li>
<li><p>Each element of that sample will have associated with it a value of the likelihood function, let us say $l_i$.  Calculate them.</p></li>
<li><p>We can then form resampling acceptance probabilities $p_i = l_i / \max l_i$.</p></li>
<li><p>Generate your posterior sample as follows:</p>

<p>a) For each $j = 1, \dots,$ some large $M$, select some index $k$
uniformly from  $\{1,\dots,N\}$.<br>
b) Generate $u \sim
    \text{U}(0,1)$.<br>
c) If $u &lt; p_k$, then $\theta_k$ is put into your
posterior sample.  Otherwise, go to the next $j$, and nothing is put
into your posterior sample.</p></li>
</ol>

<p>Of course, if you can't generate a large random sample from the prior, or you have a few relatively large values of the likelihood and a lot of very small ones (which would happen if your posterior is very concentrated with respect to the prior), you won't get very good results.  No panaceas here, I'm afraid!  But this method works quite well in many cases.</p>
",2013-10-28 03:07:04.160
58335,23087.0,2,,58333.0,,,,CC BY-SA 3.0,"<p>The maximum likelihood fitted model is</p>

<pre><code>BAI2013 = 1.24194 * diameterJul12 + 10.72336 * treatmentelevated - 0.54953 * diameterJul12 * treatmentelevated - 20.49357
</code></pre>

<p>Here <code>treatmentelevated</code> is a binary variable which is 1 for ""elevated"". All terms have significant p-values (these come from Wald statistics) suggesting they should be kept in the model. The interaction term is negative suggesting the slope of BAI~diameter is <em>less</em> steep (by 0.54953) in the elevated treatment group. </p>
",2013-10-28 04:12:45.507
58336,23087.0,2,,58307.0,,,,CC BY-SA 3.0,"<p>How high dimensional is your state space? If it's a univariate problem I would suggest slice sampling. If it's higher dimensional you might be able to use slice sampling within Gibbs sampling still. If not, the other suggestions of ABC or importance sampling (some versions of ABC use importance sampling as an inner loop also), may be your best bet if there really isn't any additional structure you can exploit. As @jbowman says however, if your prior and posterior are very mismatched these methods will struggle. </p>
",2013-10-28 04:20:45.933
58337,19750.0,1,58340.0,,,Multiple definitions of AdaBoost,<classification><boosting>,CC BY-SA 3.0,"<p>The description of AdaBoost in <a href=""http://rads.stackoverflow.com/amzn/click/0262018020"" rel=""nofollow noreferrer"">Kevin Murphy's Machine Learning book</a> (shown in a snapshot below) differs from the one <a href=""http://en.wikipedia.org/wiki/Adaboost"" rel=""nofollow noreferrer"">in Wikipedia</a>. I am trying to relate both definitions. Step by step, my questions are:</p>

<ol>
<li><p>What exactly is $\text{err}_m$  (step 4) supposed to capture below? Is this equivalent to the $\epsilon_t$ in Wikipedia's definition?</p></li>
<li><p>Why isn't there a stopping rule in Kevin Murphy's method but there is one in Wikipedia's definition?</p></li>
<li><p>There seems to be a typo in the parenthesis in the denominator, just in case - is it supposed to say the following?:</p>

<p>$\text{err}_m = \frac{\sum_{i=1}^N w_{i,m} \mathbb{I}(\hat{y} \neq \phi(\mathbf{x_i)}}{\sum_{i=1}^N w_{i,m}} $</p></li>
<li><p>Most importantly, Wikipedia provides the following criteria for <strong>choosing the weak learner</strong> and for stopping:</p>

<blockquote>
  <p>$h_{t} = \underset{h_{t} \in \mathcal{H}}{\operatorname{argmax}} \; \left\vert 0.5 - \epsilon_{t}\right\vert$
  where  $\epsilon_{t} = \sum_{i=1}^{m} D_{t}(i)I(y_i \ne h_{t}(x_{i}))$</p>
  
  <p>$If \left\vert 0.5 - \epsilon_{t}\right\vert \leq \beta$, where $\beta$ is a previously chosen threshold, then stop. </p>
</blockquote>

<p>while Kevin's book defines the full algorithm as follows, and I don't see those two steps above in it:</p></li>
</ol>

<p><img src=""https://i.stack.imgur.com/YejfD.png"" alt=""enter image description here""></p>
",2013-10-28 04:28:34.460
58338,23089.0,1,,,,How to interpret the results of ADF test using SAS ARIMA?,<interpretation><sas><unit-root>,CC BY-SA 3.0,"<pre><code>                                       The SAS System      14:11 Thursday, October 6, 2013   1

                                      The ARIMA Procedure

                                Name of Variable = ln_G_S_Index

                     Period(s) of Differencing                           1
                     Mean of Working Series                       0.094293
                     Standard Deviation                           0.316757
                     Number of Observations                             15
                     Observation(s) eliminated by differencing           1


                                        Autocorrelations

 Lag    Covariance    Correlation    -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 1      Std Error

   0      0.100335        1.00000    |                    |********************|             0
   1     0.0026693        0.02660    |          .         |*        .          |      0.258199
   2     -0.018517        -.18456    |          .     ****|         .          |      0.258382
   3      0.029440        0.29342    |         .          |******    .         |      0.267025

                                 ""."" marks two standard errors


                                    Inverse Autocorrelations

               Lag    Correlation    -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 1

                 1       -0.14763    |          .      ***|         .          |
                 2        0.19526    |          .         |****     .          |
                 3       -0.27516    |          .   ******|         .          |


                                    Partial Autocorrelations

               Lag    Correlation    -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 1

                 1        0.02660    |          .         |*        .          |
                 2       -0.18539    |          .     ****|         .          |
                 3        0.31522    |          .         |******   .          |


                               Phillips-Perron Unit Root Tests

              Type           Lags         Rho    Pr &lt; Rho        Tau    Pr &lt; Tau

              Zero Mean         0    -11.6883      0.0066      -3.23      0.0033
                                1    -11.4504      0.0074      -3.23      0.0034
              Single Mean       0    -13.7527      0.0129      -3.71      0.0171
                                1    -12.6667      0.0218      -3.76      0.0157
              Trend             0    -14.5288      0.0601      -3.25      0.1144
                                1    -13.1531      0.1022      -3.20      0.1239
                                        The SAS System      14:11 Thursday, October 6, 2013   2

                                      The ARIMA Procedure

                                Name of Variable = ln_G_S_Index

                     Period(s) of Differencing                           1
                     Mean of Working Series                       0.094293
                     Standard Deviation                           0.316757
                     Number of Observations                             15
                     Observation(s) eliminated by differencing           1


                                        Autocorrelations

 Lag    Covariance    Correlation    -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 1      Std Error

   0      0.100335        1.00000    |                    |********************|             0
   1     0.0026693        0.02660    |          .         |*        .          |      0.258199
   2     -0.018517        -.18456    |          .     ****|         .          |      0.258382
   3      0.029440        0.29342    |         .          |******    .         |      0.267025

                                 ""."" marks two standard errors


                                    Inverse Autocorrelations

               Lag    Correlation    -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 1

                 1       -0.14763    |          .      ***|         .          |
                 2        0.19526    |          .         |****     .          |
                 3       -0.27516    |          .   ******|         .          |


                                    Partial Autocorrelations

               Lag    Correlation    -1 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 1

                 1        0.02660    |          .         |*        .          |
                 2       -0.18539    |          .     ****|         .          |
                 3        0.31522    |          .         |******   .          |


                            Augmented Dickey-Fuller Unit Root Tests

    Type           Lags         Rho    Pr &lt; Rho        Tau    Pr &lt; Tau          F    Pr &gt; F

    Zero Mean         0    -11.6883      0.0066      -3.23      0.0033
                      1    -12.4302      0.0041      -2.42      0.0197
    Single Mean       0    -13.7527      0.0129      -3.71      0.0171       6.91    0.0157
                      1    -25.2133      &lt;.0001      -3.63      0.0214       6.59    0.0206
    Trend             0    -14.5288      0.0601      -3.25      0.1144       6.44    0.0799
                      1    -45.0252      &lt;.0001      -3.20      0.1265       6.92    0.0622
                                        The SAS System      14:11 Thursday, October 6, 2013   3
</code></pre>
",2013-10-28 05:34:54.517
58339,23090.0,1,,,,Is this test answer good enough to show Granger Causality?,<hypothesis-testing><matlab><p-value><granger-causality>,CC BY-SA 3.0,"<p>I used an inbuilt Matlab function to check for Granger Causality between two time series P and T that have a correlation coefficient of 0.6.</p>

<p>The function is : 
<a href=""https://www.mathworks.com/matlabcentral/fileexchange/25467-granger-causality-test/content/granger_cause.m"" rel=""nofollow"">https://www.mathworks.com/matlabcentral/fileexchange/25467-granger-causality-test/content/granger_cause.m</a></p>

<p>On running it, I got the following answer:</p>

<pre><code>&gt;&gt; [fs,cv] = granger_cause(P, T,0.05,2)

fs =

  1.0281e+003


cv =

    2.9966
</code></pre>

<p>where FS is the F-Statistic and cv is the critical value from the F-Distribution.</p>

<p>Does this show causality? Actually... what does it show?</p>
",2013-10-28 05:47:22.693
58344,23091.0,1,,,,Exponential regression: calculating p-value and F significance,<regression><anova><descriptive-statistics><nonlinear-regression><excel>,CC BY-SA 3.0,"<p>I have a set of independent data and dependent data $(X,Y)$, where I would like to do an exponential regression to obtain its p-value and significant $F$ (already obtained $R^2$ and also the coefficients through mathematical calculation).</p>

<p>Often an exponential data, $y=be^{mx}$ will be converted first to a linear data, $\ln y = mx + \ln b $. Then a linear regression will done on the converted data, obtaining its p-value etc. Assume we use a statistical tool such as Excel's Analysis ToolPak: Data Analysis : Regression, it will produce a result such as below,</p>

<p><img src=""https://i.stack.imgur.com/pCOPq.png"" alt=""enter image description here""></p>

<p>I believe the p-value and Significant $F$ value above is representing the converted linear data and not the original exponential data.</p>

<p>Questions:</p>

<ol>
<li><p>What is the approach/steps used by Excel to get the p-value and Significant F value for the converted linear data as shown in the statistic output in the image above? It is not clear in their help page or website.</p></li>
<li><p>Can the p-value and Significant F could be mathematically calculated for <strong>exponential regression</strong> without using a statistical tool? Can you assist to point me to the right link if this has been answered before.</p></li>
</ol>

<p>Spent a week on the internet to study this but unable to find the right answer.</p>
",2013-10-28 08:36:36.887
58345,21886.0,1,,,,Gaussian process estimation,<gaussian-process>,CC BY-SA 4.0,"<p>The stochastic process <span class=""math-container"">$(X_t)_{t\in T}$</span> is called Gaussian if for all <span class=""math-container"">$t_1,\dots,t_k\in T$</span>, for all <span class=""math-container"">$k$</span>, the joint distribution of <span class=""math-container"">$X_{t_1},\dots,X_{t_k}$</span> is multivariate normal. The process is completely characterized by its mean function <span class=""math-container"">$$\mu(t) = \mathbb{E}[X_t]$$</span> and its covariance function <span class=""math-container"">$$\sigma(s,t) = \operatorname{Cov}[X_s,X_t].$$</span></p>
<p>Given a centered (0-mean) Gaussian process, is it possible to estimate its covariance function?</p>
",2013-10-28 09:44:43.930
58346,20144.0,1,,,,"Using cross correlation to infer dependence, can it be done?",<cross-correlation><bernoulli-distribution>,CC BY-SA 3.0,"<p>I have a very particular question, I have seen a similar one <a href=""https://stats.stackexchange.com/questions/46597/dependent-bernoulli-trials"">here</a>, but my knowledge is too limited to make use of it. I will try to explain myself as clearly as possible... Wish me luck!</p>

<p>I have a sequence of (<em>a priori</em>) probabilities of a binary variable, we could say $X_i \sim Ber(p_i)$, i.e. $P(X_i = 1) = 1 - P(X_i = 0)=p_i$. I know that $X_i$ are not independent, but I don't know exactly $P(X_i|X_{j\neq i})$ nor $P(X_i|X_{j_1\neq i},...,X_{j_n\neq i})$ (I don't know anything about their dependence), and I want to know $$P(\sum_{j=-K}^{j=K} X_{i+j} =0)$$
What kind of knowledge/hypothesis do you think I need in order to approximate this more efficiently? Assuming that they are independent does not work well enough. What would you do if you found a similar situation? I am not an expert at all in this matters, and the more I learn about statistics the less I know! </p>

<p>For example, I can see that the probabilities form small ""triangle"" shapes, so maybe something like $P(X_i = 1 | X_{i-1} = 1, X_{i-2} = 0) = P(X_i = 0 | X_{i-1} = 0, X_{i-2} = 1) $ can help? If so, is there any way to use this? As I said, I have no idea...</p>

<p>Thank you very much for your help!</p>

<p>edit: I think the title is not very good, but I don't know how to explain it better... One more question, can I say $Y_i = X_i - X_{i-1}$ and try to see $P(X_i|Y_{i-1})$?</p>
",2013-10-28 10:00:02.867
58347,11772.0,2,,58345.0,,,,CC BY-SA 3.0,"<p>If the form of the kernel is known <em>(many real applications use an RBF kernel for example)</em>, it is possible, given a set of observations $(x_t, y_t)$ to estimate its hyperparameters <em>(the length-scale for RBF)</em> via <strong>maximisation of the marginal likelihood</strong>.</p>

<p>You should take a look on <a href=""http://www.gaussianprocess.org/gpml/chapters/RW5.pdf"" rel=""nofollow"">Chapter 5</a> of Gaussian Process for Machine Learning. You will find an example of MATLAB code in the <a href=""http://www.gaussianprocess.org/gpml/code/matlab/doc/"" rel=""nofollow"">gpml documentation</a>, in the paragraph of section ""Regression"" starting by ""Typically, we would not a priori know the values of the hyperparameters...""</p>
",2013-10-28 10:06:38.203
58349,503.0,5,,,,,,CC BY-SA 3.0,,2013-10-28 10:42:29.940
58348,503.0,4,,,,,,CC BY-SA 3.0,See `continuous-data`,2013-10-28 10:42:29.940
58350,20144.0,2,,58346.0,,,,CC BY-SA 3.0,"<p>O.K., I think I found a way of doing it with the correct assumptions. Although it is only useful for my particular problem, maybe somebody can tell me if I am being too ""sloppy"", correct me or maybe my approach might be useful to somebody in the future.</p>

<p>First of all, I had not realized that the ""triangles"" account for ""long  independent events"". This means that $P(X_i = 0 | \sum_{j\neq i, |j-i| &lt; D} X_j = 1) = 1$ in my notation ($X_i$ represents that an event starts in the moment $i$). What I started doing was a smoothing of $2D$ of $p_i$, so I took the average (can I do this?) in windows. This gave me a way of seeing how many events are there in a sequence.
<img src=""https://i.stack.imgur.com/OUOpw.png"" alt=""The sequence pi""></p>

<p>The orange line is the original $p_i$ sequence, the black one is the smoothed. After this, in a window of length $2K$, I count how many peaks are there (how many possible events), and the probability of each event is the sum of probabilities from the beginning until the end of the ""hill"", although, as can be seen in the picture, sometimes they can overlap, but I have no idea of how can I take that into account. Then, the probability of no event happening in the window of length $2K$ is the product of probabilities of no ""long event"" happening in this window.</p>

<p>Do you think it is a good answer? Do you have any comment/suggestion? Thank you very much.</p>
",2013-10-28 10:49:42.880
58351,503.0,2,,52126.0,,,,CC BY-SA 3.0,"<p>First, we'll need to know whether you are interested in the response to each Likert question or to a sum of Likert questions; if the latter, it matters how many questions and what the distribution of the scale looks like.</p>

<p>Either way, you will have to account for the nonindependence of the data, because the same people are answering the questions multiple times. Repeated measures ANOVA is one solution to this, but it makes unrealistic assumptions including sphericity, and would only be usable for the scale score, and only if the scores ranged fairly widely so that you could pretend they were continuous.</p>

<p>A better option is a mixed model. If you treat the scores as continuous data, then this would be a linear mixed model; if you treat them as ordinal (as you would have to do if you were interested in each question) then you would need a nonlinear mixed model.</p>

<p>Unfortunately, these models are not simple to implement. If you currently know only about t-tests, then you may need to hire a consultant to help. </p>
",2013-10-28 10:53:25.943
58352,6708.0,1,,,,Spearman's Rho - from partial ranked variables,<ranking><spearman-rho><ranks>,CC BY-SA 3.0,"<p>I have two variables which represent two performance measures.<br>
I ranked a finite set of elements according these two variables.<br>
Therefore, I have to ranks. Suppose the ranks are performed in descending order (the highest the measure the highest the value in the decision process of each element).</p>

<p>For instance, an ordered set $\Omega$ of $10$ elements according the two measures A and B.</p>

<pre><code>R#A = [5 3 1 9 2 10 6 7 4 8];  
R#B = [9 7 4 8 5 6 1 3 2 10];
</code></pre>

<p>Suppose that now I truncate R#A and R#B in order to select the ""top 5"" elements:</p>

<pre><code>R#A_5 = [5 3 1 9 2];
R#B_5 = [9 7 4 8 5];
</code></pre>

<p>In your opinion it is still possible to get the Spearman's rho correlation coefficient with these two partial orders?</p>

<p>I know that<br>
1) We are in the second step of the Spearman's because we are alrady dealing with ranks.<br>
2) The Sample size is very low but it is just for explanation. </p>
",2013-10-28 11:21:57.617
58353,23094.0,1,59809.0,,,(Standalone) Software for plotting graphs of large amounts of data and allowing you to scroll/zoom,<data-visualization>,CC BY-SA 3.0,"<p>I've got some temporal data taken from a data logger that I'm trying to plot in a graphical form (as a line graph). Because it's a large amount of data, plotting it one one big graph (e.g. in Excel) makes it difficult to explore the visualised data as you can't really zoom in and scroll through the data. What I'm looking for is some standalone software that can plot the data as a line graph, but also allow the user to easily scroll through the graph along the horizontal (time) axis and be able to zoom that axis in and out. Ideally, the software would be free and be GUI driven. Does anyone know of any such software?</p>

<p>Thanks,</p>
",2013-10-28 11:29:09.030
58354,23095.0,1,,,,How to mix probability estimators of the same phenomenon?,<probability><mixture-distribution>,CC BY-SA 3.0,"<p>Also posted <a href=""https://mathoverflow.net/questions/86936/minimum-distance-estimation-of-mixed-mixture-distributions"">here</a> and <a href=""https://math.stackexchange.com/questions/459747/how-to-mix-probability-estimators-of-the-same-phenomenon"">here</a>.</p>

<p>I have the following problem:</p>

<p>I have N models that give me an estimation of the probability distribution function p(x) of a certain phenomenon x. Let's call them: $p_1(x),...,p_N(x)$. They come from different sources of information, so they can give different values, but they all refere to the same observable fact. Is there a formal and valid way to combine them into a single formula?</p>

<p>I have read about mixture distributions, are they applicable to my case?</p>

<p>Suppose that my models can be more or less reliable, depending on the source of information they are based on, can I also combine them giving more weight to one model rather than another?</p>

<p>One possible solution I have thought is to make a weighted average of all the PDFs: $p(x) = w_1*p_1(x) + ... + w_N * p_N(x)$ where $\sum\limits_{i=0}^N w_i = 1$, does it make sense?</p>

<p>Thanks very much for your suggestions!</p>

<p>PS: to be more concrete my models give me the probability distribution that a certain person is in the position (x,y) and they rely on different sources of information like the power of a received signal or some other observable fact.</p>
",2013-10-28 11:59:45.803
58355,503.0,2,,58352.0,,,,CC BY-SA 3.0,"<p>I think you have one of two problems, depending on the what exactly R#A and R#B are. For example, does the <code>5</code> in R#A mean that the first element has a rank of 5, or does it mean that the fifth element has a rank of 1?</p>

<p>If it is the former, then you have not selected the top 5 elements. If it is the latter then R#A and R#B contain different elements.</p>

<p>You can certainly run a correlation on the two vectors; but what will the output mean?</p>

<p>Perhaps you can tell us what you are trying to accomplish. </p>
",2013-10-28 12:25:34.520
58356,23097.0,1,,,,How can to compare 1750 samples between 3 groups by R?,<r><bioinformatics>,CC BY-SA 3.0,"<p>I have 1750 proteins  that I want to compare the expression level of them between 3 groups (cell-type) using R. How can I do it?</p>
",2013-10-28 12:27:32.710
58357,22159.0,1,58358.0,,,What are the limitations of Kernel methods and when to use kernel methods?,<machine-learning><kernel-trick>,CC BY-SA 3.0,"<p>Kernel methods are very effective in many supervised classification tasks. So what are the limitations of kernel methods and when to use kernel methods? Especially in the large scale data era, what are the advances of kernel methods? What is the difference between kernel methods and multiple instance learning?
If the data is <code>500x10000</code>, <code>500</code> is the count of samples, and <code>10000</code> is the dimension of each feature, then in this circumstance, can we use the kernel methods?</p>
",2013-10-28 12:33:15.317
58358,17740.0,2,,58357.0,,,,CC BY-SA 3.0,"<p>Kernel methods can be used for supervised and unsupervised problems. Well-known examples are the <a href=""http://en.wikipedia.org/wiki/Support_vector_machine"" rel=""noreferrer"">support vector machine</a> and <a href=""https://www.google.be/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;cad=rja&amp;ved=0CGQQFjAE&amp;url=http://www.esat.kuleuven.be/sista/lssvmlab/iccha4.pdf&amp;ei=xF5uUqTOJOXd4QSRjYGYAw&amp;usg=AFQjCNGR9oQRvyL9KC2_Og4JT2APGxFxvw&amp;sig2=PsiFFW1EFi6T-g4R0OOzCw&amp;bvm=bv.55123115,d.bGE"" rel=""noreferrer"">kernel spectral clustering</a>, respectively.</p>

<p>Kernel methods provide a structured way to use a linear algorithm in a transformed feature space, for which the transformation is typically nonlinear (and to a higher dimensional space). The key advantage this so-called kernel trick brings is that nonlinear patterns can be found at a <em>reasonable</em> computational cost. </p>

<p>Note that I said the computational cost is reasonable, but not negligible. Kernel methods typically construct a kernel matrix $\mathbf{K} \in \mathbb{R}^{N\times N}$ with $N$ the number of training instances. The complexity of kernel methods is therefore a function of the number of training instances, rather than the number of input dimensions. Support vector machines, for example, have a training complexity between $O(N^2)$ and $O(N^3)$. For problems with very large $N$, this complexity is currently prohibitive. </p>

<p>This makes kernel methods very interesting from a computational perspective when the number of dimensions is large and the number of samples is relatively low (say, less than 1 million).</p>

<p>Related: <a href=""https://stats.stackexchange.com/questions/73032/linear-kernel-and-non-linear-kernel-for-support-vector-machine/73156#73156"">Linear kernel and non-linear kernel for support vector machine?</a></p>

<h2>SVM for Large Scale Problems</h2>

<p>For <em>very</em> high dimensional problems, such as the <code>10000</code> dimensions you mention in the question, there is often no need to map to a higher dimensional feature space. The input space is already good enough. For such problems, linear methods are <em>orders of magnitude</em> faster with almost the same predictive performance. Examples of these methods can be found in <a href=""http://www.csie.ntu.edu.tw/~cjlin/liblinear/"" rel=""noreferrer"">LIBLINEAR</a> or <a href=""http://hunch.net/~vw/"" rel=""noreferrer"">Vowpal Wabbit</a>. </p>

<p>Linear methods are particularly interesting when you have many samples in a high dimensional input space. When you have only $500$ samples, using a nonlinear kernel method will also be cheap (since $N$ is small). If you had, say, $5.000.000$ samples in $10.000$ dimensions, kernel methods would be infeasible.</p>

<p>For low-dimensional problems with many training instances (so-called large $N$ small $p$ problems), linear methods may yield poor predictive accuracy. For such problems, ensemble methods such as <a href=""http://esat.kuleuven.be/sista/ensemblesvm"" rel=""noreferrer"">EnsembleSVM</a> provide nonlinear decision boundaries at significantly reduced computational cost compared to standard SVM.</p>
",2013-10-28 12:57:31.210
58359,192.0,2,,58353.0,,,,CC BY-SA 3.0,"<p><a href=""http://www.gnuplot.info/index.html"" rel=""nofollow"">Gnuplot</a> is free, open source and highly versatile and what I use and I think it will meet your needs. You can point and click with the mouse to zoom in and out on any part of a graph, and you can even write a script to scroll through the data as if watching a film.</p>
",2013-10-28 13:08:05.197
58360,9554.0,2,,58354.0,,,,CC BY-SA 3.0,"<p>I think Bayesian model comparison might be what you are looking for. See for example <a href=""http://www.amazon.de/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1382965736&amp;sr=8-1&amp;keywords=bishop"" rel=""nofollow"">Bishop</a>, Chapter 3.4.
Generally speaking, given a set of $N$ models, you choose your weights to correspond to the posterior probability of each model.
$$ p(M_i | D) \propto p(M_i)p(D | M_i) $$
where $p(M_i)$ is the prior of model importance, you can assume this to be uniform, and $D$ is your data. Hence $p(D | M_i)$ is simply the likelihood of model $i$ given data.</p>

<p>The predictive probability for a new value $y^*$ and explanatory values $\mathbf{x}$ is then:
$$ p(y^* | \mathbf{x}, D) = \sum_{i=1}^N p(y^*|\mathbf{x},D,M_i)p(M_i | D)$$
where we use the posteriors as weighting between models.</p>

<p>That's the theory. Now in practice, unless you happen to be dealing with conjugated probabilities, you won't get a closed form solution for those posteriors and the above is pretty much useless.</p>

<p>When people fit a mixture of distributions, they usually use a mixture of Gaussians and in rare occasions a mixture of t distributions. I think the reason is simply that the mixture is fit using the EM algorithm (again, see Bishop) and Gaussians are particularly useful since their posterior is again a Gaussian and you can get all the required updates for the EM algorithm in closed form solution. And when I say ""fit"", they don't fit them individually, but learn the best parameters for all mixture components from the data, which is not what you are doing.</p>

<p>Don't worry about that for now and simply check whether you can get the posteriors for your model, or whether you don't want to fit your mixture using the EM and some well known distribution, such as the Gaussian, or t distribution in case of many outliers.</p>

<p>EDIT (to your comment):</p>

<p>So first of all, my notation: $y$ is the quantity you are trying to model, $x$ is a vector of data used to model $y$.</p>

<p>The data $D$ is just a tuple of vectors $x$, used to model a tuple of $y$'s. (basically think any normal dataset with a dependent variable and multiple independent ones)</p>

<p>OK. now, if I understand correctly you are trying to fit a complex distribution and you somehow already know the models that explain the data (perhaps because you know the generating mechanism) so you only need the mixing proportions.</p>

<p>You could try the following, which is guaranteed to work for Gaussians, and I don't see why it shouldn't work for other distributions too. (though there is a big caveat here!)</p>

<p>Calculate for each point x the likelihood that the point was generated by model $f1$ and $f2$, where the parameters are known. You end up with a matrix of likelihoods $2 x N$ where N is the number of your points. sum each row and divide by N. You should get the responsibility of each of the models for generating the data.</p>

<p>Use that to weight your mixture. You should also check if the resulting density integrates to 1, and if not normalize appropriately.</p>
",2013-10-28 13:20:37.797
58361,,2,,58356.0,anon,,,CC BY-SA 3.0,"<p>Bioconductor project produces software (add-ons for R) for bioinformatics. Bioconductor offers several solutions to your problem. Possibly the easiest one is to use the limma package (<a href=""http://www.bioconductor.org/packages/release/bioc/html/limma.html"" rel=""nofollow noreferrer"">http://www.bioconductor.org/packages/release/bioc/html/limma.html</a>). It has an extensive user guide that walks you through the basics of the analysis.</p>

<p>In addition, see the answer <a href=""https://stats.stackexchange.com/questions/66600/ebayes-lmfit"">eBayes() lmFit()</a> for a quick overview of the workflow and the functions in the limma package.</p>

<p>What you need to consider is the question you are interested in. These questions are coded in a design matrix. For example, if you have one control group (C), and two treatment groups (T1, T2), and you are interested in comparing both treatments with the control group, you might generate the following model matrix. But, before generating the model matrix, let's assume your data matrix (containing the expression values) contains the controls in the three first columns, then three T1 columns, and last three T2 columns. A vector (listing the groups in the same order they appear in the data matrix) containing the group information can be turned into a model matrix as follows:</p>

<pre><code># ""group"" vector
groups&lt;-c(""C"",""C"",""C"",""T1"",""T1"",""T1"",""T2"",""T2"",""T2"")
design&lt;-model.matrix(~groups)
design
  (Intercept) groupsT1 groupsT2
1           1        0        0
2           1        0        0
3           1        0        0
4           1        1        0
5           1        1        0
6           1        1        0
7           1        0        1
8           1        0        1
9           1        0        1
</code></pre>

<p>If you do not have a control group, but some comparison group anyhow, just specify it as the first group (alphabetically) in the groups vector. The model matrix will automatically use it as a baseline with which all others are compared.</p>

<p>This does a simple comparison of groups (T1 v. C and T2 v. C). If you have something more complex in mind, please elaborate your question a bit to address this.</p>
",2013-10-28 13:22:00.490
58433,23131.0,1,,,,Can we make the Irwin-Hall distribution more general?,<distributions><density-function><uniform-distribution><cumulative-distribution-function>,CC BY-SA 3.0,"<p>I need to find a symmetric low-kurtosis distribution class, which includes the uniform, the triangular and the normal Gaussian distribution. The Irwin-Hall distribution (sum of standard uniform) offers this characteristic, but is not treating non-integer orders $N$. However, if you e.g. simply independently sum up e.g. 2 standard uniform $[0,1]$ and one 3rd with a smaller range like $[0,0.25]$ you will indeed nicely obtain a more general and smoothly extended version of Irwin-Hall for any arbitrary order (like $N=2.25$ in this case). However, I wonder if it is possible to find a practical closed formula for the CDF?</p>
",2013-10-29 14:07:10.673
58362,21362.0,1,,,,Random forest â€œcertainty / likelihood scoreâ€ - how to score records in RF mode in R?,<r><random-forest>,CC BY-SA 3.0,"<p>My question is similar to this link <a href=""https://stats.stackexchange.com/questions/12425/creating-a-certainty-score-from-the-votes-in-random-forests"">Creating a &quot;certainty score&quot; from the votes in random forests?</a></p>

<p>I am trying to build a random forest for a binary response (1 &amp; 0). Let's say we have 10,000 different records and I am building 500 trees. Is there a way to score the records in terms of the certainty / confidence / likelihood of being categorized in category 1 (for example)? The link above suggests using the number of votes among all 500 trees, but this way can only give me up to 500 different scores, how can I differentiate further for these 10,000 records? (Like regression, the scores can be easily obtained). </p>

<p>One solution is to average the score of each tree in the forest. the tree is the probability of 1s in the final node. Anyone know how to produce that average in R? I couldnt find this in the randomForest package. I think if I write my own codes to do that it , the run time may not be as fast as a built-in function. </p>
",2013-10-28 14:10:32.017
58363,,2,,58328.0,user31668,,,CC BY-SA 3.0,"<p>Clarkson - a convenient method for this particular problem will be to recognise that the sum of poisson variables is also poisson. In this case, you would model the total number of visits in the year as poisson(30) and see what you can infer from there.</p>
",2013-10-28 14:12:15.860
58364,22262.0,1,,,,Should I de-mean a predictor variable before a dummy interaction,<multiple-regression><forecasting><multicollinearity><standardization><centering>,CC BY-SA 3.0,"<p>Suppose I have the following time-series linear model where $\beta$ is misspecified:</p>

<p>$Y(t+1) = \alpha + \beta X(t) + \sum_{i=1}^{10000}\gamma_i Z_i(T) + \varepsilon$</p>

<p>where all parameters are in $\mathbb{R}$ and all predictors are normally distributed and play nicely with respect to Gauss Markov.</p>

<p>In the population ('correct') model that corresponds to the above, the slope parameter on $X(t)$ is equal to $\beta_1$ when $X(t) &gt; \text{Q}_{0.95}(X(t))$ and is equal to $\beta_2$ when $X(t) &lt; \text{Q}_{0.95}(X(t))$, where $Q$ is the quantile function. This break in parameters is a priori knowledge. We also have that $\beta_1 \neq \beta_2$.</p>

<p>I want to model this as follows:</p>

<p>$Y(t+1) = \alpha + \beta_1 X(t)I(X(t) &gt; Q_{0.95}(X(t))) $</p>

<p>$+ \beta_2 X(t)I(X(t) &lt; Q_{0.95}(X(t)))$ </p>

<p>$+ \sum_{i=1}^{10000}\gamma_i Z_i(T)  + \varepsilon$</p>

<p>My <strong>question</strong> is whether it is good practice to de-mean $X(t)$ before estimating the equation with the indicator function breaks, supposing my objective is out of sample forecast accuracy? I ask because this has an effect on collinearity between predictors (which could impact some, but not all, feature selection algorithms). Note that feature selection and regularisation is done in an automated way.</p>

<p>I do not care about standard errors.</p>
",2013-10-28 14:16:33.520
58365,23103.0,1,,,,Linear Discriminant Function,<multivariate-analysis><discriminant-analysis>,CC BY-SA 3.0,"<p>In linear discriminant analysis, how is the linear discriminant function determined? Assuming equal variance-covariance matrices, is the linear discriminant function determined from the training data? </p>
",2013-10-28 14:55:58.223
58366,12884.0,2,,58365.0,,,,CC BY-SA 3.0,"<p>For <a href=""http://en.wikipedia.org/wiki/Linear_discriminant_analysis"" rel=""nofollow"">Linear discriminant analysis</a> the linear discriminant function is just the inner product of a given data point $\vec{x}$ with the vector $\vec{w}$, with the criterion $\vec{x} \cdot \vec{w} &gt; c$. The vector $\vec{w}$ is calculated as:</p>

<p>$$\vec{w} = \Sigma^{-1}(\vec{\mu_0} - \vec{\mu_1})$$</p>

<p>Where $\vec{\mu_n}$ is the vector mean of sample class $n$, that is, the mean of the training data for class $n$.</p>
",2013-10-28 15:03:29.057
58367,9749.0,2,,58322.0,,,,CC BY-SA 3.0,"<p>I am not sure why you used a t-test at all, descriptive statistics would suffice for your hypothesis and you could lump all 50 results together, as the difference between averages of 357 and 358 is experimentally irrelevant for your sample sizes, and in any case is piffling.</p>

<p>If, for example, you used <a href=""http://www.graphpad.com/quickcalcs/"" rel=""nofollow"">Graphpad free quickcalcs</a>, you would get a 95% confidence interval of 343 to 371, for a sample size of 50 and a conjectural and probably grossly excessive standard deviation of 50.  This shows that your ants eat significantly (though probably not substantially relevantly) more that 20 times their own weight on average.</p>

<p>If you really wanted to compare this using a t-test against an hypothetical average of 20 times their own weight, then you could use the one sample t-test that they provide, but I don't think that was really the point of your experiment.  To be sure that these tests are appropriate, you should also check that the distributions are normal, which considering the nature of your experiment, I would imagine that they almost certainly are.</p>

<p>In addition to the suggestions made by others, I would also want to be sure that the ants didn't leave any excreta on their uneaten foods.</p>
",2013-10-28 15:54:38.527
58368,21932.0,1,58378.0,,,Writing null hypothesis and deciding on rejection criteria,<hypothesis-testing><poisson-distribution>,CC BY-SA 3.0,"<p>The number of faults in one metre of a thread is Poisson distributed.It is claimed that the average number of faults is 0.02 per metre.A random sample of 100 one metre lengths of the thread reveals a total of 6 faults.Does this information support the claim?</p>

<p>What I want to know whether my H0 should be, H0:<strong>lambda=0.02</strong> or should it be <strong>lamda=2</strong></p>

<p>And my next question is in deciding the rejection criteria, Is it enough to check to reject H0,P[X>=6]<strong>&lt;0.025</strong> (Testing at 5% significance) or should I have looked for P[X&lt;=6]&lt;0.025 as well and checked whether either one of these is satisfied.</p>

<p>Or calculate <strong>2</strong>*P[X>=6] and check if it is less than 0.05</p>
",2013-10-28 15:54:41.290
58369,22865.0,1,,,,What is an 'atom' and what are 'atomic weights'?,<machine-learning><markov-chain-montecarlo><dirichlet-distribution>,CC BY-SA 3.0,"<p>I have come across the following statement:</p>

<blockquote>
  <p>A notable feature of the Hierarchical Dirichlet Process is that all Dirichlet Processes' $G_j$ share the same set of atoms and only the atom weights differ. This is a result of the almost sure discreteness of the top-level DP.</p>
</blockquote>

<p>What is meant by atom and atom weights? Googling gets to me Wiki articles about measure theory and I'm not able to understand them and also not sure if they are what I'm looking for.</p>

<p>Could anyone provide a simple explanation of their meaning?</p>
",2013-10-28 15:58:37.740
58370,23108.0,2,,54724.0,,,,CC BY-SA 3.0,"<p>Well, for the second part, I think more more flexible model will try to fit the model hard and training data contains a high noise, so flexible model will also try to learn that noise and will result in more test error. I know the source of this question as I'm also reading the same book :)</p>
",2013-10-28 16:08:53.440
58371,13165.0,1,58374.0,,,"Is ""model selection"" the same as traning?",<machine-learning><model-selection><terminology><train>,CC BY-SA 3.0,"<p>A terminology problem. In machine learning we have the following problem: </p>

<p>Choosing the optimal model (or training): 
$$
f^* = \arg\min_{f \in \mathcal{F}} \sum_i l(x_i,y_i)
$$</p>

<p>Is the term ""model selection"" always ""exactly"" referring to this? Or something else? </p>
",2013-10-28 17:06:49.973
58372,13165.0,1,61894.0,,,"How to train in models, with efficient inferences, like belief-propagation ?",<machine-learning><inference><graphical-model>,CC BY-SA 3.0,"<p>There are many papers that are devoted to efficient inference in graphical models. Though many of these paper don't explicitly talk about the learning (training, etc) problem. For example: </p>

<p><a href=""http://videolectures.net/mlss09uk_minka_ai/"" rel=""nofollow"">http://videolectures.net/mlss09uk_minka_ai/</a></p>

<p>I am a little confused on how these models being trained. I thought they are probably doing an EM-like algorithm, i.e. </p>

<ol>
<li><p>Inference  (and calculating all marginals, using VB or EP)</p></li>
<li><p>Maximizing the likelihood using some blackbox optimization toolboxes using the marginals in the previous case</p></li>
</ol>

<p>For example, consider different variants of Belief-Propagation. There are HUGE number of variants for BP, but how a graphical model could be trained? </p>

<p>Any comments? </p>
",2013-10-28 17:12:01.877
58373,9129.0,1,58617.0,,,How to evaluate/validate clusters using multiple clustering methods,<clustering>,CC BY-SA 3.0,"<p>From some reading I did online, I understand that there are various methods for determining ""similarity"" used by different clustering algorithms. I am curious if it is good practice to run multiple clustering algorithms/methods (i.e Hiearchical w/ Ward, single linkage, centroid, etc or maybe even K-means) on a dataset and if there is some automated way to to get a ""consensus"" of clusters. In other words to get some sense of confidence that the right items are clustered together. Items that tend to cluster together using various methods would be considered valid. For example in my example below G and Z tend to cluster together using multiple methods as do S and F. </p>

<p>Label = what I am clustering; X &amp; Y are my variables I use to cluster; Cluster1-3 are the results of three clustering algorithms.</p>

<p><img src=""https://i.stack.imgur.com/7m55l.png"" alt=""enter image description here""></p>

<p><em>Edit: I removed a side note I had here regarding how large the actual data set I plan to use might be so as not to detract from the main questions.</em></p>
",2013-10-28 17:28:21.230
58374,10450.0,2,,58371.0,,,,CC BY-SA 3.0,"<p>The best model is not necessarily the model which minimizes error, but typically attempts to reduce overfitting bias by <a href=""http://en.wikipedia.org/wiki/Regularization_%28mathematics%29"" rel=""nofollow"">adding penalties for cost complexity</a> and  by cross-validating between training and validation samples.</p>

<p><a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0CEAQFjAC&amp;url=http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/model-selection-11.pdf&amp;ei=AJ9uUoCzJM_figLYp4CADQ&amp;usg=AFQjCNG37gEoA8QlmK1hPAkKhJrGWKDkeA&amp;bvm=bv.55123115,d.cGE"" rel=""nofollow"">web.engr.oregonstate.edu/cs534 slides</a></p>
",2013-10-28 17:29:05.487
58375,3048.0,1,58918.0,,,"In weighted least squares, how do I weight the residuals to get an accurate ""z score""",<regression>,CC BY-SA 3.0,"<p>I am regressing spreads in yield curves in certain countries, in the chart below, the Spanish 2-5-10 spread, against the Italian 2-5-10 spread. 
<img src=""https://i.stack.imgur.com/mteWO.png"" alt=""enter image description here""></p>

<p>I want recent data to count more, so I weight the inputs using a decay weighting scheme with a 1 year halflife. </p>

<p>A ""simple"" regression line and the weighted regression line are shown. </p>

<p>I want to calculate the perpendicular distance of the current point (green) from the regression line, in number of standard errors. In the unweighted regression, in R, I will simply say:</p>

<pre><code>&gt; l &lt;- lm(SP ~ IT, data = ss)
&gt; last(l$residuals) / sd(l$residuals)
2013-10-28 
-0.1817122 
</code></pre>

<p>Which gives -.18 standard errors away from the regression line. </p>

<p>How do I do this same analysis for the weighted regression though? I am sure the following is incorrect:</p>

<pre><code>&gt; decay
function(len, halflife, sumone = TRUE) {
#function generates an exponentially decaying series
    t &lt;- len:1 # generate a series of numbers reverse order so biggest weights last
    lambda &lt;- log(2) / halflife #figure out the lambda for the halflife
    w &lt;- exp(-lambda * t) #create the weights series  
    if(sumone) w &lt;- w / sum(w) #normalise sum to 1 if necessary
    return(w) 
}
&gt; d &lt;- decay(nrow(ss), 260)
&gt; ld &lt;- lm(SP ~ IT, data = ss, weights = d)
&gt; last(ld$residuals) / sd(ld$residuals)
2013-10-28 
-0.3667876 
</code></pre>

<p>I should surely weight the residuals somehow, before doing the above, is that correct? Could I for example take the weighted standard deviation of the residuals that is:</p>

<pre><code>&gt; last(ld$residuals) / wt.sd(ld$residuals, d)
2013-10-28 
  -0.39717 
</code></pre>

<p>where my wt.sd function looks like this:</p>

<pre><code>&gt; wt.sd
function (x, wt) {
    return(sqrt(wt.var(x, wt)))
}

&gt; wt.var
function (x, wt) {
    s = which(is.finite(x + wt))
    wt = wt[s]
    x = x[s]
    xbar = wt.mean(x, wt)
    return(sum(wt * (x - xbar)^2) * (sum(wt)/(sum(wt)^2 - sum(wt^2))))
}
</code></pre>

<p>Basically, I want to know how to find the distance from the weighted regression line, in standard errors, accounting for the weights. </p>
",2013-10-28 18:16:36.610
58376,23110.0,1,,,,What is a reasonable process to understand a collection of data?,<unsupervised-learning>,CC BY-SA 3.0,"<p>Can someone provide their thoughts on a structured process one might go through to understand a collection of data.
The scenario is: you've been given a set of data (features and observations - with descriptions) and been told to ""tell me what kind of interesting things this data can tell me"".
I.e., what are interesting questions that this data can answer. The meaning of ""interesting"" is certainly subjective.</p>

<p>This appears to be classical unsupervised learning. </p>

<p><strong>My initial thoughts:</strong></p>

<ol>
<li>Cluster all pairs of variables to see interesting clusters  </li>
<li>Run PCA find high-variance groupings</li>
</ol>

<p>Is there a general ""how to understand a set of data"" process that you've found successful?</p>

<p>Thanks</p>
",2013-10-28 18:36:54.563
58377,21762.0,2,,58371.0,,,,CC BY-SA 3.0,"<p>Training often involves model selection (choice of model structure, set of input variables, transformations, ...). But, as @MarcClaesen pointed out, training also includes the process of fitting the model, i.e. finding best values for its parameters.</p>
",2013-10-28 18:38:04.773
58378,21029.0,2,,58368.0,,,,CC BY-SA 3.0,"<p>For question one, I assume you want to test if the claim in the question is true or not. The claim is that there are 0.02 faults per meter. In other words, the expected value is 0.02. However, the observed value is 6 faults in 100 meters, or 0.06 faults/meter. So,</p>

<ol>
<li>$H_0 : \lambda = 0.02 $ </li>
<li>$H_1 : \lambda \gt 0.02 $</li>
</ol>

<p>You can also write the alternative as $\lambda \ne 0.02$. </p>

<p>In the second question, the rejection criteria depends on what level of $ \alpha $ you choose. It also depends on which of the two alternative hypotheses you chose. Since the observed value is greater, it is natural to chose greater than, not equal to. Assume a 5% type-1 error. Then, the sum of iid poissons variables is a new poisson random variable. You want to test </p>

<p>$ P\{ X_1 + X_2 + \dots + X_{100} \ge 6 | \lambda' = 2 \} 
    = 1 - P\{X_1+\dots \le 6 | \lambda'=2  \}$</p>

<p>You are still testing if $ \lambda = 0.02 $, but indirectly by calculating if the new poisson variable $ \lambda' = 100\lambda $. Since this will be a one-tailed distribution, you still want P[X>=5] if it is less than 5%, not 2*P, which corresponds to the two-tailed test</p>

<p>Hope this helps.</p>
",2013-10-28 18:45:04.827
58434,306.0,2,,58432.0,,,,CC BY-SA 3.0,"<p>The degrees of freedom is n-2. this lets you test for whether rho (the correlation coefficient) is 0 or not. so calculate this value, find the p value using the degrees of freedom and decide if the null hypothesis of correlation coefficient being 0 is significant or not.</p>

<p>The autocorrelation of the variables do not have any effect on the correlation coefficient of the variables unless you want to check out if the lag of one variable has any affect on the other which the question does not indicate. So leave the autocorrelation part.</p>
",2013-10-29 14:21:26.503
58379,16992.0,2,,57691.0,,,,CC BY-SA 3.0,"<p>Here are a couple thoughts that may be helpful:</p>

<ul>
<li><p>Auto-correlation doesn't matter when you only look at a single t at a time. So, at a fixed time t, you could just run a t-test to check for a difference in means. If you run the t-test for each time separately, then you get a bunch of p-values. Because of auto-correlation these p-values are not independent, but each p-value considered alone is just fine.</p></li>
<li><p>So now you want to find the times for which there is a difference in means. I would try using false discovery rate (FDR) methods (see the ""Benjamini-Hochberg procedure"" at <a href=""http://en.wikipedia.org/wiki/False_discovery_rate"" rel=""nofollow"">http://en.wikipedia.org/wiki/False_discovery_rate</a>). Luckily, this procedure controls the FDR even when there is positive dependence among your p-values. (see ""The Control of the False Discovery Rate in Multiple Testing under Dependency"", free version here <a href=""http://thom.jouve.free.fr/work/thesis/sitecopy_save/Biblio/ToCheck/fdr/Benjamini2001.pdf"" rel=""nofollow"">http://thom.jouve.free.fr/work/thesis/sitecopy_save/Biblio/ToCheck/fdr/Benjamini2001.pdf</a>) This should give you a reasonable first answer to your original question.</p></li>
<li><p>Finally, I think the two plots you drew are very clear. They are probably more informative than any kind of statistical analysis you can run... Good luck!</p></li>
</ul>

<p><strong>Edit by Roland:</strong></p>

<p>Here is an R implementation of the FDR method for the example in the question. The result looks reasonable.</p>

<pre><code>dat &lt;- setNames(cbind(stack(as.data.frame(t(a))), 
                      stack(as.data.frame(t(b)))), 
                c(""a"", ""i"", ""b"", ""i""))
dat &lt;- dat[,-4]
library(plyr)
p.raw &lt;- ddply(dat, .(i), function(df) t.test(df$a, df$b)$p.value)
p.fdr &lt;- cbind(p.adjust(p.raw[,2], method=""fdr""),
               t[as.numeric(gsub(""V"","""",p.raw[,1]))])
p.fdr[order(p.fdr[,2]),]

#             [,1] [,2]
#  [1,] 0.63001435    3
#  [2,] 0.19439226    4
#  [3,] 0.06200315    5
#  [4,] 0.07335654    6
#  [5,] 0.05336699    7
#  [6,] 0.06115999    8
#  [7,] 0.06115999    9
#  [8,] 0.06103370   10
#  [9,] 0.04324050   11
# [10,] 0.04324050   12
# [11,] 0.04324050   13
# [12,] 0.04324050   14
# [13,] 0.06103370   15
# [14,] 0.05533972   16
# [15,] 0.15489402   17
# [16,] 0.58234624   18
# [17,] 0.05533972   19
# [18,] 0.04324050   20
</code></pre>
",2013-10-28 19:10:49.717
58380,21029.0,2,,58302.0,,,,CC BY-SA 3.0,"<p>The answer to your question should logicaly be ""Yes."" The group means of the PC should differ from the PC of the means. This should happen for two reasons. </p>

<ol>
<li>You're transforming your variables into PCs, which try to maximize the total interia. This depends on the spread of the data in the different variables.</li>
<li>Once you take the means you eliminate most of the inertia. The PCA will be much more ""accurate"" (not surprisingly almost 100% of the intertia is explained by the first component), but the composition of the PC will be different because there is no longer much intertia to explain. </li>
</ol>

<p>You can think of this in terms of how a PCA operates. The PCA is computed using squared distances, maximized for the first PCA. If you remove all this variability, the estimations change.</p>
",2013-10-28 20:02:34.957
58381,,2,,58376.0,user31668,,,CC BY-SA 3.0,"<p>John Tukey came up with an entire field devoted to this: <a href=""http://www.itl.nist.gov/div898/handbook/eda/eda.htm"" rel=""nofollow"">Exploratory Data Analysis</a> PCA is one part of this. Take a look and I'm sure youll find some good ideas.</p>
",2013-10-28 20:13:15.203
58382,18372.0,1,58383.0,,,Large scale ridge regression,<large-data><ridge-regression>,CC BY-SA 3.0,"<p>I'm trying to solve a problem of the form</p>

<p>$\min_x \frac{1}{2}||Ax-b||^2_2 + \frac{\rho}{2}||x-z||^2_F$</p>

<p>where both $x$ and $b$ are high dimensional, and $b$ is much higher dimensional than $x$. The solution is given by $x^* = (A^T A+\rho I)^{-1}(A^T b + z)$, but the problem is so large that even inverting $A^T A + \rho I$ is infeasible. However, due to structure in the problem we can efficiently multiply by $A$ and $A^T$. Basically this is large scale linear ridge regression. What would be the ideal algorithm for efficiently implementing this minimization? Would something like biconjugate gradient work?</p>
",2013-10-28 20:23:57.770
58383,9245.0,2,,58382.0,,,,CC BY-SA 3.0,"<p>I've found that LSQR is ideal for problems like this - I've used it successfully for operators of about 3e5 * 1e6 or so.  Check <a href=""http://www.stanford.edu/group/SOL/software/lsqr.html"" rel=""noreferrer"">http://www.stanford.edu/group/SOL/software/lsqr.html</a> for details.  I've used Friedlander's (I think) C port and the python port, which I have (hastily and sloppily) ported to R. </p>
",2013-10-28 20:29:10.780
58384,15782.0,1,,,,Cointegration but no Granger-Causality found,<cointegration><granger-causality>,CC BY-SA 3.0,"<p>I have found Cointegration based on Engle/ Granger and Johansen. However, Granger-causality is rejected for both variables. How is that possible?</p>

<p>According to theory, 
if x and y are I(1) and cointegrated, x is Granger causal to y and/or y is Granger causal to x. 
However, Granger-causality has been rejected in my bivariate case, despite their cointegration relationship. </p>

<p>Did I understand it correctly that there has to be at least one granger causality flow in a bivariate cointegrated system?</p>

<p>Thank you for your answer!</p>

<p>Applying an VECM, I get the following results: with only the -0.022460 being significant...</p>

<pre><code>    Vector Error Correction Estimates   
    Date: 10/28/13   Time: 23:58    
    Included observations: 1113 after adjustments   
    Standard errors in ( ) &amp; t-statistics in [ ]    

    Cointegrating Eq:   CointEq1

    CAD(-1)             1.000000

    NATGAS(-1)          0.067366
                       (0.02646)
                       [ 2.54615]

     C                  -0.077093


Error Correction:   D(CAD)  D(NATGAS)

CointEq1        -0.022460   -0.006601
 (0.00514)   (0.01384)
[-4.37213]  [-0.47714]

D(CAD(-1))  -0.054710    0.029241
  (0.02998)  (0.08073)
[-1.82508]  [ 0.36220]

D(CAD(-2))   0.035656    0.101838
 (0.02996)   (0.08070)
[ 1.18998]  [ 1.26200]

D(NATGAS(-1))   -0.004642   -0.077700
 (0.01120)   (0.03016)
[-0.41449]  [-2.57591]

D(NATGAS(-2))    0.004712    0.056858
 (0.01120)   (0.03016)
[ 0.42067]  [ 1.88491]

C    0.000176   -0.000850
 (0.00019)   (0.00051)
[ 0.92332]  [-1.65571]

 R-squared   0.022437    0.011948
</code></pre>
",2013-10-28 20:33:47.817
58385,2915.0,1,,,,Is there anything special about Gamma distribution with the shape parameter k=e?,<simulation><fitting><negative-binomial-distribution><gamma-distribution>,CC BY-SA 3.0,"<p>Is there any unique property of $\mathrm{Gamma}(k=e, \text{ scale})$ or a Negative binomial distribution with $r=e$? Here, $e$ is Euler's number, $e \approx 2.71828$.</p>

<p>The reason I'm asking is that one of the variables in my computer simulations can be fitted by $\mathrm{Gamma}(k=e, \text{ scale})$ or by $\mathrm{NB}(r=e,\ p)$ very robustly. That makes me wonder if there is something special about this process and this particular value of the shape parameter might hint to that.</p>
",2013-10-28 20:58:19.477
58386,14360.0,1,,,,Why does the log likelihood need to go to minus infinity when the parameter approaches the boundary of the parameter space?,<maximum-likelihood>,CC BY-SA 3.0,"<p>In a recent lecture I was told that, in order for the maximum likelihood estimate to be valid, the log likelihood needs to go to minus infinity as the parameter goes to the boundary of the parameter space. But I don't understand why this is essential. Suppose the log likelihood goes to some kind of asymptote. Then the parameter that maximizes the likelihood is still the maximum likelihood estimate, right?</p>
",2013-10-28 21:05:46.417
58387,23111.0,2,,54506.0,,,,CC BY-SA 3.0,"<p>Yes, logistic regression would work, but also classification trees. I don't think you need to worry about false positives.  It seems that the ""confusion matrix"" the model produces will tell you what you are looking for in terms of false positives and false negatives</p>
",2013-10-28 21:18:20.893
58388,15293.0,2,,53384.0,,,,CC BY-SA 3.0,"<p>Here is the general (semi-parametric-bootstrap) algorithm in more detail:</p>

<p>$\text{B}$ = number of bootstraps</p>

<p>the model:<br>
$y = x\beta + \epsilon$</p>

<p>let $\hat{\epsilon}$ be the residuals</p>

<ol>
<li>Run the regression and obtain the estimator(s) $\hat\beta$ and residuals $\hat\epsilon$.</li>
<li>Resample the residuals with replacement and obtain the bootstrapped residual vector $\hat\epsilon_\text{B}$.</li>
<li>Obtain the bootstrapped dependent variable by multiplying the estimator(s) from (1) with the original regressors and adding the bootstrapped residual: $y_\text{B} = x\hat\beta + \hat\epsilon_\text{B}$.</li>
<li>Run the regression with the bootstrapped dependent variables and the original regressors, this gives the bootstrapped estimator, i.e. regress $y_B$ on $x$, this gives $\hat\beta_\text{B}$.</li>
<li>Repeat the <em>procedure</em> $\text{B}$-times by going back to (2).</li>
</ol>
",2013-10-28 21:19:21.240
58389,23104.0,2,,30957.0,,,,CC BY-SA 3.0,"<p>On a related note, you can accomplish the same objective if your ARIMA model has external regressors. This has been helpful for me on occasion.</p>

<p>For instance, say your first model was created as follows:</p>

<pre><code>fit.arimax &lt;- Arima(response, order=c(1, 0, 1), xreg=xreg)
</code></pre>

<p>Then suppose that after creating your model, you observe additional values in your response and external regression variables, and would like to forecast or simulate future outcomes given these new observations. E.g., say you are predicting electricity demand, and you observe another hour of demand (i.e. response) and temperature (i.e. external regression) data.</p>

<p>Then, you may fit the original model to the updated time series as follows, where response.new and xreg.new are your updated response and regression variables.</p>

<pre><code>fit.arimax.new &lt;- Arima(response.new, model=fit.arimax, xreg=xreg.new)
</code></pre>

<p>You can use this new model to forecast or simulate future outcomes, conditional on all observed data. Note that you must provide forecast external regressors for each. E.g.,</p>

<pre><code>forecast.Arima(fit.arimax.new, h=length(xreg.forecast), xreg=xreg.forecast)

simulate.Arima(fit.arimax.new, n=length(xreg.forecast), xreg=xreg.forecast)
</code></pre>

<p>Another way to accomplish all of this is to make an entirely new model using the updated data. But the method described above is appropriate in real-time applications, in which case fitting a new ARIMA model would take too long.</p>
",2013-10-28 21:28:33.573
58390,9554.0,2,,58386.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>in order for the maximum likelihood estimate to be valid, the log likelihood needs to go to minus infinity as the parameter goes to the boundary</p>
</blockquote>

<p>This is equal to saying, the Likelihood of a parameter needs to become 0 at the boundary of the parameter space in order for the result to be valid.</p>

<p>Well first of all, you can restrict the parameter space to values that all have a positive likelihood and still obtain a valid estimate.</p>

<p>Secondly, even if you use, say $(-\infty,\infty)$, you don't come close to the boundary since any off the shelf optimisation package performs some sort of random initialisation and then approaches the minimum using some method such as gradient descent, conjugate gradient or another. In either case, you almost never end up approaching the boundary of the parameter space, so I don't quite understand why the boundaries matter in the first place.</p>

<p>And even if you do that on purpose, at one point you will hit the floating point precision of your operating system. I can guarantee you that at that point, you haven't really approached the boundary $-\infty$ by much. :)</p>

<p>Personally I find the underflow problem arising when calculating sums and products of very small likelihoods and the <a href=""http://jblevins.org/log/log-sum-exp"" rel=""nofollow"">log sum exp trick</a> much more interesting and more noteworthy issue that actually matters a lot in practice, unlike reaching the boundaries of the parameter space.</p>
",2013-10-28 21:47:00.457
58391,23112.0,1,,,,How to detect step changes in GPS time-series data?,<time-series><change-point>,CC BY-SA 3.0,"<p>The graph below shows GPS heading data (sampled every second) and I am trying to find the best way to detect (right/left) turns in the data. Appreciate suggestions for algorithms/methods for it (perhaps step detection)?  </p>

<p><img src=""https://i.stack.imgur.com/nFMPt.png"" alt=""GPS heading sampled 1 sec apart""></p>

<p>While turns typically result in sharp change in heading value, heading might also change gradually due to road curvature which should not be detected.</p>
",2013-10-28 22:02:30.007
58392,12358.0,2,,58252.0,,,,CC BY-SA 3.0,"<p>I suspect that it is just that the authors/editors got confused because the physics problem is usually described in terms of energies, which map to (play the a role analogous to) <em>negative</em> log-likelihoods.  Note that in the final sentence they refer to ""low-energy"".</p>

<p>In the physics literature the Ising model is cannonically defined as
$$
H(Y)=- \sum_{&lt;ij&gt;} J_{ij} y_i y_j 
$$
the sum is over all pairs of interacting sites, $Y$ is my notation for the entire state of the sites (i.e. a given $Y$ specifies $y_i$ for all $i$).  Note the minus sign.  One might be inclined to write $H(Y)=\vec{y}^T W \vec{y}$</p>

<p>The partition function is given by $Z=\sum_{Y} e^{-\beta H(Y)}$; yet another minus sign.</p>

<p>The probability that you'll see the physical system in the state $Y$  is given by
$$ 
P(Y) = \frac{ e^{-\beta H(Y)}}{Z} = \frac{ e^{\beta \sum_{&lt;ij&gt;} J_{ij} y_i y_j} } {Z} = \frac{e^{-\beta  \vec{y}^T W \vec{y}}}{Z}
$$</p>

<p>I left $\beta=1/k_BT$ in these expressions since these are the expressions typically written by physicists, but in this problem, one can just absorb it into the definition of the $J_{ij}$ (or equivalently set $\beta=1$).</p>

<p>The main point is that when dealing with statistical mechanics physicists are use to dealing with ""energies"" which have the property that lower (more negative) energy states are more likely to be occupied.  This is in the opposite sense from log-likelihoods (higher log-likelihoods are associated with more probable outcomes), and can lead to confusion when one tries to switch back and forth between these two conceptions.</p>
",2013-10-28 22:22:22.797
58393,14110.0,1,,,,"How to estimate a pdf of x under the model of y = x+n, when the pdf of y and the pdf of n are given",<distributions><normal-distribution><estimation>,CC BY-SA 3.0,"<p>I guess I come up with a classic question, but I failed to find any useful solutions by far. My question is about the following model 
$$y=x+n$$
where $x$ is a hidden random variable that cannot be observed, $n$ is the random variable of white noise, namely $n$ follows a known Gaussian distribution of variance $\sigma^2$ ($f_n= {\cal{N}}(0,\sigma^2)$), and $y$ is a random variable that we can observe. </p>

<p>Suppose we know the distribution of $y$ as $f_y$, and I wonder how to find the pdf of $x$. Theoretically, it seems to be equivalent of the sum of two dependent random variables i.e. $x = y-n$, but in this question how $y$ and $n$ are correlated is unknown. Is there any existing solution?</p>

<p>Thanks</p>
",2013-10-28 22:49:35.873
58543,1506.0,2,,58542.0,,,,CC BY-SA 3.0,"<p>It is certainly possible and does happen quite frequently, especially if there are many pairwise comparisons (which is likely the case if you're investigating an interaction term).  </p>

<p>The Tukey procedure controls the Type I error rate and requires a larger difference to declare significance compared to if no adjustment was used.  The ANOVA F-test uses MSE in the denominator which borrows information from all the data and is not affected by this adjustment.</p>
",2013-10-30 20:52:06.820
58394,594.0,2,,53404.0,,,,CC BY-SA 3.0,"<p>F tests are most commonly used for two purposes:</p>

<ol>
<li><p>in ANOVA, for testing equality of means (and various similar analyses); and</p></li>
<li><p>in testing equality of variances</p></li>
</ol>

<p>Let's consider each in turn:</p>

<p>1) F tests in ANOVA (and similarly, the usual kinds of chi-square tests for count data) are constructed so that the more the data are consistent with the alternative hypothesis, the larger the test statistic tends to be, while arrangements of sample data that looks most consistent with the null corresponds to the smallest values of the test statistic.</p>

<p>Consider three samples (of size 10, with equal sample variance), and arrange them to have equal sample means, and then move their means around in different patterns. As the variation in the sample means increases from zero, the F statistic becomes larger:</p>

<p><img src=""https://i.stack.imgur.com/0SI5a.png"" alt=""Arrangements of 3 samples and corresponding F statistic""></p>

<p>The black lines ($^{\:_|}$) are the data values. The heavy red lines ($\color{red}{\mathbf{|}}$) are the group means.</p>

<p>If the null hypothesis (equality of population means) were true, you'd expect some variation in sample means, and would typically expect to see F ratios roughly around 1. Smaller F statistics result from samples that are closer together than you'd typically expect ... so you aren't going to conclude the population means differ.</p>

<p>That is, for ANOVA, you'll reject the hypothesis of equality of means when you get unusually large F-values and you won't reject the hypothesis of equality of means when you get unusually small values (it may indicate <em>something</em>, but not that the population means differ).</p>

<p>Here's an illustration that might help you see that we only want to reject when F is in its upper tail:</p>

<h2><img src=""https://i.stack.imgur.com/NcUQN.png"" alt=""Description of F for ANOVA, reject when F-statistic is in upper tail""></h2>

<p>2) F tests for equality of variance* (based on variance ratios). Here, the ratio of two sample variance estimates will be large if the numerator sample variance is much larger than the variance in the denominator, and the ratio will be small if the denominator sample variance is much larger than variance in the numerator. </p>

<p>That is, for testing whether the ratio of population variances differs from 1, you'll want to reject the null for both large and small values of F.</p>

<p>* (Leaving aside the issue of the high sensitivity to the distributional assumption of this test (there are better alternatives) and also the issue that if you're interested in suitability of ANOVA equal-variance assumptions, your best strategy probably isn't a formal test.)</p>
",2013-10-28 23:09:43.140
58395,18690.0,1,58400.0,,,Stationarity in OLS time series and asymptotic properties,<stationarity><asymptotics>,CC BY-SA 3.0,"<p>I think I lack somewhat deeper understanding of this topic, but I thought stationarity is required in order for OLS to have asymptotic properties.</p>

<p>""But stationarity is not at all critical for OLS to have its standard asymptotic properties""(<a href=""http://rads.stackoverflow.com/amzn/click/1111531048"" rel=""noreferrer"">Wooldridge, 2012</a>)</p>

<p>I thought stationarity is needed or otherwise OLS would not be consistent, but I guess I'm wrong. Could someone tell me why stationarity is not critical for LLN?</p>

<p>Thanks in advance!</p>
",2013-10-28 23:09:59.747
58396,19120.0,2,,58372.0,,,,CC BY-SA 3.0,"<p>Training is done by EM, repeating the E-step and M-step until convergence.</p>

<ol>
<li>E-step: calculate sufficient statistics using the posterior over the hidden variables given the observed variables.</li>
<li>M-step: update the parameters using the sufficient statistics computed in the E-step</li>
</ol>

<p>For example, see this paper on how it proves learning parameters of HMM is also a belief propagation.
<a href=""http://homepages.inf.ed.ac.uk/csutton/notes/sutton04fbbp.pdf"" rel=""nofollow"">http://homepages.inf.ed.ac.uk/csutton/notes/sutton04fbbp.pdf</a></p>
",2013-10-28 23:16:22.427
58397,3894.0,2,,58372.0,,,,CC BY-SA 3.0,"<p>I'm a little confused by your question. Tom Minka's tutorial you are referring to is completely devoted to inference in graphical models. Learning the parameters of a graphical model is an inference problem, and, therefore, methods explained in the tutorial such as expectation propagation or variational inference can be applied to it. In fact, all of the examples in the tutorial show how to learn parameters in various graphical models.</p>

<p>EM algorithm can be applied to this problem as well. It should be, however, noted the the EM algorithm delivers point estimates for the parameters of interest, and, therefore, can be inferior to the approximate Bayesian inference methods discussed in the tutorial, since these methods aim to capture all the uncertainty present in the posterior distribution over parameters.</p>
",2013-10-28 23:27:37.970
58398,,1,,,user30490,How to plot spectra of an AR(2) process,<time-series><self-study><autoregressive>,CC BY-SA 3.0,"<p>I am stuggling with this problem and was hoping to find some guidance to answer it. </p>

<p>Let $y_t=\phi_1y_{t-1}+\phi_2y_{t-2}+\epsilon_t$, with $\epsilon_t\sim N(0,1)$.  Now, I want to plot the spectra of $y_t$ in the following cases:</p>

<p>Case 1: When the AR(2) characteristic polynomial has two real reciprocal roots given by $r_1=0.9$ and $r_2=-0.95.$</p>

<p>Case 2: When the AR(2) characteristic polynomial has a pair of complex reciprocal roots with modulus $r=0.95$ and frequency $2\pi/8$.</p>

<p>Now, before plotting the spectra of $y_t$ in the following cases, I have tried to make use of the following important facts.  The AR(2) process $y_t=\phi_1y_{t-1}+\phi_2y_{t-2}+\epsilon_t$ has the general linear process form $\psi(u)=1/(1-\phi_1u-\phi_2u^2)$ and hence
$$f(\omega)=\frac{v}{2\pi}|(1-\phi_1e^{-i\omega}-\phi_2e^{-2i\omega})|^{-2}$$
This can be expanded to give
$$f(\omega)=\frac{v}{2\pi[1+\phi^2_1+2\phi_2+\phi_2^2+2(\phi_1\phi_2-\phi_1)\cos(\omega)-4\phi_2\cos^2(\omega)]}$$
Now if the roots are real, then $f(\omega)$ has a mode at either zero or $\pi$; otherwise, the roots are complex conjugates and $f(\omega)$ is unimodal at $\omega=\arccos[-\phi_1(1-\phi_2)/4\phi_2]$ lying strictly between zero and $\pi$.</p>

<p>So if anyone could help explain to me how I am supposed to relate the above facts with the two different case that would be very helpful.  I guess what I am struggling with is what values to plug into $f(\omega)$.</p>
",2013-10-28 23:53:40.903
58399,23115.0,1,,,,Bayesian and frequentist interpretations vs approaches,<hypothesis-testing><bayesian><frequentist>,CC BY-SA 3.0,"<p>I have been reading about the frequentist vs bayesian issue (<a href=""http://blog.keithw.org/2013/02/q-what-is-difference-between-bayesian.html"" rel=""nofollow"">this article</a>Â has helped a lot, specially with the example; also <a href=""http://oikosjournal.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/"" rel=""nofollow"">this one</a>), and I haven't come to terms with it.Â At the moment it seems like there are the frequentist and bayesian interpretation of probability; and, separately, the frequentist and bayesian approach to problems. The former is about the belief vs frequency issue (illustrated in the second article). The later is illustrated in the first article. Both put together seem to me like this:</p>

<ul>
<li>The frequentist interpretation of the frequentist approach ensures to
be right a% of the time for large number of trials assuming only the
likelihood distribution, no matter which parameter we get, as long we
as assume that we'll get a good range of data.</li>
<li>The frequentist interpretation of the bayesian approach ensures to be
right a% of the time for large number of trials assuming the
likelihood distribution and the prior, no matter which data we get,
as long as we assume that we'll get a good range of parameters.</li>
<li>The bayesian interpretation of the frequentist approach says that we
are right with a probability of a% assuming only the likelihood
distribution, no matter which parameter we get, as long as we assume
fairness in the randomness of the data.</li>
<li>The bayesian interpretation of the bayesian approachÂ says that we are
right with a probability of a%Â assuming the likelihood distribution
and the prior, no matter which data we get,Â as long as we assume
fairness in the randomness of the parameters.</li>
</ul>

<p>This is the only consistent view that I have been able to form from what I've read. However, I I still think I maybe missing something (as I actually haven't found this view like this anywhere else, it's my own conclusion), so taking the null hypothesis that I'm wrong, where's my mistake? </p>
",2013-10-29 00:06:34.723
58448,21029.0,2,,58443.0,,,,CC BY-SA 3.0,"<p>It makes sense to take a large random sample of N cases (10,000) to estimate the real distribution (the full 1 million). The area under the curve will be an approximate, but a very good approximate as N increases. </p>

<p>If this is something that needs to be done frequently, you can try the ROC calculation with increasingly large sample sizes to find an optimally large sub-set. Optimal here would mean that the loss of information is acceptable. Be warned that a random sample needs to still be representative of the full dataset (whatever that means for your study).</p>

<p>I can't cite any literature off-hand, but I know this type of sampling is used often in practice for different reasons. I for one often use sampling to reduce a large data-set (1-2 million) to something more easily handled (~5-10K) before starting on a data analysis. </p>
",2013-10-29 18:19:04.047
58400,20473.0,2,,58395.0,,,,CC BY-SA 3.0,"<p><em>(Stationarity of what? What kind/level of stationarity?)</em>  </p>

<p>Given the standard linear regression specification (without <em>any</em> specific stochastic assumptions)</p>

<p>$$\mathbf y = \mathbf X\beta +\mathbf u $$
as a  matter of mathematics we have 
$$\hat\beta_{OLS} = \left(\mathbf X'\mathbf X\right)^{-1}\mathbf X'\mathbf y=\beta + \left(\mathbf X'\mathbf X\right)^{-1}\mathbf X'\mathbf u$$</p>

<p><strong>For consistency</strong> of  $\hat\beta_{OLS}$  we need (as sample size $n$ goes to infinity)
$$\operatorname {plim}\left [\left(\mathbf X'\mathbf X\right)^{-1}\mathbf X'\mathbf u \right ]= \mathbf 0 \Rightarrow \left(\operatorname {plim}\frac 1n\mathbf X'\mathbf X\right)^{-1} \operatorname {plim}\left (\frac 1n\mathbf X'\mathbf u \right )= \mathbf 0$$</p>

<p>This requires   </p>

<p>a) that $\left(\operatorname {plim}\frac 1n\mathbf X'\mathbf X\right)^{-1} &lt; \infty$, and that it converges to a positive definite matrix, which is a condition usually just assumed, and it will be satisfied if the ""Grenander conditions"" are satisfied (in short, as $n\rightarrow \infty$, no regressor degenerates to a sequence of zeros, no single observation dominates the sum of squares of its series, and the regressor matrix always has full rank). These conditions <em>exclude some kinds of</em> non-stationarity of the regressors, but they do not <em>require</em>  covariance-stationarity (which is the one usually meant under the term ""stationarity"").</p>

<p>b) that 
$$\operatorname {plim}\left (\frac 1n\mathbf X'\mathbf u \right )= \mathbf 0\Rightarrow \left [\begin{matrix}
\operatorname {plim}\frac 1n\sum_{i=1}^nx_{1i}u_i \\
...\\
\operatorname {plim}\frac 1n\sum_{i=1}^nx_{ki}u_i
\end{matrix}\right ] =\mathbf 0 $$</p>

<p>Now <em>Markov's</em> Law of Large Numbers, in order to hold  requires that 
$$\frac 1{n^2}\operatorname {Var}\left(\sum_{i=1}^nx_{1i}u_i\right)\rightarrow 0,\; \text {as}\; n\rightarrow \infty$$</p>

<p>Here too, this condition excludes some kinds of non-stationarity, but it does not require covariance stationarity (for example, both the mean and the variance of each $x_{ji}$ and each $u_i$ may be different -we only need that the variance of the sum is of smaller order than $n^2$).
If this condition holds then the Law of Large Numbers applies and we have (abusing notation a bit)</p>

<p>$$\operatorname {plim}\left (\frac 1n\mathbf X'\mathbf u \right )=  \left [\begin{matrix}
\operatorname {lim}\frac 1n\sum_{i=1}^nE(x_{1i}u_i) \\
...\\
\operatorname {lim}\frac 1n\sum_{i=1}^nE(x_{ki}u_i)
\end{matrix}\right ]$$</p>

<p>For this to be equal to the zero-vector we need that each regressor is <em>contemporaneously uncorrelated</em> with the error term, $E(x_{ji}u_i)=0,\; \forall j,i$. This is a condition related to stochastic dependence/independence, and has nothing to do with stationarity.</p>

<p><strong>For asymptotic normality</strong> of  $\hat\beta_{OLS}$ we examine</p>

<p>$$\operatorname {plim}\sqrt n(\hat\beta_{OLS} -\beta)= \operatorname {plim}\left[\sqrt n\left(\frac 1n \mathbf X'\mathbf X\right)^{-1}\frac 1n\mathbf X'\mathbf u\right] = \left(\operatorname {plim}\frac 1n\mathbf X'\mathbf X\right)^{-1} \operatorname {plim}\left (\frac 1{\sqrt n}\mathbf X'\mathbf u \right )$$</p>

<p>The first plim was discussed previously. For the <em>Lindeberg-Feller</em> Central Limit Theorem to hold for the second plim, what is required is<br>
a) that each regressor series is comprised of independent r.v.'s,<br>
b) that the errors are independent from each other,<br>
(both these can be relaxed)<br>
c) that the expected values and the variances of the rv's involved are finite, but <em>not necessarily equal</em><br>
d)and finally that ""no term dominates the whole"", which is expressed as a condition on the <em>relative</em> magnitude of the variances involved.  </p>

<p>So again, some forms of non-stationarity are excluded, but covariance-stationarity is not needed.</p>
",2013-10-29 01:11:19.870
58401,22544.0,1,58422.0,,,What model should one use for this short time series?,<regression><time-series><multiple-regression><predictive-models>,CC BY-SA 3.0,"<p>Below I have quarterly total sales on the left (dependent variable), and a sample of the sales on the right.  The two variables share a correlation of 98.7%.  What model should I use to predict X?  For that model, should I include a constant?  Seasonal adjustments?  Remove outliers?  The most important criteria is minimizing out of sample prediction error.</p>

<pre><code>Q3'10   40.19   0.2386
Q4'10   39.36   0.2000
Q1'11   51.25   0.2173
Q2'11   54.99   0.2630
Q3'11   50.38   0.2242
Q4'11   50.77   0.2623
Q1'12   67.39   0.3548
Q2'12   77.14   0.3716
Q3'12   72.54   0.3451
Q4'12   80.21   0.3816
Q1'13   94.57   0.4661
Q2'13   102.13  0.4919
Q3'13E  X       0.4424
</code></pre>
",2013-10-29 01:17:39.457
58402,1359.0,1,,,,Incremental SVD in Collaborative Filtering,<machine-learning><svd><recommender-system>,CC BY-SA 3.0,"<p>In the so-called incremental SVD used for collaborative filtering:</p>

<p><a href=""http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf"" rel=""nofollow"">http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf</a></p>

<p><a href=""http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf"" rel=""nofollow"">http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf</a></p>

<p><a href=""http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/"" rel=""nofollow"">http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/</a></p>

<p>The user x item matrix R is factored as QP using gradient descent. In the classical SVD there is the diagonal matrix S which holds the singular values. What happens(ed) to that matrix in this formulation? Is it just omitted and they still call it SVD or is it implicitly part of Q and/or P?</p>
",2013-10-29 01:21:37.867
58403,23087.0,2,,58401.0,,,,CC BY-SA 3.0,"<p>I would consider using Gaussian Process regression. Carl Rasmussen's excellent book and associated Matlab software are freely available here: <a href=""http://www.gaussianprocess.org/gpml/"" rel=""nofollow"">http://www.gaussianprocess.org/gpml/</a>.  You would probably want to use a mixture of a periodic kernel for the seasonal effect plus a linear kernel since there looks to be a roughly linear growth. If you want to use the ""sample of sales"" variable as a covariate that is possible too. </p>
",2013-10-29 01:27:27.993
58404,23087.0,2,,52099.0,,,,CC BY-SA 3.0,"<p>As a general rule you should work with log probabilities rather than probabilities themselves, since multiplying small doubles is very imprecise. Multiplication then becomes addition, which is much more accurate. If you need to sum probabilities you need a stable ""<code>logsumexp</code>"" function, see [http://machineintelligence.tumblr.com/post/4998477107/the-log-sum-exp-trick] for example. SlowlyFailing is absolutely correct about using <code>lfactorial</code> (i.e. <code>lgamma</code> really) rather than <code>factorial</code>. </p>
",2013-10-29 01:34:57.827
58405,22262.0,1,58412.0,,,What if Lasso selects transformed terms but not untransformed terms,<feature-selection><lasso><lars>,CC BY-SA 3.0,"<p>Suppose I have standard normal features $X_i \in \{X_i : i \in \{1,...,1000\}\}$. I extend this set of predictors with transformations as follows: $\{X_i,X_i^2,X_iI(X_i &gt; 0) : i \in \{1,...,1000\}\}$.</p>

<p>What happens if Lasso would pick $X_i^2$ or $X_iI(X_i &gt; 0)$ but not $X_i$ itself. What do I do? Is this even a problem?</p>
",2013-10-29 02:43:02.493
58406,594.0,2,,44635.0,,,,CC BY-SA 4.0,"<p>As with many such situations, one must take care to avoid confusing sample and population quantities.  (Given some particular distributional assumptions, we might choose to test for symmetry about a population mean using a statistic based on sample medians for example.)</p>
<p>We should also keep in mind that failure to reject a null of symmetry is not the same as showing symmetry.</p>
<p>Let's begin by simplifying things by assuming continuity.</p>
<p>First, what is meant by symmetry of a distribution? While it's usually conceived in the elementary treatments in terms of the density - i.e. as <span class=""math-container"">$f(\theta+x)=f(\theta-x)$</span>, when we say 'that the <em>distribution</em> is symmetric', I often tend to conceive it in terms of the distribution function (though the distinction won't matter, generally).</p>
<p>Note that symmetry around the population mean implies symmetry about the population median, so we needn't distinguish them - if the mean exists, the two will be the same.</p>
<p>There are two cases to distinguish:</p>
<ol>
<li><p>testing for symmetry about a specified location and</p>
</li>
<li><p>testing for symmetry about an unspecified location</p>
</li>
</ol>
<p>Let's consider each in turn</p>
<ol>
<li>One example of a way to test for symmetry about a specified mean <span class=""math-container"">$\theta_0$</span> is to create a second sample, <span class=""math-container"">$Y=2\theta_0-X$</span> and compute a test statistic that measures discrepancy in the  distributions of X and Y (such as a two-sample Kolmogorov-Smirnov statistic).</li>
</ol>
<p>[I'm not certain the distribution of the test statistic under the null is still the same as for the KS test <span class=""math-container"">$-$</span> and I'm not going to try to work it out right now <span class=""math-container"">$-$</span> but the distribution could easily be simulated for this circumstance, so it's not a huge issue.]</p>
<p>Note further that testing for symmetry about a known location may be reduced to testing for symmetry about 0 simply by subtracting the given location from all the observations. The test mentioned above would then be a test for symmetry about 0.</p>
<p>There are many other tests that could be used in this situation, such as a sign test (if the distribution is not symmetric about 0, there will typically tend to be an excess or deficit of positive signs, though counterexamples are certainly possible), or the signed rank test mentioned before. (They all act as a test of symmetry about the specified population mean)</p>
<hr />
<ol start=""2"">
<li>Some tests for symmetry about an unknown center. There are <em>many</em> of these; I'll mention just a few.</li>
</ol>
<p>i) The <a href=""https://www.jstor.org/stable/10.2307/228740"" rel=""nofollow noreferrer"">triples test</a> of Randles et al (1980)</p>
<p>This test is (IMO) intuitively appealing. It looks at sets of three observations, checking whether in each case the triple has the middle observation closer to the smaller (suggesting right skew) or larger (suggesting left skew) observation (the right skew case gets a score of 1/3, the left skew case gets -1/3 and anything else scores 0. Then the test statistic, <span class=""math-container"">$R$</span>, is the average of the scores over all possible triples.</p>
<p>(This test is not distribution free, but with a consistent estimator of the variance of <span class=""math-container"">$R/\sqrt n$</span> it is asymptotically distribution free.)</p>
<p>Randles, Fligner, Policello and Wolfe (1980)<br />
An Asymptotically Distribution-Free Test for Symmetry Versus Asymmetry<br />
<em>Journal of the American Statistical Association</em><br />
Vol. 75, No. 369, Mar., pp. 168-172</p>
<p>ii) <a href=""https://www.jstor.org/stable/10.2307/2284233"" rel=""nofollow noreferrer"">Gastwirth's</a> (1971) modified sign test. Gastwirth considered a sign test about the sample mean. It's no longer distribution-free, but again, with a consistent estimator of the variance of an appropriately scaled statistic, it is asymptotically so. However, note that this test would have essentially no power against asymmetric distributions with <span class=""math-container"">$P(X&gt;\mu) = 1/2$</span></p>
<p>Gastwirth, J.L. (1971)<br />
On the Sign Test for Symmetry.<br />
<em>Journal of the American Statistical Association</em>, 66, 821-828.</p>
<p>iii) <a href=""https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-3/issue-2/The-Limits-of-a-Measure-of-Skewness/10.1214/aoms/1177732911.full"" rel=""nofollow noreferrer"">Hotelling and Solomons test</a> (1932)  of the Pearson skewness (scaled mean-median).   Gastwirths 1971 paper (mentioned above) gives an expression for the asymptotic variance of a suitably normalized statistic and this, too, is thereby asymptotically distribution free.</p>
<p>Hotelling, H. and L. M. Solomons (1932)<br />
The Limits of a Measure of Skewness<br />
<em>Ann. Math. Statist</em>. Vol 3, No. 2, 141-142.</p>
<p>On this test, also see <a href=""https://en.wikipedia.org/wiki/Nonparametric_skew"" rel=""nofollow noreferrer"">here</a></p>
<hr />
<p>Note that Gastwirth's test in (ii) is quite similar to the test you propose, with only the substitution of the sign test for the signed rank test. Your test would <em>also</em> not be distribution-free, but you should probably be able to find a consistent estimator of the variance of your statistic (appropriately standardized), and thus get an aymptotically distribution free test. (Alternatively, you might be able to come up with a bootstrap test based off such a statistic.)</p>
<hr />
<p>A review of tests of symmetry can be found <a href=""https://web.archive.org/web/20160809234331/http://www.stat.ncsu.edu/information/library/mimeo.archive/ISMS_1984_1460.pdf"" rel=""nofollow noreferrer"">here</a>. Also see <a href=""https://web.archive.org/web/20151227101037/http://stat.fsu.edu/techreports/M599.pdf"" rel=""nofollow noreferrer"">this tech report</a></p>
",2013-10-29 03:26:42.400
58407,20752.0,2,,58405.0,,,,CC BY-SA 3.0,"<p>It's not a problem strictly speaking, but you should pay attention to whether that interpretation makes sense in the context of your problem. If the squared value is what's important to your dependent variable, then this is what you'd expect to happen because it would have more explanatory power than the raw variable. </p>

<p>For example, if you're a sports fan, you could use a players age to predict their performance. If you mean-shift that to something like 27, then the age variable can be negative if it is below 27 and positive if it's greater than 27. Well, players performance tends to have an arc that peaks at age 27, so if you put the raw age into the regression, chances are it would show not significant. If you put age^2 in though, it would. And likewise if you gave Lasso the choice between the two, it's a no brainer.</p>
",2013-10-29 03:58:53.220
58435,13740.0,1,58890.0,,,Classifier for uncertain class labels,<classification><weka><uncertainty>,CC BY-SA 3.0,"<p>Let's say I have a set of instances with class labels associated. It does not matter <em>how</em> these instances were labelled, but <em>how certain</em> their class membership is. Each instancs belongs to <em>exactly one</em> class. Let's say I can quantify the certainty of each class membership with a nominal attribute that goes from 1 to 3 (very certain to uncertain, respectively). </p>

<p>Is there some sort of classifier that takes into consideration such a certainty measure and if yes, is it available in the WEKA toolkit? </p>

<p>I imagine this situation occurs quite often, for example when instances are classified by human beings which are not always completely sure. In my case, I have to classify images, and sometimes an image could belong to more than one class. If this happens, I give the class a high uncertainty, but still classify it with only one class. </p>

<p>Or are there any other approaches to this problem, without a specialized classifier? E.g. only taking ""certain"" classifications for training? I fear that in this case, there will be more misclassifications because ""border"" cases are not covered. </p>
",2013-10-29 14:38:10.610
58408,22923.0,1,58428.0,,,Does zero correlation between 2 differenced series implies no cointegration between original series?,<regression><time-series><correlation><econometrics><cointegration>,CC BY-SA 3.0,"<p>The question is related to <a href=""https://stats.stackexchange.com/questions/73486/non-stationary-series-keep-close-to-each-other-but-correlation-between-growth-ra"">this one</a>.</p>

<p>In <a href=""https://stats.stackexchange.com/questions/8161/testing-two-i1-vectors-for-a-relationship?rq=1"">this question</a> @mpiktas gives an answer on why checking correlation is not enough but the answer doesn't seem completely correct to me for the following reason:</p>

<p>If  2 time-series are cointegrated, i.e. there is a linear relation between them $$y_t = a + b x_t + \varepsilon_t$$ with stationary $\varepsilon_t$ it <em>implies</em> a linear relation between their differences $$\Delta y_t = b \Delta x_t + \Delta \varepsilon_t.$$ So if series are cointegrated, their differences should be correlated. And this means that if we don't see significant correlation between differences in our data, then there is no cointegration either. Is this correct or am I missing something?</p>

<p>The question arises because I look for relations between hundreds of time-series (mainly non-stationary) and the way I do this is by considering correlations between their differenced counterparties. And I assume that if I don't see correlation between differenced series there is no cointegration either - it suffices to check only correlations.</p>
",2013-10-29 04:16:50.607
58409,22262.0,1,,,,When would I choose Lasso over Elastic Net,<feature-selection><lasso><elastic-net><regularization>,CC BY-SA 3.0,"<p>What are the scenarios where Lasso is likely to perform better than Elastic Net (out of sample prediction)?</p>
",2013-10-29 05:22:25.660
58410,23121.0,1,58411.0,,,Simple linear regression - understanding given,<regression><self-study>,CC BY-SA 3.0,"<p>The question is to fill out the missing numbers (A-L) of a simple linear regression model.
I am having problems with converting and interpreting the given table in terms of variables. Would it be possible for someone to confirm and clarify things for me.</p>

<p>The first table represents regression statistics</p>

<p><img src=""https://i.stack.imgur.com/bPGns.jpg"" alt=""""></p>

<p>True model
$$
Y_t = \beta_o + \beta_1X_t + \mu_t
$$</p>

<p>Estimated model
$$
\hat Y_t = \hat\beta_0 + \hat\beta_1x_t
$$</p>

<p>This is what I am confused about</p>

<ul>
<li>Does the first standard error (12.8478) mean $\sum\hat\mu_t^2$ ?</li>
<li>Does the standard error for the intercept in last table (14.6208) mean $\sum\mu_t^2$ ?</li>
<li>Does 3.8508 equal $\hat\beta_1$ ?</li>
<li>In order to calculate RSS (for J) I need $\sum \hat\mu_t^2$ does this confirm that my first two points are incorrect</li>
<li>I know $G=\hat\beta_1^2\sum x_t^2$, how do I find $\sum x_t^2$</li>
</ul>

<p>If I am wrong, would it be possible to know what those numbers mean in terms of variables</p>
",2013-10-29 05:47:54.477
58411,22419.0,2,,58410.0,,,,CC BY-SA 3.0,"<ol>
<li><p>The standard error here refers to the standard error of the model as a whole, and it is the <strong>standard deviation divided by the square root of the sample size</strong>.</p>
</li>
<li><p>Here the standard error refers to the individual standard error for the intercept. The formula is same as that in the first point.</p>
<p>(To get the answer for K and L; use this -&gt; T stat = Coefficient / Std .Error)</p>
</li>
<li><p>Yes its the estimated coefficient of X variable 1</p>
</li>
</ol>
<p>For the rest of the confusion, watch this youtube video!</p>
<p><a href=""http://www.youtube.com/watch?v=zwr0bs8znEE"" rel=""nofollow noreferrer"">http://www.youtube.com/watch?v=zwr0bs8znEE</a></p>
<p>EDIT:</p>
<p>I used some identities to figure out G H I J, so i cant guarantee this is what your lecturer wants.</p>
<p><strong>Calculating D, E, F:</strong></p>
<p>these are the degrees of freedom, some formulas here,
D -&gt; 1
E -&gt; 13
F -&gt; 14</p>
<p><strong>Calculating H, G:</strong></p>
<ol>
<li>F statistic(=24.15) =  I / J.</li>
<li>I is G/D; similarly J is H/E</li>
<li>You know D and E so just some math will get you the value for G and H.</li>
</ol>
<p>This is a really weird approach, but I don't think you can get the sum of squares for regression and residual without the actual observations.</p>
",2013-10-29 06:19:06.987
58412,16992.0,2,,58405.0,,,,CC BY-SA 4.0,"<p>You're right: in general, people don't like to put interactions into a model before putting in the primary effects. There is a recent <a href=""https://projecteuclid.org/journals/annals-of-statistics/volume-41/issue-3/A-lasso-for-hierarchical-interactions/10.1214/13-AOS1096.full"" rel=""nofollow noreferrer"">paper</a> that solves this problem for the lasso: &quot;A lasso for hierarchical interactions&quot; by Jacob Bien, Jonathan Taylor, and Robert Tibshirani. Their solution is implemented in the R package hierNet. Hope this helps!</p>
",2013-10-29 07:17:44.107
58413,21434.0,1,58426.0,,,Which statistical method to use for finding systematic patterns in data,<categorical-data><pattern-recognition><social-science>,CC BY-SA 3.0,"<p>As part of a broader study I am analysing 30 websites that fall into 3 categories:</p>

<ul>
<li>Consumer (10 sites)</li>
<li>Commercial (10 sites)</li>
<li>Health (10 sites)</li>
</ul>

<p>The approach I used was a 'tick and flick' spreadsheet with 24 dichotomous variables that represent features of the website that are either absent or not (i.e. they receive a tick if they exhibit that particular feature).</p>

<p>Here is an <a href=""http://tinyurl.com/mppcj7m"" rel=""nofollow"">example of the data</a>.</p>

<p>The numbers represent how many websites from each category contain each particular feature (variable).</p>

<p>I want to know which kind of statistical test would be used to find if there are any systematic patterns about which 'Category' of website tends to correlate with particular variables. For example, which websites tend to share power with users to edit/contribute web content (measured by variables 2,3,4,5,6,7,8,13,16,19,23,24)?</p>

<p>I would rather use a more robust/rigorous statistical approach than simply counting up totals, or 'eye-balling' patterns in the data.</p>

<p>Thank you in advance.</p>
",2013-10-29 07:54:14.847
58414,20470.0,2,,45457.0,,,,CC BY-SA 3.0,"<p>You need to use the Baum-Welch algorithm to learn the transition, emission and prior probabilities from your data. </p>

<p>If you are using the <a href=""http://cran.r-project.org/web/packages/HiddenMarkov/index.html"" rel=""nofollow"">HiddenMarkov CRAN package</a>, you can achieve this by using the <code>BaumWelch()</code> function.</p>
",2013-10-29 08:53:43.243
58415,22804.0,1,58431.0,,,beta distributions assigned to represent uncertainty,<r><sensitivity-analysis>,CC BY-SA 3.0,"<p>I need to calculate the Probabilistic Sensitivity Analysis for a function.
I was given this:</p>

<p>beta distributions assigned to represent uncertainty<br>
And have this parameter with this data:</p>

<pre><code>variable = d_progress
probability of variable : 0.1
n = 100
r = cases = 3
</code></pre>

<p>Now, I need to use this function or something?</p>

<pre><code>p &lt;- rbeta(n, shape1=alpha, shape2=beta)
</code></pre>

<p>I already have the parameters so I can find <code>p</code>.
If I have:</p>

<pre><code>p&lt;-rbeta(100, 1, 99) 
</code></pre>

<p>What I do is I create a vector now? Of 100 values?
So, I need to create a loop, 100 times for the following formula:</p>

<pre><code>for each of 100 p generated (
  EV = p*b_par
  I then submit each EV in a vector or something...
  I need to Find the different values of EV for different p,
    so I just find the mean of all the 100 EV in the end
)
</code></pre>
",2013-10-29 08:57:10.730
58416,23126.0,2,,10008.0,,,,CC BY-SA 3.0,"<p>There are various processes in nature that involve only an interaction effect and laws that decribe them. For instance Ohm's law. In psychology you have for instance the performance model of Vroom (1964): Performance = Ability x Motivation.Now, you might expect finding an significant interaction effect when this law is true. Regretfully, this is not the case. You might easily end up with finding two main effects and an insignificant interaction effect (for a demonstration and further explanation see Landsheer, van den Wittenboer and Maassen (2006), Social Science Research 35, 274-294). The linear model is not very well suited for detecting interaction effects; Ohm might never have found his law when he had used linear models.</p>

<p>As a result, interpreting interaction effects in linear models is difficult. If you have a theory that predicts an interaction effect, you should include it even when insignificant. You may want to ignore main effects if your theory excludes those, but you will find that difficult, as significant main effects are often found in the case of a true data generating mechanism that has only a multiplicative effect. </p>

<p>My answer is: Yes, it can be valid to include a two-way interaction in a model without including the main effects. Linear models are excellent tools to approximate the outcomes of a large variety of data generating mechanisms, but their formula's can not be easily interpreted as a valid description of the data generating mechanism.</p>
",2013-10-29 09:19:13.857
58417,15293.0,2,,42885.0,,,,CC BY-SA 3.0,"<p>Firstly, your 2SLS-estimator is wrong. Check wikipedia:
<a href=""http://en.wikipedia.org/wiki/Instrumental_variable"" rel=""nofollow"">http://en.wikipedia.org/wiki/Instrumental_variable</a></p>

<p>Let n be the number of observations. Let Z be the instrument matrix and X be endogenous regressor. You say you have 2 instruments and 1 endogenous regressor, hence Z is n by 2 and X is n by 1. </p>

<p>Projection matrix $Pz(=Z(Z'Z)^{-1}Z')$ is n by n, hence the 2SLS-estimator 
$\beta_{2sls} = (X'PzX)^{-1}X'Pzy$
works like a charm. Hence there is probably something wrong with you code/data.</p>

<p>When you get stuck at these things go back to matrix form.</p>
",2013-10-29 09:57:19.897
58418,23127.0,1,,,,Multiple regression and hypothesis test $H_0$:$\beta_2=0$,<regression><hypothesis-testing><self-study><regression-coefficients>,CC BY-SA 3.0,"<p>Multiple regression model<br>
$H_0$:$\beta_2=0$, $H_1$:$\beta_2 \neq 0$<br>
where $\beta_2$ is the vector of elements ($\beta_2, \beta_3, \dots, \beta_k$) and $\beta$ is slope of regression line.</p>

<p>Why it is equivalent to a test based on the statistic
$$\frac{R^2/(k-1)}{(1-R^2)/(T-k)}$$
where $R^2$ is the square of the multiple correlation coefficient of the equation.  </p>

<p>I don't know how to solve this. Please give me any suggestion or hint.</p>
",2013-10-29 10:03:02.447
58419,18841.0,1,58421.0,,,Neighborhood analysis in Matlab using a dot plot,<classification><data-visualization><clustering><matlab>,CC BY-SA 3.0,"<p>I have points in a 2D graph (coordinates: X,Y property: Z). I would like to find for every point the closest, for example, 5 points and save their properties.</p>

<p>What would be the easiest approach?</p>

<p>Update:<br>
Using the following code:</p>

<pre><code>%Synthetic data
A = {[1,1]; 'A'};
B = {[2,2]; 'B'};
C = {[3,3]; 'C'};

%plot
D = {A{1};B{1};C{1}};
VarPlot = cell2mat(D);
plot (VarPlot,'.');

%knnsearch
[IDX,dist] = knnsearch(VarPlot(:,1),VarPlot(:,2))
</code></pre>

<p>I receive the following result:</p>

<pre><code>IDX =

 1
 2
 3

dist =

 0
 0
 0
</code></pre>

<p>What does this mean? And how can I link the result to the properties A,B and C. I am new to this kind of questions.</p>
",2013-10-29 10:33:32.380
58420,23096.0,1,,,,Inferring an unmeasured value,<r><mean>,CC BY-SA 3.0,"<p>Suppose we have a table of finishing times for an number of 100 metre races where each competition has a different mix of entrants. We suspect that some races are slower than others due to, say, a headwind, but this wasn't measured. How do I find the underlying time for each runner and the correction for each race?</p>

<p>I have devised my own crude technique to solve this problem. I assume  </p>

<pre>
Tobs(e,c) = Tund(e) + Tcor(c) + E(e,c)

where 

Tobs(e,c) = observed time of entrant e in competition c
Tund(e) = underlying time of entrant e
Tcor(c) = time correction for competition c
E(e,c) = error matrix 

sum of all Tcor = 0
sum of all rows in E(e,c) = 0
sum of all columns in E(e,c) = 0

</pre>

<p>I then iteratively choose values for Tcor until the above relations are satisfied. </p>

<p>Is there a better way? Using R? Please be gentle. As you might have guessed by now, I don't much much about stats or maths. Thanks!</p>
",2013-10-29 11:37:29.087
58421,22923.0,2,,58419.0,,,,CC BY-SA 3.0,"<p>Matlab Statistics Toolbox has a 'knnsearch' function that does exactly this: <a href=""http://www.mathworks.com/help/stats/knnsearch.html"" rel=""nofollow"">http://www.mathworks.com/help/stats/knnsearch.html</a></p>
",2013-10-29 11:52:20.217
58422,2149.0,2,,58401.0,,,,CC BY-SA 4.0,"<p>You can just use the history of Y or also your suggested causal.  I have not seen â€œsample of salesâ€ before as a causal, so I am hesitant to want to use that variable, but I am sure you know what you are doing.</p>
<p>Yes, you should consider the adjustment of outliers.  Yes, you should allow for a constant.  Yes, you should consider seasonal impacts.</p>
<p>The ACF/PACF doesn't show that the lag of 4 is important so autoregressive seasonality is weak.  The data are short so this can be expected.  Q4 is flat and then the last year Q4 is high which might due to the short data or a change in the behavior of Q4. Tough to tell.</p>
<p>A possible model (automatically developed using <a href=""http://http:/www.autobox.com"" rel=""nofollow noreferrer"">AUTOBOX</a>), a piece of software I have helped develop is <img src=""https://i.stack.imgur.com/xCvuR.jpg"" alt=""enter image description here"" /> providing <img src=""https://i.stack.imgur.com/cNy6v.jpg"" alt=""enter image description here"" /> There are two seasonal dummies detected consistent 1st and 2nd qtr positive effects.</p>
<p>If one did not use the predictor then a very similar forecast is developed using this equation</p>
<p><a href=""https://i.stack.imgur.com/3gWSK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3gWSK.jpg"" alt=""enter image description here"" /></a> <img src=""https://i.stack.imgur.com/glIQ9.jpg"" alt=""enter image description here"" />
<img src=""https://i.stack.imgur.com/SiC4V.jpg"" alt=""enter image description here"" /> It is interesting (at least to me !) that the two quarterly negative seasonal pulses (qtrs 3 and 4) are the â€œreflectionâ€ of the two quarterly seasonal pulses developed using the predictor series.</p>
<p>EDITED to respond to Nick's OLS MODEL:</p>
<p>If you take Y and divide it by X to get a new variable called Z and THEN run an OLS model restricting the intercept to be 0., you in fact will obtain <img src=""https://i.stack.imgur.com/b4FID.jpg"" alt=""enter image description here"" /> . The residuals from this assumed model (as you have wisely said in previous posts it is always a good thing to bring the residuals to your &quot;doctor&quot; for a checkup) have <img src=""https://i.stack.imgur.com/VS3lm.jpg"" alt=""enter image description here"" /> a serious violation/malady at period 1 and clearly evident non-randomness.  The whole idea is to avoid entertaining insufficient models and adequately capturing the signal. Clearly, the simple OLS model for Z ignores the very clear need for seasonal/quarterly dummies which are lost in translation when converting Y and X to Z.</p>
",2013-10-29 13:00:03.613
58436,23136.0,1,,,,Plotting variables in transformed space,<data-visualization><data-transformation><logarithm>,CC BY-SA 3.0,"<p>Suppose $A = X_1/X_2$ and $B = X_3/X_4$. Why would one plot the data in $(\log A, \log B)$ space as opposed to $(A,B)$ space?</p>
",2013-10-29 14:38:39.240
58437,,2,,58436.0,user31668,,,CC BY-SA 3.0,"<p>If one wanted to use linear regression, then the logs of A and B will be linear in the numerator and denominator. That is the usual reason to use log transforms. It also may make the sampling errors better behaved, since you are converting ratios to sums.</p>
",2013-10-29 14:41:29.037
58438,18513.0,1,,,,How do residuals affect the t-statistic?,<regression><t-test>,CC BY-SA 3.0,"<p>Could a t-statistic for a simple regression relationship with a smaller sum of squared residuals but a smaller or less positive slope be larger than that of a plot with a larger sum of squared residuals and a large slope?</p>
",2013-10-29 14:52:38.303
58423,20613.0,2,,58420.0,,,,CC BY-SA 3.0,"<p>You are on the right path, I guess.  (<em>This is me being gentle</em>.)</p>

<p>If you are looking for a quick and dirty solution (<em>this is me being helpful</em>), you may want to fit a linear model, specifically an ANOVA.  Your response would be Tobs(e,c) and your factors of interest would be Ecor (time correction for entrant e) and Ccor (time correction for competition c).  This is different from what you had posted with Tund (an underlying time of entrant e), because by default the ANOVA model includes an overall mean (mu).  Make sure that both factors of interest (Ecor and Ccor) are treated as ""factors"" or ""categorical variables"" in whatever package you use.  If you treat them as ""continuous"" or ""numeric"" variables, you will get nonsensical results.  (Since you seem to be interested in the estimates for each race and each runner, you avoid going down the wormhole of random effects or mixed effects models.)</p>

<p>If you are looking for a more thorough and vetted solution (<em>this is me being realistic</em>), you should consult with a statistician.  S/he can help ensure that your model answers your questions, suits the ""design"" of the data, meets all the necessary assumptions (independence, homogeneity, normality), and avoids commonly made mistakes.</p>

<p>If you are looking for a long term solution (<em>this is me being encouraging</em>), take a statistics or data analysis class.  It can be very rewarding, especially when you have specific analysis needs in mind when taking the course.</p>
",2013-10-29 13:05:48.223
58424,0.0,5,,,,,,CC BY-SA 3.0,,2013-10-29 13:11:25.997
58425,16043.0,4,,,,,,CC BY-SA 3.0,Describes data models with a time-series component and a spatial component.,2013-10-29 13:11:25.997
58426,22985.0,2,,58413.0,,,,CC BY-SA 3.0,"<p>It seems you don't really know what you want except some sort of pattern. So why not making a <a href=""http://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">Principal Component Analysis</a> to reduce the complexity and get the direction of greatest variabilities.</p>
",2013-10-29 13:20:35.720
58427,20416.0,1,58478.0,,,Modelling relation between two persistent AR(1) processes? Is my approach reasonable and how to adjust standard errors?,<standard-error><autocorrelation>,CC BY-SA 3.0,"<p>I want to run a simulation in which I want to find out whether there is a relation between the independent variable $x_t$ and the dependent variable $y_t$. I.e., in the following regression I want to find out if $\beta$ is signficantly different from zero:</p>

<p>$$ y_t = \beta x_t + \epsilon_t. $$</p>

<p>In empirical data, $y_t$ is stationary, but close to a random walk.I want to model it as an AR(1) process. Of course, I could just model $x_t$ as an AR(1) process and if $\beta$ is different from zero, $y_t$ would end up as such a process as well. However, I also want to consider the case in which $\beta=0$ and I don't want to confuse the reader with changing descriptions for different scenarios (i.e., ""if $\beta=1$ I simulate $y_t$ from the above equation, if $\beta=0$, I simulate $y_t$ as an AR(1) process."")</p>

<p>I came up with the following solution to this dilemma. Why not model both $y_t$ and $x_t$ as AR(1) processes with potentially correlated errors. That is:</p>

<p>$$ y_{t+1} =  \tau_y y_t  + u_{t+1}, \quad 0 &lt; \tau_y &lt; 1, \quad u_{t+1} \sim N(0, \sigma_u^2) $$ and
$$ x_{t+1} =  \tau_x x_t + v_{t+1} , \quad 0 &lt; \tau_x &lt; 1, \quad v_{t+1} \sim N(0, \sigma_{v}^2), $$</p>

<p>where </p>

<p>$$ cov\left(\begin{bmatrix} v_{t+1} \\ u_{t+1} \end{bmatrix}, \begin{bmatrix} v_{t+1} &amp; u_{t+1} \end{bmatrix}\right) = \begin{bmatrix} \sigma_v^2 &amp; \sigma_{v} \sigma_u \rho_{u,v} \\ 
\sigma_{v} \sigma_u \rho_{u,v} &amp; \sigma_u^2 \end{bmatrix} $$</p>

<p>Since $\beta = \frac{Cov(y, x)}{\sigma_x^2}$ and </p>

<p>$$
\begin{aligned}
Cov(y, x) &amp;= E[y_{t+1}x_{t+1}] \\
&amp;= E[(\tau_y y_t + u_{t+1})(\tau_x x_t + v_{t+1})] \\
&amp;= \tau_y \tau_x E[y_t x_t] + E[u_{t+1} v_{t+1}] \\
&amp;= \tau_y \tau_x Cov(y, x)  + \sigma_{u} \sigma_{v} \rho_{v, u} \\
\end{aligned}
$$</p>

<p>we get by rearranging</p>

<p>$$ \beta = \frac{\sigma_u}{\sigma_{v}}\rho_{v, u} \frac{1 - \tau_x^2}{1-\tau_x \tau_y}. $$</p>

<p>Fair enough, this is a little bit complicated, but it allows me to control $\beta$ without changing the structure of either $y_t$ or $x_t$ and both those processes are AR(1) in this setup. </p>

<p>I basically have two questions:</p>

<ol>
<li>Is this a reasonable approach given the requirement that preferably both $x_t$ and $y_t$ should be AR(1) processes. The interpretation of the relation should really be just like in a simple OLS setup, i.e. has $x_t$ an impact on $y_t$ (I'm not concerned about causality here, just relation). Would you consider my setup still as a reasonable way of modelling it? (I just don't make a direct link, but use the error structure for that. I don't see a problem with this approach. If $x_t$ and $y_t$ are related via correlated shocks, so be it.)</li>
<li><p>I already simulated this and it works fine. However, I also want to obtain correct confidence intervals for each run. That is, I want to run the regression in the first equation, get $\widehat{\beta}$ and also standard errors that should be reasonable. Now I noticed that the errors are highly autocorrelated and this autocorrelation seems to be identical to $\tau_y$. However, I could not formalize it. So it would be great to know how I would have to adjust the standard errors. (It would be extra awesome if I could get a hint how to implement that in R.)</p>

<pre><code>set.seed(123)
library(MASS)
### Set start values
nrT &lt;- 1e5
burnin &lt;- 1e3
sd_y_shock  &lt;- 1
sd_x_shock  &lt;- 1
corr_x_y    &lt;- 0.5
tau_y  &lt;- 0.8
tau_x  &lt;- 0.1
### Simulate the correlated shocks
shocks &lt;- mvrnorm(nrT + burnin, 
                  mu = c(0,0), 
                  Sigma = matrix(c(sd_y_shock^2, corr_x_y * sd_y_shock * sd_x_shock, 
                                   corr_x_y * sd_y_shock * sd_x_shock, sd_x_shock^2), 
                                 nrow=2))

vec_y &lt;- arima.sim(list(order = c(1, 0, 0), ar = tau_y),
                    n = nrT + burnin, 
                    innov = shocks[, 1]) 

vec_x &lt;- arima.sim(list(order = c(1, 0, 0), ar = tau_x),
                   n = nrT + burnin, 
                   innov = shocks[,2])
### Check that formula derived above is correct; the two should be similar
sd_y_shock/sd_x_shock * corr_x_y * (1 - tau_x^2)/(1 - tau_x * tau_y) 
coef(lm(vec_y ~ vec_x))[2]
### Plot ACF
# Note that, independent from corr_x_y and tau_x, the autocorrelation structure seems
# always to be tau_y
acf(lm(vec_y ~ vec_x)$resid)
</code></pre></li>
</ol>

<p><img src=""https://i.stack.imgur.com/74Jjm.png"" alt=""enter image description here""></p>
",2013-10-29 13:23:50.210
58428,20473.0,2,,58408.0,,,,CC BY-SA 3.0,"<p>The existence or not of a linear relationship does not necessarily go hand-in-hand with co-integration. Variables co-integrated in levels won't necessarily exhibit correlation in first-differences.</p>

<p>Assume that the following relation holds:
$$y_t = a + b x_t + \varepsilon_t, \; \varepsilon_t=\text {i.i.d} $$</p>

<p>i.e. the variables are co-integrated. Then the relation</p>

<p>$$\Delta y_t = b \Delta x_t + \Delta \varepsilon_t$$<br>
also holds. Calculating the sample correlation of first-differences we will estimate the Covariance as</p>

<p>$$ \begin{align}\operatorname{\hat Cov}(\Delta y_t,\Delta x_t)=&amp; \frac 1{T-1} \sum_{t=2}^{T}\left(b \Delta x_t + \Delta \varepsilon_t\right)\Delta x_t \\-&amp;\left(\frac 1{T-1} \sum_{t=2}^{T}\left(b \Delta x_t + \Delta \varepsilon_t\right)\right)\left(\frac 1{T-1} \sum_{t=2}^{T}\Delta x_t\right)\end{align} $$</p>

<p>$$ \begin{align}=b\frac 1{T-1}&amp; \sum_{t=2}^{T}\left(\Delta x_t \right)^2 + \frac 1{T-1} \sum_{t=2}^{T}\left(\Delta x_t \Delta \varepsilon_t\right) \\ -&amp; b\left(\frac 1{T-1}\sum_{t=2}^{T} \Delta x_t\right)^2 -\left(\frac 1{T-1} \sum_{t=2}^{T}\Delta \varepsilon_t\right)\left(\frac 1{T-1} \sum_{t=2}^{T}\Delta x_t\right) \end{align}$$</p>

<p>To the degree that $x_t$ and $\varepsilon_t$ are independent, the terms involving the error will tend to vanish and so </p>

<p>$$ \operatorname{\hat Cov}(\Delta y_t,\Delta x_t)\rightarrow bs^2_{\Delta x_t} $$</p>

<p>where $s^2$ is the sample variance (irrespective of whether the variance of $x_t$, or $\Delta x_t$ is constant or not). </p>

<p>The sample variance of $\Delta y_t$ will be </p>

<p>$$s^2_{\Delta s_t} \approx b^2s^2_{\Delta x_t} + s^2_{\Delta \varepsilon_t}$$</p>

<p>again, irrespective of whether these sample moments estimate anything meaningfull.</p>

<p>So 
$$\operatorname {\hat Corr}(\Delta y_t,\Delta x_t)  \approx \frac {bs^2_{\Delta x_t}}{\sqrt {\left(b^2s^2_{\Delta x_t} + s^2_{\Delta \varepsilon_t}\right)}\sqrt {s^2_{\Delta x_t} }} = \frac {bs_{\Delta x_t}}{\sqrt {\left(b^2s^2_{\Delta x_t} + s^2_{\Delta \varepsilon_t}\right)}}$$</p>

<p>So the magnitude of the empirically estimated correlation of first differences, will depend on the magnitude of the variance of the error term (which moreover enters the expression <em>doubled</em> since we consider first differences). If this (constant) variance is large compared to the variance of $x_t$, then the estimated correlation of first-differences may be small to non-existent, even though the variables are co-integrated in levels.  </p>
",2013-10-29 13:24:15.223
58429,,2,,58408.0,user31668,,,CC BY-SA 3.0,"<p>Since correlation is a measure of the degree of linear dependence, first differences should tease this out. Now, I am assuming you check cointegration across multiple lags, not just contemporaneous values, since there could be something like $y_t = a + b x_{t-1} + \varepsilon_t$ going on, which may complicate matters. Alecos' observation that there may be not <em>detectible</em> cointegration is also important.</p>
",2013-10-29 13:24:54.120
58430,,2,,58399.0,user31668,,,CC BY-SA 3.0,"<p>Your notes about frequentists relying on ""repeated sampling"" properites and Bayesians relying on ""fairness"" is in line with Neyman/Pearson and deFinetti's justifications of each paradigm, respectively. Bayesian and frequentist approaches are appropriate in different contexts. A controversial aspect of frequentist approaches is the relevance of the concept of ""confidence"" in the case where it is not clear what is the ""embedding series"" of experiements (there can be many, look up <a href=""http://en.wikipedia.org/wiki/CLs_upper_limits#Relation_to_foundational_principles"" rel=""nofollow"">""relevant subsets problem""</a> for more on this). Bayesians get critisized for applying priors when the underlying property is not random...hence there is a ""calibration"" problem with a, say, 95% posterior interval...95% of what, and why do we care? </p>

<p>I'd take a look at another school/paradigm as well...the <a href=""http://www.utstat.utoronto.ca/reid/research/likelihood-final.pdf"" rel=""nofollow"">Likelihood school</a>, as described by the accessible and useful book ""<a href=""http://books.google.com/books?id=M-3pSCVxV5oC&amp;printsec=frontcover&amp;dq=in%20all%20likelihood&amp;hl=en&amp;sa=X&amp;ei=27pvUqPzMubbyQGyt4DgDA&amp;ved=0CC0Q6AEwAA#v=onepage&amp;q=in%20all%20likelihood&amp;f=false"" rel=""nofollow"">In All Likelihood</a>"" by Yudi Pawitan. This approach shows how the objective and subjective aspects of probability are related via the distinction between likelihood and probability.</p>

<p>Also, there is an interesting ""meeting of the minds"" when it comes to random<a href=""http://en.wikipedia.org/wiki/Random_effects_model"" rel=""nofollow"">-effects modeling</a>. Take a look at that to see how the two, in practice, can converge in concepts.</p>
",2013-10-29 13:42:17.477
58431,,2,,58415.0,user31668,,,CC BY-SA 3.0,"<p>Based on OPs goal of creating 100 error-dispersed EV values generated from a vector of beta errors:</p>

<p>Assumptions:</p>

<p><strong>Function</strong>: EV($b_{par}$)= $b_{par}$</p>

<p><strong>Uncertainty type</strong>: multiplicative beta-distributed, alpha=1, beta=99</p>

<p><strong>Number of realizations of the error:</strong> n=100</p>

<p>You would do the following:</p>

<p>Create vector of p-values: </p>

<p><code>p&lt;-rbeta(100, shape1=1, shape2=99);</code>
  <code>EV&lt;-p*b_par;</code>
  <code>mean(EV)</code></p>

<p>If this is all you need to do, and if b_par is a constant, then you can take a shortcut and just say that $$\text{mean}(EV) = b_{par}\mathbb{E}[\text{Beta}(1,99)]$$ since expectation is a linear operation.</p>
",2013-10-29 13:50:41.777
58432,23132.0,1,,,,T-statistic of correlation coefficient,<autocorrelation><cross-correlation>,CC BY-SA 3.0,"<p>Hi I am trying to calculate the t-statistic for a correlation coefficient between two vectors $x$ and $y$.</p>

<p>The individual vectors shows signs of autocorrelation. I have made use of the formula:</p>

<p>$\frac{r}{\sqrt{(1-r^2)(n-2)}}$</p>

<p>but I'm not sure this is the correct way of solving the problem.</p>

<p>The correlation between the two vectors has been calculated with the Excel formula for correlation.</p>
",2013-10-29 14:06:15.503
58439,14298.0,1,,,,Why don't asymptotically consistent estimators have zero variance at infinity?,<asymptotics><estimators><consistency>,CC BY-SA 3.0,"<p>I know that the statement in question is wrong because estimators cannot have asymptotic variances that are lower than the Cramer-Rao bound.</p>

<p>However, if asymptotic consistence means that an estimator converges in probability to a value, then doesn't this also mean that its variance becomes 0?</p>

<p>Where in this train of thought am I wrong?</p>
",2013-10-29 15:57:09.100
58440,23115.0,2,,58399.0,,,,CC BY-SA 3.0,"<p>I think I may have found the problem with my argument, i.e. how what i called ""the frequentist interpretation of the bayesian approach"" and viceversa don't really make sense:</p>

<p>The ""frequentist interpretation of the bayesian approach"", as described in my quesion, doesn't make much sense because it says that it assumes the likelihood function (and the prior), but then says ""no matter which data we get [in the hypothetical large set of experiments]"" which is incompatible with the frequentist interpretation of the likelihood function.</p>

<p>The ""bayesian interpretation of the frequentist approach"", is also wrong because it doesn't ensure what I say below. For example, in the frequentist approach, I may well make a measurement and have an emtpy confidence interval for it, which clearly means that it doesn't ensure an a% probability of being right.</p>

<p>So I think I understand it better now. And so it seems that if you want to be as cautious as possible, frequentist is the way to go. But if you don't have much statistical significance, or have good reasons for your prior, bayesian is the way to go.</p>
",2013-10-29 16:13:50.840
58441,23138.0,1,,,,What is my experimental design?,<mixed-model><repeated-measures><experiment-design><multilevel-analysis><ancova>,CC BY-SA 3.0,"<p>I am very new to mixed/multilevel models. I have an experiment where we measured 2 scale variables (varA and varB) in 2 different groups of subjects (control and treatment) at 4 different time points.</p>

<p>I suspect that the relationship between varA and varB is different for the 2 groups (control vs treatment), however it should be roughly consistent across the 4 time points. Researching on the internet I've come to conclusion that I need to model this using a multilevel (mixed effects) model. So, I am interested in the difference between the regression line between varA and varB for the 2 groups, but I want to account for the repeated measures.</p>

<p>What is my experimental design in this case? It's not very clear to me what are the fixed and random effects. Additionally, is it also possible to test if the regression lines are significantly different across the time point? Would that call for a different model? Thanks very much, any help on how I should go about doing this in R/Matlab/SPSS would be greatly appreciated as well!</p>
",2013-10-29 16:27:36.400
58442,19750.0,1,58455.0,,,Variational Bayes vs EP and other message-passing methods,<bayesian><markov-process>,CC BY-SA 3.0,"<p>I am trying to understand the difference between:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Expectation_propagation"" rel=""nofollow"">Expectation Propagation</a> (EP)</li>
<li><a href=""http://en.wikipedia.org/wiki/Variational_Bayesian_methods"" rel=""nofollow"">Variational Bayes</a></li>
</ul>

<p>Wikipedia says:</p>

<blockquote>
  <p>Expectation Propagation differs from other Bayesian approximation approaches such as
  Variational Bayesian methods.</p>
</blockquote>

<p>Why isn't <strong>EP</strong> considered a <strong>Variational Bayes</strong> method? Isn't EP Bayesian, and relies on message-passing to approximate the posterior? What makes a method Variational Bayes?</p>

<p>Also, what about the following methods. Can they be considered Variational Bayes?</p>

<ul>
<li>Sum-product</li>
<li>Mean-field methods</li>
<li>Bethe-Kikuchi approximations</li>
</ul>
",2013-10-29 16:30:49.660
58443,3868.0,1,,,,Estimating ROC/AUC on large data sets?,<roc><large-data><auc>,CC BY-SA 3.0,"<p>Plotting an ROC curve of a classifier compared to cases requires that the data set be sorted first on the classifier score.  I am in a position where I need to calculate ROC on a large data set very quickly and sorting is the bottleneck (even using quicksort in C or F90).  If instead of calculating ROC by thresholding at each case in the data set I instead threshold at every 100 cases then my execution time decreases by orders of magnitude based upon how I can write the code. The result is an ROC curve with let's say 10,000 points instead of 1,000,000.  My tests show that the area under these two curves are the same out to > 5 decimal places.</p>

<p>I would like to use this method but have not ran into anyone trying to speed up calculation in this way.  Most of the lit. is on uses of ROC analysis where the data sets are relatively small and execution time is not an issue, so I have not found anyone else using this method or another to speed calculation by ""thinning"" out the points on the curve.</p>

<p>Has anyone ran into a reference/study that has used or evaluated this or another method for speeding up ROC analysis?  If so, or if you have other thoughts, please share.</p>
",2013-10-29 16:35:36.747
58444,1322.0,1,58450.0,,,How to interpret BIC,<fitting><model><bic>,CC BY-SA 3.0,"<p>I am fitting two different models to the same data. In one model, there is one free parameter for three different experimental conditions. In another model, I fit three free parameters, one for each condition. I do this for 10 subjects in a dataset.</p>

<p>For each subject, the model with fewer free parameters has a higher BIC. But for every single subject, the difference in BIC is roughly the same (about 10). I find this very suspicious, since the BIC values themselves range from ~30 to ~1000.</p>

<p>I have never used BIC before, and would like to say that the model with one free parameter is better. </p>
",2013-10-29 17:15:05.517
58445,23140.0,1,58567.0,,,Hierarchical Bayes Normal-Normal Model,<r><self-study>,CC BY-SA 3.0,"<p>I have the following data for 8 runners in a 100 meter dash:</p>

<pre><code>runner 1 88 91 87 82
runner 2 81 85 78 91
runner 3 75 77 83 81
runner 4 92 89 84 82
runner 5 78 79 84 92
runner 6 89 75 79 83
runner 7 91 89 92 91
runner 8 87 86 88 91
</code></pre>

<p>The ratings represent a performance rating and are normally distributed with unknown mean and unknown variance. Each runner can be considered as a sub-group with a mean and variance.</p>

<p>Any guidance will be highly appreciated</p>
",2013-10-29 17:21:38.730
58446,23144.0,1,58447.0,,,Assessing the power of a normality test (in R),<r><simulation><statistical-power><normality-assumption>,CC BY-SA 3.0,"<p>I want to assess the accuracy of normality tests over different sample sizes in R (I realize that normality tests may be <a href=""https://stackoverflow.com/questions/7781798/seeing-if-data-is-normally-distributed-in-r/7788452#7788452"">misleading</a>). For example, to look at the Shapiro-Wilk test, I'm conducting the following simulation (as well as plotting the results) and would expect that as the sample size increases the probability of rejecting the null decreases:</p>

<pre><code>n &lt;- 1000
pvalue_mat &lt;- matrix(NA, ncol = 1, nrow = n)

for(i in 10:n){
    x1 &lt;- rnorm(i, mean = 0, sd = 1)
    pvalue_mat[i,] &lt;- shapiro.test(x1)$p.value
}   

plot(pvalue_mat)
</code></pre>

<p>My thought would be that as the sample size grows there should be a lower rejection rate, however it seems pretty uniform. I think I am misunderstanding this - any and all thoughts welcome. </p>
",2013-10-29 17:53:42.027
58447,21762.0,2,,58446.0,,,,CC BY-SA 3.0,"<p>You are simulating under the null hypothesis (normal distribution), therefore the rejection rate will tend to the significance level as expected. To assess the power, you need to simulate under any non-normal distribution. There are infinite possibilities/scenarios (e.g. gamma distributions with increasing skewness, t-distribution with decreasing df etc.) to choose from, depending on the scope of your study.</p>
",2013-10-29 18:04:16.460
58451,23147.0,1,,,,how to test most frequent variable between groups?,<correlation><multiple-regression><normality-assumption>,CC BY-SA 3.0,"<p>I have different groups of dolphins, some with offspring, some without. I'll count how many types of vocalizations were emitted by each group, and want to find out if there is a statistical difference between groups with and without offspring.</p>

<p>My dataset is something like:</p>

<pre><code>Group_number    Vocal1  Vocal2  Offspring
Gr01    5   3   0
Gr02    7   3   0
Gr03    4   4   0
Gr04    1   6   0
Gr05    7   9   0
Gr06    6   2   1
Gr07    2   4   1
Gr08    2   6   1
Gr09    9   7   1
Gr10    8   8   1
</code></pre>

<p>I have to do this one vocalization type at a time (I have some tens of types)? Or can I do it all in a single run (like a multiple linear regression)? Someone told me to do a chi-square, but am not sure how to do it, or if it's appropriate. T-student, ANOVA, MANOVA, GLM, what's the pros and cons of each method, supposing my data is normally distributed? And what if it's not? Sorry if it seems a silly and vague question, but maybe someone could point me to a simple tutorial or something.</p>

<p>Thanks in advance!</p>
",2013-10-29 19:18:51.163
58452,14179.0,1,,,,Looking for the 'Elbow' in data,<linear-model><contrasts>,CC BY-SA 3.0,"<p>Subitization is the rapid, accurate enumeration of low-numerosity displays, distinguished from 
counting by a sharp non-linearity in the plot of response times. Below is a representative plot, from Watson, D. G., Maylor, E. A., &amp; Bruce, L. A. M. (2007). Notice that mean enumeration times for displays 1-3 increases roughly linearly, but mean enumeration time for 4 does not follow the linear trend. Some research suggests that the subitization 'limit' is dependent on task conditions and participant working memory.</p>

<p><img src=""https://i.stack.imgur.com/krvVw.gif"" alt=""enter image description here""></p>

<p>I'm looking for a way to test where the elbow is, with the ultimate goal of identifying what a participant's subitization limit is. Currently, my best idea is to do something like repeated polynomial contrasts. Basically, I would test for a quadratic trend in numerosities 1-3, then in numerosities 1-4, etc. I would want to say that I have passed the subitization limit when the quadratic trend becomes significant (adjusting for repeated tests). </p>

<p>That's about the limits of my statistical savvy, though, so I can't evaluate this idea too well. Thoughts?</p>

<p>Thanks in advance.</p>
",2013-10-29 19:23:42.563
58453,20304.0,1,58460.0,,,How to think of reduced dimensions in PCA on facial images (eigenfaces)?,<pca><dimensionality-reduction><image-processing>,CC BY-SA 3.0,"<p>I've been reading up a bit on eigenfaces.  I think I understand the basic concept of it - vectorize a set of facial images then reduce the dimensionality of the images using PCA.  What I don't really understand is the visualization of the lower-dimensional representation of the images. </p>

<p>In the facial images, the number of dimensions is the number of pixels so if you reduce the dimensionality of an image, you reduce the number of pixels.  But then how do you visualize this image? Is it just a much smaller version of the full-dimensional original?  The examples that I have seen do not look like this.  Or do you alternatively make each pixel bigger so that the overall image is the same size as the original?</p>
",2013-10-29 19:29:32.170
58454,22804.0,1,58458.0,,,Find the difference between two vectors and the mean of the result,<r>,CC BY-SA 3.0,"<p>So, with the help of someone here in stats i could solve  most of my problems.</p>

<p>Now i'm stuck at this part:</p>

<p>I have the below code, which worked very well if this values were fixed.</p>

<pre><code>immediate.amput.below.EV
watchful.wait.EV
</code></pre>

<p>Now what they are is vectors. They represent a vector with [n_psa] size.
What i need to do is, i need iterate for n_psa values, i.e for 100 values and find the difference between each  immediate.amput.below.EV[1] - watchful.wait.EV[1] and so on, save the results only as positive numbers, even if the output is a negative number in a new vector, and find the mean of the new vector.</p>

<p>This is what i used to do.</p>

<pre><code>DELTA_COST &lt;- max(immediate.amput.below.EV,watchful.wait.EV) - min(immediate.amput.below.EV,watchful.wait.EV)
mean(DELTA_COST)
</code></pre>
",2013-10-29 19:39:47.533
58455,3580.0,2,,58442.0,,,,CC BY-SA 3.0,"<p>To me VB just means we will approximate posterior by turning the problem into a minimization problem and then changing the solution space. In that sense, EP is a type of VB algorithm, because it does exactly that. When I think of VB, though, the first thing I think of is mean-field, and if I had to guess I would think that whoever wrote that EP is ""different"" from VB might have had mean-field algorithms in mind. </p>

<p><a href=""http://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf"">This source</a>, which I take to be canonical, explicitly classifies all algorithms you mention as VB (i.e. Bethe-Kikuchi, sum-product, EP, and mean-field). </p>
",2013-10-29 19:42:46.853
58456,23149.0,1,,,,Permutation test of meta-analysis : correlation coefficient,<r><correlation><permutation-test><false-discovery-rate>,CC BY-SA 3.0,"<p>I am going to estimate False Discovery Rate using permutation tests. </p>

<p>To my knowledge, several R packages are applicable for multiple testing. </p>

<p>I have several independent datasets for meta-analysis. </p>

<p>I computed a Pearson correlation coefficient, converted it into a Fisher-Z score and then calculated the mean effect size of each gene pair from several independent datasets. </p>

<p>Actually, in each dataset, there are two different subgroups - healthy subjects and patients. In this co-expression network, I analyzed the gene expression profiles of the <em>patient</em> group only.</p>

<p>Here is my question.</p>

<blockquote>
  <p>What should I do to estimate FDR using permutation test with creating random and independent shuffling gene expression values of all genes in each dataset
  to break the inter-gene relationships while keeping intact the expression mean and standard deviation of the genes in every dataset?</p>
</blockquote>

<p>Unfortunately, I have no idea how to do it. I am still new to statistics and as well as R. </p>

<p>I would appreciate it if you could answer the question with corresponding code I could run. </p>
",2013-10-29 19:50:50.240
58476,23165.0,1,,,,Should I standardize my variables for this particular case of cluster analysis?,<clustering><k-means><standardization>,CC BY-SA 3.0,"<p>I'm trying to cluster a list of records based on a (percentage) frequency distribution of variables which add up to 100%. 
Like</p>

<ol>
<li>Record1 - VarA(25%) VarB(25%) varC(50%) varD(0%)</li>
<li>Record2- VarA(50%) VarB(15%) varc(0%) VarD(35%) </li>
</ol>

<p>and so on. 
I have standardized variables before while dealing with different dimensions(lengths and weights) etc. 
In this case I do not think standardization is appropriate. 
Also is k means clustering appropriate in this context, I wanted to use k means and use the distribution observed at the centroid of the cluster for the whole cluster. 
Thanks a ton. </p>
",2013-10-30 05:05:39.567
58649,4656.0,2,,58645.0,,,,CC BY-SA 3.0,"<p>If your datum $\alpha$ is $X$ or $Y$ with
probabilities $\pi_X$ and $\pi_Y = 1 - \pi_X$
respectively, and the probability of mis-classification involves $\pi_X$ and $\pi_Y$, then you are a Bayesian and all you need to do is compare the likelihood
ratio $\displaystyle \frac{p_X(\alpha)}{p_Y(\alpha)}$ (which takes on values 
$0,  \frac{d-c}{b-a}, \infty$ depending on the value of 
$\alpha \in [a,d]$ to an appropriate threshold (which I will leave you to
determine), and this can be reduced to a threshold test on the value of
the datum $\alpha$.</p>
",2013-11-01 13:43:15.560
58457,9554.0,2,,58452.0,,,,CC BY-SA 3.0,"<p>Depending on your definition of the ""elbow"" there are many statistical tests at your disposal. With an entire <a href=""http://cran.r-project.org/web/packages/changepoint/changepoint.pdf"" rel=""nofollow noreferrer"">R package</a> dedicated to this topic.</p>

<p>I personally tend to avoid them, since you never know in advance what will they consider an ""elbow"" and whether your and their opinions will coincide. (but this might be considered an extreme position)
It would also depend whether you want to know if there is an ""elbow"" in a specific location, or whether you want to ask if there is one in general.</p>

<p>For the case of a specific location, you can of course fit a local regression, compare the coefficients and declare one an elbow according to your own rule about the difference in slopes. </p>

<p>The real problem occurs in the latter case. If you have only a couple of points anyway you can just try them all. Otherwise I would fit something non-parametric such as LOESS, calculate the gradient of the line at regular intervals (with sufficient density), such as shown here:
<a href=""https://stackoverflow.com/questions/12183137/calculate-min-max-slope-of-loess-fitted-curve-with-r"">https://stackoverflow.com/questions/12183137/calculate-min-max-slope-of-loess-fitted-curve-with-r</a></p>

<p>and use again some rule that you find convenient to declare something an ""elbow"". I view the ""elbow"" as the case when a large enough change of gradient of a function occurs over a short enough interval. Of course the thresholds for the above rules are a matter of individual taste, which is why there is no test. </p>

<p>In general, I presume this would be quite useless if the data is wiggly (as there would be a lot of changes in the gradient).</p>
",2013-10-29 19:52:16.080
58458,21029.0,2,,58454.0,,,,CC BY-SA 3.0,"<p>To calculate the absolute value in R, use the function <code>abs()</code>. Below is an example for vectors a and b. The sizes can change as long as they have the same dimension, otherwise you will get strange results or errors.</p>

<pre><code>a &lt;- c(1, 4, 8, 20)
b &lt;- c(50, 2, 4, 21)
DELTA_COST &lt;- abs(a-b)

DELTA_COST
[1] 49   2   4  1

mean(DELTA_COST)
[1] 14
</code></pre>
",2013-10-29 20:06:53.583
58459,12544.0,2,,58153.0,,,,CC BY-SA 3.0,"<p>You can do this in R with Lavaan by specifying the model as a structural equation model and adding constraints. I'm not sure if it's a good idea, but it can be done.</p>

<pre><code>#load library and generate some data
library(lavaan)

d &lt;- as.data.frame(matrix(rnorm(1:3000), ncol=3, dimnames=list(NULL, c(""y"", ""x1"", ""x2""))))
</code></pre>

<p>Run it with GLM:</p>

<pre><code>&gt; summary(glm(y ~ x1 + x2, data=d))

Call:
glm(formula = y ~ x1 + x2, data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.6385  -0.5899  -0.0224   0.6024   3.0131  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) -0.01855    0.03021  -0.614    0.539
x1           0.01208    0.03049   0.396    0.692
x2          -0.03676    0.03021  -1.217    0.224

(Dispersion parameter for gaussian family taken to be 0.912437)

    Null deviance: 911.2  on 999  degrees of freedom
Residual deviance: 909.7  on 997  degrees of freedom
AIC: 2751.2
</code></pre>

<p>Then run the same model with lavaan, to check equivalence:</p>

<pre><code>&gt; model1.syntax &lt;- '
+ y ~ x1 + x2
+ '
&gt; summary(sem(model1.syntax, data=d))
lavaan (0.5-14) converged normally after   1 iterations

  Number of observations                          1000

  Estimator                                         ML
  Minimum Function Test Statistic                0.000
  Degrees of freedom                                 0
  P-value (Chi-square)                           1.000

Parameter estimates:

  Information                                 Expected
  Standard Errors                             Standard

                   Estimate  Std.err  Z-value  P(&gt;|z|)
Regressions:
  y ~
    x1                0.012    0.030    0.397    0.691
    x2               -0.037    0.030   -1.219    0.223

Variances:
    y                 0.910    0.041
</code></pre>

<p>In lavaan, you then add constraints, by naming the parameters and adding a constraint section:</p>

<pre><code>&gt; model2.syntax &lt;- '
+ y ~ b1 * x1 + b2 * x2
+ '
&gt; 
&gt; model2.constraints &lt;- 
+   ' 
+     b1 &gt; 0
+     b2 &gt; 0
+   '
&gt; 
&gt; summary(sem(model=model2.syntax, constraints=model2.constraints, data=d))
lavaan (0.5-14) converged normally after   1 iterations

  Number of observations                          1000

  Estimator                                         ML
  Minimum Function Test Statistic                1.484
  Degrees of freedom                                 0
  P-value (Chi-square)                           0.000

Parameter estimates:

  Information                                 Observed
  Standard Errors                             Standard

                   Estimate  Std.err  Z-value  P(&gt;|z|)
Regressions:
  y ~
    x1       (b1)     0.012       NA
    x2       (b2)     0.000       NA

Variances:
    y                 0.911    0.041

Constraints:                               Slack (&gt;=0)
    b1 - 0                                       0.012
    b2 - 0                                       0.000
</code></pre>

<p>Instead of being negative, the b2 parameter is fixed to zero.</p>

<p>Notice that you don't get any standard errors - if you want them, you have to bootstrap. (That's described in the lavaan manual).</p>
",2013-10-29 20:34:39.747
58460,9554.0,2,,58453.0,,,,CC BY-SA 3.0,"<p>Just a hint, after reading your comment. Each image (face) is represented as a stacked vector of length $N$. The different faces make up a dataset stored in a matrix $X$ of size $K\times N$. You might be confused about the fact that you use the PCA to obtain a set of eigenvectors (eigenfaces) $I = \{u_1, u_2, \ldots, u_D\}$ of the covariance matrix $X^TX$, where each $u_i \in \mathbb{R}^{N}$. You don't reduce the number of pixels used to represent a face, but rather you find a small number of eigenfaces that span a space which suitably represents your faces. The eigenfaces still live in the original space though (they have the same number of pixels as the original faces).</p>

<p>The idea is, that you use the obtained eigenfaces as a sort of <a href=""http://en.wikipedia.org/wiki/Archetype"" rel=""nofollow"">archetypes</a> that can be used to perform face detection.</p>

<p>Also, purely in terms of storage costs, imagine you have to keep an album of $K$ faces, each composed of $N$ pixels. Instead of keeping all the $K$ faces, you just keep $D$ eigenfaces, where $D \ll K$, together with the component scores and you can recreate any face (with a certain loss in precision).</p>
",2013-10-29 20:35:34.133
58461,21029.0,2,,58451.0,,,,CC BY-SA 3.0,"<p>First of all, your data is not normally distributed because it takes discrete values. But, if there is enough range in the data, you may say it approximates a normal distribution. It's an assumption you need to verify.</p>

<p>The question is vague because this is a data mining/data analysis question, and many methods will provide you with many (similar but different) results. There are no pros/cons here, since they all test for different things. For the methods you mentioned, here's a brief description:</p>

<ol>
<li>Chi-squared. This will tell you if there is a relation between your groups. I assume the test will be offspring versus all the vocalizations. You may find signficance but not be certain where it comes from. If you find a relation, you have to dig deeper to find the source. Avoid having groups with few observations otherwise this test will not be valid.</li>
<li>(Student) T-test, or a test of equivalence in means. You compare the mean value of two populations, versus the alternative hypothesis that they are not equal. You will need to test each vocalization individually.</li>
<li>ANOVA. You are testing if the variance is explained across different factors (offspring). This needs to be done for each different variable.  The idea is to look at the variance of the continuous variable within each class $s_i$ and compare it to the total variance st. The correlation coefficient for one class compared to the total is then $\nu_i=s_i/s_t$. This test also assumes normality.</li>
<li>MANOVA. This is testing if your qualitative factor (offspring) has an effect on ALL of the other variables. It is a generalization of ANOVA to multiple variables, which sounds appropriate for your study. We still have the normality assumption.</li>
<li>GLM. Modeling can show you the relationships in your data as well, but you need to have a predictor variable defined in advance. (Offspring?)</li>
</ol>

<p>Other ideas: If you are looking to see how your vocalizations are related only to offspring, then try perhaps a discriminant analysis (normality assumption again) or a logistic regression (no normality assumption).</p>
",2013-10-29 20:35:36.090
58507,23176.0,1,,,,Sampling technique to estimate how many toxic waste sites are in a country?,<sampling>,CC BY-SA 3.0,"<p>I work for an environmental health nonprofit and I have moderate expertise in statistics. We want to estimate the total number of toxic industrial waste sites within a small African country. I would love to hear your thoughts on how we should start. At this stage, if you can recommend a book, an idea, or a general sampling method that's all I'm asking for. I basically need a place to start.</p>

<p>For instance, I thought that maybe we could we divide the country into  three zones: low, medium, and high industrial zones (based on UN data). We could then create 100 equal-sized sectors within each zone, and randomly select 10 sectors from each of the three industrial zones. We would then survey these small sectors areas and find all toxic sites within those sectors. If we do this, could we estimate the total number of industrial sites along with a measure of uncertainty? Is there a name for this type of geographic sampling?</p>

<p>There is very little pre-existing data on the number of toxic waste sites. Also, to simplify things, assume it is very easy to identify a toxic waste site once you are on the ground with a team.</p>
",2013-10-30 15:38:39.880
58462,5821.0,1,,,,Weights for retrospective cohort data,<epidemiology><biostatistics><poisson-regression>,CC BY-SA 3.0,"<p>Suppose you randomly identify individuals with a certain disease (cases) at time $T$, then you identify a sample of individuals without disease at time $T$ (but at risk for disease) in a population of individuals undergoing routine screening for that disease. The controls are sampled in a known proportion from the population of interest. Suppose further you are able to retrospectively ascertain all screening information in both groups and you interested in inferring the population level screening rates and the relative risk for disease adjusting for screening rates. Lastly, there are some subject level effects which you believe may mediate screening behavior and disease risk.</p>

<p>A probability model for the screening process is: </p>

<p>$$t_{i,j} \sim \mbox{exponential}(\lambda_s; t_{i,j-1})$$</p>

<p>where an $\mbox{exponential}(\lambda, t)$ random variable is a location shifted exponential distribution with location parameter $t$. So the number of screens that a person utilizes in a fixed time period has a Poisson distribution.</p>

<p>Disease develops according to another approximately exponential process </p>

<p>$$D_i \sim \mbox{exponential} (\lambda_c)$$</p>

<p>and is clinically detectable with screening for a short interval, $\theta_{d}$. </p>

<p>$\lambda_s \gg \lambda_c$ obviously. Disease prevalence is very rare in the population.</p>

<p>Disease status $D_i(t)$ is 1 if the subject has either screen detected or symptomatic disease at time $t$, 0 otherwise.</p>

<p>The time varying subject level effects (age) we denote with $X_{i,t}$, so all $\lambda_c$, $\lambda_s$, and $\theta_d$ are conditionally dependent on $X_{i,t}$.</p>

<p>This is a type of analysis whose name is not familiar to me. I'm inclined to call it ""cluster case control"" since the individual observation is a single screening period. However, we did not sample screens based on case/control status, but individuals. Because of the variability in lead time and individual screening rates, cases screen above the population rate and controls screen below it. Thus it's tempting to use some kind of sampling weight to account for this and obtain an unbiased estimate of the population level screening rate. However, the subject frequency:</p>

<p>$$w_i = \left\{\begin{array}{ccc} 
1 &amp; \mbox{if} &amp; D_{i}(T) = 1 \\
1/p &amp; \mbox{if} &amp; D_{i}(T) = 0 \\
\end{array} \right.$$</p>

<p>does not produce valid weights because of the lead time bias and differential screening (controls are followed for a longer period of time and so accrue more screening instances <em>and</em> person years, while cases are followed for shorter times and still accrue more screening intervals than cases in shorter time periods, yet still not more overall).</p>

<p>Is this even an identifiable problem???</p>
",2013-10-29 20:53:40.900
58463,21346.0,1,58465.0,,,Standard error of marginal effect for binary/categorical variable,<bootstrap><standard-error><delta-method>,CC BY-SA 3.0,"<p>The delta method/bootstrap method is used to obtain the standard error of marginal effect in case of limited dependent variable model (like tobit model). I have seen these being applied for the continuous variables, but my question is whether these can be also applied for categorical variables (say with 4 categories) or binary variables (say only with two categories). Please suggest academic papers if it is relevant to the question.  </p>
",2013-10-29 20:58:37.147
58464,13459.0,1,,,,How to deal with data having huge disparity in number in each class,<logistic><predictive-models><cart>,CC BY-SA 3.0,"<p>I have data in which the number of negative cases in response is approximately 98% of the total sample size (total # records are approximately 1 million, Response is <code>binary</code>). The positive cases are roughly 2%. What are the limitations of applying 'glm' and 'cart'  on such data? What option do I have in such cases?</p>

<p>On test data I did get a very good AUC ~0.92. How much faith should I have in this model considering such a disparity in the number of cases in the positive and negative categories?</p>
",2013-10-29 21:10:26.307
58465,5045.0,2,,58463.0,,,,CC BY-SA 3.0,"<p>Take a look at the formulas on page 324 of TamÃ¡s Bartus' <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0086"" rel=""nofollow"">""Estimation of Marginal Effects Using Margeff""</a> from Stata Journal. The formulas and explanations are not Stata-specific.</p>

<p>Stata now handles these calculations using the <code>margins</code> command.</p>
",2013-10-29 21:43:17.067
58466,20981.0,2,,58464.0,,,,CC BY-SA 3.0,"<p>Is this a binary response variable?</p>

<p>One limitation of models with very small (or large) rates is the amount of data needed to get accurate and stable estimates of variance and sample errors. As a rule of thumb you would want both <code>Np = 5</code> and <code>N(1-p) = 5</code> (or higher), so with an estimated <code>p</code> of 0.02 you need <code>N</code> of 250.  So in order to get accurate sample errors etc you want a minimum 250 observations.</p>
",2013-10-29 22:33:45.173
58467,23150.0,1,58488.0,,,What is the difference in Bayesian estimate and maximum likelihood estimate?,<bayesian><maximum-likelihood>,CC BY-SA 3.0,"<p>Please explain to me the difference in Bayesian estimate and Maximum likelihood estimate?</p>
",2013-10-29 23:15:00.087
58468,594.0,2,,58446.0,,,,CC BY-SA 3.0,"<p>(More than a comment, perhaps not a complete answer)</p>

<blockquote>
  <p>[I] would expect that as the sample size increases the probability of rejecting the null decreases</p>
</blockquote>

<p>Leaving aside considerations of biased tests (which are not uncommon in goodness of fit, so it's worth a mention), there are three situations relating to rejection rate one might want to consider:</p>

<p>1) the rejection rate when simulating from the null (as you seem to be doing in your question)</p>

<p>Here, the rejection rate should be at or near the significance level, so, no, if you hold the significance level constant, the rejection rate doesn't decrease as <em>n</em> increases, but stays at/near $\alpha$.</p>

<p>2) the rejection rate when simulating from some alternative</p>

<p>Here the rejection rate should increase as <em>n</em> increases.</p>

<p>3) the rejection rate for some collection of real data</p>

<p>Practically, the null is never actually true, and real data will have some mixture of amounts of non-normality (as measured by the test statistic). If the degree of non-normality is not related to sample size, the rejection rate should increase as <em>n</em> increases.</p>

<p>So in fact, in none of these situations should we see the rejection rate decrease with sample size.</p>
",2013-10-29 23:27:51.550
58469,,1,,,Andreas Pasternak,Finding the right model,<r><svm>,CC BY-SA 3.0,"<p>I have a data frame with several predictors, lets call them <code>pred1</code> through <code>pred3</code>, and a result column. Now I need to specify the right model. I could randomly try:</p>

<pre><code>svm.model &lt;- svm(result ~ pred1+pred2+pred3,       data = train)
# or
svm.model &lt;- svm(result ~ pred1*pred2+pred3^2,     data = train)
# or
svm.model &lt;- svm(result ~ log2(pred1)+pred2*pred3, data = train)
# etc. etc.
</code></pre>

<p>But there must be a better automatic approach in R to model selection?!</p>
",2013-10-30 00:05:53.997
58477,22792.0,1,,,,Understanding d-separation theory in causal Bayesian networks,<bayesian><causality><graphical-model><bayesian-network>,CC BY-SA 3.0,"<p>I am trying to understand the d-Separation logic in Causal Bayesian Networks. I know how the algorithm works, but I don't exactly understand <strong><em>why</em></strong> the ""flow of information"" works as stated in the algorithm.</p>

<p><img src=""https://i.stack.imgur.com/gPnim.png"" alt=""enter image description here""></p>

<p>For example in the graph above, lets think that we are only given X and no other variable has been observed. Then according to the rules of d-separation, the information flow from X to D:</p>

<ol>
<li><p>X influences A, which is $P(A)\neq P(A|X)$. This is OK, since A causes X and if we know about the effect X, this affects our belief about the cause A. Information flows.</p></li>
<li><p>X influences B,which is $P(B)\neq P(B|X)$. This is OK, since A has been changed by our knowledge about X, the change at A can influence our beliefs about its cause, B, as well.</p></li>
<li><p>X influences C,which is $P(C)\neq P(C|X)$. This is OK because we know that B is biased by our knowledge about its indirect effect, X, and since B is biased by X, this will influence B's all direct and indirect effects. C is a direct effect of B and it is influenced by our knowledge about X.</p></li>
</ol>

<p>Well, up to this point, everything is OK for me since the flow of the information occurs according to intuitive cause-effect relationships. But I don't get the special behavior of so called ""V-structures"" or ""Colliders"" in this scheme. According to the d-Separation theory, B and D are the common causes of C in the graph above and it says that if we did not observe C or any of its descendants, the flow information from X is blocked at C. Well, OK, but my question is <strong><em>why?</em></strong></p>

<p>From the three steps above, started from X, we saw that C is influenced by our knowledge about X and the information flow occurred according to the cause-effect relationship. The d-Separation theory says that we cannot go from C to D since C is not observed. But I think that since we know that C is biased and D is a cause of C, D should be affected as well while the theory says the opposite. I am clearly missing something in my thinking pattern but can't see what it is.</p>

<p>So I need an explanation of why the flow of information blocked at C, if C is not observed.</p>
",2013-10-30 07:03:41.870
58470,5821.0,2,,58467.0,,,,CC BY-SA 3.0,"<p>I think you're talking about point estimation as in parametric inference, so that we can assume a parametric probability model for a data generating mechanism but the actual value of the parameter is unknown.</p>

<p>Maximum likelihood estimation refers to using a probability model for data and optimizing the joint likelihood function of the observed data over one or more parameters. It's therefore seen that the estimated parameters are most consistent with the observed data relative to any other parameter in the parameter space. Note such likelihood functions aren't necessarily viewed as being ""conditional"" upon the parameters since the parameters aren't random variables, hence it's somewhat more sophisticated to conceive of the likelihood of various outcomes comparing two different parameterizations. It turns out this is a philosophically sound approach.</p>

<p>Bayesian estimation is a bit more general because we're not necessarily maximizing the Bayesian analogue of the likelihood (the posterior density). However, the analogous type of estimation (or posterior mode estimation) is seen as maximizing the probability of the posterior parameter conditional upon the data. Usually, Bayes' estimates obtained in such a manner behave nearly exactly like those of ML. The key difference is that Bayes inference allows for an explicit method to incorporate prior information.</p>

<p>Also 'The Epic History of Maximum Likelihood makes for an illuminating read</p>

<p><a href=""http://arxiv.org/pdf/0804.2996.pdf"" rel=""noreferrer"">http://arxiv.org/pdf/0804.2996.pdf</a></p>
",2013-10-30 00:08:32.913
58471,5480.0,1,,,,Clustering microblogs,<clustering>,CC BY-SA 3.0,"<p>I have a microblog dataset with about 100k tweets and I would like to cluster them effectively using some less advanced algorithm. Is there anywhere I can find like a single-pass K-Means or similar algorithms. Thanks.</p>

<p>I have tried using K-Means and it takes about 5 minutes to cluster 100k tweets, using standard pre-processing (tokenization, stemming). I have also used Online LDA, but I can't retrieve tweets from the topics. </p>
",2013-10-30 01:58:09.657
58472,22600.0,1,,,,Mean and Covariance of Office Hours,<self-study><mean><covariance>,CC BY-SA 3.0,"<p>I have a homework assignment with the following question. I've attempted to solve the problems and my answers are below, but I feel like they are incorrect, and have no idea how to check. I would be very grateful for any help you can provide:</p>

<pre><code>Posted office hours notwithstanding, students make calls at the offices of their 
professors and have formed the impression that professors are more likely to be away
from their offices on Friday than any other working day. A review of calls, 
1/5 of which are on Fridays, indicates that for 16% of Friday calls, the professor 
is away from his or her office, while this occurs for only 12% of calls on every 
other working day.  Define two random variables as follows:

X = 1 if call is made on a Friday; otherwise, X = 0

Y = 1 if professor is away from his or her office; otherwise, Y = 0


a.  Find the mean value of X, namely, E(X).

b.  Find the mean value of Y, namely, E(Y).

c.  Find (to four decimal places) and interpret the covariance between X and Y.  
</code></pre>

<hr>

<p>My Answers:</p>

<p><code>A</code> : 0.2(1) = <strong>0.2</strong></p>

<p><code>B</code> : (0.12)0.2 + (0.12)0.2 +(0.12)0.2 +(0.12)0.2 +(0.16)0.2  = <strong>0.128</strong></p>

<p><code>C</code> : E(xy) â€“ E(x)*E(y) :  </p>
",2013-10-30 02:58:31.177
58473,20473.0,2,,58063.0,,,,CC BY-SA 3.0,"<p>This is a digressing answer, but since computers don't do what we want them to do, but only what they tell them to do, I believe that when we can become more specific, we reduce uncertainty (actual or perceived).</p>

<p>The implicit equation that determines $y$ gives a quadratic equation in $y$, with the roots of the quadratic being functions of the parameters (and in fact $y$ is seen to be the logistic cdf).</p>

<p>For comapactness, denote $\ln \bigl(\frac{-y}{y - 1} \bigl)\equiv h$. Then </p>

<p>$$g(\theta) = 0 \Rightarrow -(h-\theta_1)(h-\theta_3) + a\theta_2(h-\theta_3) + (h-\theta_1)b\theta_4 =0$$
$$\Rightarrow -h^2+\theta_3h+\theta_1h-\theta_1\theta_3+a\theta_2h - a\theta_2\theta_3+hb\theta_4-b\theta_1\theta_4=0$$
$$-h^2+(\theta_1+\theta_3+a\theta_2+b\theta_4)h-(\theta_1\theta_3+a\theta_2\theta_3+b\theta_1\theta_4)=0   $$</p>

<p>Set</p>

<p>$$ \phi_1 \equiv\theta_1+\theta_3+a\theta_2+b\theta_4, \;\; \phi_2= \theta_1\theta_3+a\theta_2\theta_3+b\theta_1\theta_4$$</p>

<p>Then the roots of the polynomial $-h^2+\phi_1h-\phi_2=0$ are</p>

<p>$$h^*_A,h^*_B = \frac {-\phi_1 \pm \sqrt {\phi_1^2 -4\phi_2}}{-2}=\frac {\phi_1}{2}\pm\sqrt {\left(\frac {\phi_1}{2}\right)^2-\phi_2}$$</p>

<p>Then we obtain two equations for $y$</p>

<p>$$\ln \bigl(\frac{-y}{y - 1} \bigl) = h^*_A \Rightarrow y_A = \frac 1{1+e^{-h^*_A}}$$
and </p>

<p>$$\ln \bigl(\frac{-y}{y - 1} \bigl) = h^*_B \Rightarrow y_B = \frac 1{1+e^{-h^*_B}}$$</p>

<p>which is the cdf of the logistic distribution, call it $\Lambda_j,\; j=A,B$, and denote $\lambda_j$ the derivative w.r.t its argument, $\lambda_j = \Lambda_j(1-\Lambda_j)$.</p>

<p>Your log-likelihood becomes</p>

<p>$$\ln L = \ln \Bigl(\frac {n!}{k!(n-k)!}\Bigl)+k\ln\Lambda_j + (n-k)\ln(1-\Lambda_j) $$</p>

<p>Now even the Hessian can be calculated by hand (with patience), let alone the gradient. It is also feasible to check concavity of the log-likelihood. Since you have two equations for $y$, you maximize separately and pick the solution that gives the higher value for the log-likelihood.</p>
",2013-10-30 03:55:29.550
58474,23163.0,1,,,,Analyzing repeated rank data.,<ranking>,CC BY-SA 3.0,"<p>I have a data set of N people, T items. Let's say N=100, and T=10. </p>

<p>Each person goes through the following exercise. </p>

<ol>
<li>She is shown 2 random items from the set of T=10, and ranks them as rank 1 and 2. </li>
<li>She is next shown 2 more random items from the remaining 8 out of 10 items, and ranks as rank 1 and 2. </li>
</ol>

<p>At the end, the data set is of size 100x10, where each row has 4 numeric entries (two of which will be 1, and the other two will be 2) and 6 empty entries. </p>

<p>My goal is to compare the 10 items against one another, and come up with an estimated rank value for a given item. </p>

<p>What is the best way to analyze such data ? </p>

<p>Thank you. </p>
",2013-10-30 04:27:36.030
58475,23002.0,1,58498.0,,,"$P(X_1 < \min(X_i,\ldots, X_n))$ across different normal random variables",<probability><normal-distribution><conditional-probability>,CC BY-SA 3.0,"<p>I have a set of mutually independent normal distributions $X_1$ to $X_5$ (with means and standard deviations) which represent finishing times for swimmers over a certain distance. The actual data is as follows:</p>

<p>$$X_1(60, 3.0)$$
$$X_2(61, 1.0)$$
$$X_3(58, 2.3)$$
$$X_4(63, 2.4)$$
$$X_5(61, 1.7)$$
So swimmer 1 ($X_1$) has a mean finishing time of 60 seconds with a standard deviation of 3.0 seconds.</p>

<p>Question 1: What is the probability of an event where $X_i$ finishes first. e.g.</p>

<p>$$P(X_1 \lt X_i, i=2,\ldots,n)$$</p>

<p>Question 2: If I calculate this for all swimmers, can I simply order the results to determine the most probable finishing order?</p>

<p>This is not homework.</p>

<p>Based on the answers to this <a href=""https://stats.stackexchange.com/questions/44139/what-is-px-1x-2-x-1x-3-x-1x-n"">Cross Validated question</a>, I have tried to solve this problem <em>based on</em> the first answer. i.e.</p>

<p>$$\Pr(X_1 \le X_i, i=2,\ldots,n) = \int_{-\infty}^{\infty} \phi_1(t) [1 - \Phi_2(t)]\cdots[1 - \Phi_n(t)]dt$$</p>

<p>Where  $\phi_i$ is the PDF of $X_i$ and $\Phi_i$ is its CDF.</p>

<p>Based on this formula, the results I obtained were:</p>

<p>$$\Pr(X_1 \le X_i, i=2\ldots5) = 0.259653$$
$$\Pr(X_2 \le X_i, i=1,3\ldots5) = 0.214375$$
$$\Pr(X_3 \le X_i, i=1\ldots2, 4\ldots5) = 0.611999$$
$$\Pr(X_4 \le X_i, i=1\ldots3, 5) = 0.0263479$$
$$\Pr(X_5 \le X_i, i=1\ldots4) = 0.0697597$$
However, the probabilities add to 1.182135 when they should add to 1.0. Iâ€™m not sure if the formula is incorrect or my implementation of the integral (I used Excel and the trapezoidal method).</p>

<p>I also attempted to solve the problem using Dillipâ€™s method (from the above mentioned question) as follows:</p>

<p>\begin{align*}
P(X_1 &lt; \max X_i) &amp;=  P\{(X_1 &lt; X_2) \cup \cdots \cup (X_1 &lt; X_n)\\
&amp;\leq \sum_{i=2}^n P(X_1 &lt; X_i)\\
&amp;= \sum_{i=2}^n Q\left(\frac{\mu_1 - \mu_i}{\sqrt{\sigma_1^2 + \sigma_i^2}}\right)
\end{align*}</p>

<p>However, the probability results were much greater than 1 in most cases so abandoned this approach. By the way, what exactly does $\max X_i$ mean?</p>

<p>Any assistance in calculating the probability would be appreciated.</p>
",2013-10-30 05:04:28.233
58508,2149.0,2,,58119.0,,,,CC BY-SA 3.0,"<p>Outliers are a reflection of an unknown/unspecified external factor. If there is a relationship between two series then there would be an increased probability that both series would be affected. My answer to your question is ""yes"" since there may be a relationship between the two series. </p>
",2013-10-30 15:38:58.053
58478,1406.0,2,,58427.0,,,,CC BY-SA 3.0,"<p>Given the information in the question I do not see any problems with such simulation set-up. You are modelling VAR(1) process and then explore the contemporaneous relationship between its components. Perfectly reasonable, although I would double check that this conforms to your empirical model. </p>

<p>As for the second question, the best way would be to figure out the exact covariance structure of the stationary process $\varepsilon_t=y_t-\beta x_t$ and then use GLS instead of OLS with the covariance matrix exactly specified.  This way you would get the efficient estimates of $\beta$ and subsequently the smallest possible standard errors. Another way is to use autocorrelation-robust standard errors. In R you can calculate them using package <strong>sandwich</strong>:</p>

<pre><code>library(lmtest)
library(sandwich)
mod &lt;- lm(vec_y ~ vec_x)
vv &lt;- vcovHAC(mod)
coeftest(mod,vcov=vv)


  t test of coefficients:

              Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) -0.0093830  0.0111428  -0.8421   0.3998    
vec_x        0.5333422  0.0052502 101.5857   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here I used function <code>coeftest</code> from the package <strong>lmtest</strong> to get the usual summary output. Note that for large sample sizes the calculation of robust covariance matrix can take up some time.</p>
",2013-10-30 07:27:33.617
58479,5671.0,2,,58476.0,,,,CC BY-SA 3.0,"<p>As the dimensions are on the same scale and measure the same kind of quantity (relative share), you don't need to standardize / whiten your data.</p>

<p>I'm not convinced that k-means is appropriate here. It will likely work though.</p>

<p>The reason is that since your data is histograms (they sum up to 1), you will likely get much better results with distance functions designed for this type of distributions; i.e. histogram intersection distance, jensen-shannon divergence etc.</p>

<p>Unfortunately, k-means is <em>really</em> designed for squared Euclidean distance (= Variance minimization), and you shouldn't blindly combine it with other measures. Instead, use a k-means variant that will converge with arbitrary distances, such as k-medoids = PAM.</p>
",2013-10-30 08:13:16.343
58480,5671.0,2,,58471.0,,,,CC BY-SA 3.0,"<p>With single-pass algorithms you won't be able to discover useful structure. K-means already is too simple for actual structure discovery; it's merely a vector quantization method.</p>

<p>100k isn't a lot, it should run in a few minutes with full k-means, actually...</p>

<p>Have you experimented with hashing, in particularly LSH/MinHash? These seem to be fairly standard techniques. Please update your question and list what you have already tried.</p>
",2013-10-30 08:15:20.053
58481,22013.0,1,58550.0,,,R package to make a linear discriminant analysis scatter plot,<r><data-visualization><discriminant-analysis>,CC BY-SA 3.0,"<p>Is there a package in <code>R</code> that can do this plot on LDA data 
graph plot: <img src=""https://i.stack.imgur.com/x1wle.jpg"" alt=""enter image description here""></p>

<p>original question: <a href=""https://stats.stackexchange.com/questions/22884/how-does-linear-discriminant-analysis-reduce-the-dimensions"">How does linear discriminant analysis reduce the dimensions?</a></p>
",2013-10-30 08:37:20.010
58482,20062.0,1,58486.0,,,How to best visualize one-sample test?,<r><data-visualization><wilcoxon-signed-rank>,CC BY-SA 3.0,"<p>We are currently writing a paper with several one-sample Wilcoxon tests. While visualizing two-sample tests is easy via <em>boxplots</em>, I was wondering whether there is any <strong>good way to visualize one-sample test results?</strong></p>

<pre><code># Example data
pd &lt;- c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64,
        0.73, 1.46, 1.15, 0.88, 0.90, 0.74, 1.21)

wilcox.test(pd, mu = 1.1)

#   Wilcoxon signed rank test
#
# data:  pd
# V = 72, p-value = 0.5245
# alternative hypothesis: true location is not equal to 1.1
</code></pre>

<p>...and also:</p>

<p>I would like to get Z-value instead of V-value. I know that if I use <code>coin</code> package instead of basic <code>stats</code> I will have z-values, but <code>coin</code> package
seems not to be able perform one-sample Wilcoxon test.</p>
",2013-10-30 08:55:08.630
58483,23168.0,1,,,,Is a mixed/ random effects model required if fixed effects model shows no pattern in the residuals?,<generalized-linear-model><mixed-model><effects>,CC BY-SA 3.0,"<p>I've got some data on discrete flood events (response variable - duration) on several rivers.</p>

<p>I've modelled the response variable by site and time using a Generalized Linear Model:</p>

<pre><code> mod = glm(duration ~ site + time)
</code></pre>

<p>I've then looked at the residual plots and cannot find any evidence for temporal correlation in the residuals. Does this mean that this model approach is valid (and that using a mixed/ random effects model is unnecessary) (or should one always use mixed/ random effects models for time-series data)?</p>

<p>Any links to reading references regarding this subject would be brilliant,</p>
",2013-10-30 09:10:58.437
58484,23169.0,1,58531.0,,,Probability of a given password,<probability><self-study><combinatorics>,CC BY-SA 3.0,"<p>A password consists of 4 alphabet letters and 4 numbers. Calculate the following two probabilities:</p>

<ol>
<li>$p_1$: the probability that the letters are all equal and that the numerical part contains one eight.</li>
<li>$p_2$: the probability that the password has 3 numbers followed by 4 letters.</li>
</ol>

<p>Although it sounds like an easy question, but how would I apply the definition of permutations and combinations here? Here is how I thought of solving it.</p>

<p>$p_1= (1/21)^4*(1/10)*(9/10)^2$</p>

<p>Do I need to calculate all the possible combinations here?</p>

<p>$p_2= 1/\binom{7}{7}=1/7!/(7-7)!= 1/7!$</p>

<p>since we are considering only one case among a permutation of 7 elements over 7 places. </p>
",2013-10-30 09:32:23.277
58485,17180.0,1,,,,Self organizing maps vs. kernel k-means,<clustering><unsupervised-learning>,CC BY-SA 3.0,"<p>For an application, I want to cluster data (potentially high dimensional) and extract probability of belonging to a cluster. I consider at the moment Self organizing maps or kernel k-means to do the job. What are the pros and cons of each classifier for this task? Am-I missing others clustering algorithms that could be performant in this case? </p>
",2013-10-30 09:37:41.590
58486,594.0,2,,58482.0,,,,CC BY-SA 3.0,"<p>Something like this?</p>

<p><img src=""https://i.stack.imgur.com/XWPcm.png"" alt=""One sample boxplot""></p>

<p>Or were you after some interval for the median, like you get with notched boxplots (but suited to a one sample comparison, naturally)?</p>

<p>Here's an example of that:</p>

<p><img src=""https://i.stack.imgur.com/akmpG.png"" alt=""enter image description here""></p>

<p>This uses the interval suggested in McGill et al (the one in the references of <code>?boxplot.stats</code>). One could actually use notches, but that might increase the chance that it is interpreted instead as an ordinary notched boxplot.</p>

<p>Of course if you need something to more directly replicate the signed rank test, various things can be constructed that do that, which could even include the interval for the pseudo-median (i.e. the one-sample Hodges-Lehmann location estimate, the median of pairwise averages). </p>

<p>Indeed, <code>wilcox.test</code> can generate the necessary information for us, so this is straightforward:</p>

<pre><code>&gt; wilcox.test(pd,mu=1.1,conf.int=TRUE)

    Wilcoxon signed rank test

data:  pd
V = 72, p-value = 0.5245
alternative hypothesis: true location is not equal to 1.1
95 percent confidence interval:
 0.94 1.42
sample estimates:
(pseudo)median 
        1.1775 
</code></pre>

<p>and this can be plotted also:</p>

<p><img src=""https://i.stack.imgur.com/YipHs.png"" alt=""boxp with signed rank interval for pseudomedian""></p>

<p>[The reason the boxplot interval is wider is that the standard error of a median at the normal (which is the assumption underlying the calculation based off the IQR) tends to be larger than that for a pseudomedian when the data are reasonably normalish.]</p>

<p>And of course, one might want to add the actual data to the plot:</p>

<p><img src=""https://i.stack.imgur.com/Cqxk3.png"" alt=""same plot with jittered strip chart under the interval""></p>

<hr>

<p><strong>Z-value</strong></p>

<p>R uses the sum of the positive ranks as its test statistic (this is not the same statistic as discussed on the Wikipedia page on the test).</p>

<p>Hollander and Wolfe give the mean of the statistic as $n(n+1)/4$ and the variance as $n(n+1)(2n+1)/24$.</p>

<p>So for your data, this is a mean of 60 and a standard deviation of 17.61 and a z-value of 0.682  (ignoring continuity correction)</p>

<hr>

<p>The code I used to generate the fourth plot (from which the earlier ones can also be done by omitting unneeded parts) is a bit rough (it's mostly specific to the question, rather than being a general plotting function), but I figured someone might want it:</p>

<pre><code>notch1len &lt;- function(x) {
  stats &lt;- stats::fivenum(x, na.rm = TRUE)
  iqr &lt;- diff(stats[c(2, 4)])
  (1.96*1.253/1.35)*(iqr/sqrt(sum(!is.na(x))))
}

w &lt;- notch1len(pd)
m &lt;- median(pd)

boxplot(pd,horizontal=TRUE,boxwex=.4)

abline(v=1.1,col=8)
points(c(m-w,m+w),c(1,1),col=2,lwd=6,pch=""|"")

ci=wilcox.test(pd,mu=1.1,conf.int=TRUE)$conf.int                       #$
est=wilcox.test(pd,mu=1.1,conf.int=TRUE)$estimate

stripchart(pd,pch=16,add=TRUE,at=0.7,cex=.7,method=""jitter"",col=8)

points(c(ci,est),c(0.7,0.7,0.7),pch=""|"",col=4,cex=c(.9,.9,1.5))
lines(ci,c(0.7,0.7),col=4)
</code></pre>

<p>I may come back and post more functional code later.</p>
",2013-10-30 09:59:34.693
58495,503.0,2,,58494.0,,,,CC BY-SA 3.0,"<p>Where did you hear this? The usual reason for preferring the median is that it is less affected by extreme values than the mean. However, it is in general less sensitive to changes in the data. </p>

<p>I ran a tiny example in <code>R</code> </p>

<pre><code>set.seed(1234)
true &lt;- rnorm(1000)
smallerror &lt;- true + rnorm(1000,0,.1)
largeerror &lt;- true + rnorm(1000, 0, 1)
bias &lt;- true + rnorm(1000,1, .5)

mean(true) - mean(smallerror)
quantile(true, .5) - quantile(smallerror, .5)

mean(true) - mean(largeerror)
quantile(true, .5) - quantile(largeerror, .5)
</code></pre>

<p>In this particular case, the mean <em>was</em> more affected than the median. </p>
",2013-10-30 11:19:25.383
58693,594.0,2,,58688.0,,,,CC BY-SA 3.0,"<p>It's a simple matter of playing 'spot the probability function':</p>

<p>$$\pi(s)=e^{-\lambda}\sum^{\infty}_{i=0}\frac{e^{\lambda s}}{e^{\lambda s}}\frac{(\lambda s)^i}{i!}=e^{-\lambda}e^{\lambda s}\cdot\sum^{\infty}_{i=0}e^{-\lambda s}\frac{(\lambda s)^i}{i!}=e^{-\lambda}e^{\lambda s}\cdot 1$$</p>

<p>since the term in the sum is just the sum over the probability function of a Poisson$(\lambda s)$</p>
",2013-11-02 09:44:16.833
58487,22923.0,2,,58408.0,,,,CC BY-SA 3.0,"<p>So if my understanding of @Alecos very insightful analysis is correct, he has 2 points: even if returns are linearly related $\Delta y_t = b\cdot\Delta x_t + \Delta\varepsilon_t$ then</p>

<ol>
<li><p><em>true</em> correlation between $\Delta y_t$ and $\Delta x_t$ can be anything between 0 and 1 depending on noise/signal ratio $var(\Delta \varepsilon_t)/ var(\Delta x_t)$</p></li>
<li><p>because we estimate above true correlation from a finite sample, our estimate can be something different from the true one.</p></li>
</ol>

<p>Now, for point 1 I can object that if noise/signal ratio is big then <em>both</em> correlation and original cointegration will be ""weak"" (not sure if there is a measure of cointegration strength - probably ADF-test p-value?). </p>

<p>So if we know that <em>true</em> correlation between differenced series is ~0 due to high $var(\Delta \varepsilon)$, we probably still can conclude that cointegration between original series is very weak due to same high $var(\varepsilon)$.</p>

<p>Now, the second point probably makes this conclusion less certain - if finite sample estimate of correlation is ~0 this doesn't mean true one is ~0 and thus cointegration may be in place. So the question is how far can sample correlation be from the true one given a sample size :)</p>
",2013-10-30 10:00:27.300
58488,20470.0,2,,58467.0,,,,CC BY-SA 3.0,"<p>It is a very broad question and my answer here only begins to scratch the surface a bit. I will use the Bayes's rule to explain the concepts. </p>

<p>Letâ€™s assume that a set of probability distribution parameters,  $\theta$, best explains the dataset $D$. We may wish to estimate the parameters $\theta$ with the help of the Bayesâ€™ Rule:</p>

<p>$$p(\theta|D)=\frac{p(D|\theta) * p(\theta)}{p(D)}$$</p>

<p>$$posterior = \frac{likelihood * prior}{evidence}$$</p>

<p>The explanations follow:</p>

<p><strong>Maximum Likelihood Estimate</strong></p>

<p>With MLE,we seek a point value for $\theta$ which maximizes the likelihood, $p(D|\theta)$, shown in the equation(s) above. We can denote this value as $\hat{\theta}$. In MLE, $\hat{\theta}$ is a point estimate, not a random variable.</p>

<p>In other words, in the equation above, MLE treats the term $\frac{p(\theta)}{p(D)}$ as a constant and does NOT allow us to inject our prior beliefs, $p(\theta)$, about the likely values for $\theta$ in the estimation calculations.</p>

<p><strong>Bayesian Estimate</strong></p>

<p>Bayesian estimation, by contrast, fully calculates (or at times approximates) the posterior distribution $p(\theta|D)$. Bayesian inference treats $\theta$ as a random variable. In Bayesian estimation, we put in probability density functions and get out probability density functions, rather than a single point as in MLE. </p>

<p>Of all the $\theta$ values made possible by the output distribution $p(\theta|D)$, it is our job to select a value that we consider best in some sense. For example, we may choose the expected value of $\theta$ assuming its variance is small enough. The variance that we can calculate for the parameter $\theta$ from its posterior distribution allows us to express our confidence in any specific value we may use as an estimate. If the variance is too large, we may declare that there does not exist a good estimate for $\theta$.</p>

<p>As a trade-off, Bayesian estimation is made complex by the fact that we now have to deal with the denominator in the Bayes' rule, i.e. $evidence$. Here evidence -or probability of evidence- is represented by:</p>

<p>$$p(D) = \int_{\theta} p(D|\theta) * p(\theta) d\theta$$</p>

<p>This leads to the concept of 'conjugate priors' in Bayesian estimation. For a given likelihood function, if we have a choice regarding how we express our prior beliefs, we must use that form which allows us to carry out the integration shown above. The idea of conjugate priors and how they are practically implemented are explained quite well in <a href=""https://stats.stackexchange.com/questions/58564/help-me-understand-bayesian-prior-and-posterior-distributions/58792#58792"">this post</a> by COOlSerdash.</p>
",2013-10-30 10:07:41.400
58489,15827.0,2,,58482.0,,,,CC BY-SA 3.0,"<p>If you like boxplots, you can as readily show a single boxplot with a line or other reference showing your hypothesised value. (@Glen_b posted an answer with an excellent simple example precisely as I was first writing this.) </p>

<p>It is arguable that boxplots, now very popular, are massively overused for one-sample and two-sample exploration. (Their real value, in my view, is when you are comparing many sets of values, with number of samples or groups or variables more like 10, 30 or 100, and there is a major need to see overall patterns amid a mass of possible detail.) </p>

<p>The key point is that with just one or two samples (groups, variables), you have space on a plot to show much more detail, detail that could be interesting or important for comparison. With a good design, such detail need not be distracting in visual comparison. </p>

<p>Evidently, in most usual versions the box plot suppresses all detail in its box, showing the middle half of the data, except in so far as the position of the median inside the box conveys some information. Depending on the exact rules used, such as the 1.5 IQR convention of showing data points individually if and only if they are 1.5 IQR or more from the nearer quartile, it is even possible that the box plot suppresses most of the detail about the other half of the data. Often, and perhaps even usually, such detail may be irrelevant to something like a Wilcoxon test, but being prepared to see something illuminating in the data display is always a good idea. </p>

<p>A display that remains drastically underused in many fields is the quantile plot, a display of the ordered values against an associated cumulative probability. (For other slightly technical reasons, this cumulative probability is typically not $1/n, \cdots, n/n$ for sample size $n$ but something like $(i - 0.5)/n$ for rank $i$, 1 being the rank of the smallest value.)  </p>

<p>Here are your example data with a reference line added for 1.1. </p>

<p><img src=""https://i.stack.imgur.com/sA7Jl.png"" alt=""enter image description here""></p>

<p>In other examples, key points include </p>

<ul>
<li><p>For two-sample comparisons, there are easy choices between superimposing traces, juxtaposing traces, or using related plots such as quantile-quantile plots. </p></li>
<li><p>The plot performs well over a range of sample sizes. </p></li>
<li><p>Outliers, granularity (lots of ties), gaps, bi- or multimodality will all be shown as or much more clearly than in box plots. </p></li>
<li><p>Quantile plots mesh well with monotonic transformations, which is not so true for box plots. </p></li>
</ul>

<p>Some will want to point out that cumulative distribution plots or survival function plots show the same information, and that's fine by me.  </p>

<p>See W.S. Cleveland's books (details at <a href=""http://store.hobart.com/"" rel=""nofollow noreferrer"">http://store.hobart.com/</a>) for restrained but effective advocacy of quantile plots. </p>

<p>Another very useful plot is the dot or strip plot (which goes under many other names too), but I wanted to blow a small trumpet for quantile plots here. </p>

<p>R details I leave for others. I am focusing here on the more general statistical graphics question, which clearly cuts across statistical science and all software possibilities. </p>

<p>Incidentally, I don't know the background story but the name <code>wilcox.test</code> in R seems a poor choice to me. So, you save on typing two characters, but the name encourages confusion, not least because of past and present people in statistical fields called Wilcox. Lack of justice for Mann and Whitney is another detail. The person being honoured was Wilcoxon. </p>
",2013-10-30 10:11:06.140
58490,18690.0,1,,,,First differences interpretation,<time-series><interpretation>,CC BY-SA 3.0,"<p>I have a small issue regarding first difference models. Suppose we have</p>

<p>$$ \Delta y_{t} = \beta_{0} + \beta_{1} \Delta \log{y_{t-1}} + \Delta u_{t} $$</p>

<p>The interpretation of the delta would be the same as if I did with:</p>

<p>$$ y_{t} = \beta_{0} + \beta_{1} \log{y_{t-1}} + u_{t} $$</p>

<p>That is correct right?</p>

<p>However if we have:</p>

<p>$$ \Delta y_{t} = \beta_{0} + \beta_{1} \log{p} + u_{t} $$</p>

<p>Then the interpretation would be if p increases by 1%, the increase in $y_{t}$ would be $\beta_{1}$ right?</p>

<p>Thanks in advance for clearing things up!</p>
",2013-10-30 10:14:20.797
58491,23171.0,1,,,,"Show that the best mean square estimator of $X$ given $(X_{1},...,X_{n})$ is $\hat X =E[X|\sigma(X_{1},...,X_{n})]$",<self-study><conditional-probability><stochastic-processes><conditional-expectation>,CC BY-SA 3.0,"<p>Let $X$ and $X_{i}$,  $i=1,...,n$ be random variables on a probability space $(\Omega , \mathcal F,P)$. Show that the best mean square estimator of $X$ given $(X_{1},...,X_{n})$ is  $\hat X =E[X|\sigma(X_{1},...,X_{n})]$</p>
",2013-10-30 10:37:22.313
58492,12683.0,2,,58464.0,,,,CC BY-SA 3.0,"<p>For logistic regression there's no problem with imbalanced samples per se, though if the absolute number in either response class is small for separate covariate patterns the maximum-likelihood estimators of odds ratios can be rather too biased for comfort, and some prefer to used penalized-likelihood methods as discussed <a href=""https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression/68917"">here</a>. For larger models computational constraints might necessitate down-sampling the most common class, which reduces the precision of all estimates somewhat but otherwise only affects the intercept (an estimate of which for the original population can be recovered &mdash;<a href=""https://stats.stackexchange.com/questions/67903/does-down-sampling-change-logistic-regression-coefficients/68726"">see here</a>). In your case a minority class numbering 20k shouldn't give cause for concern unless you're trying to estimate odds ratios for some very rare predictor categories.</p>

<p>For classification trees there's what seems to be a good answer <a href=""https://stats.stackexchange.com/questions/28029/training-a-decision-tree-against-unbalanced-data"">here</a>.</p>
",2013-10-30 10:50:44.987
58493,23172.0,1,,,,Correlation of one binary input series and continous response series,<time-series><arma><binary-data><cross-correlation>,CC BY-SA 3.0,"<p>I have one input series of type 0/1 (it is an intervention time series) and some metric response series. I need to have a look at cross correlations. Normally I would use Box-Jenkins-technique. But now I have to prewhiten an input series which is binary. Can I handle it like a metric one? Does it make sense to look at the autocorrelations and fit an ARMA-model in the ""normal"" way?</p>

<p>Or would it be possible to fit ARMA-models for the metric response variables and then fit the same model to the input variable and then look at the cross correlations?</p>

<p>Nearly all of my response series are white noise - but as I understand I have to fit an ARMA-model to the input if this is not white noise anyway. Is this correct or is it sufficiently for the response to be white noise?</p>

<p>Would be very happy if anyone can help me!
Thank you very much.</p>
",2013-10-30 11:01:36.227
58494,6805.0,1,58527.0,,,When is the median more affected by sampling error than the mean?,<distributions><sampling><mean><robust><median>,CC BY-SA 3.0,"<p>I'm writing a paper on making probability estimates, and it's been asserted to me that I should take the median of the estimates given by my participants, rather than the mean. I've been told I should do this because the mean is more affected by sampling error than the median.</p>

<p>Why is this? Is this something that is always true, or which only holds under certain circumstances?</p>
",2013-10-30 11:09:20.073
58506,23057.0,2,,57403.0,,,,CC BY-SA 3.0,"<p>Mantel test and Moran's I refer to two very different concepts.  </p>

<p>The reason for using Moran's I is the question of spatial autocorrelation: correlation of a variable with itself through space. One uses Moran's I when wants to know to which extent the occurrence of an event in an areal unit makes more likely or unlikely the occurrence of an event in a neighboring areal unit. In other words (using your example): if there is a noisy crow on a tree, how likely or unlikely are there other noisy crows in the neighborhood? The null hypothesis for Moran's I is no spatial autocorrelation in the variable of interest.  </p>

<p>The reason for using the Mantel test is the question of similarities or dissimilarities between variables. One uses the Mantel test when wants to know whether samples that are similar in terms of the predictor (space) variables also tend to be similar in terms of the dependent (species) variable. To put it simply: Are samples that are close together also compositionally similar and are samples that are spatially distant from each other also compositionally dissimilar? Using your example: it tests whether quiet crows are located near other quiet crows, while noisy crows have noisy neighbors. The null hypothesis is no relationship between spatial location and the DV.<br>
Besides this, the partial Mantel test allows comparing two variables while controlling for a third one.<br>
For example, one needs the Mantel test when compares </p>

<ul>
<li>Two groups of organisms, which form the same set of sample units;</li>
<li>Community structure before and after disturbance;</li>
<li>Genetic/ecological distance and geographic distance.</li>
</ul>

<p><a href=""http://www.jstor.org/stable/1400528"" rel=""noreferrer"">Here</a> is a good discussion on the Mantel test and its application.</p>

<p>(Edited in response to Ladislav Nado's new examples)</p>

<p>If I may guess, the reason for your confusion is that you keep thinking of space and noise in your examples either as of two continuous variables, or as of one distance matrix (position in space) and one continuous variable (noise). In fact, to analyze similarities between two such variables, one should think of <em>both</em> of them as <em>distance matrices</em>. That is: </p>

<ul>
<li>one matrix (for example, for space) describes the differences for each pair of geographic coordinates. Value for 2 crows sitting next to each other is lower than the value for crows sitting far apart;</li>
<li>another matrix (for environmental, genetic, or any other structure) describes the differences between measured outcomes at given points. The value for 2 crows with a similar level of noise (it doesn't matter if they are quiet or noisy--it's just a measure of similarity!) is lower than the value for a pair of crows with dissimilar levels of noise.</li>
</ul>

<p>Then the Mantel test computes the cross-product of the corresponding values in these two matrices. Let me underline again that <em>the Mantel statistic is the correlation between two distance matrices and is not equivalent to the correlation between the variables</em>, used to form those matrices. </p>

<p>Now let's take two structures you showed in pictures A and B.<br>
In picture A, the distance in each pair of crows corresponds to similarities in their level of noise. Crows with small differences in their level of noise (each quiet crow vs. another quiet crow, each noisy crow vs. another noisy crow) stay close, while each and every pair of crows with big difference in their level of noise (a quiet crow vs. a noisy crow) stay away from each other. The Mantel test correctly shows that there is a spatial correlation between the two matrices.<br>
In picture B, however, the distance between crows does <em>not</em> correspond to the similarities in their level of noise. While all noisy crows stay together, quiet crows may or may not stay close. In fact, the distance in some pairs of dissimilar crows (one quiet+one noisy) is smaller than the distance for some pairs of similar crows (when both are quiet).<br>
There is no evidence in picture B that if a researcher picks up two similar crows at random, they would be neighbors. There is no evidence that if a researcher picks up two neighboring (or not so distant) crows at random, they would be similar. Hence, the initial claim that <code>On both plots the hypothesis valid</code> is incorrect. The structure as in picture B shows no spatial correlation between the two matrices and accordingly fails the Mantel test.</p>

<p>Of course, different types of structures (with one or more clusters of similar objects or without clear cluster borders at all) exist in reality. And the Mantel test is perfectly applicable and very useful for testing what it tests. If I may recommend another good reading, <a href=""http://horizon.documentation.ird.fr/exl-doc/pleins_textes/pleins_textes_6/b_fdi_47-48/010010612.pdf"" rel=""noreferrer"">this article</a> uses real data and discusses Moran's I, Geary's c, and the Mantel test in quite simple and understandable terms.</p>

<p>Hope everything is slightly more clear now; though, I can expand this explanation if you feel like there is still something missing.</p>
",2013-10-30 15:07:43.040
58725,7155.0,2,,58669.0,,,,CC BY-SA 3.0,"<p>The obvious answer is to pick the polynomial order with the best cross-validation.</p>

<p>A slight improvement is to use something like Gaussian Processes, which tend to do the right thing without tuning parameters.</p>

<p>If you don't need function approximation and you're trying to lump them into groups, you could do perform some kind of kernel-based clustering, like Spectral Clustering. The use of a kernel is crucial here because you can confer to the algorithm exactly what it is that makes these observations similar.</p>
",2013-11-02 21:28:52.147
58496,20473.0,2,,58119.0,,,,CC BY-SA 3.0,"<p><strong>NARROW VIEW 1</strong><br>
A narrow view of this question (since the other answers and comments I believe have covered adequately various general approaches) is ""Assume two random variables $X$ and $Y$ are dependent. Does the variance of $X$ conditional on $Y$ is a function of the variance of $Y$?</p>

<p>To take refuge in the normal distribution, if both $X$ and $Y$ are normal and dependent, denote $\sigma^2_x$, $\sigma^2_y$, $\sigma_{xy}$ their unconditional variances and their covariance respectively. Then </p>

<p>$$\operatorname {Var}(X\mid Y) = \sigma^2_x-\frac {\sigma_{xy}^2}{\sigma^2_y} $$</p>

<p>which is increasing in $\sigma^2_y$. Note that the direction of their covariance (positive/negative) doesn't matter.</p>

<p>But also, note that the conditional variance is <em>lower</em> than the unconditional variance...</p>

<p><strong>NARROW VIEW 2</strong><br>
A 2nd narrow view of the matter is: consider pairs of random variables $(X_i,Y_i),\; i=1,...,n$, and $\sigma^2_x(i)$ and $\sigma^2_y(i)$ their unconditional variances. Assume that for some indices $k,j \in [1,...,n]$, we have $\sigma^2_y(k) &gt; \sigma^2_y(j), \forall j\neq k$.<br>
Should we ""expect"" that $\sigma^2_x(k) &gt; \sigma^2_x(j), \forall j\neq k$ also?<br>
This formalization of the OP's question requires from us to consider directly the dependence of each $(X_i,Y_i)$ on a common source, say a vector of random variables $\mathbf Z_i$,  So we have</p>

<p>$$X_i = h_i(\mathbf Z_i),\;\; Y_i = g_i(\mathbf Z_i)$$</p>

<p>Then we ask ""does (or when)
$$ \operatorname {Var}[g_k(\mathbf Z_k)]&gt;\operatorname {Var}[g_j(\mathbf Z_j)] \Rightarrow ? \operatorname {Var}[h_k(\mathbf Z_k)]&gt;\operatorname {Var}[h_j(\mathbf Z_j)],\; \forall j\neq k$$</p>

<p>To be able to tell something about such an inequality, it must be the case that at least some of the variables that appear in $Z_j$, must also appear in the other $Z_i$'s: then as a necessary condition, we do not only require that the elements of each pair of $(X_i,Y_i)$ rv's are dependent -we require also that the pairs are dependent between them: good-bye i.i.d samples...</p>
",2013-10-30 11:28:40.740
58497,22262.0,1,,,,Pearson correlation between discrete variable that's mostly 0 and a standard normal variable,<correlation><bias><pearson-r>,CC BY-SA 3.0,"<p>Suppose I want to estimate the correlation between $X\sim N(0,1)$ and $Y$, where $Y \in \{-1,0,1\}$ and is equal to zero for 99 per cent of the sample. Sample size is 10 million.</p>

<p>What are the properties of the Pearson correlation in this instance (bias, usefulness, etc)? Would an alternative correlation estimator be better (e.g. a rank correlation)?</p>
",2013-10-30 11:41:41.313
58498,21638.0,2,,58475.0,,,,CC BY-SA 3.0,"<p>I always find it best in these situations to run a Monte Carlo simulation to check (roughly) what the correct answer should be. Here is some <code>R</code> code for doing that:</p>

<pre><code>doRace &lt;- function()
{
  times &lt;- rnorm(mean=c(60,61,58,63,61),sd=c(3,1,2.3,2.4,1.7),n=5)
  winner &lt;- which.min(times)
  winner
}

winners &lt;- replicate(n=10000,expr=doRace())
table(winners) / length(winners)
</code></pre>

<p>Which gives the following output for me (of course you will get slightly different answers depending on the state of your random number generator):</p>

<pre><code>winners
 1      2      3      4      5 
0.2573 0.0317 0.6108 0.0282 0.0720
</code></pre>

<p>This indicates that the issue is with swimmer 2, as these results otherwise agree well with yours. I suspect you just have an incorrect cell reference somewhere. <strong>Note that a reasonable resolution to the problem is to use the Monte Carlo simulation not just as a verification method but as the final implementation for calculating the probabilities. After all, numerical integration is itself an approximate and computationally expensive procedure.</strong></p>

<p>In order to be absolutely sure, we can use the <code>integrate()</code> function in <code>R</code>. First define the integral:</p>

<pre><code>integral&lt;- function(x,whichSwimmer)
{
  means &lt;- c(60,61,58,63,61)
  sds &lt;- c(3,1,2.3,2.4,1.7)

  dnorm(x,mean=means[whichSwimmer],sd=sds[whichSwimmer]) *
    (1 - pnorm(x,mean=means[-whichSwimmer][1],sd=sds[-whichSwimmer][1])) *
    (1 - pnorm(x,mean=means[-whichSwimmer][2],sd=sds[-whichSwimmer][2])) *
    (1 - pnorm(x,mean=means[-whichSwimmer][3],sd=sds[-whichSwimmer][3])) *
    (1 - pnorm(x,mean=means[-whichSwimmer][4],sd=sds[-whichSwimmer][4]))
}
</code></pre>

<p>Then we can calculate the probability for each swimmer in turn:</p>

<pre><code>&gt;integrate(integral,whichSwimmer=1,lower=0,upper=100)
0.2596532 with absolute error &lt; 2.5e-05

&gt;integrate(integral,whichSwimmer=2,lower=0,upper=100)
0.03223977 with absolute error &lt; 6.4e-06

&gt;integrate(integral,whichSwimmer=3,lower=0,upper=100)
0.6119995 with absolute error &lt; 1.5e-06

&gt;integrate(integral,whichSwimmer=4,lower=0,upper=100)
0.02634785 with absolute error &lt; 1.4e-06

&gt;integrate(integral,whichSwimmer=5,lower=0,upper=100)
0.06975967 with absolute error &lt; 8.1e-05
</code></pre>

<p>Which gives very good agreement with the Monte Carlo simulation.</p>

<p>Note that although you can technically give <code>lower</code> and <code>upper</code> bounds of negative/positive infinity to <code>integrate()</code> I found that this caused the procedure to break down, giving clearly incorrect results.</p>

<p>EDIT: I've just noticed you had a second question regarding the most likely ordering of swimmers. Again, we can easily check whether the intuition about just ordering the probability of each swimmer winning is correct by running a Monte Carlo simulation. We just need to adapt the sampling function to return the order of the swimmers rather than only the winner:</p>

<pre><code>doRace &lt;- function()
{
  times &lt;- rnorm(mean=c(60,61,58,63,61),sd=c(3,1,2.3,2.4,1.7),n=5)
  finishOrder &lt;- order(times)

  paste(finishOrder,collapse="""")
}

finishOrders &lt;- replicate(n=1e6,expr=doRace())
which.max(table(finishOrders) / length(finishOrders))
</code></pre>

<p>I get the ouput:</p>

<pre><code>31254 
   50
</code></pre>

<p>In other words, the most likely order is $3,1,2,5,4$ which is not the same as ordering the swimmers by their probability of winning!</p>

<p>For me, this is another reason to prefer the Monte Carlo approach as the final implementation as you can easily answer this and other questions - e.g. what is each swimmer's probability of finishing <em>second</em> or, given that swimmer $1$ finishes first, what is the most likely ordering of the remaining swimmers?</p>

<p>EDIT 2: To be able to answer these other questions, we need to change the sampling function again, this time to return the complete order in which the swimmers finish:</p>

<pre><code>doRace &lt;- function()
{
  times &lt;- rnorm(mean=c(60,61,58,63,61),sd=c(3,1,2.3,2.4,1.7),n=5)
  finishOrder &lt;- order(times)

  finishOrder
}

finishOrders &lt;- replicate(n=1e6,expr=doRace())
</code></pre>

<p><code>finishOrders</code> is a matrix where each column corresponds to a single simulated race, the first row gives the winner of each race, the second row the second placed swimmer of each race and so on. So, to get the probability that each swimmer finishes second we do:</p>

<pre><code>&gt; table(finishOrders[2,]) / ncol(finishOrders)

       1        2        3        4        5 
0.271749 0.198205 0.235460 0.075165 0.219421
</code></pre>

<p>To find the most likely order given that swimmer $1$ wins the race is a little more fiddly. First, extract all races where the first row is equal to $1$:</p>

<pre><code>finishOrdersWhen1WinsRace &lt;- finishOrders[,finishOrders[1,]==1]
</code></pre>

<p>Then turn the finish order of each race from a vector of numbers into a character string so we can use the <code>table</code> function to find the most frequent one:</p>

<pre><code>&gt; which.max(table(apply(finishOrdersWhen1WinsRace,2,paste,collapse="""")))
13254 
    8
</code></pre>

<p>In other words, given that swimmer $1$ wins the race, the most likely order of the remaining swimmers is $3,2,5,4$, which occurs:</p>

<pre><code>&gt; max(table(apply(finishOrdersWhen1WinsRace,2,paste,collapse="""")) / ncol(finishOrdersWhen1WinsRace))
[1] 0.2341227
</code></pre>

<p>$23.4\%$ of the time.</p>

<p>I'm not sure whether a Monte Carlo approach is the only way to answer these questions but it seems likely that even if you can obtain closed-form expressions for the probabilities you'll need to rely on numerical integration like you did to find the winning probabilities.</p>
",2013-10-30 11:42:35.240
58499,503.0,2,,58497.0,,,,CC BY-SA 3.0,"<p>There are a couple issues here:</p>

<p>First, the Pearson correlation would be assuming that both variables are continuous. This is not the case here. </p>

<p>Second, when 99% of one variable takes on only 1 value, correlation measures will be limited.</p>

<p>e.g</p>

<pre><code>set.seed(1234)
x &lt;- rnorm(10000)
y &lt;- cut(x, breaks = c(-100,-3,3,100), labels = FALSE)
boxplot(x~y)
cor(x,y, method = ""spearman"")
</code></pre>

<p>The boxplot shows the perfect relationship, but the correlation is 0.09 (and Spearman's correlation is 0.17). This doesn't seem useful.</p>

<p>What is it you are trying to do, in non-statistical terms? </p>
",2013-10-30 11:59:51.077
58500,7333.0,2,,58477.0,,,,CC BY-SA 3.0,"<p>Let's forget about X for a moment and consider just the collider of B, C and D. The reason that the v-structure can block the path between B and D is that, in general, if you have two independent random variables (B and D) that affect the same outcome (C), then knowing the outcome can allow you to draw conclusions about the relationship between the random variables, thus allowing for information flow.</p>

<p>Using an example from Pearl's book on causality, let C be the observation that the lawn is wet, D the event that it rained, and B the event that the sprinkler was on. Then if you don't know whether the lawn is wet or not, C and D are clearly independent (no information flows). But if you do know that the lawn is wet, then if it rained, it's less likely that the sprinkler was on ($P(B|D) \neq P(B)$) and if the sprinkler was on, it's less likely that the wet grass was caused by the rain ($P(D|B) \neq P(D)$). Hence, knowing that the lawn is wet unblocks the path and makes B and D dependent.</p>

<p>To understand this better, it might be useful to have a look at <a href=""http://en.wikipedia.org/wiki/Berkson%27s_paradox"" rel=""noreferrer"">Berkson's Paradox</a>, which describes the same situation.</p>
",2013-10-30 12:02:11.747
58501,2071.0,2,,58453.0,,,,CC BY-SA 3.0,"<p>To clarify a bit more: in your original high-dimensional space, pixels are the dimensions. In the new space, each image is represented as a linear combination of a relatively small number of basis images, the eigenfaces. So in the new space, the eigenfaces are the dimensions.</p>
",2013-10-30 13:09:12.900
58502,22637.0,1,58504.0,,,Understanding Sufficient Statistics,<sufficient-statistics>,CC BY-SA 3.0,"<p>As I began my study of sufficient statistics I stumbled upon a definition that puzzled me. The conditional probability distribution of the sample values given an estimator $\hat{\Theta}=\hat{\theta} $ is given by </p>

<p>$$ f\left( x_1,x_2,\ldots,x_n|\hat{\theta} \right) = \frac{f \left(x_1,x_2,\ldots,x_n,\hat{\theta} \right)}{g\left( \hat{\theta} \right)}=\frac{f\left( x_1,x_2,\ldots,x_n \right) }{g \left(\hat{\theta} \right)} $$</p>

<p>The first equality is of course the definition of the conditional distribution $P \left(A| B \right) = \frac{P\left( A \cap B \right)}{P \left( B \right)} $. What I do not understand is where the numerator in the second equality comes from. </p>

<p>It looks like we are assuming that $A \subset B \Rightarrow A \cap B =A $. But how is that possible in our case? Any insight on that? Thank you!</p>
",2013-10-30 13:25:39.063
58503,23173.0,1,,,,Test if a variable is a good predictor of a transition to a state,<time-series><hypothesis-testing><vector-autoregression><hidden-markov-model>,CC BY-SA 3.0,"<p>I have a dataset of the wealth of 10 different countries in the world since 1800, one data point per year. </p>

<p>Let's say I have noticed that when the wealth of a country goes above $1,000,000, this country goes from state A to state B (no matter what those states really mean).</p>

<p>What kind of statistical test can I use to try to test the hypothesis that wealth above $1,000,000 is a good predictor of the transition to state B, and in particular how do I test this hypothesis against an alternative hypothesis that it's not wealth, but population size for example that matters to make the transition?</p>
",2013-10-30 14:06:37.333
58504,,2,,58502.0,user31668,,,CC BY-SA 3.0,"<p>In short, the value of a statistic is completely determined by the observed data, so given the values for $x_i$, the probability that the sufficient statistic takes a particular value is guaranteed, as $\hat \theta = h(x_i)$ for some function h. </p>

<p>Theoretically, a sufficient statistic ""encapsulates"" the information in your data about a particular parameter, so the conditional distribution of the data should no longer depend on the parameter you have estimated. In actual calculations, $\theta$ will drop out of your final formula.</p>
",2013-10-30 14:08:30.877
58505,23175.0,1,,,,Estimating the effects of cumulative odds ratios - additive or exponential?,<logistic><odds-ratio><logarithm>,CC BY-SA 3.0,"<p>I have run some regression models that examine the effect of variable X (a 15 point continuous scale) on Y (a binary variable - not perform/perform a particular behaviour) using logistic regression.</p>

<p>For the sake of this example, letâ€™s say that the OR in one of the models is 1.08. I can argue that for each extra exposure on scale X, the respondents are 8% more likely to do Y. So far, so good.</p>

<p>The average score on the X scale is 7. I originally considered that somebody with an average score on the X scale (7) was 56% more likely to do Y (7 x 8%) than somebody with a 0 score on the X scale. However, I have been told that this is incorrect â€“ that the correct calculation is 1.08 to the power of 7 (= 1.71), or in other words that they are 71% more likely to do Y if they have an average score on X compared to somebody who has a 0 score on X. </p>

<p>Cam anybody confirm (1) that this is correct and (2) provide a good reference for this?</p>

<p>Thanks!</p>
",2013-10-30 15:07:26.137
58509,8958.0,2,,58485.0,,,,CC BY-SA 3.0,"<p>This has the potential to be an interesting question. Clustering algorithms perform 'well' or 'not-well' depending on the topology of your data and what you are looking for in that data. Â¿What do you want the clusters to represent? I attach a diagram which sadly does not include kernel k-means or SOM but I think it is of great value for understanding the grave differences between the techniques. You probably need to ask and respond this to yourself before you dig in to measuring the ""pros"" and ""cons"". </p>

<p><img src=""https://i.stack.imgur.com/ssjyq.png"" alt=""clustering techniques""> <a href=""http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html"" rel=""nofollow noreferrer"">This</a> is the source of the image.</p>
",2013-10-30 15:49:08.193
58510,22200.0,1,58615.0,,,Discriminant analysis for the validation of cluster analysis,<clustering><discriminant-analysis>,CC BY-SA 3.0,"<p>I did a discriminant analysis for the validation of my cluster analysis. The cluster analysis is based on a PCA, so I used the components as the independant variables in the discriminant analysis. My question is: is there a restriction considering the number of the independant variables (components) compared to the number of my (cluster) groups?</p>

<p>Thanks a lot!</p>
",2013-10-30 16:00:04.767
58511,19750.0,1,,,,"In message-passing methods, what is the actual content of the messages?",<markov-process><inference><graphical-model>,CC BY-SA 3.0,"<p>In message-passing methods, factors and random variables exchange messages that typically encode marginals, but as much as I look at their formulas, I still don't understand what those messages actually look like.</p>
<p>For example, in <a href=""http://en.wikipedia.org/wiki/Belief_propagation"" rel=""nofollow noreferrer"">belief propagation</a>, when using the factor graph representation, we have the following scheme:</p>
<ul>
<li><p>Messages <strong>from a variable</strong> node <span class=""math-container"">$v$</span> <strong>to a factor</strong> node <span class=""math-container"">$u$</span></p>
<p><span class=""math-container"">$\forall x_v\in Dom(v),\; \mu_{v \to u} (x_v) = \prod_{u^* \in N(v)\setminus\{u\} } \mu_{u^* \to v} (x_v).$</span></p>
</li>
<li><p>Messages <strong>from a factor</strong> node <span class=""math-container"">$u$</span> <strong>to a variable</strong> node <span class=""math-container"">$v$</span> are the product of the factor with messages from all other nodes, marginalised over all variables except the one associated with <span class=""math-container"">$v$</span>:</p>
<p><span class=""math-container"">$\forall x_v\in Dom(v),\; \mu_{u \to v} (x_v) = \sum_{\mathbf{x}'_u:x'_v = x_v } f_u (\mathbf{x}'_u) \prod_{v^* \in N(u) \setminus \{v\}} \mu_{v^* \to u} (x'_{v^*}).$</span></p>
</li>
</ul>
<p>I understand that these messages represent marginals and that the eventually we end up with the marginals in every node/factor, but my question is:</p>
<ul>
<li>Are these messages  <strong>vectors</strong> holding real numbers?</li>
<li>What is the <strong>dimensionality</strong> of those vectors?</li>
<li>What does this dimensionality <strong>represent</strong>?</li>
</ul>
<p>To make things more concrete let's assume a distribution over <strong>real</strong> random variables (e.g. gaussian), although I am also interested in the discrete case (e.g. categorical random variables).  Also, how are these messages initialized?</p>
",2013-10-30 16:05:23.563
58512,668.0,2,,58446.0,,,,CC BY-SA 4.0,"<p><strong>Understanding power analysis of statistical hypothesis tests can be enhanced by carrying some out and looking closely at the results.</strong></p>

<hr>

<p><strong>By design, a test of size <span class=""math-container"">$\alpha$</span> is <em>intended</em> to reject the null hypothesis with a chance of at least <span class=""math-container"">$\alpha$</span> when the null is true (its expected <em>false positive rate</em>).</strong>  When we have the ability (or luxury) of choosing among alternative procedures with this property we would prefer those that (a) actually come close to the nominal false positive rate and (b) have relatively higher chances of rejecting the null hypothesis when it is not true.</p>

<p>The second criterion requires us to stipulate <em>in what way(s)</em> and <em>by how much</em> the null fails to be true.  In textbook cases this is easy, because the alternatives are limited in scope and clearly specified.  With distribution tests like the Shapiro-Wilk, the alternative are much more vague: they are ""non-normal.""  When choosing among distribution tests, then, the analyst is likely to have to conduct their own one-off power study to assess how well the tests work against more specific alternative hypotheses that are of concern in the problem at hand.</p>

<p><strong>An example</strong> motivated by Michael Mayer's answer posits that the alternative distribution may have qualities similar to those of the family of Student t distributions.  This family, parameterized by a number <span class=""math-container"">$\nu\ge 1$</span> (as well as by location and scale) includes in the limit of large <span class=""math-container"">$\nu$</span> the Normal distributions.</p>

<p>In either situation--whether evaluating the actual test size or its power--<strong>we must generate independent samples from a specified distribution, run the test on each sample, and find the rate at which it rejects the null hypothesis.</strong>  However, there is more information available in any test result: its P-value.  By retaining the set of P-values produced during such a simulation, we can later assess the rate at which the test would reject the null for <em>any</em> value of <span class=""math-container"">$\alpha$</span> we might care about.  The heart of the power analysis, then, is a subroutine that generates this P-value distribution (either by simulation, as just described, or--occasionally--with a theoretical formula).  Here is an example coded in <code>R</code>.  Its arguments include</p>

<ul>
<li><p><code>rdist</code>, the name of a function to produce a random sample from some distribution</p></li>
<li><p><code>n</code>, the size of samples to request of <code>rdist</code></p></li>
<li><p><code>n.iter</code>, the number of such samples to obtain</p></li>
<li><p><code>...</code>, any optional parameters to be passed on to <code>rdist</code> (such as the degrees of freedom <span class=""math-container"">$\nu$</span>).</p></li>
</ul>

<p>The remaining parameters control the display of the results; they are included mainly as a convenience for generating the figures in this answer.</p>

<pre><code>sim &lt;- function(rdist, n, n.iter, prefix="""",
                breaks=seq(0, 1, length.out=20), alpha=0.05,
                plot=TRUE, ...) {

  # The simulated P-values.
  # NB: The optional arguments ""..."" are passed to `rdist` to specify
  #     its parameters (if any).
  x &lt;- apply(matrix(rdist(n*n.iter, ...), ncol=n.iter), 2, 
             function(y) shapiro.test(y)$p.value)

  # The histogram of P-values, if requested.
  if (plot) {
    power &lt;- mean(x &lt;= alpha)
    round.n &lt;- 1+ceiling(log(1 + n.iter * power * (1-power), base=10) / 2)
    hist(x[x &lt;= max(breaks)], xlab=paste(""P value (n="", n, "")"", sep=""""), 
         breaks=breaks, 
         main=paste(prefix, ""(power="", format(power, digits=round.n), "")"", sep=""""))
    # Specially color the ""significant"" part of the histogram
    hist(x[x &lt;= alpha], breaks=breaks, col=""#e0404080"", add=TRUE)
  }

  # Return the array of P-values for any further processing.
  return(x)
}
</code></pre>

<p>You can see the computation actually takes just one line; the rest of the code plots the histogram.  To illustrate, let's use it to compute the expected false positive rates.  ""Rates"" is in the plural because the properties of a test usually vary with the sample size.  Since it is well-known that distributional tests have high power against qualitatively small alternatives when sample sizes are large, this study focuses on a range of small sample sizes where such tests of often applied in practice: typically about <span class=""math-container"">$5$</span> to <span class=""math-container"">$100.$</span>  To save computation time, I report only on values of <span class=""math-container"">$n$</span> from <span class=""math-container"">$5$</span> to <span class=""math-container"">$20.$</span></p>

<pre><code>n.iter &lt;- 10^5                 # Number of samples to generate
n.spec &lt;- c(5, 10, 20)         # Sample sizes to study
par(mfrow=c(1,length(n.spec))) # Organize subsequent plots into a tableau
system.time(
  invisible(sapply(n.spec, function(n) sim(rnorm, n, n.iter, prefix=""DF = Inf "")))
)
</code></pre>

<p>After specifying the parameters, this code also is just one line.  It yields the following output:</p>

<p><img src=""https://i.stack.imgur.com/LrE8p.png"" alt=""Histograms for the null""></p>

<p><strong>This is the expected appearance:</strong> the histograms show nearly uniform distributions of P-values across the full range from <span class=""math-container"">$0$</span> to <span class=""math-container"">$1$</span>.  With the nominal size set at <span class=""math-container"">$\alpha=0.05,$</span> the simulations report between <span class=""math-container"">$.0481$</span> and <span class=""math-container"">$0.0499$</span> of the P-values were actually less than that threshold: these are the results highlighted in red.  The closeness of these frequencies to the nominal value attests that the Shapiro-Wilk test does perform as advertised.</p>

<p>(There does seem to be a tendency towards an unusually high frequency of P-values near <span class=""math-container"">$1$</span>.  This is of little concern, because in almost all applications the only P-values one looks at are <span class=""math-container"">$0.2$</span> or less.)</p>

<p><strong>Let's turn now to assessing the power.</strong>  The full range of values of <span class=""math-container"">$\nu$</span> for the Student t distribution can adequately be studied by assessing a few instances from around <span class=""math-container"">$\nu=100$</span> down to <span class=""math-container"">$\nu=1$</span>.  How do I know that?  I performed some preliminary runs using very small numbers of iterations (from <span class=""math-container"">$100$</span> to <span class=""math-container"">$1000$</span>), which takes no time at all.  The code now requires a double loop (and in more complex situations we often need triple or quadruple loops to accommodate all the aspects we need to vary): one to study how the power varies with the sample size and another to study how it varies with the degrees of freedom.  Once again, though, everything is done in just one line of code (the third and final):</p>

<pre><code>df.spec &lt;- c(64, 16, 4, 2, 1)
par(mfrow=c(length(n.spec), length(df.spec)))
for (n in n.spec) 
  for (df in df.spec)
    tmp &lt;- sim(rt, n, n.iter, prefix=paste(""DF ="", df, """"), df=df)
</code></pre>

<p><img src=""https://i.stack.imgur.com/Fql7v.png"" alt=""Histograms for the alternatives""></p>

<p><strong>A little study of this tableau provides good intuition about power.</strong>  I would like to draw attention to its most salient and useful aspects:</p>

<ul>
<li><p>As the degrees of freedom reduce from <span class=""math-container"">$\nu=64$</span> on the left to <span class=""math-container"">$\nu=1$</span> on the right, more and more of the P-values are small, showing that the power to discriminate these distributions from a Normal distribution increases.  (The power is quantified in each plot title: it equals the proportion of the histogram's area that is red.)</p></li>
<li><p>As the sample size increase from <span class=""math-container"">$n=5$</span> on the top row to <span class=""math-container"">$n=20$</span> on the bottom, the power also increases.</p></li>
<li><p>Notice how as the alternative distribution differs more from the null distribution and the sample size increases, the P-values start collecting to the left, but there is still a ""tail"" of them stretching all the way to <span class=""math-container"">$1$</span>.  This is characteristic of power studies.  It shows that <strong>testing is a gamble</strong>: even when the null hypothesis is flagrantly violated and even when our sample size is reasonably large, our formal test may fail to produce a significant result.</p></li>
<li><p>Even in the extreme case at the bottom right, where a sample of <span class=""math-container"">$20$</span> is drawn from a Student t distribution with <span class=""math-container"">$1$</span> degree of freedom (a Cauchy distribution), the power is not <span class=""math-container"">$1$</span>: there is a <span class=""math-container"">$100 - 86.57 = 13\%$</span> chance that a sample of <span class=""math-container"">$20$</span> iid Cauchy variates will not be considered significantly different from Normal at a level of <span class=""math-container"">$5\%$</span> (that is, with <span class=""math-container"">$95\%$</span> confidence).</p></li>
<li><p>We could assess the power at any value of <span class=""math-container"">$\alpha$</span> we choose by coloring more or fewer of the bars on these histograms.  For instance, to evaluate the power at <span class=""math-container"">$\alpha=0.10$</span>, color in the left two bars on each histogram and estimate its area as a fraction of the total.</p>

<p>(This won't work too well for values of <span class=""math-container"">$\alpha$</span> smaller than <span class=""math-container"">$0.05$</span> with this figure.  In practice, one would limit the histograms to P-values only in the range that would be used, perhaps from <span class=""math-container"">$0$</span> to <span class=""math-container"">$20\%$</span>, and show them in enough detail to enable visual assessment of power down to <span class=""math-container"">$\alpha=0.01$</span> or even <span class=""math-container"">$\alpha=0.005$</span>.  (That is what the <code>breaks</code> option to <code>sim</code> is for.)  Post-processing of the simulation results can provide even more detail.)</p></li>
</ul>

<hr>

<p>It is amusing that so much can be gleaned from what, in effect, amounts to three lines of code: one to simulate i.i.d. samples from a specified distribution, one to apply that to an array of null distributions, and the third to apply it to an array of alternative distributions.  These are the three steps that go into any power analysis: the rest is just summarizing and interpreting the results.</p>
",2013-10-30 16:21:55.193
58513,23177.0,1,,,,Correlated features produce strange weights in Logistic Regression,<machine-learning><logistic>,CC BY-SA 3.0,"<p>I have a data set with highly positively correlated features that I'm classifying with LR. AFAIK correlated weights are not a problem in the same way they are in Naive Bayes - overcounting will not occur with LR. </p>

<p>The strange things that I'm seeing is that some of the highly correlated features assume opposite weights: feature A might be highly positive and feature B highly negative, though not as much. Is this a symptom of something going wrong with optimization or is this expected (a priori I expect A and B to be positive class indicators) </p>
",2013-10-30 16:29:34.377
58514,3999.0,2,,58505.0,,,,CC BY-SA 3.0,"<p>It's additive on the log scale.</p>

<p>The natural log of your odds ratio is 0.7696 (this should be the estimate from your model). As such, the odds ratio of going from 0 to 7 is exp(0.07696*7) = exp(0.53872) = 1.71</p>
",2013-10-30 16:37:02.997
58515,23180.0,1,,,,Econometrics - choosing the best model when removing variables,<multiple-regression><modeling><model-selection><econometrics><predictor>,CC BY-SA 3.0,"<p>So I am looking into a regression model that is supposed to predict the value of a house based on numerous independent variables. What I don't quite understand is how to select the ""best"" model when eliminating insignificant variables.
The original model contained four independant variables of which three turned out to be insignificant (p-value > 0.05). So I removed, say X1, which had the largest p-value (small t-stat). The new model containing three variables clearly appeared to be better, as Adjusted R2 rose (I understand Adjusted R Square is suitable for comparing models with different number of parameters). The F-tests also seemed to confirm that removing X1 was the right thing to do due to its insignificance.
However, in the three variable model there was still one insignificant variable, X3. So I removed it as well. Now, in the two variable model, both variables were reliable (small p-values), but Adjusted R Square was actually somewhat lower than that of the previous model. F-tests seemed to indicate that X3 had a small enough impact on the model to be eliminated.
Which is actually the better model statistically and economically? The three variable model with the highest Adjusted R Square or the two variable model with no insignificant variables but lower Adjusted R2? I couldn't quite figure out the logic behind this.</p>
",2013-10-30 16:41:10.440
58516,1889.0,2,,58475.0,,,,CC BY-SA 3.0,"<p>You had two questions</p>

<blockquote>
  <p>Question 1: What is the probability of an event where $X_i$ finishes first.</p>
</blockquote>

<p>Your proposed solution look correct but, as you say, you clearly have an error in implelentation as the probablities do not add up to $1$. M. Berk has shown that it is likely to be an issue with Swimmer 2.</p>

<blockquote>
  <p>Question 2: If I calculate this for all swimmers, can I simply order the results to determine the most probable finishing order? </p>
</blockquote>

<p>Not quite - if you have two swimmers with the same mean time in the middle of the group then the chance of one beating the other is $\frac12$, but the one with the higher standard deviation is more likely to be the overall winner: in your particular example, the chance of $X_2$ beating $X_5$ is $0.5$ but $X_5$ is more likely than $X_2$ to be the overall winner, so $X_5$ is less likely than $X_2$ to be third because of its higher standard deviation.</p>

<p>Using simulation, the most likely finishing order is $X_3,X_1,X_2,X_5,X_4$ (with a probability about 9.0%) above $X_3,X_1,X_5,X_2,X_4$ (about 8.0%), $X_1,X_3,X_2,X_5,X_4$ (about 6.1%), $X_3,X_2,X_5,X_1,X_4$ (about 5.8%), $X_1,X_3,X_5,X_2,X_4$ (also about 5.8%), $X_3,X_5,X_1,X_2,X_4$ (about 4.5%) and other less likely outcomes.  Your idea would have predicted 31524, the second most likely outcome. </p>
",2013-10-30 16:46:19.640
58517,22049.0,1,,,,Finding similarities using Wavelet transform,<time-series><signal-processing><similarities><wavelet>,CC BY-SA 3.0,"<p>I have a time serie and I want to find similarities in it. For the first step I have calculated Haar-wavelet coefficients for this time serie, and now I don't know exactly how should I continue</p>

<p>should I extract features from this transformed data, to find similarities? how can I do that? </p>
",2013-10-30 16:52:29.967
58534,13165.0,1,58541.0,,,Why semi/nonparametric models?,<bayesian><nonparametric><hierarchical-bayesian><nonparametric-bayes>,CC BY-SA 3.0,"<p>Increasing the flexibility of models makes it prone to overfitting. On the other hand, it looks to me that, if the space function classes $\mathcal{F}$ is too big, it is hard to prove bounds on empirical risks bounds and that sort of stuff. That's why I am questioning the necessity/importance/applicability of non-parametric models. </p>

<p>Here by nonparametrics I mostly mean, Dirichlet Processes and Beta Processes (and related family). </p>

<p>Any comments?  </p>
",2013-10-30 19:13:32.363
58518,23179.0,1,,,,Similarity distance score to remove outliers for survey data,<data-mining><distance-functions><association-rules>,CC BY-SA 3.0,"<p>I'm still a beginner at data mining. I'm working on finding the association rules from hypothesis X to conclusion Y. To this end, I've conducted a survey with questions that go something like this:</p>

<pre><code>Q1: Do you have any relatives with ABC property? (Ans: yes or no)
Q2: Do you have an interest in XYZ field? (Ans: yes or no)
Q3: Which institute are you from? (Ans: Option 1, 2, 3, 4 or 5)
</code></pre>

<p>and so on.. So there are lots of ""parameters"" or ""features"" or ""dimensions"" to my data. </p>

<p>My data is formatted similar to this: <a href=""http://www.hakank.org/weka/weather.arff"" rel=""nofollow"">http://www.hakank.org/weka/weather.arff</a>, and I'll be using WEKA.</p>

<p>However I'm still currently in the data pre-processing stage. Removing duplicate entries and dealing with missing values is no issue. What I'm worried about is removing outliers.</p>

<p>Firstly, how can I represent this type of record data in such a way that similarity measures like Euclidean or Minkowski (or perhaps any!) distance can even be applied to it?</p>

<p>And secondly, what's the most reasonable similarity measure to use in this type of case? I've looked at the Mahalanobi distance and it seems useless for my project because I have no ""ideal"" set of features against which I could compare other sections of data. Is it usual to even need to detect outliers before finding association rules? Or are outliers usually detected after the rules have been learned?</p>

<p>I've been thinking about this for a while but can't seem to reach a sensible conclusion. Could any of the more experienced data miners help please?</p>
",2013-10-30 16:52:48.587
58519,22637.0,1,58647.0,,,Order Statistics-Expected Value of Random Length,<confidence-interval><expected-value><order-statistics>,CC BY-SA 3.0,"<p>Let $Y_1&lt;Y_2 $ denote the order statistics of a random sample of size 2 from a distribution that is $N\left( \mu,\sigma^2 \right) $, where $\sigma^2$ is known. Compute the expected value of the random length $Y_2-Y_1$.</p>

<p>I can see that the answer is $\frac{2\sigma}{\sqrt{\pi}}$ but I do not know how to get there since I cannot evaluate the double integral:</p>

<p>$$ \int_{-\infty}^{\infty} \int_{-\infty}^{y_2} \left( y_2-y_1 \right) \frac{1}{2\pi \sigma^2} exp \left\{ -\frac{1}{2\sigma^2}\left[ \left( y_1-\mu \right)^2 +\left( y_2-\mu \right)^2 \right]\right\} \mathrm {dy_1 dy_2}$$</p>

<p>Any ideas on how to compute this are greatly appreciated, thank you!</p>
",2013-10-30 16:53:13.367
58520,23082.0,1,58559.0,,,Multivariate normal density function,<normal-distribution><matlab>,CC BY-SA 3.0,"<p>I am trying to compute multivariate normal distributions at some points.</p>

<p>I am using Matlab's <code>mvnpdf</code> function: <code>y = mvnpdf(X,MU,SIGMA)</code></p>

<p>The first argument is the point where I compute the density, MU is the mean and SIGMA the covariance.</p>

<p>I am puzzled by the following result:</p>

<pre><code> mvnpdf([0 0 0],[0 0 0],0.001*eye(3))

ans =

   2.0078e+03

&gt;&gt; mvnpdf([0 0 0.002],[0 0 0],0.001*eye(3))

ans =

   2.0038e+03
</code></pre>

<p>I am going at $2\sigma^2$ from the mean and the density is almost the same?
Shouldn't the result be close to zero?</p>
",2013-10-30 16:55:59.407
58521,22682.0,1,58562.0,,,Expectation of Quotient of Sums of IID Random Variables (Cambridge University worksheet),<probability><self-study><random-variable>,CC BY-SA 3.0,"<p>I'm preparing for an interview which requires a decent knowledge of basic probability (at least to get through the interview itself). I'm working through the sheet below from my student days as revision. It's mostly been fairly straightforward, but I am completely stumped on question 12.</p>

<p><a href=""http://www.trin.cam.ac.uk/dpk10/IA/exsheet2.pdf"" rel=""nofollow"">http://www.trin.cam.ac.uk/dpk10/IA/exsheet2.pdf</a></p>

<p>Any help would be appreciated.</p>

<p>Edit: the question is:</p>

<p>Suppose that $X_1, X_2, ... $ are independent identically distributed positive random variables with $\mathbb{E}(X_1) = \mu &lt; \infty$ and $\mathbb{E}(X_1^{-1}) &lt; \infty$. Let $S_n =  \sum_{i=1}^n  X_i$. Show that $\mathbb{E}(S_m/S_n) = m/n$ when $m&lt;=n$, and $\mathbb{E}(S_m/S_n) = 1 + (m-n)\mu\mathbb{E}(S_n^{-1}))$ when $m&gt;=n$.</p>

<p>In fact, in the process of typing this up, I've solved the second part.</p>

<p>For $m&gt;=n$, $\mathbb{E}(S_m/S_n) = \mathbb{E}(X_1+ . . . +X_m)/\mathbb{E}(X_1+ . . . +X_n)$</p>

<p>$=\mathbb{E}(1 + (X_{n+1} + ... + X_m)/(X_1 + ... + X_n)) $</p>

<p>and the numerator and denominator of the ratio above are clearly independent, so:</p>

<p>$ = 1 + \mathbb{E}(X_{n+1} + ... + X_m)\mathbb{E}(S_n^{-1})$</p>

<p>and we obtain the desired result.</p>

<p>I'm still stuck on the first part though.</p>
",2013-10-30 17:07:39.457
58522,23181.0,2,,27194.0,,,,CC BY-SA 3.0,"<p>In fact, Gujarati states: ""Today, however, the term multicollinearity is used in a broader sense to include the case of perfect multicollinearity, (...), as well
as the case where the X variables are intercorrelated but not perfectly so"" and proceeds to give a definition in correspondence to RockScience's definition. So, my guess would be that the two terms are related to each other by the argument given by him.</p>
",2013-10-30 17:09:56.460
58523,13165.0,1,,,,Which toolbox for Belief Propagation and other inference methods in graphical models?,<inference><graphical-model>,CC BY-SA 3.0,"<p>Which open-source software (toolbox) do you think is the best for modelling graphical models (e.g. factor graphs), and doing inference on them? (the language doesn't matter)</p>
",2013-10-30 17:19:54.253
58524,23182.0,1,,,,Which values to use for scaling out-of-sample PCA data?,<pca>,CC BY-SA 3.0,"<p>I have centered and scaled inputs via <code>prcomp ()</code>:</p>

<pre><code>prOut&lt;-prcomp(trainSet[,2:4],scale = TRUE,scores=TRUE)
</code></pre>

<p>I now want to use my completed model on new (future) data.
I assume the correct approach is to use the training data <code>prOut$scale</code> and <code>prOut$center</code> values and apply them to prior to calculating the principal component scores for my new data?</p>

<p>It doesn't seem right to scale the new data using the scale and center values for the new data set.</p>
",2013-10-30 17:27:52.937
58525,19996.0,2,,58513.0,,,,CC BY-SA 3.0,"<p>It is possible you are up against collinearity here (I'm assuming that when you say ""correlated"" you are assuming positive correlation, otherwise the postive/negative difference may make sense).   In any case, caution should be used when confronting collinearity in logistic regression.  Parameter estimates are often difficult to obtain and unreliable.  Of course, this depends on how highly correlated your predictors are.  To rule out collinearity, you might want to check something like the <a href=""http://en.wikipedia.org/wiki/Variance_inflation_factor"" rel=""nofollow"">Variance Inflation Factor</a>.</p>

<p>If your variables have a high correlation coefficient, but are not truly collinear, then it still isn't incredibly surprising to get the opposite sign behavior you observe (I say this without knowing more details of your problem), depending on what other variables are in your model.  Remember that fitting an LR model fits all variables simultaneously to the outcome, so you typically have to interpret the weights as a whole.  They may be correlated with each other, but have opposite effects in predicting an outcome, especially if grouped with other variables.</p>
",2013-10-30 17:46:57.703
58526,10756.0,1,,,,Algorithms for keyphrase clustering,<clustering><information-retrieval>,CC BY-SA 3.0,"<p>Are there any standard algorithms for keyphrase clustering. There are several algorithms for keyphrase extraction from a corpus. For e.g. <a href=""http://aclweb.org/anthology/C/C10/C10-2042.pdf"" rel=""nofollow"">this publication</a> reviews some of the popular keyphrase extraction algorithms. Example of possible keyphrases extracted from a corpus of real-estate data would be 'house prices', 'car parking', 'foreclosure', 'victorian houses', etc. A clever algorithm can cluster 'hardwood floors' and 'hdwd flrs' together. Another example would be clustering 'basketball court' and 'b-ball court' together.</p>

<p>Are there any algorithms to group semantically similar keyphrases together?</p>
",2013-10-30 18:03:37.327
58527,15827.0,2,,58494.0,,,,CC BY-SA 3.0,"<p>Imagine that a variable takes values 0 and 1 with probability both 0.5. Sample from that distribution and most of the medians will be 0 or 1 and a very few exactly 0.5. The means will vary far less. The mean is much more stable in this circumstance. </p>

<p>Here is a sample graph of results. The plots are quantile plots, i.e. ordered values versus plotting position, a modified cumulative probability. The results are for 10,000 bootstrap samples from 1000 values, 500 each 0 and 1. The means range fortuitously but nicely from 0.436 to 0.564 with standard error 0.016. The medians are as said, with standard error 0.493. (Closed-form results are no doubt possible here too, but a graph makes the point vivid for all.) </p>

<p><img src=""https://i.stack.imgur.com/aLpmb.png"" alt=""enter image description here""></p>

<p>But that is exceptional. It illustrates the least favourable case for medians, a symmetric bimodal distribution such that the median is likely to flip between different halves of the data. However, symmetric bimodal distributions are not especially common, but watch out for so-called U-shaped distributions in which the extremes are most common and intermediate values uncommon. Distributions that are unimodal, or in which the number of modes has only a small effect on median or mean, are more common. </p>

<p>As advised by every treatment of robust statistics, a very common situation is that your data come with tails heavier than Gaussian and/or with outliers, and in those circumstances median will almost always be more robust. The point is that that is <em>not</em> a universal general result. </p>

<p>All that said, what relevance is a general result? You can at a minimum establish by bootstrapping the relative variability of mean and median for your own data. That's what you care about. </p>
",2013-10-30 18:04:23.850
58528,,2,,58507.0,user31668,,,CC BY-SA 3.0,"<p>Your approach seems reasonable, especially your choice to stratify your sampling. This will make it more efficient provided you can easily delineate the different industrial zones. </p>

<p>I don't have a book to recommend you, but you could model your uncertainty using the Poisson distribution, with the $\lambda =$ No. of Toxic Waste Sites per Square Kilometer.
You could carry out your sampling program as you described and then find the maximum likelihood estimator for $\lambda_{Ai}$ where A is the area of a sampling sector in zone $i$. In particular, you would maximize the following formula wrt $\lambda_{Ai}$ where $N_i$= number of sectors sampled from zone $i$:</p>

<p>$\max\limits_{\lambda_{Ai}} \prod\limits_{j=1}^{N_i} \frac{e^{-\lambda_{Ai}}\lambda_{Ai}^{n_{ij}}}{n_{ij}!}$ where $n_{ij}$ is the number of toxic sites in sector $j$ of zone $i$. The value of $\lambda_{Ai}$ that maximizes the product is $\lambda_{Ai}^* = \frac{1}{N_i}\sum\limits_{j=1}^{N_i}{n_{ij}}$</p>

<p>You will get one estimate per zone, $\lambda_{Ai}^*$, which you can interpret as the frequency of toxic waste sites within a region of area $A_i$. Your uncertainty for the total number of sites in Zone $i$ with total area $A_{Ti}$can be modeled using your estimated $\lambda_{Ai}^*$ in the Poisson distribution: $Poisson(\lambda_{Ai}^*\frac{A_{Ti}}{A_i})$.</p>

<p>To get a country-wide estiamte, you would need to combine the $\lambda_{Ai}^*$ into another Poisson distribution: Total No. of Sites ~ $Poisson(\sum\limits_{i=1}^{N_{zones}}\lambda_{Ai}^*\frac{A_{Ti}}{A_i})$. </p>

<p><strong>Refinements</strong></p>

<p>The above should get you a decent estimate. However, if your country is small enough that your sample will cover an appreciable portion of the total land area or area within a zone, then you should reduce the total area for each zone by the sampled area in the above formula, so you are modleing the uncertainty on the <em>remaining</em> area (which is actually more accurate in both cases), then you add this uncertainty to your actual counts in the areas you've sampled.</p>

<p>Also, you will notice that you're using a point estimate of $\lambda$. There is some uncertianty in the actual value of this quantity, but including it requires using <a href=""http://books.google.com/books?id=8T8fAQAAQBAJ&amp;pg=PA524&amp;lpg=PA524&amp;dq=extended%20likelihood%20poisson%20for%20prediction&amp;source=bl&amp;ots=LGUZVLkdf8&amp;sig=Ga2s_bnLLqxS7HVQLxJZNE1FjjE&amp;hl=en&amp;sa=X&amp;ei=0VlxUojQMajSyAGlsoB4&amp;ved=0CDQQ6AEwAQ#v=onepage&amp;q=extended%20likelihood%20poisson%20for%20prediction&amp;f=false"" rel=""nofollow"">extended likelihood for predicting a Poisson variable</a>. The formula is pretty simple, if Y is the total number of sites in zone $i$, then the likelihood function for Y is:</p>

<p>$L(Y_i) = e^{-(N+1)\hat\theta(Y_i)}\frac{\hat\theta(Y_i)^{Y_i+\sum\limits_{j=1}^{N_i}{n_{ij}}}}{Y_i!}$ Where $\hat\theta(Y_i) = \frac{A_{Ti}}{A_i}(Y_i + \frac{\sum\limits_{j=1}^{N_i}{n_{ij}}}{N_i+1})$ You need to normalize this formula to sum to 1 over the range of relevant Y. To get the country-wide estimate, you would need to use Monte-Carlo simulation for the sum of the $Y_i$ from each area based on the above formula. There are a couple inexpensive/<a href=""http://www.montecarlito.com/"" rel=""nofollow"">free</a> simulators out there.  </p>
",2013-10-30 18:22:03.667
58529,10957.0,1,,,,pattern of ROC curve and choice of AUC,<roc><auc>,CC BY-SA 3.0,"<p>I am using ROC curves and full AUC values to compare different models, using simulated data. Now I think I am confused with the interpretations of ROC curves and AUC values. Please see the figure below (sorry it is partial from screen shots...)</p>

<p>There are three models compared, and I know that the model shown in green should preform best of all. However, as you can see, the green curve is superior to the other two <strong>before the FPR reaching around 0.2</strong>. This cut-off of 0.2 is quite interesting: it is the percentage of differentially expressed genes that I specify in my simulation (i.e. 20% of the observations are simulated to be positives).</p>

<p>My concern are:</p>

<ol>
<li><p>given that people in reality will seldom choose a FPR cut-off of 0.5 or higher, why people would prefer a ROC curve with FPR ranging from 0 to 1 and use the full AUC value (i.e. calculate the entire area under the ROC curve) instead of just reporting the area made from, say, 0 to 0.25 or to 0.5? Is that called ""partial AUC""?</p></li>
<li><p>in the figure below, what can we say about the performances of the three models? The AUC values are: green (0.805), red (0.815), blue (0.768). The red curve turns out to be superior, but as you see, the superiority is only reflected after FPR > 0.2. Thanks :)</p></li>
</ol>

<p><img src=""https://i.stack.imgur.com/Z3bKk.png"" alt=""enter image description here""></p>
",2013-10-30 18:25:16.317
58530,3999.0,2,,58119.0,,,,CC BY-SA 3.0,"<p>The answer, in my mind, is ""Yes and No, and if Yes then this isn't actually interesting"".</p>

<p>For variables with no dependency structure, the answer is no - a very high, or very low, value of a particular variable doesn't imply a very high, or very low value of another variable.</p>

<p>For variables with a dependency structure, the answer is often (but not always) yes. But this isn't a property of ""outlier-ness"", it's a property of correlation itself. What you're showing is not that ""Being an outlier begets being an outlier in other areas"" but that two associated variables behave exactly like associated variables should.</p>
",2013-10-30 18:26:09.790
58531,7007.0,2,,58484.0,,,,CC BY-SA 3.0,"<p>Suppose that each symbol of the password occupies one of $8$ numbered boxes. First you choose $4$ of the $8$ boxes to put the letters, and each choice gives you $26^4$ possible letters configurations. Now, in the remaining $4$ boxes you put the digits, and each choice gives you $10^4$ possible digits configurations. Therefore, the total number of passwords is
$$
  \binom{8}{4} \times 26^4 \times \binom{4}{4} \times 10^4 \, .
$$
For the password with the letters all equal and one digit $8$, from the $8$ boxes you choose $4$ to put the letters, and each choice gives you $26$ possible letters configurations. From the remaining $4$ boxes you choose $3$ to put digits, and each choice gives you $10^3$ digits configurations. In the last remaining box you put the digit $8$, which gives us
$$
  \binom{8}{4} \times 26 \times \binom{4}{3} \times 10^3 \times \binom{1}{1} \times 1
$$
paswords. I'm supposing that we can have more than one digit $8$.</p>

<p>P.S. For Huber's simplified problem, the number of possible passwords is
$$
  \binom{4}{2} \times 1 \times \binom{2}{2} \times 2^2 = 24 \, .
$$</p>
",2013-10-30 18:30:41.327
58532,18331.0,1,58551.0,,,"Why do they call it ""sampling distribution of the sample mean""?",<mean><standard-error><terminology><sample>,CC BY-SA 3.0,"<p>Ok, I understand that there is a true population mean and one that I get from the sample. It is different for every sample and, thus, I can build the distribution of the sample means. I arrive at a <em>distribution of sample means</em>. But why is it a <em>sampling</em> distribution? Is the whole point that the moniker must be longer than necessary? What do I lose if I omit the extra qualifier and get away with <em>distribution of sample mean</em> alone?</p>
",2013-10-30 18:44:55.693
58533,5045.0,2,,58513.0,,,,CC BY-SA 4.0,"<p>This might be a case of <em>ceteris paribus</em> confusion. It's hard to know without knowing more about your analysis. </p>

<p><a href=""https://stats.stackexchange.com/a/73608/7071"">Example 5</a> from Peter Kennedy might be relevant here:</p>

<blockquote>
  <p>In a linear regression of racehorse auction prices on various
  characteristics of the horse and information on its sire (father) and
  dam (mother), Robbins and Kennedy found that although the estimated
  coefficient on dam dollar winnings was positive, the coefficient on
  number of dam wins was negative, suggesting that yearlings from dams
  with more race wins are worth less. This wrong sign problem is
  resolved by recognizing that the negative sign means that holding dam
  dollar winnings constant, a yearling is worth less if its dam required
  more wins to earn those dollars.</p>
</blockquote>
",2013-10-30 19:01:50.803
58535,9554.0,2,,58529.0,,,,CC BY-SA 3.0,"<p>Usually, it will be your application that will determine whether your focus is on precision or recall. </p>

<p>@2
These will be dramatically different in the medical field, where you will often tolerate having a bad precision for the sake of a very good recall, when it comes to prevention, i.e. you prefer to label a lot of healthy people as sick and make additional tests, rather than to let someone die (here sickness is considered ""relevant"", and labeled as sick ""retrieved"").</p>

<p>On the other hand, in production, you can tolerate a certain quota of bad apples and you might prefer a test that does not catch all the faulty products but is much more precise in identifying the bad apples - usually the costs associated with inspecting the items can not be disregarded. This corresponds to a high precision, and low recall scenario.</p>

<p>For your models, either you know what you need and pick a better model for that purpose, or you pick the one with better AUC. Of course there are also other things you might take into consideration, such as, which model is more parsimonious (has fewer explanatory variables), where are the assumptions better met, etc.</p>

<p>@1
I don't see the advantage of putting less information in a plot, especially if it could be misleading. (unless you work in marketing)</p>
",2013-10-30 19:20:43.900
58536,21346.0,1,58537.0,,,Integral in Stata when upper limit is infinity,<stata><integral>,CC BY-SA 3.0,"<p>How can we calculate the integral of the integrand with the lower limit 0 and upper limit infinity in Stata? I am aware of the <code>integ</code> command, but I am not sure whether I can use that when the upper limit is infinity.  </p>
",2013-10-30 19:41:33.683
58537,5045.0,2,,58536.0,,,,CC BY-SA 3.0,"<p>You can use <code>integrate</code> from SSC to do numerical integration for one dimensional functions like this:</p>

<pre><code>. integrate, f(normalden(x)) l(.) u(.)

    Note: The function to be integrated will be compiled using Mata and stored in your personal directory ~/ado/personal/ (make sure this is writeable)

The integral = 1
</code></pre>
",2013-10-30 20:07:49.170
58538,17660.0,2,,58515.0,,,,CC BY-SA 3.0,"<p>From what I can tell, it generally depends on your purpose. If you are trying infer causality, then you need choose variables that specify some structure that actually has some theory behind it. Do you actually have good, intuitive reasons for including one variable or the other? (However, this is useless without some sort of identification strategy to disentangle other effects and biases). </p>

<p>On the other hand, if your goal is only to create a model with predictive power, it seems like in general you want to choose variables that make significant contributions to the fit of the model. For prediction and descriptive analysis, it seems to me like different fields have different rules. See this discussion about <a href=""http://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/hier"" rel=""nofollow"">hierarchical and stepwise regressions</a> of an example of different kinds of rules. In the case you have describe above, there isn't a hard rule to determine which case to choose.</p>
",2013-10-30 20:08:39.210
58539,23187.0,1,,,,Nonnegativity of model optimism,<machine-learning><degrees-of-freedom><statistical-learning>,CC BY-SA 3.0,"<p>The following sounds like a very basic question in learning theory to me, so I am hoping someone can point out the obvious.</p>

<p>Efron's ""expected optimism"" is the expected difference between the prediction and training errors. Efron (2004) shows that for a wide range of loss functions (the ""q-family""), and virtually all modeling approaches, the expected optimism in modeling the mean of a vector is proportional to the sum of covariances of fitted and observed components. It thus seems natural that any sensible modeling approach would have non-negative expected optimism (because one tries at least in part to minimize the training error).</p>

<p>I wonder if a more general result exists, though. Suppose that we have an arbitrary optimization criterion (not necessarily a loss in Efron's q-family, maybe not even convex/differentiable/separable; just a function of the data and a tunable parameter). We minimize this criterion on the training data over all possible parameter values in some feasible set. Is the expected minimum obtained this way (where the expectation is taken over all training sets) necessarily an underestimate of the expected criterion value applied to new data, unseen during training (where now the expectation is over all training and all iid testing datasets, as in Efron's definition of optimism)? </p>

<p>In other words, is the expected optimism of a sensible modeling approach always negative? Or maybe one has to really require something of the distribution of the data and/or know something about the criterion?</p>

<p>Counter examples may also be enlightening.</p>
",2013-10-30 20:18:03.027
58540,13165.0,1,,,,Why beta process is useful ?,<nonparametric><stochastic-processes>,CC BY-SA 3.0,"<p>Why Beta process is useful/important? How different is that from Dirichlet process? </p>
",2013-10-30 20:29:01.840
58541,5821.0,2,,58534.0,,,,CC BY-SA 3.0,"<p>Your first sentence is not necessarily correct. First off, an increase in numbers of parameters does indeed increase the degrees of freedom and the standard errors of point estimates (hence their degree of generalization). An example of this is the nested classes of Exponential and Weibull models. It's not universally agreed upon that ""model complexity"" necessarily means the parameter space, though, but it is a good place to start for discussion.</p>

<p>Semiparametric and nonparametric inference make overfitting a nonissue by generalizing the likelihood function into a new type of function where such extraneous parameters are ancillary. The only caveat is that the statistician has to correctly identify such models. Examples of such extended likelihoods are conditional likelihood (in mixed modeling), partial likelihood (in Cox models), pseudo likelihood (forgetting some applications for that...), profile likelihood, quasilikelihood, (and the list goes on). The parameter spaces for such likelihoods are seen as <em>projections</em> of high (possibly infinte) dimensional (compact) parameter spaces. </p>

<p>It's only in fully parametric inference where every causal relationship needs to be specified, such as the correlation structure for teeth within a mouth, or the correlation between failure times in a prospective cohort among denominators of individuals counted more than once. Many of these likelihoods are overly complex or intractable hence inference about them is non-existent or otherwise not popular.</p>

<p>Modeling processes is a fully parametric endeavor. You must be able to simulate data from an estimated data generating mechanism. SP/NP often cannot achieve this. Neither can the produce fitted effects nor can they claim to simulate realizations from any data generating process. SP/NP focuses on the point estimation of a specific parameter and efficiently calculates estimates and standard errors for that parameter cancelling out all other parameters in the data generating process through either conditioning, estimating them as nuisance parameters, or some other process.</p>

<p>SP/NP inference examples are the log-rank test (NP), the plain vanilla asymptotic t-test without normality assumptions (NP), conditional logistic regression (SP), generalized estimating equations (GEE), and Cox proportional hazards models (SP).</p>

<p>Examples where semi-parametric inference breaks down is in the case of missing at random data (as opposed to missing completely at random data), where the value of some observed outcome or covariate depends on the things which we deemed to be ancillary (such as informative censoring in Cox models). A fully likelihood based survival analysis would require separate models (and their correlation) for survival and censoring outcomes.</p>
",2013-10-30 20:30:32.093
58542,19043.0,1,58543.0,,,Significant ANOVA interaction but non-significant pairwise comparisons,<anova><modeling><post-hoc><tukey-hsd-test>,CC BY-SA 3.0,"<p>I ran a two-way ANOVA and got a significant interaction. I ran a <code>Tukey.HSD()</code> post-hoc test in R and no pairwise comparisons were significant. Is this an error on my part? My adviser insists that this is not possible. If it is possible, why does this happen?</p>
",2013-10-30 20:32:52.157
58544,4656.0,2,,43458.0,,,,CC BY-SA 3.0,"<p>Given a data set $\{y_1, y_2, \ldots, y_n\}$ with $n$ entries, the 
mean $\mu$ satisfies
$n\mu = \sum_{i=1}^n y_i, $ while the mean of the <em>expurgated</em> set
$\{y_1, y_2, \ldots, y_{n-1}\}$ is
$$\hat{\mu} = \frac{1}{n-1}\sum_{i=1}^{n-1} y_i 
= \left.\left.\frac{1}{n-1}\right(n\mu - y_n\right)$$
which will equal $\mu$ <em>exactly</em> if and only if the deleted
entry $y_n$ equals $\mu$.  Thus, deleting an entry from a data
set <em>will</em> change the mean unless the point deleted happens to
equal the mean of the original data set.</p>

<p>The variance $\sigma^2$ of the original data set
satisfies $(n-1)\sigma^2 = \sum_{i=1}^{n}(y_i-\mu)^2$.
If we delete an entry (say $y_n$) which happens
to have value $\mu$ (so that the mean remains the same),
then the expurgated data set has variance
$$\begin{align}
\hat{\sigma}^2 &amp;= \frac{1}{n-2}\sum_{i=1}^{n-1} (y_i-\mu)^2\\
&amp;= \frac{1}{n-2}\sum_{i=1}^{n} (y_i-\mu)^2 &amp;\text{since}~ y_n-\mu = 0,\\
&amp;= \frac{n-1}{n-2}\sigma^2\\
&amp;&gt; \sigma^2.
\end{align}$$
Thus, our effort to preserve the mean necessarily increases the variance.</p>
",2013-10-30 21:31:45.690
58545,20286.0,2,,58515.0,,,,CC BY-SA 3.0,"<p>If your goal is prediction rather than trying to infer causality, there is no need to remove any predictor variables, as @Michael Mayer has already commented, unless there is a significant cost to obtaining the values of some of them. And although your predictor variables may be the ""independent"" variables in your analysis, it's likely that many are correlated to each other (e.g., number of bedrooms, number of bathrooms, size of lot). In those cases trying to remove variables that are ""insignificant"" in analysis of a particular sample may lead to weird, counter-intuitive results that do not generalize well for future predictions.</p>

<p>If you do need to remove predictor variables for some reason, follow specific defined methods for hierarchical/stepwise analyses as suggested by @jmbejara, rather than trying to make up the rules as you go. These methods use better ways to compare models than the adjusted R-squared values, and they are available in R and other statistical analysis software.</p>
",2013-10-30 21:34:28.380
58546,22682.0,2,,58521.0,,,,CC BY-SA 3.0,"<p>Thanks to whuber for the hint for the first part.</p>

<p>Consider $nS_m/S_n$ for the case $m&lt;=n$</p>

<p>We have $\mathbb{E}(nS_m/S_n) = \mathbb{E}((nX_1 + . . . + nX_m)/(X_1 + . . . + X_n))$</p>

<p>$= \mathbb{E}(nX_1/X_1 + . . . + X_n) + . . . + \mathbb{E}(nX_m/X_1 + . . . + X_n)$</p>

<p>and by the iid property, this is equal to:</p>

<p>$m\mathbb{E}((X_1+ . .+ X_n)/(X_1+ . . . + X_n)) = m$</p>

<p>Therefore $\mathbb{E}(S_m/S_n) = m/n$ for $m&lt;=n$</p>
",2013-10-30 21:38:05.033
58547,21762.0,2,,58532.0,,,,CC BY-SA 3.0,"<p>For a given data set, the sample mean provides a single estimate of the population mean. This estimate is a constant and thus its distribution is rather boring. </p>

<p>In contrast, the sampling distribution of the mean refers to the frequentist approach of considering the distribution of the sample means between many hypothesized samples drawn from the same population.</p>

<p>So it kind of makes sense to use a 'new' word.</p>
",2013-10-30 21:55:27.957
58548,23191.0,1,,,,Negative Binomial Regression and Heteroskedasticity test,<regression><heteroscedasticity><negative-binomial-distribution>,CC BY-SA 3.0,"<p>I am running a negative binomial regression in Stata and would like to know if I need to include the <code>vce(robust)</code> option in the model. I know the negative binomial regression is the best for the data I have. I've combed through so many sources and it seems like negative binomials already seem to take care of some heteroskedasticity, but I would really like a test that would let me know if there is still some in the model that needs to be taken care of via the <code>vce(robust)</code> option. I know there is the <code>hettest</code> but I can only use that in a <code>regress</code> model, not a <code>nbreg</code> (negative binomial regression) model. I also saw this advice: </p>

<p>""You could try plotting the absolute value of Pearson residuals from your NB regression against covariates (at least, covariates you think might affect the overdispersion) or against the fitted means. If there's a discernible trend, this suggests non-constant variance.""</p>

<p>But what constitutes 'a discernible trend'? How do I know if that discernible trend would be significantly better taken care of by the <code>vce(robust)</code> option or it wouldn't matter? </p>
",2013-10-30 22:12:17.233
58549,21497.0,1,,,,Estimating if a distribution is significantly less conserved than others when one group is involved.,<correlation><covariance><variability>,CC BY-SA 3.0,"<p>I am looking at levels of genes in a dataset and want to identify genes that do not vary much in terms of their expression level. While I can do this using the coefficient of correlation, calculating the covariance or by looking for  number of genes within the botton x percent of genes using median absolute deviation those methods appear to be arbitrary. </p>

<p>What I am interested in is defining a cutoff based on P.values - is there any way of finding out which genes show significantly less variability than would be expected by chance without having external controls to compare it to? </p>

<p>Cheers,
Ankur.</p>
",2013-10-30 22:13:04.033
58550,11197.0,2,,58481.0,,,,CC BY-SA 4.0,"<p>Yes, there is a package <code>adegenet</code>. For example:</p>

<pre><code>library(adegenet)
data(dapcIllus)
x &lt;- dapcIllus<span class=""math-container"">$a
grp &lt;- find.clusters(x, max.n.clust = 40)
dapc1 &lt;- dapc(x, grp$</span>grp)
scatter(dapc1)
</code></pre>

<p><img src=""https://i.stack.imgur.com/Cna3R.png"" alt=""enter image description here""></p>

<p>For more information read <a href=""http://adegenet.r-forge.r-project.org/files/tutorial-dapc.pdf"" rel=""nofollow noreferrer"">this</a>.</p>
",2013-10-30 22:14:01.137
58551,668.0,2,,58532.0,,,,CC BY-SA 3.0,"<p>Within a particular setting where the type of distribution is known or implied, ""distribution of the sample mean"" works just fine.  But in general would the ""distribution of the sample mean"" be its <a href=""https://stats.stackexchange.com/search?q=%22sampling+distribution%22""><em>sampling</em> distribution</a>, a <a href=""https://stats.stackexchange.com/search?q=bootstrap+distribution""><em>bootstrap</em> distribution</a>, a <a href=""https://stats.stackexchange.com/search?q=permutation+distribution""><em>permutation</em> distribution</a>, or perhaps something else? </p>

<p>The existence of different kinds of distribution of a sample statistic requires some linguistic method of disambiguation.  Without that, you lose precision and perhaps miscommunicate with your audience.</p>
",2013-10-30 22:14:58.057
58552,23192.0,1,58558.0,,,Random walk on simplex as part of Metropolis-Hastings,<metropolis-hastings><random-walk>,CC BY-SA 3.0,"<p>I would like to perform a random walk on a J-dimensional simplex. However, since this is part of a metropolis-hastings algorithm application, my understanding is that the steps need to be drawn from a symmetric distribution (is this correct?)</p>

<p>I was wondering if there is a standard/established way to approach this.</p>

<p>Any help/pointers are greatly appreciated!</p>

<p>Noushin</p>
",2013-10-30 22:25:32.417
58553,2958.0,2,,58524.0,,,,CC BY-SA 3.0,"<p>Yes, you should use the center and scaling from the <code>prcomp</code> object.</p>

<p>An easy way to realize that center and variance of the new data set is not suitable is:<br>
assume the new data set consists of one row only. Its center would be the case itself and a scaling cannot be calculated.</p>
",2013-10-30 22:25:36.973
58554,2958.0,2,,58529.0,,,,CC BY-SA 3.0,"<p>I agree with your concerns.</p>

<blockquote>
  <p>given that people in reality will seldom choose a FPR cut-off of 0.5 or higher, why people would prefer a ROC curve with FPR ranging from 0 to 1 and use the full AUC value (i.e. calculate the entire area under the ROC curve) instead of just reporting the area made from, say, 0 to 0.25 or to 0.5? Is that called ""partial AUC""?</p>
</blockquote>

<ul>
<li>I'm a big fan of having the complete ROC, as it gives much more information that just the sensitivity/specificity pair of one working point of a classifier. </li>
<li>For the same reason, I'm not a big fan of summarizing all that information even further into one single number. But if you have to do so, I agree that it is better to restrict the calculations to parts of the ROC that are relevant for the application. </li>
</ul>

<blockquote>
  <p>in the figure below, what can we say about the performances of the three models? The AUC values are: green (0.805), red (0.815), blue (0.768). The red curve turns out to be superior, but as you see, the superiority is only reflected after FPR > 0.2. Thanks :)</p>
</blockquote>

<ul>
<li>That depends entirely on your application. In your example, if high specificity is needed, then the green classifier would be best. If high sensitivity is needed, go for the red one.</li>
</ul>

<p>As to the comparison of classifiers: there are lots of questions and answers here discussing this. Summary:</p>

<ul>
<li>classifier comparison is far more difficult than one would expect at first</li>
<li>not all classifier performance measures are good for this task. Read @FrankHarrells answers, and go for so-called proper scoring rules (e.g. Brier's score/mean squared error). </li>
</ul>
",2013-10-30 22:35:14.527
58555,22974.0,1,,,,Is it always bad to retrain your model to include predicted data?,<modeling><predictive-models><train>,CC BY-SA 3.0,"<p>I understand intuitively why this is a horrible idea - you assume your model is correct and then increase your number of observations which will likely result in a poor fit on future data.</p>

<p>I'm wondering if there is some mathematical/statistical property to describe this, or if there is any rare case where this may not be as fatal as I am thinking?</p>
",2013-10-30 23:29:33.807
58556,22564.0,2,,56445.0,,,,CC BY-SA 3.0,"<p>If data in the treatment group is not normal while the control group is it sounds like the treatment may only be affecting a subset of the sample or having variable levels of effect. Comparing means under such circumstances would be losing out on this information. You should attempt to offer explanations for why this change of distribution occurred rather than only comparing means. The rank tests assume that both groups come from the same shape distribution. If you believe the distributions are different the tests are not useful for your purposes.</p>

<p>Let us take an example of what can happen with the U-test. We will make our control group come from a normal distribution with mean=0. Meanwhile the treatment will have negative effects on half the subjects and positive effects on the other half. So the treatment group will come from two normal distributions. The first with mean=-5, the second with mean=5. All distributions have sd=1 and both groups have sample size=100. Red shows the treatment group while blue shows the control group:</p>

<p><img src=""https://i.stack.imgur.com/s9kV4.png"" alt=""enter image description here""></p>

<p><strong>Results of doing a U-test (which is also called the Wilcoxon test):</strong></p>

<pre><code>        Wilcoxon rank sum test with continuity correction

data:  a and b 
W = 4999, p-value = 0.999
alternative hypothesis: true location shift is not equal to 0
</code></pre>

<p>We can see it returns ""not significant"". Would you really want to conclude the treatment had no effect?</p>

<p><strong>R code for generating the above:</strong></p>

<pre><code>##Generate Data
control&lt;-rnorm(100,0,1) # create control data
treatment&lt;-c(rnorm(50,-5,1),rnorm(50,5,1)) # create treatment data


##Plot data
# Get min/max values (for plotting)
min.val&lt;-min(control,treatment)
max.val&lt;-max(control,treatment)

# make plots
hist(treatment, breaks=seq(min.val-.1,max.val+.1,.5), col=""Red"",
xlab=""Value"", ylim=c(0,20),
main=""Results""
)
hist(control, add=T, breaks=seq(min.val-.1,max.val+.1,.5),col=""Blue"")

##perform U-test
wilcox.test(treatment,control)
</code></pre>
",2013-10-30 23:32:08.437
58557,22564.0,2,,58306.0,,,,CC BY-SA 3.0,"<p>The tests you mention are not appropriate for your situation. They will only tell you the probability of getting a difference difference between years 1-2 and years 3-5 as or more extreme than the difference you observed if there was exactly zero difference from year to year. This null hypothesis is highly unlikely to be true regardless of whether tools were changed. It is also unlikely that classes in the future will be exactly the same type of students as in the past.</p>

<p>What you care about should be (I think) whether tool B will lead to higher participation in the future than tool A. This is an ""analytic"" problem, while the statistical tests you are attempting to use are meant for ""enumerative problems"". Yes, this type of use is very common and it has lead to about 80 years of misleading results in many fields.</p>

<p>The only way to make rational decisions is to have understanding of the underlying data generating process. If there is little background knowledge all you can do is plot the data and look for patterns that indicate there may be some lurking/confounding variable that offers an alternative explanation for the increase in participation. You should try to break up the data into as many plausible subgroups as possible (e.g., type of student) and look for patterns.</p>

<p>If you and other experts cannot think of any plausible alternative explanations then it would be rational to decide to continue using tool B. If an alternative explanation is available then further study is necessary to determine which is responsible. A good source on this issue would be William Edwards Deming.</p>

<p><a href=""https://en.wikipedia.org/wiki/Analytic_and_enumerative_statistical_studies"" rel=""nofollow"">https://en.wikipedia.org/wiki/Analytic_and_enumerative_statistical_studies</a></p>

<p><a href=""https://www.deming.org/media/pdf/081.pdf"" rel=""nofollow"">https://www.deming.org/media/pdf/081.pdf</a></p>

<p><strong>EDIT:</strong> 
Here is a quote from Deming (decide for yourself whether he is a credible source, but he knew both Fisher and J Neyman):</p>

<blockquote>
  <p>Limitations of statistical inference. All results are conditional on
  (a) the frame whence came the units for test; (b) the method of
  investigation (the questionnaire or the test-method and how it was
  used) ; (c) the people that carry out the interviews or measurements.
  In addition (d), the results of an analytic study are conditional also
  on certain environmental states, such as the geographic locations of
  the comparison, the date and duration of the test, the soil, rainfall,
  climate, description and medical histories of the patients or subjects
  that took part in the test, the observers, the hospital or hospitals,
  duration of test, levels of radiation, range of voltage, speed, range
  of temperature, range of pressure, thickness (as of plating), number
  of flexures, number of jolts, maximum thrust, maximum gust, maximum
  load. </p>
  
  <p>The exact environmental conditions for any experiment will never
  be seen again. Two treatments that show little difference under one
  set of environmental circumstances or even within a range of
  conditions, may differ greatly under other conditions-other soils,
  other climate, etc. The converse may also be true: two treatments that
  show a large difference under one set of conditions may be nearly
  equal under other conditions. </p>
  
  <p>There is no statistical method by which
  to extrapolate to longer usage of a drug beyond the peritd of test,
  nor to other patients, soils, climates, higher voltages, nor to other
  limits of severity outside the range studied. Side effects may develop
  later on. Problems of maintenance of machinery that show up well in a
  test that covers three weeks may cause grief and regret after a few
  months. A competitor may stop in with a new product, or put on a blast
  of advertising. Economic conditions change, and upset predictions and
  plans. These are some of the reasons why information on an analytic
  problem can never be complete, and why computations by use of a
  loss-function can only be conditional. The gap beyond statistical
  inference can be filled in only by knowledge of the subject-matter
  (economics, medicine, chemistry, engineering, psychology, agricultural
  science, etc.), which may take the formality of a model [12], [14],
  [15].</p>
</blockquote>

<p>Deming, W. Edwards ""On probability as a basis for action"" The American Statistician, volume 29, 1975</p>

<p><a href=""https://www.deming.org/media/pdf/145.pdf"" rel=""nofollow"">https://www.deming.org/media/pdf/145.pdf</a></p>
",2013-10-31 00:28:18.800
58558,23087.0,2,,58552.0,,,,CC BY-SA 3.0,"<p>I think you're asking whether the MH proposal distribution has to be symmetric. No, it doesn't have to be symmetric, it just can't depend on the current state. For sampling on a constrained space it's perfectly valid just to use a Gaussian proposal distribution and reject any proposals that fall outside the space. However, this may not work well in practice, particularly if J is large or the mass is concentrated towards the corners of the simplex. </p>
",2013-10-31 00:47:09.833
58559,12501.0,2,,58520.0,,,,CC BY-SA 3.0,"<p>What you are specifying with the second argument to <code>mvnpdf</code> is, as you correctly state, the (co-) variance. The standard deviation corresponding to a variance of 0.001 is about 0.0316. The point you are looking at is at a distance of 0.002 from the center of the distribution, or about 0.0632 standard deviations from the center, i.e. it is very close to the center. It is therefore to be expected that the density is only slightly smaller than at the center.</p>
",2013-10-31 00:48:13.660
58560,20981.0,2,,58532.0,,,,CC BY-SA 3.0,"<p>You can have a sampling distribution of other statistics than the mean, such as the estimated median, or estimated variance.</p>

<p>Sometimes ""sampling distribution"" might be a loose term referring to the estimated mean <em>and</em> estimated variance of the sample taken together (with the unspoken assumption that the distribution of sample means is approximately normal).</p>
",2013-10-31 00:49:14.583
58561,6384.0,1,58564.0,,,How to prove that $X^T$e = 0,<regression><self-study><multiple-regression>,CC BY-SA 4.0,"<p>I need to prove that $X^T e$ = 0 where $e$ is the residual in multiple linear regression model in matrix algebra?</p>

<p>I need some guidance on how to do it. Is there any good pdf for the proofs for multiple linear regression model for matrix alegbra?</p>
",2013-10-31 01:32:10.817
58562,15972.0,2,,58521.0,,,,CC BY-SA 3.0,"<p>Spotting to add $n$ identical copies of $S_m/S_n$ is very clever! But some of us are not so clever, so it is nice to be able to ""postpone"" the Big Idea to a stage where it is more obvious what to do. Without knowing where to start, there seem be a number of clues that <em>symmetry</em> could be really important (addition is symmetric and we have some summations, and iid variables have the same expectation so maybe they can be swapped around or renamed in useful ways). In fact the ""hard"" bit of this question is how to deal with the division, the operation which <em>isn't</em> symmetric. How can we exploit the symmetry of summation? From linearity of expectation we have:</p>

<p>$\mathbb{E}(S_m/S_n) = \mathbb{E}\left(\frac{X_1 + ... + X_m}{X_1 + ... + X_n}\right) = \mathbb{E}\left(\frac{X_1}{X_1 + .... + X_n}\right) + ... + \mathbb{E}\left(\frac{X_m}{X_1 + .... + X_n}\right)$</p>

<p>But then on symmetry grounds, given that $X_i$ are iid and $m \le n$, all the terms on the right-hand side are the same! Why? Switch the labels of $X_i$ and $X_j$ for $i, j \le n$. Two terms in the denominator switch position but after reordering it still sums to $S_n$, whereas the numerator changes from $X_i$ to $X_j$. So $\mathbb{E}(X_i/S_n) = \mathbb{E}(X_j/S_n)$. Let's write $\mathbb{E}(X_i/S_n)=k$ for $1 \le i \le n$ and since there are $m$ such terms we have $\mathbb{E}(S_m/S_n) = mk$.</p>

<p>It looks as if $k=1/n$ which would produce the correct result. But how to prove it? We know</p>

<p>$k=\mathbb{E}\left(\frac{X_1}{X_1 + .... + X_n}\right)=\mathbb{E}\left(\frac{X_2}{X_1 + .... + X_n}\right)=...=\mathbb{E}\left(\frac{X_n}{X_1 + .... + X_n}\right)$ </p>

<p>It's only at this stage it dawned on me I should be adding these together, to obtain</p>

<p>$nk = \mathbb{E}\left(\frac{X_1}{X_1 + .... + X_n}\right) + \mathbb{E}\left(\frac{X_2}{X_1 + .... + X_n}\right) + ... + \mathbb{E}\left(\frac{X_n}{X_1 + .... + X_n}\right)$ 
$\implies nk = \mathbb{E}\left(\frac{X_1 + ... + X_n}{X_1 + .... + X_n}\right) = \mathbb{E}(1) = 1$</p>

<p>What's nice about this method is that it preserves the unity of the two parts of the question. The reason symmetry is broken, requiring adjustment when $m&gt;n$, is that the terms on the right-hand side after applying linearity of expectation will be of two types, depending on whether the $X_i$ in the numerator lies in the sum in the denominator. (As before, I can switch the labels of $X_i$ and $X_j$ if both appear in the denominator as this just reorders the sum $S_n$, or if neither does as this clearly leaves the sum unchanged, but if one does and one doesn't then one of the terms in the denominator changes and it no longer sums to $S_n$.) For $i \le n$ we have $\mathbb{E}\left(\frac{X_i}{X_1 + .... + X_n}\right)=k$ and for $i&gt;n$ we have $\mathbb{E}\left(\frac{X_i}{X_1 + .... + X_n}\right)=r$, say. Since we have $n$ of the former terms, and $m-n$ of the latter,</p>

<p>$\mathbb{E}(S_m/S_n) = nk + (m-n)r = 1 + (m-n)r$</p>

<p>Then finding $r$ is straightforward using independence of $S_n^{-1}$ and $X_i$ for $i&gt;n$: $r=\mathbb{E}(X_i S_n^{-1})=\mathbb{E}(X_i) \mathbb{E}(S_n^{-1})=\mu \mathbb{E}(S_n^{-1})$</p>

<p>So the same ""trick"" works for both parts, it just involves dealing with two cases if $m&gt;n$. I suspect this is why the two parts of the question were given in this order.</p>
",2013-10-31 01:55:16.093
58563,11775.0,1,,,,Data normalization and sufficient statistic,<machine-learning><normalization><online-algorithms><sufficient-statistics>,CC BY-SA 3.0,"<p>I was taught that when we feed our data to machine learning algorithm (e.g. SVM), we should first normalize our data.</p>

<p>Suppose I have a set of data $X = \{x_1,x_2,...,x_n\}$, I knew two-way of normalizing them, let $\hat{\mu}$ and $\hat{\sigma}^2$ be the sample mean and sample variance of X. I can normalize each data point by</p>

<p>$$
y_k = \frac{x_k-\hat{\mu}}{\hat{\sigma}}
$$</p>

<p>I think I know this when I first learn PCA.</p>

<p>I can also normalize it using the minimum and maximum of the data:</p>

<p>$$
y_k = \frac{x-m}{M-m}
$$</p>

<p>where $M = \max(X)$ and $m = \min(X)$. By using this normalization, I can make sure the normalized data will be in the interval $[0,1]$.</p>

<p>I observe a fact that:</p>

<p>($\hat{\mu}$,$\hat{\sigma}^2$) is sufficient statistic to a normal distribution and the projection matrix of PCA is a solution to a minimization problem that minimize $l^2$-norm.</p>

<p>On the other hand, $(M,m)$ is sufficient statistic to a uniform distribution.</p>

<p>My questions are:</p>

<ol>
<li><p>The observation gives me an intuition that what normalization technique I should employ is depends on my belief (or the learning algorithm believes) of underlying distribution of my data. If I believe the distribution is normal distributed, I should use z-score normalization, if I believe the distribution is uniform distributed, I should use min-max normalization. Is my thought correct?</p></li>
<li><p>If I do not know the underlying distribution, how should I do data normalization?</p></li>
<li><p>If I am going to feed my data to an online learning algorithm (like winnow algorithm), are there any online data normalization technique?</p></li>
</ol>
",2013-10-31 01:59:37.967
58564,20473.0,2,,58561.0,,,,CC BY-SA 3.0,"<p>$$\mathbf X'\mathbf e = \mathbf X'(\mathbf y -\mathbf {\hat  y})= \mathbf X'(\mathbf y -\mathbf X\hat \beta) =...$$</p>

<p><strong>ADDENDUM</strong> </p>

<p>$$=\mathbf X'\left(\mathbf y -\mathbf X (\mathbf X'\mathbf X)^{-1}\mathbf X' \mathbf y\right) =\mathbf X'\mathbf y -\mathbf X'\mathbf X (\mathbf X'\mathbf X)^{-1}\mathbf X' \mathbf y$$</p>

<p>$$\mathbf X'\mathbf y -\mathbf X' \mathbf y = \mathbf 0$$</p>
",2013-10-31 02:40:29.167
58565,23087.0,2,,58555.0,,,,CC BY-SA 3.0,"<p>I can give you a probabilistic/Bayesian interpretation of why this is not helpful. A probabilistic model for data $X$ and parameters $\theta$ is defined by a likelihood $P(X|\theta)$ and a prior $P(\theta)$. Now imagine I have some training data $X_\text{train}$ and want to make predictions about future data $X_\text{future}$, which means I need to calculate, or approximate
$$
P(X_\text{future}|X_\text{train}) = \int P(X_\text{future}|\theta) P(\theta|X_\text{train}) d\theta
$$
where $P(\theta|X_\text{train})$ is the posterior. What you suggest is to sample predictions $X_\text{pred}$ from $P(X_\text{pred}|X_\text{train})$ (which can be represented in the same way as the above equation). However, since $X_\text{pred}$ is not observed you can integrate it away and your posterior on $\theta$ will be unchanged. Conditioning on $X_\text{pred}$ is therefore not a reasonable thing to do. </p>

<p>To speculate on the typical effect it might have: if you <em>sample</em> from $P(X_\text{pred}|X_\text{train})$ then you are both adding noise to your estimate, and reducing the uncertainty in the estimate of $\theta$ (so you would probably be both overconfident and more wrong!), whereas if you optimise $P(X_\text{pred}|X_\text{train})$ I expect the main effect would be reducing the uncertainty in the posterior and thereby making your predictions overly confident (i.e. overfitting).</p>
",2013-10-31 03:14:25.267
58566,23193.0,1,,,,Hypothesis test for difference in preference pre and post treatment,<hypothesis-testing><self-study><references>,CC BY-SA 3.0,"<p>Two large beach holiday destinations near Bangkok, Thailand are Pattaya and Hua Hin. In a random sample of 100 individuals in Bangkok who have been on a weekend away to nearby beach destinations 6 preferred Pattaya and  3 preferred Hua Hin. The other 91 did not have a preference. These individuals were then exposed to certain tourism promotional material and after such exposure 12 preferred Pattaya and 1 preferred Hua Hin. The remaining 87 had no preference. I would like to set up a hypothesis test to determine if the promotional material had an effect on preference. Your assistance or a referral to a URL for a tutorial on this kind of hypothesis testing would be greatly appreciated. </p>
",2013-10-31 03:19:00.427
58567,22564.0,2,,58445.0,,,,CC BY-SA 3.0,"<p>My example here may help: <a href=""https://stats.stackexchange.com/questions/72573/when-making-inferences-about-group-means-are-credible-intervals-sensitive-to-wi?lq=1"">When making inferences about group means, are credible Intervals sensitive to within-subject variance while confidence intervals are not?</a></p>

<p>I modified the model slightly for your data. Note that with such little data your results will be heavily dependent on the priors you use so I would attempt modifying the priors on the group means and precisions (1/variance) and seeing the different results to learn.</p>

<p>Here are the results I got:
<img src=""https://i.stack.imgur.com/uvS4S.png"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/tz1QD.png"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/cyz5V.png"" alt=""enter image description here""></p>

<p>This is modified from John Krushke's example here:
<a href=""http://psy2.ucsd.edu/~dhuber/cr_SimpleLinearRegressionRepeatedBrugs.R"" rel=""nofollow noreferrer"">http://psy2.ucsd.edu/~dhuber/cr_SimpleLinearRegressionRepeatedBrugs.R</a></p>

<p>He has a helpful website and blog:
<a href=""http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/"" rel=""nofollow noreferrer"">http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/</a></p>

<pre><code>#Note. To use rjags you need to first install JAGS from here: 
#http://sourceforge.net/projects/mcmc-jags/files/

install.packages(""rjags"") #run first time to install package

require(rjags) #load rjags package


#Format your data
subID&lt;-rep(1:8,each=4)

dat&lt;-rbind(88, 91, 87, 82,
81, 85, 78, 91,
75, 77, 83, 81,
92, 89, 84, 82,
78, 79, 84, 92,
89, 75, 79, 83,
91, 89, 92, 91,
87, 86, 88, 91
)

dat&lt;-cbind(subID,dat)
colnames(dat)&lt;-c(""Subject"",""Value"")
dat&lt;-as.data.frame(dat)



#Jags fit function
jags.fit&lt;-function(dat){

  #Create JAGS model
  modelstring = ""

  model{
  for(n in 1:Ndata){
  y[n]~dnorm(mu[subj[n]],tau[subj[n]]) T(0, )
  }

  for(s in 1:Nsubj){
  mu[s]~dnorm(muG,tauG) T(0, )
  tau[s] ~ dgamma(5,5)
  }


  muG~dnorm(80,.01) T(0, )
  tauG~dgamma(1,1)

  }
  ""
  writeLines(modelstring,con=""model.txt"")

#############  

  #Format Data
  Ndata = nrow(dat)
  subj = as.integer( factor( dat$Subject ,
                                 levels=unique(dat$Subject ) ) )
  Nsubj = length(unique(subj))
  y = as.numeric(dat$Value)

  dataList = list(
    Ndata = Ndata ,
    Nsubj = Nsubj ,
    subj = subj ,
    y = y
  )

  #Nodes to monitor
  parameters=c(""muG"",""tauG"",""mu"",""tau"")


  #MCMC Settings
  adaptSteps = 1000             
  burnInSteps = 1000            
  nChains = 1                   
  numSavedSteps= nChains*10000          
  thinSteps=20                      
  nPerChain = ceiling( ( numSavedSteps * thinSteps ) / nChains )            


  #Create Model
  jagsModel = jags.model( ""model.txt"" , data=dataList, 
                          n.chains=nChains , n.adapt=adaptSteps , quiet=FALSE )
  # Burn-in:
  cat( ""Burning in the MCMC chain...\n"" )
  update( jagsModel , n.iter=burnInSteps )

  # Getting DIC data:
  load.module(""dic"")


  # The saved MCMC chain:
  cat( ""Sampling final MCMC chain...\n"" )
  codaSamples = coda.samples( jagsModel , variable.names=parameters , 
                              n.iter=nPerChain , thin=thinSteps )  

  mcmcChain = as.matrix( codaSamples )

  result = list(codaSamples=codaSamples, mcmcChain=mcmcChain)

}


output&lt;-jags.fit(dat) # fit the model to your data



###make plots
##Overall plots
par(mfrow=c(2,1))
#Plot overall means
hist(output$mcmcChain[,""muG""],col=""Grey"", freq=F,
main=""Overall Mean"", xlab=""Performance""
)
#Plot overall variance
hist(1/output$mcmcChain[,""tauG""],col=""Grey"", freq=F,
main=""Overall Variance"", xlab=""Performance"")


##Indidvidual Mean Plots
dev.new()
par(mfrow=c(2,4))
for(i in 1:8){
hist(output$mcmcChain[,paste(""mu["",i,""]"",sep="""")],
main=paste(""Mean of Runner"", i), xlab=""Performance"", freq=F, col=""Grey""
)
}


##Indidvidual Variance Plots
dev.new()
par(mfrow=c(2,4))
for(i in 1:8){
hist(1/output$mcmcChain[,paste(""tau["",i,""]"",sep="""")],
main=paste(""Variance of Runner"", i), xlab=""Performance"", freq=F, col=""Grey""
)
}

# see what is in the output
attributes(output$mcmcChain)
</code></pre>

<p><strong>Edit:</strong>
To see the percent of time the model predicts each runner will win we can take the mean and variance estimated for each individual at each mcmc step, then sample a performance from a distribution determined by those parameters. We can then simply count the number of times each runner had the highest performance.</p>

<p><img src=""https://i.stack.imgur.com/WqYSv.png"" alt=""enter image description here""></p>

<pre><code>nSamps&lt;-length(output$mcmcChain[,paste(""mu["",i,""]"",sep="""")])
out=matrix(nrow=nSamps*8,ncol=3)
cnt&lt;-1
for(j in 1:nSamps){
for(i in 1:8){
m&lt;-output$mcmcChain[,paste(""mu["",i,""]"",sep="""")][j]
v&lt;-1/output$mcmcChain[,paste(""tau["",i,""]"",sep="""")][j]
t&lt;-rnorm(1,m,sqrt(v))
out[cnt,]&lt;-cbind(j,i,t)
cnt&lt;-cnt+1
}
}
colnames(out)&lt;-c(""N"",""RunnerID"",""Time"")


winners=matrix(nrow=nSamps,ncol=1)
for(i in 1:nSamps){
sub&lt;-out[which(out[,""N""]==i),]
winners[i]&lt;-sub[which(sub[,""Time""]==max(sub[,""Time""])),""RunnerID""]
}

dev.new()
barplot(100*table(winners)/nSamps, xlab=""Runner ID"", ylab=""% of Wins"")
</code></pre>
",2013-10-31 03:47:01.427
58568,23196.0,1,,,,Ridge regression results different in using lm.ridge and glmnet,<r><regression><ridge-regression><glmnet>,CC BY-SA 3.0,"<p>I applied some data to find the best variables solution of regression model using ridge regression in R. I have used <code>lm.ridge</code> and <code>glmnet</code> (when <code>alpha=0</code>), but the results are very different especially when <code>lambda=0</code>. It suppose that both parameter estimators have the same values. So, what is the problem here? 
 best regards</p>
",2013-10-31 04:07:29.983
58569,16464.0,2,,52567.0,,,,CC BY-SA 3.0,"<p>To give an example in line with @neverKnowsBest's response, consider that in a $2^3$ factorial experiment there are 3 factors, each treated as categorical variables with 2 levels, and each possible combination of the factor levels is tested within each replication. If the experiment were only administered once (no replication) this design would require $2^3=8$ runs. The runs can be described by the following 8x3 matrix:
$$
\left[\begin{array}{rr}
0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
\end{array}
\right]
$$
where the rows represent the runs and the columns represent the levels of the factors: 
$$
\left[\begin{array}{rr}
A &amp; B &amp; C \\
\end{array}
\right].
$$ 
(The first column represents the level of factor A, the second column B, and the third column C). This is referred to as the <strong>Design Matrix</strong> because it describes the design of the experiment. The first run is collected at the 'low' level of all of the factors, the second run is collected at the 'high' level of factor A and the 'low' levels of factors B and C, and so on.</p>

<p>This is contrasted with the model matrix, which if you were evaluating main effects and all possible interactions for the experiment discussed in this post would look like:
$$
\left[\begin{array}{rr}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\
\end{array}
\right]
$$
where the columns represent independent variables:
$$
\left[\begin{array}{rr}
I &amp; A &amp; B &amp; C &amp; AB &amp; AC &amp; BC &amp; ABC \\
\end{array}
\right].
$$
Although the two matrices are related the design matrix describes how data is collected, while the model matrix is used in analyzing the results of the experiment.</p>

<p><strong>Citations</strong></p>

<p>Montgomery, D. (2009). Design and Analysis of Experiments, 7th Edition. John Wiley &amp; Sons Inc.</p>
",2013-10-31 04:28:15.877
58570,23198.0,1,,,,Probability of winning money with a pair of loaded dice,<self-study><binomial-distribution>,CC BY-SA 3.0,"<p>You go gambling with a pair of loaded dice. Because of this, your odds of winning are 53% on every throw. Assuming the game pays 2:1 and you keep betting the same amount, how many games do you need to play to ensure an 80% likelihood of winning money?</p>

<p>I am lost with where to start on this problem. I would appreciate some help so I can figure it out. Thanks.</p>
",2013-10-31 05:32:00.743
58571,22507.0,2,,58570.0,,,,CC BY-SA 3.0,"<p>I assume this is a ""self-study"" problem.  Here are the hints for you:</p>

<ol>
<li>Suppose you play N throws. What is the probability to win in exactly M throws?</li>
<li>How many throws you need to win money?</li>
</ol>
",2013-10-31 06:27:11.663
58738,503.0,5,,,,,,CC BY-SA 3.0,"<p><strong>Tag usage</strong></p>

<ul>
<li>Do <strong>not</strong> use <a href=""/questions/tagged/dispersion"" class=""post-tag"" title=""show questions tagged &#39;dispersion&#39;"" rel=""tag"">dispersion</a> for questions related to dissemination of diseases. Use tag <a href=""/questions/tagged/epidemiology"" class=""post-tag"" title=""show questions tagged &#39;epidemiology&#39;"" rel=""tag"">epidemiology</a> instead.</li>
</ul>
",2013-11-03 00:18:10.927
58572,21029.0,2,,58549.0,,,,CC BY-SA 3.0,"<p>If you're only interested in checking the variability, it sounds like an analysis of variance would be a good start. </p>

<p>Idea 1: Perform an F-test against the different variances for each control. The null hypothesis is that the two controls come from the same normal distribution, but potentially with different means. </p>

<p>Idea 2: an analysis of variance (ANOVA) on the continuous variable separated into the controls. The idea is to look at the variance of the continuous variable <em>within</em> each class $s_i$ and compare it to the total variance $s_t$. The correlation coefficient for one class compared to the total is then $\eta_i = \sqrt{s_i / s_t}$. The test is then an F-test. There is an assumption of a normal distribution here also.</p>

<p>As for the p-values, they represent the level where you can accept or reject the null hypothesis (the variances are all equal). If the p-value is low (below a 5% level, for example) you reject the null hypothesis and assume that the variables have different variances based on the control groups.</p>
",2013-10-31 06:42:01.353
58573,13740.0,1,58576.0,,,Transform multiclass classification to binary - benefits?,<classification><binary-data><multi-class>,CC BY-SA 3.0,"<p>I have 400 instances which must be categorized into 4 classes. Using WEKA, I tried out a couple of multiclass classifiers like J48 and Random Forests, but never made it above Kappa 0.6 and ~65% correctly classified instances (10-fold X-V)</p>

<p>Then I thought about transforming the problem into a 1-vs-all classification, which usually yields accuracies of ~90%. I would then remove the one ""single"" class and keep the merged ones. Then, again, having only instances with 3 classes, I would perform 1-vs-2 and remove the instances classified as belonging to the single class, ending up with a binary classification problem. As I said - I always have like 90% correctly classified instances, but I fear that the 10% incorrectly classified instances add up and propagate through the splitting and dataset reduction process ---</p>

<p>so in the end I would maybe end up with the same garbage output I'd have when performing the original multiclass classification?! What's the stand on this approach? Does it have any benefits at all? </p>
",2013-10-31 07:26:22.987
58574,21823.0,2,,58566.0,,,,CC BY-SA 3.0,"<p>You can apply a chi-squared Test of independence test, but you will face an issue with the fact that only 1 respondent has chosen Hua Hin post receiving a stimulus.</p>

<p>Your alternative would be to run a Fisher's exact test to accommodate less than a frequency of 5 in each cell.</p>

<p>Note: Since your samples are paired, you would want to use a McNemar test instead. </p>
",2013-10-31 07:28:29.130
58575,21823.0,2,,57636.0,,,,CC BY-SA 3.0,"<p>An alternate method of finding variable importance is using random forests.</p>

<p>A package called varSelRF was built specifically for this purpose. This method isn't designed to be right all the time, but is a rather quick way of dealing with large dimensions to get a semblance of which variables could at the first level affect the response variable.</p>

<p>Combine this with an MANOVA and you stand a decent chance of finding your key variables.</p>
",2013-10-31 07:35:55.953
58576,17740.0,2,,58573.0,,,,CC BY-SA 3.0,"<p>Translating a multiclass problem into a set of binary ones (using 1-vs-all or 1-vs-1) is typically done when you want to use algorithms that don't actually have a multiclass formulation, such as SVM. </p>

<p>If you do not plan to change the classification algorithm, you will probably end up with similar results after transforming your problem. </p>

<p><sub>Note that changing algorithm will not necessarily improve your performance.</sub></p>
",2013-10-31 08:01:38.190
58577,22970.0,1,58580.0,,,Generalized linear mixed models: model selection,<mixed-model><model-selection><aic><glmm><stepwise-regression>,CC BY-SA 3.0,"<p>This question/topic came up in a discussion with a colleague and I was looking for some opinions on this:</p>

<p>I am modeling some data using a random effects logistic regression, more precisely a random intercept logistic regression. For the fixed effects I have 9 variables that are of interest and come into consideration. I would like to do some sort of model selection to find the variables that are significant and give the â€œbestâ€ model (main effects only).</p>

<p>My first idea was to use the AIC to compare different models but with 9 variables I was not too exciting to compare 2^9=512 different models (keyword: data dredging).</p>

<p>I discussed this with a colleague and he told me that he remembered reading about using stepwise (or forward) model selection with GLMMs. But instead of using a p-value (e.g. based on a likelihood ratio test for GLMMs), one should use the AIC as entry/exit criterion.</p>

<p>I found this idea very interesting, but I did not find any references that further discussed this and my colleague did not remember where he read it. Many books suggest using the AIC to compare models but I did not find any discussion about using this together with a stepwise or forward model selection procedure.</p>

<p>So I have basically two questions:</p>

<ol>
<li><p>Is there anything wrong with using the AIC in a stepwise model selection procedure as entry/exit criterion? If yes, what would be the alternative?</p></li>
<li><p>Do you have some references that discuss the above procedure that (also as reference for a final report?</p></li>
</ol>

<p>Best,</p>

<p>Emilia</p>
",2013-10-31 08:48:04.587
58578,23205.0,1,,,,random forest classification in R - no separation in training set,<r><classification><random-forest>,CC BY-SA 3.0,"<p>Originally posted on Stack Overflow, but suggested to move here...</p>

<p>I'm new to machine learning, but I've tried to perform a Random Forest classification (randomForest package in R) on some metabolomics data with bad results. My normal approach in this case would be to employ a PLS-DA strategy. However, I decided to try both RF and SVM as there are some publications highly recommending these machine learning approaches for Omics data.</p>

<p>In my case, 'X' is a 16*100 data frame (16 individuals with 100 recorded features/predictors) read from a CSV file. 'Y' is a factor vector (length=16) with 8 'high' and 8 'low'. In both PLS-DA and SVM (both linear and radial kernel) I get excellent separation. However, I get 3 misclassifications out of 16 in the RF model.</p>

<p>The RF model looks like: RFA1=randomForest(X,Y)</p>

<pre><code>## read file and fix data frame
in.data = read.csv2(file='Indata.csv', header = FALSE, skip=5)[,-4] # Col 1-3 are identifiers. Features/predictors from col 4
names(in.data)=names(read.csv2(file='Indata.csv',header=T)[,-4])
# str(in.data)
 # $ ID                       : Factor w/ 27 levels ""2"",""3"",""4"",""5"",..: 2 3 4 6 8 10 20 23 5 11 ...
     # $ period                   : Factor w/ 2 levels ""A"",""B"": 1 1 1 1 1 1 1 1 1 1 ...
 # $ consumption          : Factor w/ 2 levels ""high"",""low"": 1 1 1 1 1 1 1 1 2 2 ...
     # $ FEATURES...

## Sort DF into X (features) and Y (classifier based on consumption)
y = in.data$consumption                   # Classifier based on high vs low consumption
x = in.data[,-1:-3]                       # 100 features/predictors into X NB Contains many NAs
nr=nrow(x)
nc=ncol(x)
x.na = as.data.frame(is.na(x))            # Find NAs in X
col.min=apply(x,2,min,na.rm=T)            # Find min value per feature (omitting NAs)
## Deal with zero/missing data-situation
x2=x                                      # Compute new x2 matrix without NA
for (i in 1:nc) {
    x2[x.na[,i],i]=col.min[i]             # Substitute missing data with col.min
}

## Make classifiers according to period (A vs B)
a.ind = in.data$period=='A'
    b.ind = in.data$period=='B'

## Choose data from period A only &amp; transform/scale X
x2a=x2[a.ind,]                 # Original data
x2a.scale=scale(x2a)           # Scaled
x2a.log=log(x2a)               # Log-transformed
x2a.logscale=scale(log(x2a))   # Log-transformed and scaled
ya=y[a.ind]

## Perform analysis for period A
library(randomForest)
(rfa1=randomForest(x2a,ya))
(rfa2=randomForest(x2a.scale,ya))
(rfa3=randomForest(x2a.log,ya))
(rfa4=randomForest(x2a.logscale,ya))
</code></pre>

<p>This generates output like:</p>

<pre><code>Call:
 randomForest(x = x2a, y = ya) 
               Type of random forest: classification
                 Number of trees: 500
No. of variables tried at each split: 10

        OOB estimate of  error rate: 18.75%
Confusion matrix:
     high low class.error
high    6   2       0.250
low     1   7       0.125
</code></pre>

<p>I have played around with both mtry (5-50) and ntree (500-2000) with no apparent success. I've also tried combinations of transforms and scaling of 'X'. But as I understand it, RF is a non-parametric method and as such, transformations and scaling won't do anything for the results.</p>

<p>For comparison, using the exact same data, PLS-DA using SIMCA13 provides excellent separation already in the 1st component. SVM using the kernlab package in R provides 0 training error. At this stage I'm not looking at validation or using test sets. I want to first make sure I get good classification on my training set.</p>

<p>I'm sure I'm missing something, but I don't know what. I hope to have supplied sufficient information to describe the problem.</p>

<p>Thanks in advance for any help!</p>

<p>Sincerely,</p>

<p>Calle</p>
",2013-10-31 09:48:41.600
58579,1575.0,1,,,,What languages are commonly used in medical statistics?,<r><python><matlab><software>,CC BY-SA 3.0,"<p>A doctor friend has asked if I would teach them some basic programming, because they think it will be useful when they start to do research in the future.</p>

<p>To give some background - they don't know what language their lab-mates will use (because they don't know what lab they are joining) but they would like to learn something that will be generally useful. They are not interested in doing statistical research, but rather applying statistical techniques to medical data.</p>

<p>There are some obvious options (I know enough about all of these to teach them)</p>

<ul>
<li>Python</li>
<li>R</li>
<li>MATLAB</li>
</ul>

<p>and a few less obvious ones (I don't know anything about these)</p>

<ul>
<li>Stata</li>
<li>SAS</li>
<li>SPSS</li>
</ul>

<p>I'd like to know what languages are most commonly used in medical statistics - do people use general purpose languages like Python, or more specialized languages like R and MATLAB, or statistical software like Stata/SAS/SPSS?</p>

<p>I should clarify that I'm thinking of someone whose primary job is as a doctor/medical researcher who will be applying statistics, not someone who is a professional statistician or software developer who happens to be working in medicine (so e.g. I think that C/C++/Java would all be poor choices).</p>
",2013-10-31 10:09:54.433
58580,503.0,2,,58577.0,,,,CC BY-SA 3.0,"<p>Stepwise selection is wrong in multilevel models for the same reasons it is wrong in ""regular"" regression: The p-values will be too low, the standard errors too small, the parameter estimates biased away from 0 etc.  Most important, it denies you the opportunity to think.</p>

<p>9 IVs is not so very many. Why did you choose those 9? Surely you had a reason.</p>

<p>One initial thing to do is look at a lot of plots; which precise ones depends a little on whether your data are longitudinal (in which case plots with time on the x-axis are often useful) or clustered. But surely look at relationships between the 9 IVs and your DV (parallel box plots are one simple possibility). </p>

<p>The ideal would be to build a few models based on substantive sense and compare them using AIC, BIC or some other measure. But don't be surprised if no particular model comes forth as clearly best. You don't say what field you work in, but in many (most?) fields, nature is complicated. Several models may fit about equally well and a different model may fit better on a different data set (even if both are random samples from the same population).</p>

<p>As for references - there are lots of good books on nonlinear mixed models. Which one is best for you depends on a) What field you are in b) What the nature of the data is c) What software you use. </p>

<p><strong>Responding to your comment</strong></p>

<ol>
<li><p>If all 9 variables are scientifically important, I would at least consider including them all. If a variable that everyone thinks is important winds up having a small effect, that is interesting.</p></li>
<li><p>Certainly plot all your variables over time and in various ways. </p></li>
<li><p>For general issues about longitudinal multilevel models I like <a href=""http://rads.stackoverflow.com/amzn/click/0471420271"">Hedeker and Gibbons</a>; for nonlinear longitudinal models in SAS I like <a href=""http://rads.stackoverflow.com/amzn/click/0387251448"">Molenberghs and Verbeke</a>. The SAS documentation itself (for <code>PROC GLIMMIX</code>) also provides guidance. </p></li>
</ol>
",2013-10-31 10:11:45.777
58698,21762.0,2,,58694.0,,,,CC BY-SA 3.0,"<p>The distribution of the response is irrelevant. Inference based on small samples requires the errors to be approximately normal (better look at the QQ-plot of the residuals than at its density because the tails are important). If you are only interested in descriptive results or if the sample size is not too small, you therefore do not need to worry about normality. </p>

<p>Much more important are the other assumptions of linear regression (correct model structure, no large outliers in the predictors and, if you are interested in inference, homoscedastic and uncorrelated errors).</p>
",2013-11-02 11:05:26.930
58581,12503.0,1,58607.0,,,Measure of similarity/distance of data points in geographic space,<clustering><distance-functions>,CC BY-SA 3.0,"<p>Given two points $p_1=(x_1,y_1,t_1)$ and $p_2=(x_2,y_2,t_2)$, where $x$ and $y$ refer to the geographic coordinates in the plane, and $t$ to some measured value.
Two distance measures to evaluate the similarity between these two points come to my mind:
$$d_1(p_1,p_2) = \sqrt{ (x_1-x_2)^2+(y_1-y_2)^2+(t_1-t_2)^2 }$$
$$d_2(p_1,p_2) = \sqrt{ (x_1-x_2)^2+(y_1-y_2)^2 }+ \sqrt{ (t_1-t_2)^2 }$$</p>

<p>Measure $d_1$ is simply Euclidean distance in 3d-space, while $d_2$ is the sum between spatial distance and attribute distance.</p>

<p>Which measure does make more sense and should be applied for e.g. clustering?</p>
",2013-10-31 10:13:33.410
58582,503.0,2,,58579.0,,,,CC BY-SA 3.0,"<p>This question seems to me to conflate two issues: Programming and statistics. </p>

<p>I don't know what programming languages are used in medical labs, although I get the sense that none are. In terms of statistics, I'd say <code>R</code> and <code>SAS</code> dominate. These are radically different languages. The problem with trying to teach <code>SAS</code> is 1) Since it isn't on your list you probably don't know it and 2) You'd have to have access to it, and it's expensive. That would lead to teaching <code>R</code>. But if they wind up in a place that uses <code>SAS</code> exclusively, I don't think <code>R</code> would help much.</p>

<p>However, if the person will be doing their own analysis, then <code>R</code> is fine. I would try to teach reproducible research methods. </p>
",2013-10-31 10:17:18.677
58583,23208.0,1,,,,Logistic Regression: wald chi-square,<regression><logistic>,CC BY-SA 3.0,"<p>I am running a logistic regression with a binary dependent variable and 5 class independent variables.</p>

<p>One information the software I use returns, is the following:</p>

<p>Effect / DF  /  Wald / Pr > ChiSq</p>

<p>Var1 / 1    / 150 / &lt;.0001</p>

<p>Var2 / 3 / 119 / &lt;.0001</p>

<p>Var3 / 8 / 157 / &lt;.0001</p>

<p>Var4 / 6 / 1553 / &lt;.0001</p>

<p>Var5 / 4 / 15975 / &lt;.0001</p>

<p>Concerning the data above I have two questions:</p>

<p>I)  How can I interpret the fact that for var5 the value of the wald chi-square is so much higher than the values of the remaining variables?  And is it a bad indicator of the regression quality?</p>

<p>II) I saw the impact of a regression variable calculated as the division between the wald chi-square value of the variable and the total sum of all the variables wald chi-square values (shown in percentageâ€¦.is this example var5 would represent 88.9% of the probability resulting from the logistic regression), but found no statistical grounds for this methodology. Anyone knows this methodology or others?   </p>

<p>Any guidance will be helpful</p>
",2013-10-31 10:51:28.807
58584,10735.0,1,58590.0,,,"Is this a correct use of ""optimal""?",<classification><optimization><terminology>,CC BY-SA 3.0,"<p>During fine-tuning some classifier parameters, is it correct to talk about ""optimal"" configuration?</p>

<p>For example, if perfect classification is impossible, but the error converges to some relatively good small value, is it correct to call it an ""optimal"" solution, or is it ""suboptimal"" because there's no single optimal one?</p>

<p>If there are several configurations that perform best and equally good, would it be correct to call either of them ""optimal""?</p>
",2013-10-31 11:43:14.730
58585,23211.0,1,,,,Multiprocess models,<survival><random-effects-model>,CC BY-SA 3.0,"<p>I would like to  use multi-process models. in particular, I would like to apply this model: Kulu, Hill. ""Migration and fertility: Competing hypotheses re-examined."" European Journal of Population/Revue europÃ©enne de DÃ©mographie.</p>

<p>In the fertility equations I don't understand how the estimation procedure differs between the two models (conceptually I get the difference but I am rather confused when it comes to the actual methodological application). In particular, to let the errors vary across the three migration equation is the estimation procedure extended in a discrete-time setting?</p>

<p>Additionally, i donÂ´t understand whether the author distinguishes between a first birth after first migration and first birth after second migration or whether he is just considering first birth after whatever migration.</p>

<p>Can someone help me to better understand? Textbooks about the topic are welcome</p>

<p>thank you! </p>
",2013-10-31 11:57:07.880
58586,23171.0,1,58612.0,,,"Is $X_{i}$ a martingale, submartingale, or supermartingale?",<probability><model-selection><stochastic-processes><marginal-distribution>,CC BY-SA 3.0,"<p>Let $X_{i}$, $i=0,1,\cdots$ be a sequence of random variables generated by $X_{i+1}=\rho X_{i}+W_{i}$, where $\rho$ is constant and $W_{i}$ are i.i.d random variables. Suppose $X_{0}$ is independent of $W_{i}$. Is $X_{i}$ a martingale, submartingale, or supermartingale?   </p>
",2013-10-31 12:45:51.320
58587,23179.0,1,,,,coding survey data for cosine similarity and euclidean distance?,<survey><distance-functions><similarities>,CC BY-SA 3.0,"<p>I want to know how to code survey data such that a similarity function can be applied on it.</p>

<p>Say I want to use cosine similarity. All the search results and QA I've found while in my search deal only with the similarity between <em>documents</em>, with vectors consisting of word frequencies or tf/idf.</p>

<p>What about survey data? What is the sensible/common/useful way of coding survey data such that similarity can be compared? (Is it even sensible to use functions like cosine similarity for this?)</p>

<p>My data is record data, purely categorical, neither binary nor numerical. Should I code it into numerical data? My data looks like this (3 sample records):</p>

<pre><code>Do you like Technology?  | Current GPA       | Institute name
Y                        | Band 1 (3.75-4)   | UUIC
N                        | Band 3 (3.0-3.5)  | ADU
N                        | Band 2 (3.5-3.75) | UUIC
</code></pre>

<p>etc. These are just 3 questions, my survey had a lot more questions, but I hope you get the idea.</p>

<p>Is it sensible to code the data into numerical vectors, where for eg. I represent yes/no values as binary variables, and assign numbered categories to other values? In which case the above 3 records would become:</p>

<pre><code>(1, 1, 1)
(0, 3, 2)
(0, 2, 1)
</code></pre>

<p>Where UUIC = 1, ADU = 2, and the GPA bands are represented simply by 1, 2, 3, 4, etc..</p>

<p>And then apply cosine similarity or euclidean distance? Would this make sense? I've been searching for similar examples for a while now but everything that comes up seems to be about <em>document similarity</em>. There doesn't seem to be much beginner's help on how to deal with <em>survey data</em>. </p>
",2013-10-31 13:20:51.977
58588,9755.0,1,,,,How to get proper randomization in website A/B testing?,<experiment-design><ab-test><random-allocation>,CC BY-SA 3.0,"<p>In the statistical practice of experimental design, you separate your tests by 'blocks' if you can control the factors (say routing to one page or another, or categorizing by browser) or through randomization of trials to counteract variables you don't control.</p>

<p>When doing A/B testing, you obviously can't control who comes to your site when. How can good statistical practice be done?</p>
",2013-10-31 13:24:46.067
58699,15827.0,2,,58694.0,,,,CC BY-SA 3.0,"<p>Your distribution is not beta if your density plot is to be taken at face value. A beta distribution cannot have two modes within (0, 1). However, no density plot for a bounded variable (at a guess here from some kernel density estimation procedure) can be taken at face value unless the estimation includes adjustments for boundary artefacts, which is not typical. But, as it were, we see what you mean. </p>

<p>However, to focus on the major issues: </p>

<ul>
<li><p>A regression is first and foremost a model for the mean of a variable as it varies with the predictors. Even if an assumption of normal errors is made, that is <strong>not</strong> an assumption about the marginal distribution of the response and it is the <strong>least important assumption</strong> that is being made. So, it is not surprising that your regression behaves fairly well as far as can be inferred from the distribution of residuals if the functional form catches the way that conditional means behave. </p></li>
<li><p>The assertion of normality is more convincing if you show us a normal probability plot. That distribution looks to me to have higher kurtosis than a normal, although that is likely to be a little deal. </p></li>
<li><p>You need to check that your model is predicting values within [0,1]. Some of your residuals are about 0.7 in magnitude and so it seems possible that some of the predictions are qualitatively wrong. </p></li>
<li><p>At the same time, you should be able to do better with a regression that respects the bounded nature of the response. You could try beta regression or a generalised linear model with binomial family and logit link. The latter sounds wrong but often works well in practice. For a concise introductory review, see <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0147"">http://www.stata-journal.com/sjpdf.html?articlenum=st0147</a> Beta regression is supported in R and Stata (and likely so in other software) and generalised linear models are widely supported, although watch for routines that reject non-binary responses if a logit link is requested.  </p></li>
</ul>

<p>Note: The exact form of your density plot for the response is a side-issue, so I will make this an added note. It's clear that the density for a variable bounded by 0 and 1 must average 1. Your graph has a useful reference line density at 1. Visually comparing the bump above 1 on the left with the area to its right underlines that some of the density has been smoothed away by the procedure beyond the support and discarded. That is, the graph shown truncates the display: the smoothed distribution has positive density below 0 or above 1, which is not shown. There are known ways to smooth a bounded variable more respectfully, in this case including (a) to smooth logit of the variable and back-transform the density (a little problematic if observed values include 0 or 1), or (b) to reflect density inwards at the extremes. Naturally, there is scope for disagreement about whether this is trivial or secondary on the one hand or incorrect on the other. (I'd rather see a quantile plot of the data, but I'll not expand on that.) </p>
",2013-11-02 11:06:50.860
58737,503.0,4,,,,,,CC BY-SA 3.0,"This tag is very ambiguous. Dispersion is a general term for how spread apart values are. For questions related to disease dissemination content, use the tag epidemiology. For other meanings of dispersion consider using a related tag or creating a new one.",2013-11-03 00:18:10.927
58589,5273.0,1,58594.0,,,Gradient boosting in R uses only a single variable,<r><machine-learning><boosting>,CC BY-SA 3.0,"<p>I am trying to build a boosting model using the package <a href=""http://cran.r-project.org/web/packages/gbm/index.html"" rel=""nofollow"">gbm</a> in R. I have the following code:</p>

<pre><code>gb = gbm(aaa_target ~ .,
     data=myDdata,
     n.trees=100,
     verbose=TRUE)
</code></pre>

<p>and when I have trained the model, I can get a summary like this:</p>

<pre><code>summary(gb)
</code></pre>

<p>The issue I am having, is that only a single variable (out of around 30) is selected and is given 100% predictive power. I know for a fact that many of the variables carry information (although the selected one is the most significant one), and using the randomForest package gives me a model which assigns significance to many of the variables.</p>

<p>Does anybody have a clue to why this might be the case?</p>
",2013-10-31 13:48:40.793
58590,20473.0,2,,58584.0,,,,CC BY-SA 3.0,"<p>""Optimal"", ""Best"", ""Equally good"", are all concepts <em>relative</em> to some criterion. Once you specify your criterion, the ranking of alternative configurations can be determined, and the first in rank will be <em>optimal according to the specific criterion</em> -no more than that.  </p>

<p>And it so happens that in most cases, a configuration optimal given criterion A, is not optimal given criterion B.</p>

<p>If, given the same criterion, two alternative configurations ""perform equally well"", then, strictly speaking, they are both ""optimal"" -although if the criterion is some continuous quantity, it will be difficult for the two to perform <em>exactly</em> the same. Still, in practice, if their performance according always to the same criterion is ""very close"" (vague term), then we usually say that they are ""equivalent"" in terms of predictive or explanatory power.  </p>

<p>Finally, the criterion is not ""achieve the ideal"" (say, zero error), but ""how close to the ideal can you be"" (minimum error).</p>
",2013-10-31 13:58:27.493
58591,5211.0,1,58621.0,,,Converting 2nd order Markov chain to the 1st order equivalent,<self-study><markov-chain>,CC BY-SA 3.0,"<p>Given a 2nd order Markov chain where each state takes values in the set $\mathcal{X}=\{A,C,G,T\}$, such that all transition probabilities $p(x_t|x_{t-1},x_{t-2})$ are larger than zero,</p>

<p>How to convert it to the equivalent 1st order one with all the transition probabilities defined?</p>
",2013-10-31 13:59:26.240
58592,23216.0,1,,,,Regression where a subset of observations are missing data on an independent variable,<regression><missing-data>,CC BY-SA 3.0,"<p>Consider the regression equations below:</p>

<p>\begin{align}
Y_i &amp;= \beta_0 + \beta_1 X_{i1} + \varepsilon_i  \\
Y_j &amp;= \beta_0 + \beta_1 X_{j1} + \beta_2 X_{j2} + \varepsilon_j
\end{align}</p>

<p>where $Y_i,\ X_{i1},\ \varepsilon_i,\ Y_j,\ X_{j1},\ \&amp; \ X_{j2},\ \varepsilon_j$  are vectors, and $_i$ and $_j$ index distinct sets of observations. The $_i$ respondents did not meet a qualification criterion and hence were not asked the question that corresponds to $X_2$. </p>

<p>The dependent variable and the first independent variable is the same in both regression equations but the second regression equation has an independent variable that is not present in the first. Obviously, I can estimate the two regressions separately but that will not be efficient. Therefore, I was considering re-writing the first one as:</p>

<p>$$
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \varepsilon_j
$$</p>

<p>where $X_{i2}$ is a vector of $0$s. </p>

<p>Then I can estimate the parameter estimates by using OLS with the equation below:</p>

<p>$\left[ \begin{array}{ccc}
Y_i\\
Y_j\end{array} \right] = \left[ \begin{array}{ccc}
{\bf 1} &amp; X_{i1} &amp; X_{i2} \\
{\bf 1} &amp; X_{j1} &amp; X_{j2}\end{array} \right] \left[ \begin{array}{ccc}
\beta_0\\
\beta_1\\
\beta_2\end{array} \right] + \left[ \begin{array}{ccc}
\epsilon_i\\
\epsilon_j\end{array} \right]$</p>

<p>In the above equation, ${\bf 1}$ stands for a vector of $1$s of the appropriate dimension.</p>

<p>Is the above a standard approach to obtaining efficient estimates? Is there a name to this way of estimation?</p>
",2013-10-31 14:18:26.747
58593,23215.0,1,,,,Weighted Random Selection with bounded replacement,<random-variable><sample>,CC BY-SA 3.0,"<p>The problem:
Let there be an array (noted as X) where each item has a member, and a label assigned to it. For example:</p>

<p>$$
X= \{a_0, b_0, b_1, c_0, c_1, a_1\}
$$</p>

<p>Here the letter denotes a label, and the index is just another instance from the given label. Now the goal is to sample n items from X into Y, where:</p>

<p>$$
|Y|=n &lt; m = |X|
$$</p>

<p>and each label has a weight assigned to it, so that in the long run the weights will define the distribution of the elements in Y. Elements from within a label are different, but whichever of them may be selected. The multiplicity of the labels are random too (it may vary from 1 to 10). The length of X will be usually around 100, from which around 10 items need to be selected. </p>

<p>So, let's say if the weights are </p>

<p>$$
W = \{a=5, b=45, c=50\} 
$$
for 10,000 runs if we add up the elements from Y it will be true that 5% of it will be from a, 45% from b, and 50% from c. </p>

<p>The indexes just denote that you have multiple item instances from a label. There is no frequency requirement for within label (so simple random sampling can/should be used). </p>

<p>The sampling is without replacement (considering the full array (X), however it is with bounded replacement if you consider just the labels (that is a,b,c labels).</p>

<p>At the end we do not need exact match with W, expected value is okay. Successive runs may result in slightly different output, that too is okay (although the expected value of the frequency of labels, should approximate W).</p>

<p>Now, I've already tried to do this in python via the roulette wheel selection, where if a label has no more items I redo the roulette, however this sometimes goes really off. </p>

<ol>
<li><p>What's a good algorithm for this?</p></li>
<li><p>What's wrong with my code? (why the big difference sometimes - here's the python code <a href=""http://pastebin.com/SFhV74Z8"" rel=""nofollow"">http://pastebin.com/SFhV74Z8</a>)</p></li>
</ol>
",2013-10-31 14:25:15.797
58594,5917.0,2,,58589.0,,,,CC BY-SA 3.0,"<p>Because the overworked maintainers of the <code>gbm</code> package have not had time to implement random feature sampling at each split calculation yet.  I submitted a bad patch that did this as a proof of concept, but:</p>

<ul>
<li>My C++ skills are non existent</li>
<li>I provided no documentation</li>
<li>I didn't integrate with the formula interface wrapper</li>
</ul>

<p>So I feel no ill will for not picking up the patch.  I haven't maintained the fork either so I'm sure it wouldn't integrate with the current gbm.  You can see where I left off here: <a href=""https://code.google.com/r/sheaparkes-mtry-additions/source/browse"" rel=""nofollow"">https://code.google.com/r/sheaparkes-mtry-additions/source/browse</a></p>

<p>If you really need the feature sampling functionality, it's available in Python's scikit.learn package implementation of gbm.</p>
",2013-10-31 14:33:02.057
58595,20470.0,2,,58591.0,,,,CC BY-SA 3.0,"<p>The first order transition matrix: $T^1$ is of size $[k*k]$. And the second order transition matrix: $T^2$ is of size $[k^2*k]$. So you want to reduce the number of rows from $k^2$ to $k$ by merging.</p>

<p>An example is given on the Wikipedia <a href=""http://en.wikipedia.org/wiki/Markov_chain#Music"" rel=""nofollow"">link</a>, you should be able to convert $T^2$ to $T^1$ simply by marginalising over the $t-2$ states (which are not needed for $T^1$) at each column.</p>

<p>My explanation is probably not crystal clear but I think you will understand what I mean once you see the example on the link.</p>
",2013-10-31 14:34:26.360
58596,633.0,2,,58477.0,,,,CC BY-SA 3.0,"<p>Is it not intuitive that you cannot reason from cause to unobserved effect to another cause?  If the rain (B) and the sprinkler (D) are causes of the wet ground (C), then can you argue that seeing rain implies that the ground is probably wet, and continue to reason that the sprinkler must be on since the ground is wet?!  Of course not.  You argued that the ground was wet because of the rain â€” you can't look for additional causes!</p>

<p>If you observe the wet ground, of course the situation changes.  Now you may be able to reason from one cause to the other as Frank explains.</p>
",2013-10-31 14:36:28.603
58597,306.0,2,,58413.0,,,,CC BY-SA 3.0,"<p>In order to check if a variable is significant for a category, do a hypothesis testing for a binomial variable assuming that the probability of getting a 1 is 0.5. In order to answer these other questions, do the same for each of the constituent variables. if the p value in the two cases is sufficient to reject the null hypothesis, then you can claim whether that variable or the set of variables is characteristic or not for the category. if you cannot reject it, then the variables behave randomly for that category. Check the link <a href=""http://en.wikipedia.org/wiki/Binomial_test"" rel=""nofollow"">Binomial test.</a></p>
",2013-10-31 14:46:39.507
58598,23218.0,1,58703.0,,,Finding probability density function with unknown values,<probability><self-study><density-function>,CC BY-SA 3.0,"<p>I am not sure if this is the place to ask, have tried to read up on probability density function (PDF) in order to answer this question but to no avail. </p>

<p>How do I go about starting on this? </p>

<p>How do I generate a function based on the given $\lambda$ and that small $t$ symbol. </p>

<p>What is $X \sim Ga(n,\lambda)$? </p>

<p>An explanation before the answer would be nice. Appreciate all the help needed. </p>

<p>Regards</p>

<p><img src=""https://i.stack.imgur.com/OzD7G.png"" alt=""Stats Question""></p>
",2013-10-31 14:47:22.883
58599,22970.0,1,,,,Generalized Linear Mixed Models: Diagnostics,<mixed-model><outliers><glmm><cooks-distance>,CC BY-SA 3.0,"<p>I have a random intercept logistic regression (due to repeated measurements) and I would like to do some diagnostics, specifically concerning outliers and influential observations. </p>

<p>I looked at residuals to see if there are observations that stand out. But I would also like to look at something like Cook's distance or DFFITS. Hosmer and Lemeshow (2000) say that due to the lack of model diagnostic tools for correlated data, one should just fit a regular logistic regression model ignoring the correlation and use the diagnostics tools available for regular logistic regression. They argue that this would be better than doing no diagnostics at all.</p>

<p>The book is from 2000 and I wonder if there are methods available now for model diagnostics with mixed effects logistic regression? What would be a good approach to check for outliers?</p>

<p><strong>Edit (Nov 5, 2013):</strong></p>

<p>Due to the lack of responses, I am wondering if doing diagnostics with mixed models is not done in general or rather not an important step when modeling data. So let me rephrase my question: What do you do once you found a ""good"" regression model?</p>
",2013-10-31 14:55:26.660
58600,22511.0,1,,,,Finding maximum likelihood,<r><maximum-likelihood>,CC BY-SA 3.0,"<p>This is a model that is used to model soccer scores, so $i$ and $j$ are, respectively, home and away teams. Random variables $(x,y)$ are the goals scored by the home and away teams, respectively. Parameter $\lambda$ is a known mean goals scored by the home team and $\mu$ is the mean goals scored by the away team. I have managed to fix all the other parameters except for $\rho$, which I have to estimate via MLE.</p>

<p>$$Pr(X_{i,j}=x, Y_{i,j}=y)=\tau_{\lambda, \mu}(x,y)\frac{\lambda^x \text{exp}(-\lambda)}{x!}\frac{\mu^y\text{exp}(-\mu)}{y!}$$
where
$$\lambda=\alpha_{i}\beta_{j}\gamma$$
$$\mu=\alpha_{j}\beta_{i}$$
and
$$\tau_{\lambda,\mu}(x,y)=\left\{\begin{array}{cc}
1-\lambda\mu\rho &amp;\text{if $x=y=0$,} \\
1+\lambda\rho &amp;\text{if $x=0,y=1$,}\\
1+\mu\rho &amp;\text{if $x=1,y=0$,}\\
1-\rho &amp;\text{if $x=y=1$,}\\
1 &amp;\text{otherwise}\end{array}
\right.$$</p>

<p>Based on the above equations, all the parameters $(\lambda, \mu, \alpha, \beta, \gamma)$ are known constants.</p>

<p>So, now, the problem that I am having is that I have no clue on how to estimate $\rho$ using the maximum likelihood function since a piece-wise equation is involved.</p>

<p>Also, it will be great if anyone can do this using R.</p>
",2013-10-31 14:57:59.460
58601,2081.0,2,,58587.0,,,,CC BY-SA 3.0,"<ol>
<li>Both <strong>cosine</strong> similarity and <strong>euclidean</strong> distance require
<em>scale</em> (=metric) level data, that is, interval or ratio level. I suppose it is what you mean by ""numeric"". Also, <em>binary</em> data (1 vs
0) will do (though there is theoretical controversy). Nominal data -
convert it into dummy binary data first. Ordinal data - see to
choose either treat it as interval or nominal. </li>
<li>Squared euclidean
distance and cosine similarity are exactly related. You always can
transform one into the other. <a href=""https://stats.stackexchange.com/a/36158/3277"">1</a>.</li>
<li>It is not generally a good idea to compute a (dis)similarity coefficient on a hodge-podge of different characteristics (different types and/or units) even if you do recodings mentioned in point 1 and do appropriate standardizations prior the computation, because there remains issue of weighting (relative importance) of the characteristics. Gower similarity (rather than cosine/euclidean) is the measure of choice, if you are nevertheless determined to base the coefficient on mixed characteristics. It can ""take"" interval, ordinal, binary and nominal ones (and with used-defined weighting, if necessary). <a href=""https://stats.stackexchange.com/a/15313/3277"">1</a>, <a href=""https://stats.stackexchange.com/a/22213/3277"">2</a>.</li>
<li>If you are going to compute similarity based on binary characteristics only, be aware that not all of of a great variety of binary ""matching"" similarity coefficients equally well suit natural binary characteristics and <em>dummy</em> variables (i.e. former nominal ones). <a href=""https://stats.stackexchange.com/a/55802/3277"">1</a>.</li>
</ol>
",2013-10-31 15:04:11.687
58602,14965.0,1,58749.0,,,Time Varying System Matrices in Kalman Filter,<kalman-filter><time-varying-covariate>,CC BY-SA 4.0,"<p>Kalman filter can accommodate time varying system matrices. Equations to run the filter are the same and it preserves its optimality under linear gaussian model. </p>

<p>My question is the following:</p>

<p>Can the evolution of time varying system matrices be stochastic? In some references I seem to read between the lines that they should evolve deterministically. Does it mean that the entire filter breaks or do we simply lose optimality by making them stochastic?</p>

<p>For reference, please peek at section 3.2 of the following paper:</p>

<p><a href=""http://www.ims.cuhk.edu.hk/~cis/2012.1/CIS%2012-1-05.pdf"" rel=""nofollow noreferrer"">http://www.ims.cuhk.edu.hk/~cis/2012.1/CIS%2012-1-05.pdf</a></p>

<p>A similar comment is in Harvey's book on Kalman Filter.</p>
",2013-10-31 15:33:02.747
58603,16159.0,2,,58119.0,,,,CC BY-SA 3.0,"<p>If I were doing a binomial test, and I had 100% of my results indicating pass until the nth sample, at which time I had one sample indicate fail, then I would say that my estimate of the binomial distribution parameters and their confidence intervals might be very different before versus after the ""fail"".</p>

<p>My binomial test is whether or not the samples belong to an expected distribution which if true gets a ""pass"" or whether they qualify as an outlier in which case the result is ""fail"".  </p>

<p>Keywords of interest may include ""zero defect sampling"", and ""acceptance sampling"".</p>

<p>Reference links:</p>

<ul>
<li><a href=""http://asq.org/quality-progress/2007/11/basic-quality/zero-defect-sampling.html"" rel=""nofollow"">ASQ article</a>   </li>
<li><a href=""http://src.alionscience.com/toolbox/oneshotcalc.htm"" rel=""nofollow"">Calculator </a></li>
</ul>
",2013-10-31 15:42:29.387
58604,20538.0,2,,58588.0,,,,CC BY-SA 3.0,"<p>In high volume sites just randomly sending the user to one of your test pages or control (let's call them factor levels) usually works fine.  There may be some variability in the mix of browsers between the factor levels but you'll have a sample size that's large enough for it not to matter so much.  This is also very easy to implement, even if you randomly load-balance or shard across multiple servers. </p>

<p>For low-volume sites where you have $n$ factor levels you could generate random permutations and force the next $n$ users to the factor level according to the permutation.  You could do this within each group you define (e.g. browser type).  This does require keeping track of a global state, and assumes you can do so fast enough that you can observe an order in which the users arrive.  I don't know of anyone that does this.</p>
",2013-10-31 15:42:31.400
58605,7007.0,2,,58600.0,,,,CC BY-SA 3.0,"<p>If I understand it, your data is $\{(i_m,j_m,x_m,y_m) : m=1,\dots,n\}$, in which, for the $m$-th match, $i_m$ is the index of the home team, $j_m$ is the index of the away team ($i_m\neq j_m$), $x_m$ is the number of goals scored by the home team, and $y_m$ is the number of goals scored by the away team.</p>

<p>If we have $t$ teams disputing the $n$ matches, the likelihood $L(\rho,\gamma,\alpha_i,\beta_i, i=1,\dots,t)$ is proportional to
$$
  \prod_{m=1}^n \tau_{\lambda_m,\mu_m}(x_m,y_m)\,\lambda_m^{x_m} \,\mu_m^{y_m}\,e^{-(\lambda_m+\mu_m)} \, ,
$$
in which $\lambda_m = \alpha_{i_m} \beta_{j_m}\gamma$ and $\mu_m = \alpha_{i_m} \beta_{j_m}$. This likelihood is a function of $2(t+1)$ parameters. You will probably need to implement some conjugate gradient method to solve this constrained optimization problem. I wonder what happens if we put some priors on the parameters and MCMC the posterior with a random walk Metropolis. You said that you know the values of the parameters, except for $\rho$ (and I wonder how...). Hence, your first step is to write an <code>R</code> function that computes the product above for each value of $\rho$. Then, plot it and use the optimizer to see what happens.</p>
",2013-10-31 15:49:28.083
58606,23220.0,1,,,,Export variance-covariance matrix using PROC GLM,<regression><generalized-linear-model><sas><covariance-matrix>,CC BY-SA 3.0,"<p>I have a ordinary linear regression model like this</p>

<pre><code>y = b0 + b1*x + b2*z + b3*x*z
</code></pre>

<p>I used <code>PROC GLM</code> in <code>SAS</code> to test the model. Now I want to export the variance-covariance matrix of the coefficients (<code>b0</code>, <code>b1</code>, <code>b2</code>, and <code>b3</code>). However, I didn't find any option to export it. I can't not use <code>PROC REG</code> because of the interaction term. </p>

<p>Does anyone know how to get it? </p>
",2013-10-31 15:57:08.483
58607,5671.0,2,,58581.0,,,,CC BY-SA 3.0,"<p>Neither makes ultimately sense.</p>

<p>First of all, <strong>Earth is not flat</strong>. Don't use Euclidean distance on latitude, longitude coordinates, because that is highly inaccurate.</p>

<p>So lets assume you don't have GPS data, but, e.g. meters in a room; then Euclidean on this attribute makes sense.</p>

<p>Control yourself by looking at <strong>units</strong>. Physical data has units, for a very good reason...
$$
\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \sim_\text{units} \sqrt{m^2 + m^2} \sim m
$$</p>

<p>I.e. Euclidean distance, applied to two coordinates in meter, returns a distance in meters!</p>

<p>Now assume that your third attribute is e.g. Volt.</p>

<p>$$
\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(v_1-v_2)^2} \sim_\text{units} \sqrt{m^2 + m^2 + V^2} \sim ???
$$</p>

<p>You cannot add (squared) meters to (squared) volts. They are entirely different things.</p>

<p>You might, instead, want to look at an algorithm that can deal with <strong>multiple relations</strong>. For example <strong>Generalized DBSCAN</strong> can trivially be used to cluster this data by specifying a different $\varepsilon$ for each Relation. You would then specify ""neighbors"" as ""within 1 meter of distance and 10 Volts in the measurement"".</p>

<p>See how nicely this works out for some algorithms to keep different data separate?</p>
",2013-10-31 16:00:49.880
58608,5374.0,1,,,,Why is Standard Deviation used to evaluate the effect of a change in an independent variable on the dependent one?,<standard-deviation><interpretation>,CC BY-SA 3.0,"<p>I'm reading some economics papers about the relationship between inequality and growth and some of them have sentences like these:</p>

<blockquote>
  <p>an <em>increase of 0.07 (one standard deviation</em> in the sample) in the income share of the top 20 percent lowers the average annual growth rate just below half a percentage point</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>the estimated coefficients imply that an increase in, say, the land Gini coefficient <em>by one standard deviation</em> (an increase of 0.16 in the Gini index) would lead to a reduction in growth of 0.8 percentage points per year</p>
</blockquote>

<p>Why is ""one standard deviation"" used? Why is it preferred to a unitary change? Thanks</p>
",2013-10-31 16:04:03.883
58609,22564.0,1,61071.0,,,What examples of lurking variables in controlled experiments are there in publications?,<confounding><random-allocation>,CC BY-SA 3.0,"<p>In this paper:</p>

<p><a href=""http://www.cmc.edu/pages/faculty/MONeill/Math152/Handouts/Joiner.pdf"" rel=""nofollow noreferrer"">Lurking Variables: Some Examples</a>
Brian L. Joiner
The American Statistician 
Vol. 35, No. 4, Nov., 1981  227-233 </p>

<p>Brian Joiner claims that ""randomization is not a panacea"". This is contrary to common statements such as the one below:</p>

<blockquote>
  <p>A well-designed experiment includes design features that allow
  researchers to eliminate extraneous variables as an explanation for
  the observed relationship between the independent variable(s) and the
  dependent variable. These extraneous variables are called lurking
  variables.</p>
</blockquote>

<p>The quote was taken from this question and does not have a source but in my experience it is representative of the prevailing attitude:
<a href=""https://stats.stackexchange.com/questions/32941/examples-of-lurking-variable-and-influential-observation"">Examples of Lurking Variable and Influential Observation</a></p>

<p>One example given is that when testing the safety (specifically carcinogenesis) of red #40 food dye on rodents in the seventies an effect of <strong>cage position</strong> was found to confound the study. Now I have read many journal articles studying carcinogenesis in rodents and have never seen anyone report controlling for this effect.</p>

<p>Further discussion of these studies can be found here:
<a href=""http://www.ncbi.nlm.nih.gov/pubmed/6935460"" rel=""nofollow noreferrer"">A case study of statistics in the regulatory process: the FD&amp;C Red No. 40 experiments.</a></p>

<p>I could not find a non-paywalled version but here is an excerpt:</p>

<blockquote>
  <p>At the January meeting, we presented a preliminary analysis (14) that
  disclosed a strong correlation between cage row and RE (reticulo-endothelial tumor) death rates,
  which varied from 17% (bottom row) to 32% (top row) (table 2). We
  could not explain this strong association by sex, dosage group, or
  rack column or position. A subsequent analysis (18) also indicated
  that cage position (front vs. back) might be correlated with non-RE
  mortality and that position was correlated with time to non-RE death.</p>
</blockquote>

<p>I am specifically interested in why there seems to be such a problem with replication in the medical literature, but examples from all fields would be welcome. Note that I am interested in examples from randomized controlled experiments, not observational studies.</p>
",2013-10-31 16:12:08.707
58610,20470.0,1,58634.0,,,Effect of Wald-test and collinearity on Logistic Regression model selection,<logistic><categorical-data><model-selection><multicollinearity><model>,CC BY-SA 3.0,"<p>A researcher is interested in how variables, such as GRE (continuous), GPA (continuous) and rank of the undergraduate institution (categorical), affect admission into graduate school. The response variable, admit/don't admit, is a binary variable. The data set is taken from <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow noreferrer"">UCLA stats page</a>. </p>

<pre><code>admisdata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
summary(admisdata) 
admisdata$rank &lt;- factor(admisdata$rank)
mylogit &lt;- glm(admit ~ gre + gpa + rank, family = ""binomial""(link=logit), data = admisdata)
</code></pre>

<p>My questions follow:</p>

<p><strong>1)</strong> In the code (below), they check whether there is a statistically significant difference between the <code>rank3</code> and <code>rank4</code> coefficients. What would the consequence be if the difference is not significant (as below)? Are we better off merging <code>rank3</code> and <code>rank4</code> or leaving one out?</p>

<pre><code>l2 &lt;- cbind(0, 0, 0, 0, 1, -1)  # rank3 with rank4
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L = l2)
&gt;Wald test:
&gt;Chi-squared test:
&gt;X2 = 0.29, df = 1, P(&gt; X2) = 0.59
</code></pre>

<p><strong>2)</strong> In another <a href=""http://www.utdallas.edu/~pkc022000/6390/SP06/NOTES/Logistic_Regression_4.pdf"" rel=""nofollow noreferrer"">list of heuristics</a>, it is recommended to look for collinearities by checking the correlation matrix of the estimated coefficients. And it is stated: ""<em>If two covariates are highly correlated, do not need both of them in the model</em>"". For the given model fit:</p>

<pre><code>cov2cor(vcov(mylogit))

&gt;            (Intercept)          gre         gpa        rank2       rank3       rank4
&gt;(Intercept)   1.0000000 -0.241538075 -0.80278632 -0.234145435 -0.12357608 -0.18775966
&gt;gre          -0.2415381  1.000000000 -0.34207786 -0.004867914  0.04925080  0.02589326
&gt;gpa          -0.8027863 -0.342077858  1.00000000  0.043045375 -0.08263837  0.02573691
&gt;rank2        -0.2341454 -0.004867914  0.04304537  1.000000000  0.63655379  0.53030520
&gt;rank3        -0.1235761  0.049250801 -0.08263837  0.636553788  1.00000000  0.48337703
&gt;rank4        -0.1877597  0.025893262  0.02573691  0.530305204  0.48337703  1.00000000
</code></pre>

<p>It seems like the highest inter-coefficient correlation is between <code>rank3</code> and <code>rank2</code>. Does that mean it is better to leave one of them out or merge them? How do we decide what correlation value is significant enough?</p>

<p><strong>3)</strong> Or, should one <a href=""https://stats.stackexchange.com/questions/18638/model-selection-logistic-regression?rq=1"">prioritise looking at the AIC's</a> of the different models with/without these categories to compare them instead of the issues listed in 1) and 2)?</p>
",2013-10-31 16:25:10.943
58611,,2,,58608.0,user31668,,,CC BY-SA 3.0,"<p>Practically, the value of 1 may have very different meaninings in different contexts, so the standard deviation would put the sensitivity in terms of a ""typical"" deviation from the current value.</p>

<p>Theoretically, if a normal approximation is being used to model the variables, then 1 standard deviation is a convenient metirc for converting deviations into probabilities via the Z-score.</p>
",2013-10-31 16:39:06.383
58612,7007.0,2,,58586.0,,,,CC BY-SA 3.0,"<p>For $n\geq 1$, since
$$X_n = \rho^nX_0 + \rho^{n-1}W_0 + \rho^{n-2}W_1 + \dots + W_{n-1} \, ,$$
it follows that $W_n$ is independent of $X_1, \dots,X_n$, because they are functions of $X_0,W_0,W_1,\dots,W_{n-1}$ only and the $W_i$'s and $X_0$ are independent. Therefore, defining $\mathscr{F}_n=\sigma(X_1,\dots,X_n)$, we have almost surely
$$
  \mathrm{E}[X_{n+1}\mid\mathscr{F}_n] = \rho\,\mathrm{E}[X_n\mid\mathscr{F}_n]  +\mathrm{E}[W_n\mid\mathscr{F}_n] = \rho\,X_n + \mathrm{E}[W_n] \, .
$$
From here we need to know more about the value of $\rho$ and the distributions of $X_0$ and the $W_i$'s. </p>

<p>For example, if $\mathrm{E}[W_i]=0$, for every $i\geq 1$, and $\rho=1$, then $\{(X_i,\mathscr{F}_i)\}_{i\geq 1}$ is a martingale.</p>

<p>If $\mathrm{E}[W_i]\leq 0$, for every $i\geq 1$, and $\rho=1$, then $\{(X_i,\mathscr{F}_i)\}_{i\geq 1}$ is a supermartingale.</p>

<p>If $X_0$ and the $W_i$'s are almost surely positive, and $\rho&gt;1$, then $\{(X_i,\mathscr{F}_i)\}_{i\geq 1}$ is a submartingale.</p>
",2013-10-31 16:46:26.213
58613,5237.0,2,,58606.0,,,,CC BY-SA 3.0,"<p>If it is more convenient to export the variance-covariance matrix using <code>PROC REG</code>, then you can use that. You can get an interaction term by doing a <code>DATA</code> step first, and creating a new dataset with an extra variable that constitutes the <code>x</code> by <code>z</code> interaction.  The code might look something like this:  </p>

<pre><code>DATA new.data;
    SET data;
    xz = x*z;
RUN;
PROC REG DATA=new.data;
    MODEL y = x z xz /COVB;
RUN;
</code></pre>
",2013-10-31 16:50:47.407
58614,22637.0,1,58618.0,,,Cauchy Distribution and Central Limit Theorem,<probability><central-limit-theorem><asymptotics><cauchy-distribution>,CC BY-SA 3.0,"<p>In order for the CLT to hold we need the distribution we wish to approximate to have mean $\mu$ and finite variance $\sigma^2$. Would it be true to say that for the case of the Cauchy distribution, the mean and the variance of which, are undefined, the Central Limit Theorem fails to provide a good approximation even asymptotically? </p>
",2013-10-31 17:30:53.127
58615,8958.0,2,,58510.0,,,,CC BY-SA 3.0,"<p>You can use the discriminant analysis to predict the cluster using your principal components as independent variables, so your model would be: </p>

<p>$Cluster=Component_1+Component_2,...,Component_n$  </p>

<p>And no, you don't have a restriction on the number of components you can use in regards to the number of clusters you have. I would use them all. By means of cross validation I would measure how well this model (linear discriminant) predicts the cluster and if the accuracy is good you would know that the clusters are separable, crisp, which could be interpreted as cluster health. Note that you could use any classifier to do this. You could also directly use separability measures for this same purpose like Jeffries-Matusita or divergence.</p>
",2013-10-31 17:49:10.763
58616,23223.0,1,,,,Why does asymptotic distribution of LRT not depend on specific functions used to express null set?,<distributions><asymptotics><likelihood-ratio>,CC BY-SA 3.0,"<p>For a <a href=""http://en.wikipedia.org/wiki/Likelihood-ratio_test"" rel=""nofollow"">likelihood-ratio test</a> (LRT) statistic, $\Lambda(x)$, the asymptotic distribution of the statistic $-2 \log \Lambda(x)$ is a chi-squared distribution, which occurs only under certain regularity conditions. Here, we have two hypotheses for the unknown vector $\theta$:</p>

<ol>
<li>$H_0: \theta \in \omega$ versus</li>
<li>$H_1: \theta \in \Omega \cap \omega^C$.</li>
</ol>

<p>Here, $\Omega$ is the full parameter space, and $\omega$ is the restricted subspace for the null hypothesis. The subspace $\omega \subset \Omega$ is defined in terms of $q$ independent constraints of the form $g_1(\theta) = a_1, \ldots, g_q(\theta) = a_q$, where $0 &lt; q \le p$ and $a_1, a_2, \ldots, a_q$ are constants. The functions $g_1, \ldots, g_q$ must be continuously differentiable. </p>

<p>Why does the asymptotic distribution of $-2 \log \Lambda(x)$ not depend on what the functions $g_1, \ldots, g_q$ actually are as long as they are smooth and independent?</p>
",2013-10-31 17:58:29.983
58617,8958.0,2,,58373.0,,,,CC BY-SA 3.0,"<p>As daniellopez46 noted, I think you are thinking of <a href=""http://en.wikipedia.org/wiki/Consensus_clustering"" rel=""nofollow noreferrer"">consensus clustering</a> where you basically form an ensemble of different clustering runs. What is a bit strange here is that you would want the ensemble to contain results from different clustering methods which can be very misleading. I say this because unlike supervised learning, unsupervised learning always has in a larger or smaller degree a subjective component as you need to have an idea of what you consider a grouping you would be interested in based on your data. Elaborating a bit, clustering is labeling observations based on what relationship they have with other observations in your feature space. Different clustering algorithms will understand this in a totally different fashion as they are looking for different things. Depending on what kind of topology you are looking for you will (as a human) be satisfied with what one clustering algorithm produced on some data set and be totally dissatisfied with what it did on another data set. Look at <a href=""https://stats.stackexchange.com/questions/74104/self-organizing-maps-vs-kernel-k-means/74131#74131"">this</a> question I recently answered, where you can see a diagram of how different clustering techniques treat the same data sets.</p>

<p>Another thing that should be noted is that <a href=""http://en.wikipedia.org/wiki/Consensus_clustering"" rel=""nofollow noreferrer"">consensus clustering</a> is still very new and is basically just being explored so don't take it as panacea.</p>
",2013-10-31 18:03:04.320
58618,1889.0,2,,58614.0,,,,CC BY-SA 3.0,"<p>The distribution of the mean of $n$ i.i.d. samples from a Cauchy distribution has the same distribution (including the same median and inter-quartile range) as the original Cauchy distribution, no matter what the value of $n$ is.  </p>

<p>So you do not get either the Gaussian limit or the reduction in dispersion associated with the Central Limit Theorem.</p>
",2013-10-31 18:18:24.980
58626,23233.0,1,,,,Rolling twelve-month crime rate,<incidence-rate-ratio>,CC BY-SA 3.0,"<p>I need to do a 12 month rolling crime rate. I have monthly crime counts and population counts every 3 months.</p>

<p>For a calendar year I would usually use a population from mid-year as the denominator and the crime rate for the year as the numerator and then standardise using, say, 100,000. (i.e. crime count / population * 100 000). </p>

<p>However, now I need to do a rolling 12 month rate. I'm unclear Which population count to use as the denominator. Each method I can think of biases the results in some way. As I see it my options for population denominators are:
(a) An average pop of the 12 months
(b) The middle pop count for the 12 months
(c) The count at the end of the 12 months</p>

<p>Which option is most appropriate or is there a more appropriate method?</p>
",2013-11-01 00:08:38.230
58695,306.0,2,,58687.0,,,,CC BY-SA 3.0,"<p>Ideally you would want to estimate the missing data using some other data that you might have collected which has some form of relation with the data that you want to estimate. If there is no other data whatsoever, then you would want to look at some kind of autocorrelation kind of a structure so that you can predict the current value based on some of the previous ones. If there is no such information too, but you can assume the data to be from a particular distribution then use the mean to replace missing data. if no distribution can be assumed, use the median to replace the missing data.</p>
",2013-11-02 10:07:38.983
58619,22942.0,1,58765.0,,,Problem with response optimization with three variables using Response Surface in Minitab,<experiment-design><minitab>,CC BY-SA 3.0,"<p>I'm intending to do a response optimization of one response, $y$, having three predictor variables, $x_1$, $x_2$, and $x_3$. These variables are coded in the following manner:</p>

<pre><code>A    B   C   y
-1.00000    -1.00000    -1.00000    66
 1.00000    -1.00000    -1.00000    80
-1.00000     1.00000    -1.00000    78
 1.00000    1.00000     -1.00000    100
-1.00000    -1.00000     1.00000    70
 1.00000    -1.00000     1.00000    100
-1.00000    1.00000      1.00000    60
 1.00000    1.00000     1.00000     75
-1.68179    0.00000     0.00000     100
1.68179     0.00000      0.00000    80
0.00000    -1.68179     0.00000     68
0.00000     1.68179     0.00000     63
0.00000     0.00000     -1.68179    65
0.00000     0.00000      1.68179    82
 0.00000    0.00000      0.00000    113
0.00000     0.00000     0.00000     100
0.00000     0.00000     0.00000     118
0.00000     0.00000     0.00000     88
0.00000     0.00000    0.00000       100
0.00000     0.00000     0.00000     85
</code></pre>

<p>What I have tried is (in Minitab)</p>

<p>Stat -> DOE -> Define Custom Response Surface</p>

<p>And choosing 3 responses, 6 center points, 1 replicate and alpha 1,682 and subsequently choosing</p>

<p>Stat -> DOE -> Optimize Response variable</p>

<p>and choosing Maximize and lowest 100 and target 118 I got that the maximum for $y$ is</p>

<p>101.711</p>

<p>and D=0.41</p>

<p>but A=0.6, B = -0.11 and C=0.15</p>

<p>which, if I've understood correctly, is invalid for this design.</p>

<p>When I've googled all I see is people who merely make surface plots and conclude in what path the response is optimized - however using three variables I do not know how to perform this.</p>
",2013-10-31 19:53:16.933
58620,23230.0,1,,,,Correlated variables in a math model,<correlation><multiple-regression>,CC BY-SA 3.0,"<p>Let's say you have 8 variables in a regression model. If some of them are correlated, what degree or percent of correlation should you considering removing some of the variables from the equation? How about covariance - what effect does that have?</p>

<p>Lastly, how many variables are too many to inform the model? 5, 15, 50, 500? The model I have is a profit/financial type model and is fairly straightforward - profit is the output.</p>
",2013-10-31 21:33:02.790
58621,594.0,2,,58591.0,,,,CC BY-SA 3.0,"<p>Here's <em>a</em> way to do it:</p>

<p>(I may be writing my state vectors and transition matrices transposed relative to the way you might have learned them, or even the way they're usually done. If that's the case you'll need to translate back.)</p>

<p>The probability model gives you probabilities for 4 output states at time $t$ in terms of the 16 input states - the possible ordered pairs for $(x_{t-1},x_{t-2})$.</p>

<p>For speed of writing, let's write $AC$ for $(A,C)$ and so on.</p>

<p>\begin{array}{c|cccc|cccc|c}
  &amp; AA &amp; AC      &amp;AT&amp;AG&amp; CA              &amp;CC &amp;CT    &amp;CG&amp; \ldots \\ \hline
A &amp;p_{AA\to A}&amp;p_{AC\to A}&amp;p_{AT\to A}&amp;p_{AG\to A}&amp;p_{CA\to A}&amp;p_{CC\to A}&amp;p_{CT\to A}&amp;p_{CG\to A}\\
C &amp;p_{AA\to C}&amp;p_{AC\to C}&amp;p_{AT\to C}&amp;p_{AG\to C}&amp;p_{CA\to C}&amp;p_{CC\to C}&amp;p_{CT\to C}&amp;p_{CG\to C}\\
T &amp;p_{AA\to T}&amp;p_{AC\to T}&amp;p_{AT\to T}&amp;p_{AG\to T}&amp;p_{CA\to T}&amp;p_{CC\to T}&amp;p_{CT\to T}&amp;p_{CG\to T}\\
G &amp;p_{AA\to G}&amp;p_{AC\to G}&amp;p_{AT\to G}&amp;p_{AG\to G}&amp;p_{CA\to G}&amp;p_{CC\to G}&amp;p_{CT\to G}&amp;p_{CG\to G}\\ \hline
\end{array}</p>

<p>We could label the partitions with boldface versions of the state $x_{t-1}$:</p>

<p>\begin{array}{c|cccc|cc|c|cc}
           &amp; AA &amp; AC      &amp;AT&amp;AG&amp; CA              &amp; &amp;\ldots    &amp;&amp; \ldots&amp; GG \\ \hline
        A &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp; &amp;     \\
        C &amp; &amp; \mathbf{A} &amp;  &amp;  &amp; \quad\mathbf{C} &amp; &amp;\mathbf{T}&amp;      &amp;  \mathbf{G}  &amp;  \\
        T &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;     &amp;  \\
        G &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;    &amp;  \\ \hline
\end{array}</p>

<p>As I mentioned in comments, you need to extend the state. Let $z_t$ consist of pairs of states $(x_t,x_{t-1})$ and now consider a Markov Chain in $z_t$; that is, you have a transition matrix $p(z_t|z_{t-1})$.</p>

<p>So the state at time $t$ will be one of the 16 pairs $(A,A), (A,C) \ldots (G,G)$, and the transition matrix will be a 16 $\times$ 16 matrix of transition probabilities that will be mostly zero (necessarily so, because any pair that doesn't have the second component of $z_t$ match with the first component of $z_{t-1}$ is impossible).</p>

<p>As above, for speed of writing, let's also write $AC$ for $(A,C)$ and so on.</p>

<p>For ease of display I am going to define $z_{t-1}^*$ which is simply a permuted $z_{t-1}$. We can write $p(z_t|z_{t-1}^*)$ and then arrive back at $p(z_t|z_{t-1})$ by simple permutation.</p>

<p>So the transition matrix for $p(z_t|z_{t-1}^*)$ is of the form:</p>

<p>\begin{array}{c|cccc|cc|c|cc}
           &amp; AA &amp; AC      &amp;AT&amp;AG&amp; CA              &amp; &amp;\ldots    &amp;&amp; \ldots&amp; GG \\ \hline
        AA &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp; &amp;     \\
        CA &amp; &amp; \mathbf{A} &amp;  &amp;  &amp; \quad\mathbf{0} &amp; &amp;\mathbf{0}&amp;      &amp;  \mathbf{0}  &amp;  \\
        TA &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;     &amp;  \\
        GA &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;    &amp;  \\ \hline
        AC &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;    &amp; \\
    \vdots &amp; &amp; \mathbf{0} &amp;  &amp;  &amp; \quad\mathbf{C} &amp; &amp;\mathbf{0}&amp;      &amp;\mathbf{0} &amp;  \\
    \vdots &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;   &amp;  \\\hline
    \vdots &amp; &amp; \mathbf{0} &amp;  &amp;  &amp;\quad\mathbf{0}  &amp; &amp;\mathbf{T}&amp;      &amp; \mathbf{0} &amp;\\
    \vdots &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;   &amp;   \\ \hline
    \vdots &amp; &amp; \mathbf{0} &amp;  &amp;  &amp;\quad\mathbf{0}  &amp; &amp;\mathbf{0}&amp;      &amp; \mathbf{G}  &amp; \\
        GG &amp; &amp;            &amp;  &amp;  &amp;                 &amp; &amp;          &amp;      &amp;   &amp; \\
\hline
\end{array}</p>

<p>We can then rearrange either the rows or columns so they're in the same order; the transition matrix no longer has that simple structure, but contains the same values.</p>

<p>Generally, you can use this procedure to transform any $k$-th order Markov chain to a first-order MC (also holds for Hidden Markov Models).</p>
",2013-10-31 22:06:08.017
58622,16464.0,1,58902.0,,,Single or Multiple Models for Prediction with Categorical Variable,<regression><categorical-data>,CC BY-SA 3.0,"<p>For a project I need to build a model that can predict batch processing time for the steps in a manufacturing process that are performed on a specific tool type. I know that the processing time is a function of the batch size (number of individual pieces of product loaded into the equipment at once) and the step in question.</p>

<p>I assume that the relationship between processing time and batch size is linear and the data indicate that the relationship is quite different based on which step is being run. In a stylized example where there were two steps run on the equipment under investigation, I've assumed an accurate way to describe the relationship between processing time and batch size is
$$
\mathbf{y=\delta_{0}+\delta_{1}x_{1}+\epsilon}
$$
for step 1 and
$$
\mathbf{y=\alpha_{0}+\alpha_{1}x_{1}+\epsilon}
$$
for step 2, where $\mathbf{\delta}\neq\mathbf{\alpha}$. Segregating the data by step and creating a unique predictive model for each step is less efficient than using an indicator variable to represent the step (with the actual data I have more like 10 steps). Using a model that treated the step as an indicator variable, I would need to specify a model
$$
\mathbf{y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\beta_{3}x_{1}x_{2}+\epsilon}
$$
to accurately capture both models stated above within one model that treated the step as an independent variable.</p>

<p>I thought the larger model might produce predictions identical to the two smaller models but I was unsure because the parameters would be estimated with the 'full' data set in the general case and only a portion of the data set in the case of the smaller models. To test this I created a data set in R:</p>

<pre><code># create variables and define relationships
btchSize &lt;- rnorm(100,12,3)
step &lt;- vector(""character"",100)
step[1:50] &lt;- ""A""
step[51:100] &lt;- ""B""
step &lt;- as.factor(step)
y &lt;- vector(""numeric"",100)
y[1:50] &lt;- 50+25*x1[1:50]+rnorm(50,0,5)
y[51:100] &lt;- 100+15*x1[51:100]+rnorm(50,0,5)

# create linear models
fullDataLm &lt;- lm(y~x1+x2+x1*x2)
aLm &lt;- lm(y[1:50]~x1[1:50])
bLm &lt;- lm(y[51:100]~x1[51:100])

summary(fullDataLm)
summary(aLm)
summary(bLm)
</code></pre>

<p>This code gives the following results:</p>

<pre><code>Call:
lm(formula = y ~ x1 + x2 + x1 * x2)

Residuals:
    Min      1Q  Median      3Q     Max 
 -35.332  -9.246   1.227  10.716  26.028 

Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.8611     4.8206  10.343  &lt; 2e-16 ***
x1           24.8566     0.3481  71.410  &lt; 2e-16 ***
x2B          54.9103     7.1373   7.693 1.26e-11 ***
x1:x2B      -10.1553     0.5396 -18.819  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 14.07 on 96 degrees of freedom
Multiple R-squared:  0.987, Adjusted R-squared:  0.9866 
F-statistic:  2427 on 3 and 96 DF,  p-value: &lt; 2.2e-16


Call:
lm(formula = y[1:50] ~ x1[1:50])

Residuals:
    Min      1Q  Median      3Q     Max 
-35.332  -8.802  -0.608  11.255  24.448 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.8611     4.7467   10.50 4.94e-14 ***
x1[1:50]     24.8566     0.3428   72.52  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 13.85 on 48 degrees of freedom
Multiple R-squared:  0.991, Adjusted R-squared:  0.9908 
F-statistic:  5259 on 1 and 48 DF,  p-value: &lt; 2.2e-16


Call:
lm(formula = y[51:100] ~ x1[51:100])

Residuals:
    Min      1Q  Median      3Q     Max 
-33.812 -10.049   3.803  10.529  26.028 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 104.7714     5.3427   19.61   &lt;2e-16 ***
x1[51:100]   14.7013     0.4186   35.12   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 14.28 on 48 degrees of freedom
Multiple R-squared:  0.9625,    Adjusted R-squared:  0.9618 
F-statistic:  1234 on 1 and 48 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The results indicate that the larger model is identical to the smaller models in terms of the coefficient estimates. I.e.
$$
\mathbf{\beta_{0}=\delta_{0}} \\
\mathbf{\beta_{1}=\delta_{1}} \\
\mathbf{\beta_{0}+\beta_{2}=\alpha_{0}} \\
\mathbf{\beta_{1}+\beta_{3}=\alpha_{1}} \\
$$
My questions are:</p>

<p>(1) Can anyone explain algebraically how the above is true considering that the parameter estimation equation is:
$$
\mathbf{\hat{\beta}=(Xâ€™X)^{-1}Xâ€™y}
$$
and neither $\mathbf{X}$ nor $\mathbf{y}$ is the same in either of the three regressions. Obviously the indicator variables must cause the parameter estimation equations to end up being equal but Iâ€™d like to know how exactly.</p>

<p>(2) Does this result hold for parameter estimation techniques other than OLS?</p>

<p>(3) Which methodology is considered to be correct within the statistics community and why? Since the predictions from the larger model are identical to the predictions from the smaller models, I know that the residuals for the larger model are just the residuals of both smaller models (In my R example the residuals from the 100 observations used in the larger model are just the residuals from the 50 observations used in each of the smaller models). Consequently, if the regression assumptions are satisfied to a larger extent for data within one step then another it seems that the larger model may balance out the data and appear satisfactory in satisfying the regression assumption, while in fact the data lend themselves to regression analysis for one step but for another step are inconsistent with the use of regression.</p>
",2013-10-31 22:26:25.270
58623,21346.0,1,,,,Transforming the explanatory variables,<data-transformation><maximum-likelihood>,CC BY-SA 3.0,"<p>What are the considerations that we need to take into account if we need to transform just the explanatory variables (not the dependent variable). I have data on assets and liabilities and I need to use assets-liabilities or net worth as the explanatory variable. I can't use logarithm because net worth can be negative or zero or positive. In addition, if I just use the amount of networth (raw data) the maximum likelihood estimator doesn't achieve convergence. But, with the Inverse hyperbolic sine (IHS) transformation, the convergence is achieved. My first question is if it is safe to say that we are using the IHS transformation just because we don't have convergence. Are there any other considerations that we need to look into in this regard? Second question is how do we find the scale parameter (theta) in IHS transformation. ?</p>
",2013-10-31 22:41:53.390
58624,10135.0,2,,58620.0,,,,CC BY-SA 3.0,"<p>This is called <a href=""http://en.wikipedia.org/wiki/Multicollinearity"" rel=""nofollow"">Multicollinearity</a> in regression. Dropping is just one way to handle this situation and it is not the only remedy. Sometimes you standardize your independent variables, or get more data (if possible), dropping a term is another option, or even fitting a regression with the knowledge of having Multicollinearity! To assess how much they are correlated, you need to find a factor called <a href=""http://en.wikipedia.org/wiki/Variance_inflation_factor"" rel=""nofollow"">Variance inflation factor (VIF)</a>, where $VIF(\hat{\beta_j})=\dfrac{1}{1-R^2_i}$ and  $R^2_i$ is the <a href=""http://en.wikipedia.org/wiki/Coefficient_of_determination"" rel=""nofollow"">coefficient of determination</a> of the regression model, i.e. you replace your $Y$ (dependent variable) with $X_i$ and fit your regression model with all other independent variables. Then you find your $R^2$ as usual. The result is denoted by $R^2_i$ to emphasize the fact that you regressed $X_i$ against other independent variables. As a rule of thumb when $VIF(\hat{\beta_j})&gt;5$ (or in some references $VIF(\hat{\beta_j})&gt;10$), we say that multicollinearity is high.     </p>

<p>To answer your 2nd question, just think about the relation between covariance and correlation of two random variables. Then I guess you can find your answer. the correlation is just scaled version of covariance.</p>

<p>For the last question: I don't think there is specific number of variables to call ""too many"" that works in any model. The thing is that you need to fit a good regression model but it does not mean that you should add too many variables. Because over-parametrization is another problem that should be avoided.</p>
",2013-10-31 23:26:26.763
58625,23201.0,2,,143.0,,,,CC BY-SA 3.0,"<p>I used this <a href=""http://www.johndcook.com/skewness_kurtosis.html"" rel=""nofollow"">RunningStats C++ Library</a> in an embedded application. It is the most simple running stats library I have found yet.</p>

<p>From the link:</p>

<blockquote>
  <p>The code is an extension of the method of Knuth and Welford for
  computing standard deviation in one pass through the data. It computes
  skewness and kurtosis as well with a similar interface. In addition to
  only requiring one pass through the data, the algorithm is numerically
  stable and accurate.</p>
</blockquote>
",2013-10-31 23:55:13.693
58627,5086.0,1,58672.0,,,What method is simulating pvalues from re sampling from the data,<bootstrap><monte-carlo><resampling><quasi-monte-carlo>,CC BY-SA 3.0,"<p>A while back I asked a question about <a href=""https://stats.stackexchange.com/questions/17462/correlating-time-stamps"">correlating times between time stamps</a> and <a href=""https://stats.stackexchange.com/a/22333/7482"">received a response</a> from Peter Ellis that said I could calculate mean distances between codes...</p>
<blockquote>
<p>This already will give you some sense of which behaviours are clustered together, but you also should check that this isn't plausibly due just to chance.</p>
<p>To check that, I would create simulated data generated by a model under the null hypothesis of no relation. Doing this would require generating data for each behaviour's time from a plausible null model, probably based on resampling the times between each event (eg between each yawn) to create a new set of time stamps for hypothetical null model events. Then calculate the same indicator statistic for this null model and compare to the indicator from your genuine data. By repeating this simulation a number of times, you could find out whether the indicator from your data is sufficiently different from the null model's simulated data (smaller average time from each yawn to the nearest stretch, for example) to count as statistically significant evidence against your null hypothesis.</p>
</blockquote>
<p>I finally possess the skill set to do this and have done so in R but I don't know what this method or technique is called so that I can (a) learn more about it (b) speak intelligently about the theory behind what I'm doing.</p>
<p>Some people have suggested this is called a permutation test, others say similar to but not the same as bootstrapping and some have told me it's related to Monte Carlo re sampling.</p>
<p>What is this method of resampling, given the NULL is TRUE, called?  If you have a reference or two to back up your response that may be helpful but not necessary.</p>
",2013-11-01 01:12:07.763
58628,,2,,58398.0,user30490,,,CC BY-SA 3.0,"<p>I figured it out.  When the reciprocal roots are real we will have that 
$$\phi_1=(r_1+r_2)\hspace{.1in}\text{ and }\hspace{.1in}\phi_2=-r_1r_2$$
Likewise when the reciprocal roots appear as a complex pair then we have that 
$$|\phi_1|=2r\cos(\omega)\hspace{.1in}\text{ and }\hspace{.1in}\phi_2=-r^2$$</p>

<p>Thus I can just plug that into the above equation and plot as a function of $\omega$.</p>
",2013-11-01 02:30:53.827
58629,23237.0,1,,,,Does the sum of several distributions become more central or approximated to Normal,<distributions><normal-distribution><central-limit-theorem>,CC BY-SA 3.0,"<p>As the classic CLT states Xs follow the same distribution, then the sum of Xs approximate to Normal distribution.
But what about several Xs follows different distributions (maybe the same class but not the same value of parameters or totally different kinds)?
It seems that the sum will become more central or even more Normal ""likely"".
Maybe some kinds of ""Generalized"" CLT has interests about that.</p>

<p>Is there any theory doing study about such kind of research?
What characteristics if the Xs follow, we will have such results. What kinds of assumptions fails, then we have another results?</p>

<p>We know that sum of the uniforms is triangle distribution, while sum of the Poissons is still Poisson. Sum of stable family (including Normal, Cauchy) is still stable distribution.
And about student-t, the sum is not student-t but not knowing what kind it is. But it seem that the fat-tail effect of the sum is thinner.</p>

<p>So could you please tell me some more general rules about such aspects. </p>
",2013-11-01 02:34:39.047
58630,21804.0,2,,57636.0,,,,CC BY-SA 3.0,"<p>I would advise against using statistical methods for determining the ""best"" variable. You have only 98 observations (how are ""yes"" and ""no"" answers distributed?) and more variables than cases. This is a recipe for disaster in the sense that any attempt to build a model with all variables is prone to overfit the data. You will find packages that try to do the trick and some careful cross-validation might help you to avoid some obvious pitfalls, but do not assume that you will learn much on a conceptual level. My suggestion would therefore be to eliminate variables that are weak on theoretical grounds before moving to the analysis step or to collect more cases if that is possible. And: test some simple models (equal weighting of variables) as competitors to get some benchmarks.</p>

<p>There is less literature on the soft and fuzzy process of conceptual variable selection than on algorithmic ways to ""solve"" this problem, but this is not necessarily due to the superiority of the latter. Some pointers in the literature could be:</p>

<p>Dawes, R. M. (1979). The robust beauty of improper linear models in decision making. American psychologist, 34(7), 571.</p>

<p>Freedman, D. A. (1991). Statistical models and shoe leather. Sociological methodology, 21, 291-313.</p>

<p>Freedman, D. (1999). From association to causation: some remarks on the history of statistics. Statistical Science, 14(3), 243-258.</p>
",2013-11-01 03:14:02.883
58631,22891.0,1,,,,Differences Between Logistic Regression in Statistics and in Machine Learning,<machine-learning><logistic>,CC BY-SA 3.0,"<p>I just found out that machine learning also has logistic regression as one of its methods. Can someone please tell me the differences between logistic regression in statistics and machine learning? I've seen lecture slides on logistic regression from a machine learning course, but I can't see the difference with the coverage of logistic regression in a statistics course.</p>

<p>Does logistic regression in machine learning have no need to check for multicollinearity?</p>

<p>The reason I asked this is because I've tried to run a dataset through R's <code>glm</code> function with binomial logit, and then I ran the same dataset through Apache Mahout's <code>trainlogistic</code>. But the resulting coefficients are different.</p>

<p>This is the command I use in R:</p>

<pre><code>w1.glm &lt;- glm(anw ~ cs, data = w1, family = ""binomial"")
</code></pre>

<p>This is the result of <code>summary(w1.glm)</code>:</p>

<pre><code>glm(formula = anw ~ cs, family = ""binomial"", data = w1)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5400   0.1073   0.1924   1.0047   1.0047  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.42077    0.02588   16.26   &lt;2e-16 ***
cs           1.89342    0.06427   29.46   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 11762.5  on 10660  degrees of freedom
Residual deviance:  9250.3  on 10659  degrees of freedom
</code></pre>

<p>And this is the command I use in Mahout:</p>

<pre><code>/usr/local/mahout/bin/mahout trainlogistic --input w1.csv --output ./model --target anw --categories 2 --predictors cs --types numeric --features 20 --passes 100 --rate 50

Running on hadoop, using /usr/local/hadoop/bin/hadoop and HADOOP_CONF_DIR=
MAHOUT-JOB: /usr/local/mahout/mahout-examples-0.8-job.jar
20
anw ~ 
-19.553*cs + -7.512*Intercept Term
            cs -19.55265
      Intercept Term -7.51155
    0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000   -19.552646543     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000    -7.511546797     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000 
13/11/01 02:04:47 INFO driver.MahoutDriver: Program took 22118 ms (Minutes: 0.3686333333333333)
</code></pre>

<p><strong>Edited:</strong> Added the reason I asked the question in the title. Added the commands used to execute <code>glm</code> in R and <code>trainlogistic</code> in Mahout.</p>
",2013-11-01 06:49:31.823
58632,23242.0,1,58635.0,,,show that $X_{n}= E[X|\mathcal F_{n}]$ is an $\mathcal F_{n}$-martingale,<self-study><conditional-probability><stochastic-processes><marginal-distribution>,CC BY-SA 3.0,"<p>Let $X$ be an integrable random variable defined on probability space $(\Omega , \mathcal F,P)$, and let $\mathcal F_{n},n\ge0$ , be a filtration on this space.<br>
 show that $X_{n}= E[X|\mathcal F_{n}]$ is an $\mathcal F_{n}$-martingale.</p>
",2013-11-01 08:33:04.497
58648,5448.0,2,,58644.0,,,,CC BY-SA 3.0,"<p>Power is an attribute of a test that depends upon the true parameter values, not the estimated parameter values.  It may be that the assumptions about the true parameter values that you made when calculating power are contraindicated by the data, but that does not affect the power as such, since that is specific to the null/alternative hypotheses used in the calculation.  It is important to remember that power calculations take into account the randomness of the sample; in your case, for example, a power calculation based upon a true correlation of 0.5 does not assume that the sample correlation will always be 0.5.  </p>

<p>Consequently, your thinking is correct.  If your assumptions are correct, the test still has 90% power.  As Michael Meyer has pointed out in comments, given your sample size, it is not even the case that your sample correlation indicates that your assumptions are false, which means there isn't any reason to revisit your power calculations.</p>
",2013-11-01 13:25:50.947
58633,9408.0,1,58821.0,,,Comparing the effect of a treatment that was optional for its receivers,<hypothesis-testing><logistic><causality><propensity-scores>,CC BY-SA 3.0,"<p>In 2012 we collected data at our university about retention of students from first semester to second semester along with some other variables. The retention variable is binary, 'retained' or 'did not retain'. </p>

<p>In 2013 we introduced a new system. Student success advisers were assigned and any student when needed could go to them. We collected some new variables associated with the introduction of the student success advisers together with those collected in 2012. Now, going to these advisers was not mandatory. Those who felt a problem could go to them. </p>

<p>Just looking at 2013 data alone makes it seem that the advisers make things worse: the students they see do worse, and are more likely to leave university. Only the students who faced problems probably went to the advisers and those who were doing well probably didn't. If 10 students went to the advisers with problems and 8 retained to semester 2, that is surely a success. But if 80 out of 100 students who didn't have any problem and thus didn't go to the adviser retained to semester 2, that will have log odds similar to those who went to the advisers and thus tell you that going to the advisers had no significant effect!</p>

<p>Part of the reason was a major policy change that affected how universities in our country took in students.  There was a ""cap"" in place before 2013, a limit on how many students a university could take on and expect government funding for.  In 2013 that was removed.  So, successful, desirable universities started opening their doors to more students, and more students were able, suddenly, to get into desirable universities. Our university is not so desirable. So what probably happened was that our university was forced in 2013 to take significantly ""worse"" students, which could well lead to greater retention difficulty. </p>

<p>What my bosses think is that we need to weight the 2012 data so that it matches the 2013 data on some of the variables that may have been sensitive to the removal of the cap on student entry (or weight the 2013 data so that it matches the 2012 data).  Then we can compare two different cohorts as if they were a proper control group for each other.  In fact, this is illusory. Some suggested using SPSSINC RAKE, but I am not getting any clue why.</p>

<p>What we want to find out (very broadly) is whether introducing the advisers etc. worked, and the degree to which it worked. </p>
",2013-11-01 08:36:37.353
58634,12683.0,2,,58610.0,,,,CC BY-SA 3.0,"<p>First, if the researcher's interested in how those variables affect admission, doesn't that interest include how much difference there is between third &amp; fourth ranks?</p>

<p>(1) It's not a good idea to merge categories based just on their having similar responses&mdash;you're introducing bias into the coefficient estimates. Things like this are done ruefully, when they have to be, to fix problems with the model, not as a matter of course.</p>

<p>(2) Think this through. A high correlation between two predictors makes it difficult to separate their effects on the response. How many values of <code>rank</code> can a single person have? Are you ever going to be interested in predicting admission for someone having both <code>rank=2</code> &amp; <code>rank=3</code>?
[Edit in response to comment: The answer is that a single person can only have one level of a categorical predictor, &amp; you'll never be interested in predicting the response for someone with more than one level, so it's to be expected that there's correlation between the levels, &amp; poses no problem at all. Neither leave one out (which would be equivalent to merging it with the reference level) nor merge them. (This is sometimes called structural multicollinearity to distinguish it from the problematic kind of multicollinearity).]</p>

<p>(3) Read the post you linked to carefully. Gung is saying that Akaike's Information Criterion can be useful to decide between a few candidate models ""of substantive interest to you""; he's not recommending trawling through all possibilities to find the model with the lowest AIC.</p>
",2013-11-01 09:39:01.750
58635,21029.0,2,,58632.0,,,,CC BY-SA 3.0,"<p>To prove this is a martingale, you must show three things. First, that </p>

<ol>
<li>$X_n$ is $\mathcal F_{n}$ measurable, which is given by definition. </li>
<li>Next, that $E|X_n| &lt; \infty $. Since $X$ is integrable, we have this property as well.</li>
<li>Finally, show that $E[ X_n | \mathcal F_{n-1}] = X_{n-1} $. </li>
</ol>

<p>$ E[ X_n | \mathcal F_{n-1}] = E[ E[ X | \mathcal F_{n}] | \mathcal F_{n-1}] = E[ X | \mathcal F_{n-1}] = X_{n-1}$</p>

<p>So $X_n$ is a $\mathcal F_{n}$ martingale.</p>
",2013-11-01 09:54:38.803
58636,18085.0,2,,46894.0,,,,CC BY-SA 3.0,"<p>This is a kind of late answer to my question.</p>

<p>The short answer is that a variable set of data can have the same coefficients but explains variance to different proportion.
This explains the low variance within coefficients and the high variance within $R^2$.</p>

<p>So its actually a quite simple thing.</p>
",2013-11-01 10:31:39.700
58637,22923.0,2,,58375.0,,,,CC BY-SA 3.0,"<p>First, just on a side note - your yields (x and y) seems to be non-stationary - you see these 2 clouds on your scatter - they probably happened to be at that particular slope to each other just by chance because yields ""randomly walked"" in those particular directions. If they would randomly walked in other directions, you could have completely different slope (and you probably will have  in future - something like 3rd cloud somewhere aside). So when you draw a regression line it may happen to be <a href=""http://www.econ.ku.dk/metrics/Econometrics2_05_II/Slides/10_cointegration_2pp.pdf"" rel=""nofollow"">spurious</a>.</p>

<p>Next, one of interpretations of weighted least squares regression is that residuals in your underlying model have different variances in different regions (heteroscedasticity) and you put more weight on where they have smaller variance to obtain consistent estimates of regression coeffs. But then computing residuals' stdev will not be interpretable as an estimate of their true variance - because they come from different distributions with different variances :)</p>

<p>But if you still want some measure of stdev taking into account relative weights or importance of points - then <a href=""http://en.wikipedia.org/wiki/Mean_square_weighted_deviation"" rel=""nofollow"">Mean Squared Weighted Deviation</a> may be a way to go.</p>
",2013-11-01 10:59:22.467
58638,21728.0,1,58640.0,,,"Show that for a Geometric distribution, the probability generating function is given by $\frac{ps}{1-qs}$, $q=1-p$",<probability><self-study><geometric-distribution>,CC BY-SA 3.0,"<p>Suppose that $X$ has a geometric distribution with probability mass function $P(X=x) = q^{i-1}p$, $i=1,2,...$ and $q=1-p$</p>

<p>Show that its probability generating function is given by $ \pi(s)=\frac{ps}{1-qs}$. Hence show that $E(x)=\frac{1}{p}$ and $Var(X)=\frac{q}{p^2}$</p>

<p>Hi everyone, I am doing this question for exam practice, and I can't seem to get the correct answer. And to be honest, I am just working through it mechanically and don't have a great understanding of the probability generating functions.</p>

<p>Here is what I have:</p>

<p>$$\pi(s)=E(S^X)=\sum^\infty_{i=0}q^{i-1}p\cdot s^i$$
$$= p\sum^\infty_{i=0}q^{i-1}\cdot s^i=p\sum^\infty_{i=0}\frac{q^i}{q}\cdot s^i$$
$$=\frac{p}{q}\sum^\infty_{i=0}(qs)^i$$</p>

<p>Then using the sum of a geometric series formula, I get:</p>

<p>$$=\frac{p}{q}(\frac{1}{1-qs})$$</p>

<p>Now I am stuck. I feel like I am close, but am just missing something. I'll be ok with deriving the expected value and variance once I can get past this part. </p>

<p>As an addition I was wondering if anyone could also give me a bit of an 'idiots' explanation of the probability generating function, as I am struggling to understand it conceptually. $s$ seems to be the dependent variable, but my lecturer hasn't explained what exactly it is.</p>

<p>Many thanks in advance!</p>
",2013-11-01 11:16:29.010
58639,23243.0,1,58641.0,,,Goodness of fit tests for quantile regression in R,<r><regression><goodness-of-fit><quantile-regression>,CC BY-SA 3.0,"<p>What goodness of fit tests are usually used for quantile regression? Ideally I need something similar to F-test in linear regression, but something like AIC in logistic regression will suite as well. I use quantreg R package, but found only some Khmaladze test in there. To be fair I hardly understand what is does.</p>
",2013-11-01 11:19:57.813
58640,21029.0,2,,58638.0,,,,CC BY-SA 3.0,"<p>It's normal you'd arrive at the wrong answer in this case. The problem is that your index is wrong. There are two definitions for the pdf of a geometric distribution. The one you use, where $E(X)=\frac{1}{p}$ is defined from 1 to infinity. At zero it is not defined. So, the generating function needs to take this into account, as well. </p>

<p>$$\pi(s)=E(S^X)=\sum^\infty_{i=1}q^{i-1}ps^i$$
$$= ps\sum^\infty_{i=1}(qs)^{i-1}=ps\sum^\infty_{i=0}(qs)^i$$
$$=\frac{ps}{1-qs}$$</p>

<p>If you use the alternative definition, where $P(Y=y)=q^ip$, then the pdf is defined at zero. In this case the generating function converges to $\frac{p}{1-qs}$.</p>

<p>As for what $s$ represents, as far as I know it represents nothing. Generating functions are derived functions that hold information in their coefficients. They are sometimes left as an infinite sum, sometimes they have a closed form expression. Take a look at the wikipedia article, which give some examples of how they can be used. Here and here.<a href=""http://en.wikipedia.org/wiki/Probability-generating_function"">wiki article probability generating functions</a> and <a href=""http://en.wikipedia.org/wiki/Generating_function"">wiki article generating functions</a></p>
",2013-11-01 12:00:05.277
58641,503.0,2,,58639.0,,,,CC BY-SA 3.0,"<p><code>quantreg</code> includes several AIC functions: ""AIC.nlrq"", ""AIC.rq"", ""AIC.rqs"" and ""AIC.rqss"" and similar log likelihood functions. </p>

<p>It also has a vignette at vignette(""rq"",package=""quantreg"").</p>

<p>Do these do what you want? </p>
",2013-11-01 12:28:03.257
58642,22923.0,2,,56580.0,,,,CC BY-SA 3.0,"<p>SVM doesn't assume normality. But it's still a regression that minimizes some symmetric loss function (I suppose you use symmetric kernel).</p>

<p>So... this is just a feeling and I'm too tired to justify/prove all this but:</p>

<ol>
<li>Probably your output variable has highly skewed distribution;</li>
<li>And you use symmetric gaussian kernel that leads to symmetric squared loss to minimize (squared error with bump cut-off if I remember correct?);</li>
<li>Then SVM still estimates something close to conditional mean of your data if you minimize this loss for original output variable;</li>
<li>When you log-transform output variable and minimize that symmetric loss for it, then in terms of <em>original</em> variable it estimates something like a conditional median;</li>
<li>it's well-known that mean is the thing that minimizes average squared error and median is the thing that minimizes average absolute error, so when you estimate regression using log-transformed output you get worse MSE but better MAPE.</li>
</ol>

<p>Hope this helps.</p>
",2013-11-01 12:39:25.987
58643,2666.0,2,,58529.0,,,,CC BY-SA 3.0,"<p>You didn't state the ultimate goal of the exercise, hence the choice of ROC curves was not well motivated.  Many useful things can be done with log-likelihood and Brier scores, as well as with the distribution of predicted risks (ignoring $Y$).  The use of cutoffs is questionable.</p>
",2013-11-01 12:40:42.403
58644,15806.0,1,58648.0,,,Is the post hoc power of a dependant t test related to the sample correlation or actual correlation,<correlation><t-test><statistical-power>,CC BY-SA 3.0,"<p>I have a question that may be fairly easy to answer but it is making my head spin. If I do a paired t test on two groups of data with sample size 3 each AND I assume that the pairs are correlated with at least an correlation coefficient of 0.5, then I should have about 90% power to detect an effect size of 2SD. </p>

<p>Ok, so lets say that after the experiment, I check my sample correlation and it turns out that it is only 0.2 for that sample and not 0.5. I am wondering how this would affect the power of my test since, I believe, the sample correlation value influences my p-value. Would I then have to say that I actually had less than 90% power? Or did my test still have 90% power regardless of how the sample correlation turned out? I am thinking it is the latter rather than the former but I would appreciate some help as this whole business of calculating power after a test is very confusing. Hopefully my question makes sense. </p>
",2013-11-01 12:52:43.070
58645,23246.0,1,58649.0,,,How can we find the decision boundary for two overlapping continuous uniform distribution?,<machine-learning><probability><uniform-distribution>,CC BY-SA 3.0,"<p>Say I have $X \sim \text{CUnif}(a, b)$ and $Y \sim \text{CUnif}(c, d)$. The parameters of $X$ and $Y$ overlap i.e., $a &lt; c &lt; b &lt; d$. </p>

<p>How can I find a decision boundary in such case? </p>

<p>I am thinking of taking an arbitrary point (say $x_o$)  as a decision boundary in between $b$ and $c$ and then find the probability of miss classification and then minimizing it. </p>
",2013-11-01 13:01:27.717
58646,449.0,2,,58644.0,,,,CC BY-SA 3.0,"<p>Power is the probability of finding a significant effect in a sample if what you believed true about the population is true. Correlation, effect, and variance all change from sample to sample. That's why power is expressed as a probability. It's sensitive to these variations.</p>

<p>Here's a simulation in R that demonstrates what I'm saying. It generates data from the population you expect in your question. But each sample has a different correlation, variance in groups, and effect.</p>

<pre><code>library(MASS)
# mvrnorm requires a variance, covariance matrix
# with variances of 1 then the covariances will equal the correlation
sigma &lt;- matrix(c(1.0, 0.5,
                  0.5, 1.0), 2, byrow = TRUE)
y &lt;- replicate(1000, {
     mat &lt;- mvrnorm(3, c(0, 2*sqrt(2)), sigma, empirical = FALSE) # get a sample
     tt &lt;- t.test(mat[,1], mat[,2], paired = TRUE, var.equal = TRUE)
     tt$p.value })  #return p-value of t-test

sum( y &lt; 0.1 ) / 1000
</code></pre>

<p>The typical result is that about 90% of the tests pass a significance test.</p>

<p>[The alpha used is 0.1 from the comments and the effect size is diff / (pooledSD / sqrt(1-r)) based on comments as well. None of that's particularly germaine to the basic story but I note it because there are arguments to use different effect size calculations and this is an atypical significance cutoff.]</p>
",2013-11-01 13:04:17.510
58647,17328.0,2,,58519.0,,,,CC BY-SA 4.0,"<p>Here is a quick check using a computer algebra system. I am using the <code>mathStatica</code> package for <em>Mathematica</em> (I am one of the developers of the former) to automate the nitty gritties for me ... </p>

<p>Given: The parent pdf is $N(\mu, \sigma^2)$ with pdf $f(y)$:</p>

<p><a href=""https://i.stack.imgur.com/I537E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I537E.png"" alt=""enter image description here""></a></p>

<p>Then, the joint pdf of the 1st and 2nd order statistics $(Y_1, Y_2)$, in a sample of size 2, denoted say $g(y_1,y_2)$ can be easily obtained using the <code>OrderStat</code> function:</p>

<p><a href=""https://i.stack.imgur.com/Uz9BK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uz9BK.png"" alt=""enter image description here""></a></p>

<p>Note that the constant multiplier here is $\frac{1}{\pi \sigma^2}$, not $\frac{1}{2\pi \sigma^2}$ in your equation.</p>

<p>Because the constraint $Y_1 &lt; Y_2$ is already entered into the pdf definition, we can enter the domain of support on the real line as:</p>

<p><a href=""https://i.stack.imgur.com/oADKQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oADKQ.png"" alt=""enter image description here""></a></p>

<p>Finally, the expectation you seek is: </p>

<p><a href=""https://i.stack.imgur.com/eJQdy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eJQdy.png"" alt=""enter image description here""></a></p>

<p>which agrees with your stated solution. </p>

<hr>

<p>@Ioannis wrote: </p>

<blockquote>
  <p>Is there a way to see how the integral is computed though?</p>
</blockquote>

<p>One can activate VerboseMode, which shows all the integrands being sent off for calculation. With VerboseMode[On], one can see the intermediary integrands ...</p>

<p><a href=""https://i.stack.imgur.com/bFgI7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bFgI7.png"" alt=""enter image description here""></a></p>

<p>You might need to open the pic manually in a separate window to see the detail ...</p>
",2013-11-01 13:20:39.700
58696,23279.0,2,,58694.0,,,,CC BY-SA 3.0,"<p>Strictly speaking, the normality of residuals assumption is not needed for OLS to work, it becomes an issue especially in hypothesis testing. Since your residuals actually seem to be normally distributed, you're fine even in this area. Additionally, OLS does not assume anything about the distribution of variables so you do not have to worry about that.</p>
",2013-11-02 11:01:07.143
58650,23250.0,1,,,,Identifying linear relationships among many interrelated variables,<correlation><pca>,CC BY-SA 3.0,"<p>I have a dataset with many variables. Some of these variables are linked to each other in various ways, but I don't know in advance those that are.</p>

<p>For example, these are some relationships:</p>

<ul>
<li><p>$A=B$ -- obvious and easy to spot/remove;</p></li>
<li><p>$A=B\cdot\mathrm{constant}$ -- again easy to remove using a correlation matrix;</p></li>
<li><p>The problem comes with $A = B+C+D+E$ or even $A = B+(0.5\cdot C)+(0.5\cdot D+E).$</p></li>
</ul>

<p>The last set are difficult to identify and PCA doesn't provide me with all the clues to identify what A is made up of. </p>

<p>For example, a simple $A = B + C$ will not always show a link.
If half of the B values are 0.1 with the other half distributed from 0.5-1.5 randomly and C is a normal distribution with mean 1 no correlation is detected between B and A. However, correlation is often detected in A and C due to the distribution in C still being present in A.</p>

<p>Without knowing in advance these relationships are present how do identify these underlying relationships?</p>

<p>In essence, I am trying to remove these relationships before running PCA and later modelling with remaining uncorrelated variables.</p>
",2013-11-01 13:57:37.697
58651,23251.0,1,,,,Mathematical equation for a generalized linear mixed model with interactions,<mixed-model><mathematical-statistics><glmm>,CC BY-SA 3.0,"<p>I'd like to know how following R-code for a binomial GLMM (package lme4) translates into a proper mathematical notation form?</p>

<pre><code>glmer(Y ~ Var1*Var2*Var3+(Var1+Var2+Var3|i) , family=â€binomialâ€, data=df)
</code></pre>

<p>maybe something like:</p>

<p>$$
logit(Y_i)  = Î± + Î²_{Var1} Var1 Ã— Î²_{Var2} Var2 Ã— Î²_{Var3} Var3 + a_i +b_{i Var1} Var1 + b_{i Var2} Var2 +b_{i Var3} Var3 + Îµ_i
$$</p>
",2013-11-01 13:59:52.803
58652,7278.0,1,58654.0,,,Predictions of Random Forest on training data don't lie around x=y line,<regression><random-forest>,CC BY-SA 3.0,"<p>I've trained a random forest on a data set where the targets are in [0, 100]. I use a 5 fold cross validation framework to find the optimum mtry and then train the model on the whole data set using that mtry. When I make predictions on the training set I find that the low targets are over-predicted and the high targets are under-predicted and I don't know why this is happening. The RMSE would be lower if the predictions were just pushed farther from the mean value.<img src=""https://i.stack.imgur.com/auYLY.png"" alt=""Observed vs Predicted""></p>

<p>Can anyone indicate why this is happening? When the model makes predicts on a holdout set this problem is exacerbated but I though it instructive to show the predictions back on the training set in this example. I could augment the model to correct for this and that would improve results on the holdout set, but I would like to understand why this is happening so that I might fix it without augmenting the model. The distribution of target values is given below:<img src=""https://i.stack.imgur.com/9eXgH.png"" alt=""enter image description here""> </p>
",2013-11-01 14:01:56.863
58653,23253.0,1,,,,Reliability standard via relative standard error for computer science or randomized algorithms,<statistical-significance><mean><standard-error><algorithms>,CC BY-SA 3.0,"<p>I am performing experiments on randomized algorithms, and want to do it scientifically sound.</p>

<p>What relative standard error of mean (RSEM) is acceptable? I know that often RSEM&lt;30% or RSEM&lt;25% is used, but does that also apply for randomized algorithms?</p>

<p>What paper/book can I cite that give a reliability standard? This would optimally be a standard for the field of computer science or randomized algorithms, but a general reliability standard would be fine, too.</p>

<hr>

<p>Details: I only found</p>

<ul>
<li><a href=""http://www.cdc.gov/nchs/data/statnt/statnt24.pdf"" rel=""nofollow"">http://www.cdc.gov/nchs/data/statnt/statnt24.pdf</a>, which unfortunately is only for the health sector;</li>
<li><a href=""http://scholar.google.de/scholar?cluster=1360212642214424308&amp;hl=en&amp;as_sdt=0,5"" rel=""nofollow"">http://scholar.google.de/scholar?cluster=1360212642214424308&amp;hl=en&amp;as_sdt=0,5</a> covering statistics for randomized algorithms, but it unfortunately does not mention standard errors.</li>
</ul>
",2013-11-01 14:30:27.487
58654,5448.0,2,,58652.0,,,,CC BY-SA 4.0,"<p>This is actually to be expected, not just with random forests, and comes about as a consequence of the fact that the variance of the target variable = the variance of the model (the estimates) + the variance of the residuals (for least-squares type fitting procedures.)  Given that the latter is positive, unless your model fits perfectly, it must be that the variance of the model &lt; the variance of the target variable.  As a result, the prediction vs. actual plot can't lie on the 45-degree line passing through 0; if it did, the variance of the target variable would be equal to the variance of the model, and there would be no room left for residual variance.</p>

<p>Here are four plots to illustrate this point with linear regression.  In the first one, the error variance is relatively high, and, as a consequence, the predicted - vs - actual plot isn't anywhere near the diagonal line.  In the second through fourth, the error variance is much lower, and the predicted - vs - actual plot gets much closer to the diagonal line.</p>

<p>First, the code:</p>

<pre><code>x &lt;- rnorm(1000)
y &lt;- x + rnorm(1000,0,2) # rnorm(1000,0,1), rnorm(1000,0,0.5), rnorm(1000,0,0.1) 

plotlim &lt;- range(y)
plot(predict(lm(y~x))~y,ylim=plotlim,xlim=plotlim)
abline(c(0,1))
</code></pre>

<p>Now, the plots:</p>

<p><img src=""https://i.stack.imgur.com/hgFGj.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/T5VJF.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/BX800.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/tWQXh.png"" alt=""enter image description here""></p>

<p>Consequently, there's no need to alter your fitting procedure or augment your model.</p>

<p>Further heuristic explanation: Note that this comes about because $\sigma^2_Y &gt; \sigma^2_X$, in this particular linear regression model.  Therefore, even with the true parameter values (in this case, 0 intercept and 1 slope), the plot of $Y$ will be more spread out than the plot of $X$, and, since the estimated values of $Y$ with the true parameter values will equal $X$, it will also be the case that the plot of $Y$ will be more spread out than the plot of the estimated values of $Y$.  As a result, the estimated values vs. true values plot will not lie on a 45-degree line.</p>
",2013-11-01 14:35:34.233
58655,23249.0,1,58657.0,,,"Analyzing installation rates, not quite binomial. Tried bootstrapping. Is there a approximate binomial approach that would be better?",<binomial-distribution><bootstrap>,CC BY-SA 3.0,"<p>I'm analyzing a data set of installation rates of lighting fixture projects and want to determine the expected installation rate and the confidence interval. Projects have various numbers of fixtures intended to be installed and the most common rate is 100% followed in frequency by a handful of 0% and then a small number of rates besides 0 and 100%. The distribution of the sample data looks like this:</p>

<p><img src=""https://i.stack.imgur.com/zvKFL.png"" alt=""enter image description here""></p>

<p>In many ways it seems like this data should follow a binomial distribution except for the variable number of fixtures in each project and the possibility of values other than 0 and 1.</p>

<p>As a result of this confusion, I simply bootstrapped the average installation rates, weighted by project size, and arrived at this distribution of results.</p>

<p><img src=""https://i.stack.imgur.com/Azhqg.png"" alt=""enter image description here""></p>

<p>Is this a reasonable approach, or is there a better way to treat this as an approximate binomial distribution with weights for project size, or use a non-parametric approach?</p>

<p>Thanks in advance for any assistance...</p>
",2013-11-01 14:36:05.607
58697,23280.0,1,,,,"What is the difference among Indicator, Index, Variable and Measure?",<terminology><scales><measurement>,CC BY-SA 3.0,"<p>What is exactly the difference among <strong>Indicator, Index, Variable and Measure</strong>? I would appreciate some reference.</p>
",2013-11-02 11:03:54.097
58721,22682.0,2,,58720.0,,,,CC BY-SA 3.0,"<p>The solution to calculating the mean:</p>

<p>$\mathbb{E}(S_N) = 0.P(N=0) + \mathbb{E}(X_1).P(N=1) + \mathbb{E}(X_1+X_2).P(N=2) + . . .$</p>

<p>$= 0 + \mu_2P(N=1) + 2\mu_2P(N=2) + . . . = \mu_2\sum_{i=0}^\infty i.P(N=i)$</p>

<p>and the infinite sum above is just equal to the expectation of $N$, hence:</p>

<p>$\mathbb{E}(S_N) = \mu_2.\mathbb{E}(N) = \mu_2\mu_1$</p>
",2013-11-02 19:16:47.060
58656,23255.0,2,,58578.0,,,,CC BY-SA 3.0,"<p>Because of your sample size, I would recommend using <a href=""http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#Leave-one-out_cross-validation"" rel=""nofollow"">leave one out cross</a> validation to estimate model fit.  The algorithm goes like this: </p>

<ol>
<li>Take out an observation from the data set.</li>
<li>Fit the model.</li>
<li>Estimate the class label of the held out observation.</li>
<li>Repeat steps 1-3 until all observations have been held out.</li>
</ol>

<p>To see the algorithms performance, look at how well it fit the held out data. </p>

<p>You may find, as Simone suggested, that your algorithm is overfitting the data.</p>

<p>One thing that I have found is that you need much more than 16 observations to fit a good classification model.  </p>

<p>Check out the <strong>cvTools</strong> package to perform the cross validation.</p>
",2013-11-01 14:36:27.640
58657,503.0,2,,58655.0,,,,CC BY-SA 3.0,"<p>If you want an estimate of the average (arithmetic mean) and its error, the bootstrap seems reasonable to me.</p>

<p>But are you sure you want the arithmetic mean and its error? </p>

<p>Of course, it depends on your application, but for most purpose, it seems to me that you would want something else. You might want estimation of quantiles (say, 10th, median and 90th percentiles), either of the installation rate (possibly weighted, possibly not); or you might want to go straight to ""number of fixtures"" to be installed. </p>
",2013-11-01 15:09:53.100
58658,20457.0,1,58719.0,,,Training a convolution neural network,<machine-learning><neural-networks><computer-vision><backpropagation><conv-neural-network>,CC BY-SA 3.0,"<p>I am currently working on a face recognition software that uses convolution neural networks to recognize faces. Based on my readings, I've gathered that a convolutional neural network has shared weights, so as to save time during training. But, how does one adapt backpropagation so it can be used in a convolution neural network. In backpropagation, one uses a formula similar to this to train the weights.</p>

<pre><code>New Weight  = Old Weight +  LEARNING_RATE * 1 * Output Of InputNeuron * Delta
</code></pre>

<p>However, since in convolutional neural networks, the weights are shared, each weight is used with multiple neurons, so how do I decide which <code>Output of InputNeuron</code> is used?</p>

<p>In other words, since the weights are shared, how do I decide how much to change the weights by?</p>
",2013-11-01 16:05:22.290
58659,668.0,2,,14729.0,,,,CC BY-SA 3.0,"<p>The question asks about ""identifying underlying [linear] relationships"" among variables.</p>

<p><strong>The quick and easy way to <em>detect</em> relationships</strong> is to regress any other variable (use a constant, even) against those variables using your favorite software: any good regression procedure will detect and diagnose collinearity.  (You will not even bother to look at the regression results: we're just relying on a useful side-effect of setting up and analyzing the regression matrix.)</p>

<p>Assuming collinearity is detected, though, what next?  <strong>Principal Components Analysis</strong> (PCA) is exactly what is needed: its <em>smallest</em> components correspond to near-linear relations.  These relations can be read directly off the ""loadings,"" which are linear combinations of the original variables.  Small loadings (that is, those associated with small eigenvalues) correspond to near-collinearities.  An eigenvalue of $0$ would correspond to a perfect linear relation.  Slightly larger eigenvalues that are still much smaller than the largest would correspond to approximate linear relations.</p>

<p>(There is an art and quite a lot of literature associated with identifying what a ""small"" loading is.  For modeling a dependent variable, I would suggest including it within the independent variables in the PCA in order to identify the components--regardless of their sizes--in which the dependent variable plays an important role.  From this point of view, ""small"" means much smaller than any such component.)</p>

<hr>

<p><strong>Let's look at some examples.</strong>  (These use <code>R</code> for the calculations and plotting.)  Begin with a function to perform PCA, look for small components, plot them, and return the linear relations among them. </p>

<pre class=""lang-R prettyprint-override""><code>pca &lt;- function(x, threshold, ...) {
  fit &lt;- princomp(x)
  #
  # Compute the relations among ""small"" components.
  #
  if(missing(threshold)) threshold &lt;- max(fit$sdev) / ncol(x)
  i &lt;- which(fit$sdev &lt; threshold)
  relations &lt;- fit$loadings[, i, drop=FALSE]
  relations &lt;- round(t(t(relations) / apply(relations, 2, max)), digits=2)
  #
  # Plot the loadings, highlighting those for the small components.
  #
  matplot(x, pch=1, cex=.8, col=""Gray"", xlab=""Observation"", ylab=""Value"", ...)
  suppressWarnings(matplot(x %*% relations, pch=19, col=""#e0404080"", add=TRUE))

  return(t(relations))
}
</code></pre>

<p>Let's apply this to some random data.  These are built on four variables (the $B,C,D,$ and $E$ of the question).  Here is a little function to compute $A$ as a given linear combination of the others.  It then adds i.i.d. Normally-distributed values to all five variables (to see how well the procedure performs when multicollinearity is only approximate and not exact).</p>

<pre class=""lang-R prettyprint-override""><code>process &lt;- function(z, beta, sd, ...) {
  x &lt;- z %*% beta; colnames(x) &lt;- ""A""
  pca(cbind(x, z + rnorm(length(x), sd=sd)), ...)
}
</code></pre>

<p>We're all set to go: it remains only to generate $B, \ldots, E$ and apply these procedures.  I use the two scenarios described in the question: $A=B+C+D+E$ (plus some error in each) and $A=B+(C+D)/2+E$ (plus some error in each).  First, however, note that PCA is almost always applied to <em>centered</em> data, so these simulated data are centered (but not otherwise rescaled) using <code>sweep</code>.</p>

<pre class=""lang-R prettyprint-override""><code>n.obs &lt;- 80 # Number of cases
n.vars &lt;- 4 # Number of independent variables
set.seed(17)
z &lt;- matrix(rnorm(n.obs*(n.vars)), ncol=n.vars)
z.mean &lt;- apply(z, 2, mean)
z &lt;- sweep(z, 2, z.mean)
colnames(z) &lt;- c(""B"",""C"",""D"",""E"") # Optional; modify to match `n.vars` in length
</code></pre>

<p>Here we go with two scenarios and three levels of error applied to each.  The original variables $B, \ldots, E$ are retained throughout without change: only $A$ and the error terms vary.</p>

<p><img src=""https://i.stack.imgur.com/0yXVh.png"" alt=""Results""></p>

<p>The output associated with the upper left panel was</p>

<pre><code>       A  B  C  D  E
Comp.5 1 -1 -1 -1 -1
</code></pre>

<p>This says that the row of red dots--which is constantly at $0$, demonstrating a <em>perfect</em> multicollinearity--consists of the combination $0 \approx A -B-C-D-E$: exactly what was specified.</p>

<p>The output for the upper middle panel was</p>

<pre><code>       A     B     C     D     E
Comp.5 1 -0.95 -1.03 -0.98 -1.02
</code></pre>

<p>The coefficients are still close to what we expected, but they are not quite the same due to the error introduced.  It thickened the four-dimensional hyperplane within the five-dimensional space implied by $(A,B,C,D,E)$ and that tilted the estimated direction just a little.  With more error, the thickening becomes comparable to the original spread of the points, making the hyperplane almost impossible to estimate.  Now (in the upper right panel) the coefficients are</p>

<pre><code>       A     B     C     D     E
Comp.5 1 -1.33 -0.77 -0.74 -1.07
</code></pre>

<p>They have changed quite a bit but still reflect the basic underlying relationship $A' = B' + C' + D' + E'$ where the primes denote the values with the (unknown) error removed.</p>

<p>The bottom row is interpreted the same way and its output similarly reflects the coefficients $1, 1/2, 1/2, 1$.</p>

<p>In practice, it is often not the case that one variable is singled out as an obvious combination of the others: all coefficients may be of comparable sizes and of varying signs.  Moreover, when there is more than one dimension of relations, there is no unique way to specify them: further analysis (such as row reduction) is needed to identify a useful basis for those relations. That's how the world works: <strong>all you can say is that <em>these</em> particular combinations that are output by PCA correspond to almost no variation in the data.</strong>  To cope with this, some people use the largest (""principal"") components directly as the independent variables in the regression or the subsequent analysis, whatever form it might take.  If you do this, do not forget first to remove the dependent variable from the set of variables and redo the PCA!</p>

<hr>

<p>Here is the code to reproduce this figure:</p>

<pre class=""lang-R prettyprint-override""><code>par(mfrow=c(2,3))
beta &lt;- c(1,1,1,1) # Also can be a matrix with `n.obs` rows: try it!
process(z, beta, sd=0, main=""A=B+C+D+E; No error"")
process(z, beta, sd=1/10, main=""A=B+C+D+E; Small error"")
process(z, beta, sd=1/3, threshold=2/3, main=""A=B+C+D+E; Large error"")

beta &lt;- c(1,1/2,1/2,1)
process(z, beta, sd=0, main=""A=B+(C+D)/2+E; No error"")
process(z, beta, sd=1/10, main=""A=B+(C+D)/2+E; Small error"")
process(z, beta, sd=1/3, threshold=2/3, main=""A=B+(C+D)/2+E; Large error"")
</code></pre>

<p>(I had to fiddle with the threshold in the large-error cases in order to display just a single component: that's the reason for supplying this value as a parameter to <code>process</code>.)</p>

<hr>

<p>User ttnphns has kindly directed our attention to a closely related thread.  <a href=""https://stats.stackexchange.com/a/16399/919"">One of its answers (by J.M.)</a> suggests the approach described here.</p>
",2013-11-01 16:07:11.403
58660,11490.0,1,,,,Transformation to normality for random variables with different locations,<data-transformation><normalization><normality-assumption>,CC BY-SA 3.0,"<p>I have a (potentially infinite) sequence of random variables $X_i$, with $i = 1, 2, \dots$, which have the same distribution (in terms of ""shape""), but different locations. </p>

<p>I have a sample of size $N$ from only one of these distributions and I would like to find a transformation to normality that works for the whole sequence of distributions. The locations are all unknown as well the analytic form of true distributions. All I have is a sample from one element of the sequence.</p>

<p>Notice that I need a transformation that uses the same parameters for every distribution. For example if, give my initial sample, I decide to use the Box-Cox($\lambda_1$, $\lambda_2$) transform, when I get a new sample I have to transform it using exactly the same transform (otherwise I would simply shift each sample to zero).</p>

<p>So far I haven't found anything that works: Box-Cox requires positivity of the sample (the shifting parameter doesn't help because the location varies from sample to sample). Similar problem for the Inverse Hyperbolic Sine transform.</p>

<p>Any suggestion is welcome! </p>

<p>EDIT:</p>

<p>This problem arises from an application where I have a data $y_1, \dots, y_n$, generated by a stochastic model $m(\theta_0)$, where $\theta_0$ is an unknown parameter.</p>

<p>I'm trying to estimate $\theta_0$ by Maximum Likelihood. At each step $k$ of the optimization I'm simulating a sample $x_1, \dots, x_n$ from the model $m(\theta_k)$, and I rely on the sample being approximately normally distributed to estimate the likelihood. Unfortunately the sample often has departures from normality, which is why I'm interested in transformations. I need to use exactly the same transformation across the parameters space in order to compute the likelihood correctly. For different $\theta$s the mean of the sample changes, but shape of the distribution seems to be fairly constant.</p>
",2013-11-01 16:31:55.667
58661,23259.0,1,,,,How to find the support of the posterior distribution to apply Metropolis-Hastings MCMC algorithm?,<bayesian><markov-chain-montecarlo><inference>,CC BY-SA 3.0,"<p>I am trying to sample from a posterior distribution using a MCMC algorithm using the Metropolis-Hastings sampler.</p>

<p>How should I deal with the situations where I'm stuck in regions of the posterior with zero probability?</p>

<p>These regions are present because the posterior distribution is truncated and also due to numerical limitations on the computer the likelihood can become zero if you are very far from the mean. That is, say the likelihood is distributed normally; if you are 100 standard deviations away from the mean you get what appears as zero probability to the computer.</p>

<p>What I want to know is how to chose the initial value of the chain in order to be sure that it is contained in the support of the posterior.</p>
",2013-11-01 17:34:53.247
58662,23171.0,1,58668.0,,,How to compute conditional expectations with respect to a sigma field?,<probability><self-study><conditional-probability><conditional-expectation>,CC BY-SA 3.0,"<p><strong>Example:</strong> Toss a coin twice. Letting $\mathbb P$ be a probability measure, suppose $\mathbb P(HH)=p^2,\mathbb P(HT)=\mathbb P(TH)=p(1-p), \mathbb P(TT)=(1-p)^2.$  I would like to answer the following questions:  </p>

<ol>
<li><p>Define $Y$ to be the number of heads in the example. Derive the $\sigma$-field generated by $Y$.   </p></li>
<li><p>Find the expectation of the random variable $Y$ from the previous
exercise, and also the conditional expectation of $Y$ given $\mathcal F_{1}$ s.t $\mathcal F_{1}=\{\varnothing,\{HH,HT\},\{TH,TT\},\Omega\}$. Check that in this case $E[E[Y|\mathcal F_{1}]]=E[Y]$.      </p></li>
</ol>

<p><strong>My answers:</strong> the sample space is $\Omega=\{HH,HT,TH,TT\}$. I think the answer to 1 is 
$Y=\{\{TT\},\{HT,TH\},\{HH\}\}$, the $\sigma$-field generated by $Y$ is $\mathcal F(Y)=\{\varnothing,\{TT\},\{HH,HT,TH\},\{HT,TH\},\{HH,TT\},\{HH\},\{HT,TH,TT\},\Omega\}.$</p>

<p>But for question 2 I could only get $E(Y)=2p$.  How is the conditional expectation computed?</p>
",2013-11-01 18:40:11.737
58663,22705.0,2,,22797.0,,,,CC BY-SA 3.0,"<p>If X &amp; Y are indeed co-integrated, then regressing X on Y will lead to spurious regression. So, I'm guessing your residuals will have a good amount of information which is not captured in the model.</p>

<p>But, I might have not understood some parts of your question well - When you say.. ""how does it affect my analysis"", it will help if you're a little more clearer about the specific problem because it seems as though you aren't doing time series analysis at all and are doing plain regression?</p>

<p>And, what do you mean by spread? Is it residual or something else?</p>
",2013-11-01 18:59:22.840
58694,23278.0,1,,,,Linear regression with strongly non-normal response variable,<regression><residuals><assumptions>,CC BY-SA 3.0,"<p>I have carried out a linear regression. The plot below shows the distribution of the response variable:</p>

<p><img src=""https://i.stack.imgur.com/4RtjY.jpg"" alt=""enter image description here""></p>

<p>I believe the response variable is beta distributed, therefore virtually the exact opposite of normally distributed. However, when including all my predictor variables in the linear regression, the residuals turn out to be quite normally distributed, as shown in this plot:</p>

<p><img src=""https://i.stack.imgur.com/kFJSJ.jpg"" alt=""enter image description here""></p>

<p>Has my model satisfied the assumptions of linear regression? Might there be a better model to use?</p>
",2013-11-02 09:56:37.087
58664,23262.0,1,,,,Question about prediction bands for non-linear regression computation?,<nonlinear-regression><prediction-interval>,CC BY-SA 3.0,"<p>I am interested in computing prediction bands for a non-linear regression (log-logistic function with 3 parameters). I have read the Prism help page:</p>
<blockquote>
<p>The calculation of the confidence and prediction bands are fairly standard. Read on for the details of how Prism computes prediction and confidence bands of nonlinear regression.</p>
<p>First, let's define G|x, which is the gradient of the parameters at a particular value of X and using all the best-fit values of the parameters. The result is a vector, with one element per parameter. For each parameter, it is defined as dY/dP, where Y is the Y value of the curve given the particular value of X and all the best-fit parameter values, and P is one of the parameters.)</p>
<p>G'|x is that gradient vector transposed, so it is a column rather than a row of values.</p>
<p>Cov is the covariance matrix (inversed Hessian from last iteration). It is a square matrix with the number of rows and columns equal to the number of parameters. Each item in the matrix is the covariance between two parameters.</p>
<p>Now compute c = G'|x * Cov * G|x. The result is a single number for any value of X.</p>
<p>The confidence and prediction bands are centered on the best fit curve, and extend above and below the curve an equal amount.</p>
<p>The confidence bands extend above and below the curve by: = sqrt(c)*sqrt(SS/DF)*CriticalT(Confidence%, DF)</p>
<p>The prediction bands extend a further distance above and below the curve, equal to: = sqrt(c+1)*sqrt(SS/DF)*CriticalT(Confidence%, DF)</p>
</blockquote>
<p>I also read the topic <a href=""https://stats.stackexchange.com/questions/15423/how-to-compute-prediction-bands-for-non-linear-regression?answertab=active#tab-top"">How to compute prediction bands for non-linear regression?</a>.</p>
<p>But I still do not understand something : how to compute the Hessian matrix. I do not think it is (I do not get values between -1 and 1), but, is it the second-order partial derivatives of my log-logistic function ?</p>
<p>In Prism, it refers to the &quot;Hessian from last iteration&quot; while in the topic, it is written &quot;the second derivatives of the likelihood function at your estimates&quot;.</p>
<p>The likelihood function I use is ( -<em>N</em> / 2 ) * log( sum( ( <em>dat</em> - <em>model</em> ) .^2 ) ) where <em>N</em> is the number of observations, <em>dat</em> is a vector with data being fitted and <em>model</em> is a vector of model values. I do not really understand how it would be possible to do a second order derivatives of this function.</p>
<p>Could anyone give me a hand with this ?</p>
",2013-11-01 19:11:44.703
58665,22544.0,1,,,,What part of an ARMA model requires a stationary time series - the AR or the MA?,<time-series><arima><stationarity>,CC BY-SA 3.0,"<p>Could I use a non-stationary time series with simply an Autoregressive model?</p>
",2013-11-01 19:25:49.477
58666,17573.0,2,,58070.0,,,,CC BY-SA 3.0,"<p>Sorry about the delay in answering.  What's going on, I think, is seasonality.  </p>

<p>Suppose there are three temperatures.  First there is ""true"" temperature---whatever average high surface air temperature actually occurs on each day.  Second, there is ground measured temperature.  Third, there is satellite measured temperature.  The second and third temperatures are the first temperature, except measured with error.  Like this:
\begin{align}
\textrm{ground}_{d,y} &amp;= \textrm{true}_{d,y} + \nu_{d,y}\\
\textrm{satellite}_{d,y} &amp;= \textrm{true}_{d,y} + \eta_{d,y}\\
\end{align}</p>

<p>Above, $\mathrm{true}_{d,y}$ means the true temperature in year $y$ on day $d$, where day counts up from 1 on 01/01/year to 365 on 12/31/year --- I am too lazy to think about leap years here.</p>

<p>Now, if you regress ground on satellite, of course you are going to get a regression with some predictive power, because both measurements are being driven by the same true temperature.  However, the predictive power will not be perfect.  How good will the predictions be?  Well, that depends on the ratio of signal (variance of movements in true temperature) to noise (variance of movements in $\nu$ and $\eta$.  In fact, you can calculate the covariance and correlation between ground and satellite (assuming that the $\nu$ and $\eta$ are iid, independent of each other, and independent of true):
\begin{align}
Cov(\mathrm{ground},\mathrm{satellite}) &amp;= V(\mathrm{true})\\
Corr(\mathrm{ground},\mathrm{satellite}) &amp;= \frac{V(\mathrm{true})}
               {\sqrt{(V(\mathrm{true})+V(\nu))(V(\mathrm{true})+V(\eta))}}
\end{align}</p>

<p>Now, in the simple bivariate model, when you just regress ground on satellite, $R^2$ is equal to the correlation above squared, or:
\begin{align}
R^2 &amp;= \frac{(V(\mathrm{true}))^2}
               {(V(\mathrm{true})+V(\nu))(V(\mathrm{true})+V(\eta))}
\end{align}</p>

<p>So, why is $R^2$ so much higher in the Nov to Apr sample than in the Jan sample?  That's easy.  As long as you are looking at a weather station far from the equator, there are big month-to-month differences in true.  So, the more of the year you include in your time window, the higher is $V(\mathrm{true})$, and the higher is $R^2$.</p>

<p>Why, then, is the mean of the sum of squared residuals higher in the Nov to Apr sample than in the Jan sample?  The mean of the sum of squared residuals goes in probability to:
\begin{align}
\frac{1}{n}\sum e_i^2 &amp;\rightarrow V(\mathrm{ground}) + \beta^2 V(\mathrm{satellite})
\end{align}</p>

<p>The $\beta$ in the above is the probability limit of the regression coefficient.  Notice that this quantity goes up as the variance of both ground and satellite go up.  As you expand your time window from Jan only to Nov through Apr, the variance of true goes up and this drives the variance of both ground and satellite up.  In turn, this drives the variance of the residuals up.</p>

<p>If you prefer a numerical example, here is some R code where you can see what is going on.  All I have done is implement the reasoning above in a very simple monte carlo.  As you can see if you run the code, I have pretty well replicated your $R^2$ and mean squared residual results using somewhat realistic temperature data:</p>

<pre><code># This program written in response to a Cross Validated question
# http://stats.stackexchange.com/questions/73544/multiple-linear-regression-models-comparison-based-on-r-squared-and-residual-err#comment143511_73544

# The program is a toy monte carlo.
# It generates a ""true"" but unobservable-to-the-analyst high temperature series
# in degrees Celcius for a location in the northern hemisphere of the earth 
# pretty far from the equator.  It is very loosely based on Des Moines, IA.
# The series simulates seasonality with a sinusoidal curve and generates 
# temperatures for all days from 1980-2000, ignoring leap years.

# Then it generates two series which measure the true temperature with error,
# calling them satellite and ground.  Then it regresses ground on satellite 
# by OLS over various slices of the data.  The slices are only January,
# only Nov through Apr, and all year long.

# day is days since 12/31/1979, ie it starts at 1 on 01/01/1980
# I ignore leap years and assume 365 day years
# day maxes out at 20*365=7300

set.seed(12344321)
day &lt;- seq(1:7300)


true.temp &lt;- 32*sin(((day-1)/365)*2*pi + 3*pi/2) + 21 + 4*rnorm(7300)

plot(day[1:365],true.temp[1:365])

# Two fallible measures of true temp

satellite &lt;- true.temp + 4*rnorm(7300)
ground &lt;- true.temp + 4*rnorm(7300)

plot(day[1:365],satellite[1:365])
plot(day[1:365],ground[1:365])
plot(satellite[1:365],ground[1:365])

# Now predict ground with satellite over the whole year
pred.reg &lt;- lm(ground~satellite)
summary(pred.reg)

# Now predict ground with sattelite over January
january &lt;- ((day-1) %% 365) %/% 31 == 0
plot(satellite[january],ground[january],xlab=""satellite"",ylab=""ground"",main=""January Only"")
pred.reg &lt;- lm(ground[january]~satellite[january])
summary(pred.reg)

# Now predict ground with sattelite over Nov through April
novtoapr &lt;- ((day-1)%%365)&gt;304 | ((day-1)%%365)&lt;92
plot(satellite[novtoapr],ground[novtoapr],xlab=""satellite"",ylab=""ground"",main=""Nov to Apr"")
pred.reg &lt;- lm(ground[novtoapr]~satellite[novtoapr])
summary(pred.reg)
</code></pre>

<p>Look at the plots of ground against satellite for Jan only and Nov to Apr.  You can see the higher $R^2$ in the Nov to Apr:</p>

<p><img src=""https://i.stack.imgur.com/34TrE.jpg"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/zL116.jpg"" alt=""enter image description here""></p>

<p>Obviously, in your application there is a lot more going on since you have other right-hand-side variables in your analysis.  Also, the measurement errors are probably not nice, normally distributed, iid things like in the monte carlo.  Nevertheless, I think what is going on in your data is likely to be what is going on in this monte carlo.  In conclusion, I don't think your results are anything to worry about.  They are exactly what we would expect in this circumstance.</p>
",2013-11-01 19:35:04.697
58667,9081.0,2,,57540.0,,,,CC BY-SA 3.0,"<p>If you need a reference for the answer in my comment above, <a href=""http://andrewgelman.com/2009/05/27/the_benefits_of/"" rel=""noreferrer"">here is one</a>, from Andrew Gelman's blog:</p>

<blockquote>
  <p>Which reminds me of Lucien Le Camâ€™s reply when I asked him once whether he could think of any examples where the distinction between the strong law of large numbers (convergence with probability 1) and the weak law (convergence in probability) made any difference. Le Cam replied, No, he did not know of any examples. Le Cam was the theoretical statisticianâ€™s theoretical statistician, so thereâ€™s your answer.  </p>
</blockquote>

<p>One could maybe add that the real importance of this different modes of convergence lies in the mathematics, that they permit the use of different <strong>mathematical</strong> techniques, in the development of the theory, only.  And that might be important enough, but for the development of theory, not in the concrete practical applications. </p>
",2013-11-01 20:34:39.070
58668,668.0,2,,58662.0,,,,CC BY-SA 3.0,"<p>The <a href=""http://en.wikipedia.org/wiki/Conditional_expectation#Formal_definition"">formal definition of conditional expectation</a> is that $E[Y|\mathcal{F}_1]$ is any <em>random variable</em> measurable with respect to $\mathcal{F}_1$ having the property that</p>

<p>$$\int_F E[Y|\mathcal{F}_1](\omega)d\mathbb{P}(\omega) = \int_F Y(\omega) d\mathbb{P}(\omega)$$</p>

<p>for all $\mathcal{F}_1$-measurable sets $F$.</p>

<p>In the present case, this definition invites us to inspect all the measurable subsets $F$ with respect to $\mathcal{F}_1$, which you already computed in the first problem.  The trick is to <strong>begin with the smallest, most basic $\mathcal{F}_1$-measurable sets</strong> (apart from the empty set), which are $\{HH, HT\}$ and $\{TH, TT\}$.  Although we don't yet know  $E[Y|\mathcal{F}_1]$, we can use the right hand side to compute its integrals.  Because neither of these events can be decomposed (nontrivially) into smaller ones, <em>the conditional expectation must have a constant value on each one.</em>  For example, writing 
$$E[Y|\mathcal{F}_1](HH) =E[Y|\mathcal{F}_1](HT) = z,$$</p>

<p>the definition gives</p>

<p>$$\eqalign{
zp &amp;= zp^2 + zp(1-p) \\
&amp;=E[Y|\mathcal{F}_1](HH) \mathbb{P}(HH) +E[Y|\mathcal{F}_1](HT) \mathbb{P}(HT)\\
&amp;=\int_{\{HH, HT\}} E[Y|\mathcal{F}_1](\omega)d\mathbb{P}(\omega)\\
&amp;= \int_{\{HH, HT\}} Y(\omega) d\mathbb{P}(\omega)\\
&amp;= Y(HH)\mathbb{P}(HH) + Y(HT)\mathbb{P}(HT) \\
&amp;= 2p^2 + 1p(1-p)= p+p^2,
}$$</p>

<p>whence we deduce</p>

<p>$$z = \frac{p+p^2}{p} = 1 + p.$$</p>

<p>A similar calculation for $F = \{TH, TT\}$ (do it!) establishes that</p>

<p>$$E[Y|\mathcal{F}_1](TH) =E[Y|\mathcal{F}_1](TT) = p.$$</p>

<p><strong>There is a simple intuition</strong> to support these abstract calculations: $\mathcal{F}_1$ records the information available after flipping the coin the first time.  If it comes up heads, the possible events (which have only partially occurred!) are $HH$ and $HT$.  We already have one head and there is a chance $p$ that the second flip will be a head.  Thus, <em>at this stage,</em> our expectation of $Y$ equals $1$ (for what has happened) plus $1\times p$ (for what <em>could</em> happen yet), summing to $1+p$.  If instead the first flip is tails, we have seen no heads yet but there is still a chance of $p$ of seeing a head on the second flip: the expectation of $Y$ is just $0 + 1\times p = p$ in that case.</p>

<p>As a check, we can compute </p>

<p>$$\eqalign{
E[E[Y|\mathcal{F}_1]] &amp;= \int_\Omega E[Y|\mathcal{F}_1](\omega)d\mathbb{P}(\omega) \\
&amp;= (1+p)\mathbb{P}(E[Y|\mathcal{F}_1]=1+p) + (p)\mathbb{P}(E[Y|\mathcal{F}_1]=p)\\
&amp; = (1+p)\mathbb{P}(\{HH, HT\}) + (p) \mathbb{P}(\{TH, TT\})\\
&amp;= (1+p)(p^2 + p(1-p)) + p((1-p)p + (1-p)^2)\\
&amp;= 2p,
}$$</p>

<p>exactly as before.</p>

<p>It should be clear that this is just a laborious way of expressing the idea that there is a $p$ chance at the outset of heads--which has a conditional expectation of $1+p$-- and a $1-p$ chance of tails--which has a conditional expectation of $p$: everything reduces to the elementary calculation of conditional expectations, which needs no sigma fields or integrals.  The point of this exercise is to build on that intuition to develop an understanding that will hold up when these sigma algebras get much, much more complicated.</p>
",2013-11-01 20:58:01.017
58669,1602.0,1,,,,How to algorithmically determine the best order of fit?,<model-selection><curve-fitting><interpolation><polynomial>,CC BY-SA 3.0,"<p>I am doing a least squares polynomial interpolation for 10,000 data sets that look mostly like one period of a sine curve, but whose values are not evenly spaced in the time domain, and can sometimes be quite noisy. A lot of them do just fine with a 3rd order fit (a + bx + cx^2 + dx^3) but some oscillate wildly (Runge's phenomenon) and need a 2nd order instead, and some are much better fitted with a 5th or even 8th order fit.</p>

<p>What methods exist for determining which fit is best with respect to avoiding both underfitting and overfitting? </p>

<p>One thing I think that I can do is to choose the largest order $m$ such that $m&lt;2\sqrt{n}$ where $n$ is the number of points in my data set, and also check to make sure that there are some number of points in $2\sqrt{n}$ equally sized bins spanning the space. On the other hand, however, I would also like to penalize for having more parameters than necessary. Maybe I can also minimize $SSE(m)/(n-m-1)$. What better methods are out there for doing this? </p>
",2013-11-01 20:58:06.343
58679,20320.0,1,331473.0,,,Differences between clustering and segmentation,<time-series><clustering><multivariate-analysis><terminology><timeseries-segmentation>,CC BY-SA 3.0,"<p>I have read about piecewise aggregate approximation (PAA) <a href=""http://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=9&amp;ved=0CF4QFjAI&amp;url=http://www.researchgate.net/publication/227001229_Mining_Time_Series_Data/file/32bfe51188657160b5.pdf&amp;ei=Sdt3UomgMYaZiQL3sYHwCA&amp;usg=AFQjCNGPeKza7WQFTssFTlWe0fL6_HnijA&amp;sig2=BXG_vo_0pMzbrwWgifQPEw&amp;bvm=bv.55819444,d.cGE"" rel=""nofollow"">mining time series data</a>, sliding window, top down and bottom up approaches for time series segmentation but these are applicable to single dimension time series. </p>

<p>What are the techniques for multi dimensional time series segmentation? Can Gaussian mixture model or K means clustering be used for segmentation? If so then what is the difference between segmentation and clustering? </p>

<ol>
<li><p>What is the difference between segmenting and clustering</p></li>
<li><p>How do I segment motion time series data so that the temporal information is retained.</p></li>
<li><p>What are the algorithms for doing so - What are the techniques for multi dimensional time series segmentation??</p></li>
</ol>

<p>Please provide links or ideas thank you.</p>
",2013-11-02 00:32:34.453
58670,7007.0,2,,58662.0,,,,CC BY-SA 3.0,"<p>Take the sample space $\Omega$ as the Cartesian product $\{H,T\}\times\{H,T\}$, with sigma-field $\mathscr{F}$ equal to the class of all subsets of $\Omega$. The sigma-field generated by $Y$ (denoted by $\sigma(Y)$) is the smallest sub-sigma-field of $\mathscr{F}$ in which $Y$ is measurable. Since $Y\in\{0,1,2\}$, the inverse images 
$$
  Y^{-1}(\{0\}) = \{(T,T)\}\, , \quad Y^{-1}(\{1\}) = \{(H,T),(T,H)\}\, , \quad \quad Y^{-1}(\{2\}) = \{(H,H)\} \, ,
$$
show that 
$$
  \sigma(Y)=\sigma\left\{\{(T,T)\}, \{(H,T),(T,H)\}, \{(H,H)\}\right\}
  = \left\{\emptyset,\{(T,T)\}, \{(H,T),(T,H)\}, \{(H,H)\},\{(H,T),(T,H),(H,H)\},\{(T,T),(H,H)\},\{(T,T),(H,T),(T,H)\},\Omega\right\} \, .
$$
Define 
$$
  \mathscr{G}=\{\emptyset,\{(H,H),(H,T)\},\{(T,T),(T,H)\},\Omega\}\subset\mathscr{F} \, .
$$</p>

<p>The conditional expectation $Z=\mathrm{E}[Y\mid \mathscr{G}]$ is a $\mathscr{G}$-measurable random variable satisfying
$$
  \mathrm{E}[Y I_A]=\mathrm{E}[Z I_A] \, , \qquad\qquad (*)
$$
for every $A\in\mathscr{G}$. The fact that $Z$ is $\mathscr{G}$-measurable entails that it is constant in the atoms of $\mathscr{G}$ (this is the crucial idea). Let
$$
  Z(H,H) = Z(H,T) = a, \quad Z(T,T) = Z(T,H) = b \, .
$$
Taking $A=\{(H,H),(H,T)\}$, relation $(*)$ yields
$$
   2 \cdot p^2 + 1 \cdot p(1-p) = a \cdot p^2 + a \cdot p(1-p) \, ,
$$
implying that $a=1+p$. Similarly, we find $b=p$.</p>

<p>Finally,
$$
  \mathrm{E}[Y]= 1 \cdot 2 p(1-p) + 2\cdot p^2 = 2p \, ,
$$
and
$$
  \mathrm{E}[Z] = (1+p) \cdot (p^2 + p(1-p)) + p \cdot ((1-p)^2 + p(1-p)) = 2p \, ,
$$
as ""expected"".</p>

<p>P.S. May I suggest a beautiful related exercise? Let $\Omega=[0,1]$, $\mathscr{F}$ be the Borel subsets of $\Omega$, and $P$ be Lebesgue measure. Let $\mathscr{G}$ be the sub-sigma-field of $\mathscr{F}$ generated by the partition
$$\{[0,1/2],(1/2,1]\}\, .
$$
Let $X$ be the identity map ($X(\omega)=\omega$). Plot the graph of $\mathrm{E}[X\mid\mathscr{G}]$.</p>
",2013-11-01 21:16:30.703
58671,23263.0,1,,,,Cox Regression Prediction of Event Count For Given Period,<prediction><cox-model>,CC BY-SA 3.0,"<p>I have a standard event history data set, which I used to fit a Cox proportional hazards model in Stata and R. The time variable is measured in days. There are no time-varying covariates on the RHS of the equation. </p>

<p>I have recovered the usual output (coefficients, hazard ratios, SEs, etc.) as well as the predicted baseline survival for each observation. What I need to do now is estimate the predicted number of events in the next 30 days. Any ideas on how to do this?</p>

<p>Thanks very much!</p>
",2013-11-01 21:16:59.367
58672,3993.0,2,,58627.0,,,,CC BY-SA 3.0,"<p>It seems to me that Ellis could be referring to as many as three distinct ideas here. First he says something about creating ""simulated data generated by a model under the null hypothesis of no relation."" I would call this a form of <strong>parametric bootstrapping</strong>. Then he says that this would be ""probably based on resampling the times between each event (eg between each yawn) to create a new set of time stamps for hypothetical null model events."" Which, let's just be clear here, to do this is <em>not</em> to ""create simulated data."" We are instead, if I understand correctly, resampling from our actually observed data. This latter procedure is either a <strong>permutation test</strong> or <strong>nonparametric bootstrapping</strong>, depending on how the resampling takes place.</p>

<p>I guess I should say a few more words about parametric bootstrapping, permutation tests, and nonparametric bootstrapping.</p>

<p>Usually parametric bootstrapping is done by simulating based on the actually estimated model, and not based on a hypothetical model that is just like the estimated model except the null hypothesis is assumed true, as Ellis seems to suggest at first. By ""simulate data"" I mean something like as an example: my model states that my data come from two groups, each with a normal distribution, with means $\mu_1$ and $\mu_2$, respectively, and standard deviation $\sigma$, so I will generate many sets of data that satisfy this and use the distribution of test statistics computed from each of these simulated datasets as my sampling distribution. Note, I am creating this data using something like <code>rnorm()</code> in <code>R</code>, <em>not</em> directly using my observed data. Now, one could certainly do this procedure and get a sort of sampling distribution under the null hypothesis of, say, no difference in group means--we would just assume $\mu_1=\mu_2$ in all the simulated datasets, contrary to what we actually observed--and in this way we get a bootstrapped p-value (rather than a bootstrapped confidence interval, which is what the former/traditional method affords you). Again, I would just call this a way of obtaining a p-value via parametric bootstrapping.</p>

<p>A permutation test, on the other hand, involves shuffling your observed data over and over in a way that would be consistent with the null hypothesis. So for example, if the null hypothesis implies that group assignment makes no difference in terms of the group means, you can randomly shuffle the group labels among all your observations many many times and see what mean differences you would get for all possible ways of shuffling in this way. And then you would see where within the distribution of test statistics computed from these shuffled datasets does your actual observed statistic lie. Note that there is a finite (but usually large) number of ways that you can shuffle your actually observed data.</p>

<p>Finally, nonparametric bootstrapping is very similar to the permutation test, but we resample the observed data <em>with replacement</em> to try to get closer to an infinite ""population"" of values that our data might have been drawn from. There are many, many more ways to resample from your data with replacement than there are to shuffle your data (although it is technically finite in practice as well). Again, similar to parametric bootstrapping, this is usually done not under the null hypothesis, but under the model implied by the observed data, yielding confidence intervals around the observed test statistics, not p-values. But one could certainly imagine doing this under the null hypothesis like Ellis suggests and obtaining p-values in this way. As an example of nonparametric bootstrapping here (in the traditional fashion, i.e., <em>not</em> under the null hypothesis) using the same difference-in-group-means example I used in the parametric bootstrapping paragraph, to do this we would resample with replacement the observations within each group many times <em>but not mixing observations between groups</em> (unlike in the permutation test), and build up the sampling distribution of group mean differences that we get this way.</p>
",2013-11-01 21:44:34.163
58673,20320.0,1,,,,Moving average filter for outlier removal,<time-series><references><outliers><filter>,CC BY-SA 3.0,"<p>I am using a moving average filter to smooth data for outlier removal. By changing the number of average points, I am getting different result. </p>

<p>My data are multi-dimensional feature vectors. </p>

<p>I applied the moving average to the entire matrix and then on individual variables. </p>

<p>They give different results. </p>

<p>So, how to choose/guess the number of points to average over and should it be applied on the entire matrix or on a one by one basis?</p>
",2013-11-01 21:44:37.660
58674,503.0,2,,58673.0,,,,CC BY-SA 3.0,"<p>Neither. Both. All. </p>

<p>Sorry. But I think this is another attempt (albeit a clever one) to automate what can't really be automated. Of course different methods give different results; the only times they wouldn't is where the outlier is so obvious that you don't need a test. </p>

<p>My suggestion is to use a variety of methods to identify <em>possible</em> outliers, then examine those outliers on an individual basis. </p>
",2013-11-01 21:49:31.433
58675,15717.0,1,,,,Price elasticity of demand and time lags,<time-series>,CC BY-SA 3.0,"<p>This is probably a very basic question so apologies in advance, but we are stuck. </p>

<p>We currently change prices by varying amounts (both positively and negatively)every 3 months,and would like to calculate the price elasticity of demand. </p>

<p>Questions:</p>

<p>1) Should we eliminate a period of time immediately after the price change to allow time for people to adjust behavior? </p>

<p>2)Should we include the entire time between quarters (i.e. the three months before the price change (so everything right after the previous price change all the way up to the current price change) and the 3 months after the price change (so everything after the current price change all the way up to the next price change)?</p>

<p>Any help would be greatly appreciated!!!!</p>
",2013-11-01 22:35:28.023
58676,20949.0,1,,,,"Tried to overfit a Bayes net, but mean prediction error is worse than learned network?",<prediction><error><bayesian-network>,CC BY-SA 3.0,"<p>I have variables A, B, C, D, and E. I am interested in building a classifier for A. </p>

<p>I learned a Bayes net structure from the data using greedy search and BIC as a score. Call this network 1. Using cross validation, I got the mean prediction error for node A in network 1.</p>

<p>I then, thought to create a network structure where all the arcs are incoming into A (B->A, C->A, D->A, E->A). Call this network 2.</p>

<p>I also learned a third network that was constrained by having all the arcs in network 2, but could allow additional arcs between B, C, D, and E.  Call this network 3.</p>

<p>It seemed to me that the network 2 and network 3 would be better at predicting A because I was forcing all the info in the data as a direct input into predicting A -- i.e. overfitting. So I expected the mean prediction errors (MPE) for these networks to be less than the first network. But instead they were higher.</p>

<p>Also, when I repeated the cross validation several times, the mean prediction error for network 1 was always the same. But the value varied for networks 2 and 3.</p>

<p>Why would networks 2 and 3 have a higher MPE, and why would MPE for those networks be variable while not variable for network 1?</p>
",2013-11-01 22:48:23.687
58677,20949.0,2,,32388.0,,,,CC BY-SA 3.0,"<p>Actually, the structure you learned and the structure you proposed are ""Markov equivalent"".  This basically means you can use Bayes theorem to go from one to another.</p>

<p>Proof</p>

<pre><code>P(bid)P(won|bid)P(sold|won) = 
P(won, bid) P(won, sold)/P(won) = 
P(bid|won)P(won)P(won|sold)P(sold)/P(won) =
P(bid|won)P(won|sold)P(sold) 
</code></pre>

<p>which is what you want.</p>

<p>To show this in R, try this</p>

<pre><code>bn.2 &lt;- empty.graph(nodes(bn.hc))
arcs(bn.2) &lt;- matrix(c(""bid"", ""won"", ""won"", ""sold""), ncol = 2, byrow = T)
score(bn.hc, dat) == score(bn.2, dat)
&gt; TRUE
</code></pre>

<p>Also checkout ?cpdag</p>

<p>Basically, you can change the direction of any arc in the network and get a graph of the same equivalence class, so long as that arc is not in a v-structure. </p>
",2013-11-01 23:13:01.773
58678,22564.0,1,,,,Is randomization reliable with small samples?,<small-sample><random-allocation>,CC BY-SA 3.0,"<p>Jerome Cornfield has written:</p>

<blockquote>
  <p>One of the finest fruits of the Fisherian revolution was the idea of
  randomization, and statisticians who agree on few other things have at
  least agreed on this. But despite this agreement and despite the
  widespread use of randomized allocation procedures in clinical and in
  other forms of experimentation, its logical status, i.e., the exact
  function it performs, is still obscure.</p>
</blockquote>

<p>Cornfield, Jerome (1976). <a href=""http://www.epidemiology.ch/history/PDF%20bg/Cornfield%20J%201976%20recent%20methodological%20contributions.pdf"" rel=""nofollow noreferrer"">""Recent Methodological Contributions to Clinical Trials""</a>. American Journal of Epidemiology 104 (4): 408â€“421.</p>

<p>Throughout this site and in a variety of literature I consistently see confident claims about the powers of randomization. Strong terminology such as ""it <strong>eliminates</strong> the issue of confounding variables"" are common. See <a href=""https://stats.stackexchange.com/questions/32941/examples-of-lurking-variable-and-influential-observation"">here</a>, for example. However, many times experiments are run with small samples (3-10 samples per group) for practical/ethical reasons. This is very common in preclinical research using animals and cell cultures and the researchers commonly report p values in support of their conclusions.</p>

<p>This got me wondering, how good is randomization at balancing confounds. For this plot I modeled a situation comparing treatment and control groups with one confound that could take on two values with 50/50 chance (eg type1/type2, male/female). It shows the distribution of ""% Unbalanced"" (Difference in # of type1 between treatment and control samples divided by sample size) for studies of a variety of small sample sizes. The red lines and right side axes show the ecdf.</p>

<p><strong>Probability of various degrees of balance under randomization for small sample sizes:</strong>
<img src=""https://i.stack.imgur.com/H3n20.png"" alt=""enter image description here""></p>

<p>Two things are clear from this plot (unless I messed up somewhere).</p>

<p>1) The probability of getting exactly balanced samples decreases as sample size is increased.</p>

<p>2) The probability of getting a very unbalanced sample decreases as sample size increases.</p>

<p>3) In the case of n=3 for both groups, there is a 3% chance of getting a completely unbalanced set of groups (all type1 in the control, all type2 in the treatment). N=3 is common for molecular biology experiments (eg  measure mRNA with PCR, or proteins with western blot)</p>

<p>When I examined the n=3 case further, I observed strange behaviour of the p values under these conditions. The left side shows the overall distribution of pvalues calculating using t-tests under conditions of different means for the type2 subgroup. The mean for type1 was 0, and sd=1 for both groups. The right panels show the corresponding false positive rates for nominal ""significance cutoffs"" from .05 to.0001.</p>

<p><strong>Distribution of p-values for n=3 with two subgroups and different means of the second subgroup when compared via t test (10000 monte carlo runs):</strong>
<img src=""https://i.stack.imgur.com/PBxSw.png"" alt=""enter image description here""></p>

<p><strong>Here are the results for n=4 for both groups:</strong>
<img src=""https://i.stack.imgur.com/yAxux.png"" alt=""enter image description here""></p>

<p><strong>For n=5 for both groups:</strong>
<img src=""https://i.stack.imgur.com/MbM4w.png"" alt=""enter image description here""></p>

<p><strong>For n=10 for both groups:</strong>
<img src=""https://i.stack.imgur.com/9Dt0R.png"" alt=""enter image description here""></p>

<p>As can be seen from the charts above there appears to be an interaction between sample size and difference between subgroups that results in a variety of p-value distributions under the null hypothesis that are not uniform.</p>

<p>So can we conclude that p-values are not reliable for properly randomized and controlled experiments with small sample size?</p>

<p><strong>R code for first plot</strong></p>

<pre><code>require(gtools)

#pdf(""sim.pdf"")
par(mfrow=c(4,2))
for(n in c(3,4,5,6,7,8,9,10)){
  #n&lt;-3
  p&lt;-permutations(2, n, repeats.allowed=T)

  #a&lt;-p[-which(duplicated(rowSums(p))==T),]
  #b&lt;-p[-which(duplicated(rowSums(p))==T),]

  a&lt;-p
  b&lt;-p

  cnts=matrix(nrow=nrow(a))
  for(i in 1:nrow(a)){
    cnts[i]&lt;-length(which(a[i,]==1))
  }


  d=matrix(nrow=nrow(cnts)^2)
  c&lt;-1
  for(j in 1:nrow(cnts)){
    for(i in 1:nrow(cnts)){
      d[c]&lt;-cnts[j]-cnts[i]
      c&lt;-c+1
    }
  }
  d&lt;-100*abs(d)/n

  perc&lt;-round(100*length(which(d&lt;=50))/length(d),2)

  hist(d, freq=F, col=""Grey"", breaks=seq(0,100,by=1), xlab=""% Unbalanced"",
       ylim=c(0,.4), main=c(paste(""n="",n))
  )
  axis(side=4, at=seq(0,.4,by=.4*.25),labels=seq(0,1,,by=.25), pos=101)
  segments(0,seq(0,.4,by=.1),100,seq(0,.4,by=.1))
  lines(seq(1,100,by=1),.4*cumsum(hist(d, plot=F, breaks=seq(0,100,by=1))$density),
        col=""Red"", lwd=2)

}
</code></pre>

<p><strong>R code for plots 2-5</strong></p>

<pre><code>for(samp.size in c(6,8,10,20)){
  dev.new()
  par(mfrow=c(4,2))
  for(mean2 in c(2,3,10,100)){
    p.out=matrix(nrow=10000)

    for(i in 1:10000){

      d=NULL
      #samp.size&lt;-20
      for(n in 1:samp.size){
        s&lt;-rbinom(1,1,.5)
        if(s==1){
          d&lt;-rbind(d,rnorm(1,0,1))
        }else{
          d&lt;-rbind(d,rnorm(1,mean2,1))
        }
      }

      p&lt;-t.test(d[1:(samp.size/2)],d[(1+ samp.size/2):samp.size], var.equal=T)$p.value

      p.out[i]&lt;-p
    }


    hist(p.out, main=c(paste(""Sample Size="",samp.size/2),
                       paste( ""% &lt;0.05 ="", round(100*length(which(p.out&lt;0.05))/length(p.out),2)),
                       paste(""Mean2="",mean2)
    ), breaks=seq(0,1,by=.05), col=""Grey"", freq=F
    )

    out=NULL
    alpha&lt;-.05
    while(alpha &gt;.0001){

      out&lt;-rbind(out,cbind(alpha,length(which(p.out&lt;alpha))/length(p.out)))
      alpha&lt;-alpha-.0001
    }

    par(mar=c(5.1,4.1,1.1,2.1))
    plot(out, ylim=c(0,max(.05,out[,2])),
         xlab=""Nominal alpha"", ylab=""False Postive Rate""
    )
    par(mar=c(5.1,4.1,4.1,2.1))
  }

}
#dev.off()
</code></pre>
",2013-11-01 23:24:19.330
58690,23274.0,1,,,,Interaction of variables in regression,<regression><interaction>,CC BY-SA 3.0,"<p>I have two variables that predict fraud behavior (dependent variable). The independent variables are perception of fraud being wrong (1-5) and probability of being caught (1-5). The dependent variable is frequency of committing fraud in the last 5 years (never, once, 2-3 times, 4 times and more). 
Two questions: </p>

<ol>
<li>What kind of regression should I use? Ordinal?</li>
<li>Theory predicts that interaction of these two variables predicts the fraud. For those who perceive it wrong, the probability of being caught affects differently than for those who perceive it right. How should I enter this to the model?</li>
</ol>
",2013-11-02 08:55:19.447
58680,1145.0,2,,58678.0,,,,CC BY-SA 3.0,"<p>You are correct to point out the limitations of randomisation in dealing with unknown confounding variables for very small samples. However, the problem is not that the P-values are not reliable, but that their meaning varies with sample size and with the relationship between the assumptions of the method and the actual properties of the populations.</p>

<p>My take on your results is that the P-values performed quite well until the difference in the subgroup means was so large that any sensible experimenter would know that there was an issue prior to doing the experiment. </p>

<p>The idea that an experiment can be done and analysed without reference to a proper understanding of the nature of the data is mistaken. Before analysing a small dataset you must know enough about the data to be able to confidently defend the assumptions implicit in the analysis. Such knowledge commonly comes from prior studies using the same or similar system, studies that can be formal published works or informal 'preliminary' experiments.</p>
",2013-11-02 00:34:35.583
58681,3993.0,2,,58296.0,,,,CC BY-SA 3.0,"<p>Here is another geometric view of suppression, but rather than being in the <em>observation space</em> as @ttnphns's example is, this one is in the <em>variable space</em>, the space where everyday scatterplots live.</p>

<p>Consider a regression $\hat{y}_i=x_i+z_i$, that is, the intercept is 0 and both predictors have a partial slope of 1. Now, the predictors $x$ and $z$ may themselves be correlated. We will consider two cases: first the case where $x$ and $z$ are positively correlated, which I will call the ""confounding"" case (characterized by the secondary regression $\hat{x}_i=\frac{1}{2}z_i$), and second the case where $x$ and $z$ are negatively correlated, which I will call the ""suppression"" case (with secondary regression $\hat{x}_i=-\frac{1}{2}z_i$).</p>

<p>We can plot our regression equation as a plane in the variable space that looks like this:</p>

<p><img src=""https://i.stack.imgur.com/SrKCR.png"" alt=""plane""></p>

<h1>Confounding case</h1>

<p>Let's consider the slope for the $x$ predictor in the confounding case. To say that the other predictor $z$ is serving as a confounding variable is to say that when we look at a simple regression of $y$ on $x$, the effect of $x$ here is <em>stronger</em> than is the effect of x in a multiple regression of $y$ on $x$ and $z$, where we partial out the effect of $z$. The effect of $x$ that we observe in the simple regression is, in some sense (not necessarily causal), partially due to the effect of $z$, which is positively associated with both $y$ and $x$, but not included in the regression. (For the purposes of this answer I will use ""the effect of $x$"" to refer to the slope of $x$.)</p>

<p>We will call the slope of $x$ in the simple linear regression the ""simple slope"" of $x$ and the slope of $x$ in the multiple regression the ""partial slope"" of $x$. Here is what the simple and partial slopes of $x$ look like as vectors on the regression plane:</p>

<p><img src=""https://i.stack.imgur.com/3pIks.png"" alt=""confounding""></p>

<p>The partial slope of x is perhaps easier to understand. It is shown in red above. It is the slope of a vector that moves along the plane in such a way that $x$ is increasing, but $z$ is held constant. This is what it means to ""control for"" $z$.</p>

<p>The simple slope of $x$ is slightly more complicated because it implicitly also includes part of the effect of the $z$ predictor. It is shown in blue above. The simple slope of $x$ is the slope of a vector that moves along the plane in such a way that $x$ is increasing, and <em>$z$ also is increasing (or decreasing) to whatever extent $x$ and $z$ are associated in the dataset</em>. In the confounding case, we set things up so that the relationship between $x$ and $z$ was such that when we move up one unit on $x$, we also move up half a unit on $z$ (this comes from the secondary regression $\hat{x}_i=\frac{1}{2}z_i$). And since one-unit changes in both $x$ and $z$ are separately associated with one-unit changes in $y$, this means that the simple slope of $x$ in this case will be $\Delta x + \Delta z = 1 + \frac{1}{2} = 1.5$.</p>

<p>So when we control for $z$ in the multiple regression, the effect of $x$ appears to be smaller than it was in the simple regression. We can see this visually above in the fact that the red vector (representing the partial slope) is <em>less steep</em> than the blue vector (representing the simple slope). The blue vector is really the result of adding two vectors, the red vector and another vector (not shown) representing the <em>half</em> the partial slope of $z$.</p>

<p>Okay, now we turn to the slope for the $x$ predictor in the suppression case. If you followed all of the above, this is a really easy extension.</p>

<h1>Suppression case</h1>

<p>To say that the other predictor $z$ is serving as a supressor variable is to say that when we look at a simple regression of $y$ on $x$, the effect of $x$ here is <em>weaker</em> than is the effect of x in a multiple regression of $y$ on $x$ and $z$, where we partial out the effect of $z$. (Note that in extreme cases, the effect of $x$ in the multiple regression might even <em>flip directions!</em> But I am not considering that extreme case here.) The intution behind the terminology is that it appears that in the simple regression case, the effect of $x$ was being ""suppressed"" by the omitted $z$ variable. And when we include $z$ in the regression, the effect of $x$ emerges clearly for us to see, where we couldn't see it as clearly before. Here is what the simple and partial slopes of $x$ look like as vectors on the regression plane in the suppression case:</p>

<p><img src=""https://i.stack.imgur.com/NzWsa.png"" alt=""suppression""></p>

<p>So when we control for $z$ in the multiple regression, the effect of $x$ appears to <em>increase</em> relative to what it was in the simple regression. We can see this visually above in the fact that the red vector (representing the partial slope) is <em>steeper</em> than the blue vector (representing the simple slope). In this case the secondary regression was $\hat{x}_i=-\frac{1}{2}z_i$, so a one-unit increase in $x$ is associated with a half-unit <em>decrease</em> in $z$, which in turn leads to a half-unit decrease in $y$. So ultimately the simple slope of $x$ in this case will be $\Delta x + \Delta z = 1 + -\frac{1}{2} = 0.5$. As before, the blue vector is really the result of adding two vectors, the red vector and another vector (not shown) representing <em>half</em> of the <em>reverse</em> of the partial slope of $z$.</p>

<h1>Illustrative datasets</h1>

<p>In case you want to play around with these examples, here is some R code for generating data conforming to the example values and running the various regressions.</p>

<pre><code>library(MASS) # for mvrnorm()
set.seed(7310383)

# confounding case --------------------------------------------------------

mat &lt;- rbind(c(5,1.5,1.5),
             c(1.5,1,.5),
             c(1.5,.5,1))
dat &lt;- data.frame(mvrnorm(n=50, mu=numeric(3), empirical=T, Sigma=mat))
names(dat) &lt;- c(""y"",""x"",""z"")

cor(dat)
#           y         x         z
# y 1.0000000 0.6708204 0.6708204
# x 0.6708204 1.0000000 0.5000000
# z 0.6708204 0.5000000 1.0000000

lm(y ~ x, data=dat)
# 
# Call:
#   lm(formula = y ~ x, data = dat)
# 
# Coefficients:
#   (Intercept)            x  
#     -1.57e-17     1.50e+00  

lm(y ~ x + z, data=dat)
# 
# Call:
#   lm(formula = y ~ x + z, data = dat)
# 
# Coefficients:
#   (Intercept)            x            z  
#      3.14e-17     1.00e+00     1.00e+00  
# @ttnphns comment: for x, zero-order r = .671 &gt; part r = .387
#                   for z, zero-order r = .671 &gt; part r = .387

lm(x ~ z, data=dat)
# 
# Call:
#   lm(formula = x ~ z, data = dat)
# 
# Coefficients:
#   (Intercept)            z  
#     6.973e-33    5.000e-01 

# suppression case --------------------------------------------------------

mat &lt;- rbind(c(2,.5,.5),
             c(.5,1,-.5),
             c(.5,-.5,1))
dat &lt;- data.frame(mvrnorm(n=50, mu=numeric(3), empirical=T, Sigma=mat))
names(dat) &lt;- c(""y"",""x"",""z"")

cor(dat)
#           y          x          z
# y 1.0000000  0.3535534  0.3535534
# x 0.3535534  1.0000000 -0.5000000
# z 0.3535534 -0.5000000  1.0000000

lm(y ~ x, data=dat)
# 
# Call:
#   lm(formula = y ~ x, data = dat)
# 
# Coefficients:
#   (Intercept)            x  
#    -4.318e-17    5.000e-01  

lm(y ~ x + z, data=dat)
# 
# Call:
#   lm(formula = y ~ x + z, data = dat)
# 
# Coefficients:
#   (Intercept)            x            z  
#    -3.925e-17    1.000e+00    1.000e+00  
# @ttnphns comment: for x, zero-order r = .354 &lt; part r = .612
#                   for z, zero-order r = .354 &lt; part r = .612

lm(x ~ z, data=dat)
# 
# Call:
#   lm(formula = x ~ z, data = dat)
# 
# Coefficients:
#   (Intercept)            z  
#      1.57e-17    -5.00e-01  
</code></pre>
",2013-11-02 01:00:20.150
58682,21840.0,1,,,,Find the Distribution,<distributions><self-study><density-function><convergence><mean>,CC BY-SA 3.0,"<p>Given $ Y_1, Y_2..Y_n$ are iid from a distribution with pmf,<br>
$f(y) = a^{2}$ for $y=0$, </p>

<p>$f(y) = 2a(1-a)$ for $y=1$ ,  </p>

<p>$f(y) = (1-a)^{2}$ for $y=2$,  where $0&lt;a&lt;1$.</p>

<p>For large n, calculate the approximate distribution of </p>

<p>a) $\sqrt {\bar{Y}}$   - Solution to part(a) posted as answer(awaiting confirmation)</p>

<p>b) $\sqrt n ({\bar{Y}-\mu)}+\bar Y^2$  , where $\mu=E(Y_1)$</p>

<p>Could you please verify my solution for part (b) :</p>

<p>By CLT
$\sqrt n ({\bar{Y}-\mu)} \rightarrow N(0,\sigma^2)$ (convergence in probability)</p>

<p>For $\bar Y^2$, applying delta method,
$\bar Y^2 \rightarrow N(\mu^2,\frac{4\mu^2\sigma^2}{n^2})$ (converges in distribution)</p>

<p><strong>{EDIT} - Can I say :</strong>
$\bar Y^2 \rightarrow \mu^2$ in probability</p>

<p>where $\sigma^2 = Var Y$ and $Var \bar Y^2 = \sigma^2/n$</p>

<p><strong>Can I apply slusky theorem</strong>, as one distribution converges in probability and other in distribution:</p>

<p>By Slutsky theorem , </p>

<p>$\sqrt n ({\bar{Y}-\mu)} + \bar Y^2 \rightarrow [\mu^2 + N(\mu^2,\frac{4\mu^2\sigma^2}{n^2})]$</p>

<p>Thanks!</p>
",2013-11-02 01:15:04.970
58683,20473.0,2,,58675.0,,,,CC BY-SA 3.0,"<p>Although you talk about ""calculation"", in order to justify the presence of your question in this forum I will treat your problem as one of ""statistical estimation"".  </p>

<p>Moreover, I understand that this is not a supply-demand interaction framework, since it seems that you change prices and then ""wait to see what happens"", fulfilling whatever demand appears. Given these preliminaries:</p>

<p>$1$) The fact that people may exhibit some degree of inertia is an integral part of their behavior, so trying to ""eliminate"" it is not useful since what you will then be estimating would perhaps be the ""true"" price elasticity of demand, but one which will never materialize, since inertia will always be present, and will affect the total demand response. More over, how would you determine the length of time to ignore?  </p>

<p>What you need is to obtain a separate estimate of <em>both</em> effects. This calls for a  specification like the following:</p>

<p>$$\frac {\Delta q_t}{q_{t-1}} = a\frac {\Delta q_{t-1}}{q_{t-2}} + b\frac {\Delta p_{t}}{p_{t-1}} + u_t $$</p>

<p>wher $a$ captures the degree of inertia, expected positive, and  $b$ is expected negative, while $u_t$ is assumed white noise. The inclusion of the error term, and the whole statistical approach permits to allow for other ""unpredictable"", ""uncontrollable"" factors that may affect demand in each quarter.<br>
The estimate $\hat b$ will be the estimated average price elasticity of demand, ""cleansed"" from any inertia effects and other ""random"" factors.  </p>

<p>$2$) Yes you should use as your time period the whole quarter. Hopefully you have data for many quarters.</p>
",2013-11-02 02:11:40.757
58684,16464.0,2,,14729.0,,,,CC BY-SA 3.0,"<p>Not that the answer @Whuber gave really needs to be expanded on but I thought I'd provide a brief description of the math.</p>

<p>If the linear combination $\mathbf{X'Xv}=\mathbf{0}$ for $\mathbf{v}\neq\mathbf{0}$ then $\mathbf{v}$ is an eigenvector of $\mathbf{X'X}$ associated with eigenvalue $\lambda=0$. The eigenvectors and eigenvalues of $\mathbf{X'X}$ are also the eigenvectors and eigenvalues of $\mathbf{X}$, so the eigenvectors of $\mathbf{X'X}$ associated with eigenvalues near $\lambda=0$ represent the coefficients for approximate linear relationships among the regressors. Principal Component Analysis outputs the eigenvectors and eigenvalues of $\mathbf{X'X}$, so you can use the eigenvectors $\mathbf{v}$ associated with small $\lambda$ to determine if linear relationships exist among some of your regressors.</p>

<p>One method of determining if an eigenvalue is appropriately small to constitute collinearity is to use the Condition Indices:
$$
\mathbf{\kappa_j}=\frac{\lambda_{max}}{\lambda_j}
$$
which measures the size of the smallest eigenvalues relative to the largest. A general rule of thumb is that modest multicollinearity is associated with a condition index between 100 and 1,000 while severe multicollinearity is associated with a condition index above 1,000 (Montgomery, 2009). </p>

<p>It's important to use an appropriate method for determining if an eigenvalue is small because it's not the absolute size of the eigenvalues, it's the relative size of the condition index that's important, as can be seen in an example. Consider the matrix
$$
\mathbf{X'X}=\left[\begin{array}{rr}
0.001 &amp; 0 &amp; 0 \\
0 &amp; 0.001 &amp; 0 \\
0 &amp; 0 &amp; 0.001 \\
\end{array}
\right].
$$
The eigenvalues for this matrix are $\lambda_1=\lambda_2=\lambda_3=0.001$. Although these eigenvalues appear small the condition index is
$$
\mathbf{\kappa}=\frac{\lambda_{max}}{\lambda_{min}}=1
$$
indicating absence of multicolinearity and , in fact, the columns of this matrix are linearly independent.</p>

<p><strong>Citations</strong></p>

<p>Montgomery, D. (2012). Introduction to Linear Regression Analysis, 5th Edition. John Wiley &amp; Sons Inc.</p>
",2013-11-02 05:19:49.820
58685,23270.0,2,,58645.0,,,,CC BY-SA 3.0,"<p>I would propose Otsu (1979) or NG's (2006) threshold detection algorithms.  They were initially implemented to detect manufacturing defects by testing for significant thresholds in greyscale image pixel intensities however they are decent at detecting thresholds in any histogram.  They do so by identifying the threshold(s) that maximize the across class variance or minimize the within class variance of the data. Both papers are attached <a href=""https://www.dropbox.com/s/nqebqrg8o3x05nr/Archive.zip"" rel=""nofollow noreferrer"">here</a>.</p>

<p>NG's approach is a simple extension that adds what the author describes as a valley finding modification.  It simply multiplies the across class variance of any possible threshold by 1 minus the probability of an observation at the threshold. </p>

<p>The R function below will return the threshold calculated by each method and will produce a histogram like the one displayed showing the threshold if passed a single column data frame.  </p>

<pre><code>###
### NG and OTSU Thresholding of image intensity pixels.
###

threshold &lt;- function(observations, plot=TRUE, otsu=TRUE){
  # Ensure that observations is a data frame.
  observations        &lt;- as.data.frame(observations)
  names(observations) &lt;- c(""obs"")
  attach(observations)

  # Convert Observation List to Integer then back to Numeric (for histogram)   
  print(""Starting Thresholding Algorithm..."")

  observations$obs &lt;- as.integer(observations$obs)                            
  observations$obs &lt;- as.numeric(observations$obs)                            
  attach(observations)

  # Produce some image summary variables.
  # Distinct grey scale levels in image.
  levels &lt;- max(obs) - min(obs)
  print(paste(""Distribution has"", levels,"" levels...""))

  # Min and Max grey scale values in image.
  minThreshold &lt;- min(obs)
  maxThreshold &lt;- max(obs)-1

  # Number of pixels/voxels in image
  print(""Extracting frequencies for each level..."")
  obsCount                  &lt;- nrow(obs)
  frequencies               &lt;- hist(obs, breaks=levels, plot=FALSE)
  densityVector             &lt;- as.data.frame(frequencies$density)             #$
  intensity                 &lt;- (minThreshold:maxThreshold)
  intensityFrequency        &lt;- cbind(intensity, densityVector, 1-densityVector)
  names(intensityFrequency) &lt;- c(""intensity"", ""probability"", ""weight"")
  outputArray               &lt;- NULL
  outputArray               &lt;- as.data.frame(outputArray)

  # For every possible threshold value... 
  # Calculate two means, two variances.  
  # IntraClass Variance. And Spit out to an array with Intensity, 
  # and Probability of that intensity
  print(paste(""Testing all possible threshold values...""))
  for(i in minThreshold:maxThreshold){
    lowerClassArray &lt;- intensityFrequency[ which(intensity &lt;= i),]
    upperClassArray &lt;- intensityFrequency[ which(intensity &gt; i),]
    lowerClassProb  &lt;- sum(lowerClassArray$probability)                       #$
    upperClassProb  &lt;- sum(upperClassArray$probability)                       #$
    lowerClassArray &lt;- as.data.frame(lowerClassArray)
    upperClassArray &lt;- as.data.frame(upperClassArray)
    lowerClassArray$product &lt;- lowerClassArray$intensity *                    
                               lowerClassArray$probability / lowerClassProb   #$
    upperClassArray$product &lt;- upperClassArray$intensity *                    
                               upperClassArray$probability / upperClassProb   #$
    lowerMu             &lt;- sum(lowerClassArray$product)                       #$
    upperMu             &lt;- sum(upperClassArray$product)                       #$
    lowerWithinClassVar &lt;- lowerClassProb*(lowerMu^2)
    upperWithinClassVar &lt;- upperClassProb*(upperMu^2)
    betweenClassVar     &lt;- lowerWithinClassVar + upperWithinClassVar
    outputBuffer        &lt;- list(i, lowerClassProb, upperClassProb, lowerMu, 
                                upperMu, lowerWithinClassVar, upperWithinClassVar)
    print(paste(""Testing Threshold:"", i))

    outputArray &lt;- rbind(outputArray, outputBuffer)
  }
  names(outputArray)         &lt;- c(""intensity"", ""lowerClassProb"", ""upperClassProb"",
                                  ""lowerMu"", ""upperMu"", ""lowerWithinClassVar"",
                                  ""upperWithinClassVar"")
  outputArray$withinClassVar &lt;- outputArray$lowerWithinClassVar +             
                                outputArray$upperWithinClassVar               #$
  outputArray &lt;- merge(outputArray, intensityFrequency, by=c(""intensity""))
  print(paste(""Calculating inter and intra-class variances...""))
  outputArray$ngParam &lt;- outputArray$weight*outputArray$withinClassVar        #$
  attach(outputArray)
  sortedOutputNg      &lt;- outputArray[ order(-ngParam),]
  sortedOutputOtsu    &lt;- outputArray[ order(-withinClassVar),]
  paramNG             &lt;- sortedOutputNg[1,]
  paramOTSU           &lt;- sortedOutputOtsu[1,]
  paramNG             &lt;- as.integer(paramNG[1])
  paramOTSU           &lt;- as.integer(paramOTSU[1])
  print(paste(""Ng threshold:"", paramNG))

  if(otsu==TRUE){
    print(paste(""Otsu threshold:"", paramOTSU))  
  }

  if(plot==TRUE){
    #Histogram of pixel intensities.
    hist(observations$obs, xlab=""Score"", breaks = levels, 
         main=""Histogram of disease severity score frequencies"")

    #Throw on a vertical line to show what a threshold looks like...
    abline(v=paramNG, col='red', lty=4)
    print(""Ng threshold is represented by red line, Otsu by blue."")
    if(otsu==TRUE){
      abline(v=paramOTSU, col='blue', lty=3)
    }
  }
  resultArray &lt;- c(paramNG, paramOTSU)
  return(resultArray)
  detach(outputArray)
}
</code></pre>

<p><img src=""https://i.stack.imgur.com/Geazm.jpg"" alt=""Sample Plot""></p>
",2013-11-02 05:28:21.003
58686,22867.0,1,,,,Do Bayesians interpret the likelihood distribution as subjective as well?,<bayesian><frequentist>,CC BY-SA 3.0,"<p>One of the main differences between Bayesians and frequentists is that they have a subjective interpretation to probability.</p>

<p>However, do Bayesians actually interpret subjectively the probabilities attached to an outcome GIVEN a set of parameters (i.e. for the likelihood), or is it just that they attach a subjective probability to the prior, and also to the posterior as a consequence? (but $p(x | \theta)$ is thought similarly to the way frequentists think about it.)</p>
",2013-11-02 07:44:50.887
58687,23272.0,1,,,,Estimation of Missing Observations,<time-series>,CC BY-SA 3.0,"<p>I have a data monthly as well as daily data of no of patients and I want to employ time series model .How can I estimate missing counts.Any one would please guide me</p>
",2013-11-02 07:49:31.200
58688,21728.0,1,,,,Probability Generating Function of Poisson Distribution,<probability><poisson-distribution>,CC BY-SA 3.0,"<p>I was just wondering if someone could help me understand this derivation of the probability generating function for a Poisson distribution, (I understand it, until the last step):</p>

<p>$$\pi(s)=\sum^{\infty}_{i=0}e^{-\lambda}\frac{\lambda^i}{i!}s^i$$
$$\pi(s)=e^{-\lambda}\sum^{\infty}_{i=0}\frac{e^{\lambda s}}{e^{\lambda s}}\frac{(\lambda s)^i}{i!}$$
$$= e^{-\lambda}e^{\lambda s} $$</p>

<p>This is a re-production from some lecture notes, but I'm not sure how it jumps from the 2nd last step to the last step?</p>

<p>If someone can show me the intermediate steps I would be very grateful!! </p>
",2013-11-02 08:06:59.833
58689,23242.0,1,58691.0,,,Show that $T=\min\{n:X_{n}\in B\}$ is an $\mathcal F_{n}$-stopping time,<probability><self-study><stochastic-processes><marginal-distribution>,CC BY-SA 3.0,"<p>Let $X_{n}$ be an $\mathcal F_{n}$-martingale and let $B\in \mathcal B$.<br>
Show that $T=\min\{n:X_{n}\in B\}$ is an $\mathcal F_{n}$-stopping time.<br>
$\mathcal B$ is Borel $\sigma$-algebra and filtration is $\mathcal F=\sigma(X_{1},\dots,X_{n})$.
Thanks for help.</p>
",2013-11-02 08:27:20.307
58700,15827.0,2,,58697.0,,,,CC BY-SA 3.0,"<p>Without context, all that can be said is to collect some common usages: </p>

<ol>
<li><p>An indicator wide sense is anything that indicates (i.e. is considered revealing or diagnostic), which could be any counted or measured variable. Consider statements such as that unemployment rate, GDP growth and government debt are good indicators of the state of an economy. In contrast, an indicator precise sense indicates the state of a binary or dichotomous variable (e.g. survived vs not, alive vs dead, present vs absent, male vs female) by 1 or 0. Often, which state is coded 1 and which is 0 is a matter of convention or convenience, as in the case of male or female. Indicator variables are perhaps more often called dummy variables in several fields of statistical science, although that term on occasion has been misread as offensive. </p></li>
<li><p>An index is sometimes a scaled composite variable: for example, a price index or wages index is based on a weighted sum of prices or wages, often scaled so it is 100 in some base year or at some base time. An index is also sometimes any kind of summary measure designed to capture some property (segregation, diversity, whatever) in a single number. Ideally, it is recognised whenever one index is used that others are also possible.</p></li>
<li><p>A variable varies, or at least in principle might vary. It is difficult to say more if that is unclear, except by contrast: anything defined to be constant cannot be a variable. Statistics makes enormous play, however, with parameters which are constant in a given situation but variable from situation to situation. So, mean temperature and the slope of a regression line are parameters. </p></li>
<li><p>A measure may or should be something with a strict definition in measure theory (<a href=""http://en.wikipedia.org/wiki/Measure_theory"" rel=""nofollow"">http://en.wikipedia.org/wiki/Measure_theory</a>), but either you know that already or it is irrelevant to you. At elementary or introductory or lay level, it might just be yet another word for anything you (count or) measure (e.g. one measure of diversity in ecology is just the number of species present at a site). </p></li>
</ol>

<p>I don't think any of these terms is really difficult -- at the level I have chosen here -- and any good dictionary should provide authority if you need it. That said, there are numerous formal accounts or theories of measurement, which do not all agree on terminology or definitions. </p>
",2013-11-02 11:31:13.207
58701,503.0,2,,58690.0,,,,CC BY-SA 3.0,"<p>Regarding 1. Ordinal logistic is the first tool that springs to mind. If you have a good sense of the distribution of the people who committed fraud 4 + times per year, then other options may be available, but ordinal logistic looks good.</p>

<ol>
<li>The usual way to enter an interaction is by multiplying the two variables. Whether that is appropriate here depends on what 1-5 mean. If these are anchored by proportions (e.g 1 = 0%, 2 = 20% or whatever) then multiplying may be possible. If you treat them as categorical you will have a lot of terms.  How big is your data set? </li>
</ol>
",2013-11-02 11:35:52.133
58702,13202.0,1,58884.0,,,PCA and PLS: testing variables for significance,<statistical-significance><pca><partial-least-squares>,CC BY-SA 3.0,"<p>I'm trying to understand the process for statistical testing for principal component analysis or partial least squares.</p>

<p><strong>Step 1. PCA:</strong>
I feel that I have a not-terrible understanding of PCA: You find the ellipsoid described by the covariance matrix of the data, and then successively take the largest axis of variation (principal component 1), then the second largest (principal component 2), and so on. If the ellipsoid is long and stretched, then the variation is mostly along the first principal component (the eigenvector corresponding to the largest eigenvalue of the ellipsoid). If the ellipsoid is a planar ""disc"", then the variation in the data is explained well by two principal components, etc.</p>

<p>I also understand that after choosing to use (for example) only the first two principal components, then all of the data points can be plotted on a ""Scores"" plot that shows, for each data point $D^{(i)}$, the projection of $D^{(i)}$ into the plane spanned by the first two principal components. Likewise, for the ""Loadings"" plot (I think) you write the first and second principal components as linear combinations of the input variables and then for each variable, plot the coefficients that it contributes to the first and second principal components.</p>

<p><strong>Step 2. PLS or PLS-DA:</strong>
If there are labels on the data (let's say binary classes), then build a linear regression model to use the first and second principal components to discriminate class 0 (for data point $i$, that means $Y^{(i)}=0$) from class 1 (for data point $i$, that means $Y^{(i)}=1$) by first projecting all data to only lie in the plane spanned by the first and second principal components, and then regressing the projected input data $X_1', X_2'$ to $Y$. This regression could be written as (first step) the affine transformation (i.e. linear transformation + bias) that projects along $PC_1, PC_2$ (the first and second principal components), and then (second step) a second affine transformation that predicts $Y$ from $PC_1, PC_2$. Together these transformations $Y \approx Affine(Affine(X))$ can be written as a single affine transformation $Y \approx C (A X + B) + D = E X + F$. </p>

<p><strong>Step 3. Testing variables from $X$ for significance in predicting the class $Y$:</strong>
This is where I could use some help (unless I'm way off already, in which case tell me!). How do you test whether an input variable (i.e. a feature that has not yet been projected onto the principal components (hyper)plane), and decide if it has a statistically significant coefficient in the regression $Y \approx E X + F$? Qualitatively, a coefficient in $E$ that is further from zero (i.e. positive and negative values with large magnitude) indicates a larger contribution from that variable.</p>

<p>I remember seeing linear regression t-tests for normally distributed data (to test whether the coefficients were zero). Is this the standard approach? In that case, I would guess that ever variable from $X$ has been transformed to have a roughly normal distribution in Step 0 (i.e. before any of these other steps are performed).</p>

<p>Otherwise, I could see performing a permutation test (by running this entire procedure thousands of times and each time permuting $Y$ to shuffle the labels, and then comparing each single coefficient in $E$ from the un-shuffled analysis to the distribution of coefficients from shuffled analyses).</p>

<p>Can you help me see anywhere my intuition is failing? I've been trying to look through papers using similar procedures to see what they did, and as is often the case, they're clear as mud. I'm preparing a tutorial for some other researchers, and I want to do a good job. </p>
",2013-11-02 11:55:32.803
58703,594.0,2,,58598.0,,,,CC BY-SA 3.0,"<p>One general approach which you can adapt for part (a):</p>

<p>Consider a random variable $X$ and a constant, $c$, where $X\sim f_X(x), a\leq x\leq b$, say. </p>

<p>Let $Z=X+c$.</p>

<p>$P(Z\leq z) = P(X+c\leq z) = P(X\leq z-c) = F_X(z-c); a\leq z-c\leq b$.</p>

<p>You just need to get the pdf from the cdf and get the limits sorted out.</p>

<hr>

<p>Note that you don't need to integrate anything to answer (a) unless you don't know the cdf for an exponential ($F_X(x)=1âˆ’e^{âˆ’Î»x};x&gt;0,Î»&gt;0$). </p>

<p>If you know it, you just start by writing down the cdf; then $P(Xâ‰¤zâˆ’c)$ simply requires substitution, not integration. So it's nothing more than 'substitute then differentiate'... but you must take care with the limits on the random variable, since $Z$ is clearly defined over a different set of values than $X$ is; what's needed there is clear from the last part of the line of algebra in my question.  </p>

<p>In part (b), note that $nÏ„$ is just a constant. It's the same problem as before (a shifted distribution), but with a gamma instead of an exponential; the hint in my answer above is general enough to work for both. </p>

<p>The only new trick is finding the MGF, which should involve a straightforward manipulation of the integral for the MGF to get something times a Gamma MGF.</p>

<p>... and (b)(ii) simply involves recognizing the relationship between parts (a) and (b).</p>
",2013-11-02 12:47:43.727
58704,23283.0,1,,,,Mapping a value in a normal distribution,<normal-distribution>,CC BY-SA 3.0,"<p>I apologize in advance for misuse of terminology.</p>

<p>If given a number $x$ from a normal distribution with mean 0 and standard deviation 1, how can I map that onto another distribution with a different mean $m$ and standard deviation $s$?</p>

<p>Is it simply $m + s\,x$ ?</p>

<p>Clarification - the real-world problem I'm trying to solve:
I would like to generate random numbers in a normal distribution for which I know the mean and standard deviation, however, I can only generate random floating point numbers in a distribution with mean 0.0 and stddev 1.0. How can I translate or ""map"" this value onto a different distribution?</p>
",2013-11-02 14:01:34.637
58705,23285.0,1,,,,how to Normalize data(with noise) into 0-1 range good scale in mean and variance?,<variance><data-transformation><mean><normalization><matrix>,CC BY-SA 3.0,"<p>i have a matrix data. Perhaps some data in one cluster and another in some cluster.
data scale is between [0-1000](just example). and i want to normalize into [0-1] and good in mean and variance. it means, i want to have a Good visibility in mean and variance.
for more description,mean and variance Be chosen in perfect place that Consider all data, and not ignore noisy data. 
Foe example one way is zi=xiâˆ’min(x)max(x)âˆ’min(x), but this way if Most of the data in one cluster and few data in another cluster, mean and variance be chosen between large cluster and not attend to noisy data.<br>
Please introduce me a paper(if newest is better) or a way to solve this problem.
Thank u.</p>
",2013-11-02 14:23:26.503
58706,17740.0,2,,58435.0,,,,CC BY-SA 3.0,"<p>This is one of the generalizations of classification that are tackled in semi-supervised learning. If you have a measurement of certainty you can use approaches that allow weighting of training instances. The higher the certainty, the larger the corresponding instance weight. Examples of such approaches include instance-weighted SVM and logistic regression. </p>

<p>I'm sure weka has implementations of these algorithms. If all else fails, sample multiple instances from the instances with high certainty. You can use this approach for traditional SVM or LR.</p>

<h3>Example: SVM</h3>

<p>If I am not mistaken, weka has interfaces to <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"">LIBSVM</a>. LIBSVM allows you to solve class-weighted SVM in all its releases, and instance-weighted SVM in special versions of each release. I'm going to assume weka does not support the latter (which is what you need).</p>

<p>Class weighted SVM minimizes the following objective function:
$$
\min_{\mathbf{w},\xi} \|\mathbf{w}\|^2 + {\color{blue}C_{pos}} \sum_{i \in \mathcal{P}} \xi_i + {\color{blue}C_{neg}} \sum_{i \in \mathcal{N}} \xi_i,
$$
with $\mathbf{w}$ the separating hyperplane in feature space, $\xi$ the slack variables (which model training misclassification) and $\mathcal{P}$ and $\mathcal{N}$ the set of support vectors belonging to the positive and negative class, respectively. Using the weights $C_{pos}$ and $C_{neg}$ you can assign different misclassification penalties between classes.</p>

<p>Based on your question, it seems like you would ideally want to use 6 different weights (2 classes $\times$ 3 levels of certainty). You can achieve this for many approaches by duplicating samples of the points with high certainty. </p>

<p>For example, in terms of SVM, using the same data instance twice yields an identical solution to doubling its associated $C$ value. This is a very easy way to assign high misclassification penalties to certain data instances. You can follow the same approach for logistic regression.</p>
",2013-11-02 15:13:50.210
58707,23129.0,2,,16313.0,,,,CC BY-SA 3.0,"<p>Here's a quote from Andrew Gilpin (1993) advocating  Maurice Kendall's $Ï„$ over Spearman's $Ï$ for theoretical reasons: </p>

<blockquote>
  <p>[Kendall's $Ï„$] approaches a normal distribution more rapidly than $Ï$, as $N$, the sample size, increases; and $Ï„$ is also more tractable mathematically, particularly when ties are present.  </p>
</blockquote>

<p>I can't add much about Goodman-Kruskal $Î³$, other than that it seems to produce ever-so-slightly larger estimates than Kendall's $Ï„$ in a sample of survey data I've been working with lately... and of course, noticeably lower estimates than Spearman's $Ï$.  However, I also tried calculating a couple partial $Î³$ estimates (Foraita &amp; Sobotka, 2012), and those came out closer to the partial $Ï$ than the partial $Ï„$... It took a fair amount of processing time though, so I'll leave the simulation tests or mathematical comparisons to someone else... (who would know how to do them...)</p>

<p>As <a href=""https://stats.stackexchange.com/users/3277/ttnphns"">ttnphns</a> implies, you can't conclude that your $Ï$ estimates are better than your $Ï„$ estimates by magnitude alone, because their scales differ (even though the limits don't).  Gilpin cites Kendall (1962) as describing the ratio of $Ï$ to $Ï„$ to be roughly 1.5 over most of the range of values.  They get closer gradually as their magnitudes increase, so as both approach 1 (or -1), the difference becomes infinitesimal.  Gilpin gives a nice big table of equivalent values of $Ï$, $r$, $r^2$, <em>d</em>, and $Z_r$ out to the third digit for $Ï„$ at every increment of .01 across its range, just like you'd expect to see inside the cover of an intro stats textbook.  He based those values on Kendall's specific formulas, which are as follows:
$$
\begin{aligned}
r &amp;= \sin\bigg(\tau\cdot\frac \pi 2 \bigg)  \\
\rho &amp;= \frac 6 \pi \bigg(\tau\cdot\arcsin \bigg(\frac{\sin(\tau\cdot\frac \pi 2)} 2 \bigg)\bigg)
\end{aligned}
$$
(I simplified this formula for $Ï$ from the form in which Gilpin wrote, which was in terms of Pearson's $r$.)</p>

<p>Maybe it would make sense <strong>to convert your $Ï„$ into a $Ï$</strong> and see how the computational change affects your effect size estimate.  Seems that comparison would give some indication of the extent to which the problems that Spearman's $Ï$ is more sensitive to are present in your data, if at all.  More direct methods surely exist for identifying each specific problem individually; my suggestion would produce more of a quick-and-dirty omnibus effect size for those problems.  If there's no difference (after correcting for the difference in scale), then one might argue there's no need to look further for problems that only apply to $Ï$.  If there's a substantial difference, then it's probably time to break out the magnifying lens to determine what's responsible.</p>

<p>I'm not sure how people usually report effect sizes when using Kendall's $Ï„$ (to the unfortunately limited extent that people worry about reporting effect sizes in general), but since it seems likely that unfamiliar readers would try to interpret it on the scale of Pearson's $r$, it <strong>might be wise to report both</strong> your $Ï„$ statistic and its effect size on the scale of $r$ using the above conversion formula...or at least point out the difference in scale and give a shout out to Gilpin for his handy conversion table.</p>

<h3>References</h3>

<p>Foraita, R., &amp; Sobotka, F. (2012).  Validation of graphical models.  <em>gmvalid Package, v1.23.</em>  The Comprehensive R Archive Network.  URL: <a href=""http://cran.r-project.org/web/packages/gmvalid/gmvalid.pdf"" rel=""nofollow noreferrer"">http://cran.r-project.org/web/packages/gmvalid/gmvalid.pdf</a></p>

<p>Gilpin, A. R. (1993).  Table for conversion of Kendall's Tau to Spearman's Rho within the context measures of magnitude of effect for meta-analysis.  <em>Educational and Psychological Measurement, 53</em>(1), 87-92.</p>

<p>Kendall, M. G. (1962).  <em>Rank correlation methods</em> (3rd ed.).  London: Griffin.</p>
",2013-11-02 15:42:21.763
58722,2873.0,2,,58714.0,,,,CC BY-SA 3.0,"<p>In the case of the geometric distribution this will work since it (and the exponential for continuous variables) are memoryless meaning that the probability that x = k + s given that x>=k is just the probability that x = k.  This will not however work for any other distributions.</p>

<p>How are you generating the geometric to begin with? that may be what is slowing you down.</p>
",2013-11-02 19:46:15.890
58708,14919.0,1,58717.0,,,self-selection bias due to nonresponse?,<sample><bias><non-response>,CC BY-SA 3.0,"<p>I have administrative data from the whole population of new doctorates, in a given year, from my region.
We have also survey data from a sample of this same population (where the whole population was contacted to participate in the survey, but only around 65% of doctorates participated: this is the sample I should work on).
There will be self-selection bias if the non-response is not random.</p>

<p>I want to investigate possible wage gaps between different groups in my region (the dependent variable is the log of the ratio between two average wages).</p>

<p>Any inputs on how to tackle this issue? 
Literature on this issue?</p>
",2013-11-02 15:46:03.437
58709,23129.0,2,,3646.0,,,,CC BY-SA 3.0,"<p>Here's a quote from Andrew Gilpin (1993) advocating  Kendall's <em>Ï„</em> over Spearman's <em>Ï</em> for theoretical reasons:</p>
<blockquote>
<p>&quot;[Kendall's <span class=""math-container"">$Ï„$</span>] approaches a normal distribution more rapidly than <span class=""math-container"">$Ï$</span>, as <span class=""math-container"">$N$</span>, the sample size, increases; and <span class=""math-container"">$Ï„$</span> is also more tractable mathematically, particularly when ties are present.&quot;</p>
</blockquote>
<h3>Reference</h3>
<p>Gilpin, A. R. (1993).  Table for conversion of Kendall's Tau to Spearman's Rho within the context measures of magnitude of effect for meta-analysis.  <em>Educational and Psychological Measurement, 53</em>(1), 87-92.</p>
",2013-11-02 15:50:35.117
58710,23287.0,1,,,,Correct use of cross validation in LibsSVM,<machine-learning><classification><cross-validation><python><libsvm>,CC BY-SA 3.0,"<p>I am classifying data points from two different groups using <code>LibSVM</code>.</p>

<p>I do the following:</p>

<ol>
<li>Creating the input file for <code>LibSVM</code>. In the input file, I put all the data I have.</li>
<li>Scaling it (using <code>svm-scale</code>).</li>
<li>Using <code>grid.py</code> of <code>libSVM</code> for choosing <code>gamma</code> and <code>c</code> parameters. </li>
<li>Using <code>svm-train</code> for training. I use the entire dataset. I also use the <code>-v 10</code> option for 10-fold cross validation (<code>svm-train flag</code>).</li>
</ol>

<p><strong>My questions are</strong>: </p>

<p>a. Is the <code>-v 10</code> option of cross validation can replace the testing step?</p>

<p>b. The result given by the steps above is suspiciously high (96%), and so I'm wondering if I am doing something wrong? </p>

<p>c. Could the use of <code>grid.py</code> for parameter selection before the train + cross validation damage the results (as if I were testing on data I've already trained)?</p>
",2013-11-02 15:54:55.837
58711,13666.0,1,,,,Cleaning corpus,<machine-learning><classification><text-mining>,CC BY-SA 4.0,"<p>I have a classifier that I want to use on short documents and I need to improve quality of the corpus used for training.  I have 250,000 documents and manual inspection is expensive.</p>

<p>The collection of short documents has been pre-labelled by a collection of rules as good or bad.  The accuracy of this pre-processing step has been assessed by sampling to be around 98% or higher.  When I train the classifier on 80% of the corpus and test its accuracy on the remaining 20%, it has a 98% accuracy.  </p>

<p>What I am wondering is this:  if I filter the corpus and remove the documents for which there is disagreement and retrain and repeat, can I expect to arrive at a corpus for which the classifier agrees with the original labeling process within some small margin of error (less than 1%)?  </p>

<pre><code>EDIT:
</code></pre>

<p>After finding a corpus that appears to be clean, I would manually inspect the documents that were rejected.  I expect there would be around 5000 (2% of 250,000)  </p>
",2013-11-02 15:56:14.807
58712,306.0,2,,58705.0,,,,CC BY-SA 3.0,"<p>If you know or you think that there are two clusters in the data, then fit a mixture of two normal distributions, one for each cluster. then for every point, choose the set of points belonging to that cluster and then perform the transformation you think is right. when reporting or using each point, report the distribution you have assigned the point to as well as the transformed value. that way you get to preserve the natural structure of data, yet change it to a form you want it to.</p>
",2013-11-02 16:09:37.953
58713,2666.0,2,,16313.0,,,,CC BY-SA 3.0,"<p>These are all good indexes of monotonic association.  Spearman's $\rho$ is related to the probability of majority concordance among random triplets of observations, and $\tau$ (Kendall) and $\gamma$ (Goodman-Kruskal) are related to pairwise concordance.  The main decision to make in choosing $\gamma$ vs. $\tau$ is whether you want to penalize for ties in $X$ and/or $Y$.  $\gamma$ does not penalize for ties in either, so that a comparison of the predictive ability of $X_{1}$ and $X_{2}$ in predicting $Y$ will not reward one of the $X$s for being more continuous.  This lack of reward makes it a bit inconsistent with model-based likelihood ratio tests.  An $X$ that is heavily tied (say a binary $X$) can have high $\gamma$.</p>
",2013-11-02 17:10:49.090
58714,10772.0,1,,,,Sampling from truncated distribution,<self-study><simulation><gibbs>,CC BY-SA 3.0,"<p>I want to sample from a truncated distribution that appears in a Gibbs sampling scheme.
The full conditional of the distribution is given by</p>

<p>$p(X = k | \ldots) \propto (1 - p)^{k - 1} \mathbb{1} ( s \leq k)$, where $s$ is a positive integer.</p>

<p>This is a truncated geometric distribution. The tehcnique i am following to simulate from this is first to sample a random number from a geometric distribution and then add to this the number $s$.</p>

<p>First off all i want to ask if this that i am doing is right. And after, is there any obsious reason for this simulation to be slow? Or is it slow because i am doing something wrong?</p>
",2013-11-02 17:53:46.303
58723,20473.0,1,,,,For which distributions does uncorrelatedness imply independence?,<probability><distributions><correlation><mathematical-statistics><independence>,CC BY-SA 3.0,"<p>A time-honored reminder in statistics is ""uncorrelatedness does <em>not</em> imply independence"". Usually this reminder is supplemented with the psychologically soothing (and scientifically correct) statement ""when, nevertheless the two variables are <strong>jointly normally distributed</strong>, then uncorrelatedness does imply independence"".  </p>

<p>I can increase the count of happy exceptions from one to two: when two variables are <strong>Bernoulli-distributed</strong> , then again, uncorrelatedness implies independence. If $X$ and $Y$ are two Bermoulli rv's, $X \sim B(q_x),\; Y \sim B(q_y)$, for which we have $P(X=1) = E(X) = q_x$, and analogously for $Y$, their covariance is </p>

<p>$$\operatorname{Cov}(X,Y)= E(XY) - E(X)E(Y) = \sum_{S_{XY}}p(x,y)xy - q_xq_y $$</p>

<p>$$ = P(X=1,Y=1) - q_xq_y = P(X=1\mid Y=1)P(Y=1)-q_xq_y$$</p>

<p>$$= \Big(P(X=1\mid Y=1)-q_x\Big)q_y $$</p>

<p>For uncorrelatedness we require the covariance to be zero so</p>

<p>$$\operatorname{Cov}(X,Y) = 0 \Rightarrow P(X=1\mid Y=1) = P(X=1)$$</p>

<p>$$\Rightarrow P(X=1,Y=1) = P(X=1)P(Y=1) $$</p>

<p>which is the condition that is also needed for the variables to be independent.  </p>

<p>So my question is: <strong>Do you know of any other distributions (continuous or discrete) for which uncorrelatedness implies independence?</strong>  </p>

<p><strong>Meaning:</strong> Assume two random variables $X,Y$ which have <em>marginal</em> distributions that belong to the same distribution (perhaps with different values for the distribution parameters involved), but let's say with the same support eg. two exponentials, two triangulars, etc. Does all solutions to the equation $\operatorname{Cov}(X,Y) = 0$ are such that they also imply independence, by virtue of the form/properties of the distribution functions involved? This is the case with the Normal marginals (given also that they have a bivariate normal distribution), as well as with the Bernoulli marginals -are there any other cases?</p>

<p>The motivation here is that it is usually easier to check whether covariance is zero, compared to check whether independence holds. So if, given the theoretical distribution, by checking covariance you are also checking independence (as is the case with the Bernoulli or normal case), then this would be a useful thing to know.<br>
If we are given two samples from two r.v's that have normal marginals, we know that if we can statistically conclude from the samples that their covariance is zero, we can also say that they are independent (but only because they have normal marginals). It would be useful to know whether we could conclude likewise in cases where the two rv's had marginals that belonged to some other distribution.</p>
",2013-11-02 20:13:49.093
58724,1145.0,2,,58179.0,,,,CC BY-SA 3.0,"<p>This is a great question!</p>

<p>In my opinion the most important place to start an answer is to point out the distinction that should be drawn between P-values which are the result of a <em>significance test</em> and the decisions that are the results of <em>hypothesis tests</em>. Hypothesis tests are designed to control long term errors and are entirely frequentist in that the Frequentist or Repeated Sampling Principle is respected above all other considerations. In contrast the P-value from a significance test is an index of the evidence in the data and should be used for inference that is consistent with the Likelihood Principle.</p>

<p>Hypothesis tests and significance tests are usually assumed to be the same thing. Among the scientists with whom I hang out the most common test employed is a hybrid of the two which offers most of the disadvantages of both with few advantages. That hybrid has been written about many, including myself. See this paper for a description of the issues aimed at biomedical scientists: <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22394284"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/22394284</a>.</p>

<p>The various scenarios with ad hoc alterations to experimental design that you present are very problematical to a frequentist analyst because the long run error rates associated with decisions depend on <em>pre-specified</em> criteria of $\alpha$ (usually 0.05), sample size etc. If the experiment changes between the design phase and implementation it is difficult to know what characteristics of design to impute. Thus the alterations in design affect the error rates and it is not always easy to make an appropriate adjustment to the analysis that would be acceptable to everyone. </p>

<p>For example, if an experiment is extended beyond the initial design sample size then it begs the question of whether the extension is an attempt to gain 'significance' from data that didn't quite make it at the designed end point. Such behaviour increases the risk of false positive results. So does stopping prematurely with a significant result.</p>

<p>If you view the data as evidence then the matching up of experimental design and implementation is much less important. A larger sample will provide more reliable and more convincing evidence. The P-value need not be 'adjusted' or 'corrected' for deviations from the planned sample size when it is viewed as an index of evidence. </p>

<p>Of course, what it means to be an index of evidence is not clear to most people. It turns out that for any P-value from a significance test of a certain sample size (pre-specified or not) there is a specific likelihood function. That likelihood function depicts the evidence that the P-value indexes. I have written a long paper on this topic that, by happy coincidence, I arXived just two days ago. Its title is ""To P or not to P: on the evidential nature of P-values and their place in scientific inference"" <a href=""http://arxiv.org/abs/1311.0081"" rel=""nofollow"">http://arxiv.org/abs/1311.0081</a>. The next paragraph is a relevant extract:</p>

<hr>

<p>The long-run error rates associated with an experiment are a property of the experimental design and the behaviour of the experimenter rather than of the data. The 'size' of the experiment, $\alpha$, is properly set before the data is available, so it cannot be data-dependent. In contrast, the P-value from a significance test is determined by the data rather than the arbitrary setting of a threshold. It cannot logically be an error rate because it doesn't force a decision in the way that inductive behaviour does, and if a decision is made to discard the null hypothesis when a small (presumably) P-value is observed, the decision is made on the basis of the smallness of the P-value in conjunction with whatever information that the experimenter considers relevant. Thus the rate of erroneous inferences is a function of not only the P-value but the quality and availability of additional information and, sometimes, the intuition of the experimenter. P-values are not error rates, whether 'observed', 'obtained' or 'implied'.</p>

<hr>

<p>So, answers. Yes, P-values are still useful in your scenarios. Unadjusted P-values still index the likelihood functions that depict the evidence in the data and you can still make inferences using that evidence. You should not adjust the P-values for multiple comparisons, but instead understand that the evidence needs to be interpreted in light of the number of comparisons made. Deviations from the plan will invalidate frequentist hypothesis tests, but not necessarily invalidate a likelihood-based evaluation of the evidence.</p>

<p>Getting the most out of the evidence and avoiding inflated type I errors are not the same thing, and you cannot comply with the frequentist principle and the likelihood principle at the same time in many circumstances. Your edit makes it sound like you have a Bayesian model in mind. That is usually a good idea (and will usually comply with the likelihood principle).</p>
",2013-11-02 20:56:45.060
58715,17740.0,2,,58710.0,,,,CC BY-SA 3.0,"<p>It seems like you are mixing a couple of things up. First of all, cross-validation is used to get an accurate idea of the <em>generalization error</em> when certain tuning parameters are used. </p>

<p>You can use <code>svm-train</code> in k-fold cross-validation mode using the <code>-v k</code> flag. In this mode, <code>svm-train</code> does not output a model -- just a cross-validated estimate of the generalization performance.</p>

<p><code>grid.py</code> is basically a wrapper around <code>svm-train</code> in cross-validation mode. It allows you to easily assess the best parameter tuple out of a given set of options via cross-validation. It is essentially a loop over the specified parameter tuples which performs cross-validation.</p>

<blockquote>
  <p>a. Is the -v 10 option of cross validation can replace the testing step?</p>
</blockquote>

<p>Not entirely. Cross-validation is indeed used to get an estimate of the generalization performance of a model, but when performing cross-validation the entire training set is never used to construct a single model. The typical steps are (i) find optimal tuning parameters using cross-validation, (ii) train a model using these optimal parameters on the full training set and (iii) test this model on the test set.</p>

<blockquote>
  <p>b. The result given by the steps above is suspiciously high (96%), and so I'm wondering if I am doing something wrong?</p>
</blockquote>

<p>Don't worry, be happy. Such classification accuracies are quite feasible for a wide range of problems.</p>

<blockquote>
  <p>c. Could the use of grid.py for parameter selection before the train + cross validation damage the results (as if I were testing on data I've already trained)?</p>
</blockquote>

<p><code>grid.py</code> does cross-validation for you. There is no point to perform cross-validation again after you ran <code>grid.py</code>.</p>
",2013-11-02 17:59:06.290
58716,23289.0,1,58733.0,,,Wilcoxon signed rank test on anonymous sample,<hypothesis-testing><wilcoxon-mann-whitney-test><randomness><wilcoxon-signed-rank>,CC BY-SA 3.0,"<p>I carried out a test on a group of 100 people. The test contained 10 questions with 5-point Likert scale answers (1 = Strongly Agree, 5 = Strongly Disagree). After the test was conducted, the group was educated over a period of time on how to answer the questions. After the education completed, the same test was carried out again on the same group, but with 10 absent. Knowing that the pre and post tests were anonymously conducted, I would like to test whether there is a significant difference between the two test results.
If I use the Wilcoxon Signed Rank Test for a paired sample to compare results for individual question of each test, the results will be different depending of the entry of the test results. In other words, I can't tell if one student improved because I can't link his first test to his second test.</p>

<p>I don't know if using the Wilcoxon Signed Rank Test for a paired sample would be possible given the tests were done anonymously.</p>

<p>Any help is appreciated.
Thanks</p>
",2013-11-02 18:11:51.227
58717,503.0,2,,58708.0,,,,CC BY-SA 3.0,"<p>A few points:</p>

<p>1) The log of a ratio is simply the difference of the logs. Is this really what you want? </p>

<p>2) As to your main point; although it is not completely clear, the two methods that come to mind here are multiple imputation and propensity scores. There is a huge amount of literature on both of these. You could start by looking up both those terms right here on CrossValidated. That should get you to places where you can access the wider literature.</p>

<p><a href=""https://stats.stackexchange.com/questions/tagged/propensity-scores"">Here</a> are 28 posts about propensity scores.</p>

<p><a href=""https://stats.stackexchange.com/questions/tagged/multiple-imputation"">Here</a> are 53 posts about multiple imputation. </p>

<p>and <a href=""https://stats.stackexchange.com/questions/35955/propensity-score-matching-after-multiple-imputation"">here</a> is one post about using both (with links to several articles)</p>
",2013-11-02 18:22:39.377
58718,23292.0,2,,1760.0,,,,CC BY-SA 3.0,"<p>Do you want to know if his forecast is more accurate than another forecast?  If so, you can look at basic accuracy metrics for probabilistic classification like cross-entropy, precision/recall, ROC curves, and the f1-score.  </p>

<p>Determining if the forecast is objectively good is a different matter.  One option is to look at calibration.  Of all the days where he said that there would be a 90% chance of rain, did roughly 90% of those days have rain?  Take all of the days where he has a forecast and then bucket them by his estimate of the probability of rain.  For each bucket, calculate the percentage of the days where rain actually occurred.  Then for each bucket plot the actual probability of rain against his estimate for the probability of rain.  The plot will look like a straight line if the forecast is well calibrated.  </p>
",2013-11-02 19:04:43.113
58719,4320.0,2,,58658.0,,,,CC BY-SA 3.0,"<p>You need to first calculate all your updates as if the wieghts weren't shared, but just store them, don't actually do any updating yet. </p>

<p>Let $w_k$ be some weight that appears at locations $I_k = \{(i,j) \colon w_{i,j} = w_k\}$ in your network and $\Delta w_{i,j} = -\eta \frac{\partial J}{\partial w_{i,j}} $ where $\eta$ is the learning rate and $J$ is your objective function. Note that at this point if you didn't have weight sharing you would just upade $w_{i,j}$ as 
$$
    w_{i,j} = w_{i,j} + \Delta w_{i,j}.
$$
To deal with the shared weights you need to sum up all the individual updates. So set 
$$
    \Delta w_k = \sum_{(i,j) \in I_k} \Delta w_{i,j}
$$
and then update 
$$
    w_k = w_k + \Delta w_k.
$$</p>
",2013-11-02 19:10:34.450
58720,22682.0,1,58743.0,,,Variance of sum of random number of random variables (Cambridge University Worksheet),<probability><self-study><variance><random-variable>,CC BY-SA 3.0,"<p>In the vein of <a href=""https://stats.stackexchange.com/questions/74147/expectation-of-quotient-of-sums-of-iid-random-variables-cambridge-university-wo"">my last question</a>, I'm now at a roadblock on question 3 of this sheet:</p>

<p><a href=""http://www.trin.cam.ac.uk/dpk10/IA/exsheet3.pdf"" rel=""nofollow noreferrer"">http://www.trin.cam.ac.uk/dpk10/IA/exsheet3.pdf</a></p>

<p>(note: it's not my intention to ask every question I get stuck on here, merely the ones which have interesting general results; it just so happens that the two I've struggled with so far fit this criteria)</p>

<p>It goes as follows:</p>

<p>Let $N$ be a non-negative integer-valued random variable with mean $\mu_1$ and variance $\sigma_1^2$, and let $X_1, X_2, ...$ be random variables, each with mean $\mu_2$ and variance $\sigma_2^2$; furthermore, assume that $N, X_1, X_2,  . . .$ are independent. Without using generating functions, calculate the mean and variance of the random variable $S_N = X_1 + ... + X_N$ (when $N=0$ interpret $S_N$ as $0$).</p>

<p>I have the answer to calculating the mean, which I'll write up as an answer for future reference. Here's where I've got to with the variance bit:</p>

<p>$\mathbb{E}(S_N^2) = 0.P(N=0) + \sum_{r=1}^\infty\mathbb{E}((\sum_{i=1}^rX_i)^2).P(N=r)$</p>

<p>For $r=1$, we have the inner expectation as: $\mathbb{E}(X_1^2)$ which is equal to $\sigma_2^2+\mu_2^2$.</p>

<p>For $r &gt; 1$, we have the inner expectation equal to: $r(\sigma_2^2 + 2\mu_2^2)$</p>

<p>So $\mathbb{E}(S_N^2) = (\sigma_2^2+\mu_2^2)P(N=1) + (\sigma_2^2 + 2\mu_2^2)\sum_{r=2}^\infty rP(N=r)$</p>

<p>which is <em>almost</em> in the form where I can use the same trick as in calculating the mean, but not quite.</p>

<p>Any help is much appreciated.</p>
",2013-11-02 19:16:47.060
58726,23296.0,1,58746.0,,,Covariance with conditional expectation,<self-study><covariance><proof><conditional-expectation>,CC BY-SA 3.0,"<blockquote>
<p>Suppose <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> are random variables, <span class=""math-container"">$E(Y^2) &lt; \infty$</span> and
<span class=""math-container"">$\varepsilon = Y - E(Y|X)$</span> so that <span class=""math-container"">$Y = E(Y|X) + \varepsilon$</span>.</p>
<p>Given that <span class=""math-container"">$E(\varepsilon | X) = E(\varepsilon) = 0$</span>, show that
<span class=""math-container"">$Cov(\varepsilon , E(Y|X)) = 0$</span>.</p>
</blockquote>
<p>This question has multiple parts so <span class=""math-container"">$E(Y^2) &lt; \infty$</span> may or may not be applicable in this case.</p>
<p>Here's what I tried so far. I used the fact that <span class=""math-container"">$Cov(X,Y) = E(XY) - E(X)E(Y)$</span> and <span class=""math-container"">$Cov(X,Y) = E[(X - E(X))(Y-E(Y))]$</span> and concluded that <span class=""math-container"">$Cov(\varepsilon , E(Y|X)) = E(\varepsilon E(Y|X))$</span> or in other words, <span class=""math-container"">$E(\varepsilon E(Y)) = 0$</span>.</p>
<p>From there, I guess the only thing I have to show is that: <span class=""math-container"">$E(\varepsilon E(Y|X)) = 0$</span>, but I'm having trouble doing this.<br />
Am I going in the right track or is this completely the wrong approach to tackling this problem?</p>
",2013-11-02 21:34:27.800
58727,9063.0,1,58729.0,,,Survival regression and prediction using median,<predictive-models><survival><hazard>,CC BY-SA 3.0,"<p>I'm using artificially generated hazard curves (that is, I know the true hazard curve) and Aalen's additive model to fit the covariates. For example, below is an individual's hazard curve and my estimate of it: <img src=""https://i.stack.imgur.com/LC6nJ.png"" alt=""enter image description here"" /></p>
<p>It's a good fit to my eye, and here is the individual's estimated survival curve (exp of the negative of the above )</p>
<p><img src=""https://i.stack.imgur.com/3112g.png"" alt=""enter image description here"" /></p>
<p>Again, this looks fine to me. What happens next though is odd. To make predictions of individuals' lifetimes, I use the median of the survival curve (the intersection of the curve and 0.5). I then plot this prediction against the observed value, and what I was expecting to see was a high correlation between the two, instead I see this:</p>
<p><img src=""https://i.stack.imgur.com/Gk1Di.png"" alt=""enter image description here"" /></p>
<p>This seems very wrong to me. A few questions I have:</p>
<ul>
<li><p>Is using the median correct? Intuitively I see nothing wrong with it, but my results suggest otherwise.</p>
</li>
<li><p>Is predicting lifetimes just too noisy of a challenge and this is indeed that best I can do?</p>
<p>Has anyone experienced something like this before?</p>
</li>
</ul>
",2013-11-02 21:50:56.473
58728,23284.0,1,58734.0,,,Frequency of time series in R,<r><time-series><frequency>,CC BY-SA 3.0,"<p>I have 10 minutes-intervaled wait-times data for a coffee shop for 4 weeks between 9am-5pm. I use R's ts for my analysis. What should be the frequency parameter? </p>

<p>Is it (48=# of intervals per day) or just 1? </p>
",2013-11-02 22:01:03.497
58729,503.0,2,,58727.0,,,,CC BY-SA 3.0,"<p>The bottom plot is simply saying that lifetimes often vary from the median. This is inherent in the first two plots. That is, the person in the plot has a median of about 10, but he/she also has about a 20% chance of dying before 5, and about a 20% chance of making it past 20. </p>

<p>If curves for other people are similar, then the bottom plot agrees with the top 2.</p>
",2013-11-02 22:29:13.400
58730,23297.0,1,58732.0,,,Standard Deviation and Variance in Sample and Population Formulas for all?,<self-study><variance><standard-error><sample><population>,CC BY-SA 3.0,"<p>What are the differences between those Population and Standard Deviation and what are their fomulas? I was told that they are different formulas and which situation do you use population or sample?</p>

<p>If a user gives 10 random numbers, would I be using population or sample?</p>
",2013-11-02 22:30:16.207
58731,503.0,2,,58730.0,,,,CC BY-SA 3.0,"<p>OK....</p>

<p>The formulas vary because the population SD has N in the denominator and the sample SD has N-1 in the denominator. </p>

<p>As to when to use each, well.... it's sort of inherent in the names of the two terms; or are you unclear on sample vs. proportion?</p>
",2013-11-02 22:39:59.947
58732,594.0,2,,58730.0,,,,CC BY-SA 3.0,"<p>The population variance is the average squared distance from the mean. You use it when you have the <a href=""https://en.wikipedia.org/wiki/Statistical_population"" rel=""nofollow"">population</a>.</p>

<p>That is, if you have every member of the population of interest, you compute $\sigma^2 = \bar v = \frac{\sum_i v_i}{n}$ where $v_i = (x_i - \mu)^2$.</p>

<p>(If you're asking about variances of random variables, see <a href=""https://en.wikipedia.org/wiki/Variance"" rel=""nofollow"">here</a> for the details of the relevant formulas)</p>

<p>The sample variance, $s^2$ uses the same formula, but usually the denominator of the average is taken to be one smaller because observations are closer to the sample mean than they are to the population mean, which makes the squared deviations too small on average; replacing $\frac{}{n}$ with $\frac{}{n-1}$ makes them right on average. The $n-1$ denominator version is sometimes called $s^2_{n-1}$ by contrast from the version with the $n$ denominator, $s^2_n$. You use sample variance when you have a <a href=""https://en.wikipedia.org/wiki/Statistical_sample"" rel=""nofollow"">sample</a>.</p>

<p>The relevant standard deviations are simply the square roots of the corresponding variances. </p>
",2013-11-02 22:49:02.983
58733,5448.0,2,,58716.0,,,,CC BY-SA 3.0,"<p>The short answer, in three parts, is a) no, you can't do a paired test, as has been pointed out in comments, b) yes, you can do an unpaired test, and c) that 10% of non-respondents to the second test may be important.</p>

<p>Let us consider a simplistic hierarchical model of response, where there is an individual-level characteristic $\theta_i$ which has some distribution $f(\theta)$ and a test-specific response $y_{ij}$ for tests $j \in {1,2}$ that depends upon the individual-level characteristic through its distribution $p_j(y_{ij} | \theta_i)$.  If we know $i$ for each $y_{ij}$ we can obviously do the paired test, and the difference between $y_{i1}$ and $y_{i2}$ obviously are not influenced by the differences between the $\theta_i$, since it is for the same $i$.  </p>

<p>If, on the other hand, we don't know the individual $i$, we are faced with draws from two distributions $p^*_j(y_{ij}) = \int_\Theta p_j(y_{ij} | \theta_i) f(\theta_i)\text{d}\theta_i$.  The scores $y_{ij}$ are still independent across $i$ and, under the null, independent across $j$ as well.  The distribution itself no longer varies across $i$.  Under alternative hypotheses, the distributions $p_j$ will still differ across $j$, it's just that they are population-level distributions rather than individual-level distributions.  </p>

<p>Consequently, we can still perform an (unpaired) test for differences between $j$, but it's going to be less powerful than if you could get rid of the extra variability introduced by not knowing the individuals.  It's just a matter of what you can condition on; more conditioning reduces variability and thereby increases power.  </p>

<p>Personally, I'd use the unpaired version of the Wilcoxon, as you can't lose much relative to the unpaired version of the $t$-test and you might gain a lot.  See <a href=""https://stats.stackexchange.com/questions/19681/when-to-use-the-wilcoxon-rank-sum-test-instead-of-the-unpaired-t-test/19682#19682"">this question</a> for a little more information.</p>

<p>Of greater concern is that missing 10% of the original sample.  You'd really like to understand the missing data mechanism, if any.   Consider the possibility that the 10 who dropped out were among the poorest performers on the original test, and that the amount of improvement was strongly negatively related to how well an individual performed on the first test (i.e., poor performers improved a lot more on average than good performers).  That, combined with regression-to-the-mean effects, means you'd likely be missing data on some of your largest gains, thus a) weakening your ability to detect a significant difference, and b) biasing your estimate(s) of how much improvement there was downwards.  OTOH, under the null hypotheses, we expect to see a gain, because we are including the low scorers in sample 1 but removing 10 likely low scorers from sample 2.  So there's an upwards bias there too.  Which effect dominates isn't clear, but what is clear is that your test and associated estimates would almost certainly be biased.</p>

<p>For example, if I simulate from the simple model above assuming $\theta_i \sim \text{N}(0,1)$ and $y_{ij} \sim \text{N}(\theta_i,1)$, and drop the $y_{i2}$ for which $y_{i1}$ was in the 10 lowest values, the expected value of $y_{i2} \approx 0.136$ while that of $y_{i1} = 0$.  0.136 is about 0.96 standard deviations above 0, relative to the std. dev. of the difference between the means of $y_{i1}$ and $y_{i2}$, which would obviously have a huge impact on your type I and type II error probabilities.</p>

<pre><code>e2 &lt;- rep(0,100000)
for (i in 1:100000) {
   theta &lt;- rnorm(100)
   y1 &lt;- rnorm(100, theta)
   y2 &lt;- rnorm(100, theta)
   y2[order(y1)[1:10]] &lt;- NA
   e2[i] &lt;- mean(y2, na.rm=TRUE)
}
mean(e2)
[1] 0.1360364
</code></pre>
",2013-11-03 00:03:21.377
58734,132.0,2,,58728.0,,,,CC BY-SA 3.0,"<p>Most likely you have two seasonal periods: 48 (number of intervals per day) and 48x5 (number of intervals per week assuming a 5-day week).</p>

<p>The <code>tbats()</code> function from the <code>forecast</code> package in R will handle multiple seasonal periods. For example (where <code>x</code> is the data):</p>

<pre><code>library(forecast)
x &lt;- msts(x, seasonal.periods=c(48, 48*5))
fit &lt;- tbats(x)
fc &lt;- forecast(fit, h=48*5)
plot(fc)
</code></pre>

<p>Alternatively (and the only easy option if there are missing data) is to use Fourier terms for the seasonal periods and ARMA errors to handle any remaining serial correlation. The ARIMA functions in R do not automatically handle multiple seasonal periods, but the following R code should work:</p>

<pre><code>x &lt;- ts(x, frequency=48)
seas1 &lt;- fourier(x, K=3)
seas2 &lt;- fourier(ts(x, freq=48*5), K=3)
fit &lt;- auto.arima(x, xreg=cbind(seas1,seas2))

seas1.f &lt;- fourierf(x, K=3, h=48*5)
seas2.f &lt;- fourierf(ts(x, freq=48*5), K=3, h=48*5)
fc &lt;- forecast(fit, xreg=cbind(seas1.f,seas2.f))
</code></pre>

<p>The number of Fourier terms (arbitrarily set to 3 for both seasonal periods in the above code) can be selected by minimizing the AIC. </p>
",2013-11-03 00:08:43.977
58735,503.0,5,,,,,,CC BY-SA 3.0,"<p>The formula for the sample mean is </p>

<p>$$\bar{x} = \frac{\sum_i x_i}{n}$$</p>

<p>where $x_i$ are the individual values and $n$ is the number of values in the sample.</p>

<p>There could also be sample means for other means, but without further details, the mean here refers to the arithmetic mean.</p>
",2013-11-03 00:16:49.967
58736,503.0,4,,,,,,CC BY-SA 3.0,The sample mean refers to the arithmetic mean of a sample from a population.,2013-11-03 00:16:49.967
58739,17900.0,1,,,,Multiple hypothesis testing of same hypothesis,<hypothesis-testing><multiple-comparisons>,CC BY-SA 3.0,"<p>I read an interesting statement: running the same data through multiple tests no longer counts as multiple comparisons. </p>

<p>I am fairly confident this is incorrect if you are testing different hypotheses e.g. testing for hundreds of different lags on the same data will yield some false significant results (if you have enough data to test for large lags).</p>

<p>However, <strong>if you are testing exactly the same hypothesis, but with two different tests, is this still a multiple comparison?</strong></p>

<p>My thinking so far is that firstly it is rarely if ever true that two tests have exactly the same null hypothesis. e.g. the <a href=""http://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test"" rel=""nofollow"">Shapiro-wilk test</a> and <a href=""http://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test"" rel=""nofollow"">Jarqueâ€“Bera test</a>, while both ostensibly ""normality tests"" are testing slightly different null hypotheses, that the data came from a normal distribution and that the data cam from a distribution with skew and kurtosis equal to a normal distribution. Are there any tests with exactly the same null hypothesis? </p>

<p>If there are tests that against exactly the same null hypothesis, would this count as a multiple comparison? And if the tests have exactly the same null hypothesis, is it possible for them to yield different results given the same data?</p>
",2013-11-03 00:22:49.317
58740,16992.0,2,,58435.0,,,,CC BY-SA 3.0,"<p>The difficulty of the problem is highly dependent on how erroneous the uncertain labels can be. If the the uncertain labels are right, say, 90% of the time, you can probably get away with just using logistic regression. On the other hand, if the labels are wrong almost half the time, you may need to resort to some special techniques. <a href=""http://arxiv.org/abs/1310.1363"" rel=""nofollow"">Here</a>'s one stab I took at a very similar problem. (We had multiple observations per label, but otherwise the setup is quite similar.)</p>
",2013-11-03 00:58:09.247
58741,23298.0,1,58745.0,,,Kolmogorov-Smirnov test strange output,<goodness-of-fit><fitting><kolmogorov-smirnov-test><scipy>,CC BY-SA 3.0,"<p>I am trying to fit my data to the one of the continuous PDF (I suggest it to be gamma- or lognormal-distributed). The data consists of about 6000 positive floats.
But the results of the Kolmogorov-Smirnov test completely refute my expectations providing the very low p-values. </p>

<p>Data empirical distribution</p>

<p><img src=""https://i.stack.imgur.com/SdB68.png"" alt=""enter image description here""> </p>

<p>Distribution fitting
<img src=""https://i.stack.imgur.com/8gi81.png"" alt=""enter image description here""></p>

<p>Python code:</p>

<pre><code>import numpy
import sys
import json
import matplotlib.pyplot as plt
import scipy
from scipy.stats import *

dist_names = ['gamma', 'lognorm']
limit = 30

def distro():
    #input file
    with open(sys.argv[1]) as f:
        y = numpy.array(json.load(f))

    #output
    results = {}
    size = y.__len__()
    x = scipy.arange(size)
    h = plt.hist(y, bins=limit, color='w')
    for dist_name in dist_names:
        dist = getattr(scipy.stats, dist_name)
        param = dist.fit(y)
        goodness_of_fit = kstest(y, dist_name, param)
        results[dist_name] = goodness_of_fit
        pdf_fitted = dist.pdf(x, *param) * size
        plt.plot(pdf_fitted, label=dist_name)
        plt.xlim(0, limit-1)
        plt.legend(loc='upper right')
    for k, v in results.iteritems():
        print(k, v)
    plt.show()
</code></pre>

<p>This is the output:</p>

<ul>
<li><strong>p-value is almost 0</strong> <code>'lognorm', (0.1111486360863001, 1.1233698406822002e-66)</code>  </li>
<li><strong>p-value is 0</strong> <code>'gamma', (0.30531260123096859, 0.0)</code>  </li>
</ul>

<p>Does it mean that my data does not fit gamma distribution?.. But they seem so similar...  </p>
",2013-11-03 01:00:53.333
58742,9483.0,2,,10541.0,,,,CC BY-SA 3.0,"<p>The Statistics Toolbox <a href=""http://www.mathworks.com/help/stats/clustering.evaluation.gapevaluation-class.html"" rel=""nofollow noreferrer"">implements the gap statistic</a> as a class in the package <code>clustering.evaluation</code> since R2013b:</p>

<pre><code>load fisheriris;
rng('default');  % For reproducibility
eva = evalclusters(meas,'kmeans','gap','KList',[1:6])
figure;
plot(eva);
</code></pre>

<p><img src=""https://i.stack.imgur.com/YROXM.png"" alt=""enter image description here""></p>

<p>You can also use this <a href=""http://www.mathworks.com/matlabcentral/fileexchange/37905-gap-statistics"" rel=""nofollow noreferrer"">file exchange</a>.</p>
",2013-11-03 01:02:29.910
58743,594.0,2,,58720.0,,,,CC BY-SA 3.0,"<p>The easy way is to use the law of total variance:</p>

<p>$$\text{Var}(S) = E_N\left[\text{Var}(S|N)\right] + \text{Var}_N\left[E(S|N)\right] =\text{E}_N\left[N\cdot \text{Var}(X)\right] + \text{Var}_N\left[N\cdot\text{E}(X)\right]$$</p>

<p>Can you do it from there? It's pretty much just substitution (well, that and really basic properties of expectation and variance).</p>

<p>(The first part is even more straightforward using the law of total expectation.)</p>

<p>--</p>

<p>As Spy_Lord notes, the answer is $\text{E}(N)\cdot \text{Var}(X) + \text{Var}(N)\cdot\text{E}(X)^2$</p>

<hr>

<p>Alternative approach is to evaluate $E(S_N^2)$. Following the approach you seem to be aiming at:</p>

<p>\begin{eqnarray}
E(S_N^2) &amp;=&amp; \sum_r E(S_N^2|N=r) p_r\\
         &amp;=&amp; \sum_r (r\sigma_2^2+r^2 \mu_2^2) p_r\\
         &amp;=&amp; \sigma_2^2\sum_r rp_r+\mu_2^2\sum_rr^2 p_r \\
         &amp;=&amp; \sigma_2^2 \text{E}N+\mu_2^2\text{E}(N^2) 
\end{eqnarray}</p>

<p>and I assume you can do it from there. </p>

<p>However, to be honest, I think this way is easier (it's actually the same approach, you just don't need to sum over all the mutually exclusive events that way). The law of total expectation says $\text{E}(X) = \text{E}_Y[\text{E}_{X|Y}(X|Y)]$, so</p>

<p>\begin{eqnarray}
\text{E}(S^2_N) &amp;=&amp; \text{E}_N[\text{E}(S^2_N|N)]\\
                &amp;=&amp; \text{E}_N[N\sigma_2^2+N^2\mu_2^2]\\
                &amp;=&amp; \sigma_2^2\text{E}(N)+\mu_2^2\text{E}(N^2)
\end{eqnarray}</p>
",2013-11-03 01:03:41.747
58744,15972.0,2,,58723.0,,,,CC BY-SA 4.0,"<p>&quot;Nevertheless if the two variables are normally distributed, then uncorrelatedness does imply independence&quot; <a href=""http://en.wikipedia.org/wiki/Normally_distributed_and_uncorrelated_does_not_imply_independent"" rel=""nofollow noreferrer"">is a very common fallacy</a>.</p>
<p>That only applies if they are <em>jointly</em> normally distributed.</p>
<p>The counterexample I have seen most often is normal <span class=""math-container"">$X \sim N(0,1)$</span> and independent Rademacher <span class=""math-container"">$Y$</span> (so that it is 1 or -1 with probability 0.5 each); then <span class=""math-container"">$Z=XY$</span> is also normal (clear from considering its distribution function), <span class=""math-container"">$\operatorname{Cov}(X,Z)=0$</span> (the problem here is to show <span class=""math-container"">$\mathbb{E}(XZ)=0$</span> e.g. by iterating expectation on <span class=""math-container"">$Y$</span>, and noting that <span class=""math-container"">$XZ$</span> is <span class=""math-container"">$X^2$</span> or <span class=""math-container"">$-X^2$</span> with probability 0.5 each) and it is clear the variables are dependent (e.g. if I know <span class=""math-container"">$X&gt;2$</span> then either <span class=""math-container"">$Z&gt;2$</span> or <span class=""math-container"">$Z&lt;-2$</span>, so information about <span class=""math-container"">$X$</span> gives me information about <span class=""math-container"">$Z$</span>).</p>
<p>It's also worth bearing in mind that marginal distributions do not uniquely determine joint distribution. Take any two real RVs <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> with marginal CDFs <span class=""math-container"">$F_X(x)$</span> and <span class=""math-container"">$G_Y(y)$</span>. Then for any <span class=""math-container"">$\alpha&lt;1$</span> the function:</p>
<p><span class=""math-container"">$$H_{X,Y}(x,y)=F_X(x)G_Y(y)\left(1+\alpha\big(1-F_X(x)\big)\big(1-G_Y(y)\big)\right)$$</span></p>
<p>will be a bivariate CDF. (To obtain the marginal <span class=""math-container"">$F_X(x)$</span> from <span class=""math-container"">$H_{X,Y}(x,y)$</span> take the limit as <span class=""math-container"">$y$</span> goes to infinity, where <span class=""math-container"">$G_Y(y)=1$</span>. Vice-versa for <span class=""math-container"">$Y$</span>.) Clearly by selecting different values of <span class=""math-container"">$\alpha$</span> you can obtain different joint distributions!</p>
",2013-11-03 01:37:09.837
58745,5237.0,2,,58741.0,,,,CC BY-SA 4.0,"<p>Yes.  Neither of these distributions is a good fit for your data by that criterion.  There are some other distributions you could try, but it strikes me as (ultimately) unlikely that real data come from any of the well-studied distributions, and you have 6k data, so even a trivial discrepancy will make the test 'significant'.  (For more along those lines, see: <a href=""https://stats.stackexchange.com/questions/2492/"">Is normality testing 'essentially useless'?</a>)</p>
<p>On the other hand, instead of checking to see if your data significantly diverge from these distributions, you could see how well your data <em>correlate</em> with the distributions you are interested in--the fit may well be 'good enough' for your purposes.  (For more along these lines, see my answer here: <a href=""https://stats.stackexchange.com/a/27966/7290"">Testing randomly generated data against its intended distribution</a>.)</p>
",2013-11-03 01:40:09.613
58746,20473.0,2,,58726.0,,,,CC BY-SA 3.0,"<p>$$E\Big((Y-E(Y\mid X))E(Y\mid X)\Big) = E\Big(YE(Y\mid X) - E(Y\mid X)^2\Big) $$</p>

<p>$$E\Big(YE(Y\mid X)\Big) - E\Big(E(Y\mid X)^2\Big) $$</p>

<p>Now by the law of total (or is it iterated - I always forget) expectation, we have for the first term</p>

<p>$$E\Big(YE(Y\mid X)\Big) = E\Big[E\Big(YE(Y\mid X)\mid X \Big)\Big] = E\Big[E(Y\mid X)E(Y\mid X ) \Big] = E\Big(E(Y\mid X)^2\Big)$$</p>

<p>so the whole expression equals zero.</p>
",2013-11-03 01:43:01.940
58747,18403.0,2,,56780.0,,,,CC BY-SA 4.0,"<p>There's a mathematical point of view that is very simple. What you have is a projection problem in a Hilbert space, much like projecting a vector in <span class=""math-container"">$\mathbb{R}^n$</span> onto a subspace.</p>

<p>Let <span class=""math-container"">$(\Omega, \mathcal{F}, \mu)$</span> denote the underlying probability space. For the problem to make sense, consider the random variables with finite second moments, that is, the Hilbert space <span class=""math-container"">$L^2(\Omega, \mathcal{F}, \mu)$</span>. The problem now is this: given <span class=""math-container"">$X, Y \in L^2(\Omega, \mathcal{F}, \mu)$</span>, find the projection of  <span class=""math-container"">$Y$</span> onto the subspace <span class=""math-container"">$L^2(\Omega, \mathcal{F}_X, \mu)$</span>, where <span class=""math-container"">$\mathcal{F}_X$</span> is the <span class=""math-container"">$\sigma$</span>-subalgebra of <span class=""math-container"">$\mathcal{F}$</span> generated by <span class=""math-container"">$X$</span>. (Just as in the finite dimensional case, minimizing <span class=""math-container"">$L^2$</span>-distance to a subspace means finding the projection). The desired projection is <span class=""math-container"">$E(Y|X)$</span>, by construction. (This actually characterizes <span class=""math-container"">$E(Y|X)$</span>, if one inspects the proof of existence).</p>

<p><strong>Edit</strong></p>

<p>Re <em>""...,by construction.""</em></p>

<p>By definition, the conditional mean of <span class=""math-container"">$Y$</span> on <span class=""math-container"">$X$</span> is a random variable <span class=""math-container"">$\psi$</span>
with the following two properties:</p>

<ol>
<li><p><span class=""math-container"">$\psi$</span> lies in <span class=""math-container"">$L^2(\Omega, \mathcal{F}_X, \mu)$</span>.</p></li>
<li><p><span class=""math-container"">$E[\psi 1_{A}] = E[Y 1_{A}]$</span>, for all <span class=""math-container"">$A \in \mathcal{F}_X$</span>, which implies that <span class=""math-container"">$E[\psi g] = E[Y g]$</span>, for all <span class=""math-container"">$g \in L^2(\Omega, \mathcal{F}_X, \mu)$</span>, by standard argument use denseness of simple functions.  </p></li>
</ol>

<p>Standard Hilbert space projection arguments show that such a <span class=""math-container"">$\psi$</span> always exists and is unique.</p>

<p>This applies to any Hilbert space. The above can be re-phrased verbatim for, say, <span class=""math-container"">$\mathbb{R}^n$</span>:</p>

<blockquote>
  <p>Let <span class=""math-container"">$Y \in \mathbb{R}^n$</span> and <span class=""math-container"">$V$</span> be a subspace. Then the projection of
  <span class=""math-container"">$Y$</span> onto <span class=""math-container"">$V$</span> is characterized by the same two properties:</p>
  
  <ol>
  <li><p><span class=""math-container"">$\psi$</span> lies in <span class=""math-container"">$V$</span>.</p></li>
  <li><p><span class=""math-container"">$\langle \psi, g \rangle = \langle Y, g \rangle$</span>, for all <span class=""math-container"">$g \in V$</span>.</p></li>
  </ol>
</blockquote>

<p><strong>Note</strong>
This discussion is restricted to <span class=""math-container"">$L^2$</span> random variables, as the original question does implicitly. Conditional mean in general is defined for <span class=""math-container"">$L^1$</span> random variables, which is a larger than <span class=""math-container"">$L^2$</span>. <span class=""math-container"">$L^1$</span> is a Banach space and conditional mean is still a projection, in an appropriate sense.</p>
",2013-11-03 03:17:13.590
58748,23249.0,1,,,,ANOVA with very uneven sample sizes-- Can fit be improved through reference to a more complete dataset?,<bayesian><anova><small-sample>,CC BY-SA 3.0,"<p>I am analyzing a set of costs for contractor services for a particular city. These data have been extremely difficult to collect due the difficulty of contacting contractors and reluctance of contractors to divulge their pricing information. Factors are ""service type"" and ""contractor"".</p>

<p>We generally have quotes from more than one contractor on each service and a number of contractors perform most of the services. For a few services we have fairly good replication (i.e. ~15 quotes), for some of the services we have some degree of replication (i.e. 5-10 quotes per service). But for about a third of the services replication is very poor, i.e. &lt;5 quotes per service.</p>

<p>The data have proven extremely challenging and time consuming to collect and I doubt if there will be more.</p>

<p>The upside to this story is that there are several construction cost estimation tools that can give us estimates of the costs of these services. And while these costs wouldn't be specific to the area we're interested in, we think we can assume that the prices of the services relative to each other would be similar to what we expect to see in the area we're looking at.</p>

<p>Sometimes elegance is the first casualty of pragmatism, but this is what I'm thinking of doing-- I am planning to collect a parallel set of cost estimates from these cost estimation tools and compare the relationships between the results of our surveys to the relationships of costs within this set of parallel cost data.</p>

<p>Essentially using the better replicated data points in our dataset as anchors and generating expectations for our poorly replicated data points based on the relationships from the parallel dataset.</p>

<p>I'm not even sure what this approach would be called and it feels like a very specific type of issue, so it's difficult to research online. But I'm wondering if there's any theoretical basis for this type of approach that's been worked out. Shall I buy a book on Bayesian stats?</p>

<p>Most of my experience in grad school was analyzing data from designed experiments, so this sort of situation is interesting, but definitely foreign to me.</p>

<p>Thanks in advance for your help.</p>
",2013-11-03 03:18:54.287
58749,22923.0,2,,58602.0,,,,CC BY-SA 3.0,"<p>If your dynamic system is 
$$ x_t = A_t x_{t-1} + \eta_t $$
$$ y_t = B_t x_t + \varepsilon_t $$
Then when people say system matrices $A_t, B_t$ should be deterministic, this means that Kalman Filter gives you an estimate of state $x_t$ <em>conditional</em> on past and current values of parameters $$\mathbf E\left(x_t|\,y_t,\dots,y_1, \,A_t,\dots,A_1, \,B_t, \dots, B_1\right).$$<br>
So when you do a filtering step to estimate this conditional expectation of state, you consider those matrices to be already known (observed) rather than unknown and random. Of course they can be realizations of some external random process (which is often the case) or be deterministic functions of time - this doesn't matter much. </p>

<p>What seems authors in above paper describe in 3.2 is an extension of KF when they assume $A_t, B_t$ to be random but they don't what to condition on their values when filtering. So they don't assume matrices to be known at the moment of filtering, but rather assume that they come from a distribution with known mean/variance.</p>
",2013-11-03 03:20:40.113
58750,9049.0,2,,58288.0,,,,CC BY-SA 3.0,"<p>Using your example for your first question: Yes, parent's education level is a fixed effect as you set them to be at any arbitrary number you can/want to use. There is no ""randomness"" regarding the levels, you fix them, you fully observe them. </p>

<p>Regarding your second question: The <a href=""http://scholar.harvard.edu/files/nunn/files/nunn_wantchekon_aer_2011.pdf"" rel=""nofollow"">Nunn &amp; Wantchekon</a> article is a good clarification for what you want. I am no expert on the matter but I think your intuition is correct. Ultimately it is an OLS; the way they test for significance is somewhat specialized but otherwise nothing crazy. So... ""Yes"" is also your second question's answer. Î. &amp; W. strive to have a somewhat rigorous falsification of their findings, checking of their ""unknown unknowns"" but yes, otherwise nothing too exotic. In general, if there is an observable fixed effects you can include it in the OLS to start with. If you suspect there are some unobservable ones you try to control for them by adding dummies; surrogate variables if you like. </p>

<p>As mentioned, I am not an expert on panel data and I haven't work in economics ever so take it with a grain of salt. I believe your statistical intuitions are not wrong though.</p>
",2013-11-03 05:20:06.133
58751,23292.0,2,,13631.0,,,,CC BY-SA 3.0,"<p>The Hidden markov model is the sequential version of Naive Bayes.  In naive bayes, you have a label with several possible values (in your case 0/1) and a set of features.  The value for y is selected by modeling p(features | label) * p(label).  </p>

<p>In a hidden markov model, a sequence of labels is predicted by modeling p(label | previous label) and P(features | label).  </p>
",2013-11-03 06:07:01.217
58772,21823.0,2,,58435.0,,,,CC BY-SA 3.0,"<p>I had a brief run in with image recognition and classification.</p>

<p>Random Forests is an easy to use technique. I've implemented it on R, it should be available on Weka as well. Ease of use trumps prediction accuracy though. If you have a large enough training set, it can classify multiple labels.</p>

<p>It worked to recognize handwritten digits quite well, but if your images are more complex, then only a trial would tell you if it does well.</p>
",2013-11-03 17:32:41.363
58752,14253.0,1,,,,Comparing means with unequal variance and very different sample sizes,<statistical-significance><t-test><mean><sample-size><small-sample>,CC BY-SA 3.0,"<p>I am trying to compare the means of the same variable between men and women. This is the statistics:</p>

<pre><code>     N        Mean        Variance    Coef. Var.     Gender    
   2000      26.12         10.89         0.13         Male        
     50      56.10         25.01         0.09        Female
</code></pre>

<p>Neither variable is normally distributed but taking the log makes it pretty darn close. What is the appropriate way to test the means between males and females? Should I use the log or not? Any additional advice using Stata would be helpful.</p>

<p>My initial reaction is that females fare better than men, but I want to be statistically rigorous.  </p>
",2013-11-03 06:22:09.433
58753,21029.0,2,,58752.0,,,,CC BY-SA 3.0,"<p>The traditional test for comparing two sample means is the t-test. There are no assumptions about the sizes of the samples, so it is OK if they are different. </p>

<p>However, you touch upon the normality assumption. Even if the population is not normally distributed, the Central Limit Theorem allows us to infer normality as the sample sizes increase. This means your test will be approximate, but the sample size for female is a little low.</p>

<p>Finally, the result of the t-test will be different for the original and log-ed data. Do you have a specific reason based on your data to use the logarithm? Perhaps there is another assumption you would like to test about the behavior of the log of your data? Do not take the log simply to create a normal curve if there is no deeper meaning, but for fun compare the difference between the two results anyway!</p>
",2013-11-03 06:50:17.713
58754,10372.0,1,,,,Naive SE vs Time Series SE: which statistics should I report after Bayesian estimation?,<bayesian><jags><bugs>,CC BY-SA 3.0,"<p>I am new to Bayesian estimation.</p>

<p>When I do some estimations with JAGS, I find there are statistics called Naive SE and Time Series SE.</p>

<p>What exactly do they mean? Is it necessary that I report one or both of them as part of the estimation result?</p>
",2013-11-03 07:09:54.383
58755,21840.0,1,,,,Show $Y$ converges to $a$,<self-study><density-function><convergence>,CC BY-SA 3.0,"<p>Given:
$f_{Y_{(1)}}(y) = nbe^{-nb(y-a)}$, where $b&gt; 0$ and $y \geq a$.</p>

<p>Show that as $n \rightarrow\infty$, $Y_{(1)}$ converges to $a$ in probability.</p>

<p>I have calculated $E[Y_{(1)}] = \frac{1}{nb} + a$</p>

<p>Which theorem should I apply to show convergence. I was trying to use Chebyshev's inquality. </p>

<p>[EDIT]</p>

<p>If I want to find to what $Y_{(1)}$ converges in distribution, is this the right way to do it:</p>

<p>$F_{Y_{(1)}} = 1-e^{-nb(y-a)}$ </p>

<p>As $n \rightarrow \infty $</p>

<p>$F_{Y_{(1)}} = 1, y &lt; a$</p>

<p>$F_{Y_{(1)}} = 0, y \geq a$</p>

<p>$P(|Y_{(1)}|&lt; y) = P(|Y_{(1)}|&lt; \epsilon)$ [replacing $y$ with $\epsilon$] = $1-e^{-nb(\epsilon-a)}$</p>

<p>As $n \rightarrow \infty $</p>

<p>$P(|Y_{(1)}|&lt; y) \rightarrow 1$; So $ Y_{(1)} \rightarrow Y$ in distribution. So the limiting distribution is degenerate.</p>

<p>Please let me know if this approach is correct.</p>
",2013-11-03 07:43:41.483
58756,21840.0,1,,,,Convergence in distribution,<self-study><normal-distribution><mathematical-statistics><convergence><central-limit-theorem>,CC BY-SA 3.0,"<p>For a statistic $T_n = \frac{1}{n} \sum_{i=1}^nY_i - \frac{1}{a}$.
Prove directly (without CLT) that scaled and appropriately shifted version of $T_n$ converges in distribution to $N(0,1)$.</p>

<p>[EDIT]</p>

<p>$f(y|a,b)=ae^{-a(y-b)}$ for $ y\geq b$</p>

<p>How should I approach the problem?</p>

<p>[EDIT]</p>

<p>I thought that if I could find the expected value and variance of $T_n$ and then represent it in $N(\mu,\sigma^2)$</p>

<p>for $f(y)$, $E[Y] = b+1/a$, $Var [Y] = 1/a^2$</p>

<p>For $Tn$,</p>

<p>$Var[T_n] = Var[\frac{1}{n} \sum_{i=1}^nY_i - \frac{1}{a}]$ = $a^2/n$</p>

<p>$E[T_n] = E [\frac{1}{n} \sum_{i=1}^nY_i - \frac{1}{a}] = b$</p>

<p>[EDIT]</p>

<p>$M_X(t)=\frac{ae^{bt}}{(a-t)}$</p>

<p>How should I go from here?</p>
",2013-11-03 07:52:04.963
58757,594.0,2,,58752.0,,,,CC BY-SA 3.0,"<p>Taking logs and testing the mean on the log scale would normally not correspond to a difference in means on the original scale.</p>

<p><em>However</em>:</p>

<p>[Edit: my comments apply to an earlier version of the data, and don't apply to the data that are presently in the question. As such, my comments really apply to the situation where the coefficient of variation in two close-to-lognormal samples are very similar, rather than to the case now at hand.]</p>

<p>The coefficient of variation is almost identical in the two samples, which does suggest that you might consider these as having a scale shift; if you think the logs look reasonably close to normal, then that would suggest lognormal distributions with common coefficient of variation. In that case a difference of means on the log-scale would actually indicate a scale-shift on the original scale (and hence that one of the means is a multiple of the other mean on the original scale).</p>

<p>That is, under an assumption of equal variance and normal distribution on the log-scale, a rejection of equality of means implies that the means on the original scale have a ratio that differs from 1. </p>

<p>It seems like that would be a reasonable assumption.</p>

<p>There are other things you could do, though.</p>
",2013-11-03 09:20:02.827
58759,503.0,4,,,,,,CC BY-SA 4.0,"A matrix is singular when its determinant is 0; for such matrices, the inverse is not defined. Also, related topics like singular fits",2013-11-03 11:27:30.657
58758,503.0,5,,,,,,CC BY-SA 3.0,,2013-11-03 11:27:30.657
58761,503.0,5,,,,,,CC BY-SA 3.0,"<p>A <em>kernel</em> in the context of kernel smoothing is a local similarity function $K$, which must integrate to 1 and is typically symmetric and nonnegative. Kernel smoothing uses these functions to interpolate observed data points into a smooth function.</p>

<p>For example, Watson-Nadaraya kernel regression estimates a function $f : \mathcal X \to \mathbb R$ based on observations $\{ (x_i, y_i) \}_{i=1}^n$ by
$$
\hat{f}(x) = \frac{\sum_{i=1}^n K(x, x_i) \, y_i}{\sum_{i=1}^n K(x, x_i)}
,$$
i.e. a mean of the observed data points weighted by their similarity to the test point.</p>

<p>Kernel density estimation estimates a density function $\hat{p}$ from samples $\{ x_i \}_{i=1}^n$ by
$$
\hat{p}(x) = \frac{1}{n} \sum_{i=1}^n K(x, x_i)
,$$
essentially placing density ""bumps"" at each observed data point.</p>

<p>The choice of kernel function is of theoretical importance but typically does not matter much in practice for estimation quality. (Wikipedia has <a href=""https://en.wikipedia.org/wiki/Kernel_%28statistics%29#Kernel_functions_in_common_use"" rel=""nofollow"">a table</a> of the most common choices.)
Rather, the important practical problem for kernel smoothing methods is that of <em>bandwidth selection</em>: choosing the scale of the kernel function. Undersmoothing or oversmoothing can result in extremely poor estimates, and so care must be taken to choose an appropriate bandwidth, often via cross-validation.</p>

<hr />

<p>Note that the word ""kernel"" is also used to refer to the kernel of a reproducing kernel Hilbert space, as in the ""kernel trick"" common in support vector machines and other kernel methods. See <a href=""https://stats.stackexchange.com/tags/kernel-trick/info""><code>[kernel-trick]</code></a> for this usage.</p>
",2013-11-03 11:37:38.377
58760,503.0,4,,,,,,CC BY-SA 3.0,"Kernel smoothing techniques, such as kernel density estimation (KDE) and Nadaraya-Watson kernel regression, estimate functions by local interpolation from data points. Not to be confused with [kernel-trick], for the kernels used e.g. in SVMs.",2013-11-03 11:37:38.377
58762,23302.0,1,,,,How can I test the difference of two Weibull distributions?,<distributions><t-test><weibull-distribution>,CC BY-SA 3.0,"<p>I have two Weibull distribution sets from two wind datasets in order to check whether they are same.</p>

<p>I thought a 2 sample t-test would be applicable but I couldn't find any ways to do that on the Internet.</p>

<p>Does anyone know what type of test is applicable to my purpose? and what R function can you recommend?</p>

<p>Plus, if it turned out that there is a difference between the two datasets, can I just fit a linear line between the datasets?</p>
",2013-11-03 12:14:55.333
58763,23303.0,2,,53261.0,,,,CC BY-SA 3.0,"<p>This is eta-squared and it is a fairly poor measure of effect size (partial eta-squared is often reported in statistical software such as SPSS when calculating ANOVA)</p>
",2013-11-03 13:02:01.453
58764,15870.0,1,,,,The mode of multivariate Gamma distribution,<gamma-distribution><multivariate-distribution><mode>,CC BY-SA 3.0,"<p>Let X, Y, Z be i.i.d. distributed Gamma random variables. What could the mode of the vector $(X, X+Y, X+Y+Z)$ be?</p>

<p>Does the mode of a random vector equal the combination of the marginal modes?</p>
",2013-11-03 13:51:08.163
58765,12683.0,2,,58619.0,,,,CC BY-SA 3.0,"<p>This is a central composite design so I assume you're fitting a full second-order model for the mean response $\mathrm{E}(Y)$ on continuous predictors $x_1$, $x_2$, &amp; $x_3$</p>

<p>$$\mathrm{E}(Y)= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 +
\beta_{12} x_1 x_2 + \beta_{13} x_1 x_3 + \beta_{23} x_2 x_3 +
\beta_{11} x_1^2 +\beta_{22} x_2^2 +\beta_{33} x_3^2$$</p>

<p>&amp; estimating the coefficients $\beta$ by ordinary least squares.</p>

<p>I fitted the model: the estimated mean response has a stationary point, a maximum, of $\mathrm{E}(Y)=102$ at $x_1=0.62, x_2=-0.11, x_3=0.13$. (You can check for yourself by differentiating the equation for the response with respect to each predictor, setting each derivative to zero (all slopes are zero at a stationary point), &amp; solving the resulting simultaneous equations.) Contour plots at slices through the stationary point are a good way of visualizing the fitted model:
<img src=""https://i.stack.imgur.com/HzE1T.png"" alt=""response surface contour plots""></p>

<p>The $95\%$ confidence interval for $\mathrm{E}(Y)$ at the maximum is $(89,114)$, &amp; the $95\%$ prediction interval for $Y$ at the maximum is $(68,136)$. These are rather wide compared to the range of the response across the whole design. Indeed the residual standard deviation is $14$ (just look at the spread of responses over your centre points). I don't know the context of your experiment, but in many situations this would be cause for concern&mdash;are there other factors significantly contributing to process variability that you haven't taken into account?</p>
",2013-11-03 14:14:02.213
58766,9175.0,2,,58755.0,,,,CC BY-SA 3.0,"<p>You have $E[Y_{(1)}] = \frac{1}{nb} + a$. </p>

<p>$Lim_{n-&gt;\infty} = \frac{1}{\infty} + a$</p>
",2013-11-03 14:41:17.737
58767,21346.0,1,58788.0,,,Derivative of the transformed explanatory variable,<data-transformation><marginal-distribution>,CC BY-SA 3.0,"<p>I have an explanatory variable that is transformed as suggested in footnote 25 of <a href=""http://www.federalreserve.gov/pubs/feds/2013/201353/201353pap.pdf"" rel=""nofollow"">the article</a> as follows (the explanatory variable is continuous and can take negative, zero or positive values)
\begin{equation}
y=\text{sign}(x)\,\log{(|x|+1)}
\end{equation}
where $\text{sign}(x)$ takes a value of $1$ if $x&gt;0$, $0$ if $x=0$, and $-1$ if $x&lt;0$. Let's suppose my dependent variable is $y$ (not transformed). Now I need to take the derivative of $y$ with respect to $x$ to find out the marginal effect and elasticity. </p>

<p>I did this as follows: 
\begin{equation}
\frac{\partial y}{\partial x}=\text{sign}(x)\,\frac {1}{(|x|+1)}\,\frac{\partial|x|}{\partial x}
\end{equation}</p>

<p>As far as I understand, $\frac{\partial|x|}{\partial x} =1$ if $x&gt;0$ and $\frac{\partial|x|}{\partial x} =-1$ if $x&lt;0$, and is not defined for $x=0$. My question is how I compute the marginal effect for the observation with the value $x=0$ with this transformation. </p>
",2013-11-03 15:01:42.123
58768,20304.0,1,58773.0,,,k-fold cross-validation for large data sets,<cross-validation>,CC BY-SA 3.0,"<p>I am performing 5-fold cross-validation on a relatively large data set and I have noticed that the validation error for each of the 5 training sets are very similar.  So I guess, in this case, cross-validation is not very useful (it would be about the same as just using one training and test set).  So I was wondering if I am working with a special case, or is this the case for all large data sets.  I'm thinking that perhaps if you have enough training examples, the average cross-validation score would not be very different than the score for one training and test set. Is this intuitition correct?</p>
",2013-11-03 16:36:30.537
58769,22415.0,1,58795.0,,,Expected value of a series of random variables in a markov chain,<probability><markov-chain>,CC BY-SA 3.0,"<p>I have a Markov Chain such that $X_n = max(X_{n-1}+\xi _n,0)$ where the $\xi_n$ series is independent and identically distributed. I want to show that if $\mathbb E(\xi_n) &gt; 0$ (where  $\mathbb E(\xi_n)$ is the expected value of $\xi_n$) then$\frac{X_n}{n}$ tends to $\mathbb E(\xi_n)$ as n approaches infinity for any choice of $X_0$.</p>

<p>And... I have no idea how to show this. I know that repeated application of the above will yield $X_n = max(X_0+\sum\limits_{i=1}^n\xi_i, \sum\limits_{i=2}^n\xi_i, \sum\limits_{i=3}^n\xi_i, ..., 0)$, but I'm stuck here.</p>
",2013-11-03 16:57:39.573
58770,23307.0,1,59372.0,,,Probability of drawing a given word from a bag of letters in Scrabble,<probability><games><combinatorics>,CC BY-SA 3.0,"<p>Suppose you had a bag with $n$ tiles, each with a letter on it. There are $n_A$ tiles with letter 'A', $n_B$ with 'B', and so on, and $n_*$ 'wildcard' tiles (we have $n = n_A + n_B + \ldots + n_Z + n_*$). Suppose you had a dictionary with a finite number of words.</p>

<p>You pick $k$ tiles from the bag without replacement.</p>

<p><strong>How would you compute (or estimate) the probability that you can form a given word, of length $l$ (with 1 &lt; $l$ =&lt; $k$)  from the dictionary given the $k$ tiles selected?</strong></p>

<p>For those not familiar with Scrabble (TM), the wildcard character can be used to match any letter. Thus the word 'BOOT' could be 'spelled' with the tiles 'B', '*', 'O', 'T'. The order in which the letters are drawn does not matter.</p>

<p>Suggestion: in order to simplify the writing of answers, it might be better to just answer the question: what is the probability of having the word 'BOOT' among your possible moves after drawing 7 letters from a fresh bag.</p>

<p><em>(the problem's introduction has been copied from this similar <a href=""https://stats.stackexchange.com/questions/3779/probability-of-not-drawing-a-word-from-a-bag-of-letters-in-scrabble"">question</a>)</em></p>
",2013-11-03 17:15:02.737
58771,23308.0,1,,,,Statistical significance in canonical correlation analysis,<hypothesis-testing><statistical-significance><p-value><canonical-correlation>,CC BY-SA 3.0,"<p>I do canonical correlation analysis between two multivariate datasets $X$ and $Y$. For each pair of canonical variates (x-y pair) I get the canonical correlation coefficient. How can I test its statistical significance?</p>
",2013-11-03 17:24:26.470
58773,,2,,58768.0,user88,,,CC BY-SA 3.0,"<p>It is certainly adds value to a single test because you get a stronger justification that your estimated accuracy is correct.</p>

<p>Large dataset certainly helps in making robust, accurate models though it won't bias the cross-validation on its own. The only possible problem you should check for is whether the set contains significant fraction of duplicated objects -- this may happen if the number of attributes is very small in comparison.</p>
",2013-11-03 18:09:20.110
58774,21523.0,2,,54915.0,,,,CC BY-SA 3.0,"<p>After read carefully Pettitt paper, I can understand the method and check my code.</p>

<p>Pettitt starts with a general method expresed:</p>

<p>$$U_{t,T} = \sum_{i=1}^{t} \sum_{j=t+1}^{T} D_{ij}$$</p>

<p>where $D_{ij} = sign(X_i - X_j)$</p>

<p>Then Pettitt follows expressing a test for discrete Bernoulli and Binomial data and two variants for continuos data.</p>

<p>For discrete test, he use $U_{t,T} = T \cdot (S_t - \frac{tS_t}{T})$, where $S_t = \sum_{1}^{T} X_j$, $S_T = \sum_{1}^{T} X_j$ and $X_j$ is eval function expresed like Bernoulli serie where values are 0 or 1.</p>

<p>For continuos test, he use:</p>

<ol>
<li>$U_{t,T} = 2 \cdot W_t - t(T+1)$, when $W_t=\sum R_j$ and $R_j$ are the rank or the data. (There is a variation when ties exists.)</li>
<li>$U_{t,T} = U_{t-1,T} + V_{t,T}$, with $t=2,\dots,T$ and $V_{t,T} = \sum_{j=1}^{T} sign(X_t - X_j)$.</li>
</ol>

<p>Then, my original code are variant 1 for continuos test and results are correct, except for guessing critical values in Pettitt series results. </p>

<p>I calculate these critical values solving $K_T$ from 
$P_{OA} = 2e^{(\frac{-6{K_{T}}^2}{T^3+T^2})}$, but although $P_{OA}$ is solved correctly, the critical value is not. Why? Actually, I don't know and I can't reach others papers where shows how to calculate table expresed in, for example, Sahin et al paper (link in previous comments).</p>

<p>So that, I change my function code in order to include all variants and $P_{OA}$ calcul. Here are:</p>

<pre><code>&lt;!-- language: lang-R --&gt;
pettitt&lt;-function(x,alpha=0.01,method=c(""discrete"",""continuos""),
                  alternative=c(""rank"",""variation"")) {
   # Pettitt AN. 1979 A non-parametric approach to the change point detection.
   # (SecciÃ³n 2.3)
   #
   # x is a numeric vector
   # alpha is a integer
   x&lt;-na.omit(x)
   orden&lt;-rank(x)
   T&lt;-length(x)
   method&lt;-match.arg(method)
   alternative&lt;-match.arg(alternative)
   #
   U.t.T&lt;-c()
   V.i.T&lt;-c()
   P.o.a&lt;-NULL
   P.o.a.p&lt;-NULL
   P.o.a.n&lt;-NULL
   k.T&lt;-NULL
   k.T.p&lt;-NULL
   k.T.n&lt;-NULL
   #
   if (!is.numeric(x))
      stop(""'x' must be a numeric vector)
   #
   # Discrete values
   if (method == ""discrete"") {
      x.j&lt;-sign(x)
      x.j[which(x.j==-1)]&lt;-0
      S.T&lt;-sum(x.j)
      for (i in 1:T) {
        S.t&lt;-sum(x.j[1:i])
        U.t.T&lt;-c(U.t.T,
           T*(S.t-(i*S.T/T)))
      }
      k.T&lt;-max(abs(U.t.T))
      k.T.p&lt;-max(U.t.T)
      k.T.n&lt;-min(U.t.T)
      P.o.a&lt;-exp((-2*k.T.p^2)/(S.T*(T^2-T*S.T)))
      critical&lt;-sqrt((log(alpha)*(S.T*(T^2-T*S.T)))/-2)
   }
   #
   # Continuos value.
   if (method == ""continuos"" &amp; alternative == ""rank"") {
      TIES&lt;-length(unique(x)) &lt; T
      if (!TIES) { 
        for (i in 1:T) {
           U.t.T&lt;-c(U.t.T,
              2*(colSums(as.matrix(orden[1:i])))
                 -(i*(T+1))
           )
         }
      } else {
        frequency&lt;-as.vector(table(x))
        total.frequency&lt;-sum(frequency)
        for (i in 1:length(frequency)) {
           U.t.T&lt;-c(U.t.T,
              1-(total.frequency*(frequency[i]^2-1))/(T*(T^2-1))
           )
        }
      }
      k.T&lt;-max(abs(U.t.T))
      P.o.a&lt;-2*exp((-6*k.T^2)/(T^3+T^2))
      critical&lt;-sqrt((log(alpha/2)*(T^3+T^2))/-6)
   }
   if (method == ""continuos"" &amp; alternativa == ""variation"") {
      V.i.T&lt;-matrix(rep(NA,T^2),ncol=T)
      for (i in 1:T) {
        for (j in 1:T) {
           V.i.T[j,i]&lt;-sign(x[i]-x[j])
        }
        if (i==1) {
           U.t.T&lt;-sum(V.i.T[,i],na.rm=T)
        } else {
           U.t.T&lt;-c(U.t.T,
              U.t.T[(i-1)]+sum(V.i.T[,i],na.rm=T)
           )
        }
      }
      V.i.T&lt;-colSums(V.i.T,na.rm=T)
      k.T.p&lt;-max(U.t.T)
      k.T.n&lt;-min(U.t.T)
      k.T&lt;-max(abs(U.t.T))
      P.o.a.p&lt;-exp((-6*k.T.p^2)/(T^3+T^2))
      P.o.a.n&lt;-exp((-6*k.T.n^2)/(T^3+T^2))
      P.o.a&lt;-2*exp((-6*k.T^2)/(T^3+T^2))
      critical&lt;-sqrt((log(alpha/2)*(T^3+T^2))/-6)
   }
   output&lt;-list(U.t.T,V.i.T,P.o.a,P.o.a.p,P.o.a.n,k.T,k.T.p,k.T.n,critical)
   return(output)
}
</code></pre>

<p>If anyone knows how calcul critical values are welcome.</p>

<p>So many thanks.</p>
",2013-11-03 18:31:04.590
58775,23309.0,1,58776.0,,,Beyond the normal distribution: what if a particular distribution can't be assumed,<distributions><self-study>,CC BY-SA 3.0,"<p>In a random sample of 150 community college students, the mean number of hours spent studying per week is 11.7 hours and the standard deviation is 4 hours.</p>

<p>Without assuming anything about the distribution of the number of hours community college students study per week, at least what percentage (approximately) of the students study between 5.3 and 18.1 hours per week?</p>
",2013-11-03 18:56:00.920
58776,15827.0,2,,58775.0,,,,CC BY-SA 3.0,"<p>You can say for your sample data that whatever percent it is lie between 5.3 and 18.1 hours/week. </p>

<p>You may be reaching for Chebyshev's inequality. If so, go for e.g. <a href=""http://en.wikipedia.org/wiki/Chebyshev%27s_inequality"" rel=""nofollow"">http://en.wikipedia.org/wiki/Chebyshev's_inequality</a> and don't return here. The ""at least"" suggests a problem with this flavour. </p>

<p>You can't say that otherwise without making some assumptions. You can calculate the probability that values lie between 5.3 and 18.1 hours/week, which is 1.6 SD either side of the mean, easily if you do assume a normal distribution. </p>

<p>Your question sounds like self-study, so I will stop there. </p>
",2013-11-03 19:14:08.280
58777,9456.0,1,,,,Confusion related to intractability in topic models,<optimization><inference><posterior><topic-models>,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/tb2va.png"" alt=""enter image description here""></p>

<p>I was reading this paper related to <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.4050&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">topic models</a>. I am a bit confused why the marginal likelihood is not tractable and how converting the graphical model into the new one actually helps. First I don't understand how the coupling between $\beta$ and $\theta$ result in making it intractable. It points to a reference. But I cannot access the article (Dickey 1983). So I cannot understand why it is intractable. can anyone please provide me some information?</p>
",2013-11-03 19:28:19.663
58778,22942.0,1,,,,Error implemented a specific factorial design in Minitab (default generators),<experiment-design><minitab>,CC BY-SA 3.0,"<p>I'm intending to implement the following factorial design</p>

<p>I wish to obtain this alias structure - with a 2^(5-2) factorial design. </p>

<pre><code>I+ACE+BDE+ABCD
A+CE+ABDE+BCD
B+ABCE+DE+ACD
C+AE+BCDE+ABD
D+ACDE+BE+ABC
E+AC+BD+ABCDE
AB+BCE+ADE+CD
AD+CDE+ABE+BC
</code></pre>

<p>I've obtained this (the alias structure) by hand, using the design generators I=ACE and I=BDE. I've controlled my alias structure and confirmed it is the correct one.</p>

<p>However, using Stat->DOE->Create factorial design-> 2 level design (specify generators)</p>

<p>and using 5 factors</p>

<p>And subsequently using </p>

<p>Default generators: ACE, BDE</p>

<p>Gives the error ""Block are aliased with the mean"".</p>

<p>Why doesn't this work?</p>
",2013-11-03 19:43:29.497
58779,15120.0,1,,,,Issue in graph construction,<time-series><matlab><stochastic-processes><markov-process><fuzzy>,CC BY-SA 3.0,"<p>I have a symbolic representation of time series obtained from <a href=""http://www.cs.ucr.edu/~eamonn/SAX.htm"" rel=""nofollow"">SAX toolbox</a>. I was wondering if it is possible to construct a graph where each node represents a unique symbol and the edges represent the transition providing that there is no transition to itself. For ex, let the time series T of n=20 data points be represented as
 <code>T=[1 1 2 1 2 1 3 1 1 1 2 2 3 3 3 1 1 2 3 3 1]'</code> where number of alphabets used to symbolize = 3 and they are (1,2,3). I have combined co-occuring symbols together so, the compressed time series becomes </p>

<pre><code>T' = [1 2 1 2 1 3 1 2 3 1 2 3 1]'
</code></pre>

<p>In order to construct a graph (Esp. <a href=""http://en.wikipedia.org/wiki/Fuzzy_cognitive_map"" rel=""nofollow"">fuzzy cognitive map</a> ) with fuzzy membership values from T' where the nodes will be (1,2,3), there will be an edge from $Node_i$ to $Node_j$ and it will have a weight $W_{ji}$. How do I find the weights ? I do not know which theory to search for this kind of problem and so if there are any ideas as to what can form the weights. Thank you</p>
",2013-11-03 20:04:07.333
58780,2015.0,1,58844.0,,,Community finding algorithms in large heterogeneous networks,<clustering><graph-theory><networks><social-network>,CC BY-SA 3.0,"<p>Consider a network that consists of vertices with various meanings. For example: stack overflow users, keywords and user location when asking/answering a question. In this network, when a user asks a question its ID node is linked to several keyword vertices and to a vertex representing users' geographical location. </p>

<p>I would like to ask a general and vague question: are there any meaningful communities in this network? One approach would be to link (lets say) users using other vertices and then analyzing the resulting graph of users. However, are there approaches that retain the heterogeneous types of information in the graph? Are there metric measurements that take into consideration various types of nodes?</p>
",2013-11-03 20:11:09.887
58781,211.0,1,58783.0,,,"How is this sampling called? (getting a ""representative"" sample set for teaching)",<sampling><teaching>,CC BY-SA 4.0,"<p>I know of random sampling and stratified sampling, but what I am not sure what is the name of the type of sampling I need:</p>

<p>I wish to work-out an example with my students of simple correlation/regression. I found the following nice dataset: ""<a href=""http://lib.stat.cmu.edu/DASL/Stories/WhendoBabiesStarttoCrawl.html"" rel=""nofollow noreferrer"">When do Babies Start to Crawl</a>"", but it has too many observations (12). And what I wish is something even smaller that we can do calculations on (let's say 6). I would like to sample these points, but I would rather do it in such a way so that the result of the analysis would remain as similar as possible to what I would get from the larger dataset.</p>

<p>So of course the SE would change, but I would like the correlation to be as close as possible, and if possible to have the range stay similar (I'm ignoring the outlier issue).</p>

<p>Does this type of situation have a known name which I can use when searching for good solutions? </p>
",2013-11-03 22:20:23.053
58797,23322.0,2,,58338.0,,,,CC BY-SA 3.0,"<p>Look at the ADF Unit Root Test section.</p>

<p>If your data is a random walk with drift, then it will be under the type 'Single Mean'. </p>

<p>For the ADF test, 
H0: Non-stationary
Ha: Stationary</p>

<p>if P-value &lt; 0.05, you reject the null hypo (H0) and conclude that data series is stationary. It should be as you already differenced the data once.</p>

<p>Under 'Pr &lt; Rho' which stands for the P-value of your Rho (autocorrelation), it is 0.0129 and &lt;0.0001 thus, we reject the null hypo and conclude that the data is stationary.</p>
",2013-11-04 03:44:35.610
58782,22507.0,2,,55361.0,,,,CC BY-SA 3.0,"<p>I am not going to analyze the code, but below is the solution.</p>

<p>Let</p>

<ul>
<li>P(loc60) be the probability that a random locomotive has number 60</li>
<li>P(N) be the prior probability that there are exactly N locomotives</li>
<li>P(loc60|N) be the probability that a random locomotive has number 60, if the total number of locomotives is N,</li>
<li>P(N|loc60) be the probability that there are exactly N locomotives, if a random locomotive has number 60</li>
</ul>

<p>Then</p>

<p>$$ P(N|\text{loc60}) = {P(\text{loc60}|N) P(N) \over P(\text{loc60})} = {P(\text{loc60}|N) P(N) \over \sum_M P(\text{loc60}|M)}$$</p>

<p>But $$ P(\text{loc60}|N) = \cases {1/N &amp; if $N\ge 60$ \\ 0 &amp; otherwise } $$</p>

<p>From now on, we assume that $N \ge 60$.</p>

<p>$$ P(N|\text{loc60}) = {P(N)/N \over \sum_{M=60}^\infty P(M)/M} $$</p>

<p>Now we should select P(N), otherwise we are stuck.  Since we don't know even the order of magnitude of P(N), it is reasonable to assume that $\log N$ is uniformly distributed between 0 and some $\log N_\max$ (i. e. the probability that $10^2\le N&lt;10^3$ is the same as the probability that $10^3\le N&lt;10^4$).  Guestimating $N_\max$ is a tricky task, but from my prior knowledge about railroads and locomotives, I can assume that $N_\max \gg 60$  .</p>

<p>The uniform distribution of $\log N$ means that $$P(N) = c(\log (N+1)-\log N) \approx c/N$$, where c is a constant independent on N.</p>

<p>Substituting this to the previous formula, we have:
$$ P(N|\text{loc60}) \approx {c/N^2 \over \sum_{M=60}^{N_\max} c/M^2} $$</p>

<p>But $$\sum_{M=60}^{N_\max} c/M^2 \approx \int_{60}^{N_\max} {c\over M^2}dM = {c \over 60} - {c \over N_\max} \approx {c\over60} $$</p>

<p>Now we have</p>

<p>$$ P(N|\text{loc60}) \approx {60/N^2} $$</p>

<p>What is the median value of N?  Let it be $N_\text{med}$ , then</p>

<p>$$ \int_{60}^{N_\text{med}} {60 \over N^2} dN = 1/2 $$</p>

<p>$$ 60/N - {60 \over N_\text{med}} = 1/2 $$</p>

<p>$$ N_\text{med} = 120 $$</p>

<p>If what we need is mathematical expectation rather than median, then</p>

<p>$$ E(N) = \int_{60}^{N_\max} {60\over N^2} N dN = 60 \log {N_\max \over 60} $$</p>

<p>From what I know about railroads, $N_\max$ should be between $10^3$ and $10^6$, so E(N) is somewhere between 170 and 600.</p>
",2013-11-03 23:06:15.900
58783,668.0,2,,58781.0,,,,CC BY-SA 3.0,"<p>I don't know whether it has a name, but I have used similar techniques to create synthetic datasets to answer questions on this site: they <strong>frame the problem as an optimization</strong> and then carry it out.</p>

<p>The methods to use for optimization depend on the problem.  In this case you can explore the entire collection of possible six-element samples of the dataset, because $\binom{12}{6} = 924$ is small.  In general you cannot perform an exhaustive search and have to be content with some form of randomized search, guided perhaps by methods of simulated annealing or genetic algorithms.  But for your purposes you don't need a truly optimal solution, so a blind search ought to work fine.</p>

<hr>

<p><strong>To illustrate,</strong> I saved the ""crawling"" dataset in a file and applied the <code>R</code> script below.  Its output lists the case numbers of the optimal subset and compares the statistics you would like to reproduce.  (I assumed these were the coefficients of the regression of mean crawling age against temperature, but almost any small set of statistics will work provided they are not sample-size dependent, as recognized in the question itself.)</p>

<pre><code>Sample: 2 3 4 5 9 12
         (Intercept) temperature
Original    35.70254 -0.07560731
Sample      35.70062 -0.07548532
</code></pre>

<p>In this plot of the data, the optimal subset is shown in red and the two fits in corresponding colors; they are indistinguishable.</p>

<p><img src=""https://i.stack.imgur.com/AdpUD.png"" alt=""Figure""></p>

<p><strong>To compare the exhaustive search to the blind random search,</strong> I set the random number seed to 17, the number of combinations to search to 924, and forced the code to perform the randomized search (thereby going to exactly the same computational effort, but with no guarantee of optimality).  The output this time was</p>

<pre><code>Sample: 3 5 8 10 11 12
         (Intercept) temperature
Original    35.70254 -0.07560731
Sample      35.70047 -0.075770
</code></pre>

<p>It is a different sample, but the results are almost as good as before.</p>

<hr>

<pre class=""lang-r prettyprint-override""><code>f &lt;- read.csv(""f:/research/R/crawling.txt"", sep=""\t"")
#
# Function to return the statistics to match in a sample.
#
get.coef &lt;- function(g) {
  coef(lm(avg_crawling_age ~ temperature, weights=n, data=g))
}
#
# Compute these statistics for all possible samples of a specified size.
#
n.max &lt;- 10^4 # Limits the execution time.
sample.size &lt;- 6
system.time( {
  if (choose(nrow(f), sample.size) &gt; n.max) {
    print(""Using randomized search."", quote=FALSE)
    samples &lt;- replicate(n.max, sample.int(nrow(f), sample.size))
  } else {
    samples &lt;- combn(1:nrow(f), sample.size)
  }
  x &lt;- apply(samples, 2, function(i) get.coef(f[i, ]))
})
#
# Compare these statistics, using their variation across all possible 
# samples to establish relative scales, to their values for the data
# and select the closest.  (One might retain the several best, rather
# than just the one best, and choose among them using additional qualitative
# criteria.)
#
delta &lt;- apply((x - get.coef(f)) / apply(x, 1, sd), 2, function(y) sum(y*y))
sample.cases &lt;- sort(samples[, which.min(delta)])
g &lt;- f[sample.cases, ]
#
# Check that the best match `g` reproduces the coefficients reasonably closely.
#
z &lt;- rbind(get.coef(f), get.coef(g))
rownames(z) &lt;- c(""Original"", ""Sample"")
cat(""Sample:"", sample.cases)
print(z)
#
# Plot the data and the subset to compare visually.
#
col.red &lt;- ""#ff000080""
plot(subset(f, select=c(temperature, avg_crawling_age)), cex=1.2)
points(subset(g, select=c(temperature, avg_crawling_age)), pch=19, col=col.red)
abline(get.coef(f), lwd=3, col=""Gray"")
abline(get.coef(g), lwd=3, lty=2, col=col.red)
</code></pre>
",2013-11-03 23:14:32.760
58784,14227.0,1,,,,Topic models evaluation in Gensim,<python><topic-models><natural-language>,CC BY-SA 3.0,"<p>I've been experimenting with LDA topic modelling using <a href=""http://radimrehurek.com/gensim/"" rel=""nofollow noreferrer"">Gensim</a>. I couldn't seem to find any topic model evaluation facility in Gensim, which could report on the perplexity of a topic model on held-out evaluation texts thus facilitates subsequent fine tuning of LDA parameters (e.g. number of topics). It would be greatly appreciated if anyone could shed some light on how I can perform topic model evaluation in Gensim. This question had also been posted on <a href=""https://stackoverflow.com/questions/19615951/topic-models-evaluation-in-gensim"">Stackoverflow</a>.</p>
",2013-11-03 23:23:45.773
58785,15972.0,2,,58720.0,,,,CC BY-SA 3.0,"<p>The law of total variance is the easiest way to do this. But there are several occasions when we don't know how many random variables we are dealing with (e.g. <a href=""http://en.wikipedia.org/wiki/Branching_process"" rel=""noreferrer"">branching processes</a> such as <a href=""http://en.wikipedia.org/wiki/Galton%E2%80%93Watson_process"" rel=""noreferrer"">Galton-Watson</a>, <a href=""http://en.wikipedia.org/wiki/Birth%E2%80%93death_process"" rel=""noreferrer"">birth-death processes</a>, <a href=""http://en.wikipedia.org/wiki/Queueing_theory"" rel=""noreferrer"">queues</a>) where <a href=""http://en.wikipedia.org/wiki/Probability-generating_function"" rel=""noreferrer"">probability-generating functions</a> are a useful technique. It is possible to derive mean and variance using the PGF, so I want to demonstrate how this can serve as an alternative. Why bother? One motivation is that this method will generalize easily to find any <a href=""http://en.wikipedia.org/wiki/Factorial_moment"" rel=""noreferrer"">factorial moment</a>, and hence any moment, of distribution.</p>

<p>A few general results: a PGF $G_X(z)=\mathbb{E}(z^X)$ has $\lim_{z\uparrow 1}\, G_X(z)=\lim_{z\uparrow 1}\, \mathbb{E}(z^X)=1$. Factorial moments are found by taking the limit of the appropriate derivative of the PGF as $z$ goes to 1 from below. So for a random variable $X$:</p>

<p>\begin{eqnarray}
\mathbb{E}(X) &amp;=&amp; \lim_{z\uparrow 1}\, G'_X(z)\\
\mathbb{E}(X(X-1)) &amp;=&amp; \lim_{z\uparrow 1}\, G''_X(z)\\
\mathbb{E}(X(X-1)(X-2)) &amp;=&amp; \lim_{z\uparrow 1}\, G'''_X(z) 
\end{eqnarray}</p>

<p>And so on for higher moments. The key here is that if $S_N=\sum_{i=1}^N X_i$ with iid $X_i$ then $G_{S_N}(z)=G_N(G_X(z))$. Proof:</p>

<p>\begin{eqnarray}
G_{S_N}(z) &amp;=&amp; \mathbb{E}_N(\mathbb{E}(z^{\sum_{i=1}^N X_i})) &amp;=&amp; \mathbb{E}_N(\mathbb{E}(\prod_{i=1}^N z^{X_i})) &amp;=&amp; \mathbb{E}_N(\prod_{i=1}^N (\mathbb{E}(z^{X_i})) \\
  &amp;=&amp; \mathbb{E}_N(\prod_{i=1}^N G_X(z)) &amp;=&amp; \mathbb{E}_N(G_X(z)^N) &amp;=&amp; G_N(G_X(z))
\end{eqnarray}</p>

<p>Also note $\lim_{z\uparrow 1}\, G_X(z)=\lim_{z\uparrow 1}\, G_N(z)=1$, $\lim_{z\uparrow 1}\, G'_X(z)=\mu_X$, $\lim_{z\uparrow 1}\, G'_N(z)=\mu_N$, $\lim_{z\uparrow 1}\, G''_X(z)=\mathbb{E}(X^2-X)=\sigma_X^2+\mu_X^2-\mu_X$ and $\lim_{z\uparrow 1}\, G''_N(z)=\sigma_N^2+\mu_N^2-\mu_N$.</p>

<p>Since $G_{S_N}(z)=G_N(G_X(z))$ we can use the chain rule to find the mean and variance of $S_N$:</p>

<p>\begin{eqnarray}
\mathbb{E}(S_N) &amp;=&amp; \lim_{z\uparrow 1}\, \frac{d}{dz}G_N(G_X(z))=\lim_{z\uparrow 1}\, G'_X(z)G'_N(G_X(z))=\mu_X \mu_N\\
\mathbb{E}(S_N(S_N-1)) &amp;=&amp; \lim_{z\uparrow 1}\, \frac{d^2}{dz^2}G_N(G_X(z))\\
\mathbb{E}(S_N^2-S_N) &amp;=&amp; \lim_{z\uparrow 1}\, \left(G''_X(z)G'_N(G_X(z))+G'_X(z)^2 G''_N(G_X(z))\right) \\
\mathbb{E}(S_N^2)-\mu_X \mu_N &amp;=&amp; (\sigma_X^2+\mu_X^2-\mu_X)(\mu_N)+(\mu_X)^2(\sigma_N^2+\mu_N^2-\mu_N) \\
\mathbb{E}(S_N^2) &amp;=&amp; \mu_N \sigma_X^2 + \mu_X^2 \sigma_N^2 + \mu_X^2 \mu_N^2 \\
\operatorname{Var}(S_N) &amp;=&amp; \mathbb{E}(S_N^2)-\mathbb{E}(S_N)^2=\mu_N \sigma_X^2 + \mu_X^2 \sigma_N^2 + \mu_X^2 \mu_N^2-(\mu_X \mu_N)^2 \\
\operatorname{Var}(S_N) &amp;=&amp; \mu_N \sigma_X^2 + \mu_X^2 \sigma_N^2
\end{eqnarray}</p>

<p>It's a little gruesome and there's no doubt the law of total variance is easier. But if the standard results are taken for granted, this is only a couple of lines of algebra and calculus, and I've given more detail than some of the other answers which makes it look worse than it is. If you wanted the higher moments, this is a viable approach. </p>
",2013-11-03 23:38:19.507
58786,594.0,2,,58781.0,,,,CC BY-SA 3.0,"<p>I don't think there's a specific name for it, but it's the kind of task I've undertaken a number of times in various guises.</p>

<p>While the number of ways of choosing a sample of size 6 from 12 (924) is already manageable, you greatly cut down the search space by saying ""of similar range"". </p>

<p>Looking at the plot:</p>

<p><img src=""https://i.stack.imgur.com/i2nS5.gif"" alt=""Crawling plot""></p>

<p>That pretty much limits you to choosing the leftmost point from the three points at the left and the rightmost point from the two points at the right, and the four interior points from the remaining points inside the ones you choose. </p>

<p>If we simplify that a little and simply split the data into three subgroups of size (from left to right) 3, 7 and 2 from which we choose 1,4 and 1 point, we get a total of </p>

<p>$\binom{3}{1} \binom{7}{4} \binom{2}{1} = 210$, combinations which is less than a quarter the size (almost small enough to examine by hand).</p>

<p>Edit: Since whuber has given a comprehensive answer (one which I happily upvoted), so I won't go through the details of how to do it in this cut down case, but you could apply similar techniques to the search over a smaller space.</p>

<hr>

<p><em>Working by hand</em></p>

<p>In the past when I've done it, I often do a hand search first, because it often turns out that by starting with a reasonable choice and tweaking it (swapping a few points in and out to improve the characteristics I want) I can often get something quite good enough in a few tries, saving the effort of coding a more formal search.</p>

<p>So for example, just by eye, I'd start with (say) these six points:</p>

<p><img src=""https://i.stack.imgur.com/rF0iv.png"" alt=""hand selected points""></p>

<p>And then by looking at the direction of deviations of the values of the sample statistics I want (like the correlation) from the values I want them to have, and the likely effect of swapping a point in the set with a point not in, I can usually get close enough for my purposes in a minute or two. </p>

<p>Perhaps surprisingly, a spreadsheet like Excel is often a good tool for this task. I mark what's in (/not in) the sample with say a column of 1's and 0's and compute the statistics I want from that. A few moments of choosing which points to put in or out (swapping the 1s and 0s) and its done.</p>

<p>Such approaches are also useful when making up data - simulate something close to what's desired, and then manipulate a few points (adding or removing or altering) to make it look closer to what you need. Again, a spreadsheet is often a handy tool for this type of task.</p>
",2013-11-04 00:14:48.820
58787,22507.0,2,,58752.0,,,,CC BY-SA 3.0,"<p>From the data you cannot infer that the variance between males and females is same, in fact the opposite is almost certainly true.  Also, since 50 is indeed a bit low, suppose you cannot assume normality.</p>

<p>Compare each female's value with the median of men's values.  If median female is neither better nor worse than a median male (null hypothesis), then each female will have 1/2 chance to be better than a median male.  The chance that K or less females are worse than a median male is $P(K) = 2^{-50} \sum_{m=0}^K {50 \choose K}$ .  Here we consider the error in the male's median to be negligible, since there are much more males than females, and the variance between males is smaller than the variance between females.</p>
",2013-11-04 00:16:05.017
58798,14253.0,1,,,,Understanding Differences of Mean between Two Groups,<statistical-significance><sample-size><inference><sample><matching>,CC BY-SA 3.0,"<p>I have a variable <code>test score</code> that is measured for an organization consisting of 2050 people. The goal is to test if men outperform women (or vice versa). Unfortunately, only 50 of the 2050 people are women. I was told that in order to run inference when there is such a disproportion of one group within a sample that I should pursue some sort of matching strategy. </p>

<p>First, is this true, that if I match on a few characteristics that I can run inference (e.g., ordinary least squares, linear probability models) and be less worried about the fact that there are so few females--since many males will drop out? </p>

<p>Second, is there anything I can do to compare the groups before the match even as an anecdote?</p>

<p>This is the data:</p>

<pre><code>           Test Score    Test Score
     N        Mean        Variance    Coef. Var.     Gender    
   2000      26.12         10.89         0.13         Male        
     50      56.10         25.01         0.09        Female
</code></pre>
",2013-11-04 03:58:12.543
58788,594.0,2,,58767.0,,,,CC BY-SA 3.0,"<p>While it looks like its the case that as written the derivative isn't defined there, let's look more carefully. Note that, apart from at zero, </p>

<p>$\text{sign}(x)$ and $\frac{\partial|x|}{\partial x}$</p>

<p>are both the same quantity and the inverse of each other.</p>

<p>That is, except at 0, </p>

<p>\begin{equation}
\frac{\partial y}{\partial x}=\frac {1}{(|x|+1)}
\end{equation}</p>

<p><img src=""https://i.stack.imgur.com/30JTw.png"" alt=""enter image description here""></p>

<p>And then the question is, can we reasonably replace the missing point with its limiting value (the value 1 at x=0)</p>

<p>Let's look at the original function $z$:</p>

<p><img src=""https://i.stack.imgur.com/JgEha.png"" alt=""enter image description here""></p>

<p>And we then realize that the function is not only continuous but actually smooth right through 0.</p>

<p>That is, the way we've written the function down and then performed our manipulations has led us to fool ourselves about the derivative being a problem. Clearly the correct value for that derivative at zero is actually 1, and we should fill the point in with its limit without being overly concerned that we did something nefarious.</p>

<p>The derivative of the function can simply be taken as </p>

<p>\begin{equation}
\frac{\partial y}{\partial x}=\frac {1}{(|x|+1)}
\end{equation}</p>

<p>including at zero. (If you're in doubt, <em>start</em> with that derivative, and integrate it to produce the original function (using that y=0 at x=0 fixes the ""+C""), and then work from there.)</p>

<p>(Indeed, if you go back to brass tacks and do it like when we were very first learning how to take derivatives, as $\lim_{h\to 0} \frac{f(x+h) - f(x)}{h}$, there's no difficulty.)</p>
",2013-11-04 01:06:33.153
58789,13165.0,1,58952.0,,,The junction tree theorem,<inference><graphical-model>,CC BY-SA 3.0,"<p>In [1] (Page 31,equation 2.12 ) it is claiming that in a graph which is processed by the junction tree algorithm, the joint distribution of the variables could be found by 
$$
p(x_1, ..., x_m) = \frac{ \prod_{C \in \mathcal{C}} \mu_{C}(x_C) }{  \prod_{S \in \mathcal{S}} [\mu_{S}(x_S)]^{d(S)-1}  }
$$
In which $d(S)$ denote the number of maximal cliques to which it is adjacent.
The problem is that, I don't really see where this distribution is coming from. Any ideas? </p>

<p>[1] <a href=""http://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf"" rel=""nofollow"">http://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf</a></p>
",2013-11-04 01:12:24.783
58790,14597.0,1,67041.0,,,What is the loss function of hard margin SVM?,<svm><loss-functions>,CC BY-SA 3.0,"<p>People says soft margin SVM use hinge loss function: $\max(0,1-y_i(w^\intercal x_i+b))$. However, the actual objective function that soft margin SVM tries to minimize is 
$$
\frac{1}{2}\|w\|^2+C\sum_i\max(0,1-y_i(w^\intercal x_i+b))
$$
Some authors call the $\|w\|^2$ term regularizer and the $\max(0,1-y_i(w^\intercal x_i+b))$ term loss function.</p>

<p>However, for hard margin SVM, the whole objective function is just
$$
\frac{1}{2}\|w\|^2
$$
Does that mean hard margin SVM only minimize a regularizer without any loss function? That sounds very strange.</p>

<p>Well, if $\frac{1}{2}\|w\|^2$ is the loss function in this case, can we call it quadratic loss function? If so, why the loss function of hard margin SVM becomes regularizer in soft margin SVM and make a change from quadratic loss to hinge loss? </p>
",2013-11-04 01:31:36.213
58791,22507.0,2,,58469.0,,,,CC BY-SA 3.0,"<p>For unknown nonlinear dependences, there are nonlinear models. The most commonly used are tree models, i. e. random forest (package randomForest) and boosted trees (package gbm) and neural networks.</p>

<p>If you want to use linear regression for nonlinear dependence, one possibility is to create a big polynom with lots of terms, and then do stepwise elimination.  Use logarithms if (and only if) the feature change orders of magnitude, i.e. you try to predict the life span of arbitrary animal from its weight.</p>

<p>Another possibility is to try Kernel Ridge Regression.</p>

<p>You may also want to read about Generalized Additive Model.</p>
",2013-11-04 01:49:18.887
58792,10448.0,2,,58748.0,,,,CC BY-SA 3.0,"<p>This sounds more like a data collection problem. If you have smaller sample sizes there is nothing you can do from a statistical perspective to extract more information. </p>
",2013-11-04 02:22:58.883
58793,22507.0,2,,58032.0,,,,CC BY-SA 3.0,"<p>Define error as the difference between real and observed value.</p>

<p>Suppose your errors at different points are independent and normally distributed (i.e. no systematic error).</p>

<p>If you know the standard deviation of each error, you also know the standard deviation of difference between the value at two curves.  Now you have the vector of differences, with known standard deviation of each difference.  Divide each difference by its standard deviation, and you have vector of normalized values with standard deviation of 1 each.  The null hypothesis is that they are distributed as N(0,1).  Test it with any <a href=""http://en.wikipedia.org/wiki/Normality_test"" rel=""nofollow"">normality test</a>.</p>
",2013-11-04 02:29:30.647
58794,14553.0,1,,,,Unsupervised Dimensional reduction for mixed data types,<r><binary-data><dimensionality-reduction><mixed-type-data>,CC BY-SA 3.0,"<p>I have a data set with about 50K rows and 100 columns. You can consider every row to be representing one restaurant.</p>

<p>My goal is to calculate dissimilarities between all the restaurants - Gower's coefficient.</p>

<p>Of those 100 columns (features), a few of them are numeric data and nominal data. The problem is the other columns (about 90) are very sparse binary data (1/0).</p>

<p>I do think that those 90 columns of binary data can be reduced to some smaller number of columns, so that the computational time can be reduced significantly. But I don't know what method I should use to reduce such a large amount of binary data.</p>

<p>Can anyone give me some suggestions? </p>

<p>It will be most helpful if you can provide me some references and R code.</p>
",2013-11-04 02:45:08.003
58795,20473.0,2,,58769.0,,,,CC BY-SA 3.0,"<p>$$X_n = \max(X_0+\sum\limits_{i=1}^n\xi_i, \sum\limits_{i=2}^n\xi_i, \sum\limits_{i=3}^n\xi_i, ..., 0)$$</p>

<p>$$\Rightarrow \frac 1nX_n = \max\left(\frac 1n(X_0+\sum\limits_{i=1}^n\xi_i), \frac 1n\sum\limits_{i=2}^n\xi_i, \frac 1n\sum\limits_{i=3}^n\xi_i, ..., \frac 1n\cdot0\right)$$</p>

<p>$$\Rightarrow \operatorname{plim} \frac 1nX_n = \operatorname{plim}\max\left(\frac 1n(X_0+\sum\limits_{i=1}^n\xi_i), \frac 1n\sum\limits_{i=2}^n\xi_i, \frac 1n\sum\limits_{i=3}^n\xi_i, ..., \frac 1n\cdot0\right)$$</p>

<p>The $\max$ is a continuous function. Also, since the $\xi$-series is i.i.d, the Law of Large Numbers holds. Then </p>

<p>$$\operatorname{plim} \frac 1nX_n = \max\left(\operatorname{plim}\frac 1n(X_0+\sum\limits_{i=1}^n\xi_i), \operatorname{plim}\frac 1n\sum\limits_{i=2}^n\xi_i, \operatorname{plim}\frac 1n\sum\limits_{i=3}^n\xi_i, ..., \operatorname{plim}\frac 1n\cdot0\right)$$</p>

<p>$$\rightarrow_{p} \max\left(\frac 1n\sum\limits_{i=1}^nE(\xi_i), \frac 1n\sum\limits_{i=2}^nE(\xi_i), \frac 1n\sum\limits_{i=3}^nE(\xi_i), ..., 0\right)$$</p>

<p>$$= \max\left(\frac 1nnE(\xi), \frac 1n(n-1)E(\xi), \frac 1n(n-2)E(\xi),..., 0\right) = E(\xi)$$</p>

<p>under the assumption that $E(\xi)&gt;0$. QED</p>
",2013-11-04 03:32:50.263
58796,23321.0,1,,,,Compare dispersion between two distributions with thick tails,<statistical-significance>,CC BY-SA 3.0,"<p>What tests would I use to see if two distributions are 'significantly' different in dispersion if they are thick tailed, like a t-distribution? thanks</p>
",2013-11-04 03:41:59.057
58799,23292.0,1,,,,Distribution of Product of Normal and Poisson?,<normal-distribution><poisson-distribution><approximation>,CC BY-SA 3.0,"<p>Suppose that X is distributed Poisson with a known rate and Y is a normal distributed with a know mean and variance.  My goal is to approximate the distribution Z where P(Z) = P(X) * P(Y), where Z is a non-negative integer.  I could get a good approximation by sampling, but I'd really like to have a fast solution, ideally closed-form.  </p>
",2013-11-04 04:16:42.723
58800,22843.0,1,58899.0,,,In finding the moment generating function why do we multiply by $e^{tx}$ for each pmf term?,<expected-value><moments><moment-generating-function>,CC BY-SA 3.0,"<p>The moment generating function that is associated with the discrete random variable $X$ and pmf $f(x)$ is defined as: </p>

<p>$$M(t) = E\left[e^{tX}\right] = \sum_{x \in S} e^{tx} f(x).$$</p>

<p>Where does this $e^{tx}$ come from? This vaguely looks like an integrating factor from differential equations. </p>

<p>Also, I find it strange that when we take the derivative of $M(t)$ we don't say ""lets take the partial derivative with respect to t"" but isn't this what we're doing, treating $x$ as a constant? Why do we use the derivative notation instead of the partial derivative operator?</p>
",2013-11-04 04:19:45.700
58801,14227.0,2,,58784.0,,,,CC BY-SA 3.0,"<p>Found the <a href=""https://groups.google.com/forum/#!topic/gensim/LM619SB57zM"" rel=""nofollow"">answer</a> on the <a href=""https://groups.google.com/forum/#!forum/gensim"" rel=""nofollow"">gensim mailing list</a>.</p>

<p>In short, the bound() method of LdaModel computes a lower bound on perplexity, based on a held-out corpus. </p>
",2013-11-04 05:05:47.587
58802,23325.0,1,,,,Bayesian model selection in PyMC,<bayesian><modeling><python><pymc>,CC BY-SA 3.0,"<p>I'm trying to do model selection using PyMC (v2.2), but having difficulty assessing the models using various Information Criteria and/or Bayes Factor. My model is similar to a typical regression, with several parameters (~10) with priors modelled by uniform distributions, and a single observation modelled by a normal distribution with a deterministic mean and uniform standard deviation (through a precision deterministic). The mean (dynamic) response of the system is actually generated by an ordinary differential equation and typically yields around 1500 data points. I can get reasonably accurate results using either Adaptive or Non-Adaptive MCMC, with around 50K samples following 50K burn-in samples.</p>

<p>Based on issues reported with BF for complex models, I started looking at the DIC values produced after performing the MCMC analysis, but for models consisting of various combinations of the true parameter set, they were quite large with very little difference between them. For example, -14623.9 and -14624.8. Are the DIC, and other similar criterion such as BPIC, normally so insensitive to different (sub-)models?</p>

<p>Using some <a href=""https://github.com/pymc-devs/pymc/blob/2.2/pymc/sandbox/MultiModelInference/ModelPosterior.py"">code</a> from the sandbox, I also computed the sample likelihood for my models in an attempt to exploit Bayes Factor. However, the log-likelihoods produced were very large, ranging from -1e5 to 1e5. Regularization further decreased the minimum and exponentiating resulted in overflow (see function <code>weight</code>), so the array <code>exp(loglikes[m])</code> ended up comprising all zeros and one one! Why would my log-likelihoods, calculated by <code>logp</code> after a call to <code>draw_from_prior</code>, be so large?</p>

<p>I'm fairly new to Bayesian estimation, so any help would be greatly appreciated!</p>
",2013-11-04 05:42:50.047
58803,2081.0,2,,58771.0,,,,CC BY-SA 3.0,"<p>Let $p_x$ and $p_y$ be the number of variables in your sets $X$ and $Y$. $N$ is the sample size. You have obtained $m=\min(p_x,p_y)$ canonical correlations $\gamma_1, \gamma_2,...,\gamma_m$. Testing them usually goes as follows.</p>

<p>Given $\gamma_j$, its corresponding eigenvalue is $\lambda_j= \frac{1}{1-\gamma_j^2}-1$.</p>

<p>Wilk's lambda statistic for it is $w_j= \frac{1}{1+\lambda_j}w_{j+1}$. So, first compute $w_m$ which is $\frac{1}{1+\lambda_m}$, then compute $w_{m-1}$ using $w_m$, etc., backwards.</p>

<p>This statistic has approximately Chi-square distribution (under assumptions of normality and large $N$) with $df_j= (p_x-j+1)(p_y-j+1)$. To recalculate Wilk's into the Chi-square: $\chi_j^2= -\ln(w_j)(N-(p_x+p_y+3)/2)$.</p>

<p>So, substitute $\chi_j^2$ in Chi-square cdf distribution with $df_j$, subtract from 1, and have the p-value for correlation $\gamma_j$.</p>

<p>What does this p-value mean in fact? Nonsignificant p-value for $\gamma_1$ tells that all canonical correlations $\gamma_1$ through $\gamma_m$ are not significant (i.e. the hypothesis that they all are zero should not be rejected). Significant p-value for $\gamma_1$ and nonsignificant p-value for $\gamma_2$ tells that $\gamma_1$ is significant (likely to be nonzero in the population), while the rest $\gamma_2$ through $\gamma_m$ are all not significant; etc. Sometimes, p-value for $\gamma_{j+1}$ is lower than for $\gamma_{j}$. That should not be taken in the sense ""$\gamma_{j+1}$ is more significant"" because a more junior correlation cannot be more significant than more senior one. As said already, if $\gamma_{j}$ is not significant for you, all the remaining junior correlations must automatically be considered not significant too.</p>

<p>For an algorithm of CCA, look <a href=""https://stats.stackexchange.com/a/77309/3277"">here</a>.</p>
",2013-11-04 05:50:42.140
58804,12152.0,1,58808.0,,,"In stochastic gradient descent, is there only one update to $\theta$ for each iteration?",<machine-learning><maximum-likelihood><optimization><gradient-descent>,CC BY-SA 3.0,"<p>I have read that the update equation for stochastic gradient descent is as shown below, for each iteration, k. Does one iteration correspond to one training example? So for each example is there only one update to $\theta$? </p>

<p>$ \theta^{k+1} = \theta^k - \epsilon_k    \frac{\partial L(\theta^k,z)}{\partial \theta^k} $</p>

<p>Update: Is it different for Online learning?</p>
",2013-11-04 07:14:44.243
58805,11117.0,1,,,,Use the improper prior $p(v) \propto 1/v$ into Jags,<bayesian><prior><jags>,CC BY-SA 3.0,"<p>I know that one can approximate this density ($p(v) \propto 1/v$) using its truncated version and implement it this way:</p>

<pre><code>   B~dunif(log(BInf),log(BSup))
   v&lt;-exp(B)
</code></pre>

<p>but I would like to use the exact form (I checked that under this prior my posterior is proper). Is there any solution to achieve this ?</p>
",2013-11-04 08:03:10.103
58806,1406.0,2,,58756.0,,,,CC BY-SA 3.0,"<p>Exploit the fact, that characteristic function convergence is equivalent to convergence in distribution. I.e. show that </p>

<p>$$E\exp(i\tilde T_{n}t)\to \exp(-t^2/2),$$</p>

<p>for any $t$, where $\tilde T_n$ is appropriately scaled and shifted version of $T_n$. Another useful fact is that the characteristic function of the sum of independent random variables is the product of the corresponding characteristic functions of the summands.</p>
",2013-11-04 08:04:13.173
58830,2149.0,2,,58824.0,,,,CC BY-SA 3.0,"<p>You should investigate ARMAX/Dynamic Regression/Transfer Functions making sure that you deal with any Outliers/Level Shifts/Seasonal Pulses/Local Time Trends while testing for constancy of parameters and constancy of error variance. If you wish to post your data, do so and I will send you some results illustrating these ideas. </p>
",2013-11-04 16:44:17.273
58807,23328.0,1,58829.0,,,Can I simply subtract an index from another?,<index-decomposition>,CC BY-SA 3.0,"<p>I am currently using a measure of the price level (CPI) from which I would like to remove the effects of food prices (also an index). Notice that food prices are included in the CPI (so that CPI is the total index).</p>

<p>First, I made sure that both were based in the same way (that is, the base value of 100 is on the same time point). Now, if I simply subtract the food price index from the CPI, I end up with small values hovering around 2, so I am quite sure it is not the way.</p>

<p>I tried the following: CPI+(CPI-food price) and CPI-(CPI-food price) in order to get rid of the food price index. However, I am not sure whether it is the first or the second method since, obviously, both give me a value of 100 on the base date.</p>

<p>My hunch is, it is the CPI-(CPI-food price) but I would like your take on this.</p>
",2013-11-04 08:39:40.640
58808,23320.0,2,,58804.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>Does one iteration correspond to one training example?</p>
</blockquote>

<p>Yes.</p>

<blockquote>
  <p>So for each example is there only one update to $\theta$?</p>
</blockquote>

<p>It is possible for a single example to be picked and used to update theta many times.</p>
",2013-11-04 08:49:48.073
58809,1959.0,1,,,,"p-value of hypothesis ""a home court advantage exists""",<hypothesis-testing><p-value><bernoulli-distribution>,CC BY-SA 3.0,"<p><strong>The practical POV</strong>:</p>

<p>I sample 35 matches: 15 turn out as home win, 10 as tie and 10 as home lose. I want to conduct a test for the hypothesis ""A home court advantage does exist."" with a significance level of say 5%. What would be a reasonable test statistic / calculation of the p-value to accept or reject the hypothesis?</p>

<p><strong>Clarification:</strong></p>

<p>The matches I sample are independent of a specific team. I just take a number of matches and count for the home teams - wins, ties and defeats. Due to symmetry the guest teams will have deterministically corresponding results and hence can be ignored.</p>

<hr>

<p><strong>The generalized POV</strong>:</p>

<p>$A_1$ be the random variable for the number of matches the local team won in a total sample of $N$ matches. Naturally this is a bernoulli experiment (given we only assume the possibility of win and lose) and the <strong>p-value</strong> of the hypothesis ""a home court advantage exists - i.e. the probability of a home win $p_1$ is larger than 0.5."" given $a_1$ observed wins is:</p>

<p>$$\Pr[A_1 &gt;= a_1 | p_1 = 0.5]=\sum_{k={a_1}}^N{N\choose{k}}(0.5)^k$$</p>

<p>So far so good (though please correct me if I am wrong or my wording is problematic) - now the aspect that puzzles me is how to factor in ties.</p>

<p>My idea would be to create a test statistic $\tilde{A}_1 := A_1 + \lfloor{A_0/2}\rfloor$ with $A_1$ and $A_0$ being the random variables for number of home wins and ties in $N$ matches.</p>

<p>$$\Pr[\tilde{A}_1 &gt;= \tilde{a}_1 | \tilde{p}_1 = 0.5]=\sum_{k={a_1}}^N{N\choose{k}}(0.5)^k$$</p>

<p>Now in my mind this makes sense somehow because it reflects the indecisive and symmetric nature of a tie.</p>

<p>On the other hand I am not sure how to bend the bernoulli experiment to reflect the notion of $\tilde{p}_1$ being the probability of a home win in the second model (which means home win or ""50% of a tie"") while keeping up the pretence of formal correctness.</p>

<p>Does this make sense?</p>
",2013-11-04 08:57:54.620
58810,22511.0,1,,,,Helmert Transformations,<regression><correlation>,CC BY-SA 3.0,"<p>Let $Y_f, Y_1,..., Y_n$ denote measurements taken from a father and his n sons. It is assumed that $Y_f, Y_1,...,Y_n$ have equal variance with father-son correlation $Ï_f$ and equal correlation $Ï_S$ between sons. Transform $Y_f, Y_1,..., Y_n$ linearly to $Z_0, Z_1,..., Z_n$ to remove the correlations.</p>

<p>The hint is to let $Z_0=Y_f+c(Y_1+...+Y_n)$ where c is a suitably chosen constant. </p>

<p>Here is what I have tried. 
I let $Z_0, Z_1,..., Z_n$ be the Helmert transformations of $Y_f, Y_1,..., Y_n$ where $Z_j=H_jY$  for j=1,2,...,n and thus, 
<img src=""https://i.stack.imgur.com/pi4FQ.png"" alt=""enter image description here""> </p>

<p>where $Y = Y_1,..., Y_n$</p>

<p>I am stuck until this step and i don't have a clue on how to find the constant c to solve the problem. Thanks for the help.</p>
",2013-11-04 09:02:41.087
58811,22423.0,1,58834.0,,,Simple linear regression with a random predictor,<regression><variance><modeling><interpretation><inference>,CC BY-SA 3.0,"<p>We understand a SLR model as
$$y_i = \alpha + \beta x_i + \varepsilon_i$$
with $\varepsilon_i$ i.i.d with equal variance.</p>

<p>Suppose I have two instruments measuring a common entity, say, density of samples of different liquids, and assume that we do not know the exact density of each liquid (nor is there any way to accurately measure it).</p>

<p>My objective is to construct a model that will summarise the difference between the two instruments.</p>

<p>Here is how I go about it: (for simplicity sake, all models are SLR models)</p>

<p>Let readings from instrument 1 and instrument 2 when measuring the $i$-th liquid be $y_i$ and $w_i$ respectively, and the actual density of the $i$-th liquid be $x_i$ Then we have:
$$y_i=\alpha_1+\beta_1 x_i + \varepsilon_{1i} \\
w_i=\alpha_2+\beta_2 x_i + \varepsilon_{2i}$$
where $\varepsilon_{1i}$ and $\varepsilon_{2i}$ are i.i.d and equal variance w.r.t $i$.</p>

<p>Rearranging from the second equation we have:
$$x_i= \frac{w_i- \alpha_2- \varepsilon_{2i}}{\beta_2 } $$
Plugging into the first equation,
$$ \begin{align*} y_i &amp;=\alpha_1+\beta_1 \frac{w_i- \alpha_2- \varepsilon_{2i}}{\beta_2 } + \varepsilon_{1i} \\ 
&amp;= \alpha_1 - \frac{\beta_1}{\beta_2} \alpha_2 + \frac{\beta_1}{\beta_2} w_i + \varepsilon_{1i}-\frac{\beta_1}{\beta_2}\varepsilon_{2i}\\
&amp;= \alpha + \beta w_i + \varepsilon_i
\end{align*}
$$</p>

<p>where $\beta = \frac{\beta_1}{\beta_2}$, $\alpha = \alpha_1 - \beta \alpha_2$,  and $ \varepsilon_i = \varepsilon_{1i}-\beta \varepsilon_{2i}$, and $ \varepsilon_i $ are consequently i.i.d. with equal variance w.r.t. $i$.</p>

<p>I would like to ask,</p>

<ol>
<li><p>Is this a valid SLR model for my objective? Do I just treat readings from instrument 2 as an 'independent variable' and proceed like a normal SLR analysis?</p></li>
<li><p>If so, can I interpret the slope($\beta$) as the ratio of the individual slopes of the two instruments?</p></li>
<li><p>Can I then test the hypothesis that the two instruments are similar by testing $\alpha = 0$ and $ \beta =1 $?</p></li>
<li><p>Suppose there are N total liquids we are interested in. All N liquids have been measured by instrument 1. However, we only have time/money to measure n ($&lt;$N) liquids using instrument 2. How should I select the n liquids from N such that I would minimise variance for $\beta$ and $\alpha$?</p></li>
</ol>
",2013-11-04 09:12:04.710
58812,23331.0,2,,48103.0,,,,CC BY-SA 3.0,"<p>The current methods to fit a sin curve to a given data set require a first guess of the parameters, followed by an interative process. This is a non-linear regression problem.
A different method consists in transforming the non-linear regression to a linear regression thanks to a convenient integral equation. Then, there is no need for initial guess and no need for iterative process : the fitting is directly obtained.
In case of the function y = a + r*sin(w*x+phi) or y=a+b*sin(w*x)+c*cos(w*x), see pages 35-36 of the paper ""RÃ©gression sinusoidale"" published on Scribd :
<a href=""http://www.scribd.com/JJacquelin/documents"" rel=""nofollow"">http://www.scribd.com/JJacquelin/documents</a>
In case of the function y = a + p*x + r*sin(w*x+phi) : pages 49-51 of the chapter ""Mixed linear and sinusoidal regressions"".
In case of more complicated functions, the general process is explained in the chapter ""Generalized sinusoidal regression"" pages 54-61, followed by a numerical example y = r*sin(w*x+phi)+(b/x)+c*ln(x), pages 62-63</p>
",2013-11-04 10:32:01.167
58813,22049.0,1,,,,Finding anomalies using moving average in a time series,<time-series><outliers>,CC BY-SA 3.0,"<p>I want to find anomalies in a time series. Is it possible to find anomalies using moving average?</p>
",2013-11-04 10:58:38.770
58814,16665.0,1,58820.0,,,"Fligner test in R, with several variables indicating the grouping",<r><anova><heteroscedasticity>,CC BY-SA 3.0,"<p>I have to run an multiple way Anova and want to test the assumption of homoscedasticity.</p>

<p>How can I run a Fligner.test with several independent variables (variables indicating the grouping) with R? Does it make sense to do such a thing?</p>
",2013-11-04 11:10:15.180
58831,23339.0,1,,,,Beginner's Question about Plotting in R,<r>,CC BY-SA 3.0,"<p>Alright, so I'm new to R, and I imagine what I'm trying to do should be very simple. I want to use plot(x,y), but then add the curve lm(y~x^2) over the plot. I'm sure I could add more detail this, but I am not sure what is needed, so I'll be vigilant to answer any questions you all may have. I appreciate any help I can get!</p>
",2013-11-04 16:48:47.160
58815,23333.0,1,,,,Double integration diverges due to the non-Gaussian noise,<mathematical-statistics><numerical-integration>,CC BY-SA 3.0,"<p>I added Gaussian noise to my input data and then I integrated it twice (I used trapezoidal rule). I was wondering if the the integration itself transform the Gaussian noise to something else. I looked at the probability density function of which is</p>

<p>$$f(x)= \frac {1} {\sigma \sqrt{2 \pi}} e^{- \frac{1}{2}(\frac{x- \mu}{\sigma})^2}$$</p>

<p>When we integrate the probability density function it's equal to 1 (property of Gaussian distribution). 
$$\int f(x)=1$$
Thus the distribution of the noise is Gaussian.
When we integrated again (double integration) the probability density function is not equal to 1 anymore and it depends on the domain which we integrate it. I was wondering if we can conclude that after the second integration the Gaussian noise is not Gaussian anymore from the fact that the double integration of the probability density function is not 1.</p>
",2013-11-04 11:29:16.110
58816,2149.0,2,,58813.0,,,,CC BY-SA 3.0,"<p>Anomalies can be easily detected while using ""moving averages"" . Please review the woRk or Tsay <a href=""http://www.unc.edu/~jbhill/tsay.pdf"" rel=""nofollow"">http://www.unc.edu/~jbhill/tsay.pdf</a> . You might also search for ""AUTOMATIC DETECTION OF INTERVENTION VARIABLES"" using Google . Post your actual data and I will post the results.</p>
",2013-11-04 12:02:31.710
58817,9081.0,2,,58799.0,,,,CC BY-SA 4.0,"<p>There is one book dedicated to the problem of products of random variables:
<a href=""https://rads.stackoverflow.com/amzn/click/com/0824754026"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">http://www.amazon.com/Products-Random-Variables-Applications-Arithmetical/dp/0824754026/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1383564424&amp;sr=1-1&amp;keywords=product+of+random+variables</a></p>
<p>Maybe you can find it in a library. (Or search google scholar  with the author names)</p>
<p>There is a connection between products of independent random variables and the Mellin transform, see the paper: &quot;<a href=""https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-19/issue-3/Some-Applications-of-the-Mellin-Transform-in-Statistics/10.1214/aoms/1177730201.full"" rel=""nofollow noreferrer"">Some Applications of the Mellin Transform in Statistics</a>&quot; by Benjamin Epstein, which is on JSTOR. There is a <a href=""https://en.wikipedia.org/wiki/Mellin_transform#In_probability_theory"" rel=""nofollow noreferrer"">Wikipedia article on the Mellin Transform</a>, and search google scholar for &quot;Mellin transform product of random variables&quot; gives some relevant papers.</p>
",2013-11-04 12:06:40.070
58818,16441.0,1,58837.0,,,Log or square-root transformation for ARIMA,<r><forecasting><arima>,CC BY-SA 3.0,"<p>With the below dataset, I have a series which needs transforming. Easy enough. However, how do you decide which of the SQRT or LOG transformations is better? And how do you draw that conclusion?</p>

<pre><code>x&lt;-c(75800,54700,85000,74600,103900,82000,77000,103600,62900,60700,58800,134800,81200,47700,76200,81900,95400,85400,84400,103400,63000,65500,59200,128000,74400,57100,75600,88300,111100,95000,91500,111400,73700,72800,64900,146300,83100,66200,101700,100100,120100,100200,97000,120600,88400,83500,73200,141800,87700,82700,106000,103900,121000,98800,96900,115400,87500,86500,81800,135300,88900,77100,109000,104000,113000,99000,104500,109400,92900,88700,90500,140200,91700,78800,114700,100700,113300,122800,117900,122200,102900,85300,92800,143800,88400,75400,111200,96300,114600,108300,113400,116600,103400,87300,88200,149800,90100,78800,108900,126300,122000,125100,119600,148800,114600,101600,108800,174100,101100,89900,126800,126400,141400,144700,132800,149000,124200,101500,106100,168100,104200,79900,126100,121600,139500,143100,144100,154500,129500,109800,116200,171100,106700,85500,132500,133700,135600,149400,157700,144500,165400,122700,113700,175000,113200,94400,138600,132400,129200,165700,153300,141900,170300,127800,124100,206700,131700,112700,170900,153000,146700,197800,173800,165400,201700,147000,144200,244900,146700,124400,168600,193400,167900,209800,198400,184300,214300,156200,154900,251200,127900,125100,171500,167000,163900,200900,188900,168000,203100,169800,171900,241300,141400,140600,172200,192900,178700,204600,222900,179900,229900,173100,174600,265400,147600,140800,171900,189900,185100,218400,207100,178800,228800,176900,170300,251500,149900,150300,192000,185100,184500,228800,219000,180000,241500,184300,174600,264500,166100,151900,194600,214600,201700,229400,233600,197500,254600,194000,201100,279500,175800,167200,235900,207400,215900,261800,236800,222400,281500,214100,218200,295000,194400,180200,250400,212700,251300,280200,249300,240000,304200,236900,232500,300700,207300,196900,246600,262500,272800,282300,271100,265600,313500,268000,256500,318100,232700,198500,268900,244300,262400,289200,286600,281100,330700,262000,244300,309300,246900,211800,263100,307700,284900,303800,296900,290400,356200,283700,274500,378300,263100,226900,283800,299900,296000,327600,313500,291700,333000,246500,227400,333200,239500,218600,283500,267900,294500,318600,318700,283400,351600,268400,251100,365100,249100,216400,245500,232100,236300,275600,296500,296900,354300,277900,287200,420200,299700,268200,329700,353600,356200,396500,379500,349100,437900,350600,338600,509100,342300,288800,378400,371200,395800,450000,414100,387600,486600,355300,358800,526800,346300,295600,361500,415300,402900,484100,412700,395800,491300,391000,374900,569200,369500,314900,422500,436400,439700,509200,461700,449500,560600,435000,429900,633400,417900,365700,459200,466500,488500,531500,483500,485400,575700,458000,433500,642600,409600,363100,430100,503900,500400,557400,565500,526700,628900,547700,520400,731200,494400,416800,558700,537100,556200,686700,616600,582600,725800,577700,552100,806700,554200,455000,532600,693000,619400,727100,684700)
y&lt;-ts(x,frequency=12, start=c(1976,1))
#Transforming the data to log or sqrt and plotting it
log.y&lt;-log(y)
plot(log.y)
sqrt.y&lt;-sqrt(y)
plot(sqrt.y)
</code></pre>
",2013-11-04 12:55:18.167
58819,23284.0,1,,,,Time Series Analysis for Non-Uniform Data,<r><time-series>,CC BY-SA 3.0,"<p>I have wait-time data for a coffee shop for 4 weeks. Since the data is crowdsourced, it is sparse and non-uniform along the time. So my question is: how should I deal with this non-uniform data? What are the some methods in time series forecasting that can handle non-uniform data? Or is there any way to make the data uniform?</p>
",2013-11-04 12:57:26.843
58820,16665.0,2,,58814.0,,,,CC BY-SA 3.0,"<p>There are two ways of using the fligner.test() function. You can either do</p>

<blockquote>
  <p>fligner.test(x=â€¦, g=â€¦)</p>
</blockquote>

<p>or</p>

<blockquote>
  <p>fligner.test(formula= a~b+c+d, data=your_data)</p>
</blockquote>

<p>The second case allows to check an anova model with several independent variables.</p>

<p>And yes, it totally makes sense to run a Fligner test with several grouping variables.</p>
",2013-11-04 13:10:34.553
58821,12683.0,2,,58633.0,,,,CC BY-SA 3.0,"<p>Propensity score matching:
First you'd model the probability of students' going to an advisor, based on available predictors&mdash;perhaps course marks, financial situation, living arrangements, medical records, &amp;c. For each student the predicted probability of going to an advisor is the propensity score. Next you'd match each student who <em>did</em> go to an advisor with one who <em>didn't</em> but has the same, or as near as possible, propensity score. So you'd now proceed with an analysis of retention based on matched treatment-control pairs.</p>

<p>Of course, this approach doesn't provide as compelling evidence as an experiment in which students are randomly assigned to go to an advisor or not&mdash;you'd need to consider whether predictors you hadn't accounted for could be significantly contributing to the effect.</p>

<p>Rosenbaum &amp; Rubin (1983), ""The Central Role of the Propensity Score in Observational Studies for Causal Effects"", <em>Biometrika</em>, <strong>70</strong>, 1</p>

<p>Rosenbaum &amp; Rubin (1985), ""Constructing a Control Group Using Multivariate Matched Sampling Methods that Incorporate the Propensity Score"", <em>The American Statistician</em>, <strong>39</strong>, 1</p>

<p>And the <a href=""http://cran.r-project.org/web/packages/Matching/index.html"" rel=""nofollow""><code>Matching</code></a> package for R might be useful.</p>
",2013-11-04 13:52:25.530
58822,4537.0,1,58843.0,,,Why does the Lasso provide Variable Selection?,<regression><feature-selection><lasso><regularization>,CC BY-SA 3.0,"<p>I've been reading <a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"">Elements of Statistical Learning</a>, and I would like to know why the Lasso provides variable selection and ridge regression doesn't.</p>

<p>Both methods minimize the residual sum of squares and have a constraint on the possible values of the parameters $\beta$. For the Lasso, the constraint is $||\beta||_1 \le t$, whereas for ridge it is $||\beta||_2 \le t$, for some $t$.</p>

<p>I've seen the diamond vs ellipse picture in the book and I have some intuition as for why the Lasso can hit the corners of the constrained region, which implies that one of the coefficients is set to zero. However, my intuition is rather weak, and I'm not convinced. It should be easy to see, but I don't know why this is true.</p>

<p>So I guess I'm looking for a mathematical justification, or an intuitive explanation of why the contours of the residual sum of squares are likely to hit the corners of the  $||\beta||_1$ constrained region (whereas this situation is unlikely if the constraint is $||\beta||_2$).</p>
",2013-11-04 14:39:19.147
58823,23336.0,1,,,,"Fitting a ""sigmoid"" function: Why is my fit so bad?",<r><logistic><nls>,CC BY-SA 3.0,"<p>I tried to fit a curve to the black points using the following code. Why is the fit so bad? Do I need to fit another type of function?</p>

<pre><code>fit &lt;- nls(grad ~ theta1/(1 + exp(-(theta2 + theta3*x1))), 
           start=list(theta1 = 4, theta2 = 0.09, theta3 = 0.31), trace=TRUE)

p = predict(fit)

plot(x1, grad)
points(x1, p, col = ""red"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/npJzB.jpg"" alt=""I want to fit a curve to this data. The red curve is my attempt. It is bad""></p>
",2013-11-04 14:51:50.083
58855,12980.0,1,58878.0,,,How to generate from the copula by inverse conditional cdf function of the copula?,<random-variable><copula>,CC BY-SA 3.0,"<p>I am trying to write a code (I am using MATLAB) for estimating the goodness of fit of the copula based on a Rosenblatt transformation ( DobriÄ‡ and Schmid 2007, <a href=""http://dx.doi.org/10.1016/j.csda.2006.08.012"" rel=""noreferrer"">http://dx.doi.org/10.1016/j.csda.2006.08.012</a>) my question is this:</p>

<p>In the algorithm it says: ""Generate i.i.d. observations from the copula with parameter theta (I can't use <code>copularnd</code> function because it only covers a few families). If my copula is bi-variate like C(u,v, theta) how can I generate these i.i.d. observations? what will be my input to copula function?</p>

<p>Thanks</p>
",2013-11-04 22:00:27.077
58824,16504.0,1,,,,Time Series Modeling with Lagged Variables,<r><time-series><machine-learning><arima><software>,CC BY-SA 3.0,"<p>I have a dataset with columns that represent lagged values of predictors. To illustrate with a simple example, suppose we had car sales data for 3 years and the only predictors available were income and population for a number of car dealers, the dataset could be represented as follows,</p>

<pre><code>ID  IncLag1  PopLag1  SalesLag1  IncLag2  PopLag2 SalesLag2  IncCurrent  PopCurr  SalesCurr
a       100      1000     200        150      2000    300        500       2500         450
b       10        300      50         60       900     80         90       1000         100
</code></pre>

<p>...</p>

<pre><code>k       30        60      10        200      2000     60         80          800         ??
</code></pre>

<p>My dependent variable is SalesCurr - i.e., given a history of past sales and corresponding Income and Population values (which we can use as the train-test data), predict what the Sales will be in the current year (SalesCurr). </p>

<p>My question is as follows -- Using R or GRETL, how is it possible to create an ARIMA/TimeSeries model with the above data to predict the SalesCurrent variable. Using simple Linear Regression, one could simply have a formula such as say, <code>lm (SalesCurrent ~ ., data=mytable)</code>, but it would not be a time-series model since it does not take into account the relationship between the different variables.</p>

<p>Alternatively, I am quite familiar with Machine Learning models and wanted to get your thoughts on how such a dataset could be modeled using say, randomForest, GBM, etc. </p>

<p>Thanks in advance.</p>
",2013-11-04 15:05:10.873
58825,306.0,2,,58819.0,,,,CC BY-SA 3.0,"<p>Although not quite made for data for the coffee shop, you can have a look at the <a href=""http://en.wikipedia.org/wiki/Autoregressive_conditional_duration"" rel=""nofollow"">Autoregressive Conditional Duration</a> model. It was originally made for waiting times of trades data in finance but i suggest you should give it a shot. and please do report the results you got. thanks.</p>
",2013-11-04 15:05:19.823
58826,4910.0,1,227108.0,,,"Is there any ""standard"" for statistical model notation?",<references><model><notation>,CC BY-SA 4.0,"<p>In, for example, the <a href=""http://www.mrc-bsu.cam.ac.uk/bugs/documentation/exampVol1/bugs.html"" rel=""nofollow noreferrer"">BUGS manual</a> or the upcoming book by Lee and Wagenmakers (<a href=""https://webfiles.uci.edu/mdlee/BB_Free.pdf"" rel=""nofollow noreferrer"">pdf</a>) and in many other places a type of notation is used that to me seems very flexible in that it can be used to succinctly describe most statistical models. An example of this notation is the following:</p>
<p><span class=""math-container"">$$ y_i \sim \text{Binomial}(p_i,n_i) \\
 \log\left(\frac{p_i}{1 - p_i}\right) = b_i  \\
b_i \sim \text{Normal}(\mu_p,\sigma_p) $$</span></p>
<p>which would describe a hierarchical logistic model with no predictors, but with <span class=""math-container"">$i = 1\dots n $</span> groups. This way of describing models seem to work equally well for describing frequentist and Bayesian models, for example, to make this model description fully Bayesian you would just have to add priors on <span class=""math-container"">$\mu_p$</span> and <span class=""math-container"">$\sigma_p$</span>.</p>
<p><strong>Is this type of model notation/formalism described in detail in some article or book?</strong></p>
<p>If you want to use this notation to write models there are many different ways of doing things and it would be really useful with a comprehensive guide both to follow and to reference others to. Some differences I've found in how people use this type of notation:</p>
<ul>
<li>What do you call distributions? E.g., I've seen <span class=""math-container"">$\mathcal{N},\text{N},\text{Norm},\text{Normal}$</span>, etc.</li>
<li>How do you deal with indexes? E.g. I've seen <span class=""math-container"">$y_{ij}$</span>,<span class=""math-container"">$y_{i[j]}$</span>,<span class=""math-container"">$y_{j|i}$</span>, etc.</li>
<li>Which parameter symbols are usually used for parameters. For example, it is common to use <span class=""math-container"">$\mu$</span> as the mean for the normal distribution, but what about other distributions? (For this I usually check the <a href=""http://en.wikipedia.org/wiki/List_of_probability_distributions"" rel=""nofollow noreferrer"">distributions of Wikipedia</a>)</li>
</ul>
<p>Follow up question: <em>Does this notation have a name?</em> (For lack of a better name I called it the <em>probability distribution centric convention</em>
in a <a href=""http://sumsar.net/blog/2013/10/how-do-you-write-your-model-definitions/"" rel=""nofollow noreferrer"">blog post</a> I wrote...)</p>
",2013-11-04 15:05:21.753
58827,306.0,2,,58807.0,,,,CC BY-SA 3.0,"<p>What i am suggesting is not the right answer and i agree with the comments that it cannot be calculated. However if you still had to do it, and nothing else is given, then i would suggest that you can fit a regression model with the CPI as the dependent variable and the food price index as the independent variable. if you get a good fit, then you know how to remove the effect of the food price index simply by subtracting the food price index multiplied by the coefficient you get. and once again, this is just an attempt if nothing else is provided and you get a good fit.</p>
",2013-11-04 15:10:23.797
58828,23337.0,1,,,,Large coefficients in logistic regression,<r><logistic><statistical-significance>,CC BY-SA 3.0,"<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
",2013-11-04 15:29:17.310
58829,17573.0,2,,58807.0,,,,CC BY-SA 3.0,"<p>Picking up on the comment of @user31264, CPI is calculated with a formula like this:
\begin{align}
CPI_t = \frac{\sum_i w_i p_{it}}{\sum_i w_i p_{it^*}}
\end{align}
Price of item $i$ at time $t$ is $p_{it}$.  Prices in the baseline year are $p_{it^*}$.  Each price's weight in the CPI is $w_i$.  These weights are determined from a ""market basket"" which represents an attempt to match the spending habits of a typical US, urban household (assuming you are talking about the CPI-U for the US) --- you can productively think of these weights as representing the quantity of item $i$ bought by an average US household.  The weights are not literally these quantities, but it is OK to think about them this way most of the time.</p>

<p>Some of the items (some of the $i$) are food items, and some are not.  Let's denote the food items as $i \in F$ and the non-food items as $i \not\in F$.  Then CPI-food and CPI ex food look like:
\begin{align}
CPI_t^F &amp;= \frac{\sum_{i \in F} w_i p_{it}}{\sum_{i \in F} w_i p_{it^*}}\\
 &amp;\strut \\
CPI_t^{\tilde{}F} &amp;= \frac{\sum_{i \not\in F} w_i p_{it}}{\sum_{i \not\in F} w_i p_{it^*}}\\
\end{align}</p>

<p>Now, we can do some algebra:
\begin{align}
CPI_t &amp;= \frac{\sum_i w_i p_{it}}{\sum_i w_i p_{it^*}}\\ \strut \\
      &amp;= \frac{\sum_{i \in F} w_i p_{it}+\sum_{i \not\in F} w_i p_{it}} 
              {\sum_iw_ip_{it^*}} \\ \strut\\
      &amp;= \frac{\sum_{i \in F} w_i p_{it^*}}{\sum_iw_ip_{it^*}}
         \frac{\sum_{i \in F} w_i p_{it}}{\sum_{i \in F} w_i p_{it^*}}
        +\frac{\sum_{i \not\in F} w_i p_{it^*}}{\sum_iw_ip_{it^*}}
         \frac{\sum_{i \not\in F} w_i p_{it}}{\sum_{i \not\in F} w_i p_{it^*}}
         \\ \strut\\
      &amp;=\frac{\sum_{i \in F} w_i p_{it^*}}{\sum_iw_ip_{it^*}} \cdot CPI_t^F 
        +\frac{\sum_{i \not\in F}w_i p_{it^*}}{\sum_iw_ip_{it^*}}\cdot CPI_t^{\tilde{}F} 
        \\ \strut \\
CPI_t^{\tilde{}F} &amp;= \frac{\sum_iw_ip_{it^*}}{\sum_{i \not\in F}w_i p_{it^*}} 
                     \cdot CPI_t
                    -\frac{\sum_{i \in F} w_i p_{it^*}}{\sum_{i \not\in F}w_i p_{it^*}}
                     \cdot CPI_t^F
\end{align}</p>

<p>So, to back out CPI ex food from CPI and CPI food, you divide overall CPI by the proportion of spending on non-food in the baseline year (the year the market basket was measured in), then you subtract off the CPI food times the ratio of food to non-food spending in the baseline/market-basket year.</p>

<p>You need to be careful that each of the years of CPI data you use are calculated based on the same market basket.  The Bureau of Labor Statistics does occasionally update the market basket.  Currently, <a href=""http://stats.bls.gov/cpi/cpifaq.htm#Question_6"" rel=""nofollow"">CPI is based on a 2009/10 market basket.</a>  Furthermore, if you use a ""chained"" index, the market basket is updated every year, so that the method outlined here does not apply.</p>

<p>Finally, a caveat.  What the BLS actually does to calculate the CPI is more complicated than what I have laid out above.  What I have laid out above is the basic idea, but the implementation details are dizzying.  Spend a long time reading the various reports and documentation on the <a href=""http://stats.bls.gov/cpi/home.htm"" rel=""nofollow"">BLS website</a> if you want to actually understand.</p>
",2013-11-04 16:16:58.027
58832,11808.0,1,,,,Why Bayes Rule in Naive Bayes compared to simple P(class|features),<classification><naive-bayes><recommender-system>,CC BY-SA 3.0,"<p>I would like to improve on my recommendation system. Imagine I have training data of $M=7,000,000$ samples. Each training sample contains a variable number of words in the body, and a variable amount of tags assigned to the case. There are roughly 100 million unique words, and roughly 40,000 unique tags.</p>

<p>The goal is to, given a set of words, to correctly classify the tag(s) when the tags are not observed.</p>

<p>Intuition tells me to simply compute the probability for each word, and then use the conditional probability for each tag given the set of words for a tag. More concretely:</p>

<p>Imagine one case has 5 unique words. Each word will have a $P(W_i)$. This is a constant value for the training set, and can be stored in memory. The probability of $T_1$ given $P(W_1, W_2....W_5)$, is then simply the number of occurrences in the intersection of $T_1$ for each word divided by $M$.</p>

<p>Why does Naive Bayes suggest using (according to the Bayes rule) $\frac{P(W_1..W_5|T_1) * P(T_1)}{P(W_1...W_5)}$  when $P(T_1|W_1...W_5)$ seems easier to obtain, and more intuitive*?</p>

<p>*
<em>""What is the probability of a tag given some combination of words?""</em> seems to be exactly what we want to know.</p>
",2013-11-04 16:52:24.793
58833,9047.0,2,,58831.0,,,,CC BY-SA 3.0,"<pre><code>x &lt;- 1:10
set.seed(42)
y &lt;- x^2+3 + rnorm(10)
plot(x,y)

#I assume you actually want this, i.e., fit a quadratic function:
fit &lt;- lm(y~I(x^2))
pred.fit &lt;- function(xnew) predict(fit, newdata=list(x=xnew))

#add curve
curve(pred.fit, from=min(x), to=max(x), n=1e3, add=TRUE)
</code></pre>

<p><img src=""https://i.stack.imgur.com/tXZGj.png"" alt=""enter image description here""></p>
",2013-11-04 16:55:23.353
58834,17573.0,2,,58811.0,,,,CC BY-SA 3.0,"<p>The answer to 1 is no which makes the answers to all the others not applicable.</p>

<p>Let me start with your last equation:
\begin{align}
y_i = \alpha + \beta w_i + \epsilon_i
\end{align}</p>

<p>Now, let's assume that your earlier equations for $y$ and $w$ are valid classical linear regression models, so that $Cov(x,\epsilon_1)=0$ and $Cov(x,\epsilon_2)=0$.  I'm not sure what SLR stands for---Simple Linear Regression?</p>

<p>Anyway, now let's calculate $Cov(w,\epsilon)$ in order to verify whether your new equation is part of a valid classical linear regression model (recall that we need this to be zero):
\begin{align}
Cov(w,\epsilon) &amp;= Cov(w,\epsilon_1-\frac{\beta_1}{\beta_2}\epsilon_2) \\ \strut \\
                &amp;= Cov(w,\epsilon_1) - \frac{\beta_1}{\beta_2}Cov(w,\epsilon_2) 
                   \\ \strut \\
                &amp;= Cov(\epsilon_2,\epsilon_1) - \frac{\beta_1}{\beta_2}V(\epsilon_2)
\end{align}</p>

<p>The second term is not zero unless $\beta_1=0$, and that would make the example pretty silly.  Even the first term is not likely to be zero in most physical applications.  For that term to be zero, you would have to make the additional assumption that the errors made by the two instruments were completely uncorrelated.  You could get wildly lucky (in a stopped-clock-is-right-twice-a-day kind of sense) and the two terms could magically cancel out, but there is no systematic tendency of the two terms to cancel out.</p>

<p>The bias in estimating $\beta$ will be:
\begin{align}
\frac{Cov(\epsilon_2,\epsilon_1) - \frac{\beta_1}{\beta_2}V(\epsilon_2)}{V(w)}
\end{align}</p>

<p>Below, I attach a bit of <code>R</code> code which makes a toy monte carlo to demonstrate the effect.  The theoretical bias in the monte carlo is -0.25 and the answer we get in the monte carlo is too low by 0.23.  So, demonstrates the point pretty well.</p>

<p>In general, even if you can't see how to evaluate the bias in an example like this, you can always run a little monte carlo to see what is going on.  This is one of the great things about statistical software languages.  Monte Carlo simulations are amazingly powerful tools to give you feedback as to whether your ideas are really good or really not.</p>

<pre><code># This program written in response to a Cross Validated question
# http://stats.stackexchange.com/questions/74527/simple-linear-regression-with-a-random-predictor

# The program is a toy monte carlo.
# It generates a ""true"" but unobservable-to-the-analyst physical state x.
# Then it generates two measurements of that state from different instruments.
# Then it regresses one measurement on the other.

set.seed(12344321)

# True state, 1000 runs of the experiment
x &lt;- rnorm(1000)

# Set the various parameters of the monte carlo
# Play with these for fun and profit:

alpha_1 &lt;- 0
alpha_2 &lt;- 0
beta_1  &lt;- 1
beta_2  &lt;- 1
stddev_e1 &lt;- 1
stddev_e2 &lt;- 1
corr_e1e2 &lt;- 0.5

# Fallible measurements
e_1 &lt;- stddev_e1*rnorm(1000)
e_2 &lt;- stddev_e2*(corr_e1e2*e_1+sqrt(1-corr_e1e2^2)*rnorm(1000))
y &lt;- alpha_1 + beta_1*x + e_1
w &lt;- alpha_2 + beta_2*x + e_2

var(data.frame(e_1,e_2))
var(data.frame(x,w,y))

lm(y~x)
lm(w~x)

# By the bias formula in the answer, this regression should have a bias of
# -0.25 = (0.5-1*1)/2.  That is, the coefficient should not be close to 1,
# the correct value of beta_1/beta_2.  Instead, it should be close 
# to 0.75 - 1-0.25

lm(y~w)
</code></pre>
",2013-11-04 17:00:21.280
58835,20470.0,2,,58832.0,,,,CC BY-SA 3.0,"<p>As the number of words that you condition $P(T_1|W_1,..,W_N)$ on increases, you will find that calculating this probability will no longer be easy. The more conditions you add, the sparser your conditional probability tables will get.</p>

<p>Naive Bayes makes the assumption that the probability of words occurrences are independent of each other conditioned on the tag, thereby reducing $P(W_1,...,W_N|T_1)$ to $P(W_1|T_1)*...*P(W_N|T_1)$. This 'naive' independence assumption results in:</p>

<p>$P(T_1|W_1,..,W_N) \propto P(W_1|T_1) *  ... * P(W_N|T_1) * P(T_1)  $ </p>

<p>Note how the denominator: $P(W_1,...,W_N)$ is dropped since it only serves as a normalising constant that ensures the probability values you get are between $0$ and $1$. You do not really need it to calculate the $T_i$ that maximises the formula given above.</p>
",2013-11-04 17:15:52.913
58836,23341.0,1,58908.0,,,How to create forecast data prediction interval bands,<regression><forecasting>,CC BY-SA 3.0,"<p>I have seasonal data from which I create forecasts. The steps I perform are: deseasonalizing the data, finding the linear regression for the deseasonalized points, predicting a few points from the linear regression and adding seasonality to the predicted values to get forecast data. My input is quite sinusoidal so all works well.  </p>

<p>The problem is that the more in the future you predict, the more prediction errors increase. I'd like to show that on a chart, but I am not sure how to calculate these errors. I was thinking something like prediction interval bands for forecast data (whatever they are called). These bands would increase the further you predict in the future.</p>

<p>Here are some images that show what I'm trying to do:
<a href=""https://i.stack.imgur.com/jMVot.png"" rel=""nofollow noreferrer"">sample bands image1</a>
<a href=""https://i.stack.imgur.com/mD44A.png"" rel=""nofollow noreferrer"">sample bands image2</a></p>

<p>My <strong>question</strong> is what is the name for these bands? (then I can do a google search for it)
I'd also appreciate the formulas needed for the band calculations. I'm guessing there is a standard deviation in there somewhere.</p>

<p>I've looked at confidence interval, but that seems to be for the data already present, not for the forecast data.</p>
",2013-11-04 17:47:34.543
58837,2149.0,2,,58818.0,,,,CC BY-SA 3.0,"<p>Transformations are like drugs ! Some are good for you and some aren't !. Haphazard selection of  transformations should be studiously avoided.</p>

<p>a) One of the requirements in order to perform valid statistical tests of necessity is that the variance of the errors from the proposed model must not be proven to be non-constant. If the variance of the errors changes at discrete points in time then one has recourse to Generalized Least Squares or GLM . </p>

<p>b) If the variance of the errors is linearly relatable to the level of the observed series then a Logarithmic Transformation might be appropriate. If the square root of the variance of the errors is linearly relatable to the level of the original series then a Square Root transformation is appropriate. More generally the appropriate power transformation is found via the Box-Cox test where the optimal lambda is found. Note that the Box-Cox test is universally applicable and doesn't soley requite time series or spatail data.</p>

<p>All of the above ( a and b ) require that the mean of the errors cannot be proven to differ significantly from zero for all points. If your data is not time series or spatial in nature then the only anomaly you can detect is a pulse. However if your data is time series or spatial then Level Shifts , Seasonal Pulses and/or Local Time Trends might be suggested to render the mean value of the error term to be 0.0 everywhere or at least not significantly different from 0.0 .</p>

<p>In my opinion one should never willy-nilly transform the data unless one has to in order to satisfy (in part) the Gaussian assumptions. Some econometricians take logs for the simple and simply wrong reason in order to obtain direct estimates of elasticity's rather than assessing the % change in Y for a % change in x from the best model.</p>

<p>Now one caveat, if one knows from theory or at least one thinks that one knows from theory that transformations are necessary i.e. proven by previous well-documented research , then by all means follow that paradigm as it may prove to be more beneficial that the empirical procedures I have laid out here.</p>

<p>In closing use the original data, minimize any warping of the results by mindless transformations, test all assumptions and sleep well at night. Statisticians like Doctors should never do harm to their data/patients y providing drugs/transformations that have nasty and unwarranted side-effects.</p>

<p>Hope This Helps .</p>

<p>Data Analysis using time series techniques on time series data:</p>

<p>a plot <img src=""https://i.stack.imgur.com/c1ons.jpg"" alt=""enter image description here""> suggests a series that has structural change. The Chow Test yielded a signifciant break point <img src=""https://i.stack.imgur.com/mAv5e.jpg"" alt=""enter image description here""> . <img src=""https://i.stack.imgur.com/2mWUK.jpg"" alt=""enter image description here""> . Analysis of the modt recent 147 values starting at 1999/5 yielded <img src=""https://i.stack.imgur.com/02qRK.jpg"" alt=""enter image description here"">  with a Residual Plot <img src=""https://i.stack.imgur.com/xihuo.jpg"" alt=""enter image description here""> with the following ACF <img src=""https://i.stack.imgur.com/shaRL.jpg"" alt=""enter image description here""> . The forecast plot is <img src=""https://i.stack.imgur.com/rSwtH.jpg"" alt=""enter image description here""> . The final model is <img src=""https://i.stack.imgur.com/mJmtd.jpg"" alt=""enter image description here""> with all parameters statistically significant and no unwarranted power transformations which often unfortunately lead to wildly explosive and unrealistic forecasts. Power transforms are justified whrn it is proven via a Box-Cox test that the variablility of the ERRORS is related to the expected value as detailed here. N.B. that the variability of the original series is not used but the variability of model errors.</p>

<p><img src=""https://i.stack.imgur.com/bVt1Q.jpg"" alt=""enter image description here""></p>
",2013-11-04 17:51:13.340
58838,855.0,2,,24506.0,,,,CC BY-SA 3.0,"<p>You can automate the process within JMP using the JSL (JMP Scripting Language). Start by selecting <code>Script &gt; Save to Script Window</code> from the analysis contextual red-triangle menu. That will give you a script to rerun the platform. You can also issue other commands to the report object that the script creates, such as to save the prediction interval or fit formula.</p>
",2013-11-04 17:53:08.993
58839,22710.0,1,,,,Is AIC appropriate for model selection when the parameters are fitted by least-squares rather than MLE,<least-squares><aic><f-test><nls>,CC BY-SA 3.0,"<p>I want to compare the fit of a linear model (M1) and nonlinear model (M2):</p>

<ul>
<li>M1: $y = b_0 + b_1x_1 + b_2x_2 + b_3x_1x_2 + \epsilon, \epsilon \sim N(0, \sigma^2)$</li>
<li>M2: $y = b_0 + b_1x_1 + b_2x_2 + b_1 b_2x_1x_2 + \epsilon, \epsilon \sim N(0, \sigma^2)$</li>
</ul>

<p>In particular I want to know whether M1 is significantly different from M2.</p>

<p>To estimate the parameters, I am minimizing the least-squares errors rather than maximizing the likelihood through MLE procedures. In particular, I am using the R function nls() as follow:</p>

<pre><code># Creating a sample data set
n &lt;- 50
x1 &lt;- rnorm(n, 1:n, 0.5)
x2 &lt;- rnorm(n, 1:n, 0.5) 
b0 &lt;- 1
b1 &lt;- 0.5
b2 &lt;- 0.2
y &lt;- b0 + b1*x1 + b2*x2 + b1*b2*x1*x2 + rnorm(n,0,0.1)
# Actual model fit
M1 &lt;- nls(y ~ b0 + b1*x1 + b2*x2 + b3*x1*x2, start=list(b0=1, b1=0.5, b2=0.5, b3=0.5))
M2 &lt;- nls(y ~ b0 + b1*x1 + b2*x2 + b1*b2*x1*x2, start=list(b0=1, b1=0.5, b2=0.5))
</code></pre>

<p>I want to compare the models using a measure of relative fit such as AIC, which can be done in R as follow:</p>

<pre><code>AIC(M1, M2)
   df       AIC
M1  5 -88.47849
M2  4 -90.46491
</code></pre>

<p>Because $\Delta AIC \approx 2$ and the models differ by only one parameter, I would conclude that both of them fit the data similarly well.</p>

<p>In addition, I want to know whether the parameter $b_3$ from M1 significantly add to the fit using a statistical test such as an F-test. This can be done in R as follow:</p>

<pre><code>anova(M1, M2)
Analysis of Variance Table

Model 1: y ~ b0 + b1 * x1 + b2 * x2 + b3 * x1 * x2
Model 2: y ~ b0 + b1 * x1 + b2 * x2 + b1 * b2 * x1 * x2
  Res.Df Res.Sum Sq Df      Sum Sq F value Pr(&gt;F)
1     46    0.40843                              
2     47    0.40855 -1 -0.00011097  0.0125 0.9115
</code></pre>

<p>My general question is:</p>

<ul>
<li>Are these analyses appropriate? </li>
</ul>

<p>More specifically:</p>

<ul>
<li>Can I use AIC to compare least-squares fitted models?</li>
</ul>

<p>From a few posts such as this <a href=""https://stats.stackexchange.com/a/21925/31515"">one</a> it looks like AIC should be appropriate. However, I've seen posts such as this <a href=""https://stats.stackexchange.com/a/71273/31515"">one</a> that indicates that using AIC on non-MLE fitted models might be a problem. I understand that least-squares is equivalent to MLE if the error is normally distributed, but is this true even for non-linear models?</p>

<ul>
<li>Can I use a F-test to test whether $b_3$ is significantly different from $b_1 b_2$? </li>
</ul>

<p>I know such F-test makes sense if the model are nested, but I'm unsure whether it is appropriate in this case.</p>
",2013-11-04 17:58:04.380
58856,21985.0,1,,,,Likelihood ratio for normal distribution,<self-study><likelihood-ratio>,CC BY-SA 3.0,"<p>Let $X_1, \dots , X_n \sim \mathrm{N}(\mu,\sigma^2)$. $\sigma^2$ is known. We want to test $\mathrm{H}_0: \mu = 0$ versus $\mathrm{H}_1: \mu &gt; 0$.</p>

<p>For the likelihood ratio I got: $\Lambda_1 = \exp(\frac{n(\mu_0^2 - \mu_1^2)}{2 \sigma^2}) \cdot \exp(\frac{\mu_1 - \mu_0}{\sigma^2} \cdot \sum x_i)$. Where the first term is a constant. Hope this is correct.</p>

<p>Now we know that for expected value of normal randowm sample $T(X_{1:n} = \sum X_i)$ is a sufficient statistic. I have to rewrite $\Lambda_1$ as a function of $T$, which gives me $\Lambda_2$. Can I now just exchange $\sum x_i$ in $\Lambda_1$ with $T$?</p>

<p>Another question is what I can say about the rejection region of $\Lambda_1$ and $\Lambda_2$, keeping in mind that $\mu$ is zero or bigger. I do not know what is meant here...</p>
",2013-11-04 22:01:17.580
58857,594.0,2,,58796.0,,,,CC BY-SA 3.0,"<p>Here's the situation as I understand it:</p>

<p>1) you want to test for equality of spread with heavy tailed distributions. </p>

<p>2) you assume they will only differ in spread.</p>

<p>3) you have a measure of spread already in mind (median absolute deviation); what you need is a single statistic that identifies how much those spreads are unalike (since it's a measure of scale, their ratio is a reasonable choice, but in fact for what I am going to suggest we're going to be able to do something simpler).</p>

<p>4) the usual assumptions like independence etc apply.</p>

<p>(5) it sounds like you are using R; if I give any example code I'll use R also)</p>

<p>With these assumptions, under the null, the distributions of the two samples are the same; the null is a test of identical distributions, the test statistic is a measure that's sensitive to a particular kind of deviation from identical distributions.</p>

<p>If you want a test where dispersion is measured by median absolute deviation, then in these circumstances you could just do a permutation test (or randomization test if the samples are too large to evaluate all permutations).</p>

<p>Because it's a permutation test, we don't have to take the ratio of mads, anything whose size is sensitive to deviations from the null would also work. In this case I'm going to discuss looking at the mad of the smaller-sized sample, in the hopes of calculating a bit faster (though calculation speed shouldn't be the only consideration, it allows me to make some useful points about what we're doing along the way).</p>

<p>If the null were true, the labels for the two groups are 'arbitrary' - we could combine the samples and randomly relabel the observations without altering their underlying distribution. However, under the alternative, the labels signify something important - that we are sampling from distributions that differ in spread. In that case, the sample test statistic won't be 'typical' of the permutation distribution.</p>

<p>Basic outline of how to do a test of equality of distributions using a specified statistic</p>

<p>Permutation test:</p>

<ol>
<li><p>Combine both samples and allocate the set of group labels in all possible combinations to find the distribution of the test statistic under the null of identical distributions, calculating the test statistic each time.</p></li>
<li><p>find how far 'in from the end(s)' the sample value occurs, and calculate the proportion of null values from the permutation distribution that are at least as extreme as it. (the p-value)</p></li>
<li><p>If the sample statistic is sufficiently extreme in the null distribution (if that p-value is low enough), reject the null.</p></li>
</ol>

<p>In this case I assume you want a two-tailed test and then (with the mad of the smaller group as the statistic) we have the problem of what to count up to in the other tail (this would be easy with the ratio, since we just take the reciprocal). However we'll take the ratio to the combined mad (which doesn't change so we don't have to sample that), and use the reciprocal from that to get the cutoff in the other tail.</p>

<p>The randomization test is basically the same as the above but instead of evaluating all possible permutations it simply samples (with replacement) from the set of permutations.</p>

<p>[Another alternative would be to use bootstrapping; in that case you're seeing whether a ratio of median absolution deviations of 1 could plausibly have come from the bootstrap distribution of the ratio. In this case you would resample with replacement <em>within groups</em> and compute the ratio each time, and see whether the null value is consistent with it (would be inside a confidence interval for the ratio). But bootstrapping tends not to work so well on medians, and you would require fairly large samples. A smoother (but similarly robust) statistic might work better.]</p>

<hr>

<p><strong>Example</strong> (here I've chosen the sample sizes to be just a little bit too large to do a permutation test; there's 3.2 billion combinations - you <em>could</em> do it if you were determined to get the exact permutation distribution, but we won't)</p>

<pre><code># create some data with heavy tail and different spread
set.seed(3498732)
nrand &lt;- 100000
x &lt;- rt(20,df=3)*5+50
y &lt;- rt(15,df=3)*20+50
print(x,d=4)
print(y,d=4)
xy &lt;- c(x,y)

# do the randomization
madrand &lt;- replicate(nrand,mad(sample(xy,15),constant=1))
mm &lt;- mad(xy,constant=1)
t &lt;- mad(x,constant=1)/mm
tr &lt;- range(t,1/t)
madrand &lt;- madrand/mm
hist(madrand,n=200)
abline(v=mad(x,constant=1)/mm,col=3)
abline(v=1/(mad(x,constant=1)/mm),col=3)

(pvalue &lt;- (sum(madrand &lt;= tr[1])+sum(madrand &gt;= tr[2]))/nrand)
</code></pre>

<p>This produces a p-value of 0.114 (with a binomial standard error of 0.001); the histogram looks like this:</p>

<p><img src=""https://i.stack.imgur.com/J1yA3.png"" alt=""histogram of scaled mad of smaller sample""></p>

<p>[This looks 'rough' like I haven't sampled enough. Not so - the exact permutation distribution will look like this too; it's a result of using a median (of the absolute deviations from the median) on small samples -- the resulting statistic has a somewhat ""clumpy"" distribution even though the original data was continuous.]</p>

<p>If I'd been clever I'd have used a constant in the calls to <code>mad</code> of <code>1/mm</code>.</p>

<p>If we wanted to retain the scale of the mad in our results, we could still do the other tail by computing <code>mm^2/mad(x,constant=1)</code> as the cutoff and get the same resulting p-value.</p>

<p>Hopefully that's clear enough, but I can elaborate if needed.</p>

<p>The ratio of mads that I originally discussed can be done almost as easily - the sampling will be slower (it will take maybe twice as long to run) but the fiddling around at the end would be simpler.</p>

<hr>

<p>Edit: If you're not wedded to the mad, the package exactRankTests has some alternative robust permutation tests of equality of spread.</p>
",2013-11-04 22:39:54.083
58840,23343.0,1,,,,Correct calculation of effect size after Friedman test in a skewed population,<nonparametric><effect-size>,CC BY-SA 3.0,"<p>I have some data that compares behaviour in four conditions (repeated measures)(I have several different behavioural variables and each behaviour gets analysed separately, and the behaviours are currently in % of time format). The behavioural variables are very non-normal, even after transformation. So I have run Friedman tests for the omnibus tests (friedman.test in R). For posthocs I have used friedman from the Agricolae package, but I would also like to report effect sizes. I am not sure which is the correct calculation to use because the data is not only non-normal, but also very positively skewed. I have tried the rFromWilcox function from Field but I am not sure it is the right one to use. Also I am getting r results such as -0.564NULL.
I am quite new to doing my own analysis, but keen to learn, so please don't be offended if I haven't phrased something quite right. </p>

<p>@Michael: I am trying to determine if certain behaviours occur more or less often under the different conditions, and where those differences are (nothing too deep). Two similar but different treatments are applied between condition one and two, and between three and four. I am predicting that there will be certain changes (some behaviours will increase and some will decrease) between all conditions but particularly between one and two, and three and four. My posthocs test between all pair combinations and I have used a Bonferroni correction. I was planning on reporting the summary stats, for each condition for each behaviour, the test statistic and p value, and posthoc results where Friedman is significant, and effect sizes on those posthocs. </p>
",2013-11-04 18:03:06.073
58841,9804.0,1,58852.0,,,Probability as a dependent variable in a time-series regression,<time-series><probability><neweywest>,CC BY-SA 3.0,"<p>Are there any issues to run a Newey-West time-series regression on a dependent variable that is a probability? What are the biases that I am facing? I can't find anything online that can help me out with this issue.   </p>
",2013-11-04 18:13:22.133
58842,20828.0,1,58851.0,,,Bayesian model averaging for variable selection in R,<r><bayesian><model-selection><markov-chain-montecarlo><metropolis-hastings>,CC BY-SA 3.0,"<p>I am trying to use Bayesian model averaging for variable selection with a large number of variables. In R, the BMS package allows to apply the method, with the option of using MCMC sampler (Metropolis Hastings algorithm) when the number of covariates is large.</p>

<p>Here is a sample code:</p>

<pre><code> data(datafls)
 fls1 = bms(datafls, burn = 50000, iter=100000, g = ""BRIC"", mprior = ""uniform"", nmodel = 2000, mcmc=""bd"", user.int=F)
 result = coef(fls1)
</code></pre>

<p>However, if you run the same code twice, the results (i.e. the posterior probabilities) would be completely different. Does anyone know how to tune the code, so that the results are consistent for every run?</p>

<p>Thank you,</p>
",2013-11-04 18:52:57.227
58843,5448.0,2,,58822.0,,,,CC BY-SA 4.0,"<p>Let's consider a very simple model: <span class=""math-container"">$y = \beta x + e$</span>, with an L1 penalty on <span class=""math-container"">$\hat{\beta}$</span> and a least-squares loss function on <span class=""math-container"">$\hat{e}$</span>.  We can expand the expression to be minimized as:</p>

<p><span class=""math-container"">$\min y^Ty -2 y^Tx\hat{\beta} + \hat{\beta} x^Tx\hat{\beta} + 2\lambda|\hat{\beta}|$</span></p>

<p>Keep in mind this is a univariate example, with <span class=""math-container"">$\beta$</span> and <span class=""math-container"">$x$</span> being scalars, to show how LASSO can send a coefficient to zero. This can be generalized to the multivariate case.</p>

<p>Let us assume the least-squares solution is some <span class=""math-container"">$\hat{\beta} &gt; 0$</span>, which is equivalent to assuming that <span class=""math-container"">$y^Tx &gt; 0$</span>, and see what happens when we add the L1 penalty.  With <span class=""math-container"">$\hat{\beta}&gt;0$</span>, <span class=""math-container"">$|\hat{\beta}| = \hat{\beta}$</span>, so the penalty term is equal to <span class=""math-container"">$2\lambda\beta$</span>.  The derivative of the objective function w.r.t. <span class=""math-container"">$\hat{\beta}$</span> is:</p>

<p><span class=""math-container"">$-2y^Tx +2x^Tx\hat{\beta} + 2\lambda$</span>  </p>

<p>which evidently has solution <span class=""math-container"">$\hat{\beta} = (y^Tx - \lambda)/(x^Tx)$</span>.  </p>

<p>Obviously by increasing <span class=""math-container"">$\lambda$</span> we can drive <span class=""math-container"">$\hat{\beta}$</span> to zero (at <span class=""math-container"">$\lambda = y^Tx$</span>).   However, once <span class=""math-container"">$\hat{\beta} = 0$</span>, increasing <span class=""math-container"">$\lambda$</span> won't drive it negative, because, writing loosely, the instant <span class=""math-container"">$\hat{\beta}$</span> becomes negative, the derivative of the objective function changes to:</p>

<p><span class=""math-container"">$-2y^Tx +2x^Tx\hat{\beta} - 2\lambda$</span></p>

<p>where the flip in the sign of <span class=""math-container"">$\lambda$</span> is due to the absolute value nature of the penalty term; when <span class=""math-container"">$\beta$</span> becomes negative, the penalty term becomes equal to <span class=""math-container"">$-2\lambda\beta$</span>, and taking the derivative w.r.t. <span class=""math-container"">$\beta$</span> results in <span class=""math-container"">$-2\lambda$</span>.  This leads to the solution <span class=""math-container"">$\hat{\beta} = (y^Tx + \lambda)/(x^Tx)$</span>, which is obviously inconsistent with <span class=""math-container"">$\hat{\beta} &lt; 0$</span> (given that the least squares solution <span class=""math-container"">$&gt; 0$</span>, which implies <span class=""math-container"">$y^Tx &gt; 0$</span>, and <span class=""math-container"">$\lambda &gt; 0$</span>).  There is an increase in the L1 penalty AND an increase in the squared error term (as we are moving farther from the least squares solution) when moving <span class=""math-container"">$\hat{\beta}$</span> from <span class=""math-container"">$0$</span> to <span class=""math-container"">$ &lt; 0$</span>, so we don't, we just stick at <span class=""math-container"">$\hat{\beta}=0$</span>.</p>

<p>It should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with <span class=""math-container"">$\hat{\beta} &lt; 0$</span>.  </p>

<p>With the least squares penalty <span class=""math-container"">$\lambda\hat{\beta}^2$</span>, however, the derivative becomes:</p>

<p><span class=""math-container"">$-2y^Tx +2x^Tx\hat{\beta} + 2\lambda\hat{\beta}$</span></p>

<p>which evidently has solution <span class=""math-container"">$\hat{\beta} = y^Tx/(x^Tx + \lambda)$</span>.  Obviously no increase in <span class=""math-container"">$\lambda$</span> will drive this all the way to zero.  So the L2 penalty can't act as a variable selection tool without some mild ad-hockery such as ""set the parameter estimate equal to zero if it is less than <span class=""math-container"">$\epsilon$</span>"".  </p>

<p>Obviously things can change when you move to multivariate models, for example, moving one parameter estimate around might force another one to change sign, but the general principle is the same: the L2 penalty function can't get you all the way to zero, because, writing very heuristically, it in effect adds to the ""denominator"" of the expression for <span class=""math-container"">$\hat{\beta}$</span>, but the L1 penalty function can, because it in effect adds to the ""numerator"".  </p>
",2013-11-04 18:59:26.697
58844,18358.0,2,,58780.0,,,,CC BY-SA 4.0,"<p>The first graph you describe here is a tripartite graph, which means it has three types of nodes, and links only between nodes of different types. The second graph you describe, containing only user nodes, is the result of the so-called projection over the user dimension. However, performing such an operation results in a loss of information, because several multipartite graphs can lead to the same projection (as shown in <a href=""https://doi.org/10.1016/j.physa.2006.04.047"" rel=""nofollow noreferrer"">Guillaume'06</a> for bipartite graphs). So, it is better to directly detect communities in the original graph.</p>
<p>However, certain questions arise if you want to do so. The most important is: what is a community for you? A group of nodes of any type, or a group of same-type nodes? Depending on this, you can apply various methods. A lot of them were developped to handle folksonomy graphs, in which the three types of nodes respectively represent users, annotations and shared resources (the users associated annotations to resources). But they can be applied to networks representing other systems, such as yours.</p>
<p>Here're some community detection algorithms designed for tripartite graphs (the list is obviously not complete):</p>
<ul>
<li><a href=""https://doi.org/10.1145/1772690.1772853"" rel=""nofollow noreferrer"">Murata'10</a></li>
<li><a href=""https://doi.org/10.1145/1963192.1963213"" rel=""nofollow noreferrer"">Ghosh'11</a></li>
<li><a href=""http://doi.org/10.1007/978-3-642-19047-6_5"" rel=""nofollow noreferrer"">Papadopoulos'11</a></li>
</ul>
<p>However, I don't think any implementation is freely available, if that's what you're looking for. Also, note there's a generalized version of the modularity (see <a href=""https://doi.org/10.1145/1810617.1810640"" rel=""nofollow noreferrer"">Murata'10</a>), defined for tripartite network. The modularity is a metric measuring the &quot;quality&quot; of a community structure. Many community detection methods are based on the optimization of this metric. If you want to program your own tool, implementing this modularity and then applying a classic optimization method might be the easiest way.</p>
",2013-11-04 19:06:17.763
58845,23345.0,1,,,,Posterior probability,<bayesian><multivariate-analysis><discriminant-analysis><posterior>,CC BY-SA 3.0,"<p>Suppose that we have have scoring functions $f(\textbf{x})$ and $g(\textbf{x})$ for classifying an object as red or blue. These are based on linear discriminant analysis. So if $f(\textbf{x}) &gt; g(\textbf{x})$, then the object is red. Why is the posterior probability that an object is red the following:</p>

<p>$$\frac{\exp(f(\textbf{x})}{\exp(f(\textbf{x}))+\exp(g(\textbf{x}))}$$</p>

<p>Why do we exponentiate the functions?</p>
",2013-11-04 19:33:36.103
58858,13037.0,2,,58849.0,,,,CC BY-SA 3.0,"<p>I think I got it from the hints you guys gave:</p>

<p>$$\begin{align*}
 P(|X_{(1)} -\theta| &gt; \epsilon) &amp;=  P(X_{(1)} -\theta &gt; \epsilon)\\
&amp;= P(X_{(1)} &gt; \epsilon + \theta) \\
&amp;= P(X_1 &gt; \epsilon+\theta)^n\\
&amp;= (1-\epsilon^2)^n
\end{align*} $$</p>

<p>and $\lim_{n\to\infty}{(1-\epsilon^2)^n}=0$ for $\epsilon \in (0,1)$. </p>
",2013-11-04 22:53:44.773
58859,668.0,2,,58818.0,,,,CC BY-SA 3.0,"<p><strong>This question is answered beautifully by means of a <em>spread-versus-level</em> plot:</strong> a cube root transformation will stabilize the spreads of the data, providing a useful basis for further exploration and analysis.</p>

<hr>

<p>The data show a clear seasonality:</p>

<pre><code>plot(y)
</code></pre>

<p><img src=""https://i.stack.imgur.com/ULyKM.png"" alt=""Data""></p>

<p>Take advantage of this by slicing the data into annual (or possibly biennial) groups.  Within each group compute resistant descriptors of their typical value and their spread.  Good choices are based on the 5-letter summary, consisting of the median (which splits the data into upper and lower halves), the medians of the two halves (the ""hinges"" or ""fourths""), and the extremes.  Because the extremes are not resistant to outliers, use the difference of the hinges to represent the spread.  (This ""fourth-spread"" is the length of a box in a properly constructed box-and-whisker plot.)</p>

<pre><code>spread &lt;- function(x) {
  n &lt;- length(x)
  n.med &lt;- (n + 1)/2
  n.fourth &lt;- (floor(n.med) + 1)/2
  y &lt;- sort(x)[c(floor(n.fourth), ceiling(n.fourth), 
               floor(n+1 - n.fourth), ceiling(n+1 - n.fourth))]
  return( y %*% c(-1,-1,1,1)/2 )
}
years &lt;- floor((1:length(x) - 1) / 12)
z &lt;- split(x, years)
boxplot(z, names=(min(years):max(years))+1976, ylab=""y"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/eZKSp.png"" alt=""Raw boxplots""></p>

<p><em>The boxplots clearly get longer over time as the level of the data rises.  This heteroscedasticity complicates analyses and interpretations.  Often a power transformation can reduce or remove the heteroscedasticity altogether.</em></p>

<p>A <em>spread versus level plot</em> shows whether a power transformation (which includes the logarithm) will be helpful for <em>stabilizing the spread</em> within the groups and suggests an appropriate value for the power: it is directly related to the slope of the spread-vs.-level plot on log-log scales.</p>

<pre><code>z.med &lt;- unlist(lapply(z, median))
z.spread &lt;- unlist(lapply(z, spread))
fit &lt;- lm(log(z.spread) ~ log(z.med))
plot(log(z.med), log(z.spread), xlab=""Log Level"", ylab=""Log Spread"", 
     main=""Spread vs. Level Plot"")
abline(fit, lwd=2, col=""Red"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/dSrJL.png"" alt=""Spread vs level plot""></p>

<p><em>This plot shows good linearity and no large outliers, attesting to a fairly regular relationship between spread and level throughout the time period.</em></p>

<p><strong>When the fitted slope is $p$, the power to use is $\lambda=1-p$.</strong>  Upon applying the suggested power transformation, the spread is (approximately) constant regardless of the level (and therefore regardless of the year):</p>

<pre><code>lambda &lt;- 1 - coef(fit)[2]
boxplot(lapply(z, function(u) u^lambda), names=(min(years):max(years))+1976, 
        ylab=paste(""y^"", round(lambda, 2), sep=""""),
        main=""Boxplots of Re-expressed Values"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/vxYdj.png"" alt=""Boxplots of re-expressed values""></p>

<pre><code>plot(y^lambda, main= ""Re-expressed Values"", ylab=paste(""y^"", round(lambda, 2), sep=""""))
</code></pre>

<p><img src=""https://i.stack.imgur.com/ojozN.png"" alt=""Plot of re-expressed values""></p>

<p>Often, powers that are reciprocals of small integers have useful or natural interpretations.  Here, $\lambda = 0.32$ is so close to $1/3$ that it may as well be the cube root.  In practice, one might choose to use the cube root, or perhaps round it to the even simpler fraction $1/2$ and take the square root, or sometimes go all the way to the logarithm (which corresponds to $\lambda = 0$).</p>

<hr>

<h3>Conclusions</h3>

<p>In this example, the spread-versus-level plot (by virtue of its approximate linearity and lack of outliers) has shown that a power transformation will effectively stabilize the spread of the data and has automatically suggested the power to use.  Although powers can be computed using various methods, none of the standard methods provides the insight or diagnostic power afforded by the spread-versus-level plot.  This should be in the toolkit of every data analyst.</p>

<hr>

<h3>References</h3>

<p>Tukey, John W.  <em>Exploratory Data Analysis.</em>  Addison-Wesley, 1977.</p>

<p>Hoaglin, David C., Frederick Mosteller, and John W. Tukey, <em>Understanding Robust and Exploratory Data Analysis.</em>  John Wiley and Sons, 1983.</p>
",2013-11-04 22:59:09.357
26070,5911.0,1,26657.0,,,Euclidean distance is usually not good for sparse data (and more general case)?,<machine-learning><clustering><data-mining><sparse><euclidean>,CC BY-SA 4.0,"<p>I have seen somewhere that classical distances (like Euclidean distance) become weakly discriminant when we have multidimensional and sparse data. Why? Do you have an example of two sparse data vectors where the Euclidean distance does not perform well? In this case which similarity should we use?</p>
",2012-06-01 13:55:13.253
58846,21762.0,2,,58798.0,,,,CC BY-SA 3.0,"<p>This answer will not be very complete.</p>

<p>1) If the performance scores are reliable (in the sense that they do not change considerably when measured repeatedly within worker), then there is absolutely no point in doing statistical inference (i.e. estimates, tests, confidence intervals) about population parameters, because your sample is equivalent to the corresponding population. Your descriptive presentation of means and variances could be complemented by adding medians, quartiles and boxplots etc.</p>

<p>2) If the performance scores are not very reliable, then you could think of a very simple model of the form
$$
\text{Performance} = \alpha + \beta \cdot \text{Gender} + \varepsilon
$$
with Gender = 1 if female and 0 if male. The random error $\varepsilon$ accounts for the fact that performance varies within person (depending on day, mood etc.) and that the true ""gender-effect"" $\beta$, i.e. the true average overperformance of women, is not exactly $\hat\beta =56.10 - 26.12 = 29.98$ but might change from day to day. With a two-sample t-test (with Welch-correction for unequal variance) you might test whether $\beta \ne 0$. If the p-value is below 5%, you can assert with 95% confidence that women systematically outperform men. (We could now discuss about whether we should pick an other two-sample test or log transform the data etc.)</p>

<p>3) As quite always when men are compared to women, the question of confounding raises: Is the difference really due to gender or can it be explained at least partially by other factors such as age? It's possible that in your company, women are typically older than men, which could already partially explain the difference in performance. One simple way to try to adjust for confounding is to modify the simple model from 2) to
$$
\text{Performance} = \alpha + \beta' \cdot \text{Gender} + \gamma \cdot \text{Age} + \varepsilon
$$
and infer about the age-corrected gender effect $\beta'$ by linear regression.</p>

<p>An alternative to the modelling approach is matching by age followed by a paired t-test (or similar). This also adjusts for potential age-confounding but leads to another estimated ""gender-effect"" $\hat \beta''$. Depending on the variability of the performance difference between pairs (which is directly linked to the intra-pair correlation) and the difference between $\hat \beta$ and $\hat \beta''$, the power of such test is higher or lower than the two-sample test in 2). </p>

<p>Summa summarum: Matching leads to inference about a confounder-adjusted effect of gender, whereas the unmatched comparison infers about the ""crude"" gender effect without considering any confounders. Both parameters might be interesting, so you could easily summarize both of them.</p>
",2013-11-04 19:57:30.790
58847,22144.0,1,,,,Imbalanced data classification using boosting algorithms,<classification><unbalanced-classes>,CC BY-SA 3.0,"<p>I am working on a binary data classification problem. The dataset is imbalanced, it consists of 92% 'false' labels and 8% 'true' labels. The number of features is 18 and I have a small number of 650 data points. I want to use boosting algorithms in MATLAB like <code>GentleBoost</code> to solve this problem. I assign Uniform for prior as follows:</p>

<pre><code>ada = fitensemble(Xtrain,Ytrain,'GentleBoost',10,'Tree','LearnRate',0.1, 'prior', 'uniform')
</code></pre>

<p>but the performance is consistently poor. How should I set the parameters? Is it necessary to set a cost? How can I do this? Is there any classifier that perform better than this? </p>
",2013-11-04 20:25:27.970
58848,13037.0,1,58883.0,,,Convergence in probability of reciprocal,<probability><self-study><convergence>,CC BY-SA 3.0,"<p>This is a homework problem. If $X_n$ converges in probability to 1, show $X_n^{-1}$ converges in probability to 1. </p>

<p>My attempt:</p>

<p>$$\begin{align*} P(|X_n^{-1}-1| &gt; \epsilon) &amp;= P(|X_n^{-1}-X_n + X_n-1| &gt; \epsilon)\\
 &amp;\leq P(|X_n^{-1} - X_n| &gt; \epsilon/2) + P(|X_n - 1| &gt; \epsilon/2)\\
 &amp;=  \end{align*} $$ </p>

<p>I know I can bound the 2nd term, but I am not sure how to bound the first term. Perhaps another approach is necessary. Any suggestions would be appreciated. </p>
",2013-11-04 20:45:14.520
58849,13037.0,1,58858.0,,,Convergence in probability of minimum,<probability><self-study><convergence><extreme-value>,CC BY-SA 3.0,"<p>This is a homework problem. Suppose we have a random sample $X_1,\ldots,X_n \overset{iid}{\sim} F$ with density $f(x) = 2(x-\theta)$ for $x\in (\theta,\theta+1)$. Let $X_{(1)} = \min{\{X_1,\ldots,X_n\}}$. Show $X_{(1)} \overset{P}{\longrightarrow} \theta$. </p>

<p>My attempt:</p>

<p>A few calculations first:</p>

<p>$F(x) = (x-\theta)^2$ for $x\in(\theta,\theta+1)$</p>

<p>$f_{X_{(1)}} = n(1-F(x))^{n-1}f(x) = 2n(x-\theta)(1-(x-\theta)^2)^{n-1}$</p>

<p>$$ \begin{align*} P(|X_{(1)} -\theta| &gt; \epsilon) &amp;= P(|X_1-\theta|&gt; \epsilon, \ldots, |X_n-\theta|&gt; \epsilon)\\ 
&amp;= P(|X_1-\theta|&gt; \epsilon)^n\\
&amp;=  \end{align*}$$</p>

<p>Not exactly sure where to go from here. Any suggestions would be appreciated. </p>
",2013-11-04 20:54:16.367
58850,22144.0,1,,,,Imbalanced data classification using Random Forest in matlab,<random-forest>,CC BY-SA 3.0,"<p>I want to solve an imbalanced data classification, with small number of data points (approximately 600 ) with the ratio of true labels to false ,  1:12. Is there any function or matlab code for using Random forest for classification of imbalanced data? How should I access it and set the parameters? 
Thank you for your help.  </p>
",2013-11-04 20:54:41.707
58851,6204.0,2,,58842.0,,,,CC BY-SA 3.0,"<p>This is an artifact of the stochasticity of MCMC sampling. The best you can do is set a specific seed before each run, using <code>set.seed()</code>:</p>

<pre><code>set.seed(123)
fls1 = bms(datafls, burn = 50000, iter=100000, g = ""BRIC"", mprior = ""uniform"", nmodel = 2000, mcmc=""bd"", user.int=F)

set.seed(123)
fls2 = bms(datafls, burn = 50000, iter=100000, g = ""BRIC"", mprior = ""uniform"", nmodel = 2000, mcmc=""bd"", user.int=F)
</code></pre>

<p>This causes the initial conditions of R's random number generator to be the same before you run your samplers, so the results of fls1 and fls2 will be the same.</p>
",2013-11-04 21:06:03.067
58852,23348.0,2,,58841.0,,,,CC BY-SA 3.0,"<p>For dependent variables distributed continuously between 0 and 1, you'll want to use a beta regression. I don't know what statistical software you are using, but this article should be helpful, especially if you are using R: <a href=""http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf</a></p>

<p>In order to help control for auto-correlation, try including a lagged version of the outcome variable as a predictor in the model. Then you can run a set of diagnostics to see if auto-correlation is still a problem.</p>
",2013-11-04 21:09:46.613
58853,16046.0,1,,,,Can posterior distribution for a continuous variable be greater than one?,<density-function><posterior><bayes>,CC BY-SA 3.0,"<p>I already asked this question <a href=""https://math.stackexchange.com/questions/552091/can-posterior-distribution-for-a-continuous-variable-be-greater-than-one"">here</a>, but I am not sure where would be better to ask it? This might sound a dumb question but I am really confused about it. According to Bayes' rule we do have the following:
$$p(\theta|X)=\frac{p(\theta)p(X|\theta)}{\int{p(\theta)p(X|\theta)d\theta}}$$I know that probability density function can be greater than one in general but it seems to me because there exist discrete summations of denominator integral which would is greater than the nominator, therefore posterior probability density function cannot be greater than one in any point. Is this correct?!</p>

<p>To explain reasoning a bit in more detail:</p>

<p>Suppose we are interested in $p(\theta=\theta_0|X)$ and we know that:
$$\int{p(\theta)p(X|\theta)}d\theta\approx\sum\limits_{n}{p(\theta_i)p(X|\theta_i)}$$ but now only consider the summations which include $p(\theta_0)p(\theta_0|X)$. Then the denominator will obviously be larger than the nominator.</p>
",2013-11-04 21:52:19.260
58854,23349.0,2,,58822.0,,,,CC BY-SA 3.0,"<p>Suppose we have a data set with y = 1 and x = [1/10 1/10] (one data point, two features).  One solution is to pick one of the features, another feature is to weight both features.  I.e. we can either pick w = [5 5] or w = [10 0].  </p>

<p>Note that for the L1 norm both have the same penalty, but the more spread out weight has a lower penalty for the L2 norm.  </p>
",2013-11-04 21:59:58.647
58860,7246.0,1,58864.0,,,"Are non-replacing draws from an infinite, non-shuffling pool independent?",<probability>,CC BY-SA 3.0,"<p>An infinite number of opaque jars are each filled with N spheres, p N of which are gold and (1 - p) N of which are tungsten (0 â‰¤ p &lt; 1; N > 0). You have drawn from a jar x spheres without replacement, all of which were made of tungsten (0 &lt; x &lt; N). N and E[p] are constant across jars.</p>

<p>Should you (a) keep drawing from the same jar, (b) switch to a different jar (assuming zero switching cost) or (c) not care?  </p>

<p><em>Why I'm asking: trying to figure out if the probability of finding a BTC block is always independent between hashes. A block is ""found"" when the hash of the block header is below a threshold. Three variables in the block header: the nonce value, time stamp, and coinbase address. The nonce value and time stamps are in a manageably finite discrete space. The coinbase address is in a practically infinite discrete space.</em></p>
",2013-11-04 23:10:10.990
58861,1804.0,1,59515.0,,,Question on calculating power for a prediction based study,<statistical-power>,CC BY-SA 3.0,"<p>I've calculated required power for a study in the past but I've come upon a scenario that I can't figure out quite how to do it. Pretty much I have a procedure that results in a particular complication 15% of the time. My study uses an imaging intervention on all study participants that we believe can predict the occurance of that complication >= 50% of the time. How would I go about calculating the required power/patients needed for a study like this?</p>

<p>Thank you.</p>
",2013-11-04 23:13:24.217
58862,9554.0,2,,57755.0,,,,CC BY-SA 3.0,"<p>First of all, this is a great description of your project and of the problem. 
And I am big fan of your home-made measurement framework, which is super cool... so why on earth does it matter what you call ""averaging the integrals""?</p>

<p>In case you are interested in some broader positioning of your work, what you would like to do is often referred to as <a href=""http://en.wikipedia.org/wiki/Anomaly_detection"" rel=""nofollow"">Anomaly detection</a>. In its simplest setting it involves comparing a value in a time-series against the standard deviation of the previous values. The rule is then if $$x[n] &gt; \alpha SD(x[1:n-1]) =&gt; x[n]\text{ is outlier}$$
where $x[n]$ is the $n^{th}$ value in the series, $SD(x[1:n-1])$ is the standard deviation of all previous values between the $1^{st}$ and $(n-1)^{th}$ value, and $\alpha$ is some suitable parameter you pick, such as 1, or 2, depending on how sensitive you want the detector to be. You can of course adapt this formula to work only locally (on some interval of length $h$),
$$x[n] &gt; \alpha SD(x[n-h-1:n-1]) =&gt; x[n]\text{ is outlier}$$</p>

<p>If I understood correctly, you are looking for a way to automate the testing of your devices, that is, declare a device as good/faulty after it performed the entire test (drew the entire diagonal). In that case simply consider the above formulas as comparing $x[n]$ against the standard deviation of all values.</p>

<p>There are also other rules you might want to consider for the purpose of classifying a device as faulty:</p>

<ul>
<li>if any deviation (delta) is greater than some multiple of the SD of all deltas</li>
<li>if the square sum of the deviations is larger than a certain threshold</li>
<li>if the ratio of the sum of the positive and negative deltas is not approximately equal (which might be useful if you prefer smaller errors in both directions rather than a strong bias in a single direction)</li>
</ul>

<p>Of course you can find more rules and concatenate them using boolean logic, but I think you can get very far with the three above. </p>

<p>Last but not least, once you set it up, you will need to test the classifier (a classifier is a system/model mapping an input to a class, in your case the data of each device, to either ""good"", or ""faulty""). Create a testing set by manually labelling the performance of each device. Then look into <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"" rel=""nofollow"">ROC</a>, which basically tells you the offset between how many devices your system correctly picks up out of the returned, in relation to how many of the faulty devices it picks up.</p>
",2013-11-04 23:31:30.570
58863,23355.0,1,,,,Visualizing/explaining multilevel model (dichotomous IV),<data-visualization><stata><multilevel-analysis>,CC BY-SA 3.0,"<p>I am using multilevel modeling (xtmixed in Stata) to predict a quasi-continuous level-3 DV using a dichotomous IV (sex). My results are markedly different from what the basic difference between men and women would suggest. No problem there - this is why I'm using MLM and not OLS regression or a t-test.</p>

<p>However, I'm having trouble describing the results. In particular, one of my graphs shows the difference in means, and it's more than a little confusing when you compare that to the formal test results. Does anyone know of a way to graphically present this kind of information in an intuitive way, or explain the apparent disparity of results without getting too technical?</p>
",2013-11-04 23:35:58.820
58864,1889.0,2,,58860.0,,,,CC BY-SA 3.0,"<p>It depends on whether (i) there are exactly $pN$ gold spheres at the start in your jar, or (ii) each of the $N$ spheres had an <em>independent</em> probability $p$ of being gold when put into the jar. </p>

<p>In case (i), you should stick with the jar you have partly emptied.  The probability the next sphere you draw from it is gold is  $\frac{pN}{N-x}$ which is greater than the probability $p$ that a sphere drawn from another jar is gold.</p>

<p>In case (ii), it does not matter.  The probability that your next sphere is gold is $p$ whether you draw it from the same jar or a different jar.</p>
",2013-11-05 01:45:08.497
58865,10684.0,2,,45534.0,,,,CC BY-SA 3.0,"<p>The cost of compliance for the company is $c_{ij}$ and the penalty for not complying is $\Lambda$. Since $c_{ij}$ is a random variable with cdf $F$, we have that $F(\Lambda)$ is another way of writing $P(c_{ij} &lt; \Lambda)$, or in other words the probability that the compliance cost will be less than the cost of not complying. Or in other words, the probability that the company will comply.</p>

<p>So saying that $F(\Lambda) &lt;1$ is just saying that the probability that the company will comply is less than $1$. Similarly, $F(\Lambda) &lt; 0.4$ means that there is less than a $40\%$ chance that the company will comply.</p>

<p>For your questions in the comments about how to derive (3) and (4), to get (4) you observe that the only way in which a company can do more damage under dealing is if it's in category $\beta$ on page 365 of the paper. This is the same as $\mathrm{max}\{c_{i1}, c_{i2}\} &lt; \Lambda$. Since $c_{i1}$ and $c_{i2}$ are independent, the probability of both of them being less than $\Lambda$ is
$$P(c_{i1}, c_{i2} &lt; \Lambda) = P(c_{i1} &lt;\Lambda)P(c_{i2} &lt; \Lambda) = F(\Lambda)^2$$
which gives (4).</p>

<p>To get (3), the company needs to be in category $\alpha$ on page 364, which means that one $c_{ij}$ has to be between $\Lambda$ and $2\Lambda$ and the other $c_{ij}$ has to be greater than $\Lambda$. The desired probability is
$$\alpha(\Lambda) = P((\Lambda &lt; c_{i1} &lt; 2\Lambda \text{ and } c_{i2} &gt; \Lambda) \text{ OR } (\Lambda &lt; c_{i2} &lt; 2\Lambda \text{ and } c_{i1} &gt; \Lambda))$$
but when you have the ``OR"" of two events you have to take into account that they might have outcomes in common, so you need to use the formula $P(X \text{ or } Y)=P(X) + P(Y) - P(X \text{ and } Y)$. Here, this gives you
$$\alpha(\Lambda) = P(\Lambda &lt; c_{i1} &lt; 2\Lambda \text{ and } c_{i2} &gt; \Lambda) + P(\Lambda &lt; c_{i2} &lt; 2\Lambda \text{ and } c_{i1} &gt; \Lambda) - P(\Lambda &lt; c_{i1} &lt; 2\Lambda \text{ and } c_{i2} &gt; \Lambda \text{ and } \Lambda &lt; c_{i2} &lt; 2\Lambda \text{ and } c_{i1} &gt; \Lambda)$$
which using independence reduces to
$$P(\Lambda &lt; c_{i1} &lt; 2\Lambda)P(c_{i2} &gt; \Lambda) + P(\Lambda &lt; c_{i2} &lt; 2\Lambda)P(c_{i1} &gt; \Lambda) - P(\Lambda &lt; c_{i1} &lt; 2\Lambda)P(\Lambda &lt; c_{i2} &lt; 2\Lambda)$$
which gives
$$\alpha(\Lambda) = 2(1-F(\Lambda))(F(2\Lambda)-F(\Lambda))-(F(2\Lambda)-F(\Lambda))^2$$
which is (3).</p>
",2013-11-05 01:46:04.520
58866,7007.0,2,,58848.0,,,,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/rv2eB.png"" alt=""enter image description here""></p>

<p>Consider the case $0&lt;\epsilon&lt;1$. Defining $\delta = \epsilon / (1+\epsilon)$, from the figure we have
$$
  |x-1|&lt;\delta \Rightarrow \left| \frac{1}{x}-1\right| &lt; \epsilon \, .
$$
Hence, 
$$
  P\left(\left| \frac{1}{X_n}-1\right| &lt; \epsilon\right) \geq P\left(\left| X_n-1\right| &lt; \delta\right) \to 1 \, ,
$$
when $n\to\infty$. The case $\epsilon\geq 1$ is easy, because
$$
  P\left(\left| \frac{1}{X_n}-1\right| &lt; \epsilon\right) \geq P\left(\left| \frac{1}{X_n}-1\right| &lt; \frac{1}{2}\right) \, ,
$$
and we can use the previous case.</p>
",2013-11-05 02:29:05.937
58875,17196.0,1,,,,Calculating the Kolmogorov-Smirnov coefficient,<hypothesis-testing><kolmogorov-smirnov-test>,CC BY-SA 3.0,"<p>I need to calculate the coefficient of the Kolmogorov-Smirnov test for any given null hypothesis rejection level. </p>

<p>For example, have a look at the table <a href=""http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test"" rel=""nofollow"">in this wikipedia entry</a>. It only gives $c(\alpha)$ for $\alpha=0.10, 0.05, 0.025, 0.01, 0.005, 0.001$. How can I calculate $c(\alpha)$ for a wider range of $\alpha$s? </p>

<p>If it is not trivial, can any one guide me on a place I can find a table for $c(\alpha)$ with larger $\alpha$ values?</p>

<p>Thanks in advance.</p>
",2013-11-05 10:15:58.833
58867,23360.0,1,,,,Variance of sample mean of bootstrap sample,<self-study><variance><bootstrap><cumulative-distribution-function><conditional-expectation>,CC BY-SA 3.0,"<p>Let $X_{1},...,X_{n}$be distinct observations (no ties). Let $X_{1}^{*},...,X_{n}^{*}$denote
a bootstrap sample (a sample from the empirical CDF) and let $\bar{X}_{n}^{*}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{*}$.
Find $E(\bar{X}_{n}^{*})$ and $\mathrm{Var}(\bar{X}_{n}^{*})$.</p>

<p>What I have so far is that $X_{i}^{*}$ is $X_{1},...,X_{n}$ each with probability $\frac{1}{n}$ so 
$$
E(X_{i}^{*})=\frac{1}{n}E(X_{1})+...+\frac{1}{n}E(X_{n})=\frac{n\mu}{n}=\mu
$$ and 
$$E(X_{i}^{*2})=\frac{1}{n}E(X_{1}^{2})+...+\frac{1}{n}E(X_{n}^{2})=\frac{n(\mu^{2}+\sigma^{2})}{n}=\mu^{2}+\sigma^{2}\&gt;,
$$ 
which gives 
$$
\mathrm{Var}(X_{i}^{*})=E(X_{i}^{*2})-(E(X_{i}^{*}))^{2}=\mu^{2}+\sigma^{2}-\mu^{2}=\sigma^{2} \&gt;.
$$</p>

<p>Then, 
$$E(\bar{X}_{n}^{*})=E(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{*})=\frac{1}{n}\sum_{i=1}^{n}E(X_{i}^{*})=\frac{n\mu}{n}=\mu
$$ 
and 
$$
\mathrm{Var}(\bar{X}_{n}^{*})=\mathrm{Var}(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{*})=\frac{1}{n^{2}}\sum_{i=1}^{n}\mathrm{Var}(X_{i}^{*})$$ 
since the $X_{i}^{*}$'s are independent. This gives $\mathrm{Var}(\bar{X}_{n}^{*})=\frac{n\sigma^{2}}{n^{2}}=\frac{\sigma^{2}}{n}$</p>

<p>However, I don't get the same answer when I condition on $X_{1},\ldots,X_{n}$ and use the formula for conditional variance:
$$
\mathrm{Var}(\bar{X}_{n}^{*})=E(\mathrm{Var}(\bar{X}_{n}^{*}|X_{1},...,X_{n}))+\mathrm{Var}(E(\bar{X}_{n}^{*}|X_{1},\ldots,X_{n})) \&gt;.
$$  </p>

<p>$E(\bar{X}_{n}^{*}|X_{1},\ldots,X_{n})=\bar{X}_{n}$ and $\mathrm{Var}(\bar{X}_{n}^{*}|X_{1},\ldots,X_{n})=\frac{1}{n^{2}}(\sum X_{i}^{2}-n\bar{X}_{n}^{2})$ so plugging these into the formula above gives (after some algebra) $\mathrm{Var}(\bar{X}_{n}^{*})=\frac{(2n-1)\sigma^{2}}{n^{2}}$. </p>

<p>Am I doing something wrong here? My feeling is that I am not using the conditional variance formula correctly but I'm not sure. Any help would be appreciated.</p>
",2013-11-05 03:18:40.277
58868,18998.0,1,58869.0,,,Calculating the probability of 31 of 628 items are sampled (no replacement) more than 10x amongst 150 participants drawing 50 items each,<probability><bayesian><binomial-distribution><combinatorics>,CC BY-SA 3.0,"<p>I am sorry for having to ask a simple probability question, but I have been thinking about it for weeks and an extensive google search has given no answers.</p>

<p>I have a group of 628 questions. 31 Questions belong to category A. 150 participants will each be given 50 randomly sampled questions from the total 628 questions (no replacement). What is the probability that questions belonging to category A will be answered more than 10 times by the group of participants?</p>

<p>I looked at a binomial distribution using R's binomial density function, but the best I could come up with was the probability of 1 item from category A being administered to the 150 participants if only 1 question was sampled from the total of 628 questions.</p>

<pre><code>    x &lt;- seq(1,50,by=1)
    high.biDen &lt;-dbinom(x,size=150,prob=((31/628)))
    round(high.biDen,2)

    round(sum(high.biDen[10:50]),2)
</code></pre>

<p>The answer obtained is .208 or a 20.8% probability that a question from category A will be administered to the participants more than 10 times if 1 question is sampled from the pool of 628 questions. I would like to know the probability if 50 questions are sampled instead of 1.</p>

<p>Is the answer obtained by some sort of bayesian technique? It seems to rely heavily on conditional probability seeing how if one item is sampled only 30 remain to be randomly selected.</p>

<p>Thanks so much for sharing your knowledge and expertise!</p>

<p>-Xander</p>
",2013-11-05 03:58:53.787
58869,22507.0,2,,58868.0,,,,CC BY-SA 3.0,"<p>(1) What is the probability $p_m$ that one participant receives exactly $m$ A-questions? For $m&gt;31$ it is zero, otherwise</p>

<p>$$ p_m = { {31 \choose m} {597 \choose 50-m} \over  {628 \choose 50} } $$</p>

<p>(Explanation: we should select $m$ A-questions than $(50-m)$ non-A-questions, and there are ${31 \choose m} {597 \choose 50-m}$ ways to do it.)</p>

<p>(2) What is the probability $q_{k,m}$ that $k$ participants together receive exactly $m$ A-questions? For k>0:</p>

<p>$$ q_{k,m} = \cases {p_m &amp; if $k=1$ \\ \sum_{n=0}^{\min(m,31)} p_n q_{k-1,m-n} &amp; otherwise} $$ </p>

<p>(Explanation: let $n$ be the number of A-questions received by the last participant.  The probability that he/she received $n$ A-questions, and previous participants received $m-n$ questions, is $p_n q_{k-1,m-n}$. Since we don't know how A-questions he receive, we should sum it by $n$.)</p>
",2013-11-05 05:38:35.593
58870,23367.0,1,,,,Why use ${1/\sigma^2}$ as a prior for $\sigma^2$?,<bayesian><prior>,CC BY-SA 3.0,"<p>In a lot of cases, the prior for $\sigma^2$ is chosen so that it is proportional to ${1/\sigma^2}$. I have a few queries re this:</p>

<ol>
<li>What is the intuition for this choosing this prior?</li>
<li>What is the information conveyed by this prior? Does it mean that a higher value for $\sigma^2$ is less likely?</li>
<li>I know that this is an improper prior, but is it non-informative? Sorry, I am not entirely sure how non-informative priors are different from improper priors.</li>
</ol>
",2013-11-05 07:22:39.053
58871,4318.0,1,,,,Estimating (MLE) 2D Vector Entries by a Noisy Samples of its Entries and its Norm,<estimation><maximum-likelihood>,CC BY-SA 4.0,"<p>I'd like your assistance with developing the Maximum Likelihood Estimator (MLE) and CRLB of the following case:</p>

<p>Given a 2D vector (A Point in XY Plane) $ p = ({x}_{p}, {y}_{p}) $.<br>
The measurements are noisy measurements of this vector entries and its norm.<br>
Namely the measurements vector is $ m = ({x}_{m}, {y}_{m}, {r}_{m}) $.<br>
Where the the distribution of those are given by:</p>

<p>$$ {x}_{m} \sim N({x}_{p}, {\sigma}_{x}), {y}_{m} \sim N({y}_{p}, {\sigma}_{y}), {r}_{m} \sim N(\sqrt{{x}_{p}^{2} + {y}_{p}^{2}}, {\sigma}_{r}) $$</p>

<p>Namely, the measurements are Normally Distributed, Unbiased and Independent of each other.</p>

<p>The parameters to estimate are $ \Theta = ({x}_{p}, {y}_{p}) $ given the measurement vector $ m = ({x}_{m}, {y}_{m}, {r}_{m}) $ as defined above.</p>

<p>In simple words, Estimate a point coordinates in the 2D plane given a noisy measurements of its coordinates and its range / distance from (0, 0).</p>

<p>I'd be happy to hear your solutions, ideas, related articles and the CRLB (Or any other lower bound on the estimation).   </p>
",2013-11-05 07:49:41.810
58872,11117.0,2,,58870.0,,,,CC BY-SA 3.0,"<p>First of all, $p(\sigma^2) \propto 1/\sigma^2$ is the Jeffreys prior (<a href=""http://en.wikipedia.org/wiki/Jeffreys_prior"" rel=""nofollow"">http://en.wikipedia.org/wiki/Jeffreys_prior</a>) for a scale parameter. It also coincides with the reference prior under certain conditions. </p>

<p>1) and 2):
Intuitively, it can be understood as the only prior expressing correclty that $\sigma$ is a scale parameter:  let $X$ distributed according to $1/\sigma .f(x/\sigma)$ then $Y=c.X$ has the same distribution than $X$ but with a different scale, $p(\sigma^2) \propto 1/\sigma^2$ also fits this property.</p>

<p>3):
Jeffreys priors are generally improper. However, to my knowledge improperness is not related to informativeness (but simply to integrability considerations).</p>
",2013-11-05 07:50:20.087
58873,22200.0,1,,,,Post-hoc test in the context of cluster analysis,<clustering><post-hoc>,CC BY-SA 3.0,"<p>I performed a cluster analysis based on a principal components. Total sample size is around 140. Now I want to describe the differences among the clusters (by means of the components) while performing a post-hoc analysis. By using the Levene test, I found out that the homogeneity of the variances can be assumed. I have the following questions: </p>

<p>1) Can I assume normal distribution for the ANOVA, since the components are z-transformed?</p>

<p>2) Which post-hoc test is appropriate? I have unequal simple size considering the clusters. I might go for Tukey-Kramer test.</p>
",2013-11-05 08:07:24.480
58874,11072.0,1,,,,What are the tool that can help me to find non-informative words in documents for classification and eliminate them?,<classification><natural-language>,CC BY-SA 3.0,"<p>Simply I want to remove words in documents that are not informative for classification purpose instead of biased stop word elimination. Is there any tool that can help me in that way? I know NLTK but it provides a simple stop word eliminator.</p>
",2013-11-05 08:46:03.203
58876,7615.0,1,69414.0,,,Interpretation of lasso recovery results,<correlation><lasso><causality><sparse>,CC BY-SA 3.0,"<p>When people say that lasso regression can under certain assumptions recover ""the support"", i.e. non-zero regression weights, what does this mean?</p>

<p>This cannot mean <em>causal</em> recovery, because Pearl has taught us you cannot infer causality from correlation (and lasso is just glorified correlation). So are these non-zeros in some way the minimum set of variables that cannot be explained by others? Because people tend to speak in terms of removing ""spurious"" and recovering ""non-spurious"" correlation, but really how can correlation be spurious, it's just the association between two random variables, there's no room for causal language here.</p>

<p>I am confused.</p>
",2013-11-05 10:19:28.547
58877,18198.0,1,,,,Converting standardized betas back to original variables,<regression><standard-error><standardization><predictor><centering>,CC BY-SA 3.0,"<p>I realise this is probably a very simple question but after searching I can't find the answer I am looking for.</p>

<p>I have a problem where I need to standardize the variables run the (ridge regression) to calculate the ridge estimates of the betas. </p>

<p>I then need to convert these back to the original variables scale.</p>

<p>But how do I do this?</p>

<p>I found a formula for the bivariate case that</p>

<p>$$
\beta^* = \hat\beta \frac{S_x}{S_y} \&gt;.
$$</p>

<p>This was given in D. Gujarati, <em><a href=""http://rads.stackoverflow.com/amzn/click/0073375772"">Basic Econometrics</a></em>, page 175, formula (6.3.8).</p>

<p>Where $\beta^*$ are the estimators from the regression run on the standardized variables and $\hat\beta$ is the same estimator converted back to the original scale, $S_y$ is the sample standard deviation of the regressand, and $S_x$ is the sample standard deviation.</p>

<p>Unfortunately the book doesn't cover the analogous result for multiple regression.</p>

<p>Also I'm not sure I understand the bivariate case? Simple algebraic manipulation gives the formula for $\hat\beta$ in the original scale:</p>

<p>$$
\hat\beta=\beta^* \frac{S_y}{S_x}
$$</p>

<p>It seems odd to me that the $\hat\beta$ that were calculated on variables which are already deflated by $S_x$, has to be deflated by $S_x$ again to be converted back? (Plus why are the mean values not added back in?)</p>

<p>So, can someone please explain how to do this for a multivariate case ideally with a derivation so that I can understand the result?</p>
",2013-11-05 10:56:30.327
58878,21756.0,2,,58855.0,,,,CC BY-SA 3.0,"<p>A typical approach (see e.g. <a href=""http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-28659-4"" rel=""nofollow"">Nelsen 2006</a>, p. 41) is to sample two independent uniform distributed random vectors $u$ and $y$ of the desired sample length. The conditional copula $C_u$ (conditioned on $u$) is given through the partial derivative: $$ C_u(v) = \frac{\partial}{\partial u} C(u,v) $$
Hence, one needs to solve $C_u(v)=y$ for $v$ to get the desired pair $(u,v)$. For a ""custom made"" copula, one has to calculate its partial derivative and its quasi-inverse. In case the copula is not completely ""custom made"" it might already be covered in other statistical software. One might for instance take a look into the R packages <a href=""http://cran.r-project.org/web/packages/copula/index.html"" rel=""nofollow"">copula</a> and <a href=""http://cran.r-project.org/web/packages/VineCopula/"" rel=""nofollow"">VineCopula</a> offering a rich set of families (speaking from my R experience, there are more in R and of course in other languages). </p>
",2013-11-05 11:01:09.727
58879,17196.0,2,,58875.0,,,,CC BY-SA 3.0,"<p>I found <a href=""http://www.jstatsoft.org/v08/i18/paper"" rel=""nofollow"">this paper</a>  (Marsaglia, et al. (2003)), that has a method to approximate the probability of obtaining a certain $D_n$ for a certain number of data. It Also has a <code>C</code> program that I used in my program and worked excellently.</p>
",2013-11-05 11:16:10.153
58880,18690.0,1,58895.0,,,Proof that there is no autocorrelation,<autocorrelation>,CC BY-SA 3.0,"<p>I was wondering whether my reasoning is correct and whether I've written it down correctly:</p>

<p>Given the following equation $bread_i=\beta_{0} + \beta_{1}wage + u_{t}$, where $u_{t}$ is normally distributed with a mean of zero and a variancce of $\sigma^{2}$ and is independent of $wage$.</p>

<p>Furthermore we know that there is no autocorrelation, $bread$ and $wage$ are not cointegrated, however there is a unit root present in $bread$ and $wage$.</p>

<p>Given a first difference model $\Delta bread_{t} = \beta_1 \Delta wage_{t}+\epsilon_{t}$, where $\epsilon = \Delta u_{t}$. Is the error term $\epsilon_{t}$ in this equation uncorrelated over time? Please motivate your answer with a formal derivation.</p>

<p>Well I thought the following:</p>

<p>Since $\epsilon = u_{t} - u_{t-1}$ and we know that there is no autocorrelation between them, after first differencing there will be no autocorrelation afterwards.</p>

<p>Question is of course how to show it? Using COV maybe? I am currently trying to derive it, but I'm a bit stuck.</p>

<p>Thanks in advance!</p>
",2013-11-05 11:16:50.937
58881,12503.0,1,,,,How to evaluate a regression model's sensitivity to noise,<regression><sensitivity-analysis>,CC BY-SA 3.0,"<p>How can I investigate the sensitivity of a regression model to noise?</p>

<p>A basic idea is to add some (Gaussian) noise to the dependent and/or independent variables and (re)evaluate the RMSE. </p>

<p>However, the problem with this approach is, that one is mostly interested in modeling the true data, and not the noisy data. 
Hence, is it reasonable to measure the RMSE on the original dependent variable, not the noisy one, even if the latter has been used for model building?</p>
",2013-11-05 11:42:05.483
58882,16665.0,1,58898.0,,,Categorical response variable prediction,<r><logistic><anova><categorical-data><multinomial-distribution>,CC BY-SA 3.0,"<p>I have the following kind of data (coded in R):</p>

<pre><code>v.a = c('cat', 'dog', 'dog', 'goat', 'cat', 'goat', 'dog', 'dog')
v.b = c(1, 2, 1, 2, 1, 2, 1, 2)
v.c = c('blue', 'red', 'blue', 'red', 'red', 'blue', 'yellow', 'yellow')
set.seed(12)
v.d = rnorm(8)
aov(v.a ~ v.b + v.c + v.d) # Error
</code></pre>

<p>I would like to know if the value of <code>v.b</code> or the value of <code>v.c</code> has any ability to predict the value of <code>v.a</code>. I would run an ANOVA (as shown above) but I think it does not make any sense since my response variable is not ordinal (it is categorical). What should I do?</p>
",2013-11-05 11:44:13.703
58883,11383.0,2,,58848.0,,,,CC BY-SA 3.0,"<p>Fix $\varepsilon\gt 0$. We have for each positive $A$,
$$\{|X_n^{-1}-1|\gt \varepsilon\}=\{|X_n-1|\gt |X_n|\varepsilon\}\subset\{|X_n-1|\gt A\varepsilon\}\cup\{|X_n|\leqslant A\},$$
this because we wrote $S=(S\cap \{|X_n|\gt A\})\cup (S\cap\{|X_n|\leqslant A\})$.</p>

<p>Take $A:= 1/2$; then  $\{|X_n|\leqslant 1/2\}\subset\{|X_n-1|\geqslant 1/2\}$ (because $(-1/2,1/2)\subset (-\infty,1/2)\cup(3/2,\infty)$). We thus obtained</p>

<p>$$\{|X_n^{-1}-1|\gt \varepsilon\}\subset\{|X_n-1|\gt \varepsilon/2\}\cup\{|X_n-1|\gt 1/2\}.$$
The probability of the last two events goes to $0$ as $n\to\infty$.</p>
",2013-11-05 12:35:28.053
58884,2958.0,2,,58702.0,,,,CC BY-SA 3.0,"<h2>Inclusion/exclusion of variates (step 3):</h2>
<p>I understand that you ask which of the original measurement channels to include into the modeling.</p>
<ul>
<li><strong>Is such a decision sensible for your data?</strong><br />
E.g. I work mainly with spectroscopic data, for which PLS is frequently and successfully used. Well measured spectra have a high correlation betweeen neighbour variates and the relevant information in spectroscopic data sets tends to be spread out over many variates. PLS is well suited for such data, but deciding on a variate-to-variate basis which variates to use for the model IMHO is usually not appropriate (decisions about inclusion/exclusion of spectral ranges based on spectroscopic knowledge about the application is IMHO a far better approach).</li>
<li>If for your data and application variable selection is a natural choice, <strong>is PLS the regularization technique you want?</strong><br />
You may want to read the sections about regularization  (3.4 - 3.6) in the <a href=""http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/"" rel=""nofollow noreferrer"">Elements of Statistical Learning</a> where PLS as a regularization is compared to other regularization approaches. My point here is that in contrast to e.g. the Lasso, PLS is a regularization technique that does <em>not</em> tend to completely exclude variables from the model. I'd thus say that PLS is probably more suitable for data where this behaviour is sensible, but in that case variable selection is not a natural choice (e.g. spectroscopic data).</li>
<li><strong>Does your data contain enough information for such a data-driven model optimization?</strong> Doing a t-test for each input variable is a massive multiple testing situation.<br />
<em>IMHO the main point of PLS (or other regularization techniques) is to avoid the need for such a variable selection.</em></li>
</ul>
<h2>Remark to Step 2:</h2>
<p>If you build a linear regression model in PCA score space, that is usually called principal component regression (PCR) in chemometrics. It is not the same as a PLS model.</p>
<h2>How to find out which variates are used by the PCA/PLS model?</h2>
<p>There are several ways to approach this question. Obviously, variates where the PCA loadings or PLS weights are 0 do not enter the model. Whether it is sufficient to look at the loadings or whether you need to go a step further depends on your data: if the data set is not standardized you may want to calculate how much each variate &quot;contributes&quot; to the respective PCA/PLS score.<br />
Literature where we did that with LDA (works just the same way): <a href=""http://softclassval.r-forge.r-project.org/2011/2011-07-01-ABC-Glioma-paper.html"" rel=""nofollow noreferrer"">C. Beleites, K. Geiger, M. Kirsch, S. B. Sobottka, G. Schackert and R. Salzer: Raman spectroscopic grading of astrocytoma tissues: using soft reference information, Anal. Bioanal. Chem., 400 (2011), 2801 - 2816.</a> The linked page has both links to the official web page and my manuscript.</p>
<p>You can also derive e.g. bootstrap distributions of the loadings (or the contributions) and have a look at them. For PCR and PLS coefficients that is straightforward, as the Y variable automatically &quot;aligns&quot; the coefficients. PCA and PLS scores need some more care, as e.g. flipping of the directions needs to be taken into account, and you may decide to treat models as equivalent where the scores which are then used for further modeling are just rotated or scaled  versions of each other. Thus, you may want to align the scores first e.g. by Procrustes analysis. The paper linked above also discusses this (for LDA, but again, the ideas apply to the other bilinear models as well).</p>
<p>Last but not least, you need to be careful not to overinterprete the models, and you can have situations where important variates have coefficients frequently touching the zero mark in the bootstrap experiments if you have correlation between variates. However, ehat you can or cannot conclude will depend on your type of data, though.</p>
",2013-11-05 12:54:06.930
27120,568.0,1,27132.0,,,Mean square error or mean squared error,<terminology><mse>,CC BY-SA 3.0,"<p>As a non-native english speaker I was wondering which of the <strong>square</strong> or <strong>squared</strong> expression I should use. For instance in mean <strong>square</strong> error or mean <strong>squared</strong> error.</p>

<p>According to the Internet, it seems both forms are used indistinctly. Is one expression more square than the other ?</p>
",2012-06-20 16:41:10.737
58885,7860.0,1,58887.0,,,Explain the meaning of $\alpha$ in a Kolmogorov-Smirnov two-sample test,<kolmogorov-smirnov-test>,CC BY-SA 3.0,"<p>The Wikipedia article on the <a href=""https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov.E2.80.93Smirnov_test"" rel=""nofollow"">two-sample Kolmogorov-Smirnov test</a> states that:</p>

<blockquote>
  <p>The Kolmogorovâ€“Smirnov test may also be used to test whether two
  underlying one-dimensional probability distributions differ. In this
  case, the Kolmogorovâ€“Smirnov statistic is</p>
  
  <p>$$D_{n,n'}=\sup_x |F_{1,n}(x)-F_{2,n'}(x)|$$</p>
  
  <p>where $F_{1,n}$ and $F_{2,n'}$ are the empirical distribution
  functions of the first and the second sample respectively. The null
  hypothesis is rejected at level $\alpha$ if</p>
  
  <p>$$D_{n,n'}&gt;c(\alpha)\sqrt{\frac{n + n'}{n n'}}.$$</p>
</blockquote>

<p>It is not clear to me the meaning of the <em>$\alpha$ level</em>. Where does it come from and what does it mean statistically?</p>
",2013-11-05 13:03:04.053
58886,23375.0,1,,,,Analytical formula for distribution of partial sum of standardized random variable,<distributions><normal-distribution><bootstrap><standardization>,CC BY-SA 4.0,"<p>I would like to know if there is an analytical formula for the distribution of partial sums of standardized random variables. (Of course, if one standardizes a random variable, the sum of <em>all</em> the individual observations will be zero).</p>

<p>I have written a Python function that estimates quantiles by bootstrapping. Each bootstrap iteration consists of three steps:</p>

<ol>
<li><code>n</code> samples of a Normal(0, 1) random variable are generated.</li>
<li>The resulting sample is then standardized.</li>
<li>The sum of the first <code>k</code> elements of the standardized sample is calculated.</li>
</ol>

<p>This procedure is followed <code>niter</code> times, generating a sample of size <code>niter</code>. The quantiles of this sample are then calculated. </p>

<p>In case it helps, here is the code:</p>

<pre><code>import numpy as np
import scipy.stats.mstats

def boot_standardized(n, k, niter=1000, prob=None):
    if prob is None:
        prob = [0.05, 0.1, 0.5, 0.9, 0.95]

    x = np.random.randn(n, niter)
    x_std = (x - np.mean(x, axis=0))/np.std(x, axis=0)

    boot_values = np.sum(x_std[:k,:], axis=0)

    return scipy.stats.mstats.mquantiles(boot_values, prob=prob)
</code></pre>
",2013-11-05 13:08:15.937
58887,16043.0,2,,58885.0,,,,CC BY-SA 3.0,"<p>The level $\alpha$ is the ""significance level"" of the test, the rate of <a href=""http://en.wikipedia.org/wiki/Type_I_and_type_II_errors"" rel=""nofollow"">Type I error</a>, the probability of detecting a difference under the assumptions of the null hypothesis (that the two samples are drawn from the same distribution).</p>
",2013-11-05 13:09:57.440
58888,11383.0,2,,58800.0,,,,CC BY-SA 3.0,"<p>If $X$ is a discrete random variable taking its vales in $S$ and $h$ is a non negative function, then 
$$\mathbb E[h(X)]=\sum_{x\in S}h(x)\mathbb P\{X=x\}.$$
Indeed, we formally have 
$$\mathbb E[h(X)]=\mathbb E[h(X)\chi_{\bigcup_{x\in S}\{X=x\}}]=\sum_{x\in S}\mathbb E[h(X)\chi_{\{X=x\}}].$$
Now we use this with the map $x\mapsto e^{tx}$, where $t$ is fixed. </p>

<p>We don't need the partial derivative as there is no ambiguity: we can only take the derivative with respect to $t$.</p>
",2013-11-05 13:31:07.547
58889,23376.0,1,,,,General location model,<self-study><categorical-data><continuous-data><joint-distribution>,CC BY-SA 3.0,"<p>Let $Z_1$ and $Z_2$ be categorical random variables with $3$ and $2$ categories, respectively. Let $Y_1$ and $Y_2$ be $2$ continuous random variables. Define completely the GLOM (general location model) for the joint distribution of $Z=(Z_1,Z_2)^T$ and $Y=(Y_1,Y_2)^T$ </p>

<p>I couldn't solve this problem. Can anyone help me?</p>
",2013-11-05 13:33:17.720
58890,2958.0,2,,58435.0,,,,CC BY-SA 3.0,"<p>First of all, as @Marc Claesen already explained, semi-supervised classification is one of the techniques to take care of the situation where you know that the classes are really distinct, but you are not certain which class the case actually belongs to. </p>

<p>However, there are related situations as well, where the the ""reality"" isn't that clear, and the assumption of having really distinct classes is not met: bordeline cases may be a ""physical"" reality (see below for papers about an application where we met such a condition).  </p>

<p>There is one <strong>crucial assumption for semi-supervised classifers</strong> that you need to make sure is met: the assumption that <strong>in feature space, class boundaries come along with low sample density</strong>. This is referred to as the cluster assumption.<br>
Even if the reality underlying your data has distinct classes, your data set may have disproportionally more borderline cases: e.g. if your classification technique is targeted at classifying difficult cases, while the clear and easy cases are not of interest and already your training data reflects this situation. </p>

<blockquote>
  <p>only taking ""certain"" classifications for training? I fear that in this case, there will be more misclassifications because ""border"" cases are not covered.</p>
</blockquote>

<p>I fully agree with you that excluding the borderline cases is often a bad idea: 
 by removing all difficult cases you end up with an artificially easy problem. IMHO it is even worse that excluding borderline cases usually does not stop with model training, but the borderline cases are also excluded from testing, thus testing the model only with easy cases. With that you'd not even realize that the model does not perform well with borderline cases. </p>

<p>Here are two papers we wrote about a problem that differs from yours in that in our application also the reality can have ""mixed"" classes (a  more general version of your problem: uncertainty in reference labels is covered as well).</p>

<ul>
<li>The appliation: brain tumour diagnostics. We used logistic regression. Semi-supervised modeling was not appropriate as we cannot assume low sample density at class boundaries.<br>
<a href=""http://softclassval.r-forge.r-project.org/2011/2011-07-01-ABC-Glioma-paper.html"">C. Beleites, K. Geiger, M. Kirsch, S. B. Sobottka, G. Schackert and R. Salzer: Raman spectroscopic grading of astrocytoma tissues: using soft reference information, Anal. Bioanal. Chem., 400 (2011), 2801 - 2816. </a> </li>
<li>Theory paper deriving a general framework for measuring the performance of the classifier for borderline cases.<br>
<a href=""http://softclassval.r-forge.r-project.org/2013/2013-01-03-ChemomIntellLabSystTheorypaper.html"">C. Beleites, R. Salzer and V. Sergo:<br>
Validation of Soft Classification Models using Partial Class Memberships: An Extended Concept of Sensitivity &amp; Co. applied to Grading of Astrocytoma Tissues<br>
Chemom. Intell. Lab. Syst., 122 (2013), 12 - 22.</a>  </li>
</ul>

<p>The links go to a project page of an R package I developed to do the performance calculations. There are further links to  both the official web page and my manuscripts of the papers.
While I have not used Weka so far, <a href=""http://markahall.blogspot.de/2012/07/r-integration-in-weka.html"">I understand that an interface to R is available</a>.</p>

<hr>

<h3>practical considerations:</h3>

<ul>
<li>While the copy-and-label-differently approach is straightforward, it does not work well with all classifiers and implementations in practice. E.g. AFAIK there is no way to tell <code>libSVM</code>s tuning by cross validation that all copies of each data point need to be kept in the same cross validation fold. Thus <code>libSVM</code>s tuning would probably yield a massively overfit model. </li>
<li>Also for logistic regression, I found that many implementations did not allow the partial membership labels I needed. </li>
<li>The implementation I used for the papers above is actually an ANN without hidden layer using the logistic as sigmoidal link function (<code>nnet::multinom</code>). </li>
</ul>
",2013-11-05 13:38:54.717
58891,,2,,58856.0,user31668,,,CC BY-SA 3.0,"<p>For your first question: Yes, you can substitute sufficient statistic as it is just  a re-writing of $\sum x_i$</p>

<p>For your second part, since your data are normal, you can use the <a href=""http://en.wikipedia.org/wiki/Likelihood-ratio_test"" rel=""nofollow"">Wilks Likelihood ratio test</a> to get a rejection region.</p>
",2013-11-05 13:43:52.100
58892,23377.0,1,,,,Transfer functions in R (TSA package),<r><time-series><arima>,CC BY-SA 3.0,"<p>In Time Series modelsâ€™ transfer functions there is a decay parameter in the
formula (letâ€™s call it b). In TSA package that decay parameter is not mentioned. When I used other software before (such as SAS) I used to determine b after analyzing â€˜prewhitened seriesâ€™. But
in TSA package in R there is no need to specify the decay parameter once you analyze CCF?</p>

<p>If not how am I going to know when the decay starts?</p>

<p>I understand CCF is used after prewhitening to determine how to
filter the outputs but where b comes into the picture?</p>
",2013-11-05 14:10:31.143
58893,13740.0,1,58920.0,,,Comparing (and testing) two discrete distributions with different magnitudes,<distributions><chi-squared-test><survey><paired-comparisons>,CC BY-SA 3.0,"<p>I am comparing authoritative survey data (large amount of observations) with data gained from a social network (very small amount of observations). Particularly, I want to compare population per district as surveyed with population per district as found in a location based social network.</p>

<p>Example dataset:</p>

<pre><code>   type variable   value
1     1      vgi    1064
2     2      vgi     873
3     3      vgi       8
4     4      vgi     246
9     1      pop 2248360
10    2      pop 3544721
11    3      pop   70934
12    4      pop 2090647
</code></pre>

<p><code>type</code> is the district (1-4), <code>variable=vgi</code> denotes users found in the social network while <code>variable=pop</code> is the actual population size per distcrict. </p>

<p>Even though the scales are completely different in magnitude, is there a way to qualitatively (e.g. with a plot) and quantitatively compare both distributions?</p>

<p>With qualitative I mean a plot where one can easily see which district is likely <em>under- or overrepresented</em> on social media and with quantitative I mean something like a Chi-Square-Test in order to see whether the distributions <em>significantly</em> differ from each other. For example, one can see from the data that district <code>2</code> is underrepresented on <code>vgi</code>, or one could also say that district <code>1</code> is overrepresented on <code>vgi</code> -- but that is the problem - what is considered over- or underrepresented?! </p>

<p>I don't have experience with such data, thus I am asking. I was able to plot both distributions with R, but the different scales make them hard to compare - I should probably transform one of both types but I don't know how. </p>
",2013-11-05 14:11:48.850
58901,23384.0,1,,,,Equations from linear regression,<regression><self-study><mathematical-statistics><mixed-model>,CC BY-SA 3.0,"<p><strong>Notation.</strong> Let $y$, $a$, and $b$ be $n\times 1$, $p\times 1$, and $q\times1$ real vectors. Let also $X$ and $Z$ be $n\times p$ and $n \times q$ real matrices. </p>

<p>Suppose that there is no solution, $a$, to $y = X a$.</p>

<p><strong>Question</strong>. What are the conditions on $Z$ such that $y = Xa + Zb$ has no solution for each choice of $b$?</p>

<p><strong>Context</strong>. I came across this problem in the context of linear regression. The fact that $y=Xa$ has no solution can be interpreted as ""no hyperplane can perfectly fit the data"". I am analysing an extension of this problem which has lead me to the need for finding something similar for ""$y = Xa + Zb$ has no solution"", but in this case $b$ is not fixed and can actually take any value in ${\mathbb R}^q$.</p>
",2013-11-05 15:16:51.700
58894,23378.0,2,,58847.0,,,,CC BY-SA 3.0,"<p>If you have R2012b or later, use the RUSBoost algorithm. It is recommended for imbalanced datasets.</p>

<p>If you go with GentleBoost, you need to optimize the tree complexity and the number of trees in the ensemble. (You could also play with the learning rate.) Both parameters are likely far off their optimal values in your code. </p>

<p>First, fitensemble for GentleBoost by default produces decision stumps (trees with two leaves). Since the minority class is only 8% of the data, stumps are not sensitive to observations of the minority class. I often set the minimal leaf size to one half of the size of the minority class. The optimal setting for the leaf size may not be exactly that but should be in that ballpark. Do:</p>

<pre><code>tmp = ClassificationTree.template('minleaf',some_number);
ens = fitensemble(Xtrain,Ytrain,'GentleBoost',Ntrees,tmp,'prior','uniform')
</code></pre>

<p>Second, 10 trees are most usually not enough. Inspect the ensemble accuracy by cross-validation or using an independent test set to decide how many trees are needed. Typically, a few hundred should be used for boosting.</p>

<p>Also, after you train the ensemble, don't just look at the classification error. Use the perfcurve function to compute a performance curve and find the optimal threshold on the classification score. It is up to you to define what ""optimal"" means. You can assign, for instance, different misclassification costs to the two classes and find the threshold minimizing the expected cost.
.....</p>
",2013-11-05 14:15:25.160
58895,20473.0,2,,58880.0,,,,CC BY-SA 3.0,"<p>I will just use $y$ for bread and $x$ for wage. You write that ""a unit root is present in both $y$ and $x$."" Although this is rather vague, let's say that we have</p>

<p>$$y_t=y_{t-1}+v_t,\Rightarrow \Delta y_t = v_t$$
$$ x_t=x_{t-1}+\omega_t \Rightarrow \Delta x_t = \omega_t$$</p>

<p>with $v_t$ and $\omega_t$ independent white noises.</p>

<p>Then in the first difference specification
$$\Delta y_t=\beta_{1}\Delta x_t + \Delta u_{t}$$ 
we substitute using the expressions for $\Delta y_t$ and $\Delta x_t$ to obtain</p>

<p>$$v_t=\beta_{1}\omega_t + \Delta u_{t} \Rightarrow v_t-\beta_1\omega_t = \Delta u_t$$</p>

<p>So $\Delta u_t$ is a linear combination of independent white noises, and therefore white noise itself.</p>

<p><strong>ADDENDUM</strong> </p>

<p>$$\operatorname {Cov}(\Delta u_t, \Delta u_{t-1}) = E\left( \Delta u_t\Delta u_{t-1}\right)  = E\left( ( v_t-\beta_1\omega_t)( v_{t-1}-\beta_1\omega_{t-1})\right)$$
$$=E(v_tv_{t-1})-\beta_1E(v_t\omega_{t-1})-\beta_1E(\omega_t v_{t-1})+ \beta_1^2E(\omega_t\omega_{t-1})$$
$$=0+0+0+0=0$$
since we have white noises, and independent between them.</p>
",2013-11-05 14:17:00.047
58896,15827.0,2,,58839.0,,,,CC BY-SA 3.0,"<p>This may not the answer you seek, but in general </p>

<ul>
<li><p>The first things to check are how close $b_3$ in M1 is to $b_1 b_2$ in M2 and whether predicted values match each other. AIC and F-tests tell you how well each model fits, but they say nothing about how the models differ. Simple numeric and graphical comparisons may tell you more. </p></li>
<li><p>In M1 the value of $b_3$ is unconstrained and in M2 it is constrained. If the criterion is closeness of fit to the data in some absolute sense, then it would be surprising if a constrained fit were better. Otherwise the comparison will hinge on precisely whether and how you penalise yourself for using one more parameter in M1. So, watch out: you won't get anything out of AIC or similar or dissimilar figures of merit that is not a strict consequence of how they are defined. That no doubt is obvious, but it is important. </p></li>
</ul>
",2013-11-05 14:21:22.350
58897,23380.0,2,,58850.0,,,,CC BY-SA 3.0,"<p>The function would be the same as the one for balanced data - TreeBagger or fitensemble. By default, either grows deep trees; the default minimal leaf size is 1 for classification. This typically gives you enough sensitivity to find a good decision boundary between the classes. The <em>default</em> decision boundary, at which the class posterior probabilities are equal, is most usually not what you want for imbalanced data. As I advised in your other post, use the perfcurve function to find the optimal threshold on the posterior probability for the minority class.</p>

<p>By the way, try MATLAB Answers <a href=""http://www.mathworks.com/matlabcentral/answers"" rel=""nofollow"">http://www.mathworks.com/matlabcentral/answers</a> for MATLAB questions. I read that site much more often than this one.</p>
",2013-11-05 14:29:26.380
58898,8958.0,2,,58882.0,,,,CC BY-SA 4.0,"<p>You could use ANY classifier. Including Linear Discriminants, multinomial logit as Bill pointed out, Support Vector Machines, Neural Nets, CART, random forest, C5 trees, there are a world of different models that can help you predict <span class=""math-container"">$v.a$</span> using <span class=""math-container"">$v.b$</span> and <span class=""math-container"">$v.c$</span>. Here is an example using the R implementation of random forest:</p>

<pre><code># packages
library(randomForest)

#variables
v.a= c('cat','dog','dog','goat','cat','goat','dog','dog')
v.b= c(1,2,1,2,1,2,1,2)
v.c= c('blue', 'red', 'blue', 'red', 'red', 'blue', 'yellow', 'yellow')

# model fit
# note that you must turn the ordinal variables into factor or R wont use
# them properly
model &lt;- randomForest(y=as.factor(v.a),x=cbind(v.b,as.factor(v.c)),ntree=10)

#plot of model accuracy by class
plot(model)
</code></pre>

<p><img src=""https://i.stack.imgur.com/tuK6r.png"" alt=""enter image description here""></p>

<pre><code># model confusion matrix
model$confusion
</code></pre>

<p>Clearly these variables don't show a strong relation.</p>
",2013-11-05 14:33:44.167
58899,594.0,2,,58800.0,,,,CC BY-SA 3.0,"<p>There's a potential ambiguity in your question:</p>

<p>If you're asking (a) ""Why is $e^{tX}$ inside the expectation?"" then the answer is 'that's the <a href=""https://en.wikipedia.org/wiki/Moment_generating_function#Definition"" rel=""nofollow"">definition of the moment generating function</a>'. </p>

<p>On the other hand, if you're asking (b) ""Given we want $E(e^{tX})$, why does $e^{tX}$ appear in the summation?"", this would be because <a href=""https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician"" rel=""nofollow"">that's how expectation works</a>.</p>

<p>That is, for the continuous case $E[g(X)] = \int g(x) f(x) dx$ (indeed, some authors seem to regard that law as a definition of expectation).</p>

<blockquote>
  <p>Why do we use the derivative notation instead of the partial derivative operator?</p>
</blockquote>

<p>Because $X$ isn't regarded an argument of $M$, $t$ is. If it were $M(X,t)$, then we'd write $\partial$, but it's $M_X(t)$, so we write $d$. This isn't some accident of notation, however - it really is a function of $t$ we're dealing with, though calculated with respect to the distribution of $X$. Inside the integral/sum, $x$ is actually a <em>dummy variable</em>; it could as easily be represented by $u$, or any other dummy, and $M$ is certainly not a function of the dummy variable in any sense. You might think of $X$ like an indexing variable (since it determines which $M$ you get), but if $M$ is to be regarded as a function of something to do with $X$, it's really $F_X$ (through $dF$) that $M$ is a function of.</p>
",2013-11-05 14:53:19.913
58900,750.0,2,,58219.0,,,,CC BY-SA 3.0,"<p>I will walk through how I generated the exact statistics for the $\chi^2$ distribution and (hopefully) update later with a paper that gives tables for the exact distributions and where the exact distribution converges to the theoretical $\chi^2$ with $6$ degrees of freedom. Ditto for the K-S and Kuiper distributions that ttnphns mentions.</p>

<p>So the steps to generate the exact distributions are:</p>

<ol>
<li>Generate all potential combinations of outcomes allocating crimes into the $7$ weekday bins. The total number of combinations ends up being $\binom{M + N - 1}{M - 1}$ where $M = 7$ weekday bins and $N$ equals the number of crimes observed.</li>
<li>For each of the combinations, calculate the probability of observing that outcome under the null hypothesis. Here the null is that each crime has a multinomial distribution in which the weekdays are the outcomes and are equi-probable (e.g. each day has a probability of $1/7$ of being selected).</li>
<li>Generate the test statistic for that set of observations.</li>
</ol>

<p>From this information you can generate critical values for the test distributions under the null. So If you observe $3$ crimes, with two occurring on Monday and one on Tuesday, the probability of that event under the null is:</p>

<p>$$Pr(\text{Mon.} = 2, \text{Tues.} = 1) = \frac{3!}{2!1!} \cdot ({P_m}^2)({P_t}^1) = 0.00874635568513119$$</p>

<p>Where $P_m$ and $P_t$ symbolize the probability of an event on Monday and Tuesday respectively, and $P_m = P_t = 1/7$. (If you wanted to generalize to a window in which say spans 10 days, you may want to consider unequal probalities of $2/10$ for the overlapping days and $1/10$ for the others.)</p>

<p>For an example of generating the exact distribution of the test statistic, with three crimes there are only three different possible $\chi^2$ values out of the 84 different combinations (since order doesn't matter for the statistic). The below table symbolizes these potential outcomes. (Just imagine sorting the days of the week so the day of the week with the most events is in the left most column.)</p>

<pre><code>A  B   C
.
.  .
.  ..  ...
</code></pre>

<p>Subsequently, combinations of <code>A</code> appear 7 times, <code>B</code> 42 times, and <code>C</code> 35 times. The below table shows the probabilties of obtaining said $\chi^2$ statistics and how to generate the CDF of the null hypothesis. From here you can see that it is actually possible to reject the null at a .05 critical level if all three events are observed on the same day.</p>

<pre><code>    #  ChiSq    Prob(Sum) CDF
C  35    4         .61    .61
B  42    8.67      .37    .98
A   7   18         .02   1.0 
</code></pre>

<p>Also from the set of all potential combinations you can generate the distributions under various alternative hypotheses, and this allows you to evaluate the power of the test under those circumstances. So for example for $5$ crimes, the exact $\chi^2$ distribution has a $.05$ critical value at $10.4$. So for an alternative hypothesis of data having positive probability in <em>only</em> two days, you have 100% power (i.e. the only observable $\chi^2$ values if 5 crimes occur in 2 or fewer days of the week is over $10.4$).</p>

<p>The image below shows the CDFs for the exact $\chi^2$ distribution with $5$ crimes in $7$ weekday bins (in light grey lines), CDFs for different alternative hypotheses in dark grey lines, and the critical value $\chi^2$ highlighted with a red guideline. The alternative hypotheses are for differing numbers of days that are equi-probable for the crimes to occur on for $1$ to $4$ days during the week. </p>

<p><img src=""https://i.stack.imgur.com/1KLZY.png"" alt=""enter image description here""></p>

<p>You can see from this chart even for an alternative of equi-probable chances over three days for just 5 crimes the power is just under 40%, 1 minus the CDF of the alt. hypothesis where it intersects the critical value. (Earlier I wrote that a tie goes to rejecting the null, but that would be incorrect as the Type 1 error would be inflated to .39 instead of .02 in my 3 crimes example.)</p>

<hr>

<p>I have a paper going through this same analysis and generating critical values for Kuiper's $V$ and the $\chi^2$ test now posted on <a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2476536"" rel=""nofollow noreferrer"">SSRN</a>, <em>Testing for Randomness in Day of Week Crime Sprees with Small Samples</em>.</p>
",2013-11-05 14:59:31.733
58902,17573.0,2,,58622.0,,,,CC BY-SA 3.0,"<p>You asked for an algebraic answer to 1, and it is given below.  However, a non-algebraic answer would be much better, so that is given first.  </p>

<p>The usual technique used to run linear regressions is called Ordinary <em>Least</em> Squares (OLS).  The formula you give above is the formula for the OLS estimator.  However, the formula you give above is not the definition of the OLS estimator.  The definition of the OLS estimator is ""that coefficient vector, $\hat{\beta}$, which minimizes the sum of squared residuals.""  First, verbally . . .</p>

<p>Define the number of observations as $N$, so that each of the $\delta$ and $\alpha$ models have $N$ observations, and the $\beta$ model has $2N$.  Consider the $\delta$ and $\alpha$ which solve the first two OLS problems you pose above, and ask, do they (transformed as you have transformed them) solve the third, pooled, $\beta$, OLS problem?  Well, think about that third problem's objective function $\sum_{i=1}^{2N} (Y_i-\hat{Y}_i)^2$.  We know that $\hat{\alpha}$ minimizes the first $N$ terms (definition of OLS) and has no effect on the last $N$ terms (by inspection).  We know that $\hat{\delta}$ minimizes the last $N$ terms (definition of OLS) and has no effect on the first $N$ terms (by inspection).  Last, there are ""enough"" $\beta$ such that the $\alpha$ and $\delta$ may be adjusted independently in the third problem.  That is essentially a proof that the OLS estimators have to be exactly the same (again up to your transformation).  The proof is not completely formal and explicit, but it's not hard from here to make it so.</p>

<p>To your question 2, yes this result is true much more generally.  For any estimator defined by an optimization problem (any ""M Estimator,"" like maximum likelihood for example), there is a result like this. The key requirements are that the parameter spaces of the pooled and separate models are ""the same"" up to an invertible transformation (like the one you gave to transform $\alpha$ and $\delta$ to $\beta$) and that the objective function of the pooled model can be decomposed (linearly or multiplicatively) into non-interacting parts corresponding (up to an increasing transformation) to the objective functions of the separate problems.  This is a lot of models.</p>

<p>The kind of argument I give above is incredibly useful when you are using any kind of M-estimator.  For example, it is a famous result for OLS that if you re-scale (change the units of) one of the $X$ variables, that its coefficient will be rescaled by OLS in an exactly offsetting way and that nothing else about the OLS estimator will change. This can be proved by a fairly tedious algebra exercise or by a very brief optimization argument like the one I gave above.  The result is not just true for OLS, though.  Any M estimator which has the $X$ multiplied against a coefficient will have this magic re-scaling property as well.</p>

<p>Now, to the algebraic demonstration.  Changing your notation a little for clarity, start by comparing estimating these two equations by OLS:
\begin{align}
Y_1 &amp;= X_1\delta + \epsilon_1 \\
Y_2 &amp;= X_1\alpha + \epsilon_2
\end{align}
With estimating this equation by OLS:
\begin{align}
\left(\begin{array}{r} Y_1 \\ Y_2 \end{array} \right) &amp;=
\left[\begin{array}{r r} X_1 &amp; 0 \\ X_1 &amp; X_1 \end{array} 
\right]
\left(\begin{array}{r} \gamma_1 \\ \gamma_2 \end{array}
\right)
+ \left(\begin{array}{r} \epsilon_1 \\ \epsilon_2 \end{array}
\right) \\ \strut \\
Y &amp;= X\gamma + \epsilon
\end{align}
My $\gamma_1$ is your $\beta_0$ and $\beta_1$ stacked up and etc.
\begin{align}
\hat{\gamma} &amp;= (X'X)^{-1}X'Y \\ \strut \\
             &amp;= 
\left[\begin{array}{r r} X_1'X_1 &amp; 0 \\ 2X_1'X_1 &amp; X_1'X_1 \end{array} 
\right]^{-1}
\left(\begin{array}{r} X_1'Y_1 \\ X_1'Y_1 + X_1'Y_2 \end{array}
\right) \\ \strut \\
&amp;= 
\left[\begin{array}{r r} (X_1'X_1)^{-1} &amp; 0 \\ -2(X_1'X_1)^{-1} &amp; (X_1'X_1)^{-1} \end{array} 
\right]
\left(\begin{array}{r} X_1'Y_1 \\ X_1'Y_1 + X_1'Y_2  \end{array}
\right) \\ \strut \\
\left(
\begin{array}{r} \hat{\gamma}_1 \\ \hat{\gamma}_2 \end{array}
\right)
&amp;= \left(
\begin{array}{r} (X_1'X_1)^{-1}X_1'Y_1 \\ 
-2(X_1'X_1)^{-1}X_1'Y_1+(X_1'X_1)^{-1}X_1'Y_1 + (X_1'X_1)^{-1}X_1'Y_2 \end{array}
\right) \\ \strut \\
\left(
\begin{array}{r} \hat{\gamma}_1 \\ \hat{\gamma}_2 \end{array}
\right)
&amp;= \left(
\begin{array}{r} \hat{\delta}_1 \\ \hat{\alpha}_1 - \hat{\delta}_1 \end{array}
\right)
\end{align}</p>

<p>That's exactly what you got.</p>
",2013-11-05 15:36:42.763
58903,23385.0,1,,,,GPML gives too large length scale when optimising hyperparameters,<regression><matlab><gaussian-process>,CC BY-SA 3.0,"<p>I recently started trying to apply Gaussian process regression to a problem, using the MATLAB GPML toolbox. The problem has five (or more) input variables, but for now I'm just looking at one of them. </p>

<p>With manually selected hyperparameters, I get reasonable-looking output, but my problem is that when I <code>minimize()</code> the hyperparameters, I get a very large length scale, thus producing a flat mean and covariance function.</p>

<p>If I run the following code, <code>hyp2.cov = [5.4, 1.1]</code> after the call to minimize, so the length scale is <code>exp(5.4)=214</code>, even though the data points are on the interval [-0.14, 0.06].</p>

<p>Could anyone tell me why this is so?</p>

<pre><code>x=[-0.0350; 0.0550;-0.0830;-0.1360; 0.0190];
y=[3.0700;3.3200;3.0400;3.0600;2.9200];
covfunc = @covSEiso; hyp2.cov = [0; 0]; hyp2.lik = log(0.1);
likfunc = @likGauss; sn = 0.1; hyp2.lik = log(sn);
hyp2 = minimize(hyp2, @gp, -100, @infExact, [], covfunc, likfunc, x, y);
</code></pre>
",2013-11-05 15:56:56.647
58904,23389.0,1,,,,Combining results with different confidence levels,<confidence-interval>,CC BY-SA 3.0,"<p>I have an experiment where I throw a biased coin were with confidence interval [0.38, 0.42] and confidence level of 0.95 I will get heads. If I get heads then I throw a biased 6-sided die which gives me 1 with probability given in the interval [0.48, 0.52] and confidence level 0.90.</p>

<p>What is the probability of getting 1 and what is my confidence level for that result. If no intervals and no confidence levels the problem is trivial and could be done with simple multiplication, but I am pretty sure simple multiplication will not be a solution in interval case.</p>
",2013-11-05 16:51:39.300
58905,14965.0,1,58910.0,,,Entropy and Likelihood Relationship,<entropy>,CC BY-SA 3.0,"<p>This is a theoretical question.</p>

<p>Suppose that I have a sample s1 coming from distribution K, and a sample s2 coming from distribution M. But I don't know what K or M are. I hypothesize that s1 and s2 are coming from distribution T. I plug in s1 and s2 to pdf of T and calculate their likelihoods, say L(s1), L(s2).  If I know differential entropies of K and M, being H(K) and H(M) respectively, can I find a relation between L(s1) and L(s2) in terms of H(K) and H(M). Basically what I am trying to show is something along the lines of this:</p>

<p>$\frac {L(s1)}{L(s2)} \sim \frac {e^{H(K)}} {e^{H(M)}}$</p>

<p>Is this even meaningful?</p>

<p>Thanks</p>
",2013-11-05 17:08:48.903
58906,3580.0,1,,,,Lasso-ing the order of a lag?,<feature-selection><lasso><regularization>,CC BY-SA 3.0,"<p>Suppose I have longitudinal data  of the form $\mathbf Y = (Y_1, \ldots, Y_J) \sim \mathcal N(\mu, \Sigma)$ (I have multiple observations, this is just the form of a single one). I'm interested in restrictions on $\Sigma$. An unrestricted $\Sigma$ is equivalent to taking
$$
Y_j = \alpha_j + \sum_{\ell = 1} ^ {j - 1} \phi_{\ell j} Y_{j-\ell} + \varepsilon_j
$$
with $\varepsilon_j \sim N(0, \sigma_j)$. </p>

<p>This is typically not done since it requires estimating $O(J^2)$ covariance parameters. A model is ""lag-$k$"" if we take
$$
Y_j = \alpha_j + \sum_{\ell = 1} ^ k \phi_{\ell j} Y_{j - \ell} + \varepsilon_j,
$$
i.e. we only use the preceding $k$ terms to predict $Y_j$ from the history.</p>

<p>What I'd really like to do is use some kind of shrinkage idea to zero out some of the $\phi_{\ell j}$, like the LASSO. But the thing is, I also would like the method I use to prefer models which are lag-$k$ for some $k$; I'd like to penalize higher order lags more than lower order lags. I think this is something we would particularly like to do given that the predictors are highly correlated. </p>

<p>An additional issue is that if (say) $\phi_{35}$ is shrunk to $0$ I would also like it if $\phi_{36}$ is shrunk to $0$, i.e. the same lag is used in all of the conditional distributions. </p>

<p>I could speculate on this, but I don't want to reinvent the wheel. Is there any LASSO techniques designed to get at this sort of problem? Am I better off just doing something else entirely, like stepwise inclusion of lag orders? Since my model space is small, I could even use an $L_0$ penalty on this problem I guess?</p>
",2013-11-05 17:14:23.857
58907,16174.0,2,,58882.0,,,,CC BY-SA 3.0,"<p><sub> This is a more a partial practical answer, but it works for me to do some exercises before getting deeply into theory</sub>.  </p>

<p>This <a href=""http://www.ats.ucla.edu/stat/r/dae/mlogit.htm"" rel=""nofollow"">ats.ucla.edu</a> link is a reference that might help beggining to understand about multinomial logistic regression (as pointed out by Bill), in a more practical way.<br>
It presents reproducible code to understand function <code>multinom</code> from <code>nmet</code> package in <code>R</code> and also gives a briefing about outputs interpretation. </p>

<p>Consider this code:</p>

<pre class=""lang-r prettyprint-override""><code>va = c('cat','dog','dog','goat','cat','goat','dog','dog') 
     # cat will be the outcome baseline
vb = c(1,2,1,2,1,2,1,2)
vc = c('blue','red','blue','red','red','blue','yellow','yellow') 
     # blue will be the vc predictor baseline
set.seed(12)
vd = round(rnorm(8),2)

data = data.frame(cbind(va,vb,vc,vd))

library(nnet)
fit &lt;- multinom(va ~ as.numeric(vb) + vc + as.numeric(vd), data=data)

# weights:  18 (10 variable)
initial  value 8.788898 
iter  10 value 0.213098
iter  20 value 0.000278
final  value 0.000070 
converged

fit

Call:
multinom(formula = va ~ as.numeric(vb) + vc + as.numeric(vd), 
    data = data)

Coefficients:
     (Intercept) as.numeric(vb)     vcred  vcyellow as.numeric(vd)
dog    -1.044866       120.3495 -6.705314  77.41661      -21.97069
goat   47.493155       126.4840 49.856414 -41.46955      -47.72585

Residual Deviance: 0.0001656705 
AIC: 20.00017 
</code></pre>

<p>This is how you can interpret the log-linear fitted multinomial logistic model:</p>

<p>\begin{align}
\ln\left(\frac{P(va={\rm cat})}{P(va={\rm dog})}\right) &amp;= b_{10} + b_{11}vb + b_{12}(vc={\rm red}) + b_{13}(vc={\rm yellow}) + b_{14}vd  \\  
 &amp;\  \\  
\ln\left(\frac{P(va={\rm cat})}{P(va={\rm goat})}\right) &amp;= b_{20} + b_{21}vb + b_{22}(vc={\rm red}) + b_{23}(vc={\rm yellow}) + b_{24}vd
\end{align}</p>

<p>Here is an excerpt about how the model parameters can be interpreted:</p>

<blockquote>
  <ul>
  <li>A one-unit increase in the variable vd is associated with the decrease in the log odds of being ""dog"" vs. ""cat"" in the amount of 21.97069 ($b_{14}$). </li>
  </ul>
</blockquote>

<p>the same logic for the second line but, considering ""goat"" vs. ""cat"" with ($b_{24}$=-47.72585).  </p>

<blockquote>
  <ul>
  <li>The log odds of being ""dog"" vs. ""cat"" will increase by 6.705314 if moving from vc=""blue"" to vc=""red""($b_{12}$).  </li>
  </ul>
</blockquote>

<p>.....</p>

<p>There is much more in the article, but I thought this part to be the core.</p>

<hr>

<p>Reference:  </p>

<p>R Data Analysis Examples: Multinomial Logistic Regression. UCLA: Statistical Consulting Group.<br>
from <a href=""http://www.ats.ucla.edu/stat/r/dae/mlogit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/mlogit.htm</a> (accessed November 05, 2013).</p>
",2013-11-05 17:14:53.610
58908,,2,,58836.0,user31668,,,CC BY-SA 3.0,"<p>Based on OP's comment that deseasonalized time series is a linear trend (t is a true predictor), then you will either want the <a href=""http://www.stat.duke.edu/~tjl13/s101/slides/unit6lec3H.pdf"" rel=""nofollow noreferrer"">Prediction Interval</a> for linear regression (if you are trying to predict 1 time period ahead), or <a href=""http://mant.upol.cz/soubory/ActaMath/2013/52-2-03.pdf"" rel=""nofollow noreferrer"">tolerance intervals</a> if you are trying to capture a specific proportion of future measurements.</p>

<p>If the residuals from your linear fit to the deseasonalized data are approximately normal, then there are nice formulas for as you will see in the above links: Also, eee this other <a href=""https://stats.stackexchange.com/questions/26702/prediction-and-tolerance-intervals"">CrossValidated Post</a>.</p>

<p>You would then re-seasonalize these intervals/bands to get your actual forcasts.</p>
",2013-11-05 17:15:42.367
58954,1145.0,2,,58913.0,,,,CC BY-SA 3.0,"<p>You are correct in assuming that you can't (shouldn't, really) analyse the data with the controls having zero variance.</p>

<p>It sounds like you should consider using a two-way ANOVA on the raw data with the within day variance accounted for in the manner of a paired test. I wrote about the approach in this paper that is intended for pharmacologists with little statistical background: <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2042947/"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2042947/</a></p>
",2013-11-06 05:06:59.730
58909,2149.0,2,,58892.0,,,,CC BY-SA 3.0,"<p>Standard notation for identifying a Transfer Function includes r,s and b</p>

<p>r is the number of denominator coefficients 
s is the number of numerator coefficients
and 
b is the pure delay or dead time before y responds to x</p>

<p>AUTOBOX  a piece of software that I have helped develop does this ( at the user's option ) automatically or a user can specify the model themselves. You can freely download a 30 day version from (<a href=""http://www.autobox.com"" rel=""nofollow"">http://www.autobox.com</a>) and play it with and compare your model identification with it's suggestion. It is a great(free) way of learning how to identify this class of model.</p>
",2013-11-05 17:28:00.947
58910,,2,,58905.0,user31668,,,CC BY-SA 3.0,"<p>It is known that maximizing the likelihood of the data is equivalent to minimizing the <a href=""http://mathworld.wolfram.com/RelativeEntropy.html"" rel=""nofollow"">Kullback-Liebler</a> distance between the hypothesized distribution family, T, and K or M. The kullback-Liebler distance is related to entropy, but its not as simple as your assumed proportional model. See also <a href=""http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow"">link.</a> </p>
",2013-11-05 17:38:49.307
58911,261.0,2,,8529.0,,,,CC BY-SA 3.0,"<p>Jim Berger's review articles: <a href=""http://www.stat.duke.edu/~berger/papers.html"" rel=""nofollow"">http://www.stat.duke.edu/~berger/papers.html</a></p>

<p>You might start with <a href=""http://www.stat.duke.edu/~berger/papers/02-01.html"" rel=""nofollow"">Could Fisher, Jeffreys and Neyman have agreed upon testing?</a> </p>
",2013-11-05 17:52:09.723
58912,,2,,58904.0,user31668,,,CC BY-SA 4.0,"<p>A confidence interval is a &quot;post-data&quot; interval <em>estimate</em> that is supposed to bracket the true parameter in %C of the samples. What you appear to be trying to do is to <em>predict</em> a future event. For this, you need a little more structure on your problem. In particular, a confidence interval is insufficient. What you really need is a <em>distribution</em> of possible head probabilities and probabilities of rolling 1. Then, you need to calculate the probability as such:</p>
<p>Let <span class=""math-container"">$C$</span> be the outcome of the coin toss and <span class=""math-container"">$X$</span> be the results of the die roll, <span class=""math-container"">$f_H(p)$</span> isthe density function on the probability of heads (i.e., <span class=""math-container"">$p$</span>) and <span class=""math-container"">$f_{1|C=H}(q)$</span> is the density function for the probability of rolling a 1 (i.e, <span class=""math-container"">$q$</span>) given that you got a head. Therefore,</p>
<p><span class=""math-container"">$P(X=1)=E[1_{C=H}1_{X=1|C=H}]=E[1_{C=H}]E[1_{X=1|C=H}]$</span> where <span class=""math-container"">$1_{C=H}$</span> and <span class=""math-container"">$1_{X=1|C=H}$</span> are indicator functions that take value 1 when the event in subscript happens, and 0 otherwise. Conditional independence between the coin toss and die roll allow you to multiply expected values.</p>
<p>Now, <span class=""math-container"">$E[1_{C=H}]E[1_{X=1|C=H}]=\int\limits_0^1 \int\limits_0^1pf_H(p)qf_{1|C=H}(q)dqdp$</span>. In other words, you multiply the expected values of the distributions on P(heads) and P(X=1|Heads). So, you will need more info to solve your problem as formulated.</p>
<p>IF you have the underlying data that produced each CI, then you can use methods from <a href=""https://en.wikipedia.org/wiki/Bayesian_inference#Bayesian_prediction"" rel=""nofollow noreferrer"">Bayesian prediction</a> or <a href=""https://projecteuclid.org/journals/statistical-science/volume-5/issue-2/Predictive-Likelihood-A-Review/10.1214/ss/1177012175.full"" rel=""nofollow noreferrer"">predictive likelihood</a></p>
",2013-11-05 18:05:17.900
58913,23394.0,1,,,,Statistical test for normalised data,<anova><variance><t-test><controlling-for-a-variable>,CC BY-SA 3.0,"<p>I am working with culture cells where one dish has been transfected with a scrambled knockdown clone and two dishes which have been transfected with two knockdown clones each knocking down the expression of a single gene. </p>

<p>An example of an experiment I have performed is to measure the mitochondrial membrane potential (using a fluorescent dye) in these cells using a confocal microscope.
This experiment was repeated on three independent occasions.</p>

<p>On each experimental day, the intensity of the laser which I used (the laser ""gain"") varies therefore I cannot combine all experimental days without expressing the dye intensity of each knockdown clone as a percent of the control ""scrambled"" clone (e.g. control = 100% mean intensity; knockdown clone 1 = 50% mean intensity). </p>

<p>Therefore, <strong>I need to test for a difference in means between my control scrambled clone and each of the knockdown clones</strong>, where my control scrambled clone is set to 100% dye intensity on each experimental day and my knockdown clones are normalised to this control. Therefore, my control has no variance (100% for all three experimental days) while my knockdown clones do have variance. </p>

<p>I know an ANOVA would not be feasible given the difference in variance. I will look into the procedure suggested by Michael Lew, but <strong>would a t-test be unacceptable as well?</strong> (I have seen papers using ANOVA and t-tests in these circumstances, but in spite of this I am assuming these should not be used). Thanks in advance.</p>
",2013-11-05 18:28:38.613
58914,18690.0,1,,,,Variable selection (automated),<feature-selection>,CC BY-SA 3.0,"<p>I was wondering whether the following mechanical selection procedure will result in a possible bias. First let me introduce the first procedure, we start with a model and only look at the t-value and possibly correct them for it (heteroskedasticity / autocorrelation). We then only add variables into our final model that are significant. However I am well aware this gives us a bias with F-test, since even though some variables can be insignificant, they can be jointly significant.</p>

<p>However if we also take that into account, whether adding that variable gives us a ""sensible"" subset to do a F-test on and only add them if they give a significant result (indicating joinly significance), this would then actually be a pretty good automated method or does this give some bias? Will anything happen that we do <strong>not</strong> like to have? </p>

<p>Furthermore is a forward variable selection (starting with small set and increase it) better or backwards variable selection better? </p>

<p>Thanks for you answer!</p>
",2013-11-05 18:32:49.877
58915,14811.0,1,,,,Regression Tree with nested factors,<cart>,CC BY-SA 3.0,"<p>I am working on a prediction model in which I have several factor variables that have many levels. These factor variables have a nested structure, in the form of a Category, a Sub-Category, and a Sub-Sub-Category. For example suppose that I had one field that was the type of device a user browsed the website with (pc, tablet, phone), which then can be sub segmented into ((Apple, windows, linux), (kindle, iOS, android), (windows, iOS, android, RIM)), and then each of those could be subdivided into version numbers.</p>

<p>Is there a standard way of handling nested features like this in tree models. At an intuitive level, I don't think the tree should be splitting on one of the subgroupings until it has first split on one of the major groups (since a windows phone and a windows PC are quite different). Creating a single factor that describes the full tree path would have too many possible levels.</p>
",2013-11-05 18:43:11.333
58916,23397.0,1,,,,Computing probability of completing a task composed of independent events,<probability>,CC BY-SA 3.0,"<p>This is a general question.  I have a task that is composed of 3 independent events: A, B, and C.  All are mutually exclusive and don't happen at the same time.  So first A, then B, then C.  I know the probability of completing each event with respect to time, I have the pdfs.  How can I calculate the probability of completing the task at hand with respect to time?</p>
",2013-11-05 19:20:36.173
58917,16551.0,1,58930.0,,,Hazard Function - Survival Analysis,<self-study><survival><hazard>,CC BY-SA 3.0,"<p>I just started taking survival analysis class and I'm stumped on this question.</p>

<p>Let $T_1,...,T_n$ random independent continuous variables, with hazard function of $h_1(t),...,h_n(t)$.
$T=min(T_1,...,T_n)$.
And we need to show that the hazard function of T is $\sum_j{h_j(t)} $</p>

<p>Any help or direction are welcome :)</p>
",2013-11-05 19:23:03.373
58918,5448.0,2,,58375.0,,,,CC BY-SA 3.0,"<p>You can do this with the <code>lm</code> and associated functions, but you need to be a little careful about how you construct your weights.</p>

<p>Here's an example / walkthrough.  Note that the weights are normalized so that the average weight = 1.  I'll follow with what happens if they aren't normalized.  I've deleted a lot of the less relevant printout associated with various functions.</p>

<pre><code>x &lt;- rnorm(1000)
y &lt;- x + rnorm(1000)
wts &lt;- rev(0.998^(0:999)) # Weights go from 0.135 to 1
wts &lt;- wts / mean(wts)    # Now we normalize to mean 1
&gt; summary(unwtd_lm &lt;- lm(y~x))

          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.04238    0.031    ---
x            1.03071    0.03268  31.539   &lt;2e-16 ***
Residual standard error: 1.01 on 998 degrees of freedom

&gt; summary(wtd_lm &lt;- lm(y~x, weights=wts))

            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.03436    0.03227   1.065    0.287    
x            1.03869    0.03295  31.524   &lt;2e-16 ***
Residual standard error: 1.02 on 998 degrees of freedom
</code></pre>

<p>You can see that with this much data we don't have much difference between the two estimates, but there is some. </p>

<p>Now for your question.  It's not clear whether you want the distance in standard errors where the standard errors are for fitted values or for prediction, so I'll show both. Let us say we are doing this for the value $x = 1$ and the target value (green dot) $y = 1.1$):</p>

<pre><code>&gt; y_eval &lt;- 1.10
&gt; wtd_pred &lt;- predict(wtd_lm, newdata=data.frame(x=1), se.fit=TRUE)
&gt; # Distance relative to predictive std. error
&gt; (y_eval-wtd_pred$fit[1]) / sqrt(wtd_pred$se.fit^2 + wtd_pred$residual.scale^2)
[1] 0.02639818
&gt; 
&gt; # Distance relative to fitted std. error
&gt; (y_eval-wtd_pred$fit[1]) / wtd_pred$se.fit
[1] 0.5945089
</code></pre>

<p>where I've deleted the warning message associated with predictive confidence intervals and weighted model fits. </p>

<p>Now I'll show you how to do the residual variance calculation.  First, if your weights aren't normalized, you will have problems:</p>

<pre><code>&gt; wts &lt;- rev(0.998^(0:999))
&gt; summary(wtd_lm &lt;- lm(y~x, weights=wts))

            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.03436    0.03227   1.065    0.287    
x            1.03869    0.03295  31.524   &lt;2e-16 ***
Residual standard error: 0.6707 on 998 degrees of freedom

&gt; predict(wtd_lm, newdata=data.frame(x=1), interval=""prediction"")
       fit        lwr      upr
1 1.073049 -0.2461643 2.392262
</code></pre>

<p>Note how that residual standard error has gone way down and the prediction confidence interval has really changed, but the coefficient estimates themselves have not.  This is because the calculation for the residual s.e. divides by the residual degrees of freedom (998 in this case) without regard for the scale of the weights.  Here's the calculation, mostly lifted from the interior of <code>summary.lm</code>:</p>

<pre><code>w &lt;- wtd_lm$weights
r &lt;- wtd_lm$residuals
rss &lt;- sum(w * r^2)
sqrt(rss / wtd_lm$df)
[1] 0.6707338
</code></pre>

<p>which you can see matches the residual s.e. in the previous printout.  </p>

<p>Here's how you ought to do this calculation if you find yourself in a position where you need to do it by hand, so to speak:</p>

<pre><code>&gt; rss_w &lt;- sum(w*r^2)/mean(w)
&gt; sqrt(rss_w / wtd_lm$df)
[1] 1.019937
</code></pre>

<p>However, normalizing the weights up front takes care of the need to divide by <code>mean(w)</code> and the various <code>lm</code>-related calculations come out correctly without any further manual intervention.</p>
",2013-11-05 19:33:01.153
58919,16551.0,1,,,,Trying to prove Kaplan-Meier statistic with no censoring reduces to empirical survival function,<self-study><survival><kaplan-meier>,CC BY-SA 4.0,"<p>I just started taking survival analysis class and I'm stumped on this question.</p>
<p>We need to show that when there are no censored observations, <span class=""math-container"">$\hat{S}(t)=\prod_{t_{(i)\le t}}\frac{(n_i-d_i)}{n_i}$</span> equals to the empirical survival function <span class=""math-container"">$S_n(t)=\frac{\# \{ t_{(i)} \ge t \}}n . $</span></p>
<p>It looks pretty obvious to me but I'm not sure how to approach this.</p>
",2013-11-05 19:38:25.167
58920,21762.0,2,,58893.0,,,,CC BY-SA 3.0,"<p>You could present relative frequencies of people found in the social network, i.e. ""value over pop""</p>

<pre><code> type Percent
    1  0.0473
    2  0.0246
    3  0.0113
    4  0.0118
</code></pre>

<p>and just compare these percentages.</p>

<p><img src=""https://i.stack.imgur.com/E2wUI.jpg"" alt=""enter image description here""></p>

<p>As the numbers and also the barplot show, the relative frequencies of people found in the districts vary quite a bit, i.e. not all districts are equally represented in the social network.</p>

<p>I doubt whether it is useful to use methods from inductive statistics here because your data set does not seem to be a random sample from a population. Should my impression be wrong, then you could either think of adding binomial confidence intervals to each of those percentages and/or run a chi-squared goodness-of-fit test using the population distribution as the reference.</p>

<p>In R:</p>

<pre><code>N &lt;- c(2248360, 3544721, 70934, 2090647)
n &lt;- c(1064, 873, 8, 246)
chisq.test(n, p = N/sum(N))

# Output
        Chi-squared test for given probabilities

data:  n
X-squared = 526.0491, df = 3, p-value &lt; 2.2e-16
</code></pre>

<p>At the 5% level, you could reject the null hypothesis that all districts are equally represented in the social network.</p>
",2013-11-05 20:05:58.143
58921,10494.0,1,58987.0,,,Can I compare models from linear regression and nonlinear regression using RMSE?,<r><regression>,CC BY-SA 3.0,"<p>I am comparing multiple published equation forms, refit with independent data.  I'm trying to be true to the original authors' methods as much as possible. Therefore, I have 3 linear equations (fit in R using lm()), two of which use transformed Y-variables, and one equation fit using nonlinear regression (fit in R using the gnls() function).</p>

<p>In all instances cases I'm weighting the residual variance structure using the inverse of one of the predictors to account for observed heteroskedasticity.</p>

<p>I have been evaluating the models using R2, and RMSE- using back-transformed data for the two models with transformations.</p>

<p>I've calculated RMSE ""by hand"" using the following equation:</p>

<pre><code> RMSE&lt;-sqrt(sum(residuals(Equation)^2)/length(residuals(Equation))-2))
</code></pre>

<p>Should I use similar code to calculate RMSE for the linear and nonlinear regression models?  Is the metric still a valid statistic for comparison, or am I missing some important assumption?  </p>

<p>Edited: I initially stated that I was also comparing models using AIC; I later recalled that AIC would not be appropriate if the Y-variables were transformed because the models would be estimating different things.</p>
",2013-11-05 20:27:18.590
58922,4910.0,2,,58770.0,,,,CC BY-SA 3.0,"<p>So this is a <a href=""http://en.wikipedia.org/wiki/Monte_Carlo_method"">Monte Carlo</a> solution, that is, we are going to simulate drawing the tiles a zillion of times and then we are going to calculate how many of these simulated draws resulted in us being able to form the given word. I've written the solution in R, but you could use any other programming language, say Python or Ruby.</p>

<p>I'm first going to describe how to simulate one draw. First let's define the tile frequencies.</p>

<pre class=""lang-r prettyprint-override""><code># The tile frequency used in English Scrabble, using ""_"" for blank.
tile_freq &lt;- c(2, 9 ,2 ,2 ,4 ,12,2 ,3 ,2 ,9 ,1 ,1 ,4 ,2 ,6 ,8 ,2 ,1 ,6 ,4 ,6 ,4 ,2 ,2 ,1 ,2 ,1)
tile_names &lt;- as.factor(c(""_"", letters))
tiles &lt;- rep(tile_names, tile_freq)
## [1] _ _ a a a a a a a a a b b c c d d d d e e e e e e
## [26] e e e e e e f f g g g h h i i i i i i i i i j k l
## [51] l l l m m n n n n n n o o o o o o o o p p q r r r
## [76] r r r s s s s t t t t t t u u u u v v w w x y y z
## 27 Levels: _ a b c d e f g h i j k l m n o p q r ... z
</code></pre>

<p>Then encode the word as a vector of letter counts.</p>

<pre class=""lang-r prettyprint-override""><code>word &lt;- ""boot""
# A vector of the counts of the letters in the word
word_vector &lt;- table( factor(strsplit(word, """")[[1]], levels=tile_names))
## _ a b c d e f g h i j k l m n o p q r s t u v w x y z 
## 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 
</code></pre>

<p>Now draw a sample of seven tiles and encode them in the same way as the word.</p>

<pre class=""lang-r prettyprint-override""><code>tile_sample &lt;- table(sample(tiles, size=7))
## _ a b c d e f g h i j k l m n o p q r s t u v w x y z 
## 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 
</code></pre>

<p>At last, calculate what letters are missing...</p>

<pre class=""lang-r prettyprint-override""><code>missing &lt;- word_vector - tile_sample
missing &lt;- ifelse(missing &lt; 0, 0, missing)
## _ a b c d e f g h i j k l m n o p q r s t u v w x y z 
## 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 
</code></pre>

<p>... and sum the number of missing letters and subtract the number of available blanks. If the result is zero or less we succeeded in spelling the word.</p>

<pre class=""lang-r prettyprint-override""><code>sum(missing) - tile_sample[""blank""] &lt;= 0
## FALSE
</code></pre>

<p>In this particular case we didn't though... Now we just need to repeat this many times and calculate the percentage of successful draws. All this is done by the following R function:</p>

<pre class=""lang-r prettyprint-override""><code>word_prob &lt;- function(word, reps = 50000) {
  tile_freq &lt;- c(2, 9 ,2 ,2 ,4 ,12,2 ,3 ,2 ,9 ,1 ,1 ,4 ,2 ,6 ,8 ,2 ,1 ,6 ,4 ,6 ,4 ,2 ,2 ,1 ,2 ,1)
  tile_names &lt;- as.factor(c(""_"", letters))
  tiles &lt;- rep(tile_names, tile_freq)
  word_vector &lt;- table( factor(strsplit(word, """")[[1]], levels=tile_names))
  successful_draws &lt;- replicate(reps, {
    tile_sample &lt;- table(sample(tiles, size=7))
    missing &lt;- word_vector - tile_sample
    missing &lt;- ifelse(missing &lt; 0, 0, missing)
    sum(missing) - tile_sample[""_""] &lt;= 0
  })
  mean(successful_draws)
}
</code></pre>

<p>Here <code>reps</code> is the number of simulated draws. Now we can try it out on a number of different words.</p>

<pre class=""lang-r prettyprint-override""><code>&gt; word_prob(""boot"")
[1] 0.0072
&gt; word_prob(""red"")
[1] 0.07716
&gt; word_prob(""axe"")
[1] 0.05088
&gt; word_prob(""zoology"")
[1] 2e-05
</code></pre>
",2013-11-05 21:39:23.123
58955,1145.0,2,,58929.0,,,,CC BY-SA 3.0,"<p>A probability distribution has to have an area of unity. If the measure is continuous then there is an infinite number of values that it can take (i.e. an infinite number of values along the x-axis of the distribution). The only way that the total area of the probability distribution can be finite is for the value at each of the infinite number of values to be zero. One divided by infinity.</p>

<p>In 'real life' there can be no measures that take an infinite number of values (by several different philosophical arguments that don't matter much here) so no value need take a probability of exactly zero. A useful practical argument is based on the finite precision of real-world measurements. If you use a stopwatch that measures to one tenth of a second, the train will have one tenth of a second in which to arrive in 'exactly' five minutes.</p>
",2013-11-06 05:22:10.843
58923,23348.0,2,,58633.0,,,,CC BY-SA 3.0,"<p>A question about your data: Do students visit the advisers more than once? What I mean is, for the students who choose to go to the advisers, do some students visit their adviser once while others choose to visit 15-20 times? If this is the case, then I would advise against using the traditional propensity score method as laid out by Rosenbaum and Rubin, and instead look at the Generalized Propensity Score as discussed by Hirano and Imbens (2004) ""The Propensity Score with Continuous Treatments"". </p>

<p>Also, I feel the need to insert an obligatory ""correlation is not causation"" statement here because of your observation that ""Just looking at 2013 data alone makes it seem that the advisers make things worse: the students they see do worse, and are more likely to leave university."" Something I see a LOT in education data is that, whenever students are free to self-select into a treatment, the types of students who select into that treatment are <strong>very</strong> different than those who don't, and often there is huge variation even among the students that do select into treatment. This makes evaluating the impact of an optional treatment program very difficult!</p>
",2013-11-05 21:42:45.707
58924,23400.0,1,,,,What statistic procedure to use for analyzing my data?,<hypothesis-testing>,CC BY-SA 3.0,"<p>I'm currently a fourth year university student. As part of my studies, I'm taking a class called Capstone, where students design and carry out a research project. An essential part of formulating this research is choosing a statistical procedure with which to analyze and present your results.</p>

<p>My study focuses on studying the increase of middle school students' awareness on the subject of bullying. </p>

<p>To do that, we will have a group of students who will take an initial questionnaire which has multiple choice questions about different situations and what type of bullying they represent. After that test, the same students will be giving a workshop where we will discuss bullying: the types that exist, how to recognize them and the negative impact they can have. After those workshops, the students will take another test, consisting of exactly the same questions as the first. </p>

<p>The goal is that, by comparing the answers on both tests, we will find that students answers on that second test correspond to a better identification and understanding of what bullying is.</p>

<p>My question the is: what type of statistical test would you recommend I use to sort and analyze the data I recollect?</p>
",2013-11-05 21:59:38.740
58925,23401.0,2,,35097.0,,,,CC BY-SA 3.0,"<p>I don't see any problem with the frequentist's approach. If the null hypothesis is rejected, the p-value is the probability of a type 1 error. A type 1 error is rejecting a true null hypothesis. In this case we have a p-value of 0.028. This means that among all the hypothesis tests with this p-value ever conducted, roughly 3 out of a hundred will reject a true null hypothesis. By construction, this would be one of those cases. Frequentists accept that sometimes they'll reject true null hypothesis or retain false null hypothesis (Type 2 errors), they've never claimed otherwise. Moreover, they precisely quantify the frequency of their erroneous inferences in the long run.</p>

<p>Perhaps, a less confusing way of looking at this result is to exchange the roles of the hypotheses. Since the two hypotheses are simple, this is easy to do. If the null is that the sun went nova, then the p-value is 35/36=0.972. This means that this is no evidence against the hypothesis that the sun went nova, so we can't reject it based on this result. This seems more reasonable. If you are thinking. Why would anybody assume the sun went nova? I would ask you. Why would anybody carry out such an experiment if the very thought of the sun exploding seems ridiculous?</p>

<p>I think this just shows that one has to assess the usefulness of an experiment beforehand. This experiment, for example, would be completely useless because it tests something we already know simply from looking up to the sky (Which I'm sure produces a p-value that is effectively zero). Designing a good experiment is a requirement to produce good science. If your experiment is poorly designed, then no matter what statistical inference tool you use, your results are unlikely to be useful.</p>
",2013-11-05 22:06:15.713
58926,15624.0,1,,,,modeling cumulative effect of a winter,<modeling><time-varying-covariate><ecology>,CC BY-SA 3.0,"<p>I am trying to make a model that predicts the survival rate of wildlife based on the severity of an entire winter. The idea is that colder weather in combination with lots of snow has a negative effect (the colder it is the longer the snow lasts). It might be cold winter with lots of snow, but if the cold and snow did not occur in the same month it would have less of an effect. In order to preserve the interaction of snow and temp, we want to use monthly data, but our survival will always be for the entire winter. It doesn't really matter which month the bad weather occurs in so I am hesitant to use month as a variable.</p>

<p>Is there some technique that I can use where it preserves the interaction of snow and temp each month but uses the cumulative effect of all months in a winter to predict survival?</p>

<p>example of what the data looks like:</p>

<pre><code>Year1 survival = 0.9
month      temp     snow
 m1         -5       20
 m2        -15       20
 m3        -20      100
 m4          2      100

Year2 survival = 0.7
month     temp     snow
 m1        -5       20
 m2       -18      110
 m3       -20      100
 m4       -11       20
</code></pre>

<p>Temp was the average temperature for the month, and snow was the sum of all snow in a month.  I'm not sure where the comment about order statistics is going but if it makes a difference daily data is available.</p>
",2013-11-05 22:36:04.513
58927,668.0,2,,58770.0,,,,CC BY-SA 3.0,"<p>Answers to the <a href=""https://stats.stackexchange.com/questions/3779"">referenced question</a> apply here directly: create a dictionary consisting only of the target word (and its possible wildcard spellings), compute the chance that a random rack cannot form the target, and subtract that from $1$.  This computation is fast.</p>

<p>Simulations (shown at the end) support the computed answers.</p>

<hr>

<h3>Details</h3>

<p>As in the previous answer, <em>Mathematica</em> is used to perform the calculations.</p>

<ol>
<li><p><strong>Specify the problem:</strong> the word (or words, if you like), the letters, their counts, and the rack size.  Because all letters not in the word act the same, it greatly speeds the computation to replace them all by a single symbol $\chi$ representing ""any letter not in the word.""</p>

<pre class=""lang-mathematica prettyprint-override""><code>word = {b, o, o, t};
letters = {b, o, t, \[Chi], \[Psi]};
tileCounts = {2, 8, 6, 82, 2};
rack = 7;
</code></pre></li>
<li><p><strong>Create a dictionary</strong> of this word (or words) and augment it to include all possible wildcard spellings.</p>

<pre class=""lang-mathematica prettyprint-override""><code>dict[words_, nWild_Integer] := Module[{wildcard, w},
   wildcard = {xx___, _, yy___} -&gt; {xx, \[Psi], yy};
   w = Nest[Flatten[ReplaceList[#, wildcard] &amp; /@ #, 1] &amp;, words, nWild];
   Union[Times @@@ Join[w, Times @@@ words]]];
dictionary = dict[{word}, 2]
</code></pre>

<blockquote>
  <p>$\left\{b o^2 t, b o^2 \psi ,b o t \psi ,o^2 t \psi ,b o \psi ^2,o^2 \psi ^2,b t \psi ^2,o t \psi ^2\right\}$</p>
</blockquote></li>
<li><p><strong>Compute the nonwords:</strong></p>

<pre class=""lang-mathematica prettyprint-override""><code>alphabet = Plus @@ letters;
nonwords = Nest[PolynomialMod[# alphabet, dictionary] &amp;, 1, rack]
</code></pre>

<blockquote>
  <p>$b^7 + 7 b^6 o + 21 b^5 o^2 + \cdots +7 \chi  \psi ^6+\psi ^7$</p>
</blockquote>

<p>(There are $185$ non-words in this case.)</p></li>
<li><p><strong>Compute the chances.</strong>  For sampling with replacement, just substitute the tile counts for the variables:</p>

<pre class=""lang-mathematica prettyprint-override""><code>chances = (Transpose[{letters, tileCounts/(Plus @@ tileCounts)}] /. {a_, b_} -&gt; a -&gt; b);
q = nonwords /. chances;
1 - q
</code></pre>

<blockquote>
  <p>$\frac{207263413}{39062500000}$</p>
</blockquote>

<p>This value is approximately $0.00756036.$</p>

<p>For sampling without replacement, use factorial powers instead of powers:</p>

<pre class=""lang-mathematica prettyprint-override""><code>multiplicities = MapThread[Rule, {letters, tileCounts}];
chance[m_] :=  (ReplaceRepeated[m , Power[xx_, n_] -&gt; FactorialPower[xx, n]] 
               /. multiplicities);
histor = chance /@ MonomialList[nonwords];
q0 = Plus @@ histor  / FactorialPower[Total[tiles], nn];
1 - q0
</code></pre>

<blockquote>
  <p>$\frac{2381831}{333490850}$</p>
</blockquote>

<p>This value is approximately $0.00714212.$  The calculations were practically instantaneous.</p></li>
</ol>

<hr>

<h3>Simulation results</h3>

<p>Results of $10^6$ iterations with replacement:</p>

<pre class=""lang-mathematica prettyprint-override""><code>simulation = RandomChoice[tiles -&gt; letters, {10^6, 7}];
u = Tally[Times @@@ simulation];
(p = Total[Cases[Join[{PolynomialMod[u[[All, 1]], dictionary]}\[Transpose], 
       u, 2], {0, _, a_} :&gt; a]] / Length[simulation] ) // N
</code></pre>

<blockquote>
  <p>$0.007438$</p>
</blockquote>

<p>Compare it to the computed value relative to its standard error:</p>

<pre class=""lang-mathematica prettyprint-override""><code>(p - (1 - q)) / Sqrt[q (1 - q) / Length[simulation]] // N
</code></pre>

<blockquote>
  <p>$-1.41259$</p>
</blockquote>

<p>The agreement is fine, strongly supporting the computed result.</p>

<p>Results of $10^6$ iterations without replacement:</p>

<pre class=""lang-mathematica prettyprint-override""><code>tilesAll = Flatten[MapThread[ConstantArray[#1, #2] &amp;, {letters, tiles}] ]
    (p - (1 - q)) / Sqrt[q (1 - q) / Length[simulation]] // N;
simulation = Table[RandomSample[tilesAll, 7], {i, 1, 10^6}];
u = Tally[Times @@@ simulation];
(p0 = Total[Cases[Join[{PolynomialMod[u[[All, 1]], dictionary]}\[Transpose], 
       u, 2], {0, _, a_} :&gt; a]] / Length[simulation] ) // N
</code></pre>

<blockquote>
  <p>$0.00717$</p>
</blockquote>

<p>Make the comparison:</p>

<pre class=""lang-mathematica prettyprint-override""><code>(p0 - (1 - q0)) / Sqrt[q0 (1 - q0) / Length[simulation]] // N
</code></pre>

<blockquote>
  <p>$0.331106$</p>
</blockquote>

<p>The agreement in this simulation was excellent.</p>

<p>The total time for simulation was $12$ seconds.</p>
",2013-11-05 22:42:57.193
58928,17038.0,1,58933.0,,,Can the selling price be treated as a random variable?,<central-limit-theorem>,CC BY-SA 3.0,"<p>I have some data from my company that I have been looking at. I have been comparing deals that we have ""lost"" to deals that we have ""won"". I have been comparing the product averages and standard deviations, and have found that the average and standard deviation (for some industries) are lower for the ""lost"" deals. </p>

<p>We sell each product at set prices, but products can have special discounts when requested which causes ""random prices."" Some products will be sold higher or lower depending on the customer. When I look at the data, the prices are slightly negatively skewed but seem to be normally distributed. </p>

<p>Does this mean that I can treat the price as random and apply the central limit theorem? Also, I will be suggesting more aggressive lower pricing if I can treat the range as normally distributed. Is this logical?</p>
",2013-11-05 22:58:55.747
58995,3999.0,1,59094.0,,,"What aspects of the ""Iris"" data set make it so successful as an example/teaching/test data set",<dataset>,CC BY-SA 3.0,"<p>The <em><a href=""http://en.wikipedia.org/wiki/Iris_flower_data_set"" rel=""noreferrer"">""Iris""</a></em> dataset is probably familiar to most people here - it's one of the canonical test data sets and a go-to example dataset for everything from data visualization to machine learning. For example, everyone in <a href=""https://stats.stackexchange.com/questions/30788/whats-a-good-way-to-use-r-to-make-a-scatterplot-that-separates-the-data-by-trea/30789#30789"">this question</a> ended up using it for a discussion of scatterplots separated by treatment.</p>

<p>What makes the <em>Iris</em> data set so useful? Just that it was there first? If someone was trying to <em>create</em> a useful example/testing data set, what lessons could they take away from it?</p>
",2013-11-06 19:03:35.617
58929,23406.0,1,58938.0,,,Probability that a continuous random variable assumes a fixed point,<probability><mathematical-statistics><random-variable><density-function><continuous-data>,CC BY-SA 4.0,"<p>I'm in an introductory statistics class in which the probability density function for continuous random variables has been defined as <span class=""math-container"">$P\left\{X\in B\right\}=\int_B f\left(x\right)dx$</span>.  I understand that the integral of <span class=""math-container"">$\int\limits_a^af(x)dx=0$</span> but I can't rectify this with my intuition of a continuous random variable.</p>
<p>Say X is the random variable equal to the number of minutes from time t that the train arrives.  How do I calculate the probability that the train arrives exactly 5 minutes from now?  How can this probability be zero?  Is it not possible?  What if the train <em>does</em> arrive exactly 5 minutes from now, how could it occur if it had probability 0?</p>
",2013-11-05 23:21:57.263
58930,20473.0,2,,58917.0,,,,CC BY-SA 3.0,"<p>Before obtaining the hazard function of $T=\min\{T_1,...,T_n\}$, <strong>let's first derive its distribution and its density function</strong>, i.e. the CFD and PDF of the first-order statistic from a sample of independently but <em>not</em> identically distributed random variables.  </p>

<p>The distribution of the minimum of $n$ independent random variables is </p>

<p>$$F_T(t) = 1-\prod_{i=1}^n[1-F_i(t)]$$</p>

<p>(see the reasoning in <a href=""https://stats.stackexchange.com/questions/220/how-is-the-minimum-of-a-set-of-random-variables-distributed"">this CV post</a>, if you don't know it already)</p>

<p>We differentiate to obtain its density function:</p>

<p>$$f_T(t) =\frac {\partial}{\partial t}F_T(t) = f_1(t)\prod_{i\neq 1}[1-F_i(t)]+...+f_n(t)\prod_{i\neq n}[1-F_i(t)]$$</p>

<p>Using $h_i(t) = \frac {f_i(t)}{(1-F_i(t)} \Rightarrow f_i(t) = h_i(t)(1-F_i(t)) $ and substituting in $f_T(t)$ we have</p>

<p>$$f_T(t) = h_1(t)(1-F_1(t))\prod_{i\neq 1}[1-F_i(t)]+...+h_n(t)(1-F_n(t))\prod_{i\neq n}[1-F_i(t)]$$</p>

<p>$$=\left(\prod_{i=1}^n[1-F_i(t)]\right)\sum_{i=1}^nh_i(t),\;\;\; h_i(t) = \frac {f_i(t)}{1-F_i(t)} \tag{1}$$</p>

<p>which is the density function of the minimum of $n$ independent but not identically distributed random variables.</p>

<p>Then the hazard rate of $T$ is </p>

<p>$$h_T(t) = \frac {f_T(t)}{1-F_T(t)} = \frac {\left(\prod_{i=1}^n[1-F_i(t)]\right)\sum_{i=1}^nh_i(t)}{\prod_{i=1}^n[1-F_i(t)]} = \sum_{i=1}^nh_i(t) \tag{2}$$</p>
",2013-11-05 23:30:31.687
58931,9554.0,2,,58924.0,,,,CC BY-SA 3.0,"<p>I think you should do a simple pairwise difference comparison (before and after workshop) for each question separately.</p>

<p>Since you will probably use some Likert scale in your questionnaire (such as ""Strongly agree"", ""Agree"", etc.) your data will be ordinal.</p>

<p>You can use the Wilcoxon signed rank test, to estimate whether there was a significant change in the responses for each question after the workshop.</p>

<p>I think any serious statistical package will support it. I'm sure you will have for example SPSS at school.</p>

<p>If you want to be able to claim that it was the workshop that caused the change, I would recommend you to go a step further. Let the class fill out the questionnaire, then split the class in two parts randomly, and send only half of the class to the workshop. Then let the entire class repeat the questionnaire. The part of the class not taking your workshop will be your control group. You can check whether there will be significant difference even without the workshop.</p>

<p>(if the workshop offers some real additional value for the students, send the other half to the workshop after they have finished filling out the questionnaire a second time)</p>
",2013-11-05 23:47:22.403
58932,20795.0,2,,58818.0,,,,CC BY-SA 3.0,"<p>@digdeep, as usual @whuber provided an excellent and comprehensive answer from a statistical view point. I'm not trained in statistics, so take this response with a grain of salt. I have used the response below in my real world practice data, so I hope this is helpful.</p>

<p>I'll try to provide a non statistician view of transformation of time series data for Arima modeling. There is no straightforward answer. Since you are interested in knowing which transformation to use, it might be helpful to review why we do transformation.We do transformation for 3 main reasons and there might be ton of other reasons:</p>

<ol>
<li>Transformation makes the data's linear structure more usable for<br>
ARIMA modeling. </li>
<li>If variance in the data is increasing or changing then transformation of data might be helpful to stabilize the variance in data.</li>
<li>Transformation also makes the errors/residuals in 
    ARIMA model normally distributed which is a requirement in ARIMA modeling proposed by Box-Jenkins.</li>
</ol>

<p>There are several data transformations including Box-Cox, Log, square root, quartic and inverse and other transformations mentioned @irishstat. As with all the statistical methods there is no good guidance/answer on which transformation to select for a particular dataset. </p>

<p>As the famous statistician G.E.P Box said ""All models are wrong but some are useful"", this would apply to the transformations as well ""All <em>transformations</em> are wrong but some are useful"". </p>

<p>The best way to choose a transformation is to experiment. Since you have a long time series, I would hold out the last 12 - 24 months, and build a model using all the transformation and see if a particular transformation is helpful at predicting your out of sample data accurately. Also examine the residuals for normality assumption of your model. Hopefully, this would guide you in choosing an appropriate transformation. You might also want to compare this with non-transformed data and see if the transformation helped your model.</p>

<p>@whuber's excellent graphical representation of your data motivated me to explore this data graphically using a decomposition method. I might  add, R has an excellent decomposition method called STL which would be helpful in identifying patterns that you would normally not notice. For a dataset like this, STL decomposition is helpful in not only selecting an appropriate method for analyzing your data, it might also  be helpful in identifying anomalies such as outliers/level shift/change in seasonality etc., See below. Notice that the remainder (irregular) component of the data, looks like there is stochastic seasonality and the variation is not random, there appears to be a pattern. See also change in level of trend component after 2004/2005 that @whuber is refrencing.</p>

<p>Hopefully this is helpful.</p>

<pre><code>g &lt;- stl(y,s.window = ""periodic"")
plot(g)
</code></pre>

<p><img src=""https://i.stack.imgur.com/EMDl6.jpg"" alt=""enter image description here""></p>
",2013-11-05 23:49:58.747
58933,20473.0,2,,58928.0,,,,CC BY-SA 3.0,"<p>Prices cannot be normally distributed, since they are non-negative. But you write that <em>actual</em> prices fluctuate below or above the preset prices. </p>

<p>Denote $p_s$ the preset price and $u$ the special discount or increase above the pre-set price. Then, realized price is </p>

<p>$$p=p_s+u \Rightarrow u=p-p_s$$</p>

<p>Now $u$ is distributed around zero, but not necessarily symmetrically. There is no reason to ignore any skewness present in the data. Go for the <a href=""http://en.wikipedia.org/wiki/Skew_normal_distribution"" rel=""nofollow"">Skew-normal Distribution</a>, which has been conceived exactly for modelling skewness.   </p>
",2013-11-05 23:54:19.533
58934,23407.0,1,,,,Prediction using Naive Bayes of klaR package fails,<r><machine-learning><naive-bayes>,CC BY-SA 3.0,"<p>I am trying to replicate a example that I found in Tom Mitchell's book <em>Machine Learning</em> (1997), using R. It is a example from chapter 6.  </p>

<p>There are 14 training examples (shown below) of the target concept PlayTennis, where each day is described by the attributes Outlook, Temperature, Humidity, and Windy.</p>

<p>Training examples:</p>

<pre><code>Outlook,Temperature,Humidity,Windy,Play
overcast,cool,normal,true,yes
overcast,hot,high,false,yes
overcast,hot,normal,false,yes
overcast,mild,high,true,yes
rainy,cool,normal,false,yes
rainy,mild,high,false,yes
rainy,mild,normal,false,yes
sunny,cool,normal,false,yes
sunny,mild,normal,true,yes
rainy,cool,normal,true,no
rainy,mild,high,true,no
sunny,hot,high,false,no
sunny,hot,high,true,no
sunny,mild,high,false,no
</code></pre>

<p>Here's my code:</p>

<pre><code>library(""klaR"")
library(""caret"")

data = read.csv(""example.csv"")

x = data[,-5]
y = data$Play

model = train(x,y,'nb',trControl=trainControl(method='cv',number=10))

Outlook &lt;- ""sunny""
Temperature &lt;- ""cool""
Humidity &lt;- ""high""
Windy &lt;- ""true""

instance &lt;- data.frame(Outlook,Temperature,Humidity,Windy)

predict(model$finalModel,instance)
</code></pre>

<p>The example tries to predict the outcome for</p>

<pre><code>Outlook=sunny, Temperature=cool,Humidity=high and Wind=strong
</code></pre>

<p>The problem is that I am getting a different prediction from the one in the book.</p>

<p>Here are the probabilities I've got from my code:</p>

<pre><code>no          yes
0.001078835 0.9989212
</code></pre>

<p>Here are the book's probabilities:</p>

<pre><code>no     yes
0.0206 0.0053
</code></pre>

<p>My code classifies the unseen data as Yes and the book's classifier classifies it as No.</p>

<p>Shouldn't both give the same answer since we are using the same naive Bayes classifier?</p>

<p>EDIT:</p>

<p>I replicated the example using scikit-learn MultinomialNB classifier and I have got the following probabilities</p>

<pre><code>no    yes
0.769  0.231
</code></pre>

<p>which are similar to the <b>normalized</b> probabilities of the book.</p>

<p>Normalized probabilities of the book</p>

<pre><code>no     yes
0.795  0.205
</code></pre>
",2013-11-06 00:14:56.537
58935,20473.0,2,,58929.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>What if the train does arrive exactly 5 minutes from now, how could it
  occur if it had probability 0?</p>
</blockquote>

<p>A probabilistic statement is not a statement about the <em>possibility/feasibility</em> of an event. It only reflects our attempt to quantify our uncertainty about it happening.  So when a phenomenon is continuous (or is modeled as one), then our tools and current state of knowledge <em>do not permit us to make a probabilistic statement about it taking a specific value</em>. We can only make such a statement related to a <em>range</em> of values. Of course the usual trick here is to discretize the support, to consider""small"" intervals of values rather than single values. Since continuous random variables bring great benefits and flexibility compared to discrete random variables, this has been found to be a rather small price to pay, perhaps as small as the intervals we are forced to consider.</p>
",2013-11-06 00:16:00.993
58936,23408.0,2,,2509.0,,,,CC BY-SA 3.0,"<p>Here's one for Grandma:</p>

<p>In our town there are streets going north and south, some going east and west, and even some going northwest and southeast, some NE to SW. One day a guy measures all the traffic on all the streets, he finds that the most traffic is going diagonally, from northwest to southeast, the second biggest is perpendicular to this going northeast to southwest and all the rest is fairly small. So he draws a big square and puts a big line left to right and says that is the NW to SE, then draws another line vertically up and down through the middle. He says that's the second most crowded direction for traffic (NE to SW). The rest is small so it can be ignored.</p>

<p>The left right line is the first eigenvector and the up down line is the second eigenvector. The total number of cars going left and right are the first eigenvalue and those going up and down are the second eigenvalue.</p>
",2013-11-06 00:22:17.697
58937,9554.0,2,,58929.0,,,,CC BY-SA 3.0,"<p>To give you some intuition for the above, try the following (thought) experiment:</p>

<p>Draw a real line around zero with a ruler. Now take a sharp dart and let it fall from above randomly on the line(let's assume you will always hit the line and only the lateral positioning matters for the sake of the argument).</p>

<p>However many times you let the dart fall randomly on the line, you will never hit the point zero. Why? Think what is the point zero, think what is its width. And after you recognise that its width is 0, do you still think you can hit it?</p>

<p>Will you be able to hit point 1, or -2? Or any other point you pick on the line for that matter?</p>

<p>To get back to maths, this is the difference between the physical world, and a mathematical concept such as real numbers (represented by the real line in my example). Probability theory has quite a bit more complicated definition of probability than you will see in your lecture. To quantify the probability of events and any combination of their outcomes, you need a probability measure. Both the <a href=""http://en.wikipedia.org/wiki/Borel_measure"" rel=""nofollow"">Borel measure</a> and <a href=""http://en.wikipedia.org/wiki/Lebesgue_measure"" rel=""nofollow"">Lebesgue measure</a> are defined for an interval [a, b] on the real line as:
$$\mu([a,b])=b-a$$
from this definition you can see what happens with the probability if you reduce the interval to a number (setting a = b).</p>

<p>The bottom line is that based on our current definition of probability theory (dating back to Kolmogorov) the fact that an event has 0 probability does not mean it cannot occur.</p>

<p>And as far as your example with the train goes, if you will have an infinitely precise watch, your train will never arrive exactly on time.</p>
",2013-11-06 00:34:04.570
58938,594.0,2,,58929.0,,,,CC BY-SA 3.0,"<p>You may be falling into the trap of regarding 'five minutes from now' as lasting some finite period of time (which would have a nonzero probability). </p>

<p>""Five minutes from now"" in the continuous variable sense is truly instantaneous. </p>

<p>Imagine that the arrival of the next train is uniformly distributed between 8:00 and 8:15. Further imagine we define the <em>arrival of a train</em> as occurring at the instant the front of the train passes a particular point on the station (perhaps the midpoint of the platform if there's no better landmark). Consider the following sequence of probabilities: </p>

<p>a) the probability a train arrives between 8:05 and 8:10</p>

<p>b) the probability a train arrives between 8:05 and 8:06</p>

<p>c) the probability a train arrives between 8:05:00 and 8:05:01</p>

<p>d) the probability a train arrives between 8:05:00 and 8:05:00.01 (i.e. in the space of one hundredth of a second</p>

<p>e) the probability a train arrives between 8:05 and one billionth of a second later</p>

<p>f) the probability a train arrives between 8:05 and one quadrillionth of a second later</p>

<p>... and so on</p>

<p>The probability that it arrives precisely <em>at</em> 8:05 is the limiting value of a sequence of probabilities like that. The probability is smaller than every $\epsilon&gt;0$.</p>
",2013-11-06 00:48:06.460
58939,16650.0,2,,58770.0,,,,CC BY-SA 3.0,"<p>For the word ""BOOT"" with no wildcards:
$$
p_0=\frac{\binom{n_b}{1}\binom{n_o}{2}\binom{n_t}{1}\binom{n-4}{3}}{\binom{n}{7}}
$$
With wildcards, it becomes more tedious. Let $p_k$ indicate the probability of being able to play ""BOOT"" with $k$ wildcards:
$$
\begin{eqnarray*}
p_0&amp;=&amp;\frac{\binom{n_b}{1}\binom{n_o}{2}\binom{n_t}{1}\binom{n-4}{3}}{\binom{n}{7}} \\
p_1&amp;=&amp;p_0 +\frac{\binom{n_*}{1}\binom{n_o}{2}\binom{n_t}{1}\binom{n-4}{3}}{\binom{n}{7}} + \frac{\binom{n_b}{1}\binom{n_o}{1}\binom{n_*}{1}\binom{n_t}{1}\binom{n-4}{3}}{\binom{n}{7}} + \frac{\binom{n_b}{1}\binom{n_o}{2}\binom{n_*}{1}\binom{n-4}{3}}{\binom{n}{7}}\\
&amp;=&amp;p_0 +\frac{\binom{n_*}{1}\binom{n-4}{3}}{\binom{n}{7}}(\binom{n_o}{2}\binom{n_t}{1} + \binom{n_b}{1}\binom{n_o}{1}\binom{n_t}{1} + \binom{n_b}{1}\binom{n_o}{2})\\
p_2&amp;=&amp;p_1 + \frac{\binom{n_*}{2}\binom{n-4}{3}}{\binom{n}{7}}(\binom{n_b}{1}\binom{n_o}{1} + \binom{n_b}{1}\binom{n_t}{1} + \binom{n_o}{2} + \binom{n_o}{1}\binom{n_t}{1})\\
p_3&amp;=&amp;p_2 + \frac{\binom{n_*}{3}\binom{n-4}{3}}{\binom{n}{7}}(\binom{n_b}{1} + \binom{n_o}{1} + \binom{n_t}{1})\\
p_4&amp;=&amp;p_3 + \frac{\binom{n_*}{4}\binom{n-4}{3}}{\binom{n}{7}}\\
p_i&amp;=&amp;p_4, i\ge4
\end{eqnarray*}
$$</p>
",2013-11-06 00:56:42.760
58940,23411.0,1,58946.0,,,"Official name for ""symmetric percent difference"" function (x-y)/max(x,y)",<group-differences>,CC BY-SA 3.0,"<p>I frequently use this formula to compare two positive numbers $x$ and $y$ to see if they are ""more different"" than some threshold:</p>

<p>$$
x-y \over \max(x,y)
$$</p>

<p>It is nice because it is symmetric and bounded to $[-1,1]$ (unlike relative percent difference). I call it a ""symmetric percent difference."" I see a similar formula on <a href=""http://en.wikipedia.org/wiki/Percent_difference#Formulas"" rel=""nofollow"">this Wikipedia page</a>, apparently generalized to negative or positive numbers, but it's not named:</p>

<p>$$
|x-y| \over \max(|x|,|y|)
$$</p>

<p>Does anyone know the official name for this function?</p>

<p>Note: Another similar function, bounded to $[0,1]$, is used to calculate sMAPE:</p>

<p>$$
|x-y| \over x+y
$$</p>
",2013-11-06 00:59:09.070
27194,1085.0,1,,,,What does that mean that two time series are colinear?,<time-series><cointegration><multicollinearity>,CC BY-SA 3.0,"<p>I am familiar with the concept of cointegration.</p>

<p>But I hear sometimes people talking about colinearity (or collinearity) for time series.
A set of points is collinear if they are on the same line. But what does that mean for time series?</p>

<p>Is it exactly the same as cointegration of order 1?
Or is there something stronger/different in the concept of collinearity?</p>
",2012-06-22 02:20:37.490
58941,23413.0,1,,,,Correlation among categories between categorical nominal variables,<correlation><residuals><contingency-tables><categorical-data>,CC BY-SA 3.0,"<p>I have a data set with two categorical nominal variables (both with 5 categories). I would like to know if (and how) I am able to identify potential correlations between the categories from these two variables.</p>

<p>In other words whether for example the results of category $i$ in variable 1 show a strong correlation with a specific category $j$ in variable 2. Since I have two variables with 5 categories, the total correlation analysis for all the categories would come down to 25 results (at least if it works the way I hope/expect it to work). </p>

<p>I have tried to formulate the problem into concrete questions:</p>

<p>Question 1: Let's say I transfer the categorical variable into 5 different dummy variables per value (category). This same procedure I run for the second variable as well. Then I want to determine the correlation between dummy 1.i and 2.i (for example). Is it statistically correct for me to execute this procedure by means of an ordinary correlation coefficient procedure? Does the correlation coefficient resulting from this procedure provide a proper insight in a correlation among the two dummy variables?</p>

<p>Question 2: If the procedure described in question one is a valid procedure, is there a way to execute this analysis for all categories of 2 (or perhaps more) categorical nominal variables all at once?</p>

<p>The program I am using is SPSS (20).</p>
",2013-11-06 02:00:54.243
58942,23412.0,1,,,,Correlation assumption of Hochberg procedure in repeated measures design,<correlation><repeated-measures><multiple-comparisons>,CC BY-SA 3.0,"<p>I'm running a 3-factor (Between by Between by Within) ANOVA with the correlation structure modeled using a Random Intercept and Gaussian Serial Correlation. I have the following variables:</p>

<p>Dependent variable: Bodyweight  </p>

<p>Independent:  </p>

<p>Size - size of the subject (between). 2 levels: normal and large<br>
Diet - diet given (between). 2 levels: normal or experimental diet.<br>
Time - weekly bodyweight measurements (within). 12 levels: 1:12  </p>

<p>I have a significant 3-way interaction and all 2-way interactions(p&lt;0.05), and so I am stratifying the 3-factors to test all pairwise differences between Normal and Large Size, on each Diet, and at each week. I'm doing these specific comparisons because I'm interested in whether Size further increases the weight gain known to occur on the experimental diet. Using this approach, I have a total of 24 post-hoc comparisons. </p>

<p>I would like to improve power by using Hochberg's procedure rather than Holm's to adjust for multiple comparisons, but I'm unclear whether my data meet the assumption of independent or positively associated test statistics/p-values (as per Simes' test and multcomp documentation in R).</p>

<p>I'm employing the mixed model because of the positively correlated repeated measures, so I'm tempted to conclude that the post-hoc/pairwise test statistics at each time would also be positively correlated. But I could very well be wrong. </p>

<p>Here are my questions:</p>

<ol>
<li><p>Would the pairwise test statistics/p-values of these repeated measures data meet the independence/positive assumption of Hochberg's procedure?</p></li>
<li><p>Is there a way to test or at least calculate/extract the correlations of the test statistics/p-values to empirically validate the assumption?</p></li>
</ol>
",2013-11-06 02:07:17.747
58943,16588.0,2,,58779.0,,,,CC BY-SA 3.0,"<p>Here's how I'd do it in R. Note, I have used your original data with self-transitions (non-transitions) included. You can do the same thing with the ""compressed time series,"" however.</p>

<p>Say I have already constructed an edge list from your example data in the form of the following matrix</p>

<pre><code>el &lt;- structure(c(1, 2, 3, 1, 2, 1, 2, 3, 1, 1, 1, 2, 2, 3, 3, 3, 4, 
2, 3, 4, 1, 1, 2, 3), .Dim = c(8L, 3L), .Dimnames = list(NULL, 
c(""i"", ""j"", ""w"")))
el
#     i j w
#[1,] 1 1 4
#[2,] 2 1 2
#[3,] 3 1 3
#[4,] 1 2 4
#[5,] 2 2 1
#[6,] 1 3 1
#[7,] 2 3 2
#[8,] 3 3 3
</code></pre>

<p>It captures the facts that state $1$ followed state $1$ (i.e., no transition) four times, state $1$ followed state $2$ twice, and so on. I now capture this same information in the form of a square $3 \times 3$ matrix.</p>

<pre><code>X &lt;- matrix(0,3,3)
X[el[,1:2]]&lt;-el[,3]
X
#     [,1] [,2] [,3]
#[1,]    4    4    1
#[2,]    2    1    2
#[3,]    3    0    3
</code></pre>

<p>Now I want the elements to represent proportions with respect to the rest of the elements in their own row only. That is, rows should sum to $1$ meaning that in every state there will be some transition with probability $1$ and the individual elements of the row capture the distribution of that probability across all possible next states.</p>

<pre><code>X &lt;- X / rowSums(X)
X
#          [,1]      [,2]      [,3]
#[1,] 0.4444444 0.4444444 0.1111111
#[2,] 0.4000000 0.2000000 0.4000000
#[3,] 0.5000000 0.0000000 0.5000000
</code></pre>

<p>These proportions can be taken as the transition probabilities. Here we see that the probability of transitioning from state $1$ to state $3$ is found at location $X_{13}$ and is equal to $0.11$. This is to say that of all the observations of the system being in state $1$ ($9$ times not counting the last one that offered no information about transition), in approximately $11\%$ of them (once) the process transitioned to state $3$.</p>
",2013-11-06 02:11:50.383
58944,15972.0,1,58950.0,,,Necessary and sufficient condition on joint MGF for independence,<probability><independence><joint-distribution><moment-generating-function>,CC BY-SA 3.0,"<p>Suppose I have a joint moment generating function $M_{X,Y}(s,t)$ for a joint distribution with CDF $F_{X,Y}(x,y)$. Is $M_{X,Y}(s,t)=M_{X,Y}(s,0)â‹…M_{X,Y}(0,t)$ both a necessary <em>and sufficient</em> condition for independence of $X$ and $Y$? I checked a couple of textbooks, which only mentioned <em>necessity:</em></p>

<p>$$F_{X,Y}(x,y)=F_X(x)\cdot F_Y(y) \implies M_{X,Y}(s,t)=M_X(s) \cdot M_Y(t)$$   </p>

<p>That result is clear as independence implies $M_{X,Y}(s,t)=\mathbb{E}(e^{sX+tY})=\mathbb{E}(e^{sX}) \mathbb{E}(e^{tY})$. Since the MGFs of the marginals are determined by the joint MGF we have:</p>

<p>$$X,Y\text{ independent} \implies M_{X,Y}(s,t)=M_{X,Y}(s,0)â‹…M_{X,Y}(0,t)$$</p>

<p>But after searching online I found <a href=""http://math.tntech.edu/ISR/Introduction_to_Probability/Distributions_of_Functions/thispage/newnode10.html"" rel=""noreferrer"">only a fleeting reference, without proof, to the converse</a>. Is the following sketch proof workable?</p>

<p>Given a joint MGF $M_{X,Y}(s,t)$, this uniquely determines the marginal distributions of $X$ and $Y$ and their MGFs, 
$M_X(s)=M_{X,Y}(s,0)$ and $M_Y(t)=M_{X,Y}(0,t)$. The marginals alone are compatible with many other possible joint distributions, and uniquely determine a joint distribution in which $X$ and $Y$ are independent, with CDF $F_{X,Y}^{\text{ind}}(x,y)=F_X(x) \cdot F_Y(y)$ and MGF:</p>

<p>$$M_{X,Y}^{\text{ind}}(s,t) = M_X(s) \cdot M_Y(t) = M_{X,Y}(s,0)â‹…M_{X,Y}(0,t)$$</p>

<p>So if we are given, for our original MGF, that $M_{X,Y}(s,t) = M_{X,Y}(s,0)â‹…M_{X,Y}(0,t)$, this is sufficient to show $M_{X,Y}(s,t) = M_{X,Y}^{\text{ind}}(s,t)$. Then by the uniqeness of MGFs, our original joint distribution has $F_{X,Y}(x,y) = F_{X,Y}^{\text{ind}}(x,y) = F_X(x) \cdot F_Y(y)$ and $X$ and $Y$ are independent.</p>
",2013-11-06 02:13:09.800
58963,23425.0,1,,,,For a research project I'm trying to estimate how many videos currently exist on Youtube,<estimation><population><internet>,CC BY-SA 3.0,"<p>Official stats on how many videos (rather than how many hours or what volume of data) seem quite hard to come by. </p>

<p>My current idea is something like this:</p>

<p>A youtube URL is like: <a href=""http://www.youtube.com/watch?v=w1sjRD7NSec"" rel=""nofollow"">http://www.youtube.com/watch?v=w1sjRD7NSec</a></p>

<p>where</p>

<pre><code> id = w1sjRD7NSec
</code></pre>

<p>Assuming that each character in the id can be upper (26) or lower (26) case letter or number (10)</p>

<p>the number of combinations should be</p>

<pre><code> = (26 + 26 +10 ) ^ 11

 = 62^11

 = 5.2 x 10 ^19
</code></pre>

<p>So if I write a script to try random urls of 100/1000/whatever videos, and Z% are successful, then will </p>

<pre><code> Z% x 5.2 x 10 ^19
</code></pre>

<p>give me the number of videos that exist (or at least are downloadable), albeit with a very low confidence?</p>
",2013-11-06 09:53:06.613
58945,23414.0,2,,32038.0,,,,CC BY-SA 3.0,"<p>I would not use a random effects model with only 6 levels. Models using a 6-level random effect can sometime be run using many statistical programs and sometimes give unbiased estimates, but:  </p>

<ol>
<li>I think there is an arbitrary consensus in the statistical community that 10-20 is the minimum number. If you want to have your research published, you'll be advised to look for a journal without statistical review (or be able to justify your decision using fairly sophisticated language).  </li>
<li>With so few clusters, the between cluster variance is likely to be poorly estimated. Poor estimation of between cluster variance usually translates into poor estimation of the standard error of the coefficients of interest. (random effects models rely on the number of clusters theoretically going to infinity).  </li>
<li>Often the models simply don't converge. Have you tried running your model? I would surprised with only 12-16 measures per subject that the models converge. When I've managed to get this sort of model to converge I've had hundreds of measurements per cluster.</li>
</ol>

<p>This issue is addressed in most standard textbooks in the field and you've sort of addressed them in your question. I don't think I'm giving you any new information. </p>
",2013-11-06 02:18:19.680
58946,20473.0,2,,58940.0,,,,CC BY-SA 3.0,"<p>The last function you mention is the <a href=""http://en.wikipedia.org/wiki/Coefficient_of_variation"" rel=""nofollow"">coefficient of variation</a> (standard deviation over mean) of a sample of just two values:</p>

<p>$$c_v = \frac {\sigma}{\mu}$$
and when we have only two values, $\sigma = |x-y|/2$ while $\mu = (x+y)/2$.</p>

<p>As for your function, although by not using absolute value in the numerator you hint that direction may be important to you, I expect usually subtracting the smallest from the largest value.
Then, since our sample is only these two numbers essentially we have</p>

<p>$$\frac {\text {range}}{\max} = \frac {\max - \min}{\max} = 1- \frac {\min}{\max}$$</p>

<p>Now the $\frac {max}{min}$ ratio is encountered in various situations, check for example, ""dynamic range"" or ""contrast ratio"".</p>

<p>On a more mundane level, if $x$ is ""final price""$=p_f$ and $y$ is ""list price""$=p_l$, then</p>

<p>$$\frac {x-y} {\max(x,y)} = \frac {p_f -p_l}{p_l} $$</p>

<p>equals the ""percentage discount"" -with the negative sign to indicate the direction of revenues!</p>
",2013-11-06 02:23:14.020
58947,9456.0,1,,,,Confusion related to correlation in topic models,<correlation><topic-models>,CC BY-SA 3.0,"<p>I was reading this <a href=""http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2005_774.pdf"" rel=""nofollow noreferrer"">paper</a> related to correlated topic models. However, I didn't understand this correlation figure.</p>
<p>In the figure given below, I am confused about the level curves shown in the simplex. In the first case, it says that there is diagonal covariance with non zero mean. In the second case, it says that there is negative correlation between components 1 and 2. In the third case it says that there is positive correlation between 1 and 2.</p>
<p>I didn't understand how they intepreted it like this</p>
<p><img src=""https://i.stack.imgur.com/yrx7P.png"" alt=""enter image description here"" /></p>
<p>In the figure given below, the topics are uncorrelated as it says</p>
<p><img src=""https://i.stack.imgur.com/lzVlr.png"" alt=""enter image description here"" /></p>
<p>But I didn't get why it is so? Can anyone give me some insights?</p>
",2013-11-06 02:34:46.223
58948,23414.0,2,,23019.0,,,,CC BY-SA 3.0,"<p>I don't think the issues here can be addressed in a simple answer posted online. I would add:</p>

<ol>
<li>the inclusion of age and time is problematic and should be thought through. It is unclear to me what the benefit is of having both variables in the model. It can be done. But not by avoiding the issue by making one of the variables a random effect.<br>
1.5. if you want to include age, from what I understand, include age as age at start of experiment. This should not be collinear with other data and should be informative. </li>
<li>I would be very reluctant to include Age and time as random effects in this model. An assumption of the random effects model is that clusters are exchangeable.<br>
2.5. There is a tendency in the R code I've seen to include multiple random effects. I'm not sure why. Once you go beyond a single random effect, or simple single random effect clustered in another, the model complexity is significant and often not warranted.  </li>
<li><p>I don't think the models as written make sense. The following makes sense to me and are defensible:</p>

<p><code>lmer(FiringRate~ Time + (1|Subject))</code></p>

<p><code>lmer(FiringRate~ Time + (Time|Subject))</code></p>

<p><code>lmer(FiringRate~ Time + age_atstart + (Time|Subject))</code></p></li>
</ol>
",2013-11-06 02:49:31.580
58949,22488.0,1,58990.0,,,How do I check for bias of an estimator?,<self-study><inference><bias><unbiased-estimator>,CC BY-SA 3.0,"<p>I need to check if an estimator $\hat\theta$ for the parameter $\theta$ is biased. Theory says I should compare the expected value of $\hat\theta$ versus the expected value of $\theta$.</p>

<p>I assume the expected value of an estimator is the ""weighted average"" of the estimator and its distribution: $E[\hat\theta] = \int_0^\inf \hat\theta f(\hat\theta) d\hat\theta$*. If I'm right, to compute $E[\hat\theta]$ I need to know how $\hat\theta$ is distributed.</p>

<p>For example:</p>

<blockquote>
  <p>$X$ is a random var with support $0 &lt;= X &lt;= \theta$ and pdf $f(x;\theta) = 3x^2 / \theta^3$. Check if $\delta(x)=(4\bar X)/3$ is biased.</p>
</blockquote>

<p>*is it $E[\hat\theta] = \int_0^\inf \hat\theta f(\hat\theta) d\hat\theta$ or $E[\hat\theta] = \int_0^\inf \hat\theta f(x) dX$?</p>
",2013-11-06 03:08:06.123
58950,10135.0,2,,58944.0,,,,CC BY-SA 3.0,"<p>Yes, that's the necessary and sufficient condition for independence not only for two random variables but also for a (finite) sequence of random variables. Check out for example P.2 on page 242 of <a href=""http://books.google.ca/books?id=zF1kkWU_fmQC&amp;pg=PA242&amp;dq=moment%20generating%20function%20independent%20if%20and%20only%20if&amp;hl=en&amp;sa=X&amp;ei=h7J5UoaRNuGfyQGQtIGACw&amp;ved=0CEIQ6AEwAw#v=onepage&amp;q=moment%20generating%20function%20independent%20if%20and%20only%20if&amp;f=false"" rel=""noreferrer"">Probability with Statistical Applications</a>, By Rinaldo B. Schinazi. Or page 259 of
<a href=""http://books.google.ca/books?id=mFi05v3OVE0C&amp;pg=PA259&amp;dq=moment%20generating%20function%20independent%20if%20and%20only&amp;hl=en&amp;sa=X&amp;ei=PrF5UvLMNu3iyAHMmgE&amp;ved=0CEEQ6AEwAw#v=onepage&amp;q=moment%20generating%20function%20independent%20if%20and%20only&amp;f=false"" rel=""noreferrer"">Econometric Analysis of Count Data</a> which is based on probability generating function. Just note that ""the moment-generating function does not always exist"".</p>
",2013-11-06 03:14:02.577
58951,22564.0,1,,,,When does it make sense to reject/accept an hypothesis?,<hypothesis-testing><statistical-significance><p-value><likelihood>,CC BY-SA 4.0,"<p>In <a href=""http://arxiv.org/pdf/1311.0081v1.pdf"" rel=""nofollow noreferrer"">To P or not to P: on the evidential nature of P-values
and their place in scientific inference</a>, Michael Lew has shown that, at least for the t-test, the one-sided p-value and sample size can be interpreted as an &quot;address&quot; (my term) for a given likelihood function. I have repeated some of his figures below with slight modification. The left column shows the distribution of p-values expected due to theory for different effect sizes (difference between means/pooled sd) and sample sizes. The horizontal lines mark the &quot;slices&quot; from which we get the likelihood functions shown by the right panels for p=0.50 and p=0.025.</p>
<p><img src=""https://i.stack.imgur.com/c1KYL.png"" alt=""enter image description here"" /></p>
<p>These results are consistent with monte carlo simulations. For this figure I compared two groups with n=10 via t-test at a number of different effect sizes and binned 10,000 p-values into .01 intervals for each effect size. Specifically there was one group with mean=0, sd=1 and a second with a mean that I varied from -4 to 4, also with sd=1.</p>
<p><img src=""https://i.stack.imgur.com/wtNim.png"" alt=""enter image description here"" /></p>
<p>(The above figures can be directly compared to figures 7/8 from the paper linked above and are very similar, I found the heatmaps more informative than the &quot;clouds&quot; used in that paper and also wished to independently replicate his result.)</p>
<p>If we examine the likelihood functions &quot;indexed&quot; by the p-values, the behaviour of rejecting/accepting hypotheses or ignoring results giving p-values greater than 0.05 based on a cut-off value (either the arbitrary 0.05 used everywhere or determined by cost-benefit) appears to be absurd. Why should I not conclude from the n=100, p=0.5 case that &quot;the current evidence shows that any effect, if present, is small&quot;? Current practice would be to either &quot;accept&quot; there is no effect (hypothesis testing) or say &quot;more data needed&quot; (significance testing). I fail to see why I should do either of those things.</p>
<p>Perhaps when a theory <strong>predicted</strong> a precise point value, then rejecting a hypothesis could make sense. But when the hypotheses are of the form either &quot;mean1=mean2 or mean1!=mean2&quot; I see no value. Under the conditions these tests are often being used randomization does not <a href=""https://stats.stackexchange.com/questions/74350/is-randomization-reliable-with-small-samples?lq=1"">guarantee all confounds are balanced across groups</a> and there should always be the worry of <a href=""https://stats.stackexchange.com/questions/74262/what-examples-of-lurking-variables-in-controlled-experiments-are-there-in-public"">lurking variables</a>, so rejecting  the hypothesis that mean1 exactly equals mean2 has no scientific value as far as I can tell.</p>
<p>Are there cases beyond the t-test where this argument would not apply? Am I missing something of value that rejecting a hypothesis with low a priori probability provides to researchers? Ignoring results above an arbitrary cutoff seems to have lead to widespread publication bias. What useful role does ignoring results play for scientists?</p>
<p><strong>Michael Lew's R code to calculate the p-value distributions</strong></p>

<pre class=""lang-r prettyprint-override""><code>LikeFromStudentsTP &lt;- function(n, x, Pobs, test.type, 
      alt='one.sided'){
# test.type can be 'one.sample', 'two.sample' or 'paired'
# n is the sample size (per group for test.type = 'two.sample')
# Pobs is the observed P-value
# h is a small number used in the trivial differentiation
h &lt;- 10^-7
PowerDn &lt;- power.t.test('n'=n, 'delta'=x, 'sd'=1,
'sig.level' = Pobs-h, 'type'= test.type, 'alternative'=alt)
PowerUp &lt;- power.t.test('n'=n, 'delta'=deltaOnSigma, 'sd'=1,
'sig.level' = Pobs+h, 'type'= test.type, 'alternative'=alt)
PowerSlope &lt;- (PowerUp<span class=""math-container"">$power-PowerDn$</span>power)/(h*2)
L &lt;- PowerSlope
}
</code></pre>
<p><strong>R code for figure 1</strong></p>
<pre class=""lang-r prettyprint-override""><code>deltaOnSigma &lt;- 0.01*c(-400:400)
type &lt;- 'two.sample'
alt='one.sided'
p.vals &lt;- seq(0.001,.999,by=.001)

#dev.new()
par(mfrow=c(4, 2))
for(n in c(3, 5, 10, 100)){
  
  m&lt;-matrix(nrow=length(deltaOnSigma), ncol=length(p.vals))
  cnt &lt;- 1
  for(P in p.vals){
    m[, cnt] &lt;- LikeFromStudentsTP(n, deltaOnSigma, P, type, alt)
    cnt &lt;- cnt+1
  }
  
  #remove very small values
  m[which(m/max(m, na.rm=TRUE) &lt; 10^-5)] &lt;- NA
        
  m2 &lt;- log(m)
  
  par(mar=c(4.1, 5.1, 2.1, 2.1))
  image.plot(m2, axes=FALSE,
             breaks=seq(min(m2, na.rm=TRUE), max(m2,  na.rm=T), 
             length=1000), col=rainbow(999),
             xlab=&quot;Effect Size&quot;, ylab=&quot;P-value&quot;
  )
  title(main=paste(&quot;n=&quot;,n))
  axis(side=1, at=seq(0,1,by=.25), labels=seq(-4,4,by=2))
  axis(side=2, at=seq(0,1,by=.05), labels=seq(0,1,by=.05))
  axis(side=4, at =.5, labels=&quot;Log-Likelihood&quot;, pos=.95, tick=F)
  abline(v=0.5, lwd=1)
  abline(h=.5, lwd=3, lty=1)
  abline(h=.025, lwd=3, lty=2)
  par(mar=c(5.1,4.1,4.1,2.1))
        
  plot(deltaOnSigma, m[, which(p.vals==.025)], type=&quot;l&quot;, lwd=3, 
         lty=2, xlab=&quot;Effect Size&quot;, ylab=&quot;Likelihood&quot;, 
         xlim=c(-4, 4),
         main=paste(&quot;Likelihood functions for&quot;,&quot;n=&quot;,n)
  )
  lines(deltaOnSigma,m[,which(p.vals==.5)], lwd=3, lty=1)
  legend(&quot;topleft&quot;, legend=c(&quot;p=.5&quot;,&quot;p=.025&quot;), lty=c(1,2),lwd=1, 
           bty=&quot;n&quot;)
}
</code></pre>
<p><strong>R code for figure 2</strong></p>
<pre class=""lang-r prettyprint-override""><code>p.vals &lt;- seq(0, 1, by=.01)
deltaOnSigma &lt;- 0.01*c(-400:400)
n &lt;- 10
n2 &lt;- 10
sd2 &lt;- 1
num.sims &lt;- 10000
sp &lt;- sqrt((9*1^2 +(n2-1)*sd2^2)/(n+n2-2))

p.out=matrix(nrow=num.sims*length(deltaOnSigma) ,ncol=2)
m &lt;- matrix(0, nrow=length(deltaOnSigma), ncol=length(p.vals))
pb &lt;- txtProgressBar(min = 0, max = length(deltaOnSigma) , 
                     style = 3)
cnt &lt;- 1
cnt2 &lt;- 1
for(i in deltaOnSigma ){
  
  for(j in 1:num.sims){
    
    m2 &lt;- i
    a &lt;- rnorm(n, 0, 1)
    b &lt;- rnorm(n, m2, sd2)
    p &lt;- t.test(a, b, alternative=&quot;less&quot;)$p.value
    
    r &lt;- end(which(deltaOnSigma&lt;=m2/sp))[1]
    
    m[r, end(which(p.vals&lt;p))[1]] &lt;- 
         m[r, end(which(p.vals&lt;p))[1]] + 1
    p.out[cnt,] &lt;- cbind(m2/sp, p)
    cnt &lt;- cnt+1
    
  }
  cnt2 &lt;- cnt2+1
  setTxtProgressBar(pb, cnt2)
}
close(pb)


m[which(m==0)] &lt;- NA

m2 &lt;- log(m)

dev.new()
par(mfrow=c(2,1))
par(mar=c(4.1,5.1,2.1,2.1))
image.plot(m2, axes=FALSE,
           breaks=seq(min(m2, na.rm=TRUE), max(m2, na.rm=TRUE), 
           length=1000), col=rainbow(999),
           xlab=&quot;Effect Size&quot;, ylab=&quot;P-value&quot;
)
title(main=paste(&quot;n=&quot;,n))
axis(side=1, at=seq(0,1,by=.25), labels=seq(-4,4,by=2))
axis(side=2, at=seq(0,1,by=.05), labels=seq(0,1,by=.05))
axis(side=4, at =.5, labels=&quot;Log-Count&quot;, pos=.95, tick=F)
abline(h=.5, lwd=3, lty=1)
abline(h=.025, lwd=3, lty=2)
abline(v=.5, lwd=2, lty=1)
par(mar=c(5.1,4.1,4.1,2.1))


hist(p.out[which(p.out[,2]&gt;.024 &amp; p.out[,2]&lt;.026),1],
     xlim=c(-4,4), xlab=&quot;Effect Size&quot;, col=rgb(1,0,0,.5), 
     main=paste(&quot;Effect Sizes for&quot;,&quot;n=&quot;,n)
)
hist(p.out[which(p.out[,2]&gt;(.499) &amp; p.out[,2]&lt;.501),1], add=T,
     xlim=c(-4,4),col=rgb(0,0,1,.5)
)

legend(&quot;topleft&quot;, legend=c(&quot;0.499&lt;p&lt;0.501&quot;,&quot;0.024&lt;p&lt;0.026&quot;), 
       col=c(&quot;Blue&quot;,&quot;Red&quot;), lwd=3, bty=&quot;n&quot;)
</code></pre>
",2013-11-06 03:23:21.103
58952,13165.0,2,,58789.0,,,,CC BY-SA 3.0,"<p>First of all, observe that, all of the factors include all of the variables and their joint distributions. But they have more factors. In particular, each variable inside one separator $S$ is repeated $d(S)$ times inside the neighbouring  factors. That's why the distribution is normalized by dividing by the separators to the power of $d(S)$. </p>
",2013-11-06 03:40:59.913
58953,1145.0,2,,58951.0,,,,CC BY-SA 3.0,"<p>I really like your rainbow versions of my clouds, and may 'borrow' them for a future version of my paper. Thank you!</p>

<p>Your questions are not entirely clear to me, so I will paraphrase them. If they are not what you had in mind them my answers will be misdirected!</p>

<ul>
<li><p>Are there situations where rejection of the hypothesis like ""mean1 equals mean2"" is scientifically valuable? </p></li>
</ul>

<p>Frequentists would contend that the advantage of having well-defined error rates outweighs the loss of assessment of evidence that comes with their methods, but I don't think that that is very often the case. (And I would suspect that few proponents of the methods really understand the complete loss of evidential consideration of the data that they entail.) Fisher was adamant that the Neyman-Pearson approach to testing had no place in a scientific program, but he did allow that they were appropriate in the situation of 'industrial acceptance testing'. Presumably such a setting is a situation where rejection of a point hypothesis can be useful.</p>

<p>Most of science is more accurately modelled as estimation than as an acceptance procedure. P-values and the likelihood functions that they index (or, to use your term, address) provide very useful information for estimation, and for inferences based on that estimation.</p>

<p>(A couple of old StackExchange questions and answerd are relevant: <a href=""https://stats.stackexchange.com/questions/16218/what-is-the-difference-between-testing-of-hypothesis-and-test-of-significance/16227#16227"">What is the difference between &quot;testing of hypothesis&quot; and &quot;test of significance&quot;?</a> and <a href=""https://stats.stackexchange.com/questions/46856/interpretation-of-p-value-in-hypothesis-testing/46858#46858"">Interpretation of p-value in hypothesis testing</a>)</p>

<ul>
<li><p>Are you missing the point of rejection  of a hypothesis (of low a priori probability)?</p></li>
</ul>

<p>I don't know if you are missing much, but it is probably not a good idea to add prior probabilities into this mixture! Much of the argumentation around the ideas relating to hypothesis testing, significance testing and evidential evaluation come from entrenched positions. Such arguments are not very helpful. (You might have noticed how carefully I avoided bringing Bayesianism into my discussion in the paper, even though I wholeheartedly embrace it when there are reasonable prior probabilities to use. First we need to fix the P-value provide evidence, error rates do not issue.)</p>

<ul>
<li>Should scientists ignore results that fail to reach 'significance'?</li>
</ul>

<p>No, of course not. Using an arbitrary cutoff to claim significance, or to assume significance, publishability, repeatability or reality of a result is a bad idea in most situations. The results of scientific experiments should be interpreted in light of prior understanding, prior probabilities where available, theory, the weight of contrary and complementary evidence, replications, loss functions where appropriate and a myriad of other intangibles. Scientists should not hand over to insentient algorithms the responsibility for inference. However, to make full use of the evidence within their experimental results scientists will need to much better understand what the statistical analyses can and do provide. That is the purpose of the paper that you have explored. It will also be necessary that scientists make a more complete account of their acquisition of evidence and the evolution of their understanding than what is usually presented in papers, and they should provide what Abelson called a principled argument to support their inferences. Relying on P&lt;0.05 is the opposite of a principled argument.</p>
",2013-11-06 04:56:31.703
58956,,2,,58916.0,user31668,,,CC BY-SA 3.0,"<p>First, a minor terminology correction: You can't really have independent and mutually exclusive events, as mutually exclusive implies that if one event happens, the others cannot happen, which makes them not independent. I think what you mean is that they are <em>sequential</em>, in that they happen one after another. </p>

<p>Since you have the pdf of each, what you want to calculate is the sum of the three random times: $T = T_A+T_B+T_C$. Depending on the exact form of your pdfs, you can try one of three approaches: </p>

<ol>
<li>Recognize the sum as being equal to a particular distribution (e.g., sum of normals is normal)</li>
<li>Simulate the sum using Monte Carlo simulation</li>
<li>Analytically calculate the distribution of the sum by either multiplying the<a href=""http://en.wikipedia.org/wiki/Characteristic_function_%28probability_theory%29"" rel=""nofollow"">characteristic functions</a> of each pdf and then back-transforming to a denstity, or by directly performing two <a href=""http://en.wikipedia.org/wiki/Convolution#Definition"" rel=""nofollow"">convolutions</a>: $f_C*(f_A*f_B)$</li>
</ol>

<p>I would recommend method 2 if you have access to a monte carlo simulator or know R or Matlab or any other numerical package. 1 is also OK...3 is a real pain unless you are lucky to be using simple distributions...in which case you will likely find a solution as per 1.</p>
",2013-11-06 05:22:28.300
58957,23418.0,1,,,,Joint probability of two correlated RVs,<conditional-probability><density-function><truncation><conditioning>,CC BY-SA 3.0,"<p>I am trying to get the joint PDF of two RVs $X$ and $Y$ where $aX&lt;Y&lt;bX$, so I am stuck in calculating the probability of</p>

<p>$\mathbb{P}(X&lt;x,Y&lt;y|aX&lt;Y&lt;bX)$</p>

<p>any idea?</p>
",2013-11-06 06:33:28.320
58958,17459.0,1,,,,An unfair coin with different face value problem,<probability><density-function>,CC BY-SA 3.0,"<p>Could any one give me some hints for the following problem:
A coin has two face: 1 and -1/2,  and the probability for 1 is $P(x=1)=1/3$ and for $-1/2$ is $P(x=-1/2)=2/3$. The expected value is therefore $E(x)=1\cdot(1/3)+(-1/2)\cdot(2/3) = 0$.<br>
Supposing for an infinite tossing, what's the chance that the average value would be bigger than $k$? (supposing the sample number is sufficiently large)</p>

<p>Since if the face value are 1 and 0, it is simply a binomial distribution and can be approximate by normal distribution, so I also want to know if this problem can be transformed into binomial distribution $P(x&gt;k)$ ?</p>

<p>This is not any homework problem, but a problem I just think of and try to solve.</p>
",2013-11-06 06:58:05.353
58959,594.0,2,,58957.0,,,,CC BY-SA 3.0,"<p>Your question is quite general (specific cases may offer shortcuts), so I'll limit myself to suggesting strategies. Typically such a question involves constructing appropriate limits on integrals and trying to evaluate them by some means. Usually there will be a bivariate integral where the limits in the inner integral involve the variable in the outer integral. Sometimes the hardest part is simply writing the correct limits down; the general case will involve ""min"" and ""max"" functions on a problem like this. </p>

<p>To make progress I strongly suggest you get into the habit of making diagrams of the region you're trying to integrate.</p>

<p>A couple of suggested strategies for approaching such a problem, by making slightly simpler problems you might see how to write integrals for. </p>

<p>One approach: Let $Z = Y-aX$ and work out the joint probability in terms of $X$ and $Z$. </p>

<p>Another approach: First, replace your $x$ and $y$ with $x_0$ and $y_0$ so I can use $x$ and $y$ as dummy variables in the integration. If $a\leq y_0/x_0\leq b$ then you have a region like this:</p>

<p><img src=""https://i.stack.imgur.com/tdNn1.png"" alt=""region""> </p>

<p>While you can actually write the integral by splitting it up into pieces (move a vertical line along the $x$-axis and split the integral where your line hits any 'corners' on the dark green region), you might otherwise evaluate it by working out the probability of being in the rectangle $0&lt;X&lt;x_0; 0&lt;Y&lt;y_0$ and then subtract the two triangles.</p>

<p>The other cases (the other arrangements of $x_0$ and $y_0$ relative to $a$ and $b$) might be worked out by drawing the relevant diagram in order to obtain the right limits on the integrals; in each case you'll do something similar, but sometimes you might not hit a corner.</p>

<p>With more details, more specific responses might be possible.</p>
",2013-11-06 07:12:01.233
58960,20470.0,2,,58941.0,,,,CC BY-SA 3.0,"<p>Directly taken from a document on bivariate statistics with SPSS that lives <a href=""http://www.american.edu/ctrl/upload/SPSS_Bivariate_Statistics_Spring_2010-2.pdf"" rel=""nofollow"">here</a>:</p>

<blockquote>
  <p><strong>Chi-square</strong> is a useful technique because you can use it to see if
  thereâ€™s a relationship between two ordinal variables, two nominal
  variables, or between an ordinal and a nominal variable. You look at
  the assymp. Sig column and if it is less than .05, the relationship
  between the two variables is statistically significant.</p>
</blockquote>
",2013-11-06 08:58:02.350
58961,23422.0,2,,53391.0,,,,CC BY-SA 3.0,"<p>The formula that you refer to can be used when the distribution of classes is the same in the training and test set (which is commonly assumed with machine learning).</p>

<p>Take 7 classes: A, B, C, D, E, F, G. There will be #A instances with label A in your data set. And of course, #A + #B + #C + #D + #E + #F + #G = X</p>

<p>The chance of encountering an instance with label A, i.e. the probability of class A, pA, equals #A/X.</p>

<p>Now, if you consider a random baseline system, this system will assign labels to instances according to these probabilities. Because labels are assigned according to probabilities, each time you let the system label the instances a different result will be produced. A majority system or your SVM-based system will produce the same result, no matter how often they are applied. With an infinite number of runs of the random baseline system, on average, the following will happen:</p>

<p>Given an instance with gold label A, this instance will be labelled pA times as A, pB times as B, etc. This means that we have a (fractional) true positive count equal to the probability pA. There are #A instances with gold label A, the total true positive count for label A becomes #A*pA. This can be done for each label. The total number of true positives, TP, becomes:</p>

<p>TP = #A*pA + #B*pB + #C*pC + #D*pD + #E*pE + #F*pF + #G*pG</p>

<p>And the average accuracy of this baseline system becomes acc = TP/X</p>

<p>acc = 1/X * (#A*pA + #B*pB + #C*pC + #D*pD + #E*pE + #F*pF + #G*pG)</p>

<p>If the X is distributed over the different terms, and using the definition of the probabilities, this becomes:</p>

<p>acc = pA*pA + pB*pB + pC*pC + pD*pD + pE*pE + pF*pF + pG*pG</p>

<p>which is the formula that you refer to.</p>

<hr>

<p>As noted before, for an SVM-based system or a majority system, the average accuracy is equal to the accuracy of a single run. Meaning that the accuracies of a single run can be compared with the outcome of the random baseline formula.</p>

<p>If your machine learner produces slightly different results with each run (because it contains an element of randomness), you should compute the average accuracy for an infinite number of runs. But this is the ideal situation, and it may be impossible to compute. In practice, differences will be probably very small and most people stick to comparing using the outcome of a single run.  </p>
",2013-11-06 09:14:07.293
58962,22507.0,2,,58958.0,,,,CC BY-SA 3.0,"<p>Because of the central limit theorem, for large number of tosses $n$, the total value is distributed normally as $N(0, \sigma)$, where $\sigma^2 = n \left( {1\over 3} 1^2 + {2 \over 3} (1/2)^2 \right) = {n \over 2}$.</p>
",2013-11-06 09:23:20.700
58988,19752.0,2,,58984.0,,,,CC BY-SA 3.0,"<p>One possibility is to use a <a href=""http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Jeffreys_interval"" rel=""nofollow"">Jeffreys Interval</a>, which has a Bayesian derivation. One nice feature is that you're guaranteed to get values between 0 and 1. When using a Wald interval on similar problems I got negative lower bounds, which made no sense.</p>

<p>The confidence intervals are found by computing the $\alpha$ quantiles of a $\text{Beta}(x + 1/2, n â€“ x + 1/2)$ distribution. Where $x$ is the number of successes and $n$ is the number of trials. Computing these quantiles must be done numerically, but it's no problem in something like R.</p>
",2013-11-06 17:17:27.510
58964,23404.0,1,58981.0,,,Interpretation of the regression coefficient of a proportion II,<regression><logistic><regression-coefficients>,CC BY-SA 3.0,"<p>Following question: 
<a href=""https://stats.stackexchange.com/questions/47483/interpretation-of-the-regression-coefficient-of-a-proportion-type-independent-va""><a href=""https://stats.stackexchange.com/questions/47483/interpretation-of-the-regression-coefficient-of-a-proportion-type-independent-va"">Interpretation of the regression coefficient of a proportion type independent variable</a></a></p>

<p>In my model I have a log dependet variable.
As indenpendet variables I have one proportion $X_1$ that lies within [0.001, 0.30]
and a second proportion $X_2$ that lies within [0.12, 0.99].</p>

<p>$E(\log(y))$ = $\alpha + \beta_1X_1$ + $\beta_2X_2$</p>

<p>How do I interpret the coefficients? LetÂ´s say, the estimated $\beta_1$ is 1.5 , the estimated $\beta_1$ is 0.8. </p>

<p>Or do I have to adjust the range before running the regression?</p>
",2013-11-06 10:54:36.910
58965,23427.0,1,,,,Correct use of AIC,<model-selection><aic>,CC BY-SA 3.0,"<p>It is well known that the AIC can be used to compare nested models. </p>

<p>Additionally, I believe I am correct in saying that you can also use the AIC to compare non-nested models on the same dataset (please correct me if I am in fact wrong). However, it is not correct to use the AIC to compare between different data sets.</p>

<p>In my scenario, I have 5 measurements on individuals over time plus an outcome variable. If I was to regress the outcome variable linearly on all the measurements over time, I would be able to obtain an AIC for this model which uses the entire dataset. </p>

<p>Now I want to consider only 2 of the measurements of all individuals plus the outcome variable. Technically, I am now using a subset of the original dataset as I have lost information on the other three measurements. However, isn't this the same as fitting a nested model since I kept all individuals but lost 3 explanatory variables? So is it justified to compare the AIC I get from this model with that obtained from the full model?</p>
",2013-11-06 11:01:34.377
58966,23428.0,1,,,,"Given $n$ ratings from 0 to 1, how to calculate a weighted ""average"" estimating the ""true"" rating?",<confidence-interval><data-mining><mean><recommender-system>,CC BY-SA 3.0,"<p>I've read <a href=""http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"" rel=""nofollow"">How Not to Sort By Average Rating</a> regarding how to average binary positive/negative ratings in a way that takes the number of ratings into account. The author uses the ""lower bound of Wilson score confidence interval for a Bernoulli parameter"". However, the items I'm dealing with have continuous ratings from 0 to 1. Is there an analogous averaging technique for this case?</p>

<p>My ratings collection is long-tailed: the median item has only 2 ratings, but the average one has 80, and the most-rated item has 36,000 ratings. Intuitively ten ratings of 0.8 should ""average"" higher than one of 0.9, but I'd like a precise formulation of this intuition.</p>

<p>(I'm using this to design a recommender system, which has to deal with 50,000 users and 10,000 items. I'm evaluating various known recommenders, like GroupLens and LSI, and have to design one that doesn't perform too much worse than those (and hopefully better). I was reminded of this blog post on averages when using users' average ratings for a baseline RMSE calculation.)</p>
",2013-11-06 11:05:00.940
58967,449.0,2,,58965.0,,,,CC BY-SA 3.0,"<p>To you, all of the predictors in the model are data so, from that perspective, data is changing. But the data that needs to be consistent across models for AIC is the outcome or response variable. As long as that is the same data in each comparison you can use AIC. Search AIC model selection on this site. This has been discussed several times.</p>
",2013-11-06 11:34:48.943
58968,22653.0,1,,,,What is the 95% confidence interval / ellipse in a PCA plot telling me?,<confidence-interval><pca>,CC BY-SA 3.0,"<p>I am new to statistics and within the last two days I tried to get my head around PCA plots. Now, I kind of understand what they are showing but I am still not sure about the 95% confidence ellipse that is very often shown in such plots. The 2 dimensional PCA plot displays the two biggest variances (whatever these are) in the data but I don't know what the ellipse is trying to tell me and what it means if a sample/dot (whatever is displayed) is lying outside that ellipse.</p>

<p>Help is very much appreciated.</p>
",2013-11-06 11:57:06.773
58969,503.0,2,,58965.0,,,,CC BY-SA 3.0,"<p>@John is correct but I think not quite explicit enough, perhaps leading to confusion (maybe not now, but if someone else reads this thread later).</p>

<p>AIC needs to be on the same data set: That is, it has to have the same subjects. And it needs to have the same dependent variable (John's point). To be nested, one set of independent variables must be a subset of the other, but AIC can work on non-nested subsets (at least, according to some people see <a href=""https://stats.stackexchange.com/questions/20441/non-nested-model-selection"">this thread</a>). </p>
",2013-11-06 12:06:35.297
58970,503.0,2,,58968.0,,,,CC BY-SA 3.0,"<p>The ellipse around a scatter plot of ""component 1"" vs. ""component 2"" has a similar meaning to the ellipse around any other scatter plot. Unfortunately, there are two common uses of such ellipses: Prediction ellipses and confidence ellipses. <a href=""http://support.sas.com/documentation/cdl/en/procstat/63963/HTML/default/viewer.htm#procstat_corr_sect021.htm"" rel=""nofollow""><code>SAS</code> documentation</a> explains the difference (as do other sites)</p>
",2013-11-06 12:15:49.530
58971,5480.0,1,,,,Ranking topics in K-Means,<clustering>,CC BY-SA 3.0,"<p>I understand that clustering is meant to group items together, is there any ways that we can quantify saying Cluster A is more important than Cluster B? Other than counting the number of items in a cluster?</p>
",2013-11-06 12:38:16.353
58972,22972.0,2,,58963.0,,,,CC BY-SA 3.0,"<p>I think the answer to your question is yes. What you have is two columns, one is a long list of combinations and one with a 1 or a 0. 1 if there is a video at that combination and 0 if there is not a video. Were you to sum this second column, then this will give you the number of videos. For the whole population this works. </p>

<p>The only problem I see is in drawing a random sample as I have no idea how youtube generates its urls. You could of course send them an email and ask. I would just give it a shot with the script, try different ways to generate random urls take a large sample (n>1,000) and see whether the estimates are close enough.  </p>
",2013-11-06 12:40:09.360
58973,23171.0,1,58975.0,,,Show that $R_{n}^{2}-n$ and $(-1)^{n} \cos(\pi R_{n}) $ are $\mathcal F_{n}$-martingales,<self-study><stochastic-processes><marginal-distribution>,CC BY-SA 3.0,"<p>Let $X_{i}, i\ge 1$, be i.i.d. random variables defined on a probability space $(\Omega, \mathcal F,P)$ such that $P(X_{i}=1)=P(X_{i}=-1)=\frac{1}{2}$. Consider the filtration $\mathcal F_{n}=\sigma(X_{1},\dots,X_{n})$ on this space and the random walk $R_{i}=\sum_{i=1}^{n} X_{i}$.  </p>

<p>Show that $R_{n}^{2}-n$ and $(-1)^{n} \cos(\pi R_{n}) $ are $\mathcal F_{n}$-martingales. </p>
",2013-11-06 13:02:15.287
58974,22972.0,2,,58809.0,,,,CC BY-SA 3.0,"<p>First, the p-value will not tell you whether a home court advantage exists or not. It will only tell you the probability of the data given a hypothesis. Second, the data you have will not tell you anything about a home court advantage, because you have nothing to compare it to. What is the proportion of games won away or what is the proportion of games won overall? The only thing you try to find out is whether the proportion of games won is statistically significant different from .5. </p>

<p>Now if you have data on games played away, then you can set up your null hypothesis as:</p>

<p>$H_{0}: \; p_{home} = p_{away}$ </p>

<p>where, $p_{home}=\frac{won_{home}}{total_{home}}$ and $p_{away}=\frac{won_{away}}{total_{away}}$ - if you want you can write a tie as a half win or exclude the ties all together - then what you have is categorical variable $home$ which can take a value of $1$ or $0$: $1$ for home and $0$ for away and two dependent proportions. For this problem you can then use the McNemar test or the paired t-test if your sample is large enough (see here: <a href=""http://www.ats.ucla.edu/stat/stata/whatstat"" rel=""nofollow"">http://www.ats.ucla.edu/stat/stata/whatstat</a>). </p>

<p>Hope this helps. </p>
",2013-11-06 13:02:36.247
58975,1406.0,2,,58973.0,,,,CC BY-SA 3.0,"<p>Simply write $R_n=R_{n-1}+X_n$ and take the conditional expectations with respect to $\mathcal{F}_n$. Then exploit the fact that </p>

<p>$$E[g(X_n)f(R_{n-1})|\mathcal{F}_n]=f(R_{n-1})Eg(X_n),$$ 
for any measurable functions $f$ and $g$ (assuming that the expectations exist). Also recall the formula </p>

<p>$$\cos(\alpha+\beta)=\cos\alpha\cos\beta-\sin\alpha\sin\beta$$.</p>
",2013-11-06 13:19:46.403
58976,23431.0,1,,,,Data split into training and test,<machine-learning><cross-validation><train>,CC BY-SA 3.0,"<p>I am implementing an EEG classifier with 15 subjects (patients), specifically a support vector machine classifier.</p>

<p>I randomly choose the training and testing sets, but I was faced by a question ""how did you choose subjects in each set?"". I looked for the response but I couldn't find a good one (cross validation wouldn't be the best solution in my case). </p>

<p>Could you please help me with this problem?</p>
",2013-11-06 13:44:37.607
58977,16110.0,1,,,,Using nls() function in R for exponential function,<r><time-series><nls>,CC BY-SA 3.0,"<p>I know that this issue was already discussed here but I faced with the problem I can't solve. I have list of persons, each represented with some time series consisting from 4-8 points. I want to approximate them all with the function $y=a\cdot x^2\cdot exp(-bx)+c$.
Thus for each person I am going to find his own ""a"", ""b"" and ""c"".
For most of them next code works very good:</p>

<p><code>res=nls(p2[,2] ~ c+a*I(p2[,1]^2)*exp(b*p2[,1]),start=list(a=0.005,b=-0.005,c=5))</code></p>

<p>However for some persons these starting values don't work, R returned ""Missing value or an infinity produced when evaluating the model"" or ""singular gradient matrix at initial parameter estimates"". For some of these people these starting values worked:</p>

<p><code>res=nls(p2[,2] ~ c+a*I(p2[,1]^2)*exp(b*p2[,1]),start=list(a=0.1,b=-0.02,c=5))</code></p>

<p>Could anybody give any clear suggestion how to choose starting points for all the people I consider?
I tried to use <code>tryCatch</code> to try different staring values and find those which work but another problem appeared: 
this code
<code>nls(p2[,2] ~ c+a*I(p2[,1]^2)*exp(b*p2[,1]),start=list(a=5,b=0,c=5))</code>
led to: </p>

<pre><code>        a         b         c 
 -0.00166  -0.00269 140.87366 
</code></pre>

<p>while 
<code>nls(p2[,2] ~ c+a*I(p2[,1]^2)*exp(b*p2[,1]),start=list(a=0.1,b=-0.02,c=5))</code>
led to</p>

<pre><code>      a       b       c 
 0.2024 -0.0251 47.7811 
</code></pre>

<p>So by choosing different starting values we have different answers. How can this happen? I thought that since NLS function is quadratic, it can't have more than 1 extremum...
Do you have any suggestions about how should I proceed in this situation?</p>
",2013-11-06 14:19:30.577
58978,4320.0,2,,58934.0,,,,CC BY-SA 3.0,"<p>The problem is small enough you can work it out by hand. For your example you have
$$
\begin{align*}
P(outlook = sunny| play=yes) &amp;= \frac{2}{9}\\
P(temp = cool| play=yes) &amp;= \frac{3}{9}\\
P(humidity=high| play=yes) &amp;= \frac{3}{9}\\
P(windy=true| play=yes) &amp;= \frac{3}{9}\\
P(play=yes) &amp;= \frac{9}{14}.\\
\end{align*}
$$
Putting it all together you have
$$
\begin{align*}
P(play=yes|sunny, cool, high, true) &amp;\varpropto \frac{2}{9} \left(\frac{3}{9}\right)^3 \frac{9}{4}\\
&amp;\approx 0.0053,
\end{align*}
$$
which agrees with Mitchell. I don't use R, so I can't speak as to why the output is different. Obviously the package you're using is normalizing, but this shouldn't change the classification. If I had to guess I'd say it is the cross validation.</p>
",2013-11-06 14:20:07.447
58979,1575.0,1,,,,Flaw in a conditional probability argument,<probability><conditional-probability><bayes>,CC BY-SA 3.0,"<p>Imagine an experiment where you roll two fair, six-sided dice. Someone peeks at the dice, and (truthfully) tells you that ""at least one of the dice is a 4"". What is the probability that the total of the dice is 7?</p>

<p>It seems straightforward to calculate that the probability the total is 7 is 2/11.</p>

<p>However, the person who peeked at the dice could equally well have said ""at least one of the dice is a 1"" and you would come to the same conclusion - 2/11. Or they could have said ""at least one of the dice is a 2"" or ""at least one of the dice is a 3"", or indeed any number from 1 to 6, and you would still conclude that the probability that the total is 7 is 2/11.</p>

<p>Since you will always conclude that the probability that the total is 7 is 2/11, you could block your ears as they speak, and you'd still come up with 2/11. From there it's a short hop to conclude that <em>even if they don't say anything</em> the probability that the total is 7 is 2/11.</p>

<p>However, clearly if they don't say anything, the probability that the total is 7 is not 2/11, but rather 1/6.</p>

<p>Where is the flaw in the argument?</p>
",2013-11-06 15:07:02.440
58989,23438.0,1,,,,Multinomial confidence interval,<confidence-interval><multinomial-distribution>,CC BY-SA 3.0,"<p>I am trying to help out a quality auditing department and figure out sample sizes and confidence intervals and such. The sample size and confidence level (95%) is pretty well documented on the web and I am muddling through it. I even figured out how to do the exact confidence level of a <a href=""http://statpages.org/confint.html"" rel=""nofollow"">binomial sample</a> which I would use for a pass/fail condition but I wanted to figure out how to do a multinomial of that same sample so that I could generate some type of information about a situation with three possibilities: pass / fail(critical) / fail(non-critical).</p>

<p>Can someone point me in the right direction here? Ultimately I am going to want to program this into an application so a formula would get me going, a link to a way to program it would be a home run.</p>
",2013-11-06 17:21:38.667
59003,23414.0,2,,58977.0,,,,CC BY-SA 3.0,"<p>Bill has given a great answer. But maybe some practical things are worth adding.</p>

<ol>
<li><code>nls</code> is sensitive to starting values. Depending on your staring values you're going to get different answers or your model might not converge. That's life. No reason to be surprised. An important part of <code>nls</code> is working out a methodology of deriving your starting values. </li>
<li><code>nls</code> is particularly sensitive. If you're having troubles you should always <code>minpack.lm</code> before reassessing your approach. Using an optimizing function that is more robust to starting values.</li>
<li>Try what Bill recommends. </li>
</ol>
",2013-11-06 20:47:27.883
58980,23302.0,1,,,,Goodness of fit shows less than 0.01 p-values at all 16 distributions for my wind data,<distributions><goodness-of-fit><weibull-distribution>,CC BY-SA 3.0,"<p>I have got two wind datasets (one for 4 months and the other one for 12 years). Typically wind speed distributions follow Weibull distributions. In order to demonstrate my data distribution in a statistically sound manner, I ran goodness of fit test in Minitab. However, all p-values of 16 distributions including Weibull distribution showed extremely low (&lt;0.01). I have no idea what to do next as all distributions do not fit with my data.. 
Weibull distributions showed lowest AD values though. Can I use it to support that my data has Weibull distributions? Below is the description of my test result.</p>

<p>Descriptive Statistics</p>

<pre><code>   N  N*     Mean    StDev  Median    Minimum  Maximum  Skewness   Kurtosis
2894   0  15.8579  8.69187   14.56  0.0533333    44.42  0.521124  -0.317979


Box-Cox transformation: Lambda = 0.573785

Johnson transformation function:
0.910617 + 1.25421 * Ln( ( X + 2.42013 ) / ( 50.6894 - X ) )


Goodness of Fit Test

Distribution                  AD       P  LRT P
Normal                    21.568  &lt;0.005
Box-Cox Transformation     2.035  &lt;0.005
Lognormal                 52.753  &lt;0.005
3-Parameter Lognormal      4.050       *  0.000
Exponential              236.204  &lt;0.003
2-Parameter Exponential  233.814  &lt;0.010  0.000
Weibull                    1.779  &lt;0.010
3-Parameter Weibull        1.514  &lt;0.005  0.000
Smallest Extreme Value    77.762  &lt;0.010
Largest Extreme Value      4.727  &lt;0.010
Gamma                     10.989  &lt;0.005
3-Parameter Gamma          2.917       *  0.000
Logistic                  20.538  &lt;0.005
Loglogistic               23.023  &lt;0.005
3-Parameter Loglogistic    8.249       *  0.000
Johnson Transformation     0.622   0.106


ML Estimates of Distribution Parameters

Distribution             Location    Shape     Scale  Threshold
Normal*                  15.85790            8.69187
Box-Cox Transformation*   4.68271            1.59250
Lognormal*                2.56056            0.74667
3-Parameter Lognormal     3.37633            0.28606  -14.61547
Exponential                                 15.85790
2-Parameter Exponential                     15.81003    0.04787
Weibull                            1.86662  17.81083
3-Parameter Weibull                1.97275  18.54109   -0.58137
Smallest Extreme Value   20.39570            9.29332
Largest Extreme Value    11.74217            7.24790
Gamma                              2.61645   6.06084
3-Parameter Gamma                  5.10250   3.94816   -4.28763
Logistic                 15.31511            5.03370
Loglogistic               2.63670            0.38443
3-Parameter Loglogistic   3.22306            0.19638  -10.52426
Johnson Transformation*  -0.01235            1.01633

* Scale: Adjusted ML estimate
</code></pre>
",2013-11-06 15:13:06.043
58981,10060.0,2,,58964.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>The maximum by which X1 can increase is 0.30-0.001. Following your
  explanation, if I want to compare the max. value with the lowest
  value, does this mean that the average log(y) increases by 1.5/100 *
  0.3-0.001, i.e. around 45%?</p>
</blockquote>

<p>No, because the simple way of multiplying the coefficient by 100 and then interpreting as a factorial change only works when both the coefficient and the change in the independent variable are relatively small.</p>

<p>Consider the equation:</p>

<p>$\ln(y) = \alpha + 1.5X_1 + 0.8 X_2$</p>

<p>With one percent point increase:</p>

<p>$\ln(y') = \alpha + 1.5(X_1+0.01) + 0.8 X_2$</p>

<p>Subtracting for the difference:</p>

<p>$\ln(y') - \ln(y) = 1.5 \times 0.01$</p>

<p>$\ln(y'/y) = 0.015$</p>

<p>The factorial change is then:</p>

<p>$y'/y = 1.015113$,</p>

<p>which is about 1.5%. So, that approximation shortcut works.</p>

<hr>

<p>When the change is large, like your 0.299, the approximation can deviate:</p>

<p>$\ln(y'/y) = 1.5 \times 0.299$</p>

<p>The factorial change is then:</p>

<p>$y'/y = 1.565961$,</p>

<p>about 56.6% larger instead of the approximated 45%.</p>
",2013-11-06 15:13:40.960
58982,21485.0,1,58985.0,,,Caret package Varimp - feature selection question,<logistic><predictive-models><caret>,CC BY-SA 3.0,"<p>I decided to use RFE using the caret package for feature selection for a logistic regression model.</p>

<p>The documentation says the Varimp for linear model uses </p>

<p>the absolute value of the t-statistic for each model parameter is used.</p>

<p>Logistic regression is not a linear model or has any of the linear model assumptions. Does it make sense to use linear model assumptions to reduce variables for a logistic regression problem using the caret package? </p>
",2013-11-06 15:41:05.977
58983,7016.0,2,,58921.0,,,,CC BY-SA 3.0,"<p>From <a href=""http://en.wikipedia.org/wiki/Root-mean-square_deviation"" rel=""nofollow"">Wikipedia</a>,</p>

<blockquote>
  <p>RMSE is a frequently used measure of the differences between values
  predicted by a model or an estimator and the values actually observed.</p>
</blockquote>

<p>Further,</p>

<blockquote>
  <p>RMSE is a good measure of accuracy, but only to compare forecasting
  errors of different models for a particular variable and not between
  variables, as it is scale-dependent.</p>
</blockquote>

<p>Clearly, you can use RMSE for comparing the forecast error between different models (linear or nonlinear), as long as you are comparing models for a particular variable. I don't see any assumptions to consider.</p>
",2013-11-06 15:45:15.427
58984,3693.0,1,58991.0,,,Bernoulli Confidence Intervals for p very close to 0,<probability><confidence-interval><sample><small-sample><bernoulli-distribution>,CC BY-SA 3.0,"<p>Let's say I have the following observations from many Bernoulli distributions with different p (p1, p2, ..):</p>

<p>Observations from Distribution 1: 10 successes, 100,000 trials, p_hat = 0.0001 
Observations from Distribution 2: 0 successes, 100 trials, p_hat = 0
Observations from Distribution 3: 4 successes, 60,000 trials, p_hat = 0.00007</p>

<p>I want to order these distributions by their true probabilities of success and get rid of the ones that have low probability of success. However, because of the inherent nature of these distributions, the probability of success is so low, that if I use a standard Wald and Wilson confidence interval for Bernoulli distributions, the results don't make too much sense.</p>

<p>Is there a standard statistical way to deal with these types of problems? Or do I have to resort to some self defined heuristics to remove distributions with low probability of success? </p>
",2013-11-06 15:49:47.827
58985,2198.0,2,,58982.0,,,,CC BY-SA 4.0,"<p>We wouldn't be making those assumptions. The logistic regression model falls into a wider class called generalized linear models (as does linear regression).</p>

<p>The t-test discussed here is the generalized linear model t-statistic to test that the parameter is equal to zero.</p>
",2013-11-06 17:09:52.273
58986,5906.0,1,59038.0,,,Forecast accuracy measures for different forecast horizon h in R,<r><forecasting>,CC BY-SA 3.0,"<p>I have a yearly time series data, from 1980 to 2005. The data is splitted into an training sample and a out of sample; the out-of sample consists of the 6 most recent observations and the rest is considered for training sample. I need to fit a ETS model and compare different accuracy measures for different forecast step aheads h=1,2,3,4,5 and 6.  </p>

<p>Something like this:</p>

<pre><code>    h=1  h=2  h=3  h=4  h=5   h=6
</code></pre>

<p>MSE   ..   ..   ..   ..  ..    ....   ..   ..   ..  ..    ....   ..   ..   ..  ..    ..</p>

<p>MASE  ..   ..   ..   ..  ..    ......   ..   ..   ..  ..    ....   ..   ..   ..  ..      ..</p>

<p>The following code gives me the accuracy measures for h=6:</p>

<pre><code>trainx&lt;- window(x,end=1999.99)
testx&lt;- window(x,start=2000)
fit&lt;- ets(trainx)
accuracy(forecast(fit,h=6),testx)
</code></pre>

<p>The questions are:</p>

<ol>
<li><p>How can I calculate the accuracy measures for h=1,2,3,4,5 ? For instance, when h=2, I fit a model to training data and I produce the forecast that correspond to 2000 and 2001.  </p></li>
<li><p>Now, how should I produce the forecast for 2002 and 2003, etc?<br>
Should I suppose that the observations for the year 2000 an 2001 are known and then fit a new model (this time I need to add the observations of 2000 and 2001 to the training set), then, to produce the forecast for 2002 and 2003?</p></li>
</ol>
",2013-11-06 17:13:17.030
58987,2958.0,2,,58921.0,,,,CC BY-SA 3.0,"<ul>
<li>RMSE is certainly appropriate also for nonlinear models</li>
<li>However, the RMSE expressions I know actually calculate the <em>mean</em>, so no <code>-2</code> (looks like d.f. for linear model? - d.f. for nonlinear models would be different!)</li>
<li>In general, I'd not use the residuals for calculating RMSE but rather use independent test cases to avoid an optimistic bias.</li>
</ul>
",2013-11-06 17:14:02.080
58990,1636.0,2,,58949.0,,,,CC BY-SA 3.0,"<p>You seem to have some conceptual issues.</p>

<p>In the classical non-bayesian context (the fact that your are learning about bias, and your working example, suggest that this is your context) the parameter $\theta$ is ... a parameter, a number; which is perhaps unknown to us but which takes nonetheless some determined fixed value. In short: $\theta$ <strong>is not a random variable</strong>.</p>

<p>The estimator, instead, is (in general) a random variable. Because $\hat{\theta}=g(\{X\})$ where $g(\cdot)$ is some function and $\{X\}$ is a list of realizations ($X_1,X_2 \cdots.. X_n$) of a random variable. (Think for example, of the sample average $(X_1+X_2+\cdots + X_n)/n$) This is to say: in different ""experiments"" (trials) we'll get different values of the estimator $\hat{\theta}$ . But in all experiments the parameter $\theta$ will be the same.</p>

<p>That's why it makes sense to ask if $E(\hat{\theta})=\theta$ (because the left side is the expectation of a random variable, the right side is a constant). And, if the equation is valid (it might or not be, according to the estimator) the estimator is unbiased.</p>

<p>In your example, you're using $\hat{\theta} = \frac{X_1+X_2+ \cdots + X_n}{n}\frac{4}{3}$. The expectation of this is $E(\hat{\theta} )= \frac{n E(X)}{n} \frac{4}{3}$
Now, we need to compute $E(X)$ (it will be a function of $\theta$) and check if that gives $\theta$.</p>
",2013-11-06 17:34:45.527
58991,15827.0,2,,58984.0,,,,CC BY-SA 4.0,"<p>Confidence intervals are great, but</p>
<ul>
<li><p>With probabilities near zero, different takes on how to do it have to be considered. The question and discussion so far mention various possibilities. The paper by Brown and friends in <a href=""https://projecteuclid.org/journals/statistical-science/volume-16/issue-2/Interval-Estimation-for-a-Binomial-Proportion/10.1214/ss/1009213286.full"" rel=""nofollow noreferrer"">Statistical Science 2001</a>
remains the best guide I know for 21st century statisticians and data analysts.</p>
</li>
<li><p>With sample sizes this different, overlap of intervals is inevitable and a clear ordering a little elusive.</p>
</li>
<li><p>The leading evidence arguably remains the point estimates.</p>
</li>
</ul>
<p>Putting your cases in different order, of the point estimates 0.0001, 0.00007, 0, Stata gives for 95% confidence intervals:</p>
<pre> 
Exact          0.0000480         0.0001839
Agresti        0.0000515         0.0001869
Jeffreys       0.0000514         0.0001774
Wald           0.0000380         0.0001620
Wilson         0.0000543         0.0001841

Exact          0.0000182         0.0001707
Agresti        0.0000192         0.0001781
Jeffreys       0.0000225         0.0001585
Wald           0.0000013         0.0001320
Wilson         0.0000259         0.0001714

Exact          0.0000000         0.0362167
Agresti        0.0000000         0.0444121
Jeffreys       0.0000000         0.0247453
Wald           0.0000000         0.0000000
Wilson         0.0000000         0.0369935
</pre>
<p>Notes: &quot;Exact&quot; here means Clopper-Pearson. Stata is explicit that it clips at 0 (or 1).</p>
<p>Normally I would add a graph, but its main point would be that the intervals for the <span class=""math-container"">$n = 100$</span> sample are massively larger, and logarithmic scale is not appropriate here.</p>
<p>If the samples were from quite different populations, you would have to take all the samples seriously. Otherwise one possible conclusion is that the sample of <span class=""math-container"">$n = 100$</span> is far too small to take seriously compared with the other samples.</p>
",2013-11-06 18:34:54.673
58992,13165.0,1,,,,Fixed point iterations for expectation propagation using energy minimization,<bayesian><inference><graphical-model>,CC BY-SA 3.0,"<p><strong>EP Primal</strong>  </p>

<p>In <a href=""http://research.microsoft.com/en-us/um/people/minka/papers/ep/minka-ep-energy.pdf"" rel=""noreferrer"">1</a>, it is finding the EP iterations by solving a saddle-point problem on the energy function. First, the primal is claimed to be 
$$
\min_{\hat{p}_i} \max_{q} \left[  \sum_i \int_{\mathbf{y}} \hat{p}_i(\mathbf{y}) \log \frac{  \hat{p}_i(\mathbf{y})  }{ t_i(\mathbf{y})  p(\mathbf{y})  }  d\mathbf{y}â€Ž - â€Ž(n-1) \int_\mathbf{y} q_\theta(\mathbf{y})  \log \frac{q_\theta(\mathbf{y})}{p(\mathbf{y})}  d\mathbf{y} \right]â€Ž  
$$
â€Žwith the  local moment matching constraints 
$$
â€Ž\mathbb{E}_{ q_\theta(\mathbf{y}) }\left[ \phi(\mathbf{y})  \right] = \mathbb{E}_{  \hat{p}_i(\mathbf{y} )  }\left[ \phi(\mathbf{y})   \right]â€Ž, â€Ž\forall i  \quad \quad  â€Ž$$</p>

<p><strong>EP Dual</strong></p>

<p>The dual energy function is the following; 
$$
â€Ž\min_{\nu} \max_{\lambda} \left[  (n-1) \log \int_\mathbf{y}  p(\mathbf{y})  \exp \left( \nu^\top \phi(\mathbf{y}) \right) d\mathbf{y}â€Ž - â€Ž\sum_{i=1}^{n} \log  \int_\mathbf{y} \hat{t}_i(\mathbf{y}) p(\mathbf{y}) \exp \left( {\lambda_i}^\top  \phi (\mathbf{y}) \right) d\mathbf{y}   \right]â€Ž,
$$
$$
â€Ž(n-1) \nu = \sum_i \lambda_iâ€Ž.
$$</p>

<p><strong>EP fixed point iterations</strong></p>

<p>And using the dual energy function, we should be able to find the fixed point iterations: </p>

<p><em>Message elimination</em>: Choose a $\tilde{t}_i$  to do approximation withâ€Ž. â€Ž</p>

<p>â€ŽRemove the factor $\tilde{t}_i$ from approximationâ€Ž, â€Ž$\; q_\theta^{-i} = \displaystyle \frac{q_\theta}{ \tilde{t}_i }$ 
â€Ž
<em>Belief projection:</em> Project the approximate posteriorâ€Ž, â€Žwith $\tilde{t}_i$ replaced with $t_i$â€Ž, â€Žon the approximating familyâ€Ž,
â€Ž$$â€Ž
â€Žq^{new}_\theta(\mathbf{y}) = \text{proj}\left( \hat{p}_i(\mathbf{y}) \rightarrow q_\theta(\mathbf{y}) \right)â€Ž,
â€Ž$$â€Ž
â€Žwhereâ€Ž, 
â€Ž$$â€Ž
â€Ž\hat{p}_i(\mathbf{y}) = \frac{1}{Z} q_\theta^{-i}(\mathbf{y}) t_i(\mathbf{y})â€Ž,     â€Ž\; \;  Z = \int q_\theta^{-i}(\mathbf{y}) \times t_i(\mathbf{y}) d\mathbf{y}â€Ž
â€Ž$$â€Ž</p>

<p>$â€Ž
\tilde{t}_i = \arg \min_{\tilde{t}_i} \text{KL} \left( \displaystyle \frac{ t_i \prod_{j\neq i}  \tilde{t}_j  }{ \int t_i \prod_{j \neq i} \tilde{t}_j  } \parallel  q_\theta(\mathbf{y})  \right) 
$.  â€Ž</p>

<p>Message update: Compute the new approximating factorâ€Ž, 
â€Ž$$â€Ž
â€Ž\tilde{t}_i  = Z \frac{ q^{new}_\theta(\mathbf{y}) }{  q_\theta^{-i}(\mathbf{y})â€Ž }
â€Ž$$â€Ž</p>

<p>Here are the questions: </p>

<ol>
<li>I know how how to derive dual from primal, but it it not clear to me where the primal is coming from. </li>
<li>I don't see how can I find the EP iterations from Dual. Any idea?  </li>
</ol>
",2013-11-06 18:50:24.980
58993,17573.0,2,,58977.0,,,,CC BY-SA 3.0,"<p>Non-linear least squares solves $min_\beta \sum (y_i-f(x_i;\beta))^2$.  This is quadratic in $\beta$ if $f$ is linear in $\beta$.  Your $f$ is not linear in $\beta$, so the NLS objective function is not quadratic in $\beta$.  Of course, you don't need the function to be quadratic to guarantee convergence to a unique minimum, rather you need $min_\beta \sum (y_i-f(x_i;\beta))^2$ to be convex in $\beta$.  Presumably, with your $f$, the NLS objective function is not convex.  It doesn't look, to me, like the kind of $f$ which generates a convex objective function.  That's pretty much the explanation.  You can have lots of minima or one minimum.</p>

<p>If I were fitting the function that you are, I would use an entirely different approach.  I would not just blindly use NLS.  If you look carefully at your function, $f(x_i;\beta)=a*x_i^2exp(-bx_i)+c$ it is <em>almost</em> linear in the parameters.  If you fixed $b$ at some value, say 0.1, then you could fit $a$ and $c$ by OLS:
\begin{align}
y_i &amp;= a*x_i^2exp(-0.1x_i)+c \\
    &amp;= a*z_i+c
\end{align}
The variable $z_i$ is defined $z_i=x_i^2exp(-0.1x_i)$.  This means that, once you have picked $b$, the optimal value of $a=\widehat{Cov}(y,z)/\hat{V}(z)$ and the optimal value of $c=\overline{y}-a*\overline{z}$.</p>

<p>So what, right?  At the very least, this is how you should pick starting values for $a$ and $c$.  But, really, this reduces the search for optimal parameters to a one dimensional search over $b$.  With a modern computer, one dimensional searches are fast and easy.  If you have some idea of what reasonable values for $b$ are, then you can just define an interval $[b_{low},b_{high}]$ and grid search for the b which gives the lowest sum of squared errors.  Then use that $b$ and its associated optimal $a$ and $c$ to start NLS from.</p>

<p>Or, you could do something more sophisticated.  Suppose you are searching over $b$, using the optimal $a(b)$ and $c(b)$ from OLS.  Then the NLS objective function is $\sum \left(y_i - f(x_i;a(b),b,c(b))\right)^2$.  The envelope theorem makes the derivative of this very easy to calculate: 
\begin{align}
\frac{d}{d b} \sum \left(y_i - f(x_i;\beta)\right)^2 &amp;= \sum 2\left(y_i - f(x_i;\beta)\right)\frac{d}{d b}f(x_i;\beta)\\
&amp;= \sum 2\left(y_i - f(x_i;\beta)\right)(-abx_i^2exp(-bx_i))
\end{align}</p>

<p>So, you can easily write a function to calculate the NLS objective function for any given $b$ and you can easily write a function to calculate the derivative of the NLS objective function for any $b$.  These two ingredients are enough to get a optimizer going on your function.  Then, after you find the optimal $b$, just run NLS with that $b$ and its associated optimal $a$ and $c$.  It will converge in one iteration.</p>
",2013-11-06 18:55:39.330
58994,14402.0,1,,,,Cohen d for 2x2 interaction,<anova><interaction><effect-size><cohens-d>,CC BY-SA 3.0,"<p>There is a <a href=""https://stats.stackexchange.com/questions/23224/cohens-d-for-2x2-interaction"">nice answer</a> to this question, but it assumes that you have the ANOVA table available.</p>

<p>My problem is different.  Say I'm reading a paper describing a male vs female, disease vs control experiment, and I know the n's, means, and standard deviations for all four groups (healthy females, ... , diseased males), but I don't have the original data.</p>

<p>I estimate Cohen d for the sex:health interaction.  Numerator: (male_disease-female_disease)-(male_healthy-female_healthy). Denominator: I <a href=""http://en.wikipedia.org/wiki/Pooled_variance"" rel=""nofollow noreferrer"">pool the standard deviations</a>.</p>

<p>In simulations with perfect data (normally distributed, etc), my estimated Cohen d does not match the Cohen d calculated from eta-squared: $$2\sqrt\frac{\eta^2}{1-\eta^2}$$</p>

<p>I am sure there must be a closed-form estimator for Cohen's d for this case, but having looked all over google and online stats books I can't seem to find the answer. Apologies if I've missed something obvious.</p>
",2013-11-06 18:58:32.697
59004,23091.0,1,,,,Total Sum of Squares (TSS) - Exponential Regression,<regression><anova><mathematical-statistics><exponential-distribution><sums-of-squares>,CC BY-SA 3.0,"<p>The total sum of squares is expressed as below [1][2],</p>

<p>\begin{align}
\rm{TSS} &amp;= MSS + ESS
\end{align}</p>

<p>\begin{align}
\sum_{i=1}^n (y_{i}-\overline{y})^2 &amp;= \sum_{i=1}^n (\hat{y}_{i}-\overline{y})^2 + \sum_{i=1}^n (y_{i}-\hat{y}_{i})^2 
\end{align}</p>

<p>I have a set of exponentially distributed data (x,y) as below,</p>

<pre><code>x   y       
332 7.283650
342 7.231924
356 6.949199
369 7.360927
369 7.154024
315 7.379831
334 7.278457
339 7.217902
321 7.238676
300 7.282819
330 7.255710
329 7.126188
374 7.042991
353 7.292686
335 7.405174
360 7.196402
351 7.130031
357 7.218629
348 6.991577
327 7.326131
347 7.453576
335 7.292167
391 7.199726
351 7.310863
307 7.206269
349 7.125773
340 7.129408
341 7.093262
358 7.375157
306 7.516519
301 7.304112
342 7.133147
350 7.345347
333 7.546433
318 7.192559
321 7.142807
347 7.167319
327 7.197025
352 7.122761
343 7.194221
314 10.388952
332 7.425843
344 8.922770
367 7.209697
361 7.040914
309 7.236910
322 7.050262
321 7.974467
291 12.036381
264 10.103215
322 7.208139
313 8.114479
374 7.072074
343 7.376195
333 7.081941
356 7.061375
351 7.176979
349 7.145715
348 6.948888
299 13.043680
337 7.679382
331 7.025437
387 7.182068
349 7.174590
293 7.387724
335 7.240338
330 7.540512
321 9.081374
350 8.787224
270 12.606609
283 8.027543
336 7.113309
346 7.149870
317 10.554307
308 8.318993
317 7.115282
347 7.038421
309 7.059298
344 7.025853
339 7.194532
262 12.690118
291 9.464123
317 12.085718
335 7.581643
348 7.195571
231 8.631216
274 7.765591
298 12.136716
221 8.449553
190 11.299447
285 7.610830
250 9.075558
372 6.836608
312 9.058835
296 6.974543
332 7.381492
298 7.987658
312 7.734847
316 6.975374
227 13.573608
305 9.040347
280 8.783381
362 7.320523
311 6.930607
222 9.994258
299 10.843992
256 9.309050
294 9.304895
303 11.077899
181 14.128775
206 11.550493
293 8.223747
315 8.525272
289 10.906415
279 8.296662
259 9.490090
305 8.418497
267 9.286926
326 8.573466
320 7.391983
20  46.517206
18  46.796088
50  33.885254
10  53.022687
50  33.730700
46  33.914648
30  39.763380
50  34.076680
40  40.189545
66  27.821829
39  40.119954
39  39.077237
57  27.396287
29  46.103090
30  39.334099
29  40.155373
30  40.455963
47  33.197760
60  39.389356
29  40.129821
40  40.059608
38  39.861015
30  46.634160
19  45.749527
47  33.575315
68  33.705149
69  33.255718
69  33.722495
10  52.855150
18  46.515025
28  40.819288
28  39.971840
20  47.167931
28  41.367704
50  34.006985
49  33.528368
50  33.917452
39  39.867870
50  33.280126
40  39.621913
20  46.517206
18  46.796088
50  33.885254
10  53.022687
50  33.730700
46  33.914648
30  39.763380
50  34.076680
40  40.189545
66  27.821829
39  40.119954
39  39.077237
57  27.396287
29  46.103090
30  39.334099
29  40.155373
30  40.455963
47  33.197760
60  39.389356
29  40.129821
40  40.059608
38  39.861015
30  46.634160
19  45.749527
47  33.575315
68  33.705149
69  33.255718
69  33.722495
10  52.855150
18  46.515025
28  40.819288
28  39.971840
20  47.167931
28  41.367704
50  34.006985
49  33.528368
50  33.917452
39  39.867870
50  33.280126
40  39.621913
20  46.517206
18  46.539745
48  33.885254
10  53.022687
50  33.730700
46  33.592453
30  39.763380
50  34.076680
40  40.189545
64  27.515942
36  40.119954
38  39.089597
56  27.260741
28  46.103090
30  39.334099
28  40.155373
30  40.455963
46  33.043622
60  39.389356
28  40.129821
40  40.059608
36  39.724430
30  46.645377
18  45.749527
46  33.307547
66  33.456388
68  33.031573
68  33.714705
10  52.855150
18  46.253489
28  40.595767
28  39.724741
20  47.167931
28  41.117905
50  34.021215
48  33.528368
50  33.917452
38  39.867870
50  33.280126
40  39.621913
</code></pre>

<p>I am trying </p>

<p>a) to find the MSS, ESS and TSS values.<br>
 b) validate if the values I got are correct, satisfying TSS = MSS + ESS</p>

<h2>Approach 1</h2>

<p>[Best Fit approach [3], Without converting the exponential data to linear data]</p>

<p>\begin{align}
y_{i} = y,
x_{i} = x
\end{align}
\begin{align}
\overline{y}=23.62464471
\end{align}
\begin{align}
\hat{y}_{i}=Ae^{Bx_{i}}= 47.3826e^{-0.0055x_{i}}
\end{align}
where A=exp(a) and B=b. a and b obtained from eq.(3) and eq.(4) from [3]. The equations are reproduced below,</p>

<p>\begin{align}
a &amp;= (\sum_{i=1}^n ln\ y_{i} \sum_{i=1}^n x_{i}^2 - \sum_{i=1}^n x_{i} \sum_{i=1}^n x_{i}\ ln\ y_{i})\ /\ (n\ \sum_{i=1}^n x_{i}^2 - (\sum_{i=1}^n x_{i})^2)
\end{align}</p>

<p>\begin{align}
b &amp;= (n\ \sum_{i=1}^n x_{i}\ ln\ y_{i} - \sum_{i=1}^n x_{i} \sum_{i=1}^n ln\ y_{i})\ /\ (n\ \sum_{i=1}^n x_{i}^2 - (\sum_{i=1}^n x_{i})^2)
\end{align}</p>

<p>Calculating the values,</p>

<p>\begin{align}
MSS = \sum_{i=1}^n (\hat{y}_{i}-\overline{y})^2 = 55961.58373
\end{align}
\begin{align}
ESS = \sum_{i=1}^n (y_{i}-\hat{y}_{i})^2 = 1739.728157
\end{align}
\begin{align}
TSS = \sum_{i=1}^n (y_{i}-\overline{y})^2 &amp;= 62029.58951
\end{align}</p>

<p>Unfortunately MSS + ESS = 57701.31189 != TSS</p>

<h2>Approach 2</h2>

<p>[Best Fit approach [3], Converting the exponential data to linear data]</p>

<p>\begin{align}
y_{i} = ln y,
x_{i} = x
\end{align}
\begin{align}
\overline{y}=2.868328
\end{align}
\begin{align}
\hat{y}_{i}=a+{bx_{i}}= 3.858255231{-0.0055x_{i}}
\end{align}
where a and b obtained from eq.(3) and eq.(4) from [3]. The equations are reproduced below,</p>

<p>\begin{align}
a &amp;= (\sum_{i=1}^n ln\ y_{i} \sum_{i=1}^n x_{i}^2 - \sum_{i=1}^n x_{i} \sum_{i=1}^n x_{i}\ ln\ y_{i})\ /\ (n\ \sum_{i=1}^n x_{i}^2 - (\sum_{i=1}^n x_{i})^2)
\end{align}</p>

<p>\begin{align}
b &amp;= (n\ \sum_{i=1}^n x_{i}\ ln\ y_{i} - \sum_{i=1}^n x_{i} \sum_{i=1}^n ln\ y_{i})\ /\ (n\ \sum_{i=1}^n x_{i}^2 - (\sum_{i=1}^n x_{i})^2)
\end{align}</p>

<p>Calculating the values,</p>

<p>\begin{align}
MSS = \sum_{i=1}^n (\hat{y}_{i}-\overline{y})^2 = 150.6993684 
\end{align}
\begin{align}
ESS = \sum_{i=1}^n (y_{i}-\hat{y}_{i})^2 = 3.992417623
\end{align}
\begin{align}
TSS = \sum_{i=1}^n (y_{i}-\overline{y})^2 &amp;= 154.691786
\end{align}</p>

<p>Here we able to validate, MSS + ESS = 154.691786 = TSS</p>

<h2>Approach 3</h2>

<p>[Least Square Fit approach [3], Without converting the exponential data to linear data]</p>

<p>\begin{align}
y_{i} = y,
x_{i} = x
\end{align}
\begin{align}
\overline{y}=23.62464471
\end{align}
\begin{align}
\hat{y}_{i}=Ae^{Bx_{i}}= 48.6062e^{-0.0056x_{i}}
\end{align}
where A=exp(a) and B=b. a and b obtained from eq.(9) and eq.(10) from [3]. The equations are reproduced below,</p>

<p>\begin{align}
a &amp;= (\sum_{i=1}^n (x_{i}^2 y_{i}) \sum_{i=1}^n (y_{i}\ ln\ y_{i}) - \sum_{i=1}^n (x_{i} y_{i}) \sum_{i=1}^n (x_{i}y_{i}\ ln\ y_{i}))\ /\ ((\sum_{i=1}^n y_{i}\sum_{i=1}^n (x_{i}^2 y_{i}))-(\sum_{i=1}^n (x_{i} y_{i}))
\end{align}</p>

<p>\begin{align}
b &amp;= (\sum_{i=1}^n y_{i} \sum_{i=1}^n (x_{i} y_{i}\ ln\ y_{i}) - \sum_{i=1}^n (x_{i} y_{i}) \sum_{i=1}^n (y_{i}\ ln\ y_{i}))\ /\ ((\sum_{i=1}^n y_{i}\sum_{i=1}^n (x_{i}^2 y_{i}))-(\sum_{i=1}^n (x_{i} y_{i}))
\end{align}</p>

<p>Calculating the values,</p>

<p>\begin{align}
MSS = \sum_{i=1}^n (\hat{y}_{i}-\overline{y})^2 = 59282.06044
\end{align}
\begin{align}
ESS = \sum_{i=1}^n (y_{i}-\hat{y}_{i})^2 = 1622.208368
\end{align}
\begin{align}
TSS = \sum_{i=1}^n (y_{i}-\overline{y})^2 &amp;= 62029.58951
\end{align}</p>

<p>Unfortunately MSS + ESS = 60904.26881 != TSS</p>

<h2>Approach 4</h2>

<p>[Least Square Fit approach[3], Converting the exponential data to linear data]</p>

<p>\begin{align}
y_{i} = ln y,
x_{i} = x
\end{align}
\begin{align}
\overline{y}=2.868328
\end{align}
\begin{align}
\hat{y}_{i}=a+{bx_{i}}= 3.858255231{-0.0055x_{i}}
\end{align}
where a and b obtained from eq.(9) and eq.(10) from [3]. The equations are reproduced below,</p>

<p>\begin{align}
a &amp;= (\sum_{i=1}^n (x_{i}^2 y_{i}) \sum_{i=1}^n (y_{i}\ ln\ y_{i}) - \sum_{i=1}^n (x_{i} y_{i}) \sum_{i=1}^n (x_{i}y_{i}\ ln\ y_{i}))\ /\ ((\sum_{i=1}^n y_{i}\sum_{i=1}^n (x_{i}^2 y_{i}))-(\sum_{i=1}^n (x_{i} y_{i}))
\end{align}</p>

<p>\begin{align}
b &amp;= (\sum_{i=1}^n y_{i} \sum_{i=1}^n (x_{i} y_{i}\ ln\ y_{i}) - \sum_{i=1}^n (x_{i} y_{i}) \sum_{i=1}^n (y_{i}\ ln\ y_{i}))\ /\ ((\sum_{i=1}^n y_{i}\sum_{i=1}^n (x_{i}^2 y_{i}))-(\sum_{i=1}^n (x_{i} y_{i}))
\end{align}</p>

<p>Calculating the values,</p>

<p>\begin{align}
MSS = \sum_{i=1}^n (\hat{y}_{i}-\overline{y})^2 = 155.7545382 
\end{align}
\begin{align}
ESS = \sum_{i=1}^n (y_{i}-\hat{y}_{i})^2 = 4.053637091
\end{align}
\begin{align}
TSS = \sum_{i=1}^n (y_{i}-\overline{y})^2 &amp;= 154.691786
\end{align}</p>

<p>Unfortunately, MSS + ESS = 159.8081753 != TSS</p>

<p>Questions:</p>

<ol>
<li><p>Is the above equation is limited to linear data only?</p></li>
<li><p>How to calculate TSS and ESS for exponentially data without converting it to linear first? The TSS equation seems to be generic that could fit any type of data. </p></li>
</ol>

<p>Hope you can guide me on the right way to calculate TSS and ESS for exponential data.</p>

<pre><code>Reference:
[1] http://en.wikipedia.org/wiki/Explained_sum_of_squares
[2] http://www.originlab.com/forum/topic.asp?TOPIC_ID=4823 
[3] http://mathworld.wolfram.com/LeastSquaresFittingExponential.html
</code></pre>
",2013-11-06 20:50:43.547
58996,22483.0,1,,,,"Simpson's Paradox, Combining data across confounding variable when few values are missing",<missing-data><expectation-maximization><simpsons-paradox>,CC BY-SA 3.0,"<p>The statistical analysis of experimental data that I have to perform could be described as follows. Three drug treatments $D_1$, $D_2$ and $D_3$ were tested across three groups $G_1$, $G_2$ and $G_3$. For each group and treatment combination $D_i, G_k$ the data are exposure (no of patients being treated with drug) and effect (ratio of no of patients recovered from treatment to treated).<br>
I have to find a ordered list of treatments according to their effectiveness.</p>

<p>It requires combining the data taken from different groups. I know about Simpson's paradox and from the Bayesian belief networks (establishing the causal relationships) I know that groups (confounding or lurking variable) affect the exposure of the treatment and the effectiveness of the treatment, so I should use the partitioned data across different groups to come to any inference. The one way that I came across to remove this paradox is to break one of the relationships between the confounding variable and exposure or confounding variable and effectiveness. The relationship between the confounding variable and exposure could be broken if the exposure is same across groups or is proportional and that could be achieved through scaling exposure and effectiveness (multiplying by a factor that balances out the exposure across different groups). This works fine if I have all the data for each treatment, group combination.</p>

<p>But the data I have has some missing values: for a few $D_i, G_k$ I don't have their exposure and effectiveness. Should I use expectation maximization techniques here to find out the missing values and then use scaling while combining data? If I could how should I proceed to avoid getting spurious results?</p>

<p>The one assumption from the domain knowledge could be made that I could make here is that the groups should affect the effectiveness of every treatment the same way and the effect should be multiplicative,  </p>

<p>i.e. effect of drug for a group $D_i, G_k$ = effect of drug $D_i \times$ effect on group $G_k$ </p>

<p>What could be other ways to find the ordered list? </p>
",2013-11-06 19:07:01.127
58997,1895.0,2,,58979.0,,,,CC BY-SA 3.0,"<p><strong>The flaw in the argument is that the conditioning random variable is not well-defined</strong>.</p>

<p>The ambiguity lies in  how our friend peeking at the dice decides to report the information back to us. If we let $X_1$ and $X_2$ denote the random variables associated with the values of each of the dice, then it is certainly true that for each $k \in \{1,2,\ldots,6\}$,
$$
\mathbb P(X_1 + X_2 = 7 \mid X_1 = k \cup X_2 = k) = \frac{2}{11} \&gt;,
$$ 
independently of $k$.</p>

<p>However, the events $\{X_1 = k \cup X_2 = k\}$ are clearly not mutually exclusive, and so clearly we <em>cannot</em> claim
$$
\begin{align}
\mathbb P(X_1 + X_2 = 7) &amp;\stackrel{?}{=} \sum_{k=1}^6 \mathbb P(X_1 + X_2 = 7 \mid X_1 = k \cup X_2 = k) \mathbb P( X_1 = k \cup X_2 = k )  \cr
&amp;\stackrel{?}{=} \frac{2}{11} \sum_{k=1}^6  \mathbb P( X_1 = k \cup X_2 = k )  \cr
&amp;\stackrel{?}{=} \frac{2}{11}
\end{align}
$$</p>

<p>Formally, we need to properly define a random variable, say $K$, that encodes the knowledge imparted by our peeking friend.</p>

<p>Our peeking friend could always report the value of the left-most die, or the right-most, or the larger of the two. She could flip a coin and then report based on the coin flip, or employ any number of more complicated machinations.</p>

<p>But, once this process is specified, the apparent paradox vanishes.</p>

<p>Indeed, suppose that $K = X_1$. Then, we have
$$
\begin{align}
\mathbb P(X_1 + X_2 = 7) &amp;= \sum_{k=1}^6 \mathbb P(X_1+X_2 = 7, K=k) \cr
 &amp;= \sum_{k=1}^6 \mathbb P(X_1+X_2 = 7 \mid K=k) \mathbb P(K=k) \cr
 &amp;= \sum_{k=1}^6 \frac{1}{36} = \frac{1}{6} \&gt;.
\end{align}
$$</p>

<p>Similar arguments hold if we choose $K = X_2$ or $K = \max(X_1,X_2)$, etc.</p>
",2013-11-06 19:08:33.613
58998,11250.0,1,,,,Goodness-of-fit for very large sample sizes,<goodness-of-fit><large-data>,CC BY-SA 3.0,"<p>I collect very large samples (>1,000,000) of categorical data each day and want to see the data looks ""significantly"" different between days to detect errors in data collection.</p>

<p>I thought using a good-of-fit test (in particular, a G-test) would be a good fit (pun intended) for this. The expected distribution is given by the distribution of the previous day.</p>

<p>But, because my sample sizes are so large, the test has very high power and gives off many false positives. That is to say, even a very minor daily fluctuation will give a near-zero p-value.</p>

<p>I ended up multiplying my test statistic by some constant (0.001), which has the nice interpretation of sampling the data at that rate. <a href=""http://socquest.net/Q1/ResExQ1_2/ResExQ12.html"">This article</a> seems to agree with this approach. They say that:</p>

<blockquote>
  <p>Chi square is most reliable with samples of between roughly 100 to
  2500 people</p>
</blockquote>

<p>I'm looking for some more authoritative comments about this. Or perhaps some alternative solutions to false positives when running statistical tests on large data sets.</p>
",2013-11-06 19:17:37.127
58999,21057.0,1,,,,An inequality involving expectation,<expected-value><probability-inequalities>,CC BY-SA 3.0,"<p>Let $f,g$ be two pdfs, and suppose $X$ is a random variable that has pdf $f$. Is it necessarily true that $E[f(X)] \ge E[g(X)]$?</p>

<p>Although I doubt this will help, but I got this problem from studying the <a href=""http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">Kullback-Leibler divergence</a>, which is defined as $D(f,g) = E\left[\ln (f(X)/g(X))\right]$ (with $X$ having pdf $f$ like above). It can be shown using Jensen's inequality that $D(f,g) \ge 0$, which is equivalent to saying that $E[\ln(f(X))] \ge E[\ln(g(X))]$. But my question removes the $\ln$ and I am wondering whether it will still be true.</p>

<p>In fact, I hypothesize that not only is it true for the $\ln$ function, but it will be true for any function that is increasing (which includes the identity function, which is my question here). So as a bonus, maybe we could ask if $E[h(f(X))] \ge E[h(g(X))]$ where $h$ is any strictly increasing function on positive reals. It seems like a continuous form of the <a href=""http://en.wikipedia.org/wiki/Rearrangement_inequality"" rel=""nofollow noreferrer"">rearrangement inequality</a> to me.</p>
",2013-11-06 19:22:27.210
59000,23443.0,1,59001.0,,,Linear regression from data that don't represent a function,<regression><machine-learning><data-transformation><train>,CC BY-SA 3.0,"<p>I have $(x,\ y)$ pairs with a strongly suspected linear correlation. So I want to fit the ""best"" linear function in order to make predictions for unknown $x$'s. These pairs don't represent a function, i.e. there are many different values of $y$ for some $x$. I want to know if there is an issue with that considering this as a machine learning problem. Should I use some unique ""representative"" (average, maximum, minimum, most frequent, etc.) and convert the input data in a function or it is OK to work with the data as it is?</p>
",2013-11-06 19:30:31.207
59001,16464.0,2,,59000.0,,,,CC BY-SA 3.0,"<p>If you perform linear regression on your raw data using Ordinary Least Squares to estimate the model parameters you will get a model that estimates the conditional mean of $Y$ given the observed value of $X$. I.e. $\hat{f}(x)=E[Y|X=x].$ Since your model outputs the conditional expectation of the response it wouldn't be necessary to convert your $Y|X=x$ into a descriptive statistic prior to your analysis. </p>
",2013-11-06 19:53:44.297
59002,21804.0,2,,8681.0,,,,CC BY-SA 3.0,"<p>the following repositories hold a variety of datasets:</p>

<p><a href=""http://lib.stat.cmu.edu/"" rel=""nofollow noreferrer"">Statlib</a></p>

<p><a href=""http://archive.ics.uci.edu/ml/"" rel=""nofollow noreferrer"">UCI machine Learning Repository</a></p>

<p><a href=""http://www.kdnuggets.com/datasets/"" rel=""nofollow noreferrer"">KDNuggets Link List</a></p>

<p>and, as said before, many more at:
<a href=""https://stats.stackexchange.com/questions/7/locating-freely-available-data-samples"">this question.</a></p>
",2013-11-06 20:07:24.733
59022,5045.0,2,,59010.0,,,,CC BY-SA 3.0,"<p>Rob Hyndman outlines one approach <a href=""https://www.otexts.org/fpp/2/5/"" rel=""nofollow"">here</a> and <a href=""http://robjhyndman.com/hyndsight/crossvalidation/"" rel=""nofollow"">here with a survey reference</a>. Both are towards the end of the links. It seems pretty straight forward to implement in Python.</p>
",2013-11-07 00:29:07.917
59005,4656.0,2,,58979.0,,,,CC BY-SA 3.0,"<p>If $B$ is an event with the property that $P(B\mid D_i) = p$ for all events 
$\{D_1, D_2, \ldots\}$ in a countable <em>partition</em> of the sample space $\Omega$,
(that is, $D_i \cap D_j = \emptyset$ for all $i \neq j$ and 
$\bigcup_i D_i = \Omega$), then the
law of total probability tells us that
$$P(B) = \sum_i P(B\mid D_i)P(D_i) = p\sum_i P(D_i) = p.$$  However,
the law of total probability does <em>not</em> apply if the events $D_i$ are
not mutually exclusive (even though their union is still $\Omega$), and 
we cannot conclude that $P(B)$ equals the common value of $P(B\mid D_i)$.</p>

<p>Let $A_i$ denote the event that at least one of the dice shows the number $i$ and $B$ the event that the sum of the two numbers on the die is $7$.  We know that
$P(B) = \frac{1}{6}$ and that $P(A_i) = \frac{11}{36}$.  Also, 
$P(B\mid A_i) = \frac{2}{11}$.  Now, 
$A_1\cup A_2\cup A_3 \cup A_4\cup A_5\cup A_6$
is the entire sample space $\Omega$
but we <em>cannot</em> use the fact that $P(B\mid A_i)$
is the same for all choices of $i$ to conclude that $P(B) = \frac{2}{11}$ 
because the $A_i$ are <strong>not mutually exclusive events.</strong>
However, notice that regarded as a <em>multiset,</em>
$A_1\cup A_2\cup A_3 \cup A_4\cup A_5\cup A_6$ contains each outcome
$(i,j)$ exactly twice, once as a member of $A_i$ and again as a member of 
$A_j$.  Therefore,
$$\sum_{i=1}^6 P(B \mid A_i)P(A_i) 
= \sum_{i=1}^6 \frac{2}{11}\times\frac{11}{36} = \frac{1}{3} $$
which is exactly twice the value of $P(B)$. </p>
",2013-11-06 20:56:39.623
59006,23447.0,1,,,,Creating a Normalization Factor,<dataset><normalization><data-preprocessing>,CC BY-SA 3.0,"<p>I have a relatively simple problem that I can't seem to find a satisfactory solution to.  If I have three scales for three different sets of data.  One varies from [-5,5] the other from [1,10], and the last from [-10,10] what would an appropriate way to make a single scale for data from all three sets of data?  </p>
",2013-11-06 21:05:05.393
59007,22686.0,2,,58998.0,,,,CC BY-SA 3.0,"<p>One approach would be to make the goodness-of fit tests more meaningful by performing them on smaller blocks of data.</p>

<p>You could split your data from a given day into e.g. 1000 blocks of 1000 samples each, and run an individual goodness-of-fit test for each block, with the expected distribution given by the full dataset from the previous day. Keep the significance level for each individual test at the level you were using (e.g. $\alpha =  0.05$). Then look for significant departures of the total number of positive tests from the expected number of false positives (under the null hypothesis that there is no difference in the distributions, the total number of positive tests is binomially distributed, with parameter $\alpha$).</p>

<p>You could find a good block size to use by taking datasets from two days where you could assume the distribution was the same, and seeing what block size gives a frequency of positive tests that is roughly equal to $\alpha$ (i.e., what block size stops your test from reporting spurious differences).</p>
",2013-11-06 21:11:38.023
59008,22564.0,2,,58998.0,,,,CC BY-SA 3.0,"<p>The test is returning the correct result. The distributions are not the same from day to day. This is, of course,  no use to you. The issue you are facing has been long known. See: <a href=""http://www2.fiu.edu/~blissl/PearsonFisher.pdf"" rel=""noreferrer"">Karl Pearson and R. A. Fisher on Statistical Tests: A 1935 Exchange from Nature</a></p>

<p>Instead you could look back at previous data (either yours or from somewhere else) and get the distribution of day to day changes for each category. Then you check if the current change is likely to have occurred given that distribution. It is difficult to answer more specifically without knowing about the data and types of errors, but this approach seems more suited to your problem.</p>
",2013-11-06 21:49:32.890
59009,23450.0,2,,33598.0,,,,CC BY-SA 3.0,"<p>I think that Ho is proved and H1 disproved. In addition the GDP has no unit root.</p>

<p>is meaning the trend remains. There is no break point.</p>
",2013-11-06 21:54:18.113
59010,23451.0,1,,,,How do you do time series cross-validation using python?,<time-series><machine-learning><cross-validation><python>,CC BY-SA 3.0,"<p>Also, any tutorials/blogs available that you are aware of?</p>
",2013-11-06 22:21:51.163
59011,23348.0,2,,59006.0,,,,CC BY-SA 3.0,"<p>Assuming you have enough data points, I don't see why you couldn't just normalize everything into units of standard deviations. For each group, calculate the group's average value, and the group's standard deviation. Then for each data point in each group, subtract the corresponding group average, and then divide that result by the group's standard deviation.</p>

<p>For example, let's say that your first group (with the scale from -5 to 5) has a mean of 1.7, and a standard deviation of 2.3. For each observation in the group, subtract 1.7 from the observation, and then divide the result by 2.3. The result is the number of standard deviations above (if the result is positive) or below (if the result is negative) the observation is from the mean of the group.</p>

<p>All of your observations from each of the three groups will now be on a common scale, whose mean is zero, standard deviation is one, and the units of this new scale is standard deviations.</p>
",2013-11-06 22:23:08.643
59012,21652.0,1,,,,Comparing models with different number of predictors,<statistical-significance><multiple-regression><r-squared><model-comparison>,CC BY-SA 3.0,"<p>Given that the overall F-test of a multiple regression model has an F distribution, which depends on the number of predictors in the model, I understand why you cannot compare the F-statistics from models with different number of predictors.</p>

<p>However, the p-value of the F-statistics always has a uniform distribution between 0 and 1, and represents the probability that at least one Î²j â‰  0 under the null hypothesis. </p>

<p>Can I compare the F-statistics p-value from models with different number of predictors? If not, what are good alternatives? (R-squared?; adjusted R-squared?)</p>
",2013-11-06 22:32:59.840
59013,23452.0,1,,,,Calculating Standard deviation of percentages?,<standard-deviation>,CC BY-SA 3.0,"<p>I have the following data</p>

<p>X 1 2 3 4 5â€¦</p>

<p>Y 10 12 13 14 15â€¦</p>

<p>X/Y 10% 16% 23% etc.</p>

<p>How do I find the standard deviation of percentage (last line)? Can I treat the ratio as a normal distribution and apply regular SD formula?</p>
",2013-11-06 22:36:03.583
59023,,1,,,user30490,Deriving the distribution of the sum of censored variables,<distributions><censoring>,CC BY-SA 3.0,"<p>I want to be able to calculate the distribution of 
$$Y = \sum_{i=1}^n\max\{0,X_i\}$$
where the random variable $X_i\sim N(\mu_i,\sigma_i)$.  Is the calculation of $f_Y(y)$ possible and if so what is it?</p>
",2013-11-07 00:42:24.927
59024,12522.0,2,,59012.0,,,,CC BY-SA 3.0,"<p>I would NOT recommend the $R^2$ as this measure increases as the number of variables increases. In other words, the $R^2$ does not account for overfitting.</p>

<p>Among the options you mentioned the adjusted $R^2$ would be the best. If you take a look at the formula:</p>

<p>$R^2_{adj} = 1 - \frac{(1-R^2)\cdot(n-1)}{n-p-1}$</p>

<p>Since the number of parameters $p$ is in the denominator of the formula, the addition of variables that do not increase significantly the $R^2$ will penalize the $R^2_{adj}$.</p>

<p>A better approach to compare your models would be to use the Akaike Information Criterion:</p>

<p>$AIC_i = -2\cdot log(\mathcal{L}_i) + 2\cdot p_i$</p>

<p>where $\mathcal{L}_i$ is the likelihood of model $i$</p>

<p>You could obtain this very easy in R by using the AIC function:
<code> AIC(model1, model2) </code></p>
",2013-11-07 00:45:53.630
59014,11490.0,1,,,,How to get asymptotic covariance matrix when observed information matrix is singular,<optimization><covariance><asymptotics><uncertainty><fisher-information>,CC BY-SA 3.0,"<p>I'm fitting different models by Maximum Likelihood. To do this I'm using a stochastic version of Newton-Raphson algorithm, where both the gradient and the Hessian of the likelihood are estimated at each step. </p>

<p>In most cases I reach convergence, but then I often encounter the following problem: the estimate Hessian $\hat H$ at convergence is negative definite. This is a problem because I can't invert it to get an estimate of the observed Fisher information.</p>

<p>This happens because one or more parameters are weakly identified. On the other hand other parameters seem to be well identified and their corresponding entries in the estimated Hessian seem to make sense. Identifiability is difficult to assess beforehand for the dynamical models I'm trying to fit.</p>

<p>What I would like is an approach that (starting from the $\hat H$) points out what set of parameters are not identifiable and that gives variances for the remaining parameters.</p>

<p>I tried to do this by tilting the smallest eigenvalue of $\hat H$ in order to get a better conditioning number. The results seem arbitrary: depending on the conditioning number I want to achieve I get different variances for the parameters.</p>

<p>EDIT: 
This is a typical example of an Hessian I can end up with:</p>

<pre><code>H &lt;- matrix(c( 67.23586, 10.477815, 138.696877,
               10.47782, -3.238982,   2.592774,
              138.69688,  2.592774, 473.161347 ), 3, 3, byrow = TRUE)
</code></pre>

<p>You see that I have a negative entry in the main diagonal: the second parameter is weakly identifiable, while the other well identified. The other situation I often encounter is that of sub-group of parameters that are redundant - highly correlated.</p>
",2013-11-06 22:42:31.000
59015,20953.0,2,,59013.0,,,,CC BY-SA 3.0,"<p>The <em>classical</em> definition of the standard deviation estimate is independent from the theoretical distribution of the data, so you can perfectly apply it to a set of percentages $$ s = \sqrt{\frac{1}{n-1}\sum_{i=1}^n \left( z_i - \overline z\right)^2 } .$$</p>

<p>Depending on the distribution, you can have other estimates, though, with different properties.</p>
",2013-11-06 22:45:05.033
59016,20953.0,2,,59014.0,,,,CC BY-SA 3.0,"<p>It is less general than your problem, but I think this article may help: <a href=""http://www.jstor.org/stable/2347628"" rel=""nofollow"" title=""Ridge Estimators in Logistic Regression"">Ridge Estimators in Logistic Regression</a>. Basically, instead of maximizing the (log-)likelihood $L$, you maximize $$ L_\lambda(\beta) = L(\beta) - \lambda \|\beta\|_2^2 .$$ 
This regularization increases all the eigenvalues of the Hessian matrix, so it is not so far away from what you are doing right now.</p>
",2013-11-06 22:59:23.020
59017,594.0,2,,59013.0,,,,CC BY-SA 3.0,"<p>You should clarify your question to be clear whether you want the sample standard deviation of the collection of percentages or the estimated standard deviation of <em>each</em> percentage.</p>

<p>I'll discuss the second case. You should also clarify the question enough that I can remove some of the 'if's below.</p>

<p><em>If</em> the Y's are total counts of objects of which the X's are counts of some particular subset (such as X='number of people with red hair in a classroom', and Y='number of people in the classroom'), <em>and</em> if you can assume independence of occurrence of the characteristic being counted in X <em>and</em> if you can assume constant probability of occurrence of that characteristic, ...
then conditional on Y, you're in a binomial sampling situation and the estimated s.d. of the fraction X/Y is $\sqrt{\frac{1}{Y} \frac{X}{Y}(1-\frac{X}{Y})}$, which you can convert to percentage terms by multiplying by 100%.</p>
",2013-11-06 23:03:48.483
59018,23453.0,1,,,,Panel data: predictor variables with different observation times/frequencies,<time-series><panel-data><autocorrelation><assumptions><robust-standard-error>,CC BY-SA 4.0,"<p>I would like to estimate a standard logit (panel data):</p>

<p>$\text{logit}(P(y_{i,t}=1))= \alpha + \beta_1 x^1_{i,t} + \beta_2 x^2_{i,t} +\epsilon_{i,t}$. </p>

<p>The problem I am facing, however, is that $x^2$ is <em>yearly</em>, whereas $y$ and $x^1$ are <em>monthly</em>. Aggregating $x^1$ to the yearly level is not an option. So I naively created 12 observations for each $x^2$ in order to make it monthly and be able to match it to every $y$ and $x^1$. Yet I realize that this might cause problems:</p>

<ol>
<li>It may create serial correlation</li>
<li>It artificially inflates the number of observations and hence reduces the SE of $x^2$'s coefficient.</li>
<li>The SE of other coefs are not affected.</li>
<li>All coefficients remain unbiased.</li>
</ol>

<p>My <strong>first question</strong> is: is this <em>diagnosis</em> of the potential problems correct? Are there other problems I did not think about?</p>

<p>My <strong>second question</strong> is whether a solution to the problems outlined above is to use robust standard errors, clustering the observations by year.   </p>
",2013-11-06 23:39:53.850
59019,23455.0,1,,,,"Logistic regression - the model is significant in predicting the DV, yet the percent correct decreases",<logistic><predictive-models>,CC BY-SA 3.0,"<p>How can this be? My thoughts are that while the percent correct decreased from 80.8% to 80% with the model, perhaps the model is regarded to be a significant predictor due to the specificity having increased. Any thoughts?  </p>
",2013-11-06 23:43:23.503
59020,15827.0,2,,42513.0,,,,CC BY-SA 3.0,"<p>Previous answers make excellent points, but here is one fundamental to be added. </p>

<p>The mean is the centre of gravity of a distribution and so the pivot point of a histogram. It is where the distribution would balance. So, there is a reciprocal relation: not only can the mean help you think about a histogram, so also can a histogram help you think about the mean. This is even perhaps more helpful when a distribution is skewed and the mean of the distribution is not necessarily in the middle. </p>
",2013-11-07 00:05:51.277
59021,2217.0,1,,,,Determine separation between two modes from distribution,<distributions><density-function><density-estimation>,CC BY-SA 3.0,"<p>Iâ€™ve got a sample of pairwise distances between points in a 2D picture. Some of these points lie within the same object. Their distance to each other is thus smaller than some well-defined threshold (the objectâ€™s diameter). Points that lie in different objects (predominantly) have a pairwise distance greater than said threshold. Points that lie within the same object are however rare (&lt;10%).</p>

<p>I would like to determine this distance threshold empirically from my sample.</p>

<p>For â€œappropriateâ€ parameters (well, <a href=""https://stats.stackexchange.com/q/33918/3512"">herein lies the rub</a>, doesnâ€™t it?) the threshold is visible in the density plot:</p>

<p><img src=""https://i.stack.imgur.com/qqd75.png"" alt=""enter image description here""></p>

<p>The threshold is marked by the arrow. This is the <em>objectively right</em> cut-off for my application: it is the dip after the first tall plateau which corresponds to the distribution of the few points lying within the same object, and it corresponds to the object diameter that can be individually verified in the original picture, but not easily automatically deduced from my data.</p>

<p>Unfortunately, I have no idea how to determine it in an automated fashion. Even the <code>adjust</code> argument / bandwidth for the density function has been found by trial and error, and a different input data set Iâ€™ve tried requires a different bandwidth.</p>

<p>Is there <em>any</em> hope? Or should I just give up?</p>
",2013-11-07 00:15:21.987
59025,23457.0,2,,40859.0,,,,CC BY-SA 3.0,"<p>The split sample validation you proposed above has become less popular in many fields because of the issue Harrell mentions (unreliable out of bag estimates). I know Harrell has mentioned this in his textbook, but other references would be Steyerberg ""Clinical Prediction Models"" p301, James et al ""An Introduction to Statistical Learning"" p175. </p>

<p>In the biomedical field boostrap resampling has thus become the standard. This is implemented in Harrell's rms package and so fairly easy to implement. But you could really use any of the other resampling methods, bootstap has just become popular because of a Steyerberg article suggesting it is the most efficient of the resampling methods (""Internal validation of predictive models: efficiency of some procedures for logistic regression analysis"").</p>

<p>It is worth mention that the benefit of the rms package is that it easily enables you to include some of the variable selection in the bootstap (built in stepwise selection option). This can be awkward to achieve with most commercial packages. </p>

<p>I own sense is that the differences have been overemphasized. I usually get pretty reliable/consistent results irrespective of the method used. With large sample sizes the differences are really non-existent. </p>

<p>Bootstrap validation - as well as the other resampling methods - can also easily be done wrong. Often only some of the model building stages are included in the bootstrap giving inaccurate estimates. On the other hand it is fairly hard to mess up split sample validation. Given the face validity of split sampling - I know you didn't muck it up - I prefer split sample unless it is a very small dataset. It many cases the model building process is also complicated enough that it can't really be included in a resampling method. </p>

<p>If you want to publish in a biomedical journal though, and you aren't using a medicare size database, you will want to use a resampling method - likely bootstrapping. If the dataset is large, you can likely still get published with k-fold and save yourself some processing time. </p>
",2013-11-07 01:05:08.400
59026,13165.0,2,,22674.0,,,,CC BY-SA 3.0,"<p>You have many options! </p>

<ol>
<li>You can form Junction Tree of your tree and do the conventional Belief Popagation on the resulting graph. (See chapter ) </li>
<li>Choose one of the (loopy)BP family algorithms, based on your concern for time or complexity (This is a big family!). </li>
</ol>

<p>See [1] for a complete list of BP family inferences in MRFs. </p>

<p>[1] <a href=""http://www.nowozin.net/sebastian/papers/kappes2013energyminimization.pdf"" rel=""nofollow"">http://www.nowozin.net/sebastian/papers/kappes2013energyminimization.pdf</a></p>
",2013-11-07 01:22:49.347
59027,13165.0,2,,6788.0,,,,CC BY-SA 3.0,"<p>Let's say we want to bound empirical risk of a model. Given an arbitrary $(\epsilon, \delta)$, the sample complexity is $n(\epsilon, \delta)$ such that for $n\geq n(\epsilon, \delta)$ 
$$
P(|\hat{L}(f) - L(f) | \geq \epsilon ) \leq \delta
$$
The function $\delta(n,\epsilon)$ is a bound on the deviation from the main (unknown) risk (loss).</p>

<p>As a higher-level intuition: Sample complexity is the smallest number of samples for which we can make sure that we are close enough to the correct model. </p>
",2013-11-07 01:35:24.930
59028,23461.0,1,,,,Auto-Regressional & Moving Average Model Formula Properties,<time-series><predictive-models><arima><autoregressive>,CC BY-SA 3.0,"<p>I seeking help in understanding specific values underlying the formula's for the MA(p) model &amp; the AR(q) model. I am attempting to implement the models (building up to the combined ARIMA model) in the programming language Java.</p>

<p>I do not come from an overly mathematical (I'm fairly new to statistics at least) background so be gentle.</p>

<p>Here is the formula I am using for the AR(p) model:</p>

<p>$$X_t - Î¼ = Î²_1(X_{t-1} - Î¼) + ... +  Î²_p(X_{t-p} - Î¼) + Z_t$$</p>

<p>Where $X$ is the time series, $Î¼$ is the mean of the time series, $Î²$ is the auto-correlation coefficient at a specific lag, $p$ is the order of the model and $Z$ is white noise of mean $0$ and variance $Ïƒ^2$.</p>

<p>I'm fairly certain I have the above figured out, however the term ""$Z_t$"" confuses me. How would I implement this in code? I understand it is ""random"" however what are its ranges? Surely there must be a maximum and minimum of the term $Z_t$. Is it somehow based on the variance of the overall dataset? How is the ""$Z$"" value calculated on implementation exactly?</p>

<p>Here is the formula I am using for the MA(q) model:</p>

<p>$$X_t - Î¼ = Z_t - Î¸_1(Z_{t-1}) - ... - Î¸_q(Z_{t-q})$$</p>

<p>Where $X$ is again the time series dataset, $Î¼$ is the mean of the dataset, $Z$ is white noise with mean 0 and variance $Ïƒ^2$, $Î¸$ is the correlation coefficient at a specific lag and $q$ is the order of the model.</p>

<p>Again I the same issue as the above model in regards to the ""$Z$"" term. Also $Î¸$ is also the correlation coefficient of the dataset at various different lags, correct?</p>

<p>Any help on this matter would be extremely welcomed and if you have any questions I would more than happy to answer them.</p>

<p>Any use of examples alongside a full dataset i.e., X = (1,2,3,4,5,6,7) would be also extremely welcomed as it helps me understand the concept much more easily. Also please try to keep the explaination as idiot proof and contained as possible.</p>
",2013-11-07 02:27:04.507
59029,23460.0,1,59041.0,,,Significance test across multiple simulated experiments,<binomial-distribution><aggregation><hypergeometric-distribution><combining-p-values>,CC BY-SA 3.0,"<p>First time question on this site, so please bear with me, thank you:</p>

<p>I have 6 coin-flip-type experiments for which I can calculate 6 binomial p-values. I would now like to calculate the significance of observing at least 4 p-values &lt; 0.05 in six experiments total.</p>

<p>In one approach, I used Fisher's method (<a href=""http://en.wikipedia.org/wiki/Fishers_method"" rel=""nofollow"">http://en.wikipedia.org/wiki/Fishers_method</a>) to compound the p-values, but I wanted to add an additional test based on simulating the original coin-flip data.</p>

<p>To this end, I performed random coin-flips (P=0.5) for each of the 6 experiments; the total number of flips differs between these 6 experiments but is irrelevant. I then count how many times (out of 100 simulations) the binomial p-value &lt; 0.05. Simulating the original data 100 times, I get the following number of significant binomial p-values (""false positive hits"") from these 6 experiments:</p>

<p>12, 13, 9, 10, 7, 11</p>

<p>Or divided by 100 (= frequency of false positive hits in simulated experiments):</p>

<p>0.12, 0.13, 0.09, 0.1, 0.07, 0.11</p>

<p>How do I calculate the probability that 4 or more out of these 6 would be positive given these frequencies? I realize that for calculating the probability that 6/6 are positive, I would simply multiply 0.12 x 0.13 x 0.09 x 0.1 x 0.07 x 0.11. But for 1-5/6 it's more complicated. I'm leaning towards a hypergeometric test, since I have to draw 6 times and I <em>think</em> there's no replacement, but I want to double-check with you experts.
Thank you! </p>
",2013-11-07 02:40:18.163
59030,13165.0,2,,37748.0,,,,CC BY-SA 3.0,"<p>I liked your question! </p>

<p>The complexity of EM depends on the structure it is being applied on, and could be quite different from one problem to another one. But, just like any computational problem, it might be worthy of rigorous analysis to find and compare the complexity in different cases. </p>

<p>For some of the problems it is analyzed. For example in [1] see 3.1 . Some of these problems are proved to be NP-hard, whether with EM or not. See [3,4]. </p>

<ul>
<li>[1] <a href=""http://www.cs.nyu.edu/~roweis/papers/empca.pdf"" rel=""nofollow"">http://www.cs.nyu.edu/~roweis/papers/empca.pdf</a> </li>
<li>[2] <a href=""http://danroy.org/papers/SonRoy-NIPSWS-2009.pdf"" rel=""nofollow"">http://danroy.org/papers/SonRoy-NIPSWS-2009.pdf</a> </li>
<li>[3] <a href=""http://charlotte.ucsd.edu/~dasgupta/papers/kmeans.pdf"" rel=""nofollow"">http://charlotte.ucsd.edu/~dasgupta/papers/kmeans.pdf</a></li>
</ul>
",2013-11-07 03:05:16.890
59031,6608.0,1,,,,Good libraries for working with probabilistic graphical models?,<machine-learning><bayesian><references><graphical-model><software>,CC BY-SA 3.0,"<p>Could someone recommend some well-maintained and up-to-date libraries for working with probabilistic graphical models?</p>

<p>I noticed that there are some libraries for R listed <a href=""http://cran.r-project.org/web/views/gR.html"" rel=""nofollow"">here</a> and <a href=""http://staff.science.uva.nl/~jmooij1/libDAI/"" rel=""nofollow"">one for C++</a>, but are there some other good libraries in C++ or Python?</p>
",2013-11-07 03:21:23.427
59032,16588.0,2,,59019.0,,,,CC BY-SA 3.0,"<p>Say we have 2 observations, an event ($X_1=1$) and a non-event ($X_2=0$). Say our first model ""predicts"" probabilities  $P(X_1)=.55$ and $P(X_2)=.45$. If our decision rule for prediction of an event is based on a $.5$ probability threshold, then this first model is perfect (given our decision rule and the estimated probabilities). But notice the probabilities are close to $.5$ (close to that of getting tails on a coin flip!).</p>

<p>Now we change the model and the new probabilities are $P(X_1) =.95$ (much closer to observed $1$) and $P(X_2)=.51$ (not very different from the previous estimate, but importantly, crossing the threshold!). Considering the same decision rule for prediction, our predictions are not perfect anymore. However, considering the change in estimated probabilities, now we have a much better fitting model overall.</p>

<p>This brief discussion is based on the decision rule for prediction which I have assumed you are using. If your view of your model's performance is based on correct guesses, then the performance of your model depends on how you make guesses. Chances are, you could merely change the decision rule and see a completely different evaluation of your model's performance.</p>

<p>There are many ways to measure model performance.  Some more imperfect than others.</p>

<p>It's probably worth mentioning that such a decision rule is not an inherent element of logistic regression. Logistic regression models the probability of events, not dichotomous guesses about whether they occurred.</p>
",2013-11-07 03:28:18.260
59033,23414.0,2,,55436.0,,,,CC BY-SA 3.0,"<p>If by technique you mean classification method (logistic regression, classification tree, ...), you can use any of these methods to obtain the result you want. Each method usually has a build in cost-function that you can adjust to obtain your desired results. All of these methods end up as being equivalent to building an ROC curve and choosing which point on that curve you want. </p>

<p>Usually this is done automatically for you so you might not be aware that there is a tuning parameter that should be changed if you have an explicit cost function. </p>

<p>Thus logistic regression usually uses the classification split at 0.5 probability, but based on your cost function you can change this to obtain the desired sensitivity/specificity. Most standard statistical packages will contain a post-estimation command that you can use after you build your regression model to provide sen/spec/ppv/npv for all the possible cut-point from 0 to 1.</p>

<p>Perhaps it is worth noting that the cost function is rarely expressed at ""goal of 100%PPV"" but often as a ratio: the cost of false negatives/cost of false positives. In your case this ratio is low. The cost of a false positive >> cost false negative. But estimating this ratio you can give a more precise measure of your cost function. </p>

<p>Edit: what i have called the cost function above is usually called the ""utility function"" in texts.</p>
",2013-11-07 04:09:59.533
59034,23414.0,2,,51496.0,,,,CC BY-SA 3.0,"<p>The terminal nodes are mutually exclusive in that observations cannot be classified into more than one node.</p>

<p>They are not mutually exclusive in the sense that they can use the same characteristics/variables to classify observations. </p>

<p>The issue here may be that often in statistical packages like R the node number has little meaning (at least to me). So that terminal node 23 and 24 are not necessarily two branches from a single node - in fact they are likely to be branches of different nodes and they thus can use the same characteristics.</p>

<p>Hope that makes sense</p>
",2013-11-07 04:17:56.270
59035,594.0,2,,59023.0,,,,CC BY-SA 3.0,"<p>The distribution is of mixed type (it's neither discrete nor continuous) and has neither a density nor probability mass function. </p>

<p>However, it is a <a href=""http://en.wikipedia.org/wiki/Mixture_distribution#Finite_and_countable_mixtures"" rel=""nofollow"">mixture</a> of a (degenerate) discrete and a continuous distribution.</p>

<p>It should be possible to compute the CDF for small $n$; its just a sum over cases where 0,1,2,...,$n$ of the $X$ values are &lt;0; the number of terms to account for grows very rapidly with $n$, though.</p>

<p>$Y$ takes the value $0$ with probability $\prod_i F_{X_i}(0)$ and otherwise it's from a truncated-distribution based on the distribution of the $X's$. But it's going to be complicated to do exactly. </p>

<p>With middling to large $n$ I'd actually be inclined to use simulation on the continuous part (perhaps with smoothing by logspline density estimation). </p>

<p>With very large $n$ it may even be possible to come up with some kind of approximation.</p>

<p>What do you need it for?</p>
",2013-11-07 04:28:40.553
59036,7016.0,2,,23087.0,,,,CC BY-SA 3.0,"<p><strong>MA Model Estimation:</strong></p>

<p>Let us assume a series with 100 time points, and say this is characterized by MA(1) model with no intercept. Then the model is given by</p>

<p>$$y_t=\varepsilon_t-\theta\varepsilon_{t-1},\quad t=1,2,\cdots,100\quad (1)$$</p>

<p>The error term here is not observed. So to obtain this, <a href=""http://rads.stackoverflow.com/amzn/click/0130607746"">Box et al. Time Series Analysis: Forecasting and Control (3rd Edition)</a>, <strong>page 228</strong>, suggest that the error term is computed recursively by,</p>

<p>$$\varepsilon_t=y_t+\theta\varepsilon_{t-1}$$</p>

<p>So the error term for $t=1$ is,
$$\varepsilon_{1}=y_{1}+\theta\varepsilon_{0}$$
Now we cannot compute this without knowing the value of $\theta$. So to obtain this, we need to compute the Initial or Preliminary estimate of the model, refer to Box et al. of the said book, <strong>Section 6.3.2 page 202</strong> state that,</p>

<blockquote>
  <p>It has been shown that the first $q$ autocorrelations of MA($q$) process
  are nonzero and can be written in terms of the parameters of the model
  as
  $$\rho_k=\displaystyle\frac{-\theta_{k}+\theta_1\theta_{k+1}+\theta_2\theta_{k+2}+\cdots+\theta_{q-k}\theta_q}{1+\theta_1^2+\theta_2^2+\cdots+\theta_q^2}\quad k=1,2,\cdots, q$$ The expression above for$\rho_1,\rho_2\cdots,\rho_q$
  in terms $\theta_1,\theta_2,\cdots,\theta_q$, supplies $q$ equations
  in $q$ unknowns. Preliminary estimates of the $\theta$s can be
  obtained by substituting estimates $r_k$ for $\rho_k$ in above
  equation</p>
</blockquote>

<p>Note that $r_k$ is the estimated autocorrelation. There are more discussion in <strong>Section 6.3 - Initial Estimates for the Parameters</strong>, please read on that. Now, assuming we obtain the initial estimate $\theta=0.5$. Then,
$$\varepsilon_{1}=y_{1}+0.5\varepsilon_{0}$$
Now, another problem is we don't have value for $\varepsilon_0$ because $t$ starts at 1, and so we cannot compute $\varepsilon_1$. Luckily, there are two methods two obtain this,</p>

<ol>
<li>Conditional Likelihood</li>
<li>Unconditional Likelihood</li>
</ol>

<p>According to Box et al. <strong>Section 7.1.3 page 227</strong>, the values of $\varepsilon_0$ can be substituted to zero as an approximation if $n$ is moderate or large, this method is Conditional Likelihood. Otherwise, Unconditional Likelihood is used, wherein the value of $\varepsilon_0$ is obtain by back-forecasting, Box et al. recommend this method. Read more about back-forecasting at <strong>Section 7.1.4 page 231</strong>. </p>

<p>After obtaining the initial estimates and value of $\varepsilon_0$, then finally we can proceed with the recursive calculation of the error term. Then the final stage is to estimate the parameter of the model $(1)$, remember this is not the preliminary estimate anymore.</p>

<p>In estimating the parameter $\theta$, I use Nonlinear Estimation procedure, particularly the Levenberg-Marquardt algorithm, since MA models are nonlinear on its parameter.</p>

<p>Overall, I would highly recommend you to read <a href=""http://rads.stackoverflow.com/amzn/click/0130607746"">Box et al. Time Series Analysis: Forecasting and Control (3rd Edition)</a>.</p>
",2013-11-07 04:31:29.557
59037,7016.0,2,,59028.0,,,,CC BY-SA 3.0,"<p>The $z_t$ is the error term, and is obtain by </p>

<p>$$z_t = x_t-\hat{x}_t$$</p>

<p>Or the difference between the observed series ($x_t$) and the predicted ($\hat{x}_t$). To code this, you need to obtain the $\hat{x}_t$, which is just the expected value of $x_t$ or $E[x_t]$. So for example, AR($1$)
$$x_t=\beta x_{t-1}+z_t$$
where $\beta$ is the parameter, then
$$E[x_t]=\hat{x_t}=\beta x_{t-1},\quad \mathrm{since}\; E[z_t]=0$$
Thus,
$$z_t=x_t-\beta x_{t-1}.$$
For MA case, however, is quite complicated. Assuming we have MA(1),
$$x_t=z_t-\theta z_{t-1}$$
where $\theta$ is the parameter, the error term ($z_t$) is not observed, so to compute this we need to recursively calculate this using the formula,</p>

<p>$$z_t=x_t-\theta z_{t-1}$$</p>

<p><a href=""https://stats.stackexchange.com/questions/26024/moving-average-model-error-terms/74826#74826"">Here</a> is my answer on the steps of calculating the error term. </p>
",2013-11-07 05:13:27.493
59038,132.0,2,,58986.0,,,,CC BY-SA 3.0,"<p>It sounds like you need a rolling forecast origin (aka time series cross-validation). Here is an example.</p>

<pre><code>library(forecast)
x &lt;- ts(cumsum(rnorm(26)), start=1980)

k &lt;- 10 # minimum data length for fitting a model
n &lt;- length(x)
mae &lt;- matrix(NA,n-k-1,6)
st &lt;- tsp(x)[1] + k - 1

for(i in 1:(n-k-1))
{
  trainx &lt;- window(x, end=st+i-1)
  testx &lt;- window(x, start=st+i, end=st+i+5)
  fit &lt;- ets(trainx)
  fcast &lt;- forecast(fit, h=6)
  mae[i,1:length(testx)] &lt;- abs(fcast[['mean']]-testx)
}
mase &lt;- mae / mean(abs(diff(x)))
tab &lt;- rbind(colMeans(mae,na.rm=TRUE),colMeans(mase,na.rm=TRUE))
rownames(tab) &lt;- c(""MAE"",""MASE"")
colnames(tab) &lt;- paste(""h="",1:6,sep="""")
tab
</code></pre>
",2013-11-07 05:33:34.667
59039,12495.0,1,,,,Granger Causality Testing With Panel Data,<econometrics><panel-data><granger-causality><generalized-moments>,CC BY-SA 3.0,"<p>I'm trying to apply a <em>Granger Causality test</em> to panel data. I've found enough literature to understand that topic. However, I've been unable to find and <strong>R</strong> package to carry out that analysis. I'm wondering if anybody know whether there is around any package to deal with that. Thanks!</p>

<h3><em>I'm adding a potential solution, but new questions arose.</em></h3>

<p>The solution that I found is apply a <em>Granger Non- Causality test</em> and using <em>Generalized Method of Moments</em> (<em>GMM</em>). In the <a href=""http://www.fnu.zmaw.de/fileadmin/fnu-files/publication/working-papers/FNU47.pdf"" rel=""nofollow"">Erdil &amp; Yetkinerâ€™s (2004)</a> paper you can find a description of <em>Granger non-causality test</em> with panel data. To perform a <em>GMM</em> I used the <code>plm</code> package for <strong>R</strong>. If you have a look at its tutorial (<a href=""http://www.jstatsoft.org/v27/i02/paper"" rel=""nofollow"">Croissant &amp; Millo, 2008</a>), youâ€™ll see that the built-in function <code>pgmm</code> (page 17) removes the individual effect by the first difference and time dummies are included. The functionâ€™s summary also provides some tests to assess the model. For instance, to check serial autocorrelation in the residuals, Wald tests for coefficients and for time dummies and the Sargan test to evaluate if there is correlation between the instrumental variable and the residuals. Then I performed a Wald test (the first one in <a href=""http://www.fnu.zmaw.de/fileadmin/fnu-files/publication/working-papers/FNU47.pdf"" rel=""nofollow"">Erdil &amp; Yetkiner, 2004</a>) with the sum of squared residuals of an unrestricted model (<em>SSRu</em>) and of a restricted model (<em>SSRr</em>).
Now, my questions for the audience are:   </p>

<p>1) Do the time dummies remove the time effect?  I think so.</p>

<p>1.1) What if the time dummies aren't significant?</p>

<p>2) Therefore, if I got rid of the individuals and time effects, is the Wald test (<em>SSRr-SSRu</em>) be as a Wald test applied to an <code>OLS</code> model? I think so.
If so, Iâ€™m not sure about the freedom degrees. Letâ€™s see first the test suggested by Erdil &amp; Yetkiner (2004):</p>

<p>$$W=\frac{(SSRr-SSRu)/Np}{SSRu/[NT-N(1+p)-p]}$$</p>

<p>where <em>N</em>= number of individuals, <em>T</em>=years and <em>p</em>=number of lags. Note that they didnâ€™t get rid of individuals and time effects (at least that's what I understood).
Now, if I got rid of individuals and time effects the Wald test as applied to OLS models would be:
 $$W=\frac{(SSRr-SSRu)/m}{SSRu/ (n-k)}$$ where <em>m</em>= number of restrictions (number of coefficients that were removed from the unrestricted model to turn it restricted), <em>k</em>= total number of coefficients in the unrestricted model and <em>n</em>= number of observation.</p>

<p>More questions:</p>

<p>3) What is number of observation?</p>

<p>3.1) Is it the number of year or number of years*number of individuals? If it is number of years it seems reasonable, but if it is the product between years and individual it doesnâ€™t. For example, in my case I have 328 individual and 13 years, so it is 4264; therefore, the numerator in the Wald test will be very, very small and Iâ€™ll be rejecting everything.</p>

<p>Finally, </p>

<p>4) Am I right doing as I did?</p>

<p>Again, any help will be much appreciated </p>
",2013-11-07 06:18:12.660
59040,21119.0,1,59042.0,,,$E(x^k)$ under a Gaussian,<normal-distribution><expected-value>,CC BY-SA 3.0,"<p>What would be the expectation of $|x|^k$, where $x\sim\mathcal{N}(0,1)$, $k&gt;0$ and $k$ is not an integer?</p>
",2013-11-07 06:26:28.130
59041,594.0,2,,59029.0,,,,CC BY-SA 3.0,"<p>I have a number of additional comments to make regarding issues I have with what you're describing (which I will come back to), but first let's just deal with the simple question:</p>

<blockquote>
  <p>I would now like to calculate the significance of observing at least 4 p-values &lt; 0.05 in six experiments total.</p>
</blockquote>

<p>By 'significance' I assume you mean 'probability of ... under the null'.</p>

<p><em>Simple version</em>:</p>

<p>Imagine that the sample sizes were such that we could treat the distribution of p-values under the null as continuous. In that case they will be uniform under the null.</p>

<p>Then under $H_0$ each experiment has a 5% chance of giving a p-value below 0.05</p>

<p>The distribution of the number of experiments yielding p-values below 0.05 under the null is $\text{binomial}(6,0.05)$</p>

<p>Let $X\sim \text{binomial}(6,0.05)$. Then $P(X\geq 4) = 8.64\times 10^{-5}$</p>

<p><em>Less simple version</em>:</p>

<p>The distribution of p-values is discrete, and a significance level of exactly 0.05 won't typically be attainable. A more accurate answer in this case involves finding the largest possible p-value less than 0.05 (which depends on the exact sample size for each experiment), and then doing a similar calculation for that case. It will give a smaller probability than the one I just calculated. This involves some slightly more complicated calculation, but it's perfectly possible to do it exactly, without simulation. </p>

<p>(I don't think your simulation approach looks right, by the way, but since it's possible to do this question without worrying about that, I won't labor the point.)</p>
",2013-11-07 06:51:46.573
59042,594.0,2,,59040.0,,,,CC BY-SA 3.0,"<p>$E(x^k)$ can be worked out directly from the law of the unconscious statistician</p>

<p>$E(x^k) = \int_{-\infty}^\infty x^k \phi(x) dx$ where $\phi$ is the standard normal pdf</p>

<p>You may be able to make progress with a simple substitution.</p>

<p>See also:</p>

<p><a href=""http://mathworld.wolfram.com/NormalDistribution.html"" rel=""nofollow"">http://mathworld.wolfram.com/NormalDistribution.html</a> (this gives the numeric answers)</p>

<p>and</p>

<p><a href=""http://mathworld.wolfram.com/GaussianIntegral.html"" rel=""nofollow"">http://mathworld.wolfram.com/GaussianIntegral.html</a></p>

<hr>

<p>Responding to the updated question: we just follow my suggestion above (simple substitution).</p>

<p>\begin{eqnarray}
E(|x|^k) &amp;=&amp; \int_{-\infty}^\infty |x|^k \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx\\
         &amp;=&amp; 2\frac{1}{\sqrt{2\pi}} \int_{0}^\infty x^k e^{-x^2/2}dx
\end{eqnarray}</p>

<p>Let $u = x^2/2;\,$ so $ du  = x\,dx;\,x=(2u)^{1/2}$</p>

<p>\begin{eqnarray}
   &amp;=&amp; 2\frac{1}{\sqrt{2\pi}} \int_{0}^\infty (2u)^{\frac{k-1}{2}} e^{-u}du\\
   &amp;=&amp; \frac{2^{k/2}}{\sqrt{\pi}} \int_{0}^\infty u^{\frac{k-1}{2}} e^{-u}du\\
   &amp;=&amp; \frac{2^{k/2}}{\sqrt{\pi}} \Gamma\left(\frac{k+1}{2}\right)
\end{eqnarray}</p>

<p>In fact the second link above shows you how to it for the non-integer case - see eqns (9)-(12); there's nothing there requiring the power to be an integer.</p>
",2013-11-07 07:10:50.683
59043,23466.0,1,,,,Poisson process - calls arriving,<self-study><stochastic-processes>,CC BY-SA 3.0,"<p>Already posted on <a href=""https://math.stackexchange.com/q/555274/106061"">MSE</a>.  Had no answer, so will post here.</p>

<p>Assume the number of calls per hour arriving at an answering service follows a Poisson process with $\lambda = 4$.   </p>

<p>Question: If it is know that $8$ calls came in the first two hours.  What is the probability that exactly five arrived in the first hour?</p>

<p>Attempt: Isn't this just a combinatorial question?  So the answer is ${8 \choose 5}/2^8$</p>
",2013-11-07 08:19:01.863
59044,16665.0,1,59068.0,,,Analysis of interaction tables,<r><interaction>,CC BY-SA 3.0,"<p>I have two tables of interactions. One represents the number of times that a clownfish of a given species is found in an anemone of a given species. One represents the number of times that a clownfish of a given species is found in the same anemonae than a clownfish of another species.</p>

<pre><code>Table_anemone_fish

      Fish_a    Fish_b   Fish_c
An_A    23        56       12
An_B    12        5        5
An_C    23        10       68
</code></pre>

<p>Note: Summing this table gives the total number of anemones in my sample.</p>

<pre><code>Table_fish_fish

        Fish_a    Fish_b   Fish_c
Fish_a   NA         7        3
Fish_b    7        NA        1
Fish_c    3         1        NA
</code></pre>

<p>Note: Most of the times the fishes live with no other species and they are therefore not represented in this table. The frequency of each fish species can be obtain with <code>Table_anemone_fish</code></p>

<p>The questions I want to answer are:</p>

<ul>
<li>Is there differential preference of fish species in choosing their anemones?</li>
<li>Is there different preference of fish species in choosing with which other species to live with?</li>
</ul>

<p>To answer this post one can simply gives the kind of test to use and their philosophy or even better, one might give an example of how to analyze these data with R.</p>
",2013-11-07 08:43:39.197
59045,22049.0,1,,,,What is the impact of windowing function on time series,<time-series><spectral-analysis>,CC BY-SA 3.0,"<p>Greeting</p>

<p>I would like to know what is the impact of windowing functions like Hanning,... on a time series.</p>

<p>Is it possible to finde anomalies using windowing functions?</p>

<p><strong>EDIT</strong></p>

<p>I have a time series, which contains integer values (470,471,472,472,473,471,...) I try to find anomalies in this vector of value. As I have understood, with Wavelet transform , FFT, and moving average .</p>

<p>As I know any discontinuity between the 1st and last time sample (i.e., a jump in the value ) will contribute to the frequency spectrum in FFT, but we should use windowing to detect these anomalies. It means, I should calculate FFT for my time series and then I should run a window function like hanning or etc. on the result to find anomalies, Is it right?</p>
",2013-11-07 09:01:06.470
59046,23454.0,1,59077.0,,,Joint cdf of extreme values,<probability><cumulative-distribution-function><extreme-value><dice>,CC BY-SA 3.0,"<p>A die is rolled twice,</p>

<ul>
<li><p>$X_1$ : the minimum value to appear in the two rolls</p></li>
<li><p>$X_2$ : the maximum</p></li>
</ul>

<p>I would like to derive $\ F_{X_1,X_2}(x_1,x_2)$. </p>

<p>I know that that the CDF of $\ X_1 $     = $\ 1- [1-{F(x)]}  ^  n   $,   CDF  of $\ X_2  $   $\ = $    $ \  [{F(x)]}  ^  n $
 and     $\ F_{X_1,X_2}(x_1,x_2)  = P(X_1&lt;x_1|X_2&lt;x_2)F_{X_2}(x_2)  $</p>

<p>The solution seems to be : </p>

<p>$\ F_{X_1,X_2}(x_1,x_2) = 2F(\min[{x_1,x_2}])F(x_2) - F(\min[{x_1,x_2}])^2 $</p>

<p>I want to understand how such solution can be obtained.</p>
",2013-11-07 09:09:49.097
59047,11831.0,1,,,,Markov Decision Process and its generality,<stochastic-processes><markov-process><decision-theory>,CC BY-SA 3.0,"<p>My major is CS and I have a question about Markov decision process.</p>

<p>I have been reading a book, planning with markov decision process an AI perspective.
While reading it, I have a question regarding the definition of MDP and its generality.</p>

<p>The 2nd order Markov chain can be transformed into 1st order Markov chain. 
So any stochastic process that is depend on limited length history can be eventually
1st order Markov chain. </p>

<p>I think Markov Decision Process is the most general one when we consider discrete and finite state space. I don't know if there is non-Markovian decision process.</p>

<p>If we want to find optimal policy of an MDP with respect to maximum total expected utility (M.E.U) using value iteration, dynamic programming; if connectivity graph of MDP is acyclic, value iteration is same as Bellman-Ford shortest path finding algorithm.</p>

<p>I am curious about if there is something different decision process that can't be solved by dynamic programming when we are still finding an optimal policy w.r.t. M.E.U?</p>

<p>The question itself is confusing, but simply my question is MDP is the most general one if the state space is finite?</p>

<p>The additional question is when we are optimizing w.r.t. other metrics than M.E.U, the MDP can be solved by dynamic programming? </p>

<p>I hope someone suggest directions to explore this field.</p>

<p>Thanks in advance.</p>
",2013-11-07 09:26:30.730
59048,23227.0,1,59093.0,,,Binning By Equal-Width,<data-mining><binning>,CC BY-SA 4.0,"<p>I have a dataset:</p>
<pre><code>5, 10, 11, 13, 15, 35, 50 ,55, 72, 92, 204, 215
</code></pre>
<p>The formula for binning into equal-widths is this (as far as I know)
<span class=""math-container"">$$width = (max - min) / N$$</span></p>
<p>I think N is a number that divides the length of the list nicely. So in this case it is 3.
Therefore:</p>
<p>width = 70</p>
<p>How do I use that 70 to make the bins?</p>
",2013-11-07 09:31:01.880
59049,14873.0,1,,,,Fitting distribution to a given data,<distributions><nonparametric><fitting>,CC BY-SA 3.0,"<p>I have a loss data arising out of Operation risk for some particular bank. The standard procedure for arriving at the capital charge w.r.t. Operational risk needs I fit some continuous distribution to this loss data. </p>

<p>Normally, I am able to fit some standard distributions. Once the distribution is identified, the same distribution is used to simulate future loss amounts.</p>

<p>My question is assuming the data is such that I am just not able to fit any statistical distribution, how do I deal with data to simulate the loss amounts. Is there any <strong>non-parametric method</strong> available to deal with such situations.</p>

<p>Kindly give me some advice.</p>
",2013-11-07 10:06:02.833
59050,8361.0,2,,58976.0,,,,CC BY-SA 3.0,"<p>I am assuming you are seeking to classify the EEG data into one or more disease states e.g. seizure/non-seizure, pathological/non-pathological etc.</p>

<p>The best way to validate a classifier model for an application like this is to implement Leave One Out cross validation. </p>

<p>What I mean by this is to start with all data for patient 1 as the test set and all data for patients 2-15 as the training set and store the results. Next, set the data for patient 2 as the test set and the remainder as the training set. Do this for each patient's data in turn so that you have 15 classification results, one for each patient. The take the mean of these 15 values and you have an estimate for the classification performance of your classifier model on unseen data.</p>
",2013-11-07 10:12:41.690
59051,22968.0,1,,,,R Multiple Linear Regression; plotting results,<r><regression>,CC BY-SA 3.0,"<p>I'm trying to do some exploratory analysis on some weather. I'd like to do a multiple linear regression on my data and then plot the predicted value against the actual value. Here's where I've got so far: </p>

<pre><code>data&lt;-read.csv(""Amsterdam.csv"", header=TRUE)
data2&lt;-data[SolarAltitude&gt;0,]
data2.lm&lt;-lm(DirectRadiation ~ DryBulbTemperature + RelHum
   +AtmosphericPressure, data=data2)
data.data.frame(data2,fitted.value=fitted(data2.lm),residual=resid(data2.lm)) 
</code></pre>

<p>If you could help, I would be very grateful,</p>
",2013-11-07 10:15:24.467
59052,594.0,2,,59049.0,,,,CC BY-SA 3.0,"<p>You could resample the observed data (bootstrapping). </p>

<p>The problem with that is the real risk is in the extreme tail... and the sample doesn't really give you any information there (e.g. if you're interested in say a tail value at risk, for $\alpha = 0.005$ but you only have 
a couple of hundred observations, then you have no information 
about the behavior of the tail out that far.</p>

<p>On the other hand if you had many thousands of observations
it may not be a big problem.</p>

<p>You may get better benefit from investigating extreme value 
distributions (which do deal with the extreme tail), but that's not quite nonparametric.</p>
",2013-11-07 10:39:55.037
59053,10579.0,2,,59051.0,,,,CC BY-SA 3.0,"<p>The function <code>fitted</code> returns the fitted (predicted) values. To plot the fitted values against the actual values, you can use:</p>

<pre><code>plot(data2$DirectRadiation, fitted(data2.lm))
</code></pre>

<p>This will produce a plot with the actual values on the horizontal axis and the fitted values on the vertical axis.</p>

<p>If the above code doesn't work due to missing data, you can try one of the following approaches:</p>

<pre><code> plot(fitted(data2.lm) + residuals(data2.lm), fitted(data2.lm))

 plot(data2.lm$model[[1]], fitted(data2.lm))
</code></pre>
",2013-11-07 10:51:32.230
59054,23468.0,1,,,,Different results by using chi square test and logistic regression,<regression><logistic>,CC BY-SA 3.0,"<p>I am working on a problem to identify the risk factors to infection after operation. So there are risk factors such as age, pre existing condition, cause of infection etc. Since the dependent variable and most of variables are categorical data, I used logistic regression first, in which the factor A is not significant. </p>

<p>Just want to double confirm, I used chi square test just between the dependent variable and A, this time the P value is 0.03 which means they are correlated. Can anyone give me a hint? Does this imply the correlation between the factors? </p>
",2013-11-07 11:02:08.353
59070,23476.0,1,,,,Predicting absolute risk using cox regression,<r><survival><cox-model>,CC BY-SA 3.0,"<p>I am trying to use R to predict the absolute risk of developing adverse events in a cohort, and to compare that with the observed outcome. Should I use <code>survreg</code> or <code>coxph</code> to do this? Anyone kind enough to explain how to do this with R code? </p>

<p>The mean follow up period of my cohort is only up to 6 years, so am I able to predict the absolute risk for each individual at the end of the follow up period (including both censored and non censored data)?</p>
",2013-11-07 14:15:45.333
59055,12522.0,2,,59043.0,,,,CC BY-SA 3.0,"<p>Thinking this through, I believe this should be calculated with a binomial distribution with $n = 8$ and $p = 0.5$ as follows:</p>

<p>$P = \binom{8}{5} \cdot 0.5^{5} \cdot (1-0.5)^{3} $</p>

<p>Let me try to proof this:</p>

<p>Let</p>

<p>$X_1$ = number of calls that arrive in the first hour </p>

<p>$X_2$ = number of calls that arrive in the second hour</p>

<p>$X_3$ = number of calls that arrive in the two hours </p>

<p>What you want to calculate is the conditional probability of 5 calls arriving in the first hour given that 8 calls arrived in two hours:</p>

<p>$P(X_1 = 5 | X_3 = 8) = \frac {P[(X_1 = 5) \cap (X_3 = 8)]} {P(X_3 = 8)}$</p>

<p>This would be equivalent to : $\frac {P[(X_1 = 5) \cap (X_2 = 3)]} {P(X_3 = 8)}$, however now the events occur over non overlapping time frames which allow us to use the independent increment property of the poisson processes.</p>

<p>$\frac {P[(X_1 = 5) \cap (X_2 = 3)]} {P(X_3 = 8)} = \frac {P(X_1 = 5) \cdot P(X_2 = 3)]} {P(X_3 = 8)}$</p>

<p>$           =\frac {\left[\frac {e^{-4} \cdot 4^5} {5!} \right] \cdot \left[\frac {e^{-4} \cdot 4^3} {3!} \right]} {\frac {e^{-(4 \cdot 2)} \cdot {(4 \cdot 2)}^8} {8!}} $</p>

<p>$=\frac{8!} {5! \cdot 3!} \frac {(4^5) \cdot (4^3)} {8^8} $
$=\frac{8!} {5! \cdot 3!} \frac {(4^5) \cdot (4^3)} {(8^5) \cdot (8^3)} $
$=\frac{8!} {5! \cdot 3!} \cdot \left(\frac {4} {8}\right)^5 \cdot \left(\frac {4} {8}\right)^3$
$= \binom{8}{5} \cdot 0.5^{5} \cdot (0.5)^{3} $</p>
",2013-11-07 11:11:09.630
59056,21638.0,2,,59043.0,,,,CC BY-SA 3.0,"<p>Judging from the comments there appears to be a lot of confusion and lack of intuition over this question. A trivial Monte Carlo simulation will give (roughly) the correct answer that can be used to gauge the validity of the solutions. Here it is in <code>R</code>:</p>

<pre><code>&gt; firstHour &lt;- rpois(n=10000000,lambda=4) ; secondHour &lt;- rpois(n=10000000,lambda=4)
&gt; mean(firstHour[firstHour + secondHour == 8]==5)
[1] 0.2181712
</code></pre>

<p>Compare this to the OP's attempt:</p>

<pre><code>&gt; choose(8,5)/2^8
[1] 0.21875
</code></pre>

<p>Personally the combinatorial approach is not obvious to me. I would have followed @Orlando Mezquita's solution. As you can see, they arrive at the same answer.</p>
",2013-11-07 11:18:54.330
59057,503.0,2,,59054.0,,,,CC BY-SA 3.0,"<p>A few points:</p>

<p>1) The fact that the independent variables are categorical is irrelevant for the choice of logistic regression.</p>

<p>2) A significant chi-square value means the two variables are associated, but if both are categorical, it's not really correlation. If both variables have only 2 levels, there are analogues of correlation.</p>

<p>3) (your main question)  It appears you did a logistic regression with multiple independent variables and compared it to a chi-square test between only two variables (infection and A). These ask two different questions, so they get different answers. The first asks whether A affects the odds of infection after controlling for other variables. The second does not control for any other variables. If all the independent variables were completely unrelated, then I believe the effect sizes (odds ratios) would stay the same. However, this hardly ever happens in real life (except in some very controlled experiments).</p>

<p>4) Just as an aside, it is better to look at effect sizes, not just p values. </p>
",2013-11-07 11:31:18.837
59058,23470.0,1,,,,"Log-transformed variable is not significant, while variable itself is",<regression>,CC BY-SA 3.0,"<p>I'm doing a Logistic Regression on company level using the total assets to control for company size. Due to the skewedness of the data, I do a log-transformation of the asset data. While I get no significance for the log-transformed variable, the not transformed variable is highly significant (p &lt; 0.01). What could be a possible explanation of such a result? Does it mean that the relationship is not diminishing for extremely high and low values or does it show an outlier problem and should not be used for interpretation of the data.</p>
",2013-11-07 11:32:54.817
59059,13537.0,2,,58694.0,,,,CC BY-SA 3.0,"<p>Although the other answers have already addressed the question, I would like to add another powerful option that would solve most of the problems related to the distribution-assumptions: quantile regression.</p>

<p>Depending on the research interests, this method can be extremely powerful.</p>

<p>As someone has already said before, if you are merely interested in estimating the marginal mean (or any quantile) of your outcome then you don't need to worry about any assumption at all, as both quantile and ordinary regression methods perfectly estimate it.</p>

<p>If you are interested in inference, ordinary regression has a couple of problems with the distribution assumptions, whereas the quantile doesn't because it is distribution free.
It's true that you can try using mean regression and robust estimators, but personally I prefer quantile regression, which is by the way even more informative (because you can estimate the whole conditional distribution of the outcome instead of just one of its summary indicators, the mean).</p>

<p>If you are interested in both prediction and inference, then the quantile's property of invariance is quite handy.
For example, suppose you are working with probabilities, or rates (or any other ""bounded"" outcome).
With quantile regression you can transform the outcome Y so that it's transformation is not bounded (for example, using a logit or probit function), model logit(Y) and use the same model for predictions and inference.</p>

<p>With ordinary methods it's not so easy, because of Jensen's inequality: E(g(Y)) is never equal to g(E(Y)).</p>

<p>Therefore, you either use two models (one for the prediction, one for the association) or you must use other methods (beta regression, logit normal regression) that, however, have problems related to respectively parameter interpretation and distribution assumptions.</p>

<p>Finally, there can always be problems related to the linearity assumption or independent data. In the former, we can solve the problem by adding splines (which, though, complicate the interpretation of parameters).</p>

<p>For the latter, instead, mixed effect regression models could help us (if we have hierarchical or longitudinal data).</p>
",2013-11-07 11:40:08.050
59060,23473.0,2,,16998.0,,,,CC BY-SA 3.0,"<p><a href=""http://endb-consolidated.aihit.com/datasets.htm"" rel=""nofollow"">http://endb-consolidated.aihit.com/datasets.htm</a>  contains 10K companies with textual descriptions</p>
",2013-11-07 12:45:43.713
59061,20249.0,1,59067.0,,,Meaning of standard deviation of the mean difference,<standard-deviation><mean><group-differences>,CC BY-SA 3.0,"<p>I'm trying to understand the meaning of stating the standard deviation (SD) of the mean difference (MD) [or otherwise called the absolute mean difference]. This is for a paper I'm writing up where other examples also quote the SD of the MD as part of the analysis summary.</p>

<p>While I understand what the mean difference represents, i.e., being a ""measure of statistical dispersion"", I'm not sure of the utility of reporting the standard deviation of this value. I can kind of conceptualise what it means, but again the issue is the utility.</p>

<p>For reference, the analysis is of repeated measures data for equivalence. I will be reporting the correlation coefficient and also the statistical probability for equivalence (with equivalence range) using a Two One Sided T-test (TOST). Thus my question regarding the utility of the standard deviation of the mean difference.</p>

<p>Maybe I'm lacking in statistical knowledge and please feel free to point out a suitable reference for me to consult.</p>

<p>Thanks in advance.</p>

<p><strong>Update - example:</strong><br>
<em>Machine A</em> is the reference measurement system. <em>Machine B</em> is the new measurement system. A certain number of real life measurements are made with each machine (<em>N</em>) on a common object to understand how equivalent the machines are. Ideally they're identical - ""of course"" say the designers, and can even be expected - but this testing will be used to support the equivalence limit of +/- <em>x</em> units. (Given in reality it's not possible to say <em>A</em> is exactly equivalent to <em>B</em>.) So a paired measurement comparison.</p>

<p>When reporting these results it has been common to report the mean difference and the standard deviation of the mean difference, plus the correlation coefficient (Pearson's) with its p value.</p>

<p><strong>Update - comments:</strong><br>
1. This question does indeed refer to the standard deviation of the mean difference, not simply the difference.<br>
2. I well understand what the standard deviation means in terms of distributions. But my question is really ""what is the meaning of the standard deviation of the mean difference in relation to the quantity being measured?""<br>
3. If it's as simple as a guide to the variability of the difference, then so be it.<br>
4. If the result was reported as A = B +/- <em>x</em> units with a p &lt; 0.05 then I understand that this is different to quoting a standard deviation type number, but I still get it intuitively. I don't get the SD of a MD in regard to the original measurement entity nearly as well.</p>
",2013-11-07 13:03:59.207
59096,594.0,2,,59088.0,,,,CC BY-SA 3.0,"<p>Let's see what Dan Ma actually says in his blog. To quote:</p>

<blockquote>
  <p>There is uncertainty in the parameter $\theta$, reflecting the risk characteristic of the insured. Some insureds are poor risks (with large $\theta$) and some are good risks (with small $\theta$). Thus the parameter $\theta$ should be regarded as a random variable $\Theta$. The following is the conditional distribution of $N$ (conditional on $\Theta=\theta$):</p>
  
  <p>$$\displaystyle (15) \ \ \ \ \ P(N=n \lvert \Theta=\theta)=\frac{e^{-\theta} \ \theta^n}{n!} \ \ \ \ \ \ \ \ \ \ n=0,1,2,\cdots$$</p>
</blockquote>

<p>Aside from some small oddness in the wording, the gist of that is fine. The parameter of the Poisson ($\theta$ in the quoted discussion) represents the underlying rate of claims per unit time; that individuals are homogeneous, and have different 'riskiness' (different claim-rates) isn't controversial.</p>

<p>So why does he think that the distribution of the claim-rate is distributed as gamma? </p>

<p>Well, actually he doesn't say that he thinks that at all. </p>

<p>What he says is:</p>

<blockquote>
  <p>Suppose that $\Theta$ has a Gamma distribution with scale parameter $\alpha$ and shape parameter $\beta$. </p>
</blockquote>

<p>He's positing a circumstance -- discussing an assumption if you wish -- for which he then discusses the consequences. </p>

<p>He doesn't even assert anything about the plausibility of the assumption.</p>

<hr>

<p>Here's some things that might be reasonable to assert/suppose about the claim-rate distribution:</p>

<p>1) It's necessarily non-negative and may be taken to be continuous</p>

<p>2) we could expect that it would tend to be right-skew </p>

<p>3) We might not-too-unreasonably expect there to be a typical level (a mode), around which the bulk of the distribution lies, and that it tails off as we move further away (i.e. it might be reasonable to expect that it would be unimodal, at least to a first approximation)</p>

<p>That's about all we could say without collecting data. </p>

<p>The gamma at least doesn't break any of those suppositions/expectations, and so is likely to result in a more useful distribution than assuming homogeneity of claim-rate, but any number of other distributions satisfy those conditions. </p>

<p>So why gamma rather than lognormal say? Likely, a matter of convenience; the gamma works nicely with the Poisson - which even conditional on the individual underlying claim-frequency is itself another assumption that isn't actually true (though we can make some argument that the assumptions of claims having a Poisson process may not be too badly wrong, it's clear that they can't be exactly true).</p>

<p>There's no good reason to think it <em>is</em> gamma-distributed. </p>

<p>Indeed, I'll assert here and now that there's no real-world case where the claim rate <em>is</em> actually gamma distributed, in practice there will always be differences between the actual distribution of interest and some simple model for it; but that's true of essentially all our probability models. </p>

<p>They're <a href=""https://stats.stackexchange.com/a/730/805"">convenient fictions</a>, which may sometimes be not so badly inaccurate as to have some value. </p>

<blockquote>
  <p>Is there a way I can determine if my density is gamma distributed?</p>
</blockquote>

<p>Nothing will tell you it <em>is</em>; in fact you can be quite sure - even when it looks like an excellent description of the distribution - that the gamma is at best merely an approximation. You can use diagnostic displays (perhaps something like a Q-Q plot) to help check that it's not too far from gamma.</p>
",2013-11-07 18:52:34.853
59062,16174.0,2,,2156.0,,,,CC BY-SA 3.0,"<p><a href=""http://rads.stackoverflow.com/amzn/click/0471735787"" rel=""noreferrer"">Finding Groups in Data. An Introduction to Cluster Analysis</a> from professors Leonard Kaufman and Peter J. Rousseeuw.</p>

<p>I am reading the book and finding it very useful because:</p>

<ul>
<li>As stated by the authors in the preface:</li>
</ul>

<blockquote>
  <p>Our purpose was to write an applied book for the general user. We wanted to make cluster analysis available to people who do not necessarily have a strong mathematical or statistical background.</p>
</blockquote>

<ul>
<li><p>It provides theoretical content to understand the functions available in the <code>R</code> package <a href=""http://cran.r-project.org/web/packages/cluster/cluster.pdf"" rel=""noreferrer"">Cluster</a>.  </p></li>
<li><p>Chapters can be read individually according to the cluster method of interest.<br>
<sub> exception is chapter 3, which is built on chapter 2 </sub> </p></li>
</ul>

<p>The book's chapters are:</p>

<ol>
<li>Introduction</li>
<li>Partitioning Around Medoids (Program PAM).</li>
<li>Clustering Large Applications (Program CLARA).</li>
<li>Fuzzy Analysis (Program FUNNY).  </li>
<li>Agglomerative Nesting (Program AGNES).  </li>
<li>Divisive Analysis (Program DIANA).   </li>
<li>Monothetic Analysis (Program MONA).   </li>
</ol>

<hr>

<p>References:</p>

<p>Kaufman, L., &amp; Rousseeuw, P. J. (2005). Finding Groups in Data. An Introduction to Cluster Analysis (p. 342). John Wiley &amp; Sons Inc.</p>

<p>Maechler, M. (2013). Cluster Analysis Extended Rousseeuw et al. CRAN.</p>
",2013-11-07 13:09:48.110
59063,14900.0,1,60467.0,,,"Difference between ""in-sample"" and ""pseudo out-of-sample"" forecasts",<forecasting><model-comparison><out-of-sample><in-sample>,CC BY-SA 4.0,"<p>Is there an explicit difference between <em>in-sample forecasts</em> and <em>pseudo out-of-sample forecasts</em>. Both is meant in the context of evaluating and comparing forecasting models. </p>
",2013-11-07 13:11:37.360
59064,2161.0,1,59802.0,,,How can I look for a correlation between dependent variables in a repeated-measures/within-subjects design?,<r><correlation><repeated-measures>,CC BY-SA 3.0,"<p>I have a 2x3 within-subjects design, with two different dependent variables (DVs). I would like to know if the two DVs are correlated or not.</p>

<p>Here is an example of what the data look like, e.g. a data frame in R:</p>

<pre><code># Make some data:
set.seed(1154)

data &lt;- data.frame(id=gl(10, 6),
                   factor1=gl(2, 3, labels=c(""A"", ""B"")),
                   factor2=gl(3, 1),
                   DV1=rnorm(60),
                   DV2=rnorm(60))

head(data)

# Output:
#   id factor1 factor2          DV1         DV2
# 1  1       A       1  0.255579320  1.72318604
# 2  1       A       2  0.133878731 -0.32694875
# 3  1       A       3  0.890576655  0.14834580
# 4  1       B       1 -0.007879094 -0.07145311
# 5  1       B       2  0.976311664 -0.40686813
# 6  1       B       3  0.701357069 -0.50813556
</code></pre>

<p>In R, I could do something like:</p>

<pre><code>cor.test(data$DV1, data$DV2) # p = 0.048, significant
</code></pre>

<p>but there seem to be two problems with that.</p>

<p>First problem: the data are not independent (first 6 items from each DV come from the same participant in the experiment).</p>

<p>Second problem: we want to generalize from a sample to the population, so each id in the sample should just be included only once, e.g.:</p>

<pre><code># We want:
#  id  factor1  factor2  DV1  DV2
#  1      X        X     ...  ...
#  2      X        X     ...
#  3   ...

# So:
library(plyr)
data2 &lt;- ddply(data, .(id), summarize, mean.DV1=mean(DV1), mean.DV2=mean(DV2))
head(data2)

# Output:
#   id    mean.DV1    mean.DV2
# 1  1  0.49163739  0.09302105
# 2  2  0.66030997 -0.09344809
# 3  3  0.38277688  0.20274906
# 4  4 -0.35217913  0.57308528
# 5  5 -0.13470820  0.26663012
# 6  6 -0.04756911  0.60406950
</code></pre>

<p>Now I can look for a correlation and the responses are independent, but I have lost the individual factor levels.</p>

<pre><code>cor.test(data2$mean.DV1, data2$mean.DV2) # p = .15, not significant
</code></pre>

<p>What is the correct way to check for a correlation between the two dependent variables (using R)?</p>
",2013-11-07 13:20:25.830
59065,2666.0,2,,59058.0,,,,CC BY-SA 3.0,"<p>I don't prefer to think of this type of a problem as a ""choose between two transformations"" problem but rather I like to estimate the transformation as part of the modeling process.  In doing so we take care of multiplicities (possible inflated type I error) by having a parameter in the model for everything we think <em>might</em> be needed.  Consider expanding the predictor using a regression spline such as a restricted cubic spline (natural spline).  Test for association by doing a ""chunk"" test of all the parameters jointly that involve that predictor.  With a restricted cubic spline this test will have $k-1$ degrees of freedom where $k$ is the number of knots (join points), and using defaults for knots based on the marginal distribution of the predictor will work fine (this is how the R <code>rms</code> package's <code>rcs</code> function does it).  </p>

<p>Once you fit the spline model you can plot the predicted value vs. the predictor to learn about the estimated shape in the logistic model.</p>

<p>Concerning $Y$ make sure that it is truly all-or-nothing and does not represent a dichotomization.</p>
",2013-11-07 13:21:27.320
59066,20470.0,2,,1248.0,,,,CC BY-SA 3.0,"<p>I found this list of quotes from Gelman's famous <a href=""http://www.stat.columbia.edu/~gelman/book/"">Bayesian Data Analysis</a> book on this <a href=""http://www.stat.columbia.edu/~gelman/book/gelman_quotes.pdf"">link</a>. They are more like witty, stand-up one-liners but I enjoyed them a lot. Just a few below to whet your appetite:</p>

<blockquote>
  <p>1 ""As you know from teaching introductory statistics, 30 is infinity.""</p>
  
  <p>2 ""Suppose there's someone you want to get to know better, but you have
  to talk to all her friends too. They're like the nuisance parameters.""</p>
  
  <p>3 People don't go around introducing you to their ex-wives."" (on why model improvement doesn't make it into papers)</p>
</blockquote>
",2013-11-07 13:24:37.083
59067,11489.0,2,,59061.0,,,,CC BY-SA 3.0,"<p>Think of the difference like any other statistic that you are collecting. These differences are just some values that you have recorded. You calculate their mean and standard deviation to understand how they are spread (for example, in relation to 0) in a unit-independent fashion. </p>

<p>The usefulness of the SD is in its popularity -- if you tell me your mean and SD, I have a better understanding of the data than if you tell me the results of a TOST that I would have to look up first.</p>

<p>Also, I'm not sure how the difference and its SD relate to a correlation coefficient (I assume that you refer to the correlation between two variables for which you also calculate the pairwise differences). These are two very different things. You can have no correlation but a significant MD, or vice versa, or both, or none.</p>

<p>By the way, do you mean the standard deviation of the mean difference or standard deviation of the difference?</p>

<p><strong>Update</strong></p>

<p>OK, so what is the difference between SD of the difference and SD of the mean?</p>

<p>The former tells you something about how the measurements are spread; it is an estimator of the SD in the population. That is, when you do a single measurement in A and in B, how much will the difference A-B vary around its mean?</p>

<p>The latter tells us something about how well you were able to estimate the mean difference between the machines. This is why ""standard difference of the mean"" is sometimes referred to as ""standard error of the mean"". It depends on how many measurements you have performed: Since you divide by $\sqrt{n}$, the more measurements you have, the smaller the value of the SD of the mean difference will be.</p>

<p>SD of the difference will answer the question ""how much does the discrepancy between A and B vary (in reality) between measurements""?</p>

<p>SD of the mean difference will answer the question ""how confident are you about the mean difference you have measured""? (Then again, I think confidence intervals would be more appropriate.)</p>

<p>So depending on the context of your work, the latter might be more relevant for the reader. ""Oh"" - so the reviewer thinks - ""they found that the difference between A and B is x. Are they sure about that? What is the SD of the mean difference?""</p>

<p>There is also a second reason to include this value. You see, if reporting a certain statistic in a certain field is common, it is a dumb thing to <em>not</em> report it, because not reporting it raises questions in the reviewer's mind whether you are not hiding something. But you are free to comment on the usefulness of this value.</p>
",2013-11-07 13:28:05.523
59068,20613.0,2,,59044.0,,,,CC BY-SA 3.0,"<p>You could use a chi-square test for both ... although with the low sample sizes in the second example, you may want to use Fisher's exact test instead.</p>

<p>The Quick-R website gives some examples of how to code these analyses in R, <a href=""http://www.statmethods.net/stats/frequencies.html"" rel=""nofollow"">http://www.statmethods.net/stats/frequencies.html</a></p>
",2013-11-07 13:39:15.223
59069,2916.0,2,,47497.0,,,,CC BY-SA 3.0,"<p>From a recent update (2013, August 04<sup>th</sup>), <a href=""http://cran.r-project.org/web/packages/caret/caret.pdf"" rel=""nofollow"">caret</a> R package (see page 97) also supports Y-J power transformation.</p>
",2013-11-07 14:14:44.740
59071,23477.0,1,59072.0,,,Student's t-distribution,<excel><t-distribution>,CC BY-SA 3.0,"<p>I have two functions that provide an implementation of the t-distribution.</p>

<p>A webpage with a <a href=""http://www.math.ucla.edu/~tom/distributions/tDist.html"" rel=""nofollow"">Javascript algorithm</a>. The algorithm takes two input <strong><code>x-value</code></strong> and <code>degrees_of_freedom</code>.</p>

<p>Excel's <a href=""http://office.microsoft.com/en-us/excel-help/tinv-HP005209317.aspx"" rel=""nofollow"">TInv function</a>. The function also takes two inputs: <strong><code>probability</code></strong> and <code>degrees_of_freedom</code>.</p>

<p>Can you give me the relation between <code>probability</code> and <code>x-value</code>? I have doubts that the two methods calculates the same thing.</p>
",2013-11-07 14:39:23.997
59072,16043.0,2,,59071.0,,,,CC BY-SA 3.0,"<p>The first takes a value on the <code>x-value</code> and degrees of freedom and reports the amount of probability to the left of the <code>x-value</code>. If you're familiar with calculus, this is the equivalent of taking the integral of the Student's <em>t</em> probability density function over the interval $(-\infty,x]$. If you're familiar with statistics, this is the Student's <em>t</em> cumulative density function evaluated at $x$ for some degrees of freedom.</p>

<p>The second does the inverse, taking some probability and returning the corresponding <code>x-value</code>. In statistics, this is called the quantile function. </p>

<p>So the <em>mathematical</em> relationship between the two is the same as for any function and its inverse, with the <em>substantive</em> knowledge that they also have probability-based interpretations and statistical applications.</p>
",2013-11-07 14:43:37.290
59073,11353.0,2,,7965.0,,,,CC BY-SA 3.0,"<p>As CHL has already explained the use of center and scale to obtain standardized variables, I'll address collinearity:</p>

<p>There is good reason to reduce collinear variables when clustering.</p>

<p><strong>Curse of Dimensionality</strong></p>

<p>The more dimensions you use, the more likely you are to fall victim of <a href=""http://en.wikipedia.org/wiki/Curse_of_dimensionality"" rel=""nofollow"">Bellman's 'curse of dimensionality'</a>.  In brief, the greater the number of dimensions, the greater the total volume, and the greater the sparsity of your data within it.  (See the link for more detail.)</p>

<p><strong>Dimension Reduction --- manually by inspecting of pairwise collinearity...</strong></p>

<p>You mention that you have already reduced variables from some larger number down to 5 using pairwise collinearity measures.</p>

<p>While this will work, it is quite tedious, since in general you will have $n\choose 2$ number of pairs to check.  (So for example with 10 variables, you would have ${10 \choose 2} = 45$ different pairs to examine -- a few too many to do manually in my opinion!</p>

<p><strong>Dimension Reduction --- automatically using Principal Components Analysis (PCA)...</strong></p>

<p>One way to handle this automatically is to use the <a href=""http://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">PCA (principle components analysis) algorithm</a>.  The concept is more or less what you're doing manually -- ranking the variables by how much unique information each variable is contributing.</p>

<p>So you provide PCA your $n$-variable dataset as input, and PCA will rank order your variables according to the greatest variance each explains in the data -- essentially picking out the non-collinear variables.  </p>

<p>Depending on whether you want 2-D or 3-D clusters, you would use the top 2 or 3 variables from PCA.</p>

<p><strong>Principal Components in R</strong></p>

<p>The PCA algorithm is available (built-in) from R.  </p>

<p>Actually there are several functions in R that do principal components.  </p>

<p>I've had success with <code>prcomp()</code>.</p>

<p><strong>Standard Reference available free online</strong></p>

<p>One of the best references available is the classic:</p>

<p><a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"" rel=""nofollow"">Elements of Statistical Learning</a>, by Trevor Hastie, Robert Tibshirani, and Jermoe Friedman</p>

<p>The authors have graciously made the <a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"" rel=""nofollow"">entire book available (for free) as a PDF download</a> from their Stanford website.</p>

<p>There are excellent chapters on Clustering, Principal Comoonents, and a great section on the curse of dimensionality.</p>
",2013-11-07 14:55:57.120
59074,23478.0,1,,,,How to analyze intra-rater reliability with 31 raters who all rated 4 subjects twice,<repeated-measures><reliability><intraclass-correlation><repeatability>,CC BY-SA 3.0,"<p>I'm trying to analyze the intra-rater reliability of an occupational therapy assessment.</p>

<p>The assessment consists of 57 items, where some items are ratio data and some items are ordinal data.
In my study 31 raters rated 4 subjects (on video) twice (time 1 and time 2) with a six week interval.
So I have scores on each of the 57 items, for each subject, for each time and each rater.</p>

<pre><code>                Rater1Item 1, Rater1Item2 .... Rater1Item 57.... Rater31Item1....Rater31Item57

Subject1 Time1
Subject2 Time1
....
Subject3 Time2
Subject4 Time2
</code></pre>

<p>My first question is about the set-up of the database, I'm using SPSS. As showed above, is this the right set-up or do I need to change the set up and put the raters in the rows and the subjects in the colomns?</p>

<p>My second question, I found out I can use ICC and Bland &amp; Altman to analyse the intra-rater reliability. Are these two suitable or are there other statistical methods? 
The kappa statistic seems not very suitable since I have 31 raters, each rating 4 subjects. Or is there a way to deal with the kappa statistic and such a big database?</p>

<p>My third question is, which ICC can I use for the ratio data to calculate the intra-rater reliabiliy? I read different suggestions in the literature (ICC 1.1 or ICC 3.1)</p>

<p>My fourth question is, if I use Bland &amp; Altman the first thing I need to do is calculating the mean of time 1 and time 2. Do I need to do this for each item, each subject and each rater independently or can I calculate the mean for each item on each subject for all raters together? 
Or wouldn't it make sense to calculate this for the raters together, because the mean on both times can be the same, without have agreement (for example rater 1 scores less on time 2 and rater 2 scores higher on time 2, keeping the mean the same on both time 1 and time 2)</p>

<p>My last question, when I draw the Bland &amp; Altman plot can I do this for each item with all 4 subjects and all 31 raters, having 124 dots in my plot?</p>
",2013-11-07 14:57:29.900
59075,18198.0,1,59098.0,,,Numerical example for MLE for linear regression model,<matlab><maximum-likelihood>,CC BY-SA 3.0,"<p>I need to calculate the log-likelihood for a linear regression model in MATLAB. </p>

<p>Although the theoretical result is well know and given in several sources, I want to find a numerical example so that I can check my code is correct.</p>

<p>Can anyone point me to one?</p>

<p>I realize that the parameters are the same as OLS (at least asymptotically) but its the actual log-likelihood I need.</p>
",2013-11-07 15:23:21.270
59076,20470.0,2,,56928.0,,,,CC BY-SA 3.0,"<p><a href=""http://web.mit.edu/cocosci/Papers/devsci07_kempetal.pdf"" rel=""nofollow"">Here</a> is a really good 15-page article by Kemp at al. on Hierarchical Bayesian Modelling. It is more conceptual than mathematical though so I don't know whether it is good for your taste. Having said that, it definitely is less of a commitment than reading an entire book. </p>
",2013-11-07 15:59:52.550
59077,22686.0,2,,59046.0,,,,CC BY-SA 3.0,"<p>Let's work out the general answer: we have independent identically distributed random variables $X_i$, and a sample of $n$ instantiations of these RVs. I'll denote the sample maximum by $X_{max}$ and the sample minimum by $X_{min}$. Let the $X_i$ have the cdf $F$. Then we have the following:</p>

<p>$$ P(X_{max} \leq x \cap X_{min}&gt; y) = (F(x) - F(y))^n \textbf{1}_{\{x \geq y\}} $$</p>

<p>where the $\textbf{1}$ is the indicator function. This holds because for $x\geq y$, the probability that the sample maximum and minimum are in the interval $(y, x]$ is equal to the probability that each of the $n$ random variables is in this interval.
Then to get the joint density, use:</p>

<p>$$ P(X_{max} \leq x \cap X_{min}\leq y) = P(X_{max} \leq x) - P(X_{max} \leq x \cap X_{min}&gt; y) $$</p>

<p>$$P(X_{max} \leq x \cap X_{min}\leq y) = F(x)^n- (F(x) - F(y))^n \textbf{1}_{\{x \geq y\}}$$</p>

<p>If you work out what this is for $n=2$, and look at the cases $x \geq y$ and $x &lt; y$, this is equivalent to the expression you gave.</p>
",2013-11-07 16:12:12.497
59121,23138.0,1,,,,Are growth mixture models just Gaussian mixtures applied to coefficients of polynomials fitted to time-series data?,<clustering><repeated-measures><gaussian-mixture-distribution><growth-mixture-model>,CC BY-SA 3.0,"<p>Am I understanding correctly that growth mixture model is just Gaussian mixtures applied to coefficients of polynomials fitted to the time-series data?</p>

<p>For example, we have 1000  cases, with 3 measurements each. We fit, say, a quadratic equation to each which gives us 3 extra values per case (quadratic, linear, constant). Then we fit Gaussian mixture model to those 3 coefficients, which gives us clustering of the trajectories.</p>

<p>Is that it, or are growth mixtures something more involved?</p>
",2013-11-08 02:49:21.433
59078,371.0,1,,,,Estimating non-stationary Markov chain transition probabilities from data,<r><markov-chain>,CC BY-SA 3.0,"<p>I have some call centre outcome data: each person is called several times, and the result of the call is recorded as one of a few discrete outcomes (""no answer"", ""wrong number"", ""third party"", ""correct party"", etc.).  The goal of the analysis is to try and work out how many times to call before giving up.</p>

<p>My current plan is to consider the outcomes as a Markov chain.  If I assume that the data represents a stationary state, then it is easy to <a href=""https://stats.stackexchange.com/questions/14360/estimating-markov-chain-probabilities"">get the transition probabilities</a>.  The problem is, I don't believe that they are stationary: having ""no answer"" 20 times is a different situation to be in than having ""no answer"" once.</p>

<p>How do you calculate the transition probabilities if your data is non-stationary?</p>

<p>For bonus points, is there a function in R to do this?</p>
",2013-11-07 16:19:17.773
59079,23480.0,1,,,,Interpreting logit regression with both continuous and categorical variables,<regression><logistic><interaction><continuous-data><binary-data>,CC BY-SA 3.0,"<p>I have a logistic regression like this:</p>

<pre><code>Y = a1 + b1*(number positive scores) + b2*(number negative scores) + b3*Z 
       + b4*Z*(number positive scores) + b5*Z*(number negative scores) 
       + additional non-interaction terms
</code></pre>

<p><code>Y</code> is the probability of an outcome that takes on binary values (0,1).
<code>Z</code> is a continuous variable; I am trying to determine if it is significant
Number of positive and negative scores take on integer values between (0,5)</p>

<p>When I run the regression using <code>glm</code> in <code>matlab</code>, I find that all coefficients are significant except for b3; b1 and b4 are positive whereas b2 and b5 are negative. I would like to draw conclusions about whether Z is in fact a significant factor in the outcome variable via interactions b4 and b5, but I understand that in a logistic regression all coefficients and particularly interactions need to be evaluated in the context of specific values of the independent variable x. So this is what I have done so far:</p>

<p>Let's say bhat is the estimated vector of coefficients <code>b=(a1,b1,b2,b3,b4,b5...)</code> and <code>xhat</code> is the sample mean of <code>x=(num pos scores, num neg scores,Z,Z*num pos scores, Z*num neg scores)</code>. Also say that bi_sigma is the estimated standard error on the ith element of bhat and <code>xi_sigma</code> is the sample standard deviation of the ith element of x. Let's say <code>L(bx)</code> is the logistic cdf evaluated at bx.</p>

<p>Am I right to evaluate the odds ratios <code>exp(bhat*xhat-bi_sigma*xi_hat)</code> and <code>exp(bhat*xhat+bi_sigma*xi_hat)</code>, then determine whether each element bi_hat is significant based on whether the range of these odds ratios is strictly greater than or less than one? In other words, my thinking is that if the odds ratios don't include one then they significantly improve or decrease the odds. For instance an odds ratio range of (1.3,2) reflects a bi that improves the odds for mean levels of x. Yes?</p>

<p>Secondly, am I correct to evaluate the first differences, <code>L(bhat*xhat)-L(bhat*xhat-bi*xi_sigma)</code> and <code>L(bhat*xhat+bi*xi_sigma)-L(b*xhat)</code> as two measures of the size of the impact of each <code>xi</code>. Can I do this for the interaction also?</p>

<p>Thanks very much for any advice or help. If there are references that you would suggest for this, I would appreciate that too. All the examples I've found online involve binary or categorical variables only, and no continuous variables.</p>
",2013-11-07 16:23:32.583
59080,4656.0,2,,59046.0,,,,CC BY-SA 3.0,"<p>This is a problem in which working from first principles is better than specializing from poorly-understood general formulas. If $X$ and $Y$ are
the outcome on the two dice, then their joint mass function $p_{X,Y}(i,j)$ has value
$\frac{1}{36}$ for all $i,j \in \{1, 2, \dots, 6\}$ and so $p_{X_1,X_2}(i,j)$ has value $\frac{2}{36}$ if $1 \leq i &lt; j \leq 6$, and value $\frac{1}{36}$ if $1 \leq i=j \leq 6$. The CDF can be worked out from this, but writing it out explicitly
gives a long multi-case expression that I will leave to the OP to figure out.</p>
",2013-11-07 16:28:36.937
59081,22976.0,1,,,,"A problem of probabilities, calculation of 2 events",<probability><self-study>,CC BY-SA 3.0,"<p>I have this problem: A student must choose exactly two out of three electives: art, French, and mathematics. He chooses art with probability 5/8, French with probability 5/8, and art and French together with probability 1/4. What is the probability that he chooses mathematics? What is the probability that he chooses either art or French?</p>

<p>Now my rationale was this:</p>

<p>p(A and F)+p(F and M)+p(A and M)=1 something has to happen.
1/4+p(F and M)+p(A and M)=1</p>

<p>p(F and M)+p(A and M)=3/4</p>

<p>p(M)=p(F and M)+p(A and M)</p>

<p>p(M)=3/4</p>

<p>But how do I get p(A) or p(F).</p>

<p>Thanks.</p>
",2013-11-07 16:30:35.943
59082,14965.0,1,59085.0,,,Is $f(Y | X)$ in the same family as $f(Y)$?,<conditional-probability>,CC BY-SA 3.0,"<p>Is it the case that for random variables $Y$ and $X$, $f(Y | X)$ in the same family as $f(Y)$?</p>

<p>If so how can I prove it? If not are there any situations (some families of distributions) where they can be the same? Or is it related to the link function(s)?</p>

<p>Thanks</p>
",2013-11-07 16:45:23.733
59083,10546.0,1,,,,Cross validation for variable selection and coefficient shrinkage?,<cross-validation><feature-selection>,CC BY-SA 3.0,"<p>Is cross validation an appropriate technique for variable selection and regression coefficient shrinkage? </p>

<p>A former colleague of mine used 10-fold CV to compare the regression coefficients from the 10 training models. Variables were kicked out if the coefficients changed sign. The average coefficient values for the remaining variables was then used to estimate the coefficients of the final model.</p>

<p>Does this homebrewed methodology sound like a valid technique? </p>
",2013-11-07 17:03:24.260
59084,12544.0,1,,,,Probability of moving house (non-independence problem),<independence><demography>,CC BY-SA 3.0,"<p>We are analyzing a dataset where we are looking at the probability of individuals moving house, and the factors that predict moving.</p>

<p>The problem is that a lot of people are married, and (usually) if one moves house, one's spouse also moves house (although not everyone is married).  We have two options, neither of which seems satisfactory:</p>

<ol>
<li>Model at the household level: but then including predictors at the
individual level is hard (not everyone is married).  </li>
<li>Model at the  individual level, and use something which corrects for clustering. Doesn't  seem to account for the nearly complete clustering, and I don't feel like the stimates will be appropriate.</li>
</ol>

<p>In addition, the probability of moving is curious. If a couple moves, and a single person doesn't, was that a 0.5 probability of moving (because 1/2 households moved - as in (1) above, or 0.66, because 2/3 households moved, as in (2) above.</p>
",2013-11-07 17:15:26.773
59085,2873.0,2,,59082.0,,,,CC BY-SA 3.0,"<p>There are some specific cases where it is true, such as a bivariate normal, then $f(Y)$ and $f(Y|X)$ are both normal.</p>

<p>But consider the case where $f(Y|X)$ is normal (with mean depending on $X$) and $X$ follows a uniform distribution.  Then $f(Y)$ is not normally distributed.</p>

<p>There are also distributions where the marginals ($f(X)$ and $f(Y)$) are both $\text{uniform}(0,1)$, but there is a hole in the square where the probability is $0$, so the conditional $f(Y|X)$ would not be uniform and for some values of $X$ would be disjoint.</p>
",2013-11-07 17:15:51.580
59122,5480.0,1,,,,How to model the probability that a tweet is spam using normal distribution?,<probability>,CC BY-SA 3.0,"<p>I am trying to process a set of tweets, by using its length to determine the probability of spam. </p>

<p>The data looks like this, given 2000 tweets, n is the tweet length (number of words)</p>

<pre><code>when n = 1, relevant tweets = 0
when n = 2, relevant tweets = 10
when n = 3, relevant tweets = 20
when n = 4, relevant tweets = 100
when n = 5, relevant tweets = 200,
...
...
when n = 9, relevant tweets = 10
when n = 10, relevant tweets = 0.
</code></pre>

<p>My question is, how do I build a normal distribution based on this set of data, and calculate the probability of a tweet being spam, given n? </p>

<p>Thanks very much. This is of my own research work and it is not homework. Sorry if the questions are not asked in proper statistical way as I have limited knowledge on probability and normal distribution.</p>
",2013-11-08 03:02:58.640
59086,23414.0,2,,59079.0,,,,CC BY-SA 3.0,"<p>I'm reluctant to post because I don't quite understand what you're looking for here. Maybe this will jumpstart someone else to respond that has a better grasp of what you want.</p>

<p>""I would like to draw conclusions about whether Z is in fact a significant factor in the outcome variable via interactions b4 and b5, but I understand that in a logistic regression all coefficients and particularly interactions need to be evaluated in the context of specific values of the independent variable x.""</p>

<p>The statement above isn't quite correct. Z is significant, the p-value for the interaction term is significant so Z is significant. You don't need to go much further (unless you tested an unreasonable number of interactions/data size small). It does not make a difference that Z is not significant if the higher order interactions are significant.</p>

<p>To estimate the effect of Z on the outcome, this needs to be done in the context of specific values of the independent variable x. This is often best done graphically. </p>
",2013-11-07 17:26:20.267
59087,,2,,59081.0,user31668,,,CC BY-SA 3.0,"<p>You can directly solve for the smaller problem: $P(A \cup F) = P(A) + P(F) - P(A\cap F) = \frac{5}{8} + \frac{5}{8} - \frac{1}{4} = \frac{8}{8} = 1$! Which also makes logical sense, since he has to pick two of three electives. </p>
",2013-11-07 17:32:53.813
59088,658.0,1,59096.0,,,Intuition for Gamma-Poisson / Negative Binomial,<poisson-distribution><negative-binomial-distribution><gamma-distribution><intuition>,CC BY-SA 3.0,"<p>I have a data set which intuitively seems Poisson-like, but it's overdispered. So I'm investigating negative binomial.</p>

<p>From <a href=""https://stats.stackexchange.com/questions/6728/understanding-the-parameters-inside-the-negative-binomial-distribution"">this question</a> and <a href=""https://probabilityandstats.wordpress.com/tag/poisson-gamma-mixture/"" rel=""nofollow noreferrer"">this page</a> I understand one way of viewing this is to state that we are uncertain about the density parameter $\lambda$ in Poisson, and assume $\lambda \sim \text{Gamma}(\alpha, \beta)$, in which case we get a negative binomial.</p>

<p>I get the math here, but I don't understand why we would think that $\lambda$ is gamma-distributed. For example, the wordpress link says that when $\lambda$ represents ""likelihood to default on a loan"", then it would be gamma-distributed. But I don't really understand why.</p>

<p>Is there a way I can determine if my density is gamma distributed?</p>
",2013-11-07 17:36:38.483
59089,5199.0,1,,,,How to obtain the model behind a simulator?,<time-series><modeling><simulation>,CC BY-SA 3.0,"<p>I am looking for an useful statistical approach or analysis tool in order to understand the data obtained from an aeroelastic simulator of wind turbine dynamics.</p>

<p>In this case, the simulation provides data about the forces that a structure has to resist against the wind. Imagine a large structure such as a wind turbine which its base is loaded (with forces and moments) at each time step. This structural loads depend on the loads of previous instances of time (autocorrelated) and more variables such as wind intensity, angle of the attack, lift coefficients, relative speed, blade deflection of different sections of different blades, and other variables. The dataset is a time series of 12000 observations with 195 variables with many dependencies among them but some can be pointed out by an expert aeronautical engineer. She believes that some dependencies are non linear.</p>

<p><strong>The main goal is to quantify the grade of dependency, in other words, what variables influence more the response variable (loads at the tower base)</strong>. Later on I would also like to estimate the model using a data set from a different structure and see whether more influence variable are the same. I expect to see the same group of variables because the simulatorâ€™s algorithm is always the same.</p>

<p>I thought about some approaches but I want to know your experiences in similar problems.
- Generalized Linear Model or design of experiments, I have experience using them but I don't know how to capture the autocorrelation and the non-linearity.
- Multivariate Time series models, I do not have too much experience, but I believe they can capture the dependencies.
- What about more flexible and opaque models such as neural networks.</p>

<p>I am a recent graduate statistician and we hardly seen such complex problems during the degree. However, I am very confident with R and I'd love to know a good approach in order to start my research at CRAN and Wikipedia. My starting point is the <a href=""http://stat.ethz.ch/CRAN/web/views/TimeSeries.html"" rel=""nofollow"">Time Series View Task</a>.</p>
",2013-11-07 17:56:00.773
59090,23096.0,1,,,,Group effects where there is a single group,<r><multilevel-analysis><lme4-nlme>,CC BY-SA 3.0,"<p>Suppose I have a table of subjects and measurements as follows</p>

<pre>
subject measurement
s1        1
s1        2
s1        3
s1        4
s1        5
s1        6
s1        7
s1        8
s1        9
s1       10
s2       10
</pre>

<p>Now it is unlikely that the true value of s2 is 10 as observed. It is much more likely to be closer to the mean of the s1 measurements. How do I discover the true value? </p>

<p>I've discovered that if I construct the following table:</p>

<pre>
group subject measurement
g1  s1  1
g1  s1  2
g1  s1  3
g1  s1  4
g1  s1  5
g1  s1  6
g1  s1  7
g1  s1  8
g1  s1  9
g1  s1  10
g1  s2  10
g2  s1  1
g2  s1  2
g2  s1  3
g2  s1  4
g2  s1  5
g2  s1  6
g2  s1  7
g2  s1  8
g2  s1  9
g2  s1  10
g2  s2  10
</pre>

<p>and run the following R command </p>

<pre> fit = lmer(measurement ~ 1 + (subject|group)) </pre> 

<p>I get a plausible answer. Is it correct? Is there a better way? In R? I had to double the table up to coerce R to solve the problem as it doesn't seem to like a single group.</p>
",2013-11-07 17:59:00.433
59091,23414.0,2,,59058.0,,,,CC BY-SA 3.0,"<p>A. 
There are a number of textbook and statisticians that use this method of plugging in different transformations of a variable and using the p-values to provide evidence to support a conclusion about the nature of the relationship between the variable and the outcome. It seems appealing. </p>

<p>The Vittinghoff book ""Regression Methods in Biostatistics"" has a rather long section where they plug in a predictor as a categorical or continuous variable or both and discuss the p-values. </p>

<p>In practice I haven't found this approach meaningful. If you're doing exploratory analysis graphical methods including residual plots are usually more helpful. In this case I would be reluctant to make any assumption based on the p-value but would consider different graphical approaches.</p>

<p>B. 
There are a number of common reasons why you might log transform a variable: (1) you think you should but not sure why (2) you want to linearize the relationship between x &amp; y (3) youâ€™re trying to address that your data is clumped/with outliers.</p>

<p>Youâ€™ll get better answers to your question if youâ€™re explicit about your reasons for transformation. Most I think will assume you are motivates by (2) since this is perhaps the most valid reason, but from the question is seems that you might be motivated by (3) which is likely the most common reason.</p>
",2013-11-07 17:59:08.067
59092,22564.0,1,,,,How do I combine multiple prior components and a likelihood?,<prior><likelihood><decision-theory>,CC BY-SA 3.0,"<p>Lets imagine I am comparing two groups of animals (treatment/control). There is previous data from cell cultures indicating the treatment should have a positive effect. This gives me ""prior component 1"". There are also two previous studies very similar to my own. One of them had an effect of 5 +/- 1 (prior component 2), the other of 1 +/- 2 (prior component 3). I feel the cell culture data is highly convincing, and that the prior component 3 is not such a reliable study. So I choose some weights of 3,1, and .5 for each and multiply. </p>

<p>1) To calculate the ""overall prior"" do I simply add these together as shown in the lower right panel? </p>

<p>2) Am I supposed to normalize these components before adding them?</p>

<p><img src=""https://i.stack.imgur.com/U45TF.png"" alt=""enter image description here""></p>

<p>I then calculate a likelihood function for my current data as shown in the upper panel. </p>

<p><img src=""https://i.stack.imgur.com/xSnyg.png"" alt=""enter image description here""></p>

<p>3) How do I combine this information with the prior information shown in the first figure? For the lower panel I simply multiplied overall prior*likelihood.</p>

<p>4) I then want to make a decision based on this outcome. If I believe the effect is between -1 and 1 then I will stop studying the drug. If the effect is &lt; -1 then I would perform new study A, if the effect is > 1 I will perform new study B. </p>

<p>5) Obviously there are a number of ways of choosing a decision (% density between -1 and 1, etc) Is there a best choice?</p>

<p>6) I feel I am doing something incorrectly, but maybe not. Is there a name for what I am trying to accomplish?</p>

<p><strong>Edit</strong>:</p>

<p>If it helps I am trying to use the framework proposed by Richard Royall:</p>

<p>1) The likelihood function tells me ""how to interpret this body of observations as evidence""</p>

<p>2) The likelihood function + priors tells be ""what I should believe""</p>

<p>3) The likelihood function + priors +cost/benefit determines """"what I should do"".</p>

<p>Royall R (1997) <a href=""http://www.sortie-nd.org/lme/Bayesian%20methods%20in%20ecology/Royall_2004_Likelihood_Paradigm.pdf"" rel=""noreferrer"">Statistical evidence: a likelihood paradigm</a> (Chapman &amp; Hall/CRC)</p>

<p>While the priors used here are subjective/nebulous they are built out of simple building blocks (uniform and normal distributions) that mathematically unsophisticated researchers can understand quickly. I think they convey my thought processes as a researcher well. Others may of course know of different background information. They should be able to build their own ""compound prior"" which may lead to a different decision than mine, but we should always agree on the likelihood function.</p>

<p>This approach (if implemented correctly, which I am not sure I am doing here), appears to me to model the actual thought processes of researchers and thus be suitable for scientific inference. The steps map to the common sections found in scientific papers. The priors are the introduction, the likelihood is the results, and the posterior probability is the discussion.</p>

<p><strong>R code:</strong></p>

<pre><code>#Generate Priors
x&lt;-seq(-10,10,by=.1)
y1&lt;-dunif(seq(0,10,by=.1), min=-10, max=10)
y1&lt;-c(rep(0,length(x)-length(y1)),y1)
y2&lt;-dnorm(x, mean=5, sd=1)
y3&lt;-dnorm(x, mean=1, sd=2)

#Weights for Priors
wt1&lt;-3
wt2&lt;-1
wt3&lt;-.5

#Final Priors
y1&lt;-y1*wt1
y2&lt;-y2*wt2
y3&lt;-y3*wt3

#Sum to get overall Prior
y&lt;-y1+y2+y3

#Likelihood function for ""current data""
lik&lt;-10*dnorm(x, mean=1, sd=1)

#Updated Posterior Probability?
prob&lt;-lik*y


par(mfrow=c(2,2))
plot(x,y1, ylim=c(0,1), type=""l"", lwd=4, 
     ylab=""Density"", xlab=""Effect"", main=""Prior Component 1"")
plot(x,y2, ylim=c(0,1), type=""l"", lwd=4, 
     ylab=""Density"", xlab=""Effect"", main=""Prior Component 2"")
plot(x,y3, ylim=c(0,1), type=""l"", lwd=4, 
     ylab=""Density"", xlab=""Effect"", main=""Prior Component 3"")
plot(x,y, ylim=c(0,1), type=""l"", lwd=4, 
     ylab=""Density"", xlab=""Effect"", main=""Overall Prior"")




dev.new()
par(mfrow=c(2,1))
plot(x,lik, type=""l"", lwd=4, col=""Red"",
     ylab=""Likelihood"", xlab=""Effect"", main=""Likelihood"")
plot(x,prob, type=""l"", lwd=4, col=""Blue"",
     ylab=""Probability"", xlab=""Effect"", main=""Posterior Probability?"")
abline(v=c(-1,1), lty=2, lwd=3)
</code></pre>
",2013-11-07 18:03:18.783
59093,23227.0,2,,59048.0,,,,CC BY-SA 3.0,"<p>I found the answer. I was somewhat close with the question. The trick is that width is not just width, it is width of each interval. </p>

<p>Therefore</p>

<pre><code>bin1: 5,10,11,13,15,35,50,55,72 I.e. all values between 5 and 75
bin2: 92 I.e. all values between 75 and 145    
bin3: 204,215 I.e. all values between 145 and 215
</code></pre>

<p>And for equal width binning you are given number of required bins and in this case it is 3.</p>
",2013-11-07 18:19:37.763
59094,15827.0,2,,58995.0,,,,CC BY-SA 4.0,"<p>The <em>Iris</em> dataset is deservedly widely used throughout statistical science, especially for illustrating various problems in statistical graphics, multivariate statistics and machine learning.</p>
<ul>
<li><p>Containing 150 observations, it is small but not trivial.</p>
</li>
<li><p>The task it poses of discriminating between three species of <em>Iris</em> from measurements of their petals and sepals is simple but challenging.</p>
</li>
<li><p>The data are real data, but apparently of good quality. In principle and in practice, test datasets could be synthetic and that might be necessary or useful to make a point. Nevertheless, few people object to real data.</p>
</li>
<li><p>The data were used by the celebrated British statistician Ronald Fisher in 1936. (Later he was knighted and became Sir Ronald.) At least some teachers like the idea of a dataset with a link to someone so well known within the field. The data were originally published by the statistically-minded botanist Edgar Anderson, but that earlier origin does not diminish the association.</p>
</li>
<li><p>Using a few famous datasets is one of the traditions we hand down, such as telling each new generation that Student worked for Guinness or that many famous statisticians fell out with each other. That may sound like inertia, but in comparing methods old and new, and in evaluating any method, it is often considered helpful to try them out on known datasets, thus maintaining some continuity in how we assess methods.</p>
</li>
<li><p>Last, but not least, the <em>Iris</em> dataset can be enjoyably coupled with pictures of the flowers concerned, as from e.g. <a href=""http://en.wikipedia.org/wiki/Iris_flower_data_set"" rel=""nofollow noreferrer"">the useful Wikipedia entry on the dataset</a>.</p>
</li>
</ul>
<p>Note. Do your bit for biological correctness in citing the plants concerned carefully. <em>Iris setosa</em>, <em>Iris versicolor</em> and <em>Iris virginica</em> are three species (not varieties, as in some statistical accounts); their binominals should be presented in italic, as here; and <em>Iris</em> as genus name and the other names indicating particular species should begin with upper and lower case respectively.</p>
<p>(EDIT 4 May 2022 In a generally excellent book to hand on machine learning, the <em>Iris</em> data are described in terms of classes, types, kinds and subspecies, but never once correctly from a biological viewpoint. Naturally that sloppiness makes not a jot of difference to the machine learning exposition.)</p>
<p>Stebbins (1978) gave an appreciation of Anderson, a distinguished and
idiosyncratic botanist, and comments on the scientific background to
distinguishing three species of the genus <em>Iris</em>. Kleinman (2002)
surveys Anderson's graphical contributions with statistical flavor. See also Unwin and Kleinman (2021).</p>
<p>Kleinman, K. 2002.
How graphical innovations assisted Edgar Anderson's discoveries in
evolutionary biology.
<em>Chance</em> 15(3): 17-21.</p>
<p>Stebbins, G. L. 1978. Edgar Anderson 1897--1969.<br />
Biographical Memoir. Washington, DC: National Academy of Sciences.
<a href=""http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/anderson-edgar.pdf"" rel=""nofollow noreferrer"">accessible here</a></p>
<p>Unwin, A. and Kleinman, K. 2021. The iris data set: In search of the source of <em>virginica</em>. <em>Significance</em> 18: 26-29. <a href=""https://doi.org/10.1111/1740-9713.01589"" rel=""nofollow noreferrer"">https://doi.org/10.1111/1740-9713.01589</a></p>
",2013-11-07 18:35:10.543
59095,23487.0,1,59111.0,,,Iris dataset how to solve,<machine-learning>,CC BY-SA 3.0,"<p>I am new to Machine Learning. For Iris data set problem we can solve the problem with Multinomial logistic regression and as well as neural network. Which would give better performance regarding with cost and error? </p>
",2013-11-07 18:44:09.757
59097,3999.0,2,,59070.0,,,,CC BY-SA 3.0,"<p>So by ""absolute risk"" I'm going to assume you mean either the cumulative probability of an event at time t, or the hazard at time t.</p>

<p>In short, no, a Cox proportional hazards model doesn't really give you back that information - the model itself doesn't calculate the underlying hazard, just the relative difference in the hazard between covariate values. This rather nicely <em>frees</em> you from having to specify the underlying hazard of your outcome, which in many cases is unknown, not particularly of interest in the first place, or difficult to specify using a parametric model.</p>

<p>If you <em>do</em> want to estimate the underlying hazard function, you need to use parametric survival models, such as those used in survreg. There are a large number of tutorials online, including code, for survreg and parametric models.</p>

<p><a href=""http://courses.nus.edu.sg/course/stacar/internet/st3242/handouts/notes6.pdf"" rel=""nofollow"">This</a> is one of my favorites, as it includes some theoretical treatment and a good bit of code. The 'Survival' package documentation is also a good place to start.</p>
",2013-11-07 18:53:56.177
59098,9049.0,2,,59075.0,,,,CC BY-SA 3.0,"<p>The generative model under OLS is that of: $y \sim N(X\beta, \sigma^2 I)$. Fitting the model is finding the $\beta$ and $\sigma$ that maximize your log-likelihood.</p>

<p>Assuming your general covariance is $K$ (here $K = \sigma^2 I$) the loglikehood $L$ is equal to :
$L = -\frac{N}{2}\log(2\pi) - \frac{1}{2} \log(|K|) - \frac{1}{2} (y-\hat{y})^T K^{-1}(y-\hat{y})$, where as I said above $N$ is your number of readings and now $\hat{y}$ are the model fitted values; $(y-\hat{y})$ are your model's residuals and $|K|$ denotes the determinant of the covariance matrix $K$.</p>

<p>Luckily for all of us the OLS log likelihood can also be expressed as :<br>
$L = -\frac{N}{2}\log(2\pi) - N\log(\sigma) - \frac{1}{2\sigma^2} \sum (y-\hat{y})^2$. This and the above expression are equivalent. A bit of linear algebra can convince you for that.</p>

<p>OK, enough talk. Here is your numerical example in MATLAB:</p>

<pre><code>clc; clear;                %just clear stuff

X = (1:.02:20)'; 
Y = cos(X);                %dependant variable
N = length(Y);             %number of readings
X_matrix = [ ones(N,1) X]; %make design matrix

[b] =  (X_matrix\ Y);      %solve the system
%[b2] = lscov(X_matrix,Y); %equivalent statement
Fitted = X_matrix * b;     %find fitted values
Residuals = Y - Fitted;    %find residuals
sigma = std(Residuals);    %find std.dev. of residuals
format long                %set it long for visual inspec. 
                           %Calculate L using simple expression
L_simple = -N*.5*log(2*pi) - N*log(sigma) - (1/(2*sigma.^2))*sum( Residuals.^2);
                           %set K covariance matrix
K_matrix =  eye(N) * sigma^2;
                           %Calculate L using generic expression
L_generic =-N*.5*log(2*pi) - sum(log(diag(chol(K_matrix)))) ...
           - .5*Residuals' / (K_matrix)* Residuals;
% ( sum(log(diag(chol(K_matrix)))) equals -.5*log(det(K)) 
% but it is far more stable.
L_generic 
% ans = -9.929305263221722e+02
L_generic - L_simple 
% ans =  3.410605131648481e-12 %cool they are practically the same
</code></pre>

<p>But hey, can you check this in <code>R</code> that we are quite sure it is works?</p>

<pre><code>X = seq(1,20, by=.02)
Y = cos(X)
lm_test = lm(Y ~ X)
sigma = sd( residuals( lm_test))
logLik(lm_test)
#'log Lik.' -992.9303 (df=3)
#Check difference with MATLAB answer:
-9.929305263221722e+02 -  as.numeric(logLik(lm_test))
#[1] -0.0002630656 #very small difference, mostly due to numerics(*).
#Use the simple formula:
-N*.5*log(2*pi) - N*log(sigma) - (1/(2*sigma^2))*sum(  residuals( lm_test)^2)
#[1] -992.9305  #Cool it works as expected.
</code></pre>

<p>A standard reference <em>free</em> reference for all this would be the book <a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"" rel=""nofollow"">Elements of Statistical Learning: Data Mining, Inference, and Prediction</a> by Hastie, Tibshirani and Friedman. (Chapter 2.6)</p>

<p><code>(*) There are some small issues also about how you calculate the degrees of freedom for the variance; one uses (N-m) instead of just (N-1), m being the number of columns of your X matrix but you don't need to worry much about it at this point.</code></p>
",2013-11-07 19:27:08.170
59099,23194.0,1,,,,Is there a textbook / handbook with full derivations for statistical / machine learning concepts?,<machine-learning><references>,CC BY-SA 3.0,"<p>In particular, I am looking for a textbook which will go over the details of derivations (including all calculus and linear algebra) for learning models and concepts such as logistic regression, Gaussian Discriminant Analysis, with full proofs for variants like Gaussian Naive Bayes.</p>

<p>Books such as ""Elements of Statistical Learning"" tend to gloss over certain details. For example, when discussing L1 Regularized Logistic Regression (Section 4.4.4), it says that ""the score equations ... have the form"", and then presents the form without giving the derivation.</p>
",2013-11-07 19:29:11.147
59100,23490.0,1,59106.0,,,t-test or Wilcoxon test in R,<r><statistical-significance>,CC BY-SA 3.0,"<p>I have a very small data sets of web traffic to compare the effect of performing advertising over five days or seven days. Yes, running a test over 7 days would definitely give me more traffic but I would like to know if the traffic is significantly higher and worth my consideration.</p>

<p>I run each test for 2 weeks. </p>

<p>This is my dataset:</p>

<pre><code>5day advertising web traffic    7day advertising web traffic
week1   week2                     week1    week2
184418  179650                    301978    308019
</code></pre>

<p>I ran the t-test in R and got this value</p>

<pre><code>&gt; a&lt;-c(184418,179650)
&gt; b&lt;-c(301978,308019)
&gt; t.test(a,b)

t = -31.9557, df = 1.898, p-value = 0.001307
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -140407.5 -105521.5
sample estimates:
mean of x mean of y 
 182034.0  304998.5 
</code></pre>

<p>Is my dataset too small and does this mean that there is a significance difference? </p>

<p>When I run the Wilcoxon test (via <code>wilcox.test</code>), I get a different output:</p>

<pre><code>data:  a and b
W = 0, p-value = 1
alternative hypothesis: true location shift is greater than 0
</code></pre>

<p>Thanks</p>
",2013-11-07 20:10:36.020
59101,13037.0,2,,59100.0,,,,CC BY-SA 3.0,"<p>Here is what I got when I used your data. I would go with the results of the wilcoxon test since your sample size is so small. </p>

<pre><code>x1&lt;- c(184418, 179650)
x2&lt;- c(202316, 196395)

t.test(x1,x2)

    Welch Two Sample t-test

data:  x1 and x2
t = -4.557, df = 1.913, p-value = 0.0488
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -34409.9541   -233.0459
sample estimates:
mean of x mean of y 
 182034.0  199355.5 

wilcox.test(x1,x2)

data:  x1 and x2
W = 0, p-value = 0.3333
alternative hypothesis: true location shift is not equal to 0
</code></pre>
",2013-11-07 20:29:36.977
59102,23492.0,1,,,,Computational statistics book,<mathematical-statistics><references><computational-statistics>,CC BY-SA 3.0,"<p>Can anybody recommend me a good book on Computational Statistics? I am new to this subject so I am not sure how to be more specific.</p>
",2013-11-07 20:46:18.193
59103,6204.0,2,,59102.0,,,,CC BY-SA 3.0,"<p>This question is probably going to get closed for being off topic, but here's one: </p>

<blockquote>
  <p>Givens, GH and Hoeting, JA. <a href=""http://rads.stackoverflow.com/amzn/click/0470533315"" rel=""nofollow"">Computational Statistics</a>, 2nd ed. Wiley (2012)</p>
</blockquote>

<p>You might also like <a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"" rel=""nofollow"">The Elements of 
Statistical Learning</a> (Hastie, Tibshirani, and Friedman; available online as PDF).</p>
",2013-11-07 20:53:46.740
59104,20538.0,2,,59102.0,,,,CC BY-SA 3.0,"<p>I've found Monahan's <a href=""http://rads.stackoverflow.com/amzn/click/0521139511"" rel=""nofollow"">Numerical Methods of Statistics</a> very valuable. I also have Computational Statistics by Givens and Hoeting, and <a href=""http://rads.stackoverflow.com/amzn/click/1441959440"" rel=""nofollow"">Numerical Methods of Statistics</a> by Lange, both of which are good. <a href=""http://rads.stackoverflow.com/amzn/click/1584885459"" rel=""nofollow"">Statistical Computing with R</a> by Rizzo is nice to have if you're wanting to do a lot of this stuff in R.  I find myself jumping around a lot, and still doing a lot of reading outside for things like quadrature, matrix computations, and everything in Press et al.  </p>
",2013-11-07 21:07:50.560
59105,21161.0,1,59123.0,,,Modeling time: Probability distribution over time?,<time-series><machine-learning><probability><distributions>,CC BY-SA 3.0,"<p>I'm trying to model users' posting behavior during a day. Say we have a bunch of users, with the time they post tweets. Now, for each user, I would like to estimate the likelihood of he post a new tweet at 9:00am according to his historical posting behaviors. </p>

<p>I'm curious what distribution I could pose here. In the literature I saw people using Gaussian, but I'm not sure if that's suitable since it's single peak. (Mixture model would be too complex for this task)</p>

<p>Thus I'm wondering is there any distribution over time that I could use?</p>

<p>The data I'm having is 2 month worth of tweets. Each tweet contains a timestamp, and the author's id. What I'm trying to model is the user's daily activity in terms of posting a tweet. E.G. the data for a single user would looks like [9:00, 9:12, 17:00, 17:01, 22:22, 22:37, 22:45, 22:47, 22:48...]. So this user post more tweets during night (around 10:30 pm) but very rare during work hours. I wish to model the probability P(user post a tweet|time).</p>

<p>Would really appreciate the answers!</p>
",2013-11-07 21:20:06.500
59106,503.0,2,,59100.0,,,,CC BY-SA 3.0,"<p>Given what you are trying to do, I am not sure a t-test is what you want.</p>

<p>I am guessing that advertising for 7 days costs more than advertising for 5. So, let's look at cost per day:</p>

<pre><code>Week 1: 184,418/5 = 36,883 per day
        202,316/7 = 28,902 per day

Week 2: 179,650/5 = 35,930
        196,395/7 = 28,056
</code></pre>

<p>Then what impresses me is that the difference is nearly the same for the 2 weeks. The question of how to test this is, I think, not so important. </p>

<p>In fact, if 5 days is M-F and 7 is every day, you might consider trying only the weekend!</p>

<p>Of course, cost may not be the same per day, in which case the above would have to be modified. </p>
",2013-11-07 21:29:38.710
59107,19463.0,1,59108.0,,,shuffle my data to investigate differences between 3 groups,<anova><statistical-significance><simulation>,CC BY-SA 3.0,"<p>I am sure this already exists but I just don't know the terminology to look for.</p>

<p>I have three sets of 10 measurements. Each set corresponds to a different geographic region.
So in total I have 30 measurements of my variable, and I have the factor ""region"" with 3 levels (west region, middle region, east region).</p>

<p>Let's say I do a simple ANOVA and I get differences between the 3 regions. But I want to play a little with the possibility of this differences being ""by chance"". Or, in another scenario, let's say I can't use ANOVA for some reason (eg. strongly inhomogeneous variances) and I use a non-parametric test and I don't find differences</p>

<p>I want to know if it's possible to do the following (or if the idea is appropriate):</p>

<p>If there is really no difference between the 3 regions, then I can assume that any test (eg ANOVA or a non-parametric equivalent) will find approximate the same results even if I randomly mix all data once and again. So I thought I could simulate this, using my own data but just in different grouping. for example:
1- take all the 30 values from my own measurements
2- shuffle them into 3 groups, ie. randomly choose 10 values and assign them to a randomly chosen group; repeat with the next 10 data and then you have again 3 groups of 10 measurements.
3- run the test (eg. ANOVA)</p>

<p>Now I go back to 1, and repeat this eg 1000 times, and see if there is a convergence towards a ""stable"" pattern. If there is, then there are actually no differences.
If the convergence deviates a lot from the results I found with my ""real"" dataset, then I may think there are actually differences between the 3 regions.</p>

<p>Is my reasoning correct/sound? I know there is something like this, I just don't remember the name.. I thought it was related to permutations but I'm not sure...</p>
",2013-11-07 21:31:58.190
59108,594.0,2,,59107.0,,,,CC BY-SA 4.0,"<blockquote>
<p>If there is really no difference between the 3 regions, then I can assume that any test (eg ANOVA or a non-parametric equivalent) will find approximate the same results even if I randomly mix all data once and again.</p>
</blockquote>
<p>This is the central insight that underlies resampling methods, such as permutation tests / randomization tests.</p>
<p>e.g. see Wikipedia, for example <a href=""https://en.wikipedia.org/wiki/Resampling_%28statistics%29#Permutation_tests"" rel=""nofollow noreferrer"">here</a></p>
<p>The basic idea of a permutation test (let's take a one way ANOVA-like situation) is that if the null is true, the group labels are arbitrary - you don't change the distributions by shuffling them.</p>
<p>So if you look at all possible arrangements of the group labels and compute some test statistic of interest, you obtain what's called the permutation distribution of the test statistic. You can then see if your particular sample (which will be one of the possible permutations - or more accurately, possible combinations) is unusually far 'in the tails' of that null distribution (giving a p-values).</p>
<p>Many of the common nonparametric rank-based tests are actually permutation-tests carried out on the ranks (which is a practical way of doing permutation tests without computers, which are otherwise very tedious unless you have very small sample sizes).</p>
<p>When the sample sizes are large, an option is to sample (with replacement) from the permutation distribution, typically because there are too many combinations to evaluate them all. Generally this is achieved by randomly permuting the labels rather than systematically re-arranging them to cover every possibility. The test statistic is then computed for each such arrangement. The sample value of the statistic is then compared with the distribution (it is normally included as part of the distribution for computing the p-value, and counts in the values 'at least as extreme' as itself). Some authors call this sampled permutation test a randomization test (though other authors reserve that term for a somewhat different notion also connected to permutation tests).</p>
<p>What you described was pretty close to this randomly sampled permutation test (randomization test).</p>
<p>I advise trying such a randomization test, not least for its ability to expand your horizons in terms of the standard tools you have available for tackling problems. The procedure is distribution-free (conditional on the sample) - it requires fewer assumptions while still allowing you to use either familiar statistics or ones custom-designed to your circumstances (e.g. you could slot in a more robust measure of location).</p>
<p>In practice I'd advise more than 1000 resamples for a randomization test. Consider a test with a p-value near 5%. The standard error of an estimated p-value for a sample size of 1000 will be nearly 0.007; when the true p-value is just on one side of 5%, nearly 15% of the time you'll see a value more than 1% on the wrong side (more than 6% or less than 4% when it should be the other side). I usually regard 10000 as toward the low end of what I do unless I just want a rough idea of the ballpark of the p-value. If I was doing a formal test, I'd want to pin it down a bit better. I often do 100,000 and sometimes a million or more - at least for the simpler tests.</p>
<p>If you search here on <em>permutation tests</em> or <em>randomization tests</em> you should find a number of relevant questions and answers and even some examples.</p>
",2013-11-07 21:35:01.070
59109,23493.0,2,,45543.0,,,,CC BY-SA 3.0,"<p>No, the square root of the symmetrised KL divergence is not a metric. A counterexample is as follows:</p>

<ul>
<li>Let $P$ be a coin that produces a head 10% of the time.</li>
<li>Let $Q$ be a coin that produces a head 20% of the time.</li>
<li>Let $R$ be a coin that produces a head 30% of the time.</li>
<li>Then $d(P, Q) + d(Q, R) = 0.284... + 0.232... &lt; 0.519... = d(P, R)$.</li>
</ul>

<p>However, for $P$ and $Q$ very close together, $D(P, Q)$ and $J(P, Q)$ and $S(P, Q)$ are essentially the same (they are proportional to one another $+ O((P-Q)^3)$) and their square root is a metric (to the same order). We can take this local metric and integrate it up over the whole space of probability distributions to obtain a global metric. The result is:</p>

<p>$$A(P, Q) = \cos^{-1}\left(\sum_x \sqrt{P(x)Q(x)} \right)$$</p>

<p>I worked this out myself, so I'm afraid I do not know what it is called. I will use A for Alistair until I find out. ;-)</p>

<p>By construction, the triangle inequality in this metric is tight. You can actually find a unique shortest path through the space of probability distributions from $P$ to $Q$ that has the right length. In that respect it is preferable to the otherwise similar Hellinger distance:</p>

<p>$$H(P, Q) = 1 - \sqrt{\sum_x \sqrt{P(x)*Q(x)} }$$</p>

<p>Update 2013-12-05: Apparently this is called the Battacharrya arc-cos distance.</p>
",2013-11-07 21:42:38.513
59110,23496.0,1,59239.0,,,How to analyze this incomplete block design in R?,<experiment-design><lme4-nlme><mixed-model><blocking>,CC BY-SA 3.0,"<p>I was just wondering if you could point me in the right direction.
I have a dataset with 5 tree clones planted at 10 different sites, i.e. same clones are replicated twice at different sites.</p>

<pre><code>  Site Clone  
    1     A    
    2     A  
    3     B  
    4     B  
    5     C  
    6     C  
    7     D  
    8     D  
    9     E  
   10     E
</code></pre>

<p>At each site each clone is replicated multiple times. Ideally, I would want to know what is the effect of clone on my dependent variable y and whether a site effect is present. To me it looks like an <strong>incomplete</strong> block design with clone as a fixed effect and site as a random effect (and block). Using lmer from the lme4 package in R, I would specify the model as follows: </p>

<pre><code>lmer(y~clone + (1|site), data=mydata)    
</code></pre>

<p>Is this a correct way of analyzing this dataset? I could also average by clone over sites and eliminate sites. But this way I will lose potential important information as to whether a site effect is present.</p>

<p>Any pointers are appreciated!</p>
",2013-11-07 21:50:35.707
59111,7741.0,2,,59095.0,,,,CC BY-SA 3.0,"<p>As far as I know, the Iris data set should be (almost) linearly separable.</p>

<p>Multinomial logistic regression (MLR) is a linear classifier. Neural networks (NN) are nonlinear classifiers.</p>

<p>The problem with NNs is that they could overfit your training data and might not generalize as good as MLR. You can avoid that by adding a regularization term to the cost function (error function) of the NN, so that your error function will consist of a term that penalizes errors on the training set (e.g. cross entropy, sum of squared errors $\sum_n ||y^{(n)}-t^{(n)}||^2_2$) and a term that penalizes the model complexity (e.g. norm of the weight vector $\gamma||w||^2_2$, ...).</p>

<p>Adding a penalty for large weights to your error function is like using a prior $p(w)$ for your hypothesis. Which means some $w$ become more likely than others.</p>
",2013-11-07 22:05:38.010
59112,23335.0,1,,,,Repeated measurement analysis advice in R lme and / or aov and a couple of more questions,<r><anova><repeated-measures><multiple-comparisons>,CC BY-SA 3.0,"<p>I want to analyse the visual performance, operationalized via contrast threshold, depending on adaptation luminance and spectrum, gathered for 29 subjects. I'm currently kind of confused of how to do that. Some of my data:</p>

<pre><code>    ID C_measured Subject  LB Spectrum SpectrumLB
  1  1 0.1339795   AHI11 0.1       HS     HS.0.1
  2  2 0.1040440   AIC19 0.1       HS     HS.0.1
  3  3 0.1363313   AUO13 0.1       HS     HS.0.1
  4  4 0.1134103   BAR01 0.1       HS     HS.0.1
  5  5 0.1117670   BAR02 0.1       HS     HS.0.1
  6  6 0.1166350   BCL10 0.1       HS     HS.0.1
</code></pre>

<p>LB can be 0.1, 0.21, 0.3 and 1.0, Spectrum HS and LED</p>

<p>I know that I want to do 8 planned comparisons of Spectrum at each LB (4x) and the effect of reducing LB from 0.3 to 0.21 for both spectra with and without exchanging the spectrum (4x).</p>

<p>Some said I should do a two-way ANOVA, e.g. like</p>

<pre><code>   aov(C_measured ~ LB * Spectrum + Error(Subject / (LB * Spectrum)), data = anovaFrameWithGlareFoveal)
</code></pre>

<p>and then a post-hoc test on the interaction variable SpectrumLB, e.g. like</p>

<pre><code>anovaFrameWithGlareFoveal.lme &lt;- lme(C_measured ~ LB, random = ~1 | Subject / LB, data = anovaFrameWithGlareFoveal)
anovaFrameWithGlareFoveal.glht &lt;- glht(am2.subject, linfct = mcp(LB = ""Tukey""))
summary(anovaFrameWithGlareFoveal.glht, test = adjusted(type = ""none""))
</code></pre>

<p>then manually bonferroni-adjusting the p-value to the number of my planned comparisons.</p>

<p>I do that for a couple of other parameters (with glare, without glare, old reference group, young group), which I don't want to include in the statistical analysis, because it is common knowledge that this influences visual performance, I just want to analyse whether the planned comparisons differ for those parameters.</p>

<p>Till here the question: is this how to do it? DO I need the two-way ANOVA at all?</p>

<p>Then I observed some things:
for some of the parameters I observed significant values in the two-way ANOVA for the main effect of Spectrum p&lt;.05, but none of the uncorrected multiple comparisons between the two spectra at all four LBs was significant (ok one was &lt;.1, but I'm testing against .05), which some people commented with ""impossible"".
Is this possible?</p>

<p>Then people said: ""ok paired t.tests should come up with the same results"" so I did this:</p>

<pre><code>df &lt;- anovaFrameWithGlareFoveal
df.led &lt;- subset(df, Spectrum == ""LED"")
df.hs &lt;- subset(df, Spectrum == ""HS"")
t.test(df.led$C_measured[df.led$LB==.1], df.hs$C_measured[df.hs$LB==.1], paired=T)
t.test(df.led$C_measured[df.led$LB==.21], df.hs$C_measured[df.hs$LB==.21], paired=T)
t.test(df.led$C_measured[df.led$LB==.3], df.hs$C_measured[df.hs$LB==.3], paired=T)
t.test(df.led$C_measured[df.led$LB==1], df.hs$C_measured[df.hs$LB==1], paired=T)
</code></pre>

<p>then all of the t.tests were significant (&lt;.05) but only one of the uncorrected multiple comparison was significant (&lt;.05).</p>

<p>I'm not really that much into statistics that I can definitively argue for or against one method or combined methods or whether the lme + glht alone is sufficient. Had a tough time on that the last week and am really curiously looking forward to your comments!</p>
",2013-11-07 22:17:16.140
59113,23499.0,1,59116.0,,,Interpreting coefficients of first differences of logarithms,<time-series><interpretation>,CC BY-SA 3.0,"<p>My problem is interpreting coefficients of such time series model:</p>

<p>\begin{equation}
\ln Y_t - \ln Y_{t-1} =b_1 \cdot \left(X_{t}-X_{t-1}\right)+b_2 \cdot Z_t.\end{equation}</p>

<p>I don't know how to interpret coefficients $b_1$ and $b_2$.</p>

<p>Hope that someone can help.</p>
",2013-11-07 22:17:18.293
59114,23138.0,1,,,,Best approach to classifying 3-point trajectories?,<clustering><repeated-measures><gaussian-mixture-distribution><growth-mixture-model>,CC BY-SA 3.0,"<p>I have a sample of about 300 subjects who have been measured at 3 different times (morning, afternoon, evening). The variable of interest can be assumed to be approximately normal. It appears that most subjects have an increase between between morning and afternoon, followed by a decrease from afternoon to evening. Some however show an opposite pattern (devrease->increase), while yet others remain approximately the same.</p>

<p>What I am interested in is clustering, or classifying the subjects according to their trajectories. After a bit of googling, I have discovered GMM. I don't really understand what goes on behind the scenes, but it looks like the classification is done based on straight line fits when we only have 3 points. This seems highly inappropriate to me since straight line does not capture the increase followed by a decrease type of behaviour. Another thing is, people seem to be using Mplus package, which I am not familiar with and would rather avoid buying and learning (I am very comfortable with R and Matlab).</p>
",2013-11-07 22:22:10.430
59115,23500.0,1,,,,Selection of sites for wind power generation using time series from 600 candidate sites,<time-series><correlation><optimization><decision-theory>,CC BY-SA 4.0,"<p>I have 3 hourly power generation data for around 600 locations for a year. (i.e. 8 data per day for 365 days for each location.)</p>

<p>I want to find out a way where out of this 600 locations, I can say suppose choose suppose 10 locations which are producing power at different time periods.</p>

<p>suppose:  </p>

<pre><code>Location:   Hour 0, Hour 3, Hour 6, Hour 9, Hour 12,  Hour 15

   A          30       00      50      70       00      20    
   B          50       20      70     100       00      40
   C          00      100      20      00       40      30
   D          20       15      10      00       40      30
</code></pre>

<p>Here, A and B are highly correlated where A and C are inversely correlated. So, is there a way where I can distinguish the locations which are producing powers at different time periods so that, by accommodating these few location, the overall power generation can be sort of stable?</p>

<p>By sort of stable, I mean something like this which I mentioned in the comment later on to clarify my question.</p>

<p>Since A and C are inversely co-related, so if I choose power from A and C out of four locations, I would get some power all the time (always above 30 in this scenario). But if I take A and B, I would get very high amount of power when I have power (like 70+100=170 in hour 9), and I wont get any power at Hour 12 (since A and B produces 00 power in Hour 12).  I want to avoid choosing A and B, and I want to choose A and C.  My objective is to choose 10 locations out of 600 where by adding powers of each hour of 10 locations, total power for each hour should be above a certain threshold, like 30.</p>
",2013-11-07 22:49:32.913
59116,5045.0,2,,59113.0,,,,CC BY-SA 4.0,"<p>For <span class=""math-container"">$small$</span> changes, you can interpret logged differences as percentage changes after multiplying by 100.</p>

<p>For example, <span class=""math-container"">$y_t=9$</span> and <span class=""math-container"">$y_{t-1}=8$</span>. Then <span class=""math-container"">$\ln 9 - \ln 8=.118$</span> or 11.8%, which is the logarithmic approximation to the actual 12.5% increase. Note that I had to multiply by 100 here. For <span class=""math-container"">$y_t=9$</span> and <span class=""math-container"">$y_{t-1}=8.5$</span> the approximation will be much better (<span class=""math-container"">$5.9\% \approx 5.7\%$</span>).</p>

<p>Usually, a coefficient tells you the effect on <span class=""math-container"">$y$</span> of a one unit change in that explanatory variable, holding other variables constant. A one unit change in <span class=""math-container"">$\Delta \ln x$</span> corresponds to a 100% change (using the approximation above, which is terrible since this is not a small change). This means that <span class=""math-container"">$b_1$</span> tells you the percentage change in <span class=""math-container"">$y$</span> associated with a 1% increase in x.</p>

<p>But your <span class=""math-container"">$x$</span> is not logged, so the coefficient needs to be interpreted differently. When <span class=""math-container"">$x$</span> grows by one unit, you get <span class=""math-container"">$100 \cdot b_1\%$</span> more <span class=""math-container"">$y$</span>.</p>

<p>Moreover, <span class=""math-container"">$100 \cdot b_2$</span> tells you the percentage change in <span class=""math-container"">$y$</span> associated with a 1 <em>unit</em> increase in <span class=""math-container"">$z$</span>.</p>
",2013-11-07 22:57:43.387
59117,16325.0,1,,,,Most Powerful Test; Two-Parameter Normal Distribution,<hypothesis-testing><normal-distribution><mathematical-statistics><exponential-family>,CC BY-SA 3.0,"<p>Is it possible to show that the two-parameter Normal distribution has monotone likelihood ratio?</p>

<p>EDIT:</p>

<p>This is actually part of a larger problem. We have a random sample from $\mathcal N(\mu, \sigma^2)$ and are testing: 
\begin{align}
Ho\!:\quad&amp;\sigma &lt;\sigma_o , &amp;\mu &amp;\in \mathbb{R}  \\
Ha\!:\quad&amp;\sigma = \sigma_1 , &amp;\mu &amp;= \mu_1
\end{align}
Construct a most powerful level-$Î±$ test. My initial thought was to show $\mathcal N(Î¼,Ïƒ^2)$ has the monotone likelihood ratio (MLR), and from there we can construct a uniformly most powerful (UMP) test via the <a href=""http://en.wikipedia.org/wiki/Uniformly_most_powerful_test#The_Karlin-Rubin_theorem"" rel=""nofollow"">Karlin-Rubin theorem</a>. Any suggestions on general strategy would be appreciated! </p>
",2013-11-07 23:36:44.683
59118,3728.0,2,,58995.0,,,,CC BY-SA 3.0,"<p>The dataset is big and interesting enough to be non-trivial, but small enough to ""fit in your pocket"", and not slow down experimentation with it.</p>

<p>I think a key aspect is that it also teaches about over-fitting. There are not enough columns to give a perfect score: we see this immediately when we look at the scatterplots, and they overlap and run into each other. So any machine-learning approach that gets a perfect score can be regarded as suspicious.</p>
",2013-11-08 00:18:50.447
59119,21119.0,1,59120.0,,,"$E(x^k)$ under truncated $\mathcal{N}(\mu,1)$",<normal-distribution><expected-value><integral><truncation>,CC BY-SA 3.0,"<p>There is a similar question in <a href=""https://stats.stackexchange.com/questions/74833/exk-under-a-gaussian?noredirect=1#comment145996_74833"">$E(x^k)$ under a Gaussian</a>. However, it doesn't seem to be trivial when $\mu\ne0$. As mentioned in the previous question <strong>$k$ is not an integer</strong>.</p>

<p>The integral that I need to evaluate is as follows:
$$\int_0^\infty x^k\exp\left(-\frac{(x-\mu)^2}{2}\right)dx$$</p>

<p>If it helps for the case that $\mu=0$ the answer is $\frac{2^{(k-2)/2}}{\sqrt{\pi}}\Gamma(\frac{k+1}{2})$</p>
",2013-11-08 00:29:41.700
59120,20473.0,2,,59119.0,,,,CC BY-SA 3.0,"<p>This is a Mellin transform. In general notation we have ($a&gt;0, s&gt;0$)</p>

<p>$$\int_{0}^{\infty}x^{s-1}\exp\left\{-ax^2-bx\right\}dx = (2a)^{-1/2}\Gamma(s)\exp\left\{\frac {b^2}{8a}\right\}D_{-s}\left(b(2a)^{-1/2}\right)$$</p>

<p>where $D_{-s}()$ is (Whittaker's) parabolic cylinder function.</p>

<p>For your integral we have</p>

<p>$$I=\int_0^\infty x^k\exp\left\{-\frac{(x-\mu)^2}{2}\right\}dx = \exp\left\{\frac {-\mu^2}{2}\right\}\int_0^\infty x^k\exp\left(-\frac{x^2}{2} +\mu x\right)dx  $$</p>

<p>Matching coefficients we get</p>

<p>$$s-1 = k \Rightarrow s=k+1,\;\; a=\frac 12,\;\; b=-\mu$$</p>

<p>Inserting into the general solution we have</p>

<p>$$I=\exp\left\{\frac {-\mu^2}{2}\right\}\left(2\frac 12\right)^{-1/2}\Gamma(k+1)\exp\left\{\frac {\mu^2}{8\frac 12}\right\}D_{-k-1}\left(-\mu\left(2\frac 12\right)^{-1/2}\right)$$</p>

<p>$$=\exp\left\{\frac {-\mu^2}{4}\right\}\Gamma(k+1)D_{-k-1}\left(-\mu\right)$$</p>

<p>Now set $k^* =k +\frac 12$. Then $D_{-k-1}\left(-\mu\right) = D_{-k^*-\frac 12}\left(-\mu\right)$ and for the second you can look up Abramowitz and Stegun <a href=""http://people.math.sfu.ca/~cbm/aands/page_687.htm"" rel=""nofollow"">p.687</a> and 686, starting with eq. $19.3.1$. You will indeed see that when $\mu\neq 0$ the situation is not trivial. The fact that $k$ is not an integer causes no special trouble.</p>
",2013-11-08 02:27:04.930
59123,20473.0,2,,59105.0,,,,CC BY-SA 3.0,"<p>The time-stamp does not measure the magnitude of some variable, it marks points in time per se.  And from what you write, you are interested in a binary variable: to tweet or not to tweet, call it $Y$.</p>

<p>One possible modelling approach could be the following (for each individual separately): First, you have to decide on how you will partition the day in time zones (half-hours? Hours? Morning-Noon, etc? Depends on the particulars of your case). Given this partition, your data will be grouped in each time zone, for each day, as a count ""XX tweets during time zone 2"" etc.  </p>

<p>For each day separately, this will give you an empirical frequency distribution for the random variable $X=$""number of tweets per time zone"". If you divide these frequencies of each time zone by the total tweet count of the day, you will obtain an empirical relative frequency distribution, that can be considered an estimation of the random variable $Y$ (to tweet or not to tweet), for this particular day.  </p>

<p>Denote $d_{it}$ the time zone $i$ of day $t$, $i=1,...,k$ and $p_{it}$ the corresponding empirically estimated probability that the person tweets during this time zone.
Now go <em>across</em> days and consider the $k$ <em>probabilities</em> series
$p_{it}=$ ""Tweet during time zone $i$ of day $t$"", $t=1,...,60$, since you say you have data for 60 days.</p>

<p>From here on you can do various things: check each time series for stability: do the pattern remains approximately the same? Here, how you have partitioned the day becomes crucial (the smaller the time interval represented in each time zone, the more instability is to be expected).  </p>

<p>If you expect to be getting more data as days pass, you can adopt a Bayesian, updating approach, estimate a prior distribution with these first 60 days of data, and then gradually update the estimation of the distribution as new data come in: the new estimate will give you the probabilities of when the persons will tweet the next day, for each time zone.</p>

<p>But also you can view all the time zones together as a Vector Autoregression (VAR), of $k-1$ equations (since probabilities add up to unity), and do what one can do with VAR's, i.e. model the tomorrow probability of each time zone, as depending in a usually linear way on the corresponding probabilities of previous days (lag length to be determined during by the data and the model specification process).</p>
",2013-11-08 03:04:15.913
59124,14298.0,1,,,,Implications of current debate on statistical significance,<hypothesis-testing><inference><philosophical><reproducible-research><social-science>,CC BY-SA 3.0,"<p>In the past few years, various scholars have raised a detrimental problem of scientific hypothesis testing, dubbed ""researcher degree of freedom,"" meaning that scientists have numerous choices to make during their analysis that bias towards finding with p-value &lt; 5%. These ambiguous choices are, for example, which case to be included, which case is categorized as outlier, running numerous model specification until something shows up, do not publish null results, etc. (The paper that sparked this debate in psychology is <a href=""http://people.psych.cornell.edu/~jec7/pcd%20pubs/simmonsetal11.pdf"">here</a>, see a popular Slate article and follow-up debate by Andrew Gelman <a href=""http://www.slate.com/articles/health_and_science/science/2013/07/statistics_and_psychology_multiple_comparisons_give_spurious_results.html"">here</a>, and the Time magazine also touches on this topic <a href=""http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble"">here</a>.)</p>

<p><strong>First</strong>, one clarification question:</p>

<p>The <em>Time</em> magazine wrote, </p>

<blockquote>
  <p>""A power of 0.8 means that of ten true hypotheses tested, only two
  will be ruled out because their effects are not picked up in the
  data;""</p>
</blockquote>

<p>I am not sure how this fits into the definition of the power function I found in textbook, which is the probability of rejecting the null as a function of parameter $\theta$. With different $\theta$ we have different power, so I don't quite understand the above quote.</p>

<p><strong>Second</strong>, some research implications:</p>

<ol>
<li><p>In my field of political science / economics, scholars simply use up all the country-year data available. Thus, should we not be concerned with sample fiddling here?</p></li>
<li><p>Can the problem of running multiple tests but reporting only one model be fixed simply by the fact that someone else in the discipline will re-test your paper and strike you down immediately for not having robust results? Anticipating this, scholars in my field are more likely to include a <code>robustness check</code> section, where they show that multiple model specifications does not change the result. Is this sufficient?</p></li>
<li><p>Andrew Gelman and others raise the point that no matter the data, it would be always possible to find and publish some ""pattern"" that isn't really there. But this should not be a concern, given the fact that any empirical ""pattern"" must be supported by a theory, and rival theories within a discipline will just engage in an debate / race to find which camp is able to find more ""patterns"" in various places. If a pattern is truly spurious, then the theory behind will be quickly struck down when there is no similar pattern in other samples / settings. Isn't this how science progresses?</p></li>
<li><p>Assuming that the current trend of journals for null result will actually flourish, is there a way for us to aggregate all the null and positive results together and make an inference on the theory that they all try to test?</p></li>
</ol>
",2013-11-08 03:16:29.310
59125,6204.0,2,,59122.0,,,,CC BY-SA 3.0,"<p>The probability of a tweet being spam given n is just the proportion of tweets for a given n that are spam. If for n=4 you had 10 spam tweets out of the 100 tweets observed for that level of n, then P(tweet is spam|n=4) = 10/100 = 0.1 = 10%. </p>

<p>It's not clear to me what you mean by ""build a normal distribution based on this data set."" </p>
",2013-11-08 03:51:53.727
59126,5821.0,2,,59124.0,,,,CC BY-SA 3.0,"<p>The field of statistical science has addressed these issues since its outset. I keep saying the role of the statistician is to ensure that the type 1 error rate remains fixed. This implies that the risk of making false positive conclusions cannot be eliminated, but can be controlled. This should draw our attention to the extremely large volume of scientific research that's being conducted rather than toward the philosophy and ethics of general statistical practice. For every incredible (uncredible) result that surfaces in the media (or in government policy) at least 19 other uncredible results were shot down for their null findings.</p>

<p>Indeed, if you go to, say, clinicaltrials.gov, you will observe there are (for almost any disease indication) well over 1,000 clinical trials for pharmaceutical agents going on in the US at this very moment. That means, that with a false positive error rate of 0.001, on average at least 1 drug will be put on the shelves that has no effect. The validity of 0.05 as a validated threshold for statistical significance has been challenged again and again. Ironically, it's only the statisticians who feel uncomfortable with using a 1/20 false positive error rate whereas financial stakeholders (be they PIs, or Merck) will pursue beliefs tenaciously regardless of in-vitro results, theoretical proofs, or strength of prior evidence. Honestly, that tenacity is a successful and laudable personal quality of many individuals who are successful in non-statistical roles. They are generally seated above statisticians, in their respective totems, who tend to leverage that tenacity.</p>

<p>I think the Time quote you put forward is completely wrong. Power is the probability of rejecting the null hypothesis given it's false. This more importantly depends on exactly how ""false"" the null hypothesis is (which in turn depends on a measurable effect size). I rarely talk of power out of the context of the effect which we would deem ""interesting"" to detect. (for instance, a 4 month survival following chemotherapeutic treatment of stage 4 pancreatic cancer is not interesting, hence there's no reason to recruit 5,000 individuals for a phase 3 trial).</p>

<p>To address the questions you asked</p>

<ol>
<li><p>???</p></li>
<li><p>Multiplicity is difficult because it does not lead to an obvious decision rule about how to handle the data. For instance, suppose we are interested in a simple test of mean difference. Despite the infinite protestations of my colleagues, it is easy to show a t-test is well calibrated to detect differences in mean regardless of the sampling distribution of the data. Suppose we alternately pursued their path. They would begin by testing for normality using some variant of a well known distributional test (say calibration of the qqplot). If the data appeared sufficiently non-normal, they would then ask whether the data follow any well known transformation, and then apply a Box Cox transformation to determine a power transformation (possibly logarithmic) which maximizes entropy. If an obvious numerical value pops out, they will use that transformation. If not, they will use the ""distribution free"" Wilcoxon test. For this ad-hoc sequence of events, I cannot begin to hope how to calculate the calibration and power for a simple test of mean differences when the simple, stupid t-test would have sufficed. I suspect stupid acts like this can be linked mathematically to Hodge's superefficient estimation: estimators which are high power under a specific hypothesis we want to be true. Nonetheless, this process is <em>not</em> statistical because the false positive error rate has not been controlled.</p></li>
<li><p>The concept that trends can be ""discovered"" erroneously in any random set of data probably traces back to the well written article by Martin called <a href=""http://www.ncbi.nlm.nih.gov/pubmed/18788144"" rel=""nofollow"">""Munchaesen's Statistical Grid""</a>. This is a very illuminating read and dates back to 1984 before the golden calf of machine learning was born unto us as we presently know it. Indeed, a correctly stated hypothesis is falsifiable, but type 1 errors have grown to be much more costly in our data driven society than they ever were before. Consider, for instance, the falsified evidence of the anti-vaccine research that has led to a massive sequence of pertussis deaths. The results which spurned the public defenestration of vaccines was linked a <em>a single study</em> (which, although wrong, was neither confirmed by external research). There is an ethical impetus to conduct results and report honest-to-goodness strength of evidence. How strong is evidence? It has little to do with the p-value you obtain, but the p-value you said you would call significant. And remember, fudging your data changes the value of p, even when the final confirmatory test reports something different (often much smaller).</p></li>
<li><p>YES! You can clearly see in meta-analyses published by journals such as the Cochrane report that the distribution of test results looks more bimodal than noraml, with only positive and negative results making it into journals. This evidence is absolutely bonkers and confusing for anyone in clinical practice. If, instead, we publish null results (that come from studies <em>whose results we would have been interested in, regardless of what they come to be</em>), then we can expect meta-analyses to actually represent evidence that is meaningful and representative. </p></li>
</ol>
",2013-11-08 03:57:53.353
59127,5821.0,2,,9524.0,,,,CC BY-SA 3.0,"<p>There is a published a documentary about <a href=""http://www.imdb.com/title/tt2861842/"" rel=""nofollow"">Srinivasa Ramanujan</a> whose life, as we know, is tremendously interesting. However, the film is Indian and I haven't actually seen it. I recall an Indian math historian speaking about this film at our university colloquium several years ago. He boasted, ""Ben Kingsley was interested in depicting Ramanujan but was turned down for the role because he was only <em>half</em> Indian"". As a mixed race individual, I felt a mixture of anger and pity. The latter because they basically turned down the opportunity to make a movie that would attract anyone's attention.</p>
",2013-11-08 04:09:57.603
59128,23511.0,1,,,,Mixed Model t Test (Toothakerâ€™s t-Test),<hypothesis-testing><anova><mixed-model><spss><excel>,CC BY-SA 3.0,"<p>I'm trying to do a follow up test for a mixed model ANOVA, and I figured that I need do a Mixed Model t test also known as Toothaker's test. I just don't know how do it. I was wondering if anyone has an Excel sheet that can calculate this for me, because I know SPSS can't.</p>

<p>I'm doing a mixed model ANOVA and I found significant interactions between my within subjects variables and my between subjects variables. Notes from a previous year's stats course told me to use a mixed model t-test to examine these interactions, and we were given an Excel sheet to help us calculate it. However I lost this Excel sheet. </p>

<p>To clarify more I have two between subject conditions (drug/placebo) and two within subject conditions (memory tested at 2 different times). </p>

<p>In my notes the test is called Toothaker's Mixed Model t-test and is described as such ""Used when 1 (or more) within-subjects factor and 1 (or more) between subjects factor: Toothakerâ€™s mixed-model t test (pools MS error)""</p>

<p><a href=""http://en.wikipedia.org/wiki/Mixed-design_analysis_of_variance#Follow-up_tests"" rel=""nofollow"">Wikipedia</a> seems to describe a similar thing ""when there is a significant interaction between a between-subject factor and a within-subject factor, statisticians often recommended pooling the between-subject and within-subject MSerror terms.[2][page needed][citation needed] This can be calculated in the following way:
MSWCELL = SSBSError + SSWSError / dfBSError + dfWSError""</p>
",2013-11-08 04:45:17.397
59129,22082.0,1,,,,Comparison of average values of data sets,<distributions><density-function><mean><subset>,CC BY-SA 3.0,"<p>I am working with two data sets of unequal sample sizes of 998 and 857.
Average of first (998 samples), come out to be higher than the other dataset.
To my surprise, when I split my complete data into unequal halves; first into 803 and 195 samples  and other also into 819 and 38 samples.
then comparison of average values of 803 subset of first complete dataset with the average of 819 subset of second complete data set showed a reverse trend in their mean values.
Same reverse trend was observed with other subset of both the data set.</p>

<p>My question is that is it possible that if mean of total items in A>B, their two subsets showed reverse trend in their means i.e. mean of A1

<p>Is this because of unequal sample sizes???? or because of sample density distribution trend????? or both????</p>

<p>If this is possible also, then is there any way to explain this quantitatively??????</p>

<p>It would be really helpful, if anyone can help me on this..</p>
",2013-11-08 06:04:54.563
59130,10772.0,2,,9524.0,,,,CC BY-SA 3.0,"<p><a href=""http://www.imdb.com/title/tt0119217/?ref_=fn_al_tt_1"" rel=""nofollow"">Good Will Hunting</a> is also a classic. Discrete mathematics at MIT.</p>
",2013-11-08 06:21:30.950
59131,21029.0,2,,59129.0,,,,CC BY-SA 3.0,"<p>If you split your data into smaller samples, you can definitely have your means change. If you split your data by removing all the low points, then you immediately increase your mean.</p>

<p>The only time this will not be true is if all points are equal. Then removing points won't change the average.</p>

<p>Note that this does not have any significance! The fact that you took a subset does not prove anything about the overall distribution or trend. It just happened. If anything, it could imply that you incorrectly split your data.</p>
",2013-11-08 06:22:02.890
59132,10450.0,2,,59099.0,,,,CC BY-SA 3.0,"<p>Have a look at <a href=""http://rads.stackoverflow.com/amzn/click/1439824142"" rel=""nofollow"">'A First Course in Machine Learning,' Simon Rogers and Mark Girolami</a>.
There are many easy to follow step by step derivations of concepts that include calculus and linear algebra. Also, you can look at google book preview to see if it fits your needs.</p>
",2013-11-08 07:19:52.420
59163,22415.0,2,,59161.0,,,,CC BY-SA 3.0,"<p>Okay, so, my first idea was using Jaynes' $A_p$ distribution, in which case we can define $A$ = next draw will be red and $N_r$ = out of $N$ draws, $r$ were red. With an ignorant prior distribution $(A_p|X) = 1$, I get that</p>

<p>$$(A_p|N_rX) = (A_p|X)\frac{P(N_r|A_p)}{P(N_r|X)}$$</p>

<p>We know that</p>

<p>$$P(N_r|A_p) = \binom{N}{r}p^r(1-p)^{N-r}$$</p>

<p>And we can find</p>

<p>$$P(N_r|X) = \int^1_0(N_rA_p|X)dp = \int^1_0P(N_r|A_p)(A_p|X)dp = \int^1_0\binom{N}{r}p^r(1-p)^{N-r}dp$$</p>

<p>from which</p>

<p>$$P(N_r|X) = \frac 1 {N+1}, 0 \leq r \leq N$$</p>

<p>And then</p>

<p>$$(A_p|N_r) = (N+1)\binom{N}{r}p^r(1-p)^{N-r}$$</p>

<p>And that looks like my distribution. Is that correct?</p>
",2013-11-08 17:36:47.673
59133,594.0,2,,356.0,,,,CC BY-SA 4.0,"<p>You can't really even compare the two since the Kolmogorov-Smirnov is for a completely specified distribution (so if you're testing normality, you must specify the mean and variance; they can't be estimated from the data*), while the Shapiro-Wilk is for normality, with unspecified mean and variance.</p>

<p>* you also can't standardize by using estimated parameters and test for standard normal; that's actually the same thing.</p>

<p>One way to compare would be to supplement the Shapiro-Wilk with a test for specified mean and variance in a normal (combining the tests in some manner), or by having the KS tables adjusted for the parameter estimation (but then it's no longer distribution-free).</p>

<p>There is such a test (equivalent to the Kolmogorov-Smirnov with estimated parameters) - the Lilliefors test; the normality-test version could be validly compared to the Shapiro-Wilk (and will generally have lower power). More competitive is the Anderson-Darling test (which must also be adjusted for parameter estimation for a comparison to be valid).</p>

<hr>

<p>As for what they test - the KS test (and the Lilliefors) looks at the largest difference between the empirical CDF and the specified distribution, while the Shapiro Wilk effectively compares two estimates of variance; the closely related Shapiro-Francia can be regarded as a monotonic function of the squared correlation in a Q-Q plot; if I recall correctly, the Shapiro-Wilk also takes into account covariances between the  order statistics.</p>

<p>Edited to add: While the Shapiro-Wilk nearly always beats the Lilliefors test on alternatives of interest, an example where it doesn't is the <span class=""math-container"">$t_{30}$</span> in medium-large samples (<span class=""math-container"">$n&gt;60$</span>-ish). There the Lilliefors has higher power.</p>

<p>[It should be kept in mind that there are many more tests for normality that are available than these.]</p>
",2013-11-08 08:10:48.110
59134,21029.0,1,61392.0,,,Multiple resampling test/train dataset when choosing new models?,<regression><cross-validation><model-selection><resampling><type-i-and-ii-errors>,CC BY-SA 3.0,"<p>I have been reading several posts on testing multiple models on the same dataset, which can lead to problems controling type-1 errors. Mostly these posts have to do with data-mining on big datasets: <a href=""https://stats.stackexchange.com/questions/22502/how-to-draw-valid-conclusions-from-big-data/22639#22639"">How to draw valid conclusions from big-data</a> and  <a href=""https://stats.stackexchange.com/questions/20010/how-can-i-help-ensure-testing-data-does-not-leak-into-training-data/20023#20023"">ensuring testing data doesn't influence training</a></p>

<p>However, I know that this is done frequently. In fact, in my old work it was my job to find the best (logit) model given a dataset. In order to decide if a model is predictive or not, you have to validate against the test data. If the performance is poor, then you start from zero and create a new model. By the end you may have checked the test dataset dozens of times.</p>

<p>I was recently asked if multiple re-sampling of the dataset would be a possible solution. I wanted to say 'no,' but I don't actually know <em>why</em> this is bad. To give a specific example:</p>

<p>Suppose I am looking for the best linear regression given a dataset of 1,000 observations. I split the data into training and testing sets. I formulate a model which is not satisfactory on the testing sample. So, I <em>redistribute</em> the 1,000 observations into new training/test samples and attempt to find a new model. Each model will be trained and tested on their own specific training/test sample, which all come from the same 1,000 original observations.</p>

<p><strong>My question: Why is this incorrect? What are the problems that are created with this methodology?</strong></p>
",2013-11-08 08:23:59.613
59135,1959.0,1,,,,Does full subset selection suffer from the same handicaps as stepwise regression?,<regression><linear-model><feature-selection><stepwise-regression><regression-strategies>,CC BY-SA 3.0,"<p>Let's assume $p$ potential predictor variables $X_1,...,X_p$ and a single dependent variable $Y$.</p>

<p>Now I evaluate the performance of all possible linear models considering all possible combinations of predictor variables ($2^p-1$). The performance measure(s) could be pretty much any statistic but first comes to mind $R^2$, $F$-statistic and MSE. Based on them I select the ""best"" model <em>or</em> the top selection which I can check out more closely.</p>

<p>Intuitively I would assume this is a great idea&mdash;but I read around a bit and came across the infamous concept of ""stepwise regression"" and how it is considered useless by a lot of ( though apparently not all) statistically trained people. The reason seems to be that the assumed distribution underlying the involved statistics does not hold for the scenario.</p>

<p>But stepwise regression is usually described as a slightly different algorithm where you start with a model, adding and removing variables from the model based on a criterion for a statistic.</p>

<p>So my question is whether the approach I describe would also be a type of stepwise regression and hence be handicapped by design. On the latter part (if it is SwReg) I would be interested in clarification on where the handicap comes into play and whether it is possible to amend it.</p>
",2013-11-08 09:09:18.163
59136,9554.0,2,,59114.0,,,,CC BY-SA 3.0,"<p>In order to use a mixture of Gaussians for your problem, you have to assume that your three measurements are multivariate normal. In that setting, you have many measurements of a mixture of 3 dimensional normals, generated by k different underlying densities. </p>

<p>You could start by setting k=3, one for ""/ shape"", one for ""/\ shape"", and one for ""-- shape"" you mentioned having observed. This will allow you to model the underlying mixture distribution, estimate the covariance of each cluster and of course, classify all measurements. Before comparing them, I would also think about subtracting the mean (and potentially also scaling), since you probably don't care about the general level of an individual, but about the change.</p>

<p>Please provide some details where you found the information about the ""quadratic curves"" you are mentioning, or why you think the Gaussian mixture would classify your measurements using a linear boundary, if you wish to follow up on that. Based on my understanding, the results are labeled based on the likelihood of being generated by a particular density (one of the k you started with). I am not aware of any condition that would restrict the boundary to be a polynomial in the general approach, or even suggest it could be so for that matter.</p>

<p>EDIT (in response to your comment): </p>

<p>The order in your measurements will be captured, since you will encode the morning,afternoon, evening measurements for each individual as a 3dimensional, multivariate normal, with unknown mean and unknown covariance. Imagine there are 3 such Gaussians - one for each characteristic shape of the measurements. </p>

<p>A gaussian mixture model is called a ""mixture"" model since it presumes that the measurements come from a probability, which is some weighted combination of the three underlying Gaussians. Each measurement can have a different weighting.</p>

<p>When you fit the model, you need to infer the parameters of all three Gaussians and simultaneously, infer the responsibility (weighting) of each Gaussian for having generated a particular measurement.</p>

<p>If you find this concept too hard to understand, or if my explanation is completely incomprehensible I would suggest you try a much simpler and much more straightforward approach: k-means.</p>

<p>You simply take the $3xN$ table of data, stick it in a k-means function and set the number of clusters to be 3.
I have written a snippet in R to illustrate:</p>

<p>First I generate some data along the lines of your description.</p>

<pre><code>library(MASS)
library(clue)
library(mclust)    

# Generate training data
Sigma &lt;- diag(3)
mu1 &lt;- c(3,0,3)
mu2 &lt;- c(0,0,0)
mu3 &lt;- c(0,3,0)    

group1 &lt;- mvrnorm(n = 5, mu1, Sigma)
group2 &lt;- mvrnorm(n = 5, mu2, Sigma)
group3 &lt;- mvrnorm(n = 5, mu3, Sigma)    

# Generate test data
new_measurements &lt;- rbind(mvrnorm(n = 2, mu1, Sigma), mvrnorm(n = 2, mu2, Sigma),mvrnorm(n = 2, mu3, Sigma))    
</code></pre>

<p>Here is what it looks like: (I labeled the measurements according to the Gaussian which generated them purely for visual convenience. In your case, your data is not labeled.)</p>

<pre><code># Plot training data and cluster
matplot(t(group1),type=""b"",col=2,lty=1,pch=1)
matplot(t(group2),type=""b"",col=3,lty=1,pch=1,add=T)
matplot(t(group3),type=""b"",col=4,lty=1,pch=1,add=T)
</code></pre>

<p><img src=""https://i.stack.imgur.com/NGbyK.png"" alt=""Simulated measurements""></p>

<p>Now we can cluster the data using k-means, by setting the number of cluster to 3, since that is your intuition. (We could also try to investigate what is the right number of clusters using the data.)</p>

<pre><code># Cluster
data &lt;- rbind(group1,group2,group3)
fit &lt;- kmeans(scale(data), 3)
# get cluster means 
cluster_means &lt;- aggregate(data,by=list(fit$cluster),FUN=mean)    
</code></pre>

<p>And we get the mean values for each cluster, which we can then use to make predictions for new measurements.</p>

<pre><code># Predict cluster
fit$centers
predicted_clusters &lt;- cl_predict(fit,scale(new_measurements))
predicted_clusters
</code></pre>

<p>We get the means of the clusters,</p>

<pre><code>        [,1]       [,2]       [,3]
1 -0.8618375 -0.6464979 -0.5279966
2 -0.3605748  0.9793615 -0.5128992
3  1.1221597 -0.6580355  1.0378763
</code></pre>

<p>as well as the predicted cluster for the our new measurements.</p>

<pre><code>Class ids:
[1] 3 3 1 1 2 2
</code></pre>

<p>As you can see, k-means correctly predicted to which cluster they belong to, though the k-means estimations of the means are relatively far off. The third cluster corresponds to our first simulated Gaussian with means (3,0,3).</p>

<p>Lastly, we can plot the new data and the estimated means.</p>

<pre><code>matplot(t(cluster_means[,-1]),type=""b"",col=3,lty=1,pch=20,lwd=2,ylim=c(-1,6))
matplot(t(scale(new_measurements)),type=""l"",col=1,lty=1,add=T)
legend(""topright"",legend=c(""New data"",""k-means Cluster Means""),lty=1,col=c(1,3,4))
</code></pre>

<p><img src=""https://i.stack.imgur.com/UPTfL.png"" alt=""New data and k-means Cluster means""></p>
",2013-11-08 10:33:53.120
59137,23519.0,1,,,,Missing data SPSS paired samples t-test,<spss><missing-data><paired-data>,CC BY-SA 3.0,"<p>I have approximately 20% data missing in my sample (n=3215). I aim to assess the pre-post differences on a psychometric scale. Especially post measures are missing because of follow-up issues. What to do? Should I exclude cases listwise/pairwise or replace missings with series mean or linear interpolation?</p>

<p>I don't have the SPSS Multiple Imputation/Missing values module installed on my SPSS. </p>
",2013-11-08 11:17:59.117
59138,23520.0,2,,16366.0,,,,CC BY-SA 3.0,"<p>We know that parameters of LDA are estimated through Variational Inference. So </p>

<p>$\log p(w|\alpha, \beta) = E[\log p(\theta,z,w|\alpha,\beta)]-E[\log q(\theta,z)] + D(q(\theta,z)||p(\theta,z))$. </p>

<p>If your variational distribution is enough equal to the original distribution, then $D(q(\theta,z)||p(\theta,z)) = 0$. So, $\log p(w|\alpha, \beta) = E[\log p(\theta,z,w|\alpha,\beta)]-E[\log q(\theta,z)]$, which is the likelihood.</p>

<p>$\log p(w|\alpha, \beta)$ approximates to the likelihood you got from the Variational Inference.</p>
",2013-11-08 11:23:00.013
59151,20795.0,1,,,,What is the minimum historical data/sample data required for a time series forecasting analysis?,<time-series><forecasting><arima><statistical-power><exponential-smoothing>,CC BY-SA 3.0,"<p>Are there any statistical power analysis/sample size deteminations methods for time series data analysis/forecasting?</p>

<p>For example if I have time series of 30 data points, how can I with <em>confidence</em> use a particular statistical methods like exponential smoothing or arima for predict the future ?</p>

<p>I have seen in some textbooks that have a cursary mention on historical data points required for ARIMA would be 50 or 60. But I have not encountered a formal approach on how much history is required for a a particular time series forecasting method.</p>

<p>I did a thorough search on major time series textbooks and the internet, I'm unable to find any literature on this topic. Any guidance would be helpful.</p>
",2013-11-08 15:12:04.177
59139,503.0,2,,59124.0,,,,CC BY-SA 3.0,"<p>Instead of using p-values to assess claims we should follow Robert Abelson's advice and use the MAGIC criteria:</p>

<pre><code>Magnitude
Articulation
Generality
Interestingness
Credibility
</code></pre>

<p>For more on Abelson see <a href=""http://www.statisticalanalysisconsulting.com/book-review-statistics-as-principled-argument-by-robert-abelson/"">my review of his book</a></p>

<p>And we should be concentrating on effect sizes, not p-values in statistical output (with the possible exception of some sorts of data mining, on which I am not expert at all). And effect sizes are to be judged in context:</p>

<pre><code>1 in 1000 pairs of pants gets the wrong size label - not a big deal
1 in 1000 airplanes are defective in a way that leads to crashes - a big deal
1 in 1000 nuclear reactors is defective in a way that leads to meltdown - uh oh
</code></pre>

<p>A statistician/data analyst should not be some odd person, used like a black box into which data is put and out from which p values are gotten; he/she should be a collaborator in research designed to make a reasonable argument about the meaning of some set of data in the context of some field, given the current theories (or their lack) and current evidence (or lack of same). </p>

<p>Unfortunately, this approach requires thought on the part of the substantive researchers, the data analyst and whoever reviews the results (be it a pointy haired boss, a dissertation committee, a journal editor or whoever). Oddly, even academics seem averse to this sort of thought.</p>

<p>For more on my views, here is an <a href=""http://www.sciences360.com/index.php/the-insignificance-of-statistical-significance-13603/"">article I wrote</a> that got published in Sciences360.</p>
",2013-11-08 11:42:26.687
59140,23476.0,1,,,,Calibration of Cox regression survival analysis,<probability><survival><cox-model><calibration><rms>,CC BY-SA 3.0,"<ol>
<li><p>To perform calibration of a Cox regression model (i.e. assessing for the agreement between the predicted and the observed outcome), what is the best method to present the accuracy of the model in predicting the actual event? </p></li>
<li><p>As far as I understand, we can calculate the actual outcome probability by observing the number of events that occurred in a number of subjects with similar/same predicted probability from the Cox model. To perform the above calculation, do we stratify the predicted risk into several groups (&lt;15%, 15-30%, 30-45% etc.), and within each risk group we use the number of subjects as the denominator for the calculation of actual outcome?  </p></li>
<li><p>What method do we use to compare the predicted outcome with the actual outcome? Is it good enough if we simply present the predicted and actual risk% in each risk group in table format? Can <a href=""http://cran.r-project.org/web/packages/rms/index.html"" rel=""noreferrer""><code>rms</code></a> package in R do all calibrations for you?</p></li>
<li><p>Can we use <code>pec::predictSurvProb()</code> to give the absolute risk of event for each individual? Can we specify the time point for the risk/hazard function for each individual to be at the ENDPOINT of follow up?</p></li>
<li><p>When interpreting the results, do we use the <em>mean</em> follow up period (in years) as the time point on which the predicted risk and actual risk are based? (E.g. Individual A has 30% risk of event at 6.5 years (mean follow up period))</p></li>
<li><p>Is the goodness-of-fit test for Cox regression (Gronnesby and Borgan test) simply a means for calibration for cox regression? Or does it mean something else?  </p></li>
<li><p>To compare models with net reclassification, how many subjects and outcomes do we need for such method to become valid?</p></li>
</ol>
",2013-11-08 12:36:14.390
59141,12787.0,1,,,,Confidence intervals and central estimates for a functional of an estimated function with uncertain parameters,<bayesian><optimization><posterior><credible-interval><bayesian-optimization>,CC BY-SA 3.0,"<p>I've got a problem that is leading me to dip my toes into Bayesian stats, and I've got a question about confidence (or, I suppose, credible) intervals:</p>

<p>Say you want to know how $X$ maps to $y$.  You fit a model $y=f(X)+\epsilon$.</p>

<p>Then, you want to optimize $X$ to get the best $y$: 
$$y_{max} = argmax_X(\hat{f}(X),s.t. \text{whatever constraints}) $$</p>

<p>This gives you the model's best estimate of the optimal $X$ for getting the biggest $y$.</p>

<p>But obviously $\hat{f}(X)$ is uncertain.  If you take a Bayesian standpoint that $\beta$ is distributed multivariate normal, you can take samples from it, which gives new coefficients (see, for example, <a href=""http://people.bath.ac.uk/sw283/mgcv/check-select.pdf"" rel=""nofollow"">this</a>).  Taking many samples, using them to pick new optimal values of $X$, one gets a distribution of $y_{max}$ that reflects uncertainty in $\hat{f}(X)$.</p>

<p>Here is the problem:  the central estimate of $y_{max}$ (i.e.: optimizing based on the parameter estimates of the fitted model) is not necessarily the mean or the median of the $y_{max}$ distribution that one gets when optimizing the functions based on the posterior draws.</p>

<p>So what should I do with the ""central estimate""?  Which estimate should I consider to be my ""best guess"" of the value of $y_{max}$?  Should it be $y_{max}$ at the (ML) parameter estimates?  Should it be the mean or median of the posterior simulations of $y_{max}$?</p>

<p>I don't know whether there is a right answer here:  maybe this is a somewhat of a philosophical question?  (Or am I making some relatively fundamental mistake, which makes my whole question moot?  If so, I'd be grateful for replies that point it out.)</p>
",2013-11-08 13:14:33.773
59142,449.0,2,,59137.0,,,,CC BY-SA 3.0,"<p>There is nothing you can do without more data than just a list of pairs of numbers with some missing. It might be useful to really consider what getting all of these missing values might mean though. I'm doubtful you will gain much at all in going through the process of trying to make the imputation.</p>

<p>Consider the mathematical impact of this on your standard error, which is controlled by the square root of N. If you had all 3215 observations then the sqrt of N is 57. If you lose 20% it's 51. That's only a difference of about 11% in terms of what your standard error will be and consequently, smaller impact on your <em>t</em> than you might think adding in ~643 subjects would yield. Your effect, correlation, and variance estimates should all be pretty stable by the time the N is that high so those won't change much at all, and not in any predictable direction.</p>

<p>In other words, getting all of this data could change your <em>t</em> from 2 to 2.2.  Given that any imputation is fraught with complication in explanation and limitations in your conclusions, is that worth it to you? </p>

<p>A further thing to consider is that if you don't have a significant effect already, with the N you have, but you believe the effect really exists, then you have a really small effect. Let's say you currently have a non-significant <em>t</em>. If that's true then Cohen's D is under 0.04. Is that a value that's meaningful in light of your theory?</p>

<p>Of course, this assumes there's no bias from the missing data. That could change things. In that case then perhaps you should strive to impute but you'd need some argument to support the idea. That would require knowledge about those subjects or hypotheses that are supported externally that strongly support an expectation of bias. If you do have a strong reason to expect that your missing subjects are part of a group that will bias the data then perhaps that should be a variable in the analysis.</p>
",2013-11-08 13:33:20.420
59143,23524.0,1,,,,Comparing two ordinal regression models,<regression><ordinal-data><goodness-of-fit>,CC BY-SA 3.0,"<p>I am trying to find out if I can somehow assess if one model fits my data significantly better than another model? They are both ordinal regressions and we have introduced an additional interaction term in the second model. Now we want to know if the model has improved. Do I only look at the model fit indices? How do I know if the model has significantly improved?</p>
",2013-11-08 14:00:25.287
59152,9408.0,2,,59147.0,,,,CC BY-SA 3.0,"<p>The major difference between time series data and cross-section data is that the former focuses on results gained over an extended period of time, often within a small area, whilst the latter focuses on the information received from surveys and opinions at a particular time, in various locations, depending on the information sought. Moreover, gdp in one time lag is likely to be dependent on the next time lag and so on. In a cross sectional point of view, you ignore this correlation. For your problem, I guess you will be trying to see how gdp is being affected by employment over time so that you can also estimate the future scenario. </p>
",2013-11-08 15:17:06.703
59144,2161.0,1,59148.0,,,Can and should you use data from repeated responses in a linear mixed-effects model?,<r><mixed-model>,CC BY-SA 3.0,"<p>When you collect data from participants in an experiment, sometimes you can collect repeated responses for <em>the same condition</em>, e.g., in R:</p>

<pre><code>set.seed(2012) # keep the example the same each time.

data.full &lt;- data.frame(id=gl(10, 4),
                        condition=gl(2, 40),
                        response=c(rnorm(40), rnorm(40, 1)))
head(data.full)

# Output:
#   id condition    response
# 1  1         1 -0.77791825
# 2  1         1 -0.57787590
# 3  1         1  0.66325605
# 4  1         1  0.08802235
# 5  2         1  1.25707865
# 6  2         1 -0.62977450
</code></pre>

<p>To analyse this (i.e. does condition predict response) I would normally take the mean response for each participant, for each condition. I would do this on the basis that we are supposed to be generalizing from a sample to a population, i.e. there should be one 'estimate' response from each participant for each condition, and the collection of these single responses (for each condition) is our sample, then we do an analysis which generalizes to the population.</p>

<p>I would transform the data e.g. like this:</p>

<pre><code>library(plyr)
data.means &lt;- ddply(data.full, .(id, condition),
                    summarize,
                    mean.response=mean(response))
head(data.means)

# Output:
#   id condition mean.response
# 1  1         1    -0.1511289
# 2  1         2     0.8658770
# 3  2         1     0.1510842
# 4  2         2     0.0129323
# 5  3         1     0.1857577
# 6  3         2     0.9859697
</code></pre>

<p>And then proceed with the within-subjects analysis (note the same process would apply if there were more conditions or a 2x2 design etc.), e.g.:</p>

<pre><code>aov1 &lt;- aov(mean.response ~ condition + Error(id/condition), data=data.means)
summary(aov1) # F = 4.2, p = .07, not significant
</code></pre>

<p>However, I've been told that with linear mixed-effects models, you can include all the underlying data on the basis that the lme models can include correlated data. My understanding was that they could include correlated data meant they could include responses from the same participants (within-subjects effects modelled as random effects), not that you could include the underlying data that gives the participant response estimate.</p>

<p>My question is, can you include the underlying data collected from the multiple responses of each participant in the <em>same condition</em>, i.e. can you do this:</p>

<pre><code>library(nlme)
lme1 &lt;- lme(response ~ 1, random= ~ 1|id/condition, data=data.full, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 3.19, p = .07, not significant
</code></pre>

<p>Or should you do this:</p>

<pre><code>lme1 &lt;- lme(mean.response ~ 1, random= ~ 1|id/condition, data=data.means, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 5.25, p = .02, significant
</code></pre>

<p>Which is the correct approach?</p>
",2013-11-08 14:01:43.553
59145,21762.0,2,,59129.0,,,,CC BY-SA 3.0,"<p>The answer is ""Yes"". This is Simpson's paradox  applied to mean differences instead of odds ratios. You can read Wiki's article (<a href=""http://en.wikipedia.org/wiki/Simpson%27s_paradox"" rel=""nofollow"">http://en.wikipedia.org/wiki/Simpson%27s_paradox</a>) to understand the mechanisms behind it. It's a projection problem: If you only see a two dimensional projection of a three dimensional object, you can get quite a wrong impression about the whole picture. In balanced settings (equal group sizes), this is not possible.</p>

<p>Consider, for instance, the following simple setting:</p>

<ul>
<li>$A_1$ consists of 99 times the value 1</li>
<li>$A_2$ consists of the value 100 </li>
<li>$B_1$ consists of the value -9</li>
<li>$B_2$ consists of the value 99</li>
</ul>

<p>The average of $A = A_1 \cup A_2$ is about 2 and thus much smaller than the average 45 of $B = B_1 \cup B_2$. On the other hand, the average 1 of $A_1$ is larger than the average -9 of $B_1$. Similarly, the average 100 of $A_2$ is larger than the average 99 of $B_2$.</p>
",2013-11-08 14:02:03.623
59146,16665.0,1,,,,Understanding formula for the standardized selection ratio (SSR),<interaction><biostatistics>,CC BY-SA 3.0,"<p>Manyl et al defined an index called Standardized Selection Ratio (SSR). <a href=""http://books.google.ch/books?hl=fr&amp;lr=&amp;id=hNy8aM8HmrwC&amp;oi=fnd&amp;pg=PR9&amp;dq=Statistical+design+and+analysis+for+field+studies&amp;ots=e3F4Z714o0&amp;sig=ULzWk1pfmm2vkrbb6nK2YKEAYms#v=onepage&amp;q=standardized%20selection%20ratio&amp;f=false"" rel=""nofollow"">Here</a> is the source. I don't quite understand how this index is calculated and how the p.values are calculated. Can you help me. The interesting part starts at page 40 (and ends few pages after) of the book.</p>

<p>Below is an example of an article where this index was used. They described the SSR index but it doesn't make much sense to me either.</p>

<hr>

<p>On <a href=""http://link.springer.com/article/10.1007/s10641-010-9606-0#page-1"" rel=""nofollow"">this article</a> they used the Standardized Selection Ratio (SSR) in order to know what is the preferred host (anemone) of anemonefish.</p>

<p>Here is a quotation coming from their methods:</p>

<blockquote>
  <p>The â€œpreferred hostâ€ of anemonefish was assessed calculating the â€œStandardized
      Selection Ratio (SSR)â€ (values between 0 and 1) (Manly et al. 1993). Manlyâ€™s standardized
      selection ratio represents the probability that an individual will use a particular habitat
      type, taking into account the different resource availability. For each anemonefish species ($i$) inhabiting an anemone species ($j$), SSR was calculated as:</p>
</blockquote>

<p>$$SSR = \frac{w_i}{\sum{w_j}} $$</p>

<p>where $w_i = \frac{o_i}{p_j}$</p>

<blockquote>
  <p>$o_i$ is the relative frequency of the anemonefish species $i$ and $p_j$ the relative frequency
      of the anemone species $j$. Higher values of SSR indicate a strong preference for the
      selected resource. The Log- Likelihood statistic ($\chi^2$L) (Manly et al. 1993) was used to
      check the significance of the observed distribution under a null hypothesis of a random
      host choice.</p>
</blockquote>
",2013-11-08 14:09:38.360
59147,23525.0,1,,,,Time series as cross-sectional data,<r><regression><time-series>,CC BY-SA 3.0,"<p>I have time series, for example, gdp and unemployment(<code>unemp</code>), freq= 4. </p>

<p>What if I interpret it as cross-sectional data and do cross-sectional analysis instead of time series? </p>

<p>My task is to test how unemployment affects gdp. </p>

<p>Is it allowed to do that kind of analysis? </p>

<p>Do the coefficients in the model <code>lm(gdp~unemp)</code> have an economic explanation?</p>
",2013-11-08 14:46:04.277
59148,449.0,2,,59144.0,,,,CC BY-SA 3.0,"<p>Not only <em>can</em> you use the repeated measure, you <em>should</em>. You'll note that the mixed model doesn't dramatically reduce the standard error when you include lots of repeated responses. That's a hint that it's at least not doing the traditionally wrong thing. You don't have to identify these multiple responses any special way in the formula.</p>
",2013-11-08 14:55:33.263
59149,23523.0,1,,,,Detecting outlier cash movements,<time-series><outliers><finance>,CC BY-SA 3.0,"<p>If I'm watching a series of accounts for transactions going in and transactions going out, I want to notice unusually large or transactions for any particular account on any particular day.</p>

<p>So if account A typically moves a few hundred dollars and one day moves five thousand dollars, that's a clear outlier.
If account B typically moves a few million dollars in or out and one day moves 20 million dollars, that's a clear outlier.  </p>

<p>What I'd like to do is present a measure that should highlight outliers - I was thinking number of standard deviations versus a population of the rolling last 60 days, but I'm wondering if that's correct. I'm checking to see if it's a gaussian distribution, but are there better ways to hit what I'm looking for?</p>

<p>I think this poses a different set of questions than <a href=""https://stats.stackexchange.com/questions/1223/robust-outlier-detection-in-financial-timeseries"">Robust outlier detection in financial timeseries</a>. </p>
",2013-11-08 15:01:45.843
59150,16474.0,2,,59143.0,,,,CC BY-SA 3.0,"<p>The equivalent null hypothesis is that the coefficient for the interaction term is 0. If that hypothesis is true, then your model with the interaction effect is exactly the same as your model without the interaction effect, and adding the interaction effect has thus added nothing to the model fit. </p>

<p>In your output there will be next to the interaction term a test for whether or not that coefficient is 0. So that alone will be enough to answer your question. This is a <a href=""http://en.wikipedia.org/wiki/Wald_test"" rel=""nofollow"">Wald test</a>. If you insist on comparing models you can do a <a href=""http://en.wikipedia.org/wiki/Likelihood-ratio_test"" rel=""nofollow"">likelihood ratio test</a>. The Wald test and likelihood ratio test will give the same answer in large samples.</p>
",2013-11-08 15:04:33.947
59162,23529.0,1,,,,Do interactions in mixed designs inflate the main effect of fixed factor?,<interaction><mixed-model>,CC BY-SA 3.0,"<p>Explain in clear way why in a mixed design the presence of an interaction between a random and a fixed factor inflates the estimate of the main effect of the fixed factor.</p>
",2013-11-08 17:22:49.013
59153,2149.0,2,,59149.0,,,,CC BY-SA 3.0,"<p>What you have to do is to develop a reasonable model that may incorporate parameters reflecting day-of-the-week , changes in day-of-the-week parameters, week-of-the-year, month-of-the-year, week-of-the-month, day-of-the-month and activity around known events like holidays. The model should detect and incorporate level shifts and local time trends while being robust to pulses i.e. unaffected. The model should detect both parameter changes and changes in the error variance and incorporate remedies. We have been doing this for banking clients (atm machines and elsewhere) since 2002 using AUTOBOX  (<a href=""http://www.autobox.com"" rel=""nofollow"">http://www.autobox.com</a>) a piece of software that I have helped develop. If you wish to post your data ( or a coded version of your data ) please do so and I will submit to AUTOBOX in order to analyze it and then I will post the results. If you don't wish to post your data then contact me at my email address. At a minimum you might want to look at <a href=""http://www.autobox.com/cms/index.php/afs-university/intro-to-forecasting/doc_download/53-capabilities-presentation"" rel=""nofollow"">http://www.autobox.com/cms/index.php/afs-university/intro-to-forecasting/doc_download/53-capabilities-presentation</a> as Slide 44-55 speaks directly to your problem.</p>
",2013-11-08 15:26:51.573
59154,23195.0,1,,,,Suggestion for doing research in data mining and machine learning,<machine-learning>,CC BY-SA 3.0,"<p>I am a fresh graduate student. I want to do research in machine learning and data mining. There is no professor in our department doing this! </p>

<p>I want to try to do this by myself, at least for a while. But I don't where should I start. What books or review papers should I read at the beginning? </p>
",2013-11-08 15:28:34.533
59155,306.0,2,,59154.0,,,,CC BY-SA 3.0,"<p>First of all, learn a language like R or python where you can implement stuff once you read them. then finish the book called <a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"" rel=""nofollow"">Elements of Statistical Learning</a>. Then pickup the chapter you liked the most and start googling. you will find what you are looking for.</p>
",2013-11-08 15:37:27.850
59156,23404.0,1,,,,Interpretation of log-level difference-in-differences specification,<regression><interpretation><difference-in-difference>,CC BY-SA 3.0,"<p>When I run a standard difference in differences specification with a log-transformed dependent variable like:</p>

<p>$$\log(Outcome_{it}) = \beta_1 + \beta_2Treat_i +\beta_3Post_t +\beta_4(Treat\times Post)_{it} +\varepsilon_{it}$$</p>

<p>How do I interpret the coefficient $\beta4$?</p>

<p>Normally in log-level models, I would use the approximation $\%\Delta y= 100\beta_4\Delta x$, but this approximation is only valid for small changes in $x$ (and small $\beta$). In my case $x$ is a dummy variable and as such either $0$ or $1$ ($Treat\times Post$). Is this change considered to be 'small' or do I have too use  $\%\Delta y=e^{\beta-1}$
for interpreting the coefficient?</p>
",2013-11-08 16:03:25.230
59157,2352.0,2,,9524.0,,,,CC BY-SA 3.0,"<p>The documentary about Andrew Wiles proof of Fermat's Last Theorem is fantastic:
<a href=""http://www.pbs.org/wgbh/nova/proof/"" rel=""nofollow"">http://www.pbs.org/wgbh/nova/proof/</a> </p>

<p>Available on youtube:
<a href=""http://www.youtube.com/watch?v=7FnXgprKgSE"" rel=""nofollow"">http://www.youtube.com/watch?v=7FnXgprKgSE</a></p>
",2013-11-08 16:12:46.300
59158,22644.0,2,,59154.0,,,,CC BY-SA 3.0,"<p>I'd recommend taking a relevant course at <a href=""https://www.coursera.org"" rel=""nofollow"">Coursera</a>. There you can explore a myriad of topics with a good dose of flexibility. Just do a search there for relevant terms: machine learning, statistics, data science, data mining, etc.</p>

<p>You have an instructor and peers you can interact with, you get assigned homework (that will likely be graded) and of course, you get relevant reading material and recommendations. These are well prepared courses that are available for free so take advantage of them.</p>
",2013-11-08 16:12:49.973
59159,23526.0,1,,,,How to determine a good simulation time?,<simulation><networks>,CC BY-SA 4.0,"<p>So I'm doing network simulations, and I'd like to know how long does each simulation run needs to be.
My network is quite simple: its composed of <strong>multiple</strong> M/M/1/H queues (Markovian processes + finite waiting queues):</p>

<ul>
<li>clients arrive at some node(s), are put into waiting queues</li>
<li>each node process one client at a time</li>
<li>a processed client is sent to another node, or goes out of the system</li>
<li>all random processes follow a Poisson distribution</li>
</ul>

<p>The finite waiting queues prevents it of being a Poisson process: the output of a node is no longer Poisson, so neither will be the inputs of the nodes after it.</p>

<p>I thought I could try to plot the variance and mean of the waiting time (inside the queues) and see when it becomes ""stable""?
Would that be a good solution? How else could I do that?    </p>
",2013-11-08 16:15:51.660
59160,5821.0,2,,59140.0,,,,CC BY-SA 3.0,"<ol>
<li><p>Cox models do not predict outcomes! ""Best"" methods depend on whether you obtain a risk score (as with Framingham) or absolute risk (as with Gail Breast Cancer Risk). You need to tell us exactly what you're fitting</p></li>
<li><p>With absolute risk prediction, you can split groups according to their risk deciles and calculate proportions of observed vs. expected outcome frequencies. This is basically the Hosmer Lemeshow test. But, in order to use this test, you need to have an absolute risk prediction! You cannot, say, split the groups by risk <em>score</em> deciles and use the empirical risk as the risk prediction, this strips off too much information and leads to some counter intuitive results.</p></li>
<li><p>The bioconductor package has a suite of tools related to ROC analyses, predictiveness curves, etc.</p></li>
<li><p>Nowhere in Ulla's package is mention made of estimating smoothed baseline hazard estimates. This is necessary to obtain risk prediction from survival models... because of censoring! <a href=""http://jnci.oxfordjournals.org/content/81/24/1879.short"" rel=""noreferrer"">Here's an example of that method being applied</a>. I would accept no less from the package.</p></li>
<li><p>No, don't use mean follow up. You should report total person years follow-up, along with censoring rate, and event rate. The Kaplan Meier curve kinda shows you all of that.</p></li>
<li><p>I'm sure Sir David Cox is not fond of G&amp;B's test. The power of the Cox model is that it can give consistent <em>inference</em> without necessarily having predictive accuracy: a tough concept for many to grasp. Tsiatis' book ""semiparametric inference"" has a lot to say about this. However, if you aim to take the Cox model one step further and create predictions from it, then I think the G&amp;B test is very good for that purpose.</p></li>
<li><p>Reclassification indices are proportions of individuals being shuffled into different (more discriminating) risk categories comparing two competing risk prediction models (see Pencina). It's important to realize (Kerr 2011) that you can calculate confidence intervals for this value... <em>not</em> using the bootstrap (or any limit theory treating the model as fixed) but using the <em>double</em> bootstrap (bootstrap sample, refit model, bootstrap sample <em>again</em>, calibrate models). </p></li>
</ol>
",2013-11-08 16:47:04.433
59161,22415.0,1,,,,Urn probability function,<probability><conditional-probability>,CC BY-SA 3.0,"<p>Suppose I have an urn with an infinite number of balls which can be either red or white. I do not know what the proportion of each colour is, but I <em>do</em> know it's a fixed proportion. After drawing $N$ balls, I have observed $r$ red ones and $w$ white ones.</p>

<p>I believe the probability that I will observe a red ball on the next draw is given by Laplace's Law of Succession, $\frac{r+1}{N+2}$. However, how sure should I be of that? That is, before I drew any balls, I believed any proportion other than $0$ or $1$ was the true one. After I drew those $N$ balls, what should be my estimate pdf over the possible values for the proportion of red balls in the urn?</p>
",2013-11-08 17:11:12.443
59164,23414.0,2,,59134.0,,,,CC BY-SA 3.0,"<p>The concept that an out-of-bag estimate may be useful in choosing a model is standard practice and sounds reasonable.</p>

<p>I think as fair number of people will get hung up on the practical issues with your method: (1) What is the motivation? Or is this just theoretical? Seems like this would be far more complicated that just using standard model building approaches. 
(2) The model building process you describe isn't how I many might use validation. Usually use it either to assess if one has overfit the data or to compare completely different modelling approaches. I'm not convinced that the use of validation as part of an iterative model building process makes sense for a logistic model, though this practice may be common in other models (or even built into the model like MARS/EARTH)
(3) You're using split sample validation in a fairly small sample so the estimates are likely to be unreliable. You may want to increase n to 10,000 or 20,000 to get better answers to the question. 
(4) As you start making adjustments to your method as a result of (3) above you'll find you're describing LOOCV or K-fold validation. </p>
",2013-11-08 17:43:15.263
59165,13303.0,1,,,,Robustly standardize residuals in MM regression,<regression><outliers><residuals><robust>,CC BY-SA 3.0,"<p>Does anyone know how we can robustly standardize the residuals in MM regression?
First we perform MM regression and then obtain the residuals: how can we robustly standardize the residuals obtained from MM regression? I have found the method for least median squares (LMS) and least trimmed mean squares (LTS) in which the scale of the errors is estimated using a formula and then the residuals will be divided by that estimated scale. But for MM regression I could not find a formula for estimating the scale of the errors in order to standardize the residuals.</p>
",2013-11-08 18:03:57.370
59166,3993.0,2,,57508.0,,,,CC BY-SA 3.0,"<p>I have discovered that the regularity I described in my question has in fact been written about by several authors in the literature on Design of Experiments (DoE). It has been called the ""hierarchical ordering principle"" and also sometimes the ""sparsity-of-effects principle.""</p>

<p>In the chapter on fractional factorial designs in Montgomery (2013, p. 290), he writes:</p>

<blockquote>
  <p>The successful use of fractional factorial designs is based on three key ideas:</p>
  
  <ol>
  <li><em>The sparsity of effects principle</em>. When there are several variables, the system or process is likely to be driven primarily by some of the main effects and low-order interactions.</li>
  </ol>
  
  <p>...</p>
</blockquote>

<p>Wu &amp; Hamada (2000, p. 143) instead call this the ""hierarchical ordering principle"", and use the phrase ""sparsity of effects"" to refer to a related but distinct observation:</p>

<blockquote>
  <p>Three fundamental principles for factorial effects:</p>
  
  <p><em>Hierarchical ordering principle</em>: (i) Lower order effects are more likely to be important than higher order effects, (ii) effects of the same order are likely to be equally important .</p>
  
  <p><em>Effect sparsity principle</em>: The number of relatively important effects in a factorial experiment is small.</p>
  
  <p>...</p>
</blockquote>

<p>Li, Sudarsanam, &amp; Frey (2006, p. 34) give two possible explanations for why hierarchical ordering should tend to occur. First they suggest that it is ""partly due to the range over which experimenters typically explore factors"":</p>

<blockquote>
  <p>In the limit that experimenters explore small changes in factors and to the degree that systems exhibit continuity of responses and their derivatives, linear effects of factors tend to dominate. Therefore, to the extent that hierarchical ordering is common in experimentation, it is due to the fact that many experiments are conducted for the purpose of minor refinement rather than broad-scale exploration</p>
</blockquote>

<p>They next suggest that it is ""partly determined by the ability of experimenters to transform the inputs and outputs of the system to obtain a parsimonious description of system behavior"":</p>

<blockquote>
  <p>For example, it is well known to aeronautical engineers that the lift and drag of wings is more simply described as a function of wing area and aspect ratio than by wing span and chord. Therefore, when conducting experiments to guide wing design, engineers are likely to use the product of span and chord (wing area) and the ratio of span and chord (the aspect ratio) as the independent variables</p>
</blockquote>

<p><strong>References</strong></p>

<ul>
<li>Li, X., Sudarsanam, N., &amp; Frey, D. D. (2006). Regularities in data from factorial experiments. Complexity, 11(5), 32-45.</li>
<li>Montgomery, D. C. (2013). Design and analysis of experiments (Vol. 8). New York: Wiley.</li>
<li>Wu, C. J., &amp; Hamada, M. S. (2000). Experiments: planning, analysis, and optimization (Vol. 552). John Wiley &amp; Sons.</li>
</ul>
",2013-11-08 18:06:59.943
59167,1298.0,1,,,,What is the dimension (or units) of a CDF and PDF?,<probability><random-variable><density-function><cumulative-distribution-function>,CC BY-SA 3.0,"<p>Given a continuous random variable $X$, what are the units of the PDF and CDF of $X$?</p>
",2013-11-08 18:10:39.253
59168,23492.0,1,,,,What are the most commonly used predictive models when dealing with binary data?,<logistic><predictive-models><binary-data>,CC BY-SA 3.0,"<p>I know everybody uses logistic regression as the starting point, but I'm curious to know: What are the other commonly used predictive models when data is primarily binary?</p>
",2013-11-08 18:24:49.283
59169,450.0,2,,59165.0,,,,CC BY-SA 3.0,"<p>The robust scale is normally output by the routine you used to estimate the MM. 
For example, in R:</p>

<pre><code>library(robustbase)
data(coleman)
set.seed(0)
RlmST &lt;- lmrob(log.light ~ log.Te, data = starsCYG)
RlmST$scale
</code></pre>

<p>There is no explicit formula to compute it: it's the result 
of an iterative scheme.</p>
",2013-11-08 18:46:51.427
59170,22564.0,2,,59124.0,,,,CC BY-SA 3.0,"<p>First, I am not a statistician, just a researcher who has looked into it alot the last few years to figure out why the methods I observe being used around me are so lacking and why there is so much confusion about basic concepts like the ""what is a p-value?"" I will give my perspective.</p>

<blockquote>
  <p>First, one clarification question:</p>
  
  <p>The Time magazine wrote,</p>

<pre><code>""A power of 0.8 means that of ten true hypotheses tested, only two will be ruled out &gt; because their effects are not picked up in the
</code></pre>
  
  <p>data;""</p>
  
  <p>I am not sure how this fits into the definition of the power function
  I found in textbook, which is the probability of rejecting the null as
  a function of parameter Î¸. With different Î¸ we have different power,
  so I don't quite understand the above quote.</p>
</blockquote>

<p>Power is a function of Î¸, variance, and sample size. I am not sure what the confusion is. Also for many cases in which significance testing is used null hypothesis of mean1=mean2 is always false. In these cases <em>significance</em> is only a function of sample size. Please read Paul Meehl's <a href=""http://mres.gmu.edu/pmwiki/uploads/Main/Meehl1967.pdf"" rel=""nofollow"">""Theory-Testing in Psychology and Physics: A Methodological Paradox""</a> it clarified many things for me and I have never seen an adequate response. Paul Meehl has a few other papers on this you can find by searching his name. </p>

<blockquote>
  <p>In my field of political science / economics, scholars simply use up
  all the country-year data available. Thus, should we not be concerned
  with sample fiddling here?</p>
</blockquote>

<p>If you read the Simmons 2011 paper this is only one of the ""p-hacking"" techniques mentioned. If it is true that there is only one data set and no one picks out selective samples from it then I guess there is no room for increasing sample size.</p>

<blockquote>
  <p>Can the problem of running multiple tests but reporting only one model
  be fixed simply by the fact that someone else in the discipline will
  re-test your paper and strike you down immediately for not having
  robust results? Anticipating this, scholars in my field are more
  likely to include a robustness check section, where they show that
  multiple model specifications does not change the result. Is this
  sufficient?</p>
</blockquote>

<p>If replication was occurring without publication bias there would be no need for ""journals of the null result"". I would say the robustness check section is good to have but is not sufficient in the presence of researchers failing to publish what they consider null results. Also I would not consider a result robust just because multiple analysis techniques on the same data come to the same conclusion. A robust result is one that makes a correct prediction of effect/correlation/etc on <strong>new data</strong>. </p>

<p>A replication is not getting p&lt;0.05 both times. The theory should be considered more robust if it predicted a different effect/correlation/etc than used in the first study. I do not refer to the presence of an effect or correlation, but the precise value or a small range of values compared to possible range of values. The presence of increased/decreased effect or positive/negative correlation are 100% likely to be true in the case of the null hypothesis being false. Read Meehl.</p>

<blockquote>
  <p>Andrew Gelman and others raise the point that no matter the data, it
  would be always possible to find and publish some ""pattern"" that isn't
  really there. But this should not be a concern, given the fact that
  any empirical ""pattern"" must be supported by a theory, and rival
  theories within a discipline will just engage in an debate / race to
  find which camp is able to find more ""patterns"" in various places. If
  a pattern is truly spurious, then the theory behind will be quickly
  struck down when there is no similar pattern in other samples /
  settings. Isn't this how science progresses?</p>
</blockquote>

<p>Science cannot function properly if researchers are failing to publish null results. Also just because the pattern was not discovered in the second sample/setting does not mean it does not exist under the conditions of the initial study.</p>

<blockquote>
  <p>Assuming that the current trend of journals for null result will
  actually flourish, is there a way for us to aggregate all the null and
  positive results together and make an inference on the theory that
  they all try to test?</p>
</blockquote>

<p>This would be <a href=""https://en.wikipedia.org/wiki/Meta-analysis"" rel=""nofollow"">meta-analysis</a>. There is nothing special about null results in this case other than that researchers do not publish them because the p-values were above the arbitrary threshold. In the presence of publication bias meta-analysis is unreliable as is the entire literature suffering from publication bias. While it can be useful, meta analysis is far inferior for assessing a theory than having that theory make a precise prediction that is then tested. Publication bias does not matter nearly as much as long as new predictions pan out and are replicated by independent groups.</p>
",2013-11-08 18:55:14.857
59171,17740.0,2,,59168.0,,,,CC BY-SA 3.0,"<p>When the data is <em>entirely</em> binary I'd say <a href=""http://en.wikipedia.org/wiki/Association_rule_learning"" rel=""nofollow"">association rule learning</a> (aka affinity analysis or market basket analysis) and then learning a <a href=""http://en.wikipedia.org/wiki/Decision_tree"" rel=""nofollow"">decision tree</a> based on the result (a whole bunch of association rules). </p>

<p>Association rule learning attempts to find associations between predictors. The result of such an analysis is a set of rules (e.g. A ^ B) with an associated support (number of occurrences) and confidence. The amount of possible rules is exponential in terms of the amount of predictors and maximum rule length.</p>

<p>Subsequently it's common to learn models like decision trees from this (giant) set of rules.</p>
",2013-11-08 19:15:57.177
59172,23534.0,1,,,,"non-normal data for two-way ANOVA, which transformation to choose?",<anova>,CC BY-SA 3.0,"<p>I need to perform a two-way ANOVA on my data. My data is from a non-normal population. Apparently there is no two or three factor test for non-normal populations. I realized I need to transform my data, but I'm unsure about which transformation to perform on my data, I don't know which is the most appropriate. I don't know what is the criteria to choose one from the transformation list of possibilities?</p>
",2013-11-08 19:34:53.347
59173,20870.0,2,,57508.0,,,,CC BY-SA 3.0,"<blockquote>
  <p>Blockquote</p>
</blockquote>

<p>In general, I agree with the original hypotheses that higher-order terms are often associated with smaller variances.  But, this also depends on the type of data.</p>

<p>In plant breeding, a rule of thumb (Gauch, 1996, page 90) for multi-environment trials is that the variation in the data is:
70% location,
20% location-by-variety,
10% variety</p>

<p>Very approximate, but it is fairly consistent that the higher-order term ""location-by-variety"" variance is larger than the main-effect ""variety"" variance.</p>

<p>Ref: H G Gauch and R W Zobel, 1996.  Book: Genotype by Environment Interaction. Chapter: AMMI analysis of yield trials. CRC Press.</p>
",2013-11-08 19:52:22.647
59174,5045.0,2,,59156.0,,,,CC BY-SA 4.0,"<p>You can should treat the interaction variable as a dummy and follow <a href=""http://davegiles.blogspot.com/2011/03/dummies-for-dummies.html"" rel=""nofollow noreferrer"">this advice from David Giles</a>:</p>
<p>If <span class=""math-container"">$Treat\cdot Post$</span> switches from 0 to 1, the % impact on <span class=""math-container"">$Y$</span> is <span class=""math-container"">$100 \cdot (\exp(\beta_4 - \frac{1}{2} \hat \sigma_{\beta_4}^2)-1).$</span></p>
",2013-11-08 19:54:42.737
59175,17573.0,2,,59156.0,,,,CC BY-SA 3.0,"<p>What's required is that $\beta \cdot \Delta x$ be small.  If you know that $\Delta x$ is 1, then that means that $\beta$ has to be small.  How small?  The true proportionate change in $Outcome$ when the dummy rises by $1$ is $\exp(\beta)-1$.  The approximate change is $\beta$. The error from the approximation is:</p>

<p>\begin{equation}
\textrm{Error} = \exp(\beta)-1-\beta
\end{equation}</p>

<p>For small $|\beta|$, this is pretty small.  For example, for $\beta=0.1$ (approximate 10% change), the true percent change in $Outcome$ when the dummy turns on is 10.5%.  Given the usual standard errors in empirical work, I'm happy to ignore this.  By the time you get to a $\beta$ of 0.2 (approximate 20%), the true percent change is 22%.  Willing to ignore this much approximation error?  Again, I am, but you may not be.  This is now a 10% approximation error.  By the time you get to $\beta=0.3$, the true percent change in outcome is 35% rather than 30%, and I am not happy to ignore this any more.</p>

<p>So, my rule of thumb is to ignore this approximation error for $|\beta|&lt;0.2$ and worry about it for $\beta$ bigger than that.</p>
",2013-11-08 19:59:26.343
59176,23171.0,1,59199.0,,,"Find the distribution of (X, X+Y) when X and Y have a given joint Normal distribution",<self-study><normal-distribution><covariance><bivariate>,CC BY-SA 3.0,"<p>Let random variables $X$ and $Y$ be independent Normal with distributions $N(\mu_{1},\sigma_{1}^2)$ and $N(\mu_{2},\sigma_{2}^{2})$. Show that the distribution of $(X,X+Y)$ is bivariate Normal with mean vector $(\mu_{1},\mu_{1}+\mu_{2})$ and covariance matrix</p>

<p>$$ \left( \begin{array}{ccc}
\sigma_{1}^2 &amp; \sigma_{1}^2  \\
\sigma_{1}^2 &amp;\sigma_{1}^2+\sigma_{2}^2  \\
 \end{array} \right).$$</p>

<p>Thanks .</p>
",2013-11-08 19:59:35.390
59177,23536.0,1,,,,Is it ok to correlate before-and-after data?,<correlation><repeated-measures>,CC BY-SA 3.0,"<p>I am asked to draw a scatterplot and to compute a correlation coefficient for the following situation. A group of subjects are measured for a blood characteristic before and after surgery.</p>

<p>Is it OK to correlate before-and-after data?</p>

<p>I know that it is not OK to perform correlations on non independent data. I feel this is such a case--the two measurements are made on the same subjects--they should be correlated. </p>

<p>I know that correlating data to the change over time is not OK--but that is obvious and it is not the case here.</p>

<p>Also correlating two variables measured repeatedly on the same sample is a huge No. But again it is not my case.</p>
",2013-11-08 20:01:41.077
59178,23348.0,2,,59177.0,,,,CC BY-SA 3.0,"<p>I think it depends on what you are trying to do with your data. Technically, it is <strong>okay</strong> to correlate repeated measures from the same subject in the sense that it is mathematically <strong>possible</strong>. But if you trying to draw some kind of inference (for example, causality) from your data, simply correlated two observations that are from the same subject is not going to tell you anything useful.</p>

<p>Here's a <a href=""https://stats.stackexchange.com/questions/44134/correlation-among-repeated-measures-i-need-an-explanation"">nice little thread</a> talking about correlations of repeated measures within subjects.</p>
",2013-11-08 20:13:03.187
59179,449.0,2,,59177.0,,,,CC BY-SA 3.0,"<p>None of those correlations you think aren't OK really aren't OK. The correlation is just a measure of linear relationship. Sometimes you need to know the extent of a relationship that you know exists, such as this one, or any of the others you listed. In this case they may want to know the amount of correlation for a variety of reasons ranging from needing it for a repeated measures t-test report to checking to see that the data are sound.</p>

<p>Perhaps what you mean by not OK is that it's not OK to examine such a correlation with a hypothesis test where the null is a 0 correlation. That wouldn't be OK because you know that there has to be some. But that's not what you're asked to do. </p>
",2013-11-08 20:28:48.323
59180,23348.0,2,,59172.0,,,,CC BY-SA 3.0,"<p>Look at the distribution of your data via a histogram, and then see what type of distribution it resembles. For example, if your data is heavily skewed towards the low end of the scale, the data might benefit from a log(10) transformation:</p>

<p><img src=""https://i.stack.imgur.com/E6fJE.png"" alt=""Income Distribution""></p>

<p>A log transform of this particular data would make it at least close to normal. This particular example can be found <a href=""http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Examine the distribution of your outcome data, and then choose the appropriate transformation.</p>
",2013-11-08 20:30:32.803
59181,21762.0,2,,59177.0,,,,CC BY-SA 3.0,"<p>This is perfectly fine. You are considering two different variables each measured once per subject. One contains the 'pre' values, the other the 'post' values. I think you are mixing up independence between observations (subjects) and independence of variables.</p>

<p>Please note that in your situation, you might want to analyze differences between pre and post, not just looking at correlations, depending on the scientific question.</p>
",2013-11-08 20:40:35.470
59182,594.0,2,,59172.0,,,,CC BY-SA 3.0,"<p>Transformation that will change the shape leaves you no longer comparing means. If you really want to compare means you may want to avoid transform (there can be some particular exceptions where, at least with some accompanying assumptions, you can compute or approximate the means on the original scale as well).</p>

<p>If you don't need an estimate of the difference in means on the original scale (i.e. if effect sizes aren't critical to your analysis), then full-factorial models (i.e. with all interactions present) may work well enough with transformation.</p>

<p>If you are happy with more general location-comparisons than just means, there are other alternatives than transformation. </p>

<p>If you <em>do</em> want to compare means there are other alternatives than transformation. I'm not saying 'never use transformation'... but 'consider alternatives'.</p>

<blockquote>
  <p>Apparently there is no two or three factor test for non-normal populations.</p>
</blockquote>

<p>This is untrue. This could be done with GLMs for example. Or via resampling. </p>

<hr>

<p>Non-normality may not be the biggest issue you have (heteroskedasticity tends to have a bigger impact, one that doesn't diminish so nicely with sample size)</p>

<p>A nonlinear transformation will change many things. In your case, the important ones are distributional shape, variance of the transformed variables, and what means on the transformed scale correspond to on the original scale and vice versa. (In a regression situation there's also the impact on linearity of relationships)</p>

<p>You might choose a transformation that takes you to nearly constant variance. You might choose one that takes you to near symmetry. You might choose one that does either of those things less well, but is more interpretable. </p>

<p>If you're very lucky, you might be in a situation that gets you more than one of those at once.</p>

<p>But again, my advice is to first consider alternatives. As a first step, you might want to investigate what could be done with GLMs.</p>

<p>What are the characteristics of your data? What makes you say they're non-normal? Do you have counts? Are the data highly skew*?</p>

<p>* note that its not the unconditional distribution of the response that's crucial, but the conditional distribution. </p>
",2013-11-08 21:18:14.057
1760,723.0,1,1787.0,,,Is my weatherman accurate?,<hypothesis-testing><forecasting><scoring-rules>,CC BY-SA 2.5,"<p>A question which bothered me for some time, which I don't know how to address:</p>

<p>Every day, my weatherman gives a percentage chance of rain (let's assume its calculated to 9000 digits and he has never repeated a number). Every subsequent day, it either rains or does not rain.</p>

<p>I have years of data - pct chance vs rain or not. <em>Given this weatherman's history</em>, if he says tonight that tomorrow's chance of rain is X, then what's my best guess as to what the chance of rain really is?</p>
",2010-08-19 05:56:06.483
2156,114.0,1,2169.0,,,Recommended books or articles as introduction to Cluster Analysis?,<machine-learning><references><clustering>,CC BY-SA 2.5,"<p>I'm working on a small (200M) corpus of text, which I want to explore with some cluster analysis. What books or articles on that subject would you recommend?  </p>
",2010-09-01 23:57:06.760
59183,22903.0,1,,,,Organizing data to feed random forests,<r><machine-learning><data-mining><dataset><random-forest>,CC BY-SA 3.0,"<p>I'm willing to apply machine learning with <code>R</code> (I will start with random forests then maybe have a look at NNs) on some data, but I don't know where to start, probably because I don't know which words to put on my problem and what to google for.</p>

<p>My data consist in a set of events of type A, each of which contains both some specific variables and a (variable) number of elements of type B with their own variables.</p>

<p>A typical example of such data would be horse racingÂ : each race has its own parameters along with a list of horses and their own parameters.</p>

<p>Now, of course the training has to be done on each element of type A independently, so tutorials using basic <code>iris</code> data won't work â€” or at least I don't understand how to apply them on events of type A instead of elements of type B.</p>

<p>How should I organize my data set or feed it to <code>randomForest</code>Â ?  Or which keywords should I use to find relevant documentation on this kind of topicÂ ?  (I tried ""grouped data"" without much successâ€¦)</p>

<p>NBÂ : For a start I can discard the common variables of each A event, if needed.  But still every B element has to be considered equal of other B elements <em>inside</em> a single A event, and independently from other A events.</p>

<p><strong>UpdateÂ :</strong> I've found a workaround which may work in my particular situation (still to be tested, my DB needs reorganization).  The workaround is to consider parameters of the A events as parameters of each B element, so the problem simply becomes a set of B elements.  However I'm not satisfied with this solution and anyway I'm not sure it could be applicable to other similar problems, the question is still open.</p>
",2013-11-08 21:50:55.817
28,4.0,1,,,,The Two Cultures: statistics vs. machine learning?,<machine-learning><pac-learning>,CC BY-SA 3.0,"<p>Last year, I read a blog post from <a href=""http://anyall.org/"">Brendan O'Connor</a> entitled <a href=""http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/"">""Statistics vs. Machine Learning, fight!""</a> that discussed some of the differences between the two fields.  <a href=""http://andrewgelman.com/2008/12/machine_learnin/"">Andrew Gelman responded favorably to this</a>:</p>

<p>Simon Blomberg: </p>

<blockquote>
  <p>From R's fortunes
  package: To paraphrase provocatively,
  'machine learning is statistics minus
  any checking of models and
  assumptions'.
  -- Brian D. Ripley (about the difference between machine learning
  and statistics) useR! 2004, Vienna
  (May 2004) :-) Season's Greetings!</p>
</blockquote>

<p>Andrew Gelman:</p>

<blockquote>
  <p>In that case, maybe we should get rid
  of checking of models and assumptions
  more often. Then maybe we'd be able to
  solve some of the problems that the
  machine learning people can solve but
  we can't!</p>
</blockquote>

<p>There was also the <a href=""http://projecteuclid.org/euclid.ss/1009213726""><strong>""Statistical Modeling: The Two Cultures""</strong> paper</a> by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the <em>predictive accuracy</em> of models.</p>

<p>Has the statistics field changed over the last decade in response to these critiques?  Do the <em>two cultures</em> still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?</p>
",2010-07-19 19:14:44.080
143,114.0,1,3188.0,,,Algorithms to compute the running median?,<algorithms><median>,CC BY-SA 2.5,"<p>On smaller window sizes, <code>n log n</code> sorting might work. Are there any better algorithms to achieve this?</p>
",2010-07-19 21:32:38.523
356,166.0,1,59133.0,,,What is the difference between the Shapiroâ€“Wilk test of normality and the Kolmogorovâ€“Smirnov test of normality?,<distributions><statistical-significance><normality-assumption><kolmogorov-smirnov-test>,CC BY-SA 4.0,"<p>What is the difference between the Shapiroâ€“Wilk test of normality and the Kolmogorovâ€“Smirnov test of normality?  When will results from these two methods differ?</p>
",2010-07-21 00:24:35.500
412,186.0,1,,,,What book would you recommend for non-statistician scientists?,<references>,CC BY-SA 3.0,"<p>What book would you recommend for scientists who are not statisticians?</p>

<p>Clear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.</p>
",2010-07-21 15:01:21.127
414,4.0,1,,,,"What is your favorite ""data analysis"" cartoon?",<references><teaching><humor>,CC BY-SA 4.0,"<p>Data analysis cartoons can be useful for many reasons: they help communicate; they show that quantitative people have a sense of humor too; they can instigate good teaching moments; and they can help us remember important principles and lessons.</p>
<p><a href=""https://xkcd.com/552/"" rel=""noreferrer"">This is one of my favorites:</a></p>
<p><img src=""https://imgs.xkcd.com/comics/correlation.png"" alt=""XKCD irony about correlation and causation"" /></p>
<p>As a service to those who value this kind of resource, please share your favorite data analysis cartoon.  They probably don't need any explanation (if they do, they're probably not good cartoons!)  As always, <em>one entry per answer</em>. (This is in the vein of the Stack Overflow question <em><a href=""https://stackoverflow.com/questions/84556/whats-your-favorite-programmer-cartoon"">Whatâ€™s your favorite â€œprogrammerâ€ cartoon?</a></em>.)</p>
<p>P.S. Do not hotlink the cartoon without the site's permission please.</p>
",2010-07-21 15:13:21.493
541,,1,543.0,,user28,Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?,<regression><anova>,CC BY-SA 4.0,"<p>ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression.</p>
<p>In light of their equivalence, is there any reason why ANOVA is used instead of linear regression?</p>
<p>Note: I am particularly interested in hearing about <strong>technical</strong> reasons for the use of ANOVA instead of linear regression.</p>
<p><strong>Edit</strong></p>
<p>Here is one example using one-way ANOVA. Suppose, you want to know if the average height of male and females is the same. To test for your hypothesis you would collect data from a random   sample of male and females (say 30 each) and perform the ANOVA analysis (i.e., sum of squares for sex and error) to decide whether an effect exists.</p>
<p>You could also use linear regression to test for this as follows:</p>
<p>Define:  <span class=""math-container"">$\text{Sex} = 1$</span> if respondent is a male and <span class=""math-container"">$0$</span> otherwise.
<span class=""math-container"">$$
\text{Height} = \text{Intercept} + \beta * \text{Sex} + \text{error}
$$</span>
where: <span class=""math-container"">$\text{error}\sim\mathcal N(0,\sigma^2)$</span></p>
<p>Then a test of whether <span class=""math-container"">$\beta = 0$</span> is a an equivalent test for your hypothesis.</p>
",2010-07-23 15:17:56.770
1248,399.0,1,,,,Statistics Jokes,<references><humor>,CC BY-SA 3.0,"<p>Well, we've got favourite statistics quotes. What about statistics jokes?</p>
",2010-08-06 01:53:47.023
16998,5479.0,1,17000.0,,,Where to find a large text corpus?,<dataset>,CC BY-SA 4.0,"<p>I am looking for large (>1000) text corpus to download. Preferably with <strong>world news</strong> or some kind of <strong>reports</strong>. I have only found one with patents. Any suggestions?</p>
",2011-11-24 21:22:19.287
2509,628.0,1,101645.0,,,"Making sense of principal component analysis, eigenvectors & eigenvalues",<pca><intuition><eigenvalues><faq>,CC BY-SA 4.0,"<p>In today's pattern recognition class my professor talked about PCA, eigenvectors and eigenvalues. </p>

<p>I understood the mathematics of it. If I'm asked to find eigenvalues etc. I'll do it correctly like a machine. But I didn't <strong>understand</strong> it. I didn't get the purpose of it. I didn't get the feel of it.      </p>

<p>I strongly believe in the following quote:</p>

<blockquote>
  <p>You do not really understand something unless you can explain it to your grandmother. -- Albert Einstein</p>
</blockquote>

<p>Well, I can't explain these concepts to a layman or grandma.</p>

<ol>
<li>Why PCA, eigenvectors &amp; eigenvalues? What was the <em>need</em> for these concepts?</li>
<li>How would you explain these to a layman?</li>
</ol>
",2010-09-15 20:05:55.993
3646,211.0,1,3649.0,,,Kendall Tau or Spearman's rho?,<correlation><nonparametric><spearman-rho><kendall-tau>,CC BY-SA 4.0,"<p>In which cases should one prefer the one over the other?</p>

<p>I found someone who claims an advantage for Kendall, <a href=""http://web.archive.org/web/20090207060710/http://www.rsscse.org.uk/ts/bts/noether/text.html"" rel=""noreferrer"">for pedagogical reasons</a>, are there other reasons?</p>
",2010-10-24 13:15:49.687
4187,287.0,1,,,,What are common statistical sins?,<fallacy>,CC BY-SA 3.0,"<p>I'm a grad student in psychology, and as I pursue more and more independent studies in statistics, I am increasingly amazed by the inadequacy of my formal training. Both personal and second hand experience suggests that the paucity of statistical rigor in undergraduate and graduate training is rather ubiquitous within psychology. As such, I thought it would be useful for independent learners like myself to create a list of ""Statistical Sins"", tabulating statistical practices taught to grad students as standard practice that are in fact either superseded by superior (more powerful, or flexible, or robust, etc.) modern methods or shown to be frankly invalid. Anticipating that other fields might also experience a similar state of affairs, I propose a community wiki where we can collect a list of statistical sins across disciplines. Please, submit one ""sin"" per answer.</p>
",2010-11-15 18:46:37.113
4705,1209.0,1,4714.0,,,Most famous statisticians,<methodology><history>,CC BY-SA 3.0,"<p>What are the most important statisticians, and what is it that made them famous?<br>
(Reply just one scientist per answer please.)</p>
",2010-12-04 00:08:23.027
5015,1542.0,1,5020.0,,,What if interaction wipes out my direct effects in regression?,<regression><interaction>,CC BY-SA 2.5,"<p>In a regression, the interaction term wipes out both related direct effects. Do I drop the interaction or report the outcome? The interaction was not part of the original hypothesis. </p>
",2010-12-13 23:43:17.117
6788,1790.0,1,,,,Measuring and analyzing sample complexity,<machine-learning>,CC BY-SA 3.0,"<p>I recently stumbled upon the concept of <a href=""http://www.google.com/search?q=%22sample%20complexity%22"" rel=""nofollow""><strong>sample complexity</strong></a>, and was wondering if there are any texts, papers or tutorials that provide:</p>

<ol>
<li>An introduction to the concept (rigorous or informal)</li>
<li>An analysis of the sample complexity of established and popular classification methods or kernel methods.</li>
<li>Advice or information on how to measure it in practice.</li>
</ol>

<p>Any help with the topic would be greatly appreciated.</p>
",2011-02-19 22:41:23.000
7965,1691.0,1,,,,Colinearity and scaling when using k-means,<r><clustering>,CC BY-SA 2.5,"<p>I'm trying to gain a better understanding of kmeans clustering and am still unclear about colinearity and scaling of data. To explore colinearity, I made a plot of all five variables that I am considering shown in the figure below, along with a correlation calculation.
<img src=""https://i.stack.imgur.com/W5MZJ.jpg"" alt=""colinearity""></p>

<p>I started off with a larger number of parameters, and excluded any that had a correlation higher than 0.6 (an assumption I made). The five I choose to include are shown in this diagram.</p>

<p>Then, I scaled the date using the <code>R</code> function <code>scale(x)</code> before applying the <code>kmeans()</code> function. However, I'm not sure whether <code>center = TRUE</code> and <code>scale = TRUE</code> should also be included as I don't understand the differences that these arguments make. (The <code>scale()</code> description is given as <code>scale(x, center = TRUE, scale = TRUE)</code>).</p>

<p>Is the process that I describe an appropriate way of identifying clusters?</p>
",2011-03-24 14:51:27.800
8529,793.0,1,,,,What are some interesting and well-written applied statistics papers?,<references><application>,CC BY-SA 3.0,"<p>What are some good papers describing <em>applications</em> of statistics that would be fun and informative to read? Just to be clear, I'm not really looking for papers describing new statistical methods (e.g., a paper on least angle regression), but rather papers describing how to solve real-world problems.</p>

<p>For example, one paper that would fit what I'm looking is the climate paper from the <a href=""https://stats.meta.stackexchange.com/questions/685/second-cross-validated-journal-club"">second Cross-Validated Journal Club</a>. I'm kind of looking for more statistics-ish papers, rather than machine learning papers, but I guess it's kind of a fuzzy distinction (I'd classify the Netflix Prize papers as a bit borderline, and a paper on sentiment analysis as something I'm <em>not</em> looking for).</p>

<p>I'm asking because most of the applications of statistics I've seen are either the little snippets you seen in textbooks, or things related to my own work, so I'd like to branch out a bit.</p>
",2011-04-08 19:01:11.850
8681,1040.0,1,8699.0,,,Where can I find good publicly available data that I could use to teach z-scores to my college students?,<dataset>,CC BY-SA 3.0,"<p>I am sick of using the examples in the book. Is there an easy place to find data for which z-score/percentile/normal distribution stuff would be easy to see?</p>
",2011-04-14 01:33:55.987
9524,2872.0,1,9529.0,,,Are there any good movies involving mathematics or probability?,<probability><references>,CC BY-SA 3.0,"<p>Can you suggest some good movies which involve math, probabilities etc? One example is <a href=""http://en.wikipedia.org/wiki/21_%282008_film%29"">21</a>. I would also be interested in movies that involve algorithms (e.g. text decryption). In general ""geeky"" movies with famous scientific theories but no science fiction or documentaries. Thanks in advance!</p>
",2011-05-07 11:13:51.243
10008,1506.0,1,10069.0,,,Including the interaction but not the main effects in a model,<regression><modeling><interaction><regression-coefficients>,CC BY-SA 3.0,"<p>Is it ever valid to include a two-way interaction in a model without including the main effects?  What if your hypothesis is only about the interaction, do you still need to include the main effects?</p>
",2011-05-20 01:19:45.107
10541,2690.0,1,,,,Gap statistics MATLAB implementation,<clustering><matlab><mathematical-statistics>,CC BY-SA 3.0,"<p>Does any know the reference/link where i can find the MATLAB implementation of gap statistics for clustering as mentioned in <a href=""http://gremlin1.gdcb.iastate.edu/MIP/gene/MicroarrayData/gapstatistics.pdf"" rel=""nofollow"">this</a> paper?</p>
",2011-06-05 05:32:14.513
10911,22.0,1,57347.0,,,How to calculate the confidence interval of the mean of means?,<confidence-interval><multilevel-analysis>,CC BY-SA 3.0,"<p>Imagine that you repeat an experiment three times. In each experiment, you collect triplicate measurements. The triplicates tend to be fairly close together, compared to the differences among the three experimental means. Computing the grand mean is pretty easy. But how can one compute a confidence interval for the grand mean?</p>

<p>Sample data:</p>

<p>Experiment 1:   34, 41, 39</p>

<p>Experiment 2:   45, 51, 52</p>

<p>Experiment 3:   29, 31, 35</p>

<p>Assume that the replicate values within an experiment follow a Gaussian distribution, as does the mean values of each experiment. The SD of variation within an experiment is smaller than the SD among experimental means. Assume also that there is no ordering of the three values in each experiment. The left-to-right order of the three values in each row is entirely arbitrary.</p>

<p>The simple approach is to first compute the mean of each experiment: 38.0, 49.3, and 31.7, and then compute the mean, and its 95% confidence interval, of those three values. Using this method, the grand mean is 39.7 with the 95% confidence interval ranging from 17.4 to 61.9. </p>

<p>The problem with that approach is that it totally ignores the variation among triplicates. I wonder if there isn't a good way to account for that variation. </p>
",2011-06-16 16:58:13.537
13058,1124.0,1,13060.0,,,Software needed to scrape data from graph,<data-visualization><data-mining><software>,CC BY-SA 3.0,"<p>Anybody have any experience with software (preferably free, preferably open source) that will take an image of data plotted on cartesian coordinates (a standard, everyday plot) and extract the coordinates of the points plotted on the graph?</p>

<p>Essentially, this is a data-mining problem and a <strong>reverse</strong> data-visualization problem.</p>
",2011-08-18 04:14:22.583
13631,4221.0,1,,,,Forecasting binary time series,<r><time-series><forecasting><binary-data>,CC BY-SA 3.0,"<p>I have a binary time series with 1 when the car is not moving, and 0 when the car is moving. I want to make a forecast for a time horizon up to 36 hours ahead and for each hour. </p>

<p>My first approach was to use a Naive Bayes using the following inputs: t-24 (daily seasonal), t-48 (weekly seasonal), hour of the day. However, the results are not very good.</p>

<p>Which articles or software do you recommend for this problem?</p>
",2011-09-01 14:56:28.933
14729,5898.0,1,14790.0,,,Testing for linear dependence among the columns of a matrix,<r><correlation><pca><linear-model><svd>,CC BY-SA 3.0,"<p>I have a correlation matrix of security returns whose determinant is zero. (This is a bit surprising since the sample correlation matrix and the corresponding covariance matrix should theoretically be positive definite.)</p>

<p>My hypothesis is that at least one security is linearly dependent on other securities. Is there a function in R that sequentially tests each column a matrix for linear dependence?</p>

<p>For example, one approach would be to build up a correlation matrix one security at a time and calculate the determinant at each step. When the determinant = 0 then stop as you have identified the security who is a linear combination of other securities.</p>

<p>Any other techniques to identify linear dependence in such a matrix are appreciated.</p>
",2011-10-01 17:46:37.323
15281,3641.0,1,57490.0,,,How to detect structural change in a timeseries,<time-series><change-point>,CC BY-SA 4.0,"<p>Is there a specific method to detect change points(structural breaks) in a timeseries? (stocks prices).</p>
",2011-10-13 11:46:03.773
15542,4911.0,1,16537.0,,,"What are the ""hot algorithms"" for machine learning?",<machine-learning><clustering><bioinformatics>,CC BY-SA 3.0,"<p>This is a naive question from someone starting to learn machine learning. I'm reading these days the book ""Machine Learning: An algorithmic perspective"" from Marsland. I find it useful as an introductory book, but now I would like to go into advanced algorithms, those that are currently giving the best results. I'm mostly interested in bioinformatics: clustering of biological networks and finding patterns in biological sequences, particularly applied to single nucleotide polymorphism (SNP) analysis. Could you recommend me some reviews or books to read?</p>
",2011-10-18 21:24:39.543
16209,5196.0,1,16212.0,,,How to convert a vector of enumerable strings into a vector of numbers?,<r>,CC BY-SA 3.0,"<p>How to convert the x below to into a vector like y?</p>

<pre><code>x &lt;- [""a"", ""b"", ""b"", ""c"", ...]

y &lt;- [1, 2, 2, 3, ...]
</code></pre>

<p><strong>UPDATE:</strong></p>

<p>I end up with:</p>

<pre><code>levels(x) &lt;- 1:length(levels(x))
</code></pre>
",2011-11-06 09:47:15.640
16313,5234.0,1,16337.0,,,How do the Goodman-Kruskal gamma and the Kendall tau or Spearman rho correlations compare?,<spearman-rho><kendall-tau><goodman-kruskal-gamma>,CC BY-SA 3.0,"<p>In my work, we are comparing predicted rankings versus true rankings for some sets of data. Up until recently, we've been using Kendall-Tau alone. A group working on a similar project suggested we try to use the <a href=""http://en.wikipedia.org/wiki/Gamma_test_(statistics)"" rel=""noreferrer"">Goodman-Kruskal Gamma</a> instead, and that they preferred it. I was wondering what the differences between the different rank correlation algorithms were.</p>

<p>The best I've found was <a href=""https://stats.stackexchange.com/questions/3943/kendall-tau-or-spearmans-rho/3946#3946"">this answer</a>, which claims Spearman is used in place of usual linear correlations, and that Kendall-Tau is less direct and more closely resembles Goodman-Kruskal Gamma. The data I'm working with doesn't seem to have any obvious linear correlations, and the data is heavily skewed and non-normal.</p>

<p>Also, Spearman generally reports higher correlation than Kendall-Tau for our data, and I was wondering what that says about the data specifically. I'm not a statistician, so some of the papers I'm reading on these things just seem like jargon to me, sorry.</p>
",2011-11-09 02:39:58.810
16366,5249.0,1,30434.0,,,How to calculate perplexity of a holdout with Latent Dirichlet Allocation?,<text-mining><topic-models>,CC BY-SA 3.0,"<p>I'm confused about how to calculate the perplexity of a holdout sample when doing Latent Dirichlet Allocation (LDA). The papers on the topic breeze over it, making me think I'm missing something obvious...</p>

<p>Perplexity is seen as a good measure of performance for LDA. The idea is that you keep a holdout sample, train your LDA on the rest of the data, then calculate the perplexity of the holdout. </p>

<p>The perplexity could be given by the formula:</p>

<p>$per(D_{test})=exp\{-\frac{\sum_{d=1}^{M}\log p(\mathbb{w}_d)}{\sum_{d=1}^{M}N_d}\} $</p>

<p>(Taken from <a href=""http://doi.acm.org/10.1145/1282280.1282283"">Image retrieval on large-scale image databases, Horster et al</a>.)</p>

<p>Here  $M$ is the number of documents (in the test sample, presumably), $\mathbb{w}_d$ represents the words in document $d$, $N_d$ the number of words in document $d$.</p>

<p>It is not clear to me how to sensibly calcluate $p(\mathbb{w}_d)$, since we don't have topic mixtures for the held out documents. Ideally, we would integrate over the Dirichlet prior for all possible topic mixtures and use the topic multinomials we learned. Calculating this integral doesn't seem an easy task however.</p>

<p>Alternatively, we could attempt to learn an optimal topic mixture for each held out document (given our learned topics) and use this to calculate the perplexity. This would be doable, however it's not as trivial as papers such as Horter et al and Blei et al seem to suggest, and it's not immediately clear to me that the result will be equivalent to the ideal case above.</p>
",2011-11-10 03:08:12.977
20234,12900.0,1,20240.0,,vzn,"Are machine learning techniques ""approximation algorithms""?",<machine-learning><optimization><approximation>,CC BY-SA 3.0,"<p>Recently there was a ML-like question over on cstheory stackexchange, and I posted an answer recommending Powell's method, gradient descent, genetic algorithms, or other <a href=""http://en.wikipedia.org/wiki/Approximation_algorithm"">""approximation algorithms"".</a> In a comment someone told me these methods were ""heuristics"" and <em>not</em> ""approximation algorithms"" and frequently did not come close to the theoretical optimum (because they ""frequently get stuck in local minima"").</p>

<p>Do others agree with that? Also, it seems to me there is a sense of which heuristic algorithms can be guaranteed to come close to theoretical optimums if they are set up to explore a large part of the search space (eg setting parameters/step sizes small), although I haven't seen that in a paper. Does anyone know if this has been shown or proven in a paper? (if not for a large class of algorithms maybe for a small class say NNs etc.)</p>
",2012-02-10 19:03:03.517
20561,786.0,1,,,,How to deal with gaps/NaNs in time series data when using Matlab for autocorrelation and neural networks?,<time-series><dataset><matlab><autocorrelation><missing-data>,CC BY-SA 3.0,"<p>I have a time series of measurements (heights-one dimensional series). In the observation period, the measurement process went down for some time points. So the resulting data is a vector with NaNs where there were gaps in the data. Using MATLAB, this is causing me a problem when computing the autocorrelation (<code>autocorr</code>) and applying neural networks (<code>nnstart</code>). </p>

<p>How should these Gaps/NaNs be dealt with? Should I just remove these from the vector? Or replace their entry with an interpolated value? (if so how in MATLAB)</p>
",2012-02-15 19:25:44.330
20667,5911.0,1,,,user995434,Looking for 2D artificial data to demonstrate properties of clustering algorithms,<distributions><data-visualization><clustering><dataset>,CC BY-SA 3.0,"<p>I am looking for datasets of 2 dimensional datapoints (each datapoint is a vector of two values (x,y)) following different distributions and forms. Code to generate such data would also be helpful. I want to use them to plot / visualise how some clustering algorithms perform. Here are some examples:</p>

<ul>
<li><a href=""http://www.cise.ufl.edu/~jmishra/clustering/ClusterImages/KMeans4.jpg"">star like cloud data</a></li>
<li><a href=""http://www.aishack.in/wp-content/uploads/2010/07/kmeans-example.jpg"">four clusters, one easy seperable</a> </li>
<li><a href=""http://www.ti.uni-bielefeld.de/html/research/ngpca/spiral.1.png"">a spiral (no cluster)</a></li>
<li><a href=""http://courses.ee.sun.ac.za/Pattern_Recognition_813/lectures/6_em/img19.png"">a ring</a></li>
<li><a href=""http://3.bp.blogspot.com/_k1D0z3ucw7o/SITL8Et0QAI/AAAAAAAADSw/Bp_N8c9i5SE/s320/Sigma0.25.png"">two barely seperated clouds</a></li>
<li><a href=""http://www.newfolderconsulting.com/prtdoc/prtDocDataGen_01.png"">two parallel clusters forming a spiral</a></li>
<li>... etc</li>
</ul>
",2012-02-16 21:14:21.930
22674,7714.0,1,,,,Belief propagation on MRF with complex cliques,<machine-learning><graph-theory>,CC BY-SA 3.0,"<p>Is there a belief propagation algorithm for exact inference on a MRF with complex clique structures (i.e. ones involving more than 2 neighbours)?</p>

<p>For MRF's with cliques that only involve pairwise interaction, you could just search out far enough and cluster to form an acyclic graph and run the usual BP.  With more complex cliques, this seems impossible to me as clustering might involve cutting through a clique with multiple members on either side.  Is there a workaround for this?  Perhaps some clever conditioning arguments?</p>
",2012-03-30 03:51:41.627
22797,7769.0,1,,,,How does regression with and without intercept followed by test of stationarity affect cointegration test?,<regression><cointegration>,CC BY-SA 3.0,"<p>For a simple 2 variables (say X and Y) cointegration test, how does it affect our analysis, if we perform regression on X and Y with and without the intercept, and then test the spread for stationarity.</p>

<p>I am doing this analysis for stocks.</p>
",2012-04-02 07:39:49.013
23019,7739.0,1,58948.0,,,"R: How to ""control"" for another variable in Linear Mixed Effects Regression model?",<r><mixed-model><multiple-regression><random-effects-model><fixed-effects-model>,CC BY-SA 3.0,"<p>Essentially, I have two collinear variables which could be seen as either random or as fixed effects, a dependent variable I'm fitting the model to, and a variable that's assuredly a random effect.</p>

<p><strong>Dependent var:</strong> Number of neuron spikes (<code>FiringRate</code>) in a specific region of mousebrain</p>

<p><strong>Fixed effects:</strong></p>

<p><strong>1)</strong> <code>Time</code> at which data sample was taken (on a linear scale in days -- so day two would be 2, day 5 would be 5, and so on)</p>

<p><strong>2)</strong> The <code>Age</code> of the mouse in days (so there's definitely collinearity between this and the <code>Time</code> variable, but there are enough mice of different ages to make this worthwhile as a separate variable)</p>

<p><strong>Random effect:</strong> <code>Subject</code> -- ""Name"" (ID number) of the mouse</p>

<p>Essentially, I'm wondering if it would be appropriate to run two LMEs. In the first, I'd treat <code>Age</code> and <code>Subject</code> as random variables in order to control for the effects of <code>Age</code> (and thus the collinearity between <code>Age</code> and <code>Time</code>) and see if Time is a significant predictor of the # of spikes (dependent variable). In the second, I'd enter <code>Time</code> and <code>Subject</code> as random variables to see if <code>Age</code> was a significant predictor.</p>

<pre><code>library(lme4)
a = lmer(FiringRate ~ Time + (1|Age) + (1|Subject))
b = lmer(FiringRate ~ Age + (1|Time) + (1|Subject))
</code></pre>
",2012-04-05 23:08:58.800
23087,5643.0,1,59036.0,,,Moving-average model error terms,<regression><time-series><arima><box-jenkins>,CC BY-SA 3.0,"<p>This is a basic question on Box-Jenkins MA models. As I understand, an MA model is basically a linear regression of time-series values $Y$ against previous error terms $e_t,..., e_{t-n}$. That is, the observation $Y$ is first regressed against its previous values $Y_{t-1}, ..., Y_{t-n}$ and then one or more $Y - \hat{Y}$ values are used as the error terms for the MA model.</p>

<p>But how are the error terms calculated in an ARIMA(0, 0, 2) model? If the MA model is used without an autoregressive part and thus no estimated value, how can I possibly have an error term?</p>
",2012-04-07 12:48:41.467
24506,7341.0,1,24602.0,,,Confidence interval for values for a fitted line,<r><confidence-interval><jmp>,CC BY-SA 3.0,"<p>I'm using JMP to analyze some sample data to make predictions about the population.  My sample is from a destructive QC test, so I obviously want to minimize my sample.  I have a response (my Y) and a known factor (a very strong and consistent correlation that is measurable by non-destructive means) but the exact relationship between them varies from lot to lot (the slope and y offset vary).</p>

<p>So, in JMP, I am fitting a line and then showing the ""confidence limits for an individual predicted value"" which I believe gives me an indicator of how the population is likely to behave.  So I'm using that plot to make disposition decisions.  I want to automate this process, perhaps using R, but I'm a total novice at R.  I could do the math if I was just dealing with a mean and standard deviation, but I don't know how to do it with a fit line and a known factor.  Can someone please give me either the general information on how to get the confidence limits around the line, or else tell me how to do the whole thing in R?</p>

<p>Thankss much.</p>
",2012-05-04 17:02:04.137
30862,1805.0,1,30864.0,,,Why bother with low-rank approximations?,<r><matrix><approximation>,CC BY-SA 4.0,"<p>If you have a matrix with <span class=""math-container"">$n$</span> rows and <span class=""math-container"">$m$</span> columns, you can use SVD or other methods to calculate a <a href=""http://en.wikipedia.org/wiki/Low-rank_approximation"" rel=""nofollow noreferrer"">low-rank approximation</a> of the given matrix. However, the low-rank approximation will still have <span class=""math-container"">$n$</span> rows and <span class=""math-container"">$m$</span> columns. How can low-rank-approximations be useful for machine learning and natural language processing, given that you are left with the same number of features?</p>
",2012-08-28 00:12:57.667
30957,9446.0,1,30960.0,,,Initialize ARIMA simulations with different time-series,<time-series><forecasting><simulation><arima><ecology>,CC BY-SA 3.0,"<p>I have a fairly long time-series of annual abundances ($N_t$) of a wildlife species (73 years of abundances).  To forecast the populationâ€™s trajectory, I have used ARIMA modeling.  Examination of the ACF and PACF of the first-order differenced time-series suggested a 10-year cycle exists.  So I used a span 10 seasonal difference to account for this periodic pattern.  Therefore, the response variable was:
$$
Y_t=(\sqrt{N_t}-\sqrt{N_{t-1}})-(\sqrt{N_{t-10}}-\sqrt{N_{t-11}})
$$
Typically, I would have used a logarithmic transformation but it resulted in heteroscedastic residuals.  Examination of the ACF and PACF of $Y_t$ indicated a multiplicative seasonal structure so I fit the model:
$$
ARIMA(0,1,1)(0,1,1)_{10}
$$
using the Forecast Package in <code>R</code>....<code>library(forecast)</code>.</p>

<p>Example code for fitting the model:</p>

<pre><code>m1=Arima(y,order=c(0,1,1),seasonal=list(order=c(0,1,1),period=10),include.mean=FALSE)
</code></pre>

<p>The residuals of this model were normally distributed, not autocorrelated, and homoscedastic.</p>

<p>I have been using the fitted model from above for some additional simulation work using the <code>simulate.Arima</code> function.  However, I would like to initialize the simulation with a different time-series.  The <code>arima.sim</code> function allows this but the <code>arima.sim</code> function doesn't seem to handle seasonal ARIMA models.  With the <code>simulate.Arima</code> function one can use the <code>future=TRUE</code> option to simulate values that are ""future to and conditional on the data"" in the model <code>m1</code>.  Can the data in the model object <code>m1</code> simply be replaced to create a simulation that is conditional on different data?</p>

<p>For example:</p>

<pre><code># Create a new model object for simulation.
m.sim=m1
# Replace the data in the model object with the new data.
m.sim$x=new
# Simulation conditional on the new data.
sim.forecasts=replicate(1000,simulate.Arima(m.sim,future=TRUE,bootstrap=TRUE))
</code></pre>
",2012-08-29 14:56:24.737
31575,6404.0,1,31587.0,,,Estimating Markov transition probabilities from sequence data,<r><matlab><markov-process>,CC BY-SA 3.0,"<p>I have a full set of sequences (432 observations to be precise) of 4 states $A-D$: eg</p>

<p>$$Y=\left(\begin{array}{c c c c c c c}
A&amp; C&amp; D&amp;D  &amp; B &amp; A &amp;C\\
B&amp; A&amp; A&amp;C &amp; A&amp;- &amp;-\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
B&amp; C&amp; A&amp;D &amp; A &amp; B &amp; A\\
    \end{array}\right)$$</p>

<p><strong>EDIT</strong>:  The observation sequences are of unequal lengths! Does this change anything?</p>

<p>Is there a way of calculating the transition matrix $$P_{ij}(Y_{t}=j|Y_{t-1}=i)$$ in Matlab or R or similar?  I think the HMM package might help.  Any thoughts?</p>

<p>eg: <a href=""https://stats.stackexchange.com/questions/14360/estimating-markov-chain-probabilities"">Estimating Markov chain probabilities</a></p>
",2012-09-11 15:29:12.027
32038,11013.0,1,32053.0,,,What is the minimum recommended number of groups for a random effects factor?,<mixed-model><sample-size>,CC BY-SA 3.0,"<p>I'm using a mixed model in <code>R</code> (<code>lme4</code>) to analyze some repeated measures data. I have a response variable (fiber content of feces) and 3 fixed effects (body mass, etc.). My study only has 6 participants, with 16 repeated measures for each one (though two only have 12 repeats). The subjects are lizards that were given different combinations of food in different 'treatments'. </p>

<p>My question is: can I use subject ID as a random effect? </p>

<p>I know this is the usual course of action in longitudinal mixed effects models, to take account of the randomly sampled nature of the subjects and the fact that observations within subjects will be more closely correlated than those between subjects. But, treating subject ID as a random effect involves estimating a mean and variance for this variable. </p>

<ul>
<li><p>Since I have only 6 subjects (6 levels of this factor), is this enough to get an accurate characterization of the mean and variance? </p></li>
<li><p>Does the fact that I have quite a few repeated measurements for each subject help in this regard (I don't see how it matters)? </p></li>
<li><p>Finally, If I can't use subject ID as a random effect, will including it as a fixed effect allow me to control for the fact that I have repeated measures?</p></li>
</ul>

<p><strong>Edit:</strong> I'd just like to clarify that when I say ""can I"" use subject ID as a random effect, I mean ""is it a good idea to"". I know I can fit the model with a factor with just 2 levels, but surely this would be in-defensible? I'm asking at what point does it become sensible to think about treating subjects as random effects? It seems like the literature advises that 5-6 levels is a lower bound. It seems to me that the estimates of the mean and variance of the random effect would not be very precise until there were 15+ factor levels.</p>
",2012-09-20 01:56:50.007
32317,8208.0,1,,,,Good clustering Java library,<clustering><java>,CC BY-SA 3.0,"<p>I'm looking for a good Java library implementing several clustering algorithms.</p>

<p>I'll have to cluster some programs execution traces and I still don't know which algorithms I am going to need, so I'd like to use a library providing lot of them and that makes it easy to swap algorithms.</p>

<p>So far I had a look at Weka but I don't know whether there is a more complete library available I'm missing.</p>
",2012-09-25 20:11:11.667
32388,2105.0,1,,,,Odd results from Bayesian network in R,<r><bayesian><bayesian-network>,CC BY-SA 3.0,"<p>Related to question <a href=""https://stats.stackexchange.com/questions/37930/prediction-with-bayesian-networks-in-r"">here</a>.</p>

<p>I've been trying to teach myself about Network Analysis, and developing DAG charts in R. Let's say that I have the following data.</p>

<pre><code>dat=data.frame(sold=c(0,0,0,1,0,1), won=c(1,0,0,1,0,1), bid=c(5,3,2,5,3,4))
dat
</code></pre>

<p>Given what I'm trying to analyze, I know that the DAG plot should be as follows:</p>

<pre><code>bid =&gt; won =&gt; sold
</code></pre>

<p>However, when I utilize the bnlearn package to generate the plot, it comes out as follows. It just can't be correct, and should  be in the opposite direction.</p>

<pre><code>library(""bnlearn"")
library(""Rgraphviz"")

bn.hc &lt;- hc(dat, score = ""bic"")
graphviz.plot(bn.hc)
</code></pre>

<p><img src=""https://i.stack.imgur.com/Zg0rM.png"" alt=""enter image description here""></p>

<p>Now, I know that's just the data that I provided it to learn on, but I've messed around with the variable values, and it never turns our the way it should. Basically, a bid should determine whether you win, and whether you win should determine whether you can sell it. Just doesn't make sense.</p>

<p>Isn't there some way to specify what variable is the response variable? In my case, the response variable should be sold, and there should be no arcs from sold to another node.</p>

<p>Can anyone help with diagnosing the problem in R? Is there something I'm missing in the code? or of my understanding of BN's? is this an issue w/ what I pass as the algorithm to use in 'score'?</p>
",2012-09-26 20:56:22.507
55043,18198.0,1,,,,"Testing if low-variance components in PCA contain any ""signal""",<pca>,CC BY-SA 3.0,"<p>My problem is similar to this one but I am looking for a different solution: (so if it should be merged just let me know).</p>

<p><a href=""https://stats.stackexchange.com/questions/21742/measuring-whats-lost-in-pca-dimensionality-reduction"">Measuring what&#39;s &#39;lost&#39; in PCA dimensionality reduction?</a></p>

<p>I my application we have a correlation matrix of dimension 30 upon which we conduct a PCA analysis and retain the first three eigenvectors on the basis that they typically contain 90+% of the variation. </p>

<p>However this has always struck me as a little arbitrary, I would like to test whether these smaller eigenvectors do actually contain a ""signal"" rather than white noise.</p>

<p>I suppose one very simple method would be to split the data up and see if these smaller eignevectors maintain a similar shape, but I would like to find a more scientifically robust way to test this hypothesis.</p>
",2013-09-10 13:00:25.933
33598,11643.0,1,,,,How to identify structural change using a Chow test on Eviews?,<interpretation><chow-test>,CC BY-SA 3.0,"<p>I have this little problem and I would appreciate some help.</p>

<p>As part of my master thesis, I have to identify a trend in a univariate (GDP) time series for different countries.  I have to separate the trend and the stochastic element in it for each country.</p>

<p>I have managed to do so by doing:</p>

<p>variable c @trend  // for each country.</p>

<p>And then running a AR(1) on the residuals  // for each country.</p>

<p>However, now I need to identify structural breaks in one of these countries.  I've been reading and searching all over the internet and books and I've found that the test most people use to identify these structural changes is the Chow Test.</p>

<p>I know how to run the test, but I have't been able to figure out how to interpret the results, and decide whether there is a structural break or not.</p>

<p>Here there is an example of the results:</p>

<p><img src=""https://i.stack.imgur.com/RL9Lz.jpg"" alt=""enter image description here""></p>

<p>What puzzles me the most is the fact that, regardless the point I choose to break the series, I always get </p>

<p>Prob. F(2,47)  0.0016 //or any very significant value, with the same degrees of freedom.</p>

<p>Can someone please help me understand how I should interpret these results in order to identify where the breaks lie?</p>
",2012-10-16 09:30:00.727
34166,668.0,1,,,,The Sleeping Beauty Paradox,<decision-theory><paradox>,CC BY-SA 3.0,"<h3>The situation</h3>
<p>Some researchers would like to put you to sleep.  Depending on the secret toss of a fair coin, they will briefly awaken you either once (Heads) or twice (Tails).  After each waking, they will put you back to sleep with a drug that makes you forget that awakening.  When you are awakened, to what degree should <em>you</em> believe that the outcome of the coin toss was Heads?</p>
<p><em>(OK, maybe you donâ€™t want to be the subject of this experiment!  Suppose instead that Sleeping Beauty (SB) agrees to it (with the full approval of the Magic Kingdomâ€™s Institutional Review Board, of course).  Sheâ€™s about to go to sleep for one hundred years, so what are one or two more days, anyway?)</em></p>
<p><img src=""https://i.stack.imgur.com/zLmrR.png"" alt=""Maxfield Parrish illustration"" /></p>
<p><em>[Detail of a <a href=""http://en.wikipedia.org/wiki/Maxfield_Parrish"" rel=""noreferrer"">Maxfield Parrish</a> illustration.]</em></p>
<h3>Are you a Halfer or a Thirder?</h3>
<p><strong>The Halfer position.</strong>  Simple! The coin is fair--and SB knows it--so she should believe there's a one-half chance of heads.</p>
<p><strong>The Thirder position.</strong> Were this experiment to be repeated many times, then the coin will be heads only one third of the time SB is awakened.  Her probability for heads will be one third.</p>
<h3>Thirders have a problem</h3>
<p>Most, but not all, people who have written about this are thirders.  But:</p>
<ul>
<li><p>On Sunday evening, just before SB falls asleep, she must believe the chance of heads is one-half: thatâ€™s what it means to be a fair coin.</p>
</li>
<li><p>Whenever SB awakens, <em>she has learned absolutely nothing she did not know Sunday night.</em>  What rational argument can she give, then, for stating that her belief in heads is now one-third and not one-half?</p>
</li>
</ul>
<h3>Some attempted explanations</h3>
<ul>
<li><p>SB would necessarily lose money if she were to bet on heads with any odds other than 1/3.  (Vineberg, <em>inter alios</em>)</p>
</li>
<li><p>One-half really is correct: just use the Everettian â€œmany-worldsâ€ interpretation of Quantum Mechanics!  (Lewis).</p>
</li>
<li><p>SB updates her belief based on self-perception of her â€œtemporal locationâ€ in the world.  (Elga, <em>i.a.</em>)</p>
</li>
<li><p>SB is confused: â€œ[It] seems more plausible to say that her epistemic state upon waking up should not include a definite degree of belief in heads. â€¦ The real issue is how one deals with known, unavoidable, cognitive malfunction.â€  [Arntzenius]</p>
</li>
</ul>
<hr />
<h3>The question</h3>
<p>Accounting for what has already been written on this subject (see the references as well as a <a href=""https://stats.stackexchange.com/a/23812"">previous post</a>), how can this paradox be resolved in a statistically rigorous way?  Is this even possible?</p>
<hr />
<h3>References</h3>
<p>Arntzenius, Frank (2002).  <a href=""http://www.joelvelasco.net/teaching/3865/arntzenius%20-%20reflections%20on%20sleeping%20beauty.pdf"" rel=""noreferrer""><em>Reflections on Sleeping Beauty</em></a> Analysis 62.1 pp 53-62.</p>
<p>Bradley, DJ (2010).  <a href=""http://philpapers.org/archive/BRACIB.1.pdf"" rel=""noreferrer""><em>Confirmation in a Branching World: The Everett Interpretation and Sleeping Beauty</em></a>.  Brit. J. Phil. Sci. 0 (2010), 1â€“21.</p>
<p>Elga, Adam (2000).  Self-locating belief and the Sleeping Beauty Problem.  Analysis 60 pp 143-7.</p>
<p>Franceschi, Paul (2005).  <a href=""http://philsci-archive.pitt.edu/2175/1/sb-en.pdf"" rel=""noreferrer""><em>Sleeping Beauty and the Problem of World Reduction</em></a>.  Preprint.</p>
<p>Groisman, Berry (2007).  <a href=""http://philsci-archive.pitt.edu/3624/1/SB_b.groisman_last.pdf"" rel=""noreferrer""><em>The end of Sleeping Beautyâ€™s nightmare</em></a>.  Preprint.</p>
<p>Lewis, D (2001).  <em>Sleeping Beauty: reply to Elga</em>.  Analysis 61.3 pp 171-6.</p>
<p>Papineau, David and Victor Dura-Vila (2008).  <em>A Thirder and an Everettian: a reply to Lewisâ€™s â€˜Quantum Sleeping Beautyâ€™</em>.</p>
<p>Pust, Joel (2008).  <em>Horgan on Sleeping Beauty</em>.  Synthese 160 pp 97-101.</p>
<p>Vineberg, Susan (undated, perhaps 2003).  <em>Beautyâ€™s Cautionary Tale</em>.</p>
",2012-10-25 20:10:18.553
35097,9886.0,1,35160.0,,,What's wrong with XKCD's Frequentists vs. Bayesians comic?,<bayesian><frequentist>,CC BY-SA 3.0,"<p><img src=""https://i.stack.imgur.com/tStr4.png"" alt=""xkcd comic number 1132""></p>

<p><a href=""http://xkcd.com/1132"">This xkcd comic (Frequentists vs. Bayesians)</a> makes fun of a frequentist statistician who derives an obviously wrong result.</p>

<p>However it seems to me that his reasoning is actually correct in the sense that it follows  the standard frequentist methodology. </p>

<p>So my question is ""does he correctly apply the frequentist methodology?"" </p>

<ul>
<li>If no: what would be a correct frequentist inference in this scenario? How to integrate ""prior knowledge"" about the sun stability in the frequentist methodology?</li>
<li>If yes: wtf? ;-)</li>
</ul>
",2012-11-11 15:56:03.667
35249,11884.0,1,,,,Using PCA to reduce the number of variables split into groups,<pca><factor-analysis><dimensionality-reduction>,CC BY-SA 3.0,"<p>First of all, sorry for the strange title, I had no idea how to describe my problem better. My issue is the following, I think it is pretty much limited to geosciences.</p>

<p>I have several properties for every sample, which are divided by depth.</p>

<p>For instance:          </p>

<p>$ \qquad \displaystyle \small \begin{array} {r|rrr} \hline
ID                    &amp; 1    &amp; 2   &amp;3 &amp; ...\\ \hline
\text{var1}_{0-20cm}  &amp; 2.3  &amp;2.0 &amp;1.0&amp; ...\\
\text{var1}_{20-50cm} &amp; 2.1  &amp;1.1 &amp;0.0&amp; ...\\
\text{var1}_{50-100cm}&amp; 2.6  &amp;1.1 &amp;0.0&amp; ...\\ \hline
\text{var2}_{0-20cm}  &amp; 10.5 &amp;5.5 &amp;3.5&amp; ...\\
\text{var2}_{20-50cm} &amp; 10.9 &amp;5.9 &amp;1.9&amp; ...\\
\text{var2}_{50-100cm}&amp; 15.0 &amp;5.0 &amp;1.0&amp; ...\\   \hline
  \vdots &amp; \vdots &amp; \vdots\\ \hline \end{array}
$</p>

<p>Basically these are geological layers going from surface down to 100 cm depth.
I am trying to decrease the number of variables, either with PCA or factor analysis.
The issue is, that I would like to handle properties together, no matter what the depth is.</p>

<p>(For instance I do not want to get rid of a layer in between the surface and the bottom layer.)</p>

<p>Is there any way to handle them together, or group them for PCA or whatever. I tried to find some relevant information, but I think the problem is limited to a small portion of the science (maybe I am wrong), so I could not find anything useful.</p>
",2012-11-13 22:29:58.533
37182,11446.0,1,57320.0,,,How to specify in r spatial covariance structure similar to SAS sp(pow) in a marginal model?,<r><sas><spatial><panel-data><generalized-least-squares>,CC BY-SA 3.0,"<p>I'm currently translating existing code from SAS to R. I'm working on longitudinal data (CD4 count over time). I have the following SAS code :</p>

<pre><code>Proc mixed data=df;
class NUM_PAT;
model CD4t=T /s ;
repeated / sub=NUM_PAT type=sp(pow)(T);
</code></pre>

<p>The SAS spatial power covariance structure is useful for unequally spaced longitudinal measurements where the correlations decline as a function of time (as shown by the picture below). 
<img src=""https://i.stack.imgur.com/s7RnV.png"" alt=""Spatial Power Covariance Structure""></p>

<p>I think I have to use gls( ) from {nlme} since I don't have any random effects. As R 'only' provides ""spherical"", ""exponential"", ""gaussian"", ""linear"", and ""rational"" as correlation spatial structures, my guess is that I need to use corSpatial plus a weights argument.</p>

<p>I tried the following code, but it doesn't work :</p>

<pre><code>gls(CD4t~T, data=df, na.action = (na.omit), method = ""ML"",
corr=corCompSymm(form=~1|NUM_PAT), weighhts=varConstPower(form=~1|T))
</code></pre>

<p>What am I doing wrong ?</p>

<p>Thanks for any help.</p>
",2012-12-14 15:06:25.837
37748,13370.0,1,,,,What is the computational complexity of the EM algorithm?,<machine-learning><computational-statistics>,CC BY-SA 3.0,"<p>In general, and more specifically for Bernoulli mixture model (aka Latent Class Analysis).</p>
",2012-12-27 07:48:43.813
37819,12314.0,1,,,,Putting stationary variables through Johansen procedure,<time-series><econometrics><cointegration><autoregressive><stationarity>,CC BY-SA 3.0,"<p>Is it okay to feed $I(0)$ variables into the Johansen procedure? I've read three sources that seem to state that this is not what you're supposed to do. However, whenever I've done this, I notice that $\Pi$ is full rank and so it leads me to a VAR and therefore I don't see any problem with this. </p>
",2012-12-29 16:08:10.207
37981,13403.0,1,,,,"""Peakedness"" of a skewed probability density function",<density-function><descriptive-statistics><skewness><kurtosis>,CC BY-SA 3.0,"<p>I would like to describe the ""peakedness"" and tail ""heaviness"" of several skewed probability density functions.</p>

<p>The features I want to describe, would they be called ""kurtosis""? I've only seen the word ""kurtosis"" used for symmetric distributions?</p>
",2013-01-03 16:00:17.050
40030,1790.0,1,115327.0,,,Understanding stratified cross-validation,<cross-validation><stratification>,CC BY-SA 4.0,"<p>I <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)"" rel=""noreferrer"">read in Wikipedia</a>: </p>

<blockquote>
  <p>In <strong>stratified k-fold cross-validation</strong>, the folds are selected so that the <strong>mean response value</strong> is approximately equal in all the folds. In
  the case of a dichotomous classification, this means that each fold
  contains roughly the same proportions of the two types of class
  labels.</p>
</blockquote>

<ol>
<li>Say we are using CV for estimating the performance of a predictor or estimator. What would <strong>mean response value</strong> (MRV) mean in this context? Just the average value of the predictor / estimator? </li>
<li>In what scenarios  would <em>""achieving approximately the same MRV""</em> in all folds be actually <strong>important</strong>? In other words, what are the consequences of <strong>not</strong> doing so?</li>
</ol>
",2013-02-07 20:58:31.927
40104,14684.0,1,,,,The weighted sum of two independent Poisson random variables,<distributions><poisson-distribution>,CC BY-SA 3.0,"<p>Using wikipedia I found a way to calculate the probability mass function resulting from the sum of two Poisson random variables. However, I think that the approach I have is wrong.</p>

<p>Let $X_1, X_2$ be two independent Poisson random variables with mean $\lambda_1, \lambda_2$, and $S_2 =  a_1 X_1+a_2 X_2$, where the $a_1$ and $a_2$ are constants, then the probability-generating function of $S_2$ is given by
$$
G_{S_2}(z) = \operatorname{E}(z^{S_2})= \operatorname{E}(z^{a_1 X_1+a_2 X_2}) G_{X_1}(z^{a_1})G_{X_2}(z^{a_2}).
$$
Now, using the fact that the probability-generating function for a Poisson random variable is $G_{X_i}(z) = \textrm{e}^{\lambda_i(z - 1)}$, we can write the probability-generating function of the sum of the two independent Poisson random variables as
$$
\begin{aligned}
G_{S_2}(z) &amp;= \textrm{e}^{\lambda_1(z^{a_1} - 1)}\textrm{e}^{\lambda_2(z^{a_2} - 1)}  \\
&amp;= \textrm{e}^{\lambda_1(z^{a_1} - 1)+\lambda_2(z^{a_2} - 1)}.
\end{aligned}
$$
It seems that the probability mass function of $S_2$ is recovered by taking derivatives of $G_{S_2}(z)$ $\operatorname{Pr}(S_2 = k) = \frac{G_{S_2}^{(k)}(0)}{k!}$, where $G_{S_2}^{(k)} = \frac{d^k G_{S_2}(z)}{ d z^k}$.</p>

<p>Is this is correct? I have the feeling I cannot just take the derivative to obtain the probability mass function, because of the constants $a_1$ and $a_2$. Is this right? Is there an alternative approach?</p>

<p>If this is correct can I now obtain an approximation of the cumulative distribution by truncating the infinite sum over all k?</p>
",2013-02-09 19:31:13.290
40121,14728.0,1,,,,Comparing many means in JMP,<t-test><mean><tukey-hsd-test><jmp>,CC BY-SA 3.0,"<p>I'm trying to compare several sets of experiment data, by comparing means. I read there are several different tests such as <em>Each Pair, Studentâ€™s t</em> and <em>All Pairs, Tukey HSD</em>, which give different circles of different radius, an example shown below </p>

<p><img src=""https://i.stack.imgur.com/QCKE1.gif"" alt=""enter image description here""></p>

<p>How are the circles defined? How do I calculate the radius? And is there a rule what test one should use for what kind of data?</p>
",2013-02-10 03:03:22.580
40859,15044.0,1,,,,Validation: Data splitting into training vs. test datasets,<cross-validation>,CC BY-SA 3.0,"<p>I was naively validating my binomial logit models by testing on a test dataset. I had randomly divided the available data (~2000 rows) into training (~1500) and validation (~500) datasets. </p>

<p>I now read a post in another thread ( Frank Harrell) that causes me to question my approach:</p>

<blockquote>
  <p>Data splitting is not very reliable unless you have more than 15,000
  observations. In other words, if you split the data again, accuracy
  indexes will vary too much from what you obtained with the first
  split.</p>
</blockquote>

<p>How serious is this worry and what are ways around it? The OP speaks of ""resampling"" but not sure how that works here for validation. </p>

<p>Edit: Adding context as per @Bernhard's comment below:</p>

<p><a href=""https://stats.stackexchange.com/questions/15618/comparing-logistic-regression-models"">Comparing logistic regression models</a></p>
",2013-02-22 08:40:44.930
40870,1923.0,1,,,,False discovery rate calculation in target-decoy matching context,<terminology><false-discovery-rate>,CC BY-SA 3.0,"<p>A common strategy in mass spectrometry of biological molecules is to upload observed spectra to a server so that they can be matched to a LARGE database of theoretical spectra of known molecules (a.k.a. <em>target</em> database). In order to control for false positives, a <em>decoy</em> database consisting of incorrect/irrelevant spectra is used.</p>

<p>I have been reading more into this subject and have come up some questions regarding the calculation of the FDR measure from this target-decoy strategy. The basic idea of the FDR value is very intuitive: </p>

<p>$FDR = \frac{FP}{FP + TP}$</p>

<p>where FP and TP stands for false and true positives respectively. This makes perfect sense to me; if I'm trying to guess some peoples' names out of a phone book, and get 8 right and 2 wrong, I would have 2 <em>false</em> out of 10 <em>total</em> guesses, and thus my false discovery rate would be 20%.</p>

<p>However reading <a href=""http://www.proteored.org/pme6/fdr_calculation_for_pme6.pdf"" rel=""nofollow"">this tutorial</a> on how this is done in large scale on the servers, I got introduced to two different calculations, depending on whether or not the <em>target</em> and <em>decoy</em> databases are concatenated (page 2).</p>

<p>I don't think that this is a typo as I found other occurrences <sup>*</sup> of the mysterious factor 2 in front of FP in scientific literature. However the motivation behind this is never explained (at least I couldn't find it). </p>

<p>I would appreciate some insight on where this doubling comes from. Likewise I wonder whether or not FDR calculation this way <strong>assumes</strong> that the error rate for each spectra match is the same for the target database and decoy database (i.e. <em>assuming</em> that getting 25 decoy hits <em>implies</em> 25 target hits are also false positives). It's not really clear for me why the error rate has to be the same for the two databases. Any comments on this subject is also appreciated.</p>

<p><sub>* one such reference is Elias et al Nature Methods - 2, 667 - 675 (2005) </sub></p>
",2013-02-22 12:56:28.503
41244,15330.0,1,,,,Probability of heads in a biased coin,<probability>,CC BY-SA 3.0,"<p>Given $N$ flips of the same coin resulting in $k$ occurrences of 'heads', what is the probability density function of heads-probability of the coin?</p>
",2013-03-01 05:09:26.123
41914,13918.0,1,,,,Lewandowski algorithm demand forecasting,<estimation><forecasting>,CC BY-SA 3.0,"<p>I came across the Lewandowski method of demand forecasting in JDA Demand. Please help me understand at a high level the methodology it uses. I found a paper by Robert Hyndman titled 
""A state space framework for automatic forecasting using exponential smoothing methods"" and it uses this method as one of methods they compare their algorithm to in the paper. Currently for us this is a black box, we want to get some high level understanding so that we can better fine tune the parameters they have provided as part of the software. It would be great if you can share some thoughts about the Lewandowski algorithm and point to some references that I could use for further research.</p>
",2013-03-12 11:17:01.283
42513,15991.0,1,42517.0,,,Is it appropriate to plot the mean in a histogram?,<self-study><data-visualization><mean><histogram>,CC BY-SA 3.0,"<p>Is it ""okay"" to add a vertical line to a histogram to visualize the mean value? </p>

<p>It seems okay to me, but I've never seen this in textbooks and the likes, so I'm wondering if there's some sort of convention not to do that? </p>

<p>The graph is for a term paper, I just want to make sure I don't accidentally break some super important unspoken stats rule. :)</p>
",2013-03-19 21:23:49.697
42885,2615.0,1,58417.0,,,2SLS with two instruments for one endogenous variable in MATLAB,<matlab><2sls>,CC BY-SA 3.0,"<p>I have one endogenous variable and two instruments for it, and I want to calculate my beta with the direct (one step) matrix formula</p>

<p>$\beta_2sls = X' Z(Z'X)^{-1}Z'X^{-1}X'Z(Z'Z)^{-1}Z'Y$</p>

<p>But if I have two instruments for one endogenous variable X and Z are not the same length.</p>

<p>Any ideas?
Thanks!</p>
",2013-03-25 15:11:36.577
43458,16452.0,1,,,,How to check if removing a sample makes a difference in mean and stdev values?,<hypothesis-testing>,CC BY-SA 3.0,"<p>I'd like to ask if someone could help me with the following problem:</p>

<p>we have measured the same sample 5 times and we would like to check if there are significant differences in mean and stdev values if we use:</p>

<ul>
<li>All 5 datapoints</li>
<li>Only the last 4 datapoints</li>
<li>Only the last 3 datapoints</li>
</ul>

<p>We have performed ANOVA analysis but we are not sure about the results because we might not have homocedasticity.</p>

<p>Which tests would you do to investigate this issue?</p>

<p>Thanks in advance for your help. </p>
",2013-04-02 14:45:37.780
44370,8063.0,1,,,,A way to test for enrichment of differentially expressed genes in a genomic location,<r><microarray>,CC BY-SA 3.0,"<p>I have an experiment where I expect a certain genomic location to influence gene expression levels of nearby genes. I have data for expression levels (Agilent 4x44 microarrays, Drosophila) in two groups - one where I expect expression to be affected and the other wild-type and I would like to run a test for overrepresentation of differentially expressed genes in a genomic location.</p>

<p>My main problem is that I couldn't find a package (R/bioconductor) that would do it out of the box easily, so if you know about such a package, please let me know. In the meantime, this is what I figured out: I would run a sliding window over the whole genome and simply count number of differentially expressed genes in each window - this should tell me where I have the most differentially expressed genes in the genome. However, it will be dependent on gene density, so to obtain some sort of background distribution, I would run permutations of the samples (or p values), say, 1000 times, and check how often I am likely to find this number of windows with that number of differentially expressed genes compared to the observed numbers. Does this sound right?</p>

<p>I should add that while I know the location that would mess up things, I cannot exclude that any other genomic region would not be affected as well. So I have to test the whole genome.</p>

<p>Please advise on this approach and/or propose a better one...</p>
",2013-04-15 13:42:17.450
44635,728.0,1,,,,Testing symmetry of a distribution around its mean,<hypothesis-testing><ranking>,CC BY-SA 4.0,"<p>We can test the symmetry of a distribution around <span class=""math-container"">$0$</span> by Wilcoxon sign rank test, based on its sample.</p>
<p>But if we want to test if a distribution is symmetric around its mean, based on its sample <span class=""math-container"">$X_1, \dots, X_n$</span>, is it valid to first normalize <span class=""math-container"">$X_i$</span> by the sample mean as <span class=""math-container"">$Y_i := X_i - \bar{X}$</span>, and then apply Wilcoxon sign rank test to <span class=""math-container"">$Y_i$</span>'s?</p>
<p>If not, what are some ways?</p>
",2013-04-18 18:32:01.767
44772,17076.0,1,,,,"Appropriate Analysis for ordinal variable, repeated 4 times under different conditions, by the same 2 raters",<anova><repeated-measures><multilevel-analysis>,CC BY-SA 3.0,"<p>I am doubting myself on which analysis to run for the following:
18 participants were evaluated at 4 time points with different conditions at each time.
They were given scores (on a discrete visual analog scale) by 2 raters.</p>

<p>The scores were calculated for a pair of participants: the pairs changed at each time point.
I do know which participant comprises each pair.</p>

<p>Is that a 2-way repeated measures ANOVA? Some variation of Friedman test?</p>
",2013-04-20 21:00:03.237
45279,9095.0,1,,,,Visualizing results from multiple latent class models,<data-visualization><mixture-distribution><latent-class>,CC BY-SA 3.0,"<p>I am using latent class analysis to cluster a sample of observations based on a set of binary variables. I am using R and the package poLCA. In LCA, you must specify the number of clusters you want to find. In practice, people usually run several models, each specifying a different number of classes, and then use various criteria to determine which is the ""best"" explanation of the data. </p>

<p>I often find it very useful to look across the various models to try to understand how observations classified in model with class=(i) are distributed by the model with class = (i+1). At the very least you can sometimes find very robust clusters that exist regardless of the number of classes in the model. </p>

<p>I would like a way to graph these relationships, to more easily communicate these complex results in papers and to colleagues who aren't statistically oriented. I imagine this is very easy to do in R using some kind of simple network graphics package, but I simply don't know how.</p>

<p>Could anyone please point me in the right direction. Below is code to reproduce an example dataset. Each vector xi represents the classification of 100 observations, in a model with i possible classes. I want to graph how observations (rows) move from class to class across the columns. </p>

<pre><code>x1 &lt;- sample(1:1, 100, replace=T)
x2 &lt;- sample(1:2, 100, replace=T)
x3 &lt;- sample(1:3, 100, replace=T)
x4 &lt;- sample(1:4, 100, replace=T)
x5 &lt;- sample(1:5, 100, replace=T)

results &lt;- cbind (x1, x2, x3, x4, x5)
</code></pre>

<p>I imagine there is a way to produce a graph where the nodes are classifications and the edges reflect (by weights, or color maybe) the % of observations moving from classifications from one model to the next. E.g. </p>

<p><img src=""https://i.stack.imgur.com/muEii.png"" alt=""enter image description here""></p>

<p>UPDATE: Having some progress with the igraph package. Starting from the code above...</p>

<p>poLCA results recycle the same numbers to describe class membership, so you need to do a bit of recoding. </p>

<pre><code>N&lt;-ncol(results) 
n&lt;-0
for(i in 2:N) {
results[,i]&lt;- (results[,i])+((i-1)+n)
n&lt;-((i-1)+n)
}
</code></pre>

<p>Then you need to get all the cross-tabulations and their frequencies, and rbind them into one matrix defining all the edges. There is probably a much more elegant way to do this. </p>

<pre><code>results &lt;-as.data.frame(results)

g1           &lt;- count(results,c(""x1"", ""x2""))

g2           &lt;- count(results,c(""x2"", ""x3""))
colnames(g2) &lt;- c(""x1"", ""x2"", ""freq"")

g3           &lt;- count(results,c(""x3"", ""x4""))
colnames(g3) &lt;- c(""x1"", ""x2"", ""freq"")

g4           &lt;- count(results,c(""x4"", ""x5""))
colnames(g4) &lt;- c(""x1"", ""x2"", ""freq"")

results &lt;- rbind(g1, g2, g3, g4)

library(igraph)

g1 &lt;- graph.data.frame(results, directed=TRUE)

plot.igraph(g1, layout=layout.reingold.tilford)
</code></pre>

<p><img src=""https://i.stack.imgur.com/iCJ2Z.png"" alt=""enter image description here""></p>

<p>Time to play more with the igraph options I guess. </p>
",2013-04-26 17:31:28.260
45280,17326.0,1,,,,Statistical test for measure of association not assuming monotonicity in small samples (n=6)?,<hypothesis-testing><continuous-data>,CC BY-SA 3.0,"<p>I have two continuous variables which I have data from a physics experiment.</p>

<p>I want to test for association between the two variables but without assuming a monotonic relationship. I also only have 6 data point each with a large error associated with it and want the test to take this into consideration.</p>

<p>Does anyone know of a statistical test of this type?</p>
",2013-04-26 17:33:27.823
45457,17179.0,1,,,,Predicting high frequency finance time series with HMM,<r><machine-learning><hidden-markov-model>,CC BY-SA 3.0,"<p>I have a the following time series </p>

<pre><code>  Price      BrokerID 632 Behaviour  BrokerID 680 Behaviour ...BrokerID XYZ Behaviour

  5.6          IP                       SP                   
  5.7          BP                       IP
  5.8          SP                       BP
  5.83         IP                       SP
</code></pre>

<p>where <code>IP</code> is idle position, <code>BP</code> is buying position, and <code>SP</code> is selling position. I want to use Broker behaviour as the known variable and price as the hidden variable and predict it using HMM. But my question is how to find the emission matrix between a character vector (broker behaviour) and price numeric vector? </p>
",2013-04-29 17:03:08.587
45534,17447.0,1,45536.0,,,Question about Harrington paradox,<self-study><density-function><cumulative-distribution-function><paradox>,CC BY-SA 3.0,"<ol>
<li>Model<br>
The firm and enforcement agency interact in more than one domain. This may arise because a single agency is responsible for enforcing more than one regulation or because it enforces the same regulation at more than one constituent plant of a multi-plant firm.
For simplicity we will assume that the number of domains is two and that they are ex ante identical. In each domain the firm is required to comply with a regulation. If it complies it inflicts no environmental damage otherwise it inflicts damage d, which is commonly observed. The cost to the ith firm of compliance in domain j [ h1, 2j will be denoted cij where ci 1 and ci 2 are independent, privately observed draws from a distribution f(c) with associated cumulative F(c). F is common knowledge.<br>
If the agency observes non-compliance by a firm in either domain it can take that firm to court (â€˜â€˜pursueâ€™â€™ the firm), in which case the firm is subject to a penalty L which is exogenous. Penalties are assumed to be restricted in the sense that
F(L) &lt; 1. This implies that a policy of full-pursuit, whereby the agency pursues all 3
violations, will not generate full-compliance.
The firm and enforcement agency are both risk neutral and aim to maximise
expected profit and minimise expected environmental damage respectively.</li>
</ol>

<p>can someone explain to me what F(L) &lt; 1 implies?</p>

<p>if you need the context behind this model, please tell me ill explain that as well</p>
",2013-04-30 15:18:37.443
45543,17454.0,1,59109.0,,,Is the square root of the symmetric Kullback-Leibler divergence a metric?,<kullback-leibler><metric>,CC BY-SA 3.0,"<p>It is well known that the square root of the Jensen-Shannon divergence is a true metric, but how about the square root of symmetric KL: D(P||Q)+D(Q||P)? I have reasons to believe that it also is a true metric but cannot find any references on that other than anecdotal comments such as that it behaves more like a metric when used.</p>

<p>Update 1</p>

<p>Kullback-Leibler divergence: $D(P||Q) = \sum_i p_i\log(p_i/q_i)$</p>

<p>Jensen-Shannon divergence: $J(P,Q) = \big(D(P||(P+Q)/2)+D(Q||(P+Q)/2)\big)/2$</p>

<p>Symmetric KL divergence: $S(P,Q) = D(P||Q)+D(Q||P) = \sum_i (p_i-q_i)\log(p_i/q_i)$</p>

<p>Square root of symmetric KL: $d_{KL}(P,Q) = \sqrt{S(P,Q)}$ </p>

<p>Is $d_{KL}$ a metric?</p>

<p>Update 2</p>

<p>I think the following upper and lower bounds hold:</p>

<p>$\sum_i (p_i-q_i)^2 \leq  \sum_i (p_i-q_i)\log(p_i/q_i) \leq  \sum_i \log(p_i/q_i)^2$</p>

<p>Both of the square root of the bounds are metrics, I suppose, since they are the square of the Euclidean distances in the probability space and the log-prob space respectively. </p>
",2013-04-30 17:27:21.667
45804,17580.0,1,,,,Finding the similarity between two functions,<similarities><genetic-algorithms><function><kolmogorov-smirnov-test>,CC BY-SA 3.0,"<p>I am a first-year grad student in Computer Science, and I need some help with a problem that I think is statistically oriented. I have taken a statistics course, but it was abysmal and I haven't had time to rectify that. But anyway, my problem stems from a project I'm working on involving genetic programming, where I'm randomly generating functions. Please bear with my description, as it's been a while since I've had a formal theory course too.</p>

<p>I have two continuous (but not onto) functions <strong>F</strong> and <strong>G</strong>, both of which map <strong>N</strong> variables to a single output. The domain of the input variables is the integers between -100 and 100. The range of the output is the Real numbers. I want to find some statistical measure of how ""similar"" the two functions are; given the finite inputs (of which there will be 201^N possible), how much variance(?) there is between the two functions outputs. Two identical functions should return no variance, and two wildly different functions should return a high variance.</p>

<p>Since <strong>N</strong> will typically be greater than 6, I can't iterate through all the possible inputs and compare the outputs, so I figured I could take some sampling at regular intervals (e.g. every multiple of 10, so that it's only 10^N). But here's about where I realize I have no idea what I'm doing. How do I determine if two numbers are ""highly variant"" from each other? What sample size do I need to use to have confidence in my results?</p>

<p>My current approach is to compare the functions with a two-sided Kolmogorov-Smirnov Test. Since that test doesn't seem to scale well to multi-variate problems, I've taken advantage of my limited domains to just treat the problem as having a single variable by concatenating my variables. So the first value of the variable is (-100:100:100:100:100:100), the second is (-100:100:100:100:100:099), and the last is (100:100:100:100:100:100). Does that even make sense?</p>
",2013-05-03 21:05:29.093
46070,17678.0,1,,,,Variance of a time series fitted to an ARIMA model,<r><variance><arima>,CC BY-SA 3.0,"<p>I think this is a basic question, but maybe I am confusing the concepts.</p>

<p>Suppose I fit an ARIMA model to a time series using, for example, the function auto.arima() in the R forecast package. The model assumes constant variance. How do I obtain that variance? Is it the variance of the residuals?</p>

<p>If I use the model for forecasting, I know that it gives me the conditional mean. I'd like to know the (constant) variance as well.</p>

<p>Thank you.</p>

<p>Bruno</p>

<hr>

<h2>Update 1:</h2>

<p>I added some code below. The variance given by <code>sigma2</code> isn't close to the one calculated from the fitted values. I'm still wondering if <code>sigma2</code> is the right option. See figure below for time series plot.</p>

<pre><code>demand.train &lt;- c(10.06286, 9.56286, 10.51914, 12.39571, 14.72857, 15.89429, 15.89429, 17.06143, 
              17.72857, 16.56286, 14.23000, 15.39571, 13.06286, 15.39571, 15.39571, 16.56286,
              16.21765, 15.93449, 14.74856, 14.46465, 15.38132)
timePoints.train &lt;- c(""Q12006"", ""Q22006"", ""Q32006"", ""Q12007"", ""Q22007"", ""Q32007"", ""Q12008"", ""Q22008"",
                      ""Q32008"", ""Q12009"", ""Q22009"", ""Q32009"", ""Q12010"", ""Q22010"", ""Q32010"", ""Q12011"",
                      ""Q22011"", ""Q32011"", ""Q12012"", ""Q22012"", ""Q32012"")

plot(1:length(timePoints.train), demand.train, type=""o"", xaxt=""n"", ylim=c(0, max(demand.train) + 2), 
     ylab=""Demand"", xlab=""Quadrimestre"")

title(main=""Time Series Demand of Product C"", font.main=4)
axis(1, at=1:length(timePoints.train), labels=timePoints.train)
box()

### ARIMA Fit
library(forecast)

# Time series
demandts.freq &lt;- 3
demandts.train &lt;- ts(demand.train, frequency=demandts.freq, start=c(2006, 1))

# Model fitting
demandts.train.arima &lt;- auto.arima(demandts.train, max.p=10, max.q=10, max.P=10, max.Q=10, max.order=10)
print(demandts.train.arima)
summary(demandts.train.arima)
demandts.train.arima.fit &lt;- fitted(demandts.train.arima)

# Forecast ARIMA (conditional means)
demandts.arima.forecast &lt;- forecast(demandts.train.arima, h = 3, level=95)
print(demandts.arima.forecast)

# Constant variance from ARIMA
demandts.arima.var &lt;- demandts.train.arima$sigma2
print(demandts.arima.var)

# Variance from fitted values
print(var(demandts.train.arima.fit))
</code></pre>

<p><img src=""https://i.stack.imgur.com/E5gv0.png"" alt=""Time Series Plot""></p>
",2013-05-07 19:48:58.193
46384,15839.0,1,,,,gaussian mixture HMM,<hidden-markov-model>,CC BY-SA 3.0,"<p>What is the difference of gaussian HMM and gaussian mixture HMM (the emission is gaussian or  gaussian mixture)? I want to know if it is the same thing. What is the point when estimating the parameters using Baum Welch algorithm.</p>
",2013-05-13 01:51:18.740
46894,18085.0,1,58636.0,,,Imputation variance and explained variance (in vector autoregression),<multiple-imputation><vector-autoregression>,CC BY-SA 3.0,"<p>I have a question concerning the coefficients of VAR models used on multiple imputed data (high missigness in some variables: up to 40%).
In particular I would like to know how the coefficients are related to the explained variance. </p>

<p>I have used vector autoregression on multiple imputed data (m=10) and have then combined the estimated coefficient with rubin's rule.
However, what confuses me is the fact that my imputation variance is quite small in relationship to the estimates and variance of coefficients, but the difference between the explained variance is huge (17% to 0.04%) between models.</p>

<p>My idea is that since the highest imputation variance across all systems is at the constant (around a third of the variance value but 3-4 times higher then in other coefficients) and that this critically affects the explained variance.
But thats just a guess.</p>

<p>I would be very happy if somebody could help me here.</p>
",2013-05-19 14:17:34.503
47447,18356.0,1,,,,How to fit a simple count time series INAR(1) model,<r><time-series><count-data><intervention-analysis>,CC BY-SA 3.0,"<p>I am trying to perform a simple time series analysis with count time series data. My data is a sequence of small integer values like 0,1,2 and 3. I learned from various sources that INAR model would be appropriate with such data. </p>

<p>My question is whether anyone knows R codes for fitting a simple INAR(1) model (regressing time series data on a binary dummy variable).  </p>

<p>Appreciate any assistance.</p>
",2013-05-27 18:56:55.533
47497,18382.0,1,,,,R Code for Yeo-Johnson transformation,<r><data-transformation>,CC BY-SA 3.0,"<p>I have writen code for a Box-Cox transformation (see below). But now I want to do a Yeo-Johnson transformation because <code>datc$plot</code> contains zeros. I tried, but I didn't find a solution.</p>

<pre><code>lambda.fm1 &lt;- boxcox(datc$plot ~ datc$cond.evlot*datc$cond.dl*datc$version), 
                     family=""yjPower"")
lambda.max &lt;- lambda.fm1$x[which.max(lambda.fm1$y)]
require(car)
datc$plott &lt;- bcPower(datc$plot, lambda = lambda.max, jacobian.adjusted = FALSE)
</code></pre>
",2013-05-28 08:54:55.873
47846,17994.0,1,,,,"Difference between binomial, negative binomial and Poisson regression",<spss><references><binomial-distribution><poisson-distribution><negative-binomial-distribution>,CC BY-SA 3.0,"<p>I am looking for some information about the difference between binomial, negative binomial and Poisson regression and for which situations are these regression best fitted. </p>

<p>Are there any tests I can perform in SPSS that can tell me which of these regressions is the best for my situation?</p>

<p>Also, how do I run a Poisson or negative binomial in SPSS, since there are no options such as I can see in the regression part?</p>

<p>If you have any useful links I would appreciate it very much.</p>
",2013-06-02 09:36:07.877
47981,16990.0,1,57334.0,,,Is a p-value of 0.04993 enough to reject null hypothesis?,<hypothesis-testing><statistical-significance><p-value>,CC BY-SA 3.0,"<p>In a Wilcoxon signed-ranks statistical significance test, we came across some data that produces a $p$-value of $0.04993$. With a threshold of $p &lt; 0.05$, is this result enough to reject the null hypothesis, or is it safer to say the test was inconclusive, since if we round the p-value to 3 decimal places it becomes $0.050$?</p>
",2013-06-04 09:21:32.970
57128,22505.0,1,,,,Pooling regression results in SPSS,<regression><spss><pooling>,CC BY-SA 3.0,"<p>I have to solve the following issue:</p>

<ol>
<li>I run my linear regression model many times (let's say 1000 times) with two variables: y - continuous dependent variable, x - continuous independent variable (mean of several consequent measurements).</li>
<li>The independent variable in each model was randomly drawn using its mean and standard deviation</li>
<li>I have the regression coefficient and standard error for this independent variable in each of the models.</li>
</ol>

<p>Somehow I have to combine these results into one regression result. As far as I know the regression coefficients of 1000 models can be just averaged. However, this is not really clear to me how can I estimate the total variance of 1000 models.</p>
",2013-10-09 10:06:51.963
48103,11200.0,1,48133.0,,,Fit a sinusoidal term to data,<r><regression><fitting><profile-likelihood>,CC BY-SA 4.0,"<p>Although I read <a href=""https://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r"">this</a> post, I still have no idea how to apply this to my own data and hope that someone can help me out.</p>
<p>I have the following data:</p>
<pre><code>y &lt;- c(11.622967, 12.006081, 11.760928, 12.246830, 12.052126, 
       12.346154, 12.039262, 12.362163, 12.009269, 11.260743, 
       10.950483, 10.522091,  9.346292,  7.014578,  6.981853,  
       7.197708,  7.035624,  6.785289, 7.134426,  8.338514,  
       8.723832, 10.276473, 10.602792, 11.031908, 11.364901, 
       11.687638, 11.947783, 12.228909, 11.918379, 12.343574, 
       12.046851, 12.316508, 12.147746, 12.136446, 11.744371,  
       8.317413, 8.790837, 10.139807,  7.019035,  7.541484,  
       7.199672,  9.090377,  7.532161,  8.156842,  9.329572, 
       9.991522, 10.036448, 10.797905)
t &lt;- 18:65
</code></pre>
<p>And now I simply want to fit a sine wave</p>
<p><span class=""math-container"">$$
y(t)=A\cdot sin(\omega t+\phi) +C.
$$</span></p>
<p>with the four unknowns <span class=""math-container"">$A$</span>, <span class=""math-container"">$\omega$</span>, <span class=""math-container"">$\phi$</span> and <span class=""math-container"">$C$</span> to it.</p>
<p>The rest of my code looks is the following</p>
<pre><code>res &lt;- nls(y ~ A*sin(omega*t+phi)+C, data=data.frame(t,y), 
           start=list(A=1,omega=1,phi=1,C=1))
co &lt;- coef(res)

fit &lt;- function(x, a, b, c, d) {a*sin(b*x+c)+d}

# Plot result
plot(x=t, y=y)
curve(fit(x, a=co[&quot;A&quot;], b=co[&quot;omega&quot;], c=co[&quot;phi&quot;], d=co[&quot;C&quot;]), 
        add=TRUE ,lwd=2, col=&quot;steelblue&quot;)
</code></pre>
<p>But the result is really poor.</p>
<p><img src=""https://i.stack.imgur.com/IS0ae.png"" alt=""Sine fit"" /></p>
<p>I would very much appreciate any help.</p>
",2013-06-05 18:23:47.483
48125,18416.0,1,,,,Loss for Kernel Ridge Regression,<regression><machine-learning><mathematical-statistics><ridge-regression><kernel-trick>,CC BY-SA 3.0,"<p>Is $||Y-X\beta||_2^2 + \lambda\beta^T K\beta$ , the standard loss-function in kernel ridge regression, or is it different? Also, is the gaussian kernel a standard choice used for the kernel, in practice? If not, which kernels are used more often than not? Also, is $\lambda$ the only parameter to be tuned via cross-validation or is the kernel parameter like $\sigma$ in a gaussian kernel, also tuned via cross validation in practice? Please confirm and/or correct my understanding of Kernel ridge regression!</p>
",2013-06-06 01:38:15.400
48597,18905.0,1,,,,"T-test shows no differences, but the experiment group shows tendency more benefit in all variables measured than control group",<t-test><statistical-power>,CC BY-SA 3.0,"<p>I've just finished an animal experiment. I compared 1 control group and 1 experimental group, the only difference between the two is type of diet. For statistical analysis I used the independent groups t-test, and the result showed no significant differences between the two groups. However, the data shows the tendency that the experimental group has more benefit in all variables measured. So, what should I say about my data? All data are normally distributed.  </p>

<p>My supervisor said that maybe because I used very small sample (each group n=8) that I could not find any significant differences. He suggested me to do some ""probability test"" or something to extrapolate my data (unfortunately, I don't have any clue what he was talking about).  </p>

<p>So, is there any statistical analysis that I can use like what my supervisor told me to do?</p>
",2013-06-13 04:46:26.887
48658,1926.0,1,,,,Bayesian network inference using pymc (Beginner's confusion),<bayesian><inference><bayesian-network><pymc>,CC BY-SA 3.0,"<p>I am currently taking the PGM course by Daphne Koller on Coursera. In that, we generally model a Bayesian Network as a cause and effect directed graph of the variables which are part of the observed data. But on PyMC tutorials and examples I generally see that it not quite modeled in the same way as the PGM or atleast I am confused. In PyMC the parents of any observed real world variable are often the parameters of the distribution that you use to model the variable. </p>

<p>Now my question really is a practical one. Suppose I have 3 variables for which data is observed (A, B, C) (lets assume they are all continuous variables just for the sake of it). From some domain knowledge, one can say that A and B cause C. So we have a BN here - A, B are the parents and C is the children.
now from the BN equation P(A, B, C) = P(C | A, B) * P(A) * P(B)</p>

<p>I can say A and B are some normal distributions with some mu and sigma, but how do I model P(C | A, B) ?
The general idea I want to learn, is how do I learn this BN using PyMC so that I can query the BN. Or do I have to augment the BN with parameters of the model  in some fashion.</p>

<p>Is this problem solvable using pymc? or have I got some fundamentals wrong?</p>

<p>Any help would be appreciated!</p>
",2013-06-13 19:42:13.623
49879,19492.0,1,53471.0,,,What is an adaptive copula?,<data-visualization><copula>,CC BY-SA 3.0,"<p>My basic question is: What is an adaptive copula?</p>
<p>I have slides from a presentation (unfortunately, I cannot ask the author of the slides) about adaptive copulae and I am not getting, what this means resp. what this is good for?</p>
<p>Here are the slides:
<img src=""https://i.stack.imgur.com/0F76A.png"" alt=""sl1"" />
<img src=""https://i.stack.imgur.com/F3H0r.png"" alt=""sl2"" />
Then the slides continue with a change-point Test. I am wondering what this is about and why I need this in connection to copulae?</p>
<p>The slides end with an adaptively estimated parameter plot:
<img src=""https://i.stack.imgur.com/qJXPm.png"" alt=""sl3"" />
<img src=""https://i.stack.imgur.com/jYIy9.png"" alt=""sl4"" /></p>
<p>This seems to show, that my estimates are lagged behind. Any other interpretations, comments would be great!</p>
",2013-07-03 11:48:24.817
51644,3733.0,1,,,,Adding random effect influences coefficient estimates,<r><mixed-model><random-effects-model>,CC BY-SA 3.0,"<p>I have always been taught that random effects only influence the variance (error), and that fixed effects only influence the mean. But I have found an example where random effects influence also the mean - the coefficient estimate:</p>

<pre><code>require(nlme)
set.seed(128)
n &lt;- 100
k &lt;- 5
cat &lt;- as.factor(rep(1:k, each = n))
cat_i &lt;- 1:k # intercept per kategorie
x &lt;- rep(1:n, k)
sigma &lt;- 0.2
alpha &lt;- 0.001
y &lt;- cat_i[cat] + alpha * x + rnorm(n*k, 0, sigma)
plot(x, y)

# simulate missing data
y[c(1:(n/2), (n*k-n/2):(n*k))] &lt;- NA

m1 &lt;- lm(y ~ x)
summary(m1)

m2 &lt;- lm(y ~ cat + x)
summary(m2)

m3 &lt;- lme(y ~ x, random = ~ 1|cat, na.action = na.omit)
summary(m3)
</code></pre>

<p>You can see that the estimated coefficient for <code>x</code> from model <code>m1</code> is -0.013780, while from model <code>m3</code> it is 0.0011713 - both significantly different from zero.</p>

<p>Note that when I remove the line simulating missing data, the results are the same (it is full matrix).</p>

<p>Why is that?</p>

<p>PS: please note I am not a professional statistician, so if you are about to respond with a lot of math then please make also some simple summary for dummies :-)</p>
",2013-07-24 09:19:26.983
57137,21896.0,1,57209.0,,,Negative Binomial Regression: is parameter theta (R) the reciprocal of parameter kappa (SAS)?,<r><sas><negative-binomial-distribution>,CC BY-SA 3.0,"<p>After some frantic googling I do believe the answer is yes, but more so I am frustrated that the relation between the two parameters seems to be nowhere described explicitely so I do it here. (I hope this isn't against the rules of stackexchange.)</p>

<p><a href=""http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1141&amp;context=usdeptcommercepub&amp;sei-redir=1&amp;referer=http%3A%2F%2Fscholar.google.nl%2Fscholar_url%3Fhl%3Dnl%26q%3Dhttp%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%253Farticle%253D1141%2526context%253Dusdeptcommercepub%26sa%3DX%26scisig%3DAAGBfm3vz9gDbxRveIafikl02v0aeUyu0w%26oi%3Dscholarr%26ei%3DDj9VUqWlL6LG0QXe1oHwAg%26ved%3D0CDAQgAMoADAA#search=%22http%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1141%26context%3Dusdeptcommercepub%22"" rel=""nofollow noreferrer"">This very nice article</a> states: we will denote the random variable Y having a negative binomial distribution as Y ~ NB($\mu, \kappa$) with a parameterization such that E(Y) = $\mu$, var(Y) = $\mu + \kappa \mu^2$.</p>

<p>I take this latter equation as the definition of $\kappa$.</p>

<p><a href=""http://books.google.nl/books?id=Ohks0xwvyT4C&amp;pg=PA196&amp;lpg=PA196&amp;dq=kappa+parameter+negative+binomial+proc+glimmix&amp;source=bl&amp;ots=PYKpaGQ8VN&amp;sig=5sNEB-7H7ZocErTKhi35ORKd2lA&amp;hl=nl&amp;sa=X&amp;ei=lEBVUqCnNcTJ0QXppYGoAg&amp;ved=0CDYQ6AEwAA#v=onepage&amp;q=kappa%20parameter%20negative%20binomial%20proc%20glimmix&amp;f=false"" rel=""nofollow noreferrer"">Apparently</a> this kappa is implemented in SAS.</p>

<p>Now turning to R, the function <code>glm.nb</code> in the <code>MASS</code> package contains a parameter $\mu$ which is obviously the same $\mu$ as above and a parameter $\theta$. The question is how $\theta$ and $\kappa$ are related. The documentation for <code>glm.nb</code> only refers to it as an ""additional parameter"". The answers to <a href=""https://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r"">this</a> and <a href=""https://stats.stackexchange.com/questions/10457/interpreting-negative-binomial-regression-output-in-r?rq=1"">this</a> stackexchange questions directly imply that $\theta = 1/\kappa$, but <a href=""https://stats.stackexchange.com/questions/30360/what-is-the-distribution-of-theta-in-a-negative-binomial-model-glm-nb-with-r?rq=1"">this</a> question [EDIT: since removed] seems to suggest that $\theta = \kappa$. </p>

<p>The <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/NegBinomial.html"" rel=""nofollow noreferrer"">help page for negative binomial in R</a> is nice and introduces a parameter called <code>size</code> that equals $1/\kappa$. Fitting <code>glm.nb</code> on random data generated by <code>rnbinom</code> for various choices of $\mu$ and <code>size</code> seems to support the thesis that $\theta = 1/\kappa$ (i.e. that $\theta$ = <code>size</code>) but also that for large values of size the estimation is poor.</p>

<p>Summarizing: I do believe that $\theta = 1/\kappa$ but it would be nice if there were an easily googlable place on the internet stating this explicitly. Maybe one of the answers to this questions can serve as such a place? </p>
",2013-10-09 12:00:22.943
49906,5821.0,1,67660.0,,,Relationship between McNemar's test and conditional logistic regression,<logistic><mcnemar-test><clogit>,CC BY-SA 3.0,"<p>I am interested in the modeling of binary response data in paired observations. We aim to make inference about the effectiveness of a pre-post intervention in a group, potentially adjusting for several covariates and determining whether there is effect modification by a group that received particularly different training as part of an intervention.</p>

<p>Given data of the following form:</p>

<pre><code>id phase resp
1  pre   1
1  post  0
2  pre   0
2  post  0
3  pre   1
3  post  0
</code></pre>

<p>And a $2 \times 2$ contingency table of paired response information:</p>

<p>\begin{array}{cc|cc}
&amp; &amp; \mbox{Pre} &amp; \\ 
&amp; &amp; \mbox{Correct} &amp; \mbox{Incorrect} \\ \hline
\mbox{Post} &amp; \mbox{Correct} &amp; a &amp; b&amp;\\
 &amp; \mbox{Incorrect} &amp; c&amp; d&amp;\\
\end{array}</p>

<p>We're interested in the test of hypothesis: $\mathcal{H}_0: \theta_c = 1$.</p>

<p>McNemar's Test gives: $Q = \frac{(b-c)^2}{b+c} \sim \chi^2_1$ under $\mathcal{H}_0$ (asymptotically). This is intuitive because, under the null, we would expect an equal proportion of the discordant pairs ($b$ and $c$) to be favoring a positive effect ($b$) or a negative effect ($c$). With the probability of positive case definition defined $p  =\frac{b}{b+c}$ and $n=b+c$. The odds of observing a positive discordant pair is $\frac{p}{1-p}=\frac{b}{c}$.</p>

<p>On the other hand, conditional logistic regression uses a different approach to test the same hypothesis, by maximizing the conditional likelihood:</p>

<p>$$\mathcal{L}(X ; \beta) = \prod_{j=1}^n \frac{\exp(\beta X_{j,2})}{\exp(\beta X_{j,1}) +  \exp(\beta X_{j,2})}$$</p>

<p>where $\exp(\beta) = \theta_c$.</p>

<p>So, what's the relationship between these tests? How can one do a simple test of the contingency table presented earlier? Looking at calibration of p-values from clogit and McNemar's approaches under the null, you'd think they were completely unrelated!</p>

<pre><code>library(survival)
n &lt;- 100
do.one &lt;- function(n) {
  id &lt;- rep(1:n, each=2)
  ph &lt;- rep(0:1, times=n)
  rs &lt;- rbinom(n*2, 1, 0.5)
  c(
    'pclogit' = coef(summary(clogit(rs ~ ph + strata(id))))[5],
    'pmctest' = mcnemar.test(table(ph,rs))$p.value
  )
}

out &lt;- replicate(1000, do.one(n))
plot(t(out), main='Calibration plot of pvalues for McNemar and Clogit tests', 
  xlab='p-value McNemar', ylab='p-value conditional logistic regression')
</code></pre>

<p><img src=""https://i.stack.imgur.com/HC8YV.jpg"" alt=""enter image description here""></p>
",2013-07-03 17:50:20.467
50739,10492.0,1,,,,Linear Regression and ANOVA,<regression><anova>,CC BY-SA 3.0,"<p>I found two very useful posts about the difference between linear regression analysis and ANOVA and how to visualise them:</p>

<p><a href=""https://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared"">Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?</a></p>

<p><a href=""https://stats.stackexchange.com/questions/5278/how-to-visualize-what-anova-does"">How to visualize what ANOVA does?</a></p>

<p>As stated in the first post, to test whether the average height of male and females is the same you can use a regression model ($y = \alpha + \beta x + \epsilon$, where $y$ denotes height and $x$ denotes gender) and test whether $\beta = 0$. If $\beta = 0$, then there is no difference in the height between males and females. However, I am not quite sure how this is tested when you have three groups. Imagine the following example:</p>

<pre><code>height (y) -  group (x)
5          -  A
6          -  A
7          -  A
6          -  A
30         -  B
32         -  B
34         -  B
33         -  B 
20         -  C
19         -  C
21         -  C
22         -  C
</code></pre>

<p>The regression model would look like:</p>

<p>$$y = a+ b x + \epsilon$$</p>

<p>I quickly visualized the data (see image below)</p>

<p>They way I understood the regression model is that it would now test whether
any of the three slopes (AB, AC or BC) has a slope $b$ which is significantly different from 0. If that's the case one can conclude like in an ANOVA that there is at least one group in which height is significantly different from one or more groups. Afterwards, one could use a post-hoc test of course to test which of the groups really differ. Is my understanding of how the regression models tests this hypothesis correct?</p>

<p><img src=""https://i.stack.imgur.com/6LD5Q.png"" alt=""enter image description here""></p>
",2013-07-14 10:14:44.800
50982,1790.0,1,57600.0,,,Comparing distributions of generalization performance,<cross-validation><model-selection>,CC BY-SA 3.0,"<p>Say that I have two learning methods for a <strong>classification</strong> problem, $A$ and $B$, and that I estimate their generalization performance with something like repeated cross validation or bootstrapping. From this process I get a <strong>distribution of scores</strong> $P_A$ and $P_B$ for each method across these repetitions (e.g. the distribution of ROC AUC values for each model).</p>

<p>Looking at these distributions, it could be that $\mu_A \ge \mu_B$  but that $\sigma_A \ge \sigma_B$ (i.e. the expected generalization performance of $A$ is higher than $B$, but that there is more uncertainty about this estimation).</p>

<p>I think this is called the <strong><a href=""http://en.wikipedia.org/wiki/Bias-variance_dilemma"" rel=""noreferrer"">bias-variance dilemma</a></strong> in regression.</p>

<p>What <strong>mathematical methods</strong> can I use to compare $P_A$ and $P_B$ and eventually make an informed decision about which model to use?</p>

<p><strong>Note:</strong> For the sake of simplicity, I am referring to two methods $A$ and $B$ here, but I am interested in methods that can be used to compare the distribution of scores of ~1000 learning methods (e.g. from a grid search) and eventually make a final decision about which model to use.</p>
",2013-07-17 13:51:44.960
51047,17056.0,1,57714.0,,,"Confused with MCMC Metropolis-Hastings variations: Random-Walk, Non-Random-Walk, Independent, Metropolis",<markov-chain-montecarlo><metropolis-hastings>,CC BY-SA 3.0,"<p>Over the past few weeks I have been trying to understand MCMC and the Metropolis-Hastings algorithm(s). Every time I think I understand it I realise that I am wrong. Most of the code examples I find on-line implement something that is not consistent with the description. i.e.: They say they implement Metropolis-Hastings but they actually implement random-walk metropolis. Others (almost always) silently skip the implementation of the Hastings correction ratio because they are using a symmetric proposal distribution. Actually, I haven't found a single simple example that calculates the ratio so far. That makes me even more confused. Can someone give me code examples (in any language) of the following:</p>

<ul>
<li>Vanilla Non-Random Walk Metropolis-Hastings Algorithm with Hastings correction ratio calculation (even if this will end up being 1 when using a symmetric proposal distribution).</li>
<li>Vanilla Random Walk Metropolis-Hastings algorithm.</li>
<li>Vanilla Independent Metropolis-Hastings algorithm.</li>
</ul>

<p>No need to provide the Metropolis algorithms because if I am not mistaken the only difference between Metropolis and Metropolis-Hastings is that the first ones always sample from a symmetric distribution and thus they don't have the Hastings correction ratio.
No need to give detailed explanation of the algorithms. I do understand the basics but I am kinda confused with all the different names for the different variations of the Metropolis-Hastings algorithm but also with how you practically implement the Hastings correction ratio on the Vanilla non-random-walk MH. Please don't copy paste links that partially answer my questions because most likely I have already seen them. Those links led me to this confusion. Thank you.</p>
",2013-07-18 06:53:11.527
51496,19870.0,1,,,,Should CRT decision tree node be mutually exclusive?,<classification><predictive-models><cart>,CC BY-SA 3.0,"<p>I have been trying to understand the results of a CRT decision tree, my question is if the terminal nodes should be mutually exclusive? I am asking this because by reading the terminal nodes some variables seems to overlap each other. </p>

<p>For instance some terminal nodes ""share"" the same profession:</p>

<p>Node 23: carpenter, plumber, sole trader, truck driver </p>

<p>Node 24: plumber, truck driver, teacher, retired. </p>

<p>Probably I am reading the results incorrectly because it should not happen, at least in theory.</p>
",2013-07-23 03:50:02.683
51577,20097.0,1,,,,LIBSVM parameter search in time series,<regression><time-series><svm><libsvm>,CC BY-SA 3.0,"<p>I try to predict values for regression in LIBSVM. My data is in time series. I use gridregression.m file in LIBSVM to find optimal parameters c, g and p. Gridregression.m file use cross validation to find optimal parameters, but is it ok to use cross validation in time series? </p>

<p>When I use parameters from gridregression.m, sometimes the MSE is not better then the default values. (  cmd= '-s 3 -t 2' is sometimes better )</p>
",2013-07-23 19:27:17.943
51895,9384.0,1,,,,SMOTE throws error for multi class imbalance problem,<r><classification><unbalanced-classes><oversampling>,CC BY-SA 3.0,"<p>I am trying to use SMOTE to correct imbalance in my multi-class classification problem.
Although SMOTE works perfectly on the iris dataset as per the SMOTE help document, it does not work on a similar dataset.
Here is how my data looks. Note it has three classes with values 1, 2, 3.</p>

<pre><code>&gt; data
   looking risk every status
1        0    1     0      1
2        0    0     0      1
3        0    0     0      2
4        0    0     0      1
5        0    0     0      1
6        3    0     0      1
7        0    0     0      1
8        0    0     0      1
9        0    1     0      1
10       0    0     0      1
11       0    0     0      3
12       0    0     0      1
13       0    0     0      1
14       0    0     0      1
15       0    0     0      2
</code></pre>

<p>It is in the form of dataframe, same as iris:</p>

<pre><code>&gt; class(data)
[1] ""data.frame""
</code></pre>

<p>Here is my code using SMOTE and the error that it throws:</p>

<pre><code>&gt; newData &lt;- SMOTE(status ~ ., data, perc.over = 600,perc.under=100)
Error in scale.default(T, T[i, ], ranges) : subscript out of bounds
In addition: Warning messages:
1: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
2: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
3: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
4: In FUN(newX[, i], ...) : no non-missing arguments to min; returning Inf
5: In FUN(newX[, i], ...) : no non-missing arguments to min; returning Inf
6: In FUN(newX[, i], ...) : no non-missing arguments to min; returning Inf
</code></pre>
",2013-07-26 19:31:50.407
52099,20363.0,1,,,,Multinomial likelihood for large number of groups,<r><multinomial-distribution><likelihood>,CC BY-SA 3.0,"<p>I am trying to investigate the following problem using multinomial likelihoods and could really do with some advice regarding its appropriateness and implementation in R.</p>

<p>A sequence is generated by selecting with replacement from a bag of n differently coloured balls and consists of the number of occurrences of each colour in the selection (i.e. each sequence is a vector of length n with each element a count corresponding to the number of occurrences of a particular colour in the sequence). The process is then repeated a number of times to generate a group of unique sequences (duplicate sequences are rejected).</p>

<p>If a single sequence is selected at random as the test subject and a multinomial model is generated for each of the other sequences, using the colour count proportions as probabilities, can the likelihood be calculated for each multinomial model in the group using the test sequence as the data and would the greatest likelihood indicate the most alike sequence from the group?</p>

<p>I have tried implementing this in R but am struggling with a couple of points.</p>

<ol>
<li>Calculating the likelihood fails if the number of colours is large since the factorial term falls out of bounds.  </li>
<li>If the number of occurrences of each colour relative to the total number of colours is small then the probability is small and the product of the $p^x$ terms tends to zero.  </li>
</ol>

<p>I hope this makes sense and somebody is able to offer some advice. </p>
",2013-07-29 19:04:43.263
52126,20367.0,1,,,,How to evaluate Likert scale data changes over multiple surveys of the same group?,<regression><anova><likert>,CC BY-SA 3.0,"<p>I have five surveys of the same group of students over a semester.  Each survey uses a 5-point Likert scale.  The first and last survey contain some questions dealing with the beginning and end of the class (first impressions, final impressions), but most of the questions are identical for all four or five of the surveys.</p>

<p>I want to evaluate the statistical significance of changes to students' responses over time.  Unfortunately statistics is not my strong suit.  I know of the t-test, but that seems to only be applicable to two groups of data (please correct me if I'm wrong).  How should I go about evaluating this data?  Is a repeated measures one-way ANOVA appropriate?</p>
",2013-07-30 00:44:55.017
52449,18845.0,1,57768.0,,,Definition of autocorrelation time (for effective sample size),<r><time-series><correlation>,CC BY-SA 3.0,"<p>I've found two definitions in the literature for the autocorrelation time of a weakly stationary time series:</p>

<p>$$
\tau_a = 1+2\sum_{k=1}^\infty \rho_k \quad \text{versus} \quad \tau_b = 1+2\sum_{k=1}^\infty \left|\rho_k\right|
$$</p>

<p>where $\rho_k = \frac{\text{Cov}[X_t,X_{t+h}]}{\text{Var}[X_t]}$ is the autocorrelation at lag $k$.  </p>

<p>One application of the autocorrelation time is to find the ""effective sample size"": if you have $n$ observations of a time series, and you know its autocorrelation time $\tau$, then you can pretend that you have</p>

<p>$$
n_\text{eff} = \frac{n}{\tau}
$$</p>

<p>independent samples instead of $n$ correlated ones for the purposes of finding the mean.  Estimating $\tau$ from data is non-trivial, but there are a few ways of doing it (see <a href=""http://arxiv.org/abs/1011.0175"">Thompson 2010</a>).</p>

<p>The definition without absolute values, $\tau_a$, seems more common in the literature; but it admits the possibility of $\tau_a&lt;1$.  Using R and the ""coda"" package:</p>

<pre><code>require(coda)
ts.uncorr &lt;- arima.sim(model=list(),n=10000)         # white noise 
ts.corr &lt;- arima.sim(model=list(ar=-0.5),n=10000)    # AR(1)
effectiveSize(ts.uncorr)                             # Sanity check
    # result should be close to 10000
effectiveSize(ts.corr)
    # result is in the neighborhood of 30000... ???
</code></pre>

<p>The ""effectiveSize"" function in ""coda"" uses a definition of the autocorrelation time equivalent to $\tau_a$, above.  There are some other R packages out there that compute effective sample size or autocorrelation time, and all the ones I've tried give results consistent with this:  that an AR(1) process with a negative AR coefficient has <em>more</em> effective samples than the correlated time series.  This seems strange.  </p>

<p>Obviously, this can never happen in the $\tau_b$ definition of autocorrelation time.</p>

<p>What is the correct definition of autocorrelation time?  Is there something wrong with my understanding of effective sample sizes?  The $n_\text{eff} &gt; n$ result shown above seems like it must be wrong... what's going on?</p>
",2013-08-02 14:46:27.663
52567,728.0,1,,,,"Meaning of ""design"" in design matrix?",<regression><terminology>,CC BY-SA 3.0,"<p>In linear regression, $Y= X\beta$, why is $X$ called the design matrix? Can $X$ be designed or constructed arbitrarily to some degree as in art? </p>
",2013-08-04 18:26:28.673
52871,18447.0,1,,,,Trap 66 in WinBUGS in a hierarchical Bayesian modeling,<r><hierarchical-bayesian><winbugs>,CC BY-SA 3.0,"<p>I want to analyze a multilevel multidimensional model in WinBUGS. the model is as below (N=2362 students responding to K=45 items of a test, students are nested within J=116 schools):</p>

<pre><code>model{
#responses
for(i in 1:N){
    for(j in 1:K){
        logit(p[i,j])&lt;- a1[j]*th[i,1]+a2[j]*th[i,2]-b[j]
        y[i,j]~dbern(p[i,j] )
    }
    th[i,1:2]~dmnorm(mu[sc[i],1:2],tau.p[1:2,1:2])
}
#school level
for(j in 1:J){  
    mu[j,1:2]~dmnorm(m[j,1:2],tau.s[1:2,1:2])
}    

#priors
for(j in 1:J){
    m[j,1:2]~dmnorm(m0[1:2],cov[1:2,1:2])
}

tau.p[1:2,1:2]~dwish(cov[1:2,1:2],2)
tau.s[1:2,1:2]~dwish(cov[1:2,1:2],2)
sigma.p[1:2,1:2]&lt;-inverse(tau.p[,])
sigma.s[1:2,1:2]&lt;-inverse(tau.s[,])
s2p&lt;-sum(sigma.p[,])
s2s&lt;-sum(sigma.s[,])
rho&lt;-(s2s)/(s2s+s2p)

a1[1]~dlnorm(0,4)
a2[1]&lt;-0
b[1]~dnorm(0,1)
for(s in 2:K) {
    a1[s]~dlnorm(0,4)
    a2[s]~dlnorm(0,4)
    b[s]~dnorm(0,1)
}    
}
</code></pre>

<p>I've set these functions as initial values:</p>

<pre><code>ini&lt;-function(){
list(tau.p=matrix(rgamma(4,100,100),2,2),
tau.s=matrix(rgamma(4,100,100),2,2),
th=rmvnorm(N,mean=c(0,0),sigma=diag(2)),
m=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
mu=rmvnorm(J,mean=c(0,0),sigma=diag(2)),
a1=rlnorm(K,0, 0.4),
a2=c(NA,rlnorm(K-1,0, 0.4)),
b=rnorm(45,0,0.5))
}
</code></pre>

<p>I use rube package in R to check and run my analysis and everything looks fine. When I run the model I receive ""Trap 66 (postcondition violated)"" or ""undefined real result"". I think the problem is from the initials but I have no idea how to solve it.</p>

<p>Any idea?</p>
",2013-08-09 03:06:43.727
52910,3731.0,1,57224.0,,,What are typically encountered condition numbers in social science?,<regression><linear-model><multicollinearity>,CC BY-SA 3.0,"<p>As part of my thesis, I'm proving (or attempting to prove...) a few asymptotic results. Because these results depend on the condition number, I'd like to have some idea about the typical sizes of a condition numbers that crop up in social science research. That way, I can give some guidance about how large the sample size has to be before we reach the happy land of asymptopia.  </p>

<p>I'd be happy for any guidance. </p>

<p><strong>My very specific</strong> setup is as follows. For the standard Generalized Least Squares (GLS) model</p>

<p>$$Y = X\beta + e \quad \quad \quad e \sim N(0, V\sigma^2) $$</p>

<p>where $V$ is assumed to be known and positive definite, we define </p>

<p>$$ X^- = (X^\top X)^{-1} X^\top \quad \quad \quad U = (I-XX^-)V$$
and the condition number $\kappa$ </p>

<p>$$ \kappa = \frac{ \lambda_{\text{max}} }{ \lambda_{\text{min}} } $$</p>

<p>where the $\lambda_\star$ values are the maximum and minimum eigenvalues of the matrix $U$.  </p>

<p>Does anyone have pointers to references for the sizes of condition numbers in social science research? I don't even know where to look. Any pointers for either </p>

<ol>
<li>OLS estimators (used incorrectly in a GLS context as posed above)</li>
<li>GLS estimators (correctly analyzed) </li>
<li>REML/ML estimators where $V$ is
estimated and then conditioned upon, or </li>
<li>OLS fixed effect only models
where $V$ is the identity matrix</li>
</ol>

<p>would be most welcome!</p>
",2013-08-09 13:48:43.087
53261,449.0,1,53264.0,,,What is the $\mu^2$ squared effect size?,<effect-size>,CC BY-SA 3.0,"<p>I was recently looking at a paper in the journal <em>Psychological Science</em> and came across this:</p>

<p><em>F</em>(1, 71) = 4.5, <em>p</em> = .037, $\mu^2$ = .06</p>

<p><em>F</em>(1, 71) = 0.08, <em>p</em> = .78, $\mu^2$ = .001</p>

<p>I was wondering what the $\mu^2$ is in the above. Typically in APA the third thing should be either the MSE or it should be a standardized effect size (or you should have all 4). I'm guessing it's a standardized effect size of some sort but I'm not familiar with it and searching the net has turned up nothing. The actual effect, as near as I can tell from the graph, is about 12 for the first one.</p>

<p>Is this an effect size I haven't heard of yet or a typo in the article?</p>

<p>Farrelly, D., Slater, R., Elliott, H. R., Walden, H. R. and Wetherell, M. A. (2013) Competitors Who Choose to Be Red Have Higher Testosterone Levels. <em>Psychological Science</em>, DOI:10.1177/0956797613482945</p>

<p>Here's a screen shot of the text (p.2)</p>

<p><img src=""https://i.stack.imgur.com/h5BtM.png"" alt=""enter image description here""></p>
",2013-08-14 03:06:30.697
53384,20838.0,1,58388.0,,,Bootstrapping residuals: Am I doing it right?,<time-series><matlab><bootstrap><residuals>,CC BY-SA 4.0,"<p><strong>First of all:</strong> 
From what I understood, bootstrapping residuals works as follows:</p>

<ol>
<li>Fit model to data </li>
<li>Calculate the residuals</li>
<li>Resample the residuals and add them to 1.</li>
<li>Fit model to new dataset from 3.</li>
<li>Repeat <code>n</code> times, but always add the resampled residuals to the fit
from 1.</li>
</ol>

<p>Is that correct so far?</p>

<hr>

<p><strong>What I want to do</strong> is something slightly different:</p>

<p>I want to estimate parameter and prediction uncertainty for an algorithm that estimates some environmental variable.</p>

<p>What I have is a error-free time-series (from a simulation) of that variable, <code>x_true</code>, to which I add some noise, <code>x_noise</code>, in order to generate a synthetic dataset <code>x</code>.
I then try to find optimal parameters by fitting my algorithm with the sum of squares <code>sum((x_estimate - x_true)^2)</code> (! not <code>x_estimate - x</code> !) as an objective function. In order to see how my algorithm performs and to create samples of my parameters' distributions, I want to resample <code>x_noise</code>, add it to <code>x_true</code>, fit my model again, rinse and repeat. Is that a valid approach to assess parameter uncertainty? Can I interpret the fits to the bootstrapped datasets as prediction uncertainty, or do I have to follow the procedure I posted above?</p>

<p>/edit: I think I haven't really made clear what my model does. Think of it as essentially something like a de-noising method. It's not a predictive model, it's an algorithm that tries to extract the underlying signal of a noisy time-series of environmental data.</p>

<p>/edit^2: <strong>For the MATLAB-Users</strong> out there, I wrote down some quick &amp; dirty linear regression example of what I mean.</p>

<p>This is what I believe ""ordinary"" bootstrapping of residuals is (please correct me if I'm wrong): <a href=""http://pastebin.com/C0CJp3d1"" rel=""noreferrer"">http://pastebin.com/C0CJp3d1</a></p>

<p>This is what I want to do: <a href=""http://pastebin.com/mbapsz4c"" rel=""noreferrer"">http://pastebin.com/mbapsz4c</a></p>
",2013-08-15 21:35:20.763
53391,20312.0,1,,,,How do I calculate random baseline?,<machine-learning>,CC BY-SA 3.0,"<p>I am a bit confused as to how to calculate random baseline. If I understand correctly the random baseline is calculated by adding up the squared probabilities of all the classes. The random baseline classifier thus picks a class at random, instead of choosing the most frequent one.</p>

<p>I have 7 classes, each with # of items and a total of X. How do I find the probabilities? </p>
",2013-08-15 23:38:00.327
53404,20820.0,1,,,,Why do we use a one-tailed test F-test in analysis of variance (ANOVA)?,<anova><f-test><sums-of-squares><f-distribution>,CC BY-SA 3.0,"<p>Can you give the reason for using a one tailed test in the analysis of variance test?</p>

<p>Why do we use a one-tail test - the F-test - in ANOVA?</p>
",2013-08-16 06:36:58.590
53439,5208.0,1,,,,How would you frame this as a machine learning problem?,<machine-learning>,CC BY-SA 3.0,"<p>I have a trading software that buys and sells loans. There's an auction site where borrowers ask for some money and lenders bid on them until the borrower is fully funded and the auction ends. There's lots of information on each loan request. My trading bot always bids at the highest possible interest rate, if it is outbid, then it just re-bids slightly lower. Once I win the loan parts, I can sell them at a markup. Right now, I sell at the minimum markup, so that with fees I barely make a profit.</p>

<p>What I'm not sure is what markup I should sell? The lower the markup the faster my loan parts sell, but I will get less profit too. On what loans should I bid? Should I bid on a loan auction with a higher interest rate, but which is not going to end for several days, thereby leaving my money stale, or should I bid on an auction with a lower interest rate, but which is going to end very soon. Sometimes in the former case, the borrower might decide to take the loan and not wait until the end of the auction, thereby I could secure a better interest rate than just bidding on the loan auction due to end soon.</p>

<p>I was thinking of framing this problem as reinforcement learning, but I'm not sure how to do it. My goal is to maximiz the profit I make from trading loans. Any ideas?</p>
",2013-08-16 15:35:05.747
54234,21204.0,1,,,,Weighting time series coefficients using model's likelihood,<time-series><forecasting><arima><outliers>,CC BY-SA 3.0,"<p>I have a question regarding to time series forecasting. In particular I've been working with a Bayesian approach, but I think the question is independent from that.</p>

<p>I have several time series which are very stable in time, except on specific dates that they have sudden changes. The problem is that if I use a forecasting technique that looks at the past to predict the future, such as ARIMA, the days after the sudden changes have high impact on the forecast.</p>

<p>Thus, to give a simple example, suppose I'm predicting $x_{t+1} = \sum \beta_j x_j, j&lt;t+1$, I would like to add another weight witch accounts for the probability of $x_j$, something like $x_{t+1} = \sum f(x_j)\beta_j x_j, j&lt;t+1$ where $f(x_j)$ is proportional to $P(x_j)$. </p>

<p>Thus, a sudden change has low probability and should not contribute to the prediction.</p>

<p>Does anyone know how to deal with these kind of problems? I'm trying to implement this in a Bayesian model, but I'm now sure how I should do it.</p>
",2013-08-28 15:56:16.150
54506,21322.0,1,,,,How to analyse these data?,<regression><logistic>,CC BY-SA 3.0,"<p>I am conducting an experiment investigating lineup accuracy and witness confidence.  </p>

<p>A long story short: we want to know what the pattern of false positives, hits and misses on a lineup task are under different lineup conditions and how confidence may vary with/independently of accuracy.  Logically, witness confidence may also be affected by the different conditions, and we'd like to know this as well.  </p>

<p>The between subjects variables are: Gender (male, female), ethnicity (Asian, Caucasian), and lineup type (sequential- where people see each lineup member one at a time and make a decision about each one, and simultaneous- where people see all the lineup members and make a decision about whether they see the perpetrator or not)</p>

<p>The within subjects variables are: Photo type (same vs different photo of the person), lineup ethnicity (Asian vs. Caucasian lineups), confidence (5 levels of a Likert scale from 1 ""not confidence at all"" to 5 ""extremely confident)</p>

<p>The dependent variable is accuracy in terms of hits, misses and false positives (these could be coded as 0 or 1?) and correct recognition (hits-false positives)</p>

<p>One of the problems is that we want to know the relationship between confidence and accuracy, which would necessitate that confidence is an independent variable, however we also want to know if the other variables might affect confidence (such as ethnicity or lineup type), so I'm having trouble figuring out the best way to analyse this data.  </p>

<p>Does anyone have any answers for me?  Someone suggested maybe logistic regression, but they weren't really sure.  I'm really not used to dealing with categorical data, so am in need of help!  </p>
",2013-09-02 04:48:52.793
54574,11283.0,1,,,,Log-likelihood distance measure validity for clustering,<r><distance-functions><likelihood><markov-chain><k-medoids>,CC BY-SA 3.0,"<p>I have calculated log-likelihood distances between 50 sequences according to the Formula (1): </p>

<p>$$
D(X_i,X_j)= 1/2(\log p(X_i|Mod_j)+\log p(X_j|Mod_i)),  
$$
where $
p(X_i|Mod_j)
$ is the likelihood of sequence $X_i$ being produced by model $Mod_j$, where $Mod_j$ is a corresponding Markov model of the given $Seq_j$, defined by its Transition Probability Matrix and Start Probabilities Vector.  The measure is symmetrical as seen from the definition. To make the measure more ""legible"" and similar to the traditional measures, I compute distance$=(1-D)$ from formula (1). Thus, $D(X_i,X_i) = 0$ and the distance increases if the likelihood decreases. </p>

<p>Now, I have a 50x50 Distance Matrix.I have run a ""meaningfullness"" check, and it seemed ok for me - i.e. more similar sequences had smaller distance and very different ones had very large distance. The distances seemed to satisfy the triangle inequality. However, I have noticed that:</p>

<p>1)  the shorter sequences seem to be ""closer"" to all other sequences than longer ones. It seems that this distance measure is biased to favor short distances. </p>

<p>2) I have tried PAM-clustering with the distance matrix by converting my distance matrix to dist object in <code>R</code> by using as.dist(), and my results were very bad, even for 2 clusters or 49 ( max avg.silhouette width produced by <code>R</code> function pam was 0.28). With some numbers of clusters the avg.silhouette widths were even negative. </p>

<p>I am coming to conclusion that my way of computing medoids is invalid/conceptually wrong. What could be the problem? Can log-likelihood distance matrix be used with medoids clustering at all? </p>

<p>edit: I am including the heatmap of the distance matrix, where x- and y-axis represent sequences (1 through 50th). It looks strange to me but I cannot pinpoint what exactly doesn't feel right. </p>

<p><img src=""https://i.stack.imgur.com/RcSBc.png"" alt=""heatmap""></p>
",2013-09-03 10:48:31.337
54622,12744.0,1,54624.0,,,"Do ""true"" multi-level models require Bayesian methods?",<multilevel-analysis><mixed-model><hierarchical-bayesian>,CC BY-SA 3.0,"<p>I've been recently learning about mixed effects models (e.g. via Fitzmaurice, Laird, and Ware 's book <em>Applied Longitudinal Analysis</em>) as well as Bayesian hierarchical models (e.g. via Gelman and Hill's book <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>) </p>

<p>One curious thing I've noticed:  The Bayesian literature tends to emphasize that their models can handle covariates at multiple level of analysis.  For example, if the clustering is by person, and each person is measured in multiple ""trials,"" then the Bayesian hierarchical models can investigate the main effects of covariates both at the subject and trial level, as well as interactions across ""levels.""</p>

<p>However, I have not seen these kinds of models in the textbooks introducing frequentist methods.    </p>

<p>I'm not sure if this is a coincidence, or an example of where Bayesian methods can do ""more complicated things.""  Is it possible to use mixed effects models (e.g. the lme4 or nlme packages in the R statistical software) to investigate interactions of covariates across ""levels"" of analysis?</p>
",2013-09-03 21:37:13.153
54637,21382.0,1,58044.0,,,How to get pooled p-values on tests done in multiple imputed datasets?,<r><spss><p-value><multiple-imputation><pooling>,CC BY-SA 3.0,"<p>Using Amelia in R, I obtained multiple imputed datasets. After that, I performed a repeated measures test in SPSS. Now, I want to pool test results. I know that I can use Rubin's rules (implemented through any multiple imputation package in R) to pool means and standard errors, but how do I pool p-values? Is it possible? Is there a function in R to do so?
Thanks in advance.</p>
",2013-09-04 01:06:26.173
54724,10957.0,1,60584.0,,,Flexible and inflexible models in machine learning,<machine-learning><model>,CC BY-SA 3.0,"<p>I came across a simple question on comparing flexible models (i.e. splines) vs. inflexible models (e.g. linear regression) under different scenarios. The question is:</p>

<p>In general, do we expect the performance of a flexible statistical learning method to perform better or worse than an inflexible method when:</p>

<ol>
<li>The number of predictors $p$ is extremely large, and the number of observations $n$ is small?  </li>
<li>The variance of the error terms, i.e. $Ïƒ^2 = \text{Var}(e)$, is extremely high?</li>
</ol>

<p>I think for (1), when $n$ is small, inflexible models are better (not sure). For (2), I don't know which model is (relatively) better.</p>
",2013-09-04 20:24:41.247
54836,20304.0,1,58209.0,,,Implementing Latent Dirichlet Allocation - notation confusion,<gibbs><dirichlet-distribution><topic-models>,CC BY-SA 3.0,"<p>I am trying to implement LDA using the collapsed Gibbs sampler from 
<a href=""http://www.uoguelph.ca/~wdarling/research/papers/TM.pdf"" rel=""nofollow noreferrer"">http://www.uoguelph.ca/~wdarling/research/papers/TM.pdf</a></p>

<p>the main algorithm is shown below</p>

<p><img src=""https://i.stack.imgur.com/X9OwX.png"" alt=""enter image description here""></p>

<p>I'm a bit confused about the notation in the inner-most loop. n_dk refers to the count of the number of words assigned to topic k in document d, however I'm not sure which document d this is referring to.  Is it the document that <em>word</em> (from the next outer loop) is in?  Furthermore, the paper does not show how to get the hyperparameters alpha and beta.  Should these be guessed and then tuned?  Furthermore, I don't understand what the <em>W</em> refers to in the inner-most loop (or the beta without the subscript).</p>

<p>Could anyone enlighten me?</p>
",2013-09-06 15:56:05.860
54915,21523.0,1,,,,Implementing Pettitt test in R,<r><time-series><heteroscedasticity>,CC BY-SA 3.0,"<p>I'm trying to implement Pettitt test in R following papers like this <a href=""http://www.ias.ac.in/jess/forthcoming/JESS-D-13-00049.pdf"" rel=""nofollow"">pdf</a> (pp. 5 &amp; 6), or this <a href=""http://www.igu.in/17-3/paper-2.pdf"" rel=""nofollow"">pdf</a>. But, I'm misunderstanding something, because having tested it with some data, I think that output is not correct.</p>

<p>Here is the code:</p>

<pre><code>pettitt &lt;- function(x, alpha=0.99) {
# Pettitt AN. 1979 A non-parametric approach to the change point detection.
# x is a vector
# alpha, integer, level of significance
x &lt;- na.omit(x)
o &lt;- rank(x)
s &lt;- c()
L &lt;- length(x)
for (i in 1:(L-1)) {
      s &lt;- c(s, 2*(colSums(as.matrix(o[1:i]))) - (i*(L+1)) )
}
vc &lt;- sqrt((-1) * log(alpha) * (L^3 + L^2)/6)
output &lt;- list(abs(s), vc)
return(output)
}
</code></pre>

<p>Testing with <code>larain</code> and <code>tempdub</code> dataset from <code>TSA package</code>:</p>

<pre><code>library(TSA)
data(larain)
data(tempdub)
pettitt(larain)
[[1]]
  [1]  78 118 180  76  30  30 144  90 124 148 224 334 314 298 362 444 356 334
 [19] 300 302 194 121  83  55  45  57  25  95 175 195 193 287 181 231 175 213
 [37] 301 331 421 345 392 322 282 354 372 274 194 130 188 248 175  97  85 153
 [55] 105 171 181 189 245 297 401 375 449 557 467 551 594 576 602 490 406 354
 [73] 262 266 362 248 244 214 208 200 247 147  89  13   9  15  97   5   9  83
 [91]   3  95 123  63  31  12  44   6  48  34  72 108 208 164 170 282 214 148
[109] 202 140 104   6 102  86

[[2]]
[1] 50.69224

&gt; max(pettitt(larain)[[1]])
[1] 602

pettitt(tempdub)
[[1]]
  [1]  83 161 226 235 164  60  80 169 220 219 188  74  57 177 266 281 228 147
 [19]  19  82 125 140 102  41 100 197 235 254 233 141   1  97 144 153 112  26
 [37]  73 206 255 258 235 137  28  49  98 101  46  29 149 252 281 274 247 160
 [55]  43  70 115 126  79  22 157 248 317 328 287 224  96  27  86  79  27  82
 [73] 225 348 407 406 351 256 125  10  58  77  32  61 200 314 381 386 353 216
 [91] 124  40  35  70  35  36 173 302 365 386 321 242 131  10  51  38  19 146
[109] 241 319 342 359 330 223  89  45 113 144 111   2 123 228 280 275 250 177
[127]  34  50  89 102  59  22 131 248 334 359 302 198  73  46  83 100  73

[[2]]
[1] 70.96777

&gt; max(pettitt(tempdub)[[1]])
[1] 407
</code></pre>

<p>I don't know if I lost something in pettitt test or there are error in my code.</p>
",2013-09-08 13:37:22.690
45536,15663.0,2,,45534.0,,,,CC BY-SA 3.0,"<p>It means that the fine is lower than the compliance cost.</p>

<p>This is what Harrington Paradox (<a href=""http://en.wikipedia.org/wiki/Harrington_paradox"" rel=""nofollow"">http://en.wikipedia.org/wiki/Harrington_paradox</a>) show: </p>

<p>In the case of rational economics entities a firm will maximize its profit. This is not what is observed in reality. In theory, if the fine is lower than compliance cost a rationnal entity will not pay.  In reality the fine is lower than compliance cost, but firms pay.</p>

<p>This suggest image concern ( or altruism....)</p>
",2013-04-30 15:36:32.853
55150,21630.0,1,55182.0,,,Mathematical definition of causality,<econometrics><causality><conditional-expectation>,CC BY-SA 3.0,"<p>Let $Y$ and $X$ be random variables. $E(Y|X)$ is the conditional mean of $Y$ given $X$. We say $Y$ is not causally related to $X$ if $E(Y|X)$ does not depend on $X$, which implies it is equal to $E(Y)$. Now, let's go along with this definition of causality for a second. By the law of iterated expectations, $E(XE(Y|X)) = E(E(XY|X)) = E(XY)$. This means that if $E(Y|X)$ does not depend on $X$, if it is equal to $E(Y)$, then $E(X)E(Y) = E(XY)$. </p>

<p>In other words: </p>

<p>If $X$ and $Y$ are not causally related, then $X$ and $Y$ are uncorrelated! - This makes no sense and I know this must be wrong. Have I defined causality incorrectly? What have I done wrong? </p>

<p>In econometrics we generally assume $E(Y|X) = b_0 + b_1X$. So $E(Y|X) = E(Y)$ is equivalent to $b_1 = 0$. The logic applies in this specific scenario too.</p>
",2013-09-12 01:13:21.980
55209,20222.0,1,,,,Interpretation of Kolmogorov-Smirnov Critical Value Generated Distributions,<distributions><statistical-significance><confidence-interval><kolmogorov-smirnov-test>,CC BY-SA 3.0,"<p>As a non-statistician, I need help in interpreting a customer specified two-part reliability requirement that I think involves KS.</p>

<p>Requirement Part 1</p>

<p>R[4 years] must be greater than or equal to 0.95 and</p>

<p>R[8 years] must be greater than or equal to 0.85</p>

<p>I have plotted the reliability (survival) function of a 2-parameter Weibull distribution that meets the above requirement in Plot A below. The shape parameter is 1.664 and the characteristic life is 23.844 for this distribution.</p>

<p><img src=""https://i.stack.imgur.com/bY5JN.png"" alt=""Plot A]![enter image description here""></p>

<p>Requirement Part 2</p>

<p>The confidence level shall be 90% when demonstrating the Part 1 requirement via product life testing.</p>

<p>Itâ€™s the Part 2 that Iâ€™m a bit shaky on. On page 8-54 of MIL-HDBK-338B (<a href=""http://www.sre.org/pubs/Mil-Hdbk-338B.pdf"" rel=""nofollow noreferrer"">http://www.sre.org/pubs/Mil-Hdbk-338B.pdf</a>) there is a table showing KS critical â€œdâ€ values as a function samples size, N and significance level, alpha (also note the plot on page 8-57). From this table I took a d value of 0.264 based on a signficance value of 0.10 and a sample size of 20. Plot B below shows my result. My interpretation of Plot B is that after running a life test on 20 samples that if the resulting reliability plot does not fall below the lower boundary shown in Plot B then we have met the requirements.</p>

<p><img src=""https://i.stack.imgur.com/ilZqf.png"" alt=""Plot B]![enter image description here""></p>

<p>I have two questions:</p>

<ol>
<li><p>Did I translate the Part 2 requirement properly when I used an alpha of 0.10 to obtain the KS critical value of 0.264 ?  In other words, does a 90% confidence equal a 0.10 significance within the KS context ? If not, can someone provide guidance ?</p></li>
<li><p>How would <em>you</em> interpret Plot B ?</p></li>
</ol>

<p>Many thanks.</p>

<hr>

<p>Response to owensmartin's answer</p>

<p>Thank you for your reply. I'll take your ""your reasoning is not incorrect"" statement as a big vote of confidence. I just have a few items below that I'd appreciate comments from anyone on.</p>

<p>a. You are right in that my reliability functions are also survival functions. I believe the Nominal curve is simply the compliment of the nominal CDF.</p>

<p>b. Although I haven't computed the statistical power, I'm not that surprised when you say it is ""very low at this sample size"". However our customer is comfortable with the sample size of 20 and by extension the resulting low power.</p>

<p>c. Our customer is also comfortable with using the referenced Military Handbook and the associated KS critical values shown therein even though they may be approximate. As time permits I'm hoping to learn how to compute these critical values ""exactly"" so as not to have to rely on the handbook values.</p>

<p>d. To answer your question as to whether I really need the Weibull fit ? The short answer is no as there is nothing particularly ""magical"" about the Weibull distribution. The slightly longer answer is that I'm not sure how else to produce a nominal survival curve that I can then apply the d value of 0.264 to. The nominal curve shown in Plot B was constructed by solving the two simultaneous equations for the shape parameter and characteristic life knowing that R[4] =0.95 and R[8]=0.85 meet the requirements. This is shown in Plot A. I suspect that other commonly used distribution types may not be able to meet both of these requirements simultaneously. But because of the inherent flexibility of the Weibull, it <em>is</em> able.</p>

<p>e. As for being sure about the 90% confidence, I think as long as the fitted distribution resulting from the 20 sample life test does not drop below the lower boundary shown in Plot B would you agree that we can say we are 90% confident we meet the stated requirements? The only exception I can see to this is if when fitting the test data to a distribution that if the degree of fit is so poor as to raise question to its validity. But in solving engineering problems, this is always present as a concern that needs to be dealt with.</p>

<p>f. Regarding failed vs didn't fail test results, we normally strive for having each of the 20 samples fail so that we can avoid dealing with the added uncertainty associated with censored or suspended data points. We achieve this via accelerated-life testing methods which essentially compresses time by increasing either the duty cycle and/or the stress value(s).</p>

<p>Thanks for any further insight into this.</p>
",2013-09-12 19:12:06.933
55260,13459.0,1,,,,Which regression tree to use for large data?,<r><regression><cart><large-data>,CC BY-SA 3.0,"<p>I have a dataframe with 2 million rows and approximately 200 columns / features. Approximately 30-40% of the entries are blank. I am trying to find important features for a binary response variable.  The predictors may be categorical or continuous. </p>

<p>I started with applying logistic regression, but having so much missing entries I feel that this is not a good approach as glm discard all records which have any item blank. So I am now looking to apply tree based algorithms (<code>rpart</code> or <code>gbm</code>) which are capable to handle missing data in a better way. </p>

<p>Since my data is too big for <code>rpart</code> or <code>gbm</code>, I decided to randomly fetch 10,000 records from original data, apply <code>rpart</code> on that, and keep building a pool of important variables. However, even this 10,000 records seem to be too much for the <code>rpart</code> algorithm. </p>

<p>What can I do in this situation? Is there any switch that I can use to make it fast? Or it is impossible to apply <code>rpart</code> on my data. </p>

<p>I am using the following rpart command:</p>

<pre><code>varimp = rpart(fmla,  dat=tmpData, method = ""class"")$variable.importance
</code></pre>
",2013-09-13 15:45:00.397
55361,227.0,1,85707.0,,,Locomotive problem with various size companies,<bayesian><conditional-probability><bayes>,CC BY-SA 3.0,"<p>I'm working through Think Bayes (free here: <a href=""http://www.greenteapress.com/thinkbayes/"">http://www.greenteapress.com/thinkbayes/</a>) and I'm on exercise 3.1. Here's a summary of the problem:</p>

<p>""A railroad numbers its locomotives in order 1..N. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad has.""</p>

<p>This solution is found with the likelihood function and exponential prior like so:</p>

<pre><code>class Train(Suite):
  def __init__(self, hypos, alpha=1.0):
    # Create an exponential prior
    Pmf.__init__(self)
    for hypo in hypos:
      self.Set(hypo, hypo**(-alpha))
    self.Normalize()
  def Likelihood(self, data, hypo):
    if hypo &lt; data:
      return 0
    else:
      return (1.0/hypo)
</code></pre>

<p>Conceptually this is saying, if we see a train number larger than one of our hypotheses (1...1000) then every hypothesis that's smaller has a zero chance of being correct. The rest of the hypotheses have a 1/number_of_trains chance of showing us a train with this number.</p>

<p>In the exercise I'm working on the author then adds on a little extra. This assumes there's only one company. In real life however you'd have a mixture of big and small companies and bigger companies (both equally likely). However, this would mean that you're more likely to see a train from a bigger company since they'd have more trains.</p>

<p>Now the question is how to reflect this in the likelihood function?</p>

<p>This isn't Stack Overflow so I'm not really asking for coding help, but instead perhaps just help about how I might think about this problem in terms of a likelihood function.</p>
",2013-09-15 23:02:42.580
55436,21778.0,1,,,,Creating a high predictive value classifier,<classification><predictive-models>,CC BY-SA 3.0,"<p>I have a two-class classification problem with n-dimensional data. I would like to train a classifier (preferably but not necessarily linear) with 100% positive predictive value. In other words, I want the model to completely avoid one of the classes. For this application a low-ish sensitivity is OK as long as PPV is ~100%.<br>
Do you have any suggestions of good techniques to use? 
Thank you!</p>
",2013-09-16 22:19:43.457
55576,21833.0,1,,,,Generating random numbers based on partial correlation data,<time-series><correlation><matlab><random-generation><partial-correlation>,CC BY-SA 3.0,"<p>I need to generate random numbers based on already existing partial correlation data (not correlation or covariance data). Specifically, a 168*12 matrix based on a 12*12 partial correlation matrix. The idea is to simulate a data matrix that can be used for testing a few components of a project.</p>

<p>Any help in this regard would be appreciated. I have looked around but have not found any threads that talk about doing this with partial correlation data.</p>

<p>If someone has ideas about implementation in MATLAB, that would be a bonus!</p>

<p>Thanks a lot in advance!</p>

<p>Additions:
Apologies for any ambiguity. </p>

<p>-What I mean by partial correlation matrix is a matrix containing the partial correlations, calculated for any two pairs by partialling out effect of all other pairs.</p>

<p>-The goal is: given a matrix of partial correlation values, is there a way I can generate a data set (168*12) that would have these partial correlation values?</p>

<p>-If there is a method to convert partial correlation to correlation values, that would be appreciated as well.</p>

<p>Thanks again!</p>
",2013-09-18 12:09:15.653
55609,21842.0,1,,,,how to forecast daily sale using Excel,<forecasting><excel>,CC BY-SA 4.0,"<p>I am trying to find a method or a formula to forecast meals per day, which have Ì€5 meals to upload on flights, sales, wastage and passengers are what I have to consider, the old template is not completed yet, and its not so good to forecast, and I can't think of other formulas or methods to forecast , I have the sales in the past few months.. anyone can suggest me which methods can solve this problem? well I am using MS.Excel to calculate or if there is a program to suggest me it would be great.</p>
",2013-09-18 18:04:46.113
55617,21846.0,1,57825.0,,,Regression with rank order as dependent variable,<regression><multiple-regression><ordinal-data><ranking>,CC BY-SA 3.0,"<p>I have data on 44 firms that have all been ranked by an expert. The ""best"" firm has rank 1, the second best has rank 2, ..., the last one has rank 44. 
I have a bunch of explanatory variables and would like to explain the rank of the firm on the basis of these variables. My inclination is to use a regression model, but am concerned about the fact that the dependent variable is limited, it can only be a positive discrete number. </p>

<p>I have thought about ordinal regression, but that seems impossible since I would have as many categories as I have observations. </p>

<p>What regression models would be possible? (preferably to be run in R)</p>
",2013-09-18 19:30:10.233
55722,21885.0,1,56091.0,,,Looking for a good and complete probability and statistics book,<probability><self-study><mathematical-statistics><references>,CC BY-SA 3.0,"<p>I never had the  opportunity to visit a stats course from a math faculty. I am looking for a probability theory and statistics book that is complete and self-sufficient. By complete I mean that it contains all the proofs and not just states results. By self-sufficient I mean that I am not required to read another book to be able to understand the book. Of course it can require college level (math student) calculus and linear algebra.</p>

<p>I have looked at multiple books and I didn't like any of them.</p>

<ul>
<li><p>DeGroot &amp; Schervish (2011) <em><a href=""http://rads.stackoverflow.com/amzn/click/0321500466"">Probability and Statistics (4th Edition)</a></em> Pearson</p>

<p>This is not complete enough. It just states a lot of stuff without the derivation.  Besides that I like it.</p></li>
<li><p>Wasserman (2004) <em><a href=""http://www.amazon.de/All-Statistics-Statistical-Inference-Springer/dp/0387402721"">All of Statistics: A Concise Course in Statistical Inference</a></em> Springer.</p>

<p>Didn't like it at all. Almost no explanations.</p></li>
</ul>

<p>""Weighing the Odds"" from David Williams is more formal than DeGroot and seems to be complete and self-sufficient. However, I find the style strange. He also invents new terms that only he seems to use. All the stuff that is explained in DeGroot too is explained better there.</p>

<p>If you know a great book in German that's also fine as I am German.</p>
",2013-09-19 22:14:08.257
56273,22126.0,1,,,,Frequency Distribution,<descriptive-statistics>,CC BY-SA 3.0,"<p>I have a question that is very important to me related to the book <em>Basic Statistics for Business and Economics</em> for organizing data into a frequency distribution:</p>

<blockquote>
  <p>Step 1: Decide on the number of classes. The goal is to use just enough groupings or classes to reveal the shape of the distribution. Some judgment is needed here. A useful recipe to determine the number of classes ($k$) is the ""2 to the $k$ rule"". This guide suggests you select the smallest number ($k$) for the number of classes such that $2^k$ is greater than the number of observations ($n$): [$n \le 2^k$â€‹]</p>
</blockquote>

<p>I want to know, how can I prove this formula?</p>
",2013-09-27 11:50:54.613
56372,21108.0,1,57285.0,,,Item correlation for recommender system,<correlation><recommender-system>,CC BY-SA 3.0,"<p>I just made an implementation of P(A|B)/P(Â¬A|B) for a ""people who bought this also bought..."" algorithm.</p>

<p>I'm doing it by </p>

<pre><code>P(A|B) = count_users(bought_A_and_B)/count_users(bought_A)
P(Â¬A|B) = count_users(bought_B_but_not_A)/count_users(did_not_buy_A)
</code></pre>

<p>Then dividing the top one by the bottom one I get a score which makes absolute sense, but what kind of correlation am I calculating? What is this method called? Where can I read more about it?</p>

<p><strong>[EDIT]</strong> This is not for using in a production environment, it is just some algorithm which appeared out of the blue in an online course I'm taking, I was just wondering where it could come from. Also, when the number of users who bought item B but not item A is zero I just skip the pair until I get more data. The same goes on when the number of users who bought A is zero.</p>
",2013-09-28 23:42:17.863
56445,22189.0,1,,,,"Testing for significance between means, having one normal distributed sample and one non normal distributed",<distributions><experiment-design><wilcoxon-mann-whitney-test>,CC BY-SA 3.0,"<p>I have following problem: </p>

<p>Within an independent groups 1-factor design I have two independent groups, with a sample size of 20 each. The data of the treatment group is not normally distributed, whereas the data for the control group is (checked with Shapiro-Wilk Normality Test). Now I want to check if the differences of the means of both groups are significant. 
What is the appropriate test for this? I think it should be the Wilcoxon Rank Sum and Signed Rank Test, but I am not sure...</p>

<p>Could please anybody help me?</p>
",2013-09-30 08:12:44.097
56580,20190.0,1,58642.0,,,Support Vector Machine(SVM) and log transformation,<regression><svm><prediction>,CC BY-SA 3.0,"<p>Why may log(natural logarithm) transformation improve results of SVM prediction(<strong>regression</strong>, eps-svm)? Is SVM based on the assumption of normal distribution or something else?</p>

<p>update1. I use Radial basis function kernel.</p>
",2013-10-01 18:59:27.453
56684,,1,,,Ben,Significance test for highly skewed Bernoulli distribution,<hypothesis-testing><normal-distribution><bernoulli-distribution>,CC BY-SA 3.0,"<p>I am working with two highly skewed Bernoulli distributions where 96-99+% of the samples are in the ""false"" category, and the rest are in the ""true"" category (sort of speak). I am looking for a two-sided test of difference of proportions between the two samples. I can often achieve 500+ ""trues"" and tens or hundreds of thousands of ""falses"" in a reasonable time but I'm not sure if approximation to the normal distribution can withstand this extreme skewness.</p>

<p>I initially thought I might need something non-parametric, but here, I actually know the distribution.</p>

<p>I have been using a student's t-test, while paying attention to sample size estimation, but past experience has led me to be skeptical of its results. Thanks for your help.</p>
",2013-10-02 21:14:55.710
56768,11490.0,1,57271.0,,,Estimating hidden transfers of market share,<time-series><logistic>,CC BY-SA 4.0,"<p>Suppose we have yearly data representing the market share of three companies,
say A, B and C. In other words, we have observations:</p>

<p><span class=""math-container"">$$
 A_t, \; B_t \;\; \text{and} \;\; C_t \;\; \text{where} \; \; A_t+B_t+C_t = 1  
$$</span>
for <span class=""math-container"">$t = 1, \dots,T$</span>.</p>

<p>Suppose that in year <span class=""math-container"">$t$</span> the market share of company A has changed by <span class=""math-container"">$\Delta A_t = A_t - A_{t-1}$</span>. Is there any way of estimating how that change can be sub-divided into market share lost to or acquired from companies B and C? My actual problem includes 5 companies, but I guess that the solution shouldn't change too much.  </p>
",2013-10-03 21:13:50.837
56780,15280.0,1,56783.0,,,Problem with proof of Conditional expectation as best predictor,<mathematical-statistics><conditional-probability><conditional-expectation>,CC BY-SA 4.0,"<p>I have an issue with the proof of</p>
<blockquote>
<blockquote>
<p><span class=""math-container"">$E(Y|X) \in \arg \min_{g(X)} E\Big[\big(Y - g(X)\big)^2\Big]$</span></p>
</blockquote>
</blockquote>
<p>which very likely reveal a deeper misunderstanding of expectations and conditional expectations.</p>
<p>The proof I know goes as follows ( another version of this proof can be found <a href=""http://www.econ.uiuc.edu/%7Ewsosa/econ507/CEF.pdf"" rel=""nofollow noreferrer"">here</a>)</p>
<p><span class=""math-container"">\begin{align*}
&amp;\arg \min_{g(X)} E\Big[\big(Y - g(x)\big)^2\Big]\\
 = &amp;\arg \min_{g(X)} E \Big[ \big(Y - E(Y|X) + E(Y|X)  - g(X)\big)^2\Big]\\
=&amp;\arg \min_{g(x)} E \Big[ \big(Y - E(Y|X)\big)^2 + 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
=&amp;\arg \min_{g(x)} E \Big[ 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]\\
\end{align*}</span></p>
<p>The proof then typically continues with an argument showing that <span class=""math-container"">$2 E\Big[ \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big)\Big] = 0$</span>, and hence</p>
<p><span class=""math-container"">\begin{align*}
\arg \min_{g(x)} E\Big[\big(Y - g(x)\big)^2\Big] = \arg \min_{g(x)} E \Big[\big(E(Y|X)  - g(X)\big)^2\Big]
\end{align*}</span></p>
<p>which can be seen to be minimized when <span class=""math-container"">$g(X) = E(Y|X)$</span>.</p>
<p>My puzzles about the proof are the following:</p>
<ol>
<li>Consider</li>
</ol>
<blockquote>
<blockquote>
<p><span class=""math-container"">$E \Big[ 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big]$</span>.</p>
</blockquote>
</blockquote>
<p>It seems to me that, independently of any argument showing that the first term is always equal to zero, one can see that setting <span class=""math-container"">$g(X) = E(Y|X)$</span> minimizes the expression as it implies <span class=""math-container"">$\big(E(Y|X)  - g(X)\big) =0$</span> and hence</p>
<blockquote>
<blockquote>
<p><span class=""math-container"">$E \Big[ 2 \big(Y - E(Y|X)\big) \big(E(Y|X)  - g(X)\big) +   \big(E(Y|X)  - g(X)\big)^2\Big] = E( 0 + 0)$</span> = 0.</p>
</blockquote>
</blockquote>
<p>But if this is true, then one might repeat the proof replacing <span class=""math-container"">$E(Y|X)$</span> by any other function of <span class=""math-container"">$X$</span>, say <span class=""math-container"">$h(X)$</span>, and get to the conclusion that it is <span class=""math-container"">$h(X)$</span> that minimizes the expression. So there must be something I misunderstand (right?).</p>
<ol start=""2"">
<li>I have some doubts about the meaning of <span class=""math-container"">$E[(Yâˆ’g(X))^2]$</span> in the statement of the problem. How should the notation be interpreted? Does it mean</li>
</ol>
<blockquote>
<blockquote>
<p><span class=""math-container"">$E_X[(Yâˆ’g(X))^2]$</span>, <span class=""math-container"">$E_Y[(Yâˆ’g(X))^2]$</span> or <span class=""math-container"">$E_{XY}[(Yâˆ’g(X))^2]$</span>?</p>
</blockquote>
</blockquote>
",2013-10-04 00:24:13.043
56784,594.0,1,57748.0,,,Impact of data-based bin boundaries on a chi-square goodness of fit test?,<chi-squared-test><goodness-of-fit><binning>,CC BY-SA 3.0,"<p>Leaving aside the obvious issue of the low power of the chi-square in this sort of circumstance, imagine doing a chi-square goodness of test for some density with unspecified parameters, by binning the data.</p>

<p>For concreteness, let's say an exponential distribution with unknown mean and a sample size of say 100. </p>

<p>In order to get a reasonable number of expected observations per bin some account would need to be taken of the data (e.g. if we chose to put 6 bins below the mean and 4 above it, that would still be using data-based bin boundaries). </p>

<p>But this use of bins based on seeing the data would presumably affect the distribution of the test statistic under the null. </p>

<p>I have seen plenty of discussion about the fact that - <em>if</em> the parameters are estimated by maximum likelihood from the <em>binned</em> data - you lose 1 d.f per estimated parameter (an issue dating right back to Fisher vs Karl Pearson) - but I don't recall reading anything about finding the bin boundaries themselves based on the data. (If you estimate them from the unbinned data, then with $k$ bins the distribution of the test statistic lies somewhere between a $\chi^2_{k}$ and a $\chi^2_{k-p}$.) </p>

<p>Does this data-based choice of bins substantively impact significance level or power? Are there some approaches that matter more than others? If there is much of an effect, is it something that goes away in large samples?</p>

<p>If it does have a substantive impact, this would seem to make the use of a chi-squared test when parameters are unknown almost useless in many cases (in spite of still being advocated in quite a few texts), unless you had a good a-priori estimate of the parameter.</p>

<p>Discussion of the issues or pointers to references (preferably with a mention of their conclusions) would be useful.</p>

<hr>

<p>Edit, pretty much an aside to the main question:</p>

<p>It occurs to me that there are potential solutions for the specific case of the exponential* (and the uniform come to think of it), but I am still interested in the more general issue of the impact choosing bin boundaries.</p>

<p>* For example, for the exponential, one might use the smallest observation (say it is equal to $m$) to get a very rough idea of where to place the bins (since the smallest observation is exponential with mean $\mu/n$), and then test the remaining $n-1$ differences ($x_i - m$) for exponentiality. Of course that might yield a very poor estimate of $\mu$, and hence poor bin choices, though I suppose one might use the argument recursively in order to take the lowest two or three observations from which to choose reasonable bins and then test the differences of the remaining observations above the largest of those smallest order statistics for exponentiality)</p>
",2013-10-04 01:48:35.417
56859,6805.0,1,56860.0,,,What is the difference between descriptive and inferential statistics?,<terminology><descriptive-statistics><inference>,CC BY-SA 3.0,"<p>My understanding was that descriptive statistics quantitatively described features of a data sample, while inferential statistics made inferences about the populations from which samples were drawn.</p>

<p>However, the <a href=""http://en.wikipedia.org/wiki/Statistical_inference"">wikipedia page for statistical inference</a> states:</p>

<blockquote>
  <p>For the most part, statistical inference makes propositions about
  populations, using data drawn from the population of interest via some
  form of random sampling.</p>
</blockquote>

<p>The ""for the most part"" has made me think I perhaps don't properly understand these concepts. Are there examples of inferential statistics that don't make propositions about populations?</p>
",2013-10-05 04:59:21.093
56875,947.0,1,,,,Can you develop an econometrics model for stress test purpose only focusing on 2008-2009 data?,<time-series><econometrics>,CC BY-SA 3.0,"<p>I have become aware that a group at a large corporation is developing an econometrics model to forecast sales of their product.  They are using this model solely to estimate sales in specified stress test economic scenarios where they are given what the economic environment will be like, including real GDP contraction, rising unemployment rate, etc... out to 2016.  Because of the nature of those scenarios, they think the most proper way to construct this model is to focus solely on the 2008-2009 period capturing the main period of the recent financial crisis.  They have monthly data, so that gives them 24 monthly data points.  Given that GDP's frequency is really quarterly, on this one variable it gives them only 8 true datapoints.  But, they extrapolate it into 24 month observations. </p>

<p>For the record, if they chose to, they have good internal data going back to 2001 and up to the current period.  But, as mentioned they decided to focus instead solely on the 2008-2009 period.  </p>

<p>I will also answer this question as I have built many such econometrics models.  And, I invite others to debate and rebutt my answer... and to post your own better answer. </p>
",2013-10-05 14:58:52.307
56911,1506.0,1,57357.0,,,Detecting patterns in residual plot,<regression><residuals>,CC BY-SA 3.0,"<p>I wish to automatically (not by visual inspection) detect where large deviations occur in a residual plot from a regression. For example, suppose I have the residual plot below:</p>

<p><img src=""https://i.stack.imgur.com/IWgZV.png"" alt=""enter image description here""></p>

<p>I want to automatically detect the observations from about 30:35 deviate from a normal residual pattern.  Some clues are that the magnitude is quite large and the residuals do not appear independent in this region.  How can I go about this?</p>
",2013-10-06 06:53:48.957
56928,16046.0,1,,,,Reference for hierarchical Bayesian modelling,<references><hierarchical-bayesian>,CC BY-SA 3.0,"<p>I am currently reading ""Bayesian Data Analysis"" by Gelman et al. and my main goal was to learn about Hierarchical modelling on chapter 5. I read until chapter 4 and the book is written terribly for a taste of a math student as it is pretty sketchy and engineering oriented. </p>

<p>I decided to not to continue anymore with this book and I would be very grateful if somebody could introduce a reference with a more rigorous approach to the topic.</p>
",2013-10-06 15:21:17.703
56955,22423.0,1,57480.0,,,How to combine data from 5 surveys from the same population spanning 10 years,<sampling><survey><meta-analysis><population><weighted-sampling>,CC BY-SA 3.0,"<p>I have results from 5 surveys each 2 years apart and let us assume that no subjects are selected in more than one survey.</p>

<p>The sampling method used in these surveys are biased and I have sampling weights calculated(with respect to the population) for each data point in each study.</p>

<p>The question is, how would I be able to combine the 5 datasets and have the weights recalculated so as to obtain one giant dataset for analysis on this population?</p>

<p>Also, what should I do if subjects appear in more than one survey? </p>

<h1>Updates/Further Elaboration:</h1>

<p>thank you @user30523, here are some more infomation that might be useful:</p>

<p>Suppose I wish to find out the estimated distribution of height across the population using these 5 datasets. </p>

<p>In some data, younger people are oversampled because of the location where the survey are conducted. Let's assume the weights are calculated with respect to their age. </p>

<p>Eg. assuming 2% of the population are 15 years old, and the location of the survey is at a mall where 15-year-olds made up 5% of all shoppers, then sampling weight for an subject aged 15 in that survey would be calculated as 0.02 / 0.05 = 0.4. For simplicity, each person in the mall has equal chance of being surveyed and all participants complied when asked.</p>

<p>Given that 5 surveys are conducted in 5 different malls and each has their set of weights calculated in the same way, how would I then be able to combine all 5 datasets and recalculate the sampling weights?</p>

<p>P.S: I'm new to the topic on sampling weights so do correct me if I have made errors in the way I have calculated the weights.</p>
",2013-10-07 02:44:46.310
56970,22372.0,1,57858.0,,,How to test a logit model in R?,<r><regression><hypothesis-testing><logit>,CC BY-SA 3.0,"<p>I'm building a logit model using R and I'm getting a result of 88.9% of accuracy (verified using the ROC [in rattle, evaluation tab] using 30% of my 34k dataset).</p>

<p>What kind of tests would be interesting to do to certify myself that it's a good model?</p>
",2013-10-07 09:44:35.967
57012,20773.0,1,,,,How to compare different sensitivity thresholds and detection limits?,<data-imputation><censoring><threshold>,CC BY-SA 3.0,"<p>I have observations taken with different sensitivity thresholds and minimum detection levels, i.e. Lab A is less sensitive and has a minimum detection level of .2 and Lab B is more sensitive and has a minimum detection level of .02. </p>

<p><em>Edit 2: I have taken $N$ samples and have had them processed by two different labs (for stupid political reasons). Both labs send me the results and I discover that Lab A has a minimum detection level of .2 and Lab B has a minimum detection level of .02. See example:</em></p>

<p>Each row corresponds to a unique measurement taken by either lab:</p>

<pre><code>Obs | Lab A | Lab B
---------------------
 1  |  .6   |  NA
 2  |  0    |  NA
 3  |  NA   |  .53
 4  |  .2   |  NA
 5  |  NA   |  .07
</code></pre>

<p><em>Edit 2: I would like to be able to use and combine results from both labs, as if they were on the same scale. The problem is that the labs used to process the samples have very different thresholds for detection and have different sensitivity levels.</em></p>

<p>I think I would like something like:</p>

<pre><code>Obs | LabA  | LabB  | NewLab
----------------------------
 1  |  .6   |  NA   |  .64
 2  |  0    |  NA   |  .13
 3  |  NA   |  .53  |  .53
 4  |  .2   |  NA   |  .21
 5  |  NA   |  .07  |  .07
</code></pre>

<p>What techniques are available to standardize the values such that there is not a large loss of information?</p>

<ol>
<li>Obviously, I could take the values from Lab B and replace anything less than .2 with 0 and then round them, but I want to avoid throwing away information if possible.</li>
<li>One person suggested to add random noise to the values of Lab A, but I'm not sure of the benefit of this vs. simply imputing the missing values from Lab B.</li>
</ol>

<p><em>Edit 1:</em>
There are no observations for which both Lab A and Lab B values are present, one will always be missing.</p>

<p><em>Edit 2:</em>
What can I do to get results from both labs on a similar scale?</p>
",2013-10-07 20:36:21.153
57015,22454.0,1,,,,Testing statistical significance in two conditions,<hypothesis-testing><statistical-significance><p-value>,CC BY-SA 3.0,"<p>I am measuring two unpaired variables $x$ and $y$ in two different conditions ($x$ and $y$ are magnitudes of some special magnetic signals). In the first condition, my hypothesis is that $\bar{x} &gt; \bar{y}$ and in the second condition that $\bar{x} &lt; \bar{y}$. Now that I have $N$ samples from both variables, how can I test whether my hypotheses are true? I am not sure if I can safely assume that $x$ and $y$ are independent from each other. Neither do I know from what kind of distributions they are sampled from. The sample size I have is small. I have read several introductions to statistics for the past few days, but never saw a worked out example for this kind of situations. All help appreciated.</p>

<p>Edit: Like Michael Mayer wrote, there is a binary grouping variable ""condition"". Sorry for a bit unclear question.</p>
",2013-10-07 20:50:53.017
57026,22458.0,1,57333.0,,,Relationship between the kernel and the value of C in SVM's,<machine-learning><svm><kernel-trick><hyperparameter>,CC BY-SA 3.0,"<p>How exactly does the value of $C$ relate across different kernels that we can use for SVMs? As in, how does it vary when changing the polynomial degree of a kernel or while using a Gaussian kernel?</p>
",2013-10-08 01:16:28.460
57053,22475.0,1,57055.0,,,Interpretation of descriptive statistics for dummy variable,<categorical-data><interpretation><descriptive-statistics>,CC BY-SA 3.0,"<p>How can I describe descriptive statistics for a dummy variable (gender of worker in a shop)? Let's say this is the info that I have: </p>

<pre><code>mean :         0.47
median :       0
max :          1
min :          0
std. dev :     0.4998
skewness :     0.101
kurtosis :     1.01
jarque bera : 85.67
probability :  0
</code></pre>

<p>I know that some of the information is useless since it's a dummy variable. So how do I interpret it in words?</p>
",2013-10-08 12:31:47.380
57065,22477.0,1,,,,Bootstrapping - do I need to remove outliers first?,<bootstrap><outliers>,CC BY-SA 3.0,"<p>We've run a split test of a new product feature and want to measure if the uplift on revenue is significant. Our observations are definitely not normally distributed (most of our users don't spend, and within those that do, it is heavily skewed towards lots of small spenders and a few very big spenders).</p>

<p>We've decided on using bootstrapping to compare the means, to get round the issue of the data not being normally distributed (side-question: is this a legitimate use of bootstrapping?)</p>

<p>My question is, do I need to trim outliers from the data set (e.g. the few very big spenders) before I run the bootstrapping, or does that not matter?</p>
",2013-10-08 14:28:42.660
57086,22034.0,1,57184.0,,,Ideas for outputting a prediction equation for Random Forests,<random-forest><prediction>,CC BY-SA 3.0,"<p>I've read through the following posts that answered the question I was going to ask:</p>

<p><a href=""https://stats.stackexchange.com/questions/59741/use-random-forest-model-to-make-predictions-from-sensor-data"">Use Random Forest model to make predictions from sensor data</a></p>

<p><a href=""https://stats.stackexchange.com/questions/64201/decision-tree-for-output-prediction"">Decision tree for output prediction</a></p>

<p>Here's what I've done so far: I compared Logistic Regression to Random Forests and RF outperformed Logistic. Now the medical researchers I work with want to turn my RF results into a medical diagnostic tool. For example:</p>

<p>If you are an Asian Male between 25 and 35, have Vitamin D below xx and Blood Pressure above xx, you have a 76% chance of developing disease xxx. </p>

<p>However, RF doesn't lend itself to simple mathematical equations (see above links). So here's my question: what ideas do you all have for using RF to develop a diagnostic tool (without having to export hundreds of trees). </p>

<p>Here's a few of my ideas:</p>

<ol>
<li>Use RF for variable selection, then use Logistic (using all possible interactions) to make the diagnostic equation.</li>
<li>Somehow aggregate the RF forest into one ""mega-tree,"" that somehow averages the node splits across trees. </li>
<li>Similar to #2 and #1, use RF to select variables (say m variables total), then build hundreds of classification trees, all of which uses every m variable, then pick the best single tree. </li>
</ol>

<p>Any other ideas? Also, doing #1 is easy, but any ideas on how to implement #2 and #3?</p>
",2013-10-08 18:41:35.193
57110,22494.0,1,,,,How to remove seasonality from daily electricity demand,<r><time-series><arima>,CC BY-SA 3.0,"<p>I want to remove seasonality from daily electricity demand (a time series). My understanding is there is weekly (high demand on Tue, Wed, and low demand on Sat, Sun) and annual seasonality (high demand on Winter and lower on Summer). I tried to build a model to forecast daily electricity demand in R, and plot my data as shown below:
<img src=""https://i.stack.imgur.com/pz5OS.png"" alt=""Daily electricity demand""></p>

<p>I tried to remove seasonality with the following:</p>

<pre><code>demand.xts.diff&lt;-diff(demand.xts,lag=1,difference=1)
demand.xts.diff&lt;-diff(demand.xts,lag=7,difference=1)
</code></pre>

<p>I also tried to use <code>lag=365</code> and <code>lag=366</code> (I am not sure what lag to use, due to the leap year issue), but none of them successfully removed seasonality. The ACF and PACF are shown below:</p>

<p><img src=""https://i.stack.imgur.com/OV7T4.png"" alt=""ACF"">
<img src=""https://i.stack.imgur.com/HcXKi.png"" alt=""PACF""></p>

<p>Any advice is appreciated.</p>
",2013-10-09 03:35:34.333
57126,22503.0,1,,,,Is bootstrapping the right method for extreme distributions?,<hypothesis-testing><statistical-significance><bootstrap>,CC BY-SA 3.0,"<p>Wondering if anyone has an opinion on whether bootstrapping the difference in means is the right method given I have a situation with extreme data points. I've decided to use this as I don't think a t test is appropriate</p>

<p>I have about 30k observations per group (3 groups)</p>

<p>My situation is about spend, and I have extreme outliers:
the outliers aren't quite like an ""income"" distribution. That is, most users (95%+) will spend zero, a subset of users will spend 5 - 10 dollars. some will spend about 20 or 50 dollars and then a select few will spend 500+, with a couple of users spending 5000 or 10000+</p>

<p>I am trying to test which group brought in the most revenue per user.</p>

<p>Can anyone offer any advice on which statistical test is best suited?</p>
",2013-10-09 09:52:16.150
57156,19822.0,1,57269.0,,,How to perform unsupervised Random Forest classification using Breiman's code?,<machine-learning><classification><random-forest>,CC BY-SA 3.0,"<p>I am working with Breiman's random forest code (<a href=""http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_manual.htm#c2"">http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_manual.htm#c2</a>) for classification of satellite data (supervised learning). I am using a training and test dataset having sample size of 2000 and variable size 10. The data is classified into two classes, A and B. In supervised learning mode, the algorithm is performing well with very low classification error (&lt;2%). Now I want to try the unsupervised classification with no class labels in the test data set and see how the algorithm is able to predict the classes. Is there a way to implement unsupervised classification using Breiman's code? Will the error from this method will be higher than supervised classification?
The data and run parameter setting in the algorithm are given below</p>

<p>DESCRIBE DATA
1       mdim=10,ntrain=2000,nclass=2,maxcat=1,
1   ntest=2000,labelts=1,labeltr=1,</p>

<p>SET RUN PARAMETERS
2   mtry0=3,ndsize=1,jbt=500,look=100,lookcls=1,
2   jclasswt=0,mdim2nd=0,mselect=0,</p>
",2013-10-09 17:22:59.850
57160,15764.0,1,,,,ARIMA with seasonality in Statsmodels,<time-series><python><arima><arma><statsmodels>,CC BY-SA 3.0,"<p>I'd like to have a <a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow""><em>seasonal</em> ARIMA model</a> implemented with Statsmodels ARIMA. Specifically, I'd like to log before the weekly seasonality and then be able to make forecasts.</p>

<p>Perhaps an example with ARIMA's <a href=""http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.arima_model.ARIMA.from_formula.html#statsmodels.tsa.arima_model.ARIMA.from_formula"" rel=""nofollow"">from_formula</a> method could accomplish this. I'd also love to be able to do this with patsy.</p>

<p>Here's my sample code for logging before the weekly seasonality, and then transforming back to compare to the original time series (I've also skipped checking the validity of the model through testing stationarity and the residuals):</p>

<pre><code>import pandas as pd
import numpy as np
from statsmodels.tsa.arima_model import ARIMA

# ts is a time series
logged_ts = np.log(ts)
# Differencing by the week forces us to drop the first 7 values.
diffed_logged_ts = (logged_ts - logged_ts.shift(7))[7:]

p = 0
d = 1
q = 1

arima = ARIMA(diffed_logged_ts.values, [p, d, q], exog=None, dates=diffed_logged_ts.index, freq='D', missing='none')
diffed_logged_results = arima.fit(trend='c', disp=False)
predicted_diffed_logged = diffed_logged_results.predict(exog=None, dynamic=False)
predicted_diffed_logged_ts = pd.Series(predicted_diffed_logged, index=diffed_logged_ts.index[d:])
predicted_diffed_logged_ts = np.exp(logged_ts.shift(7) + diffed_logged_ts.shift(d) + predicted_diffed_logged_ts)

concatenated = pd.concat([ts, predicted_diffed_logged_ts], axis=1, keys=['original', 'predicted'])
print concatenated[-7:]
</code></pre>

<p>What do you think of this approach? I hope there's a less error-prone way coming in a future version of Statsmodels. Could someone tag this question with ""statsmodels""? Thanks!</p>
",2013-10-09 19:09:36.590
57161,20739.0,1,57197.0,,,Performing binary logistic regression with equal number of cases and non-cases,<logistic><sampling>,CC BY-SA 3.0,"<p>The best way to ask my question is to present an example scenario:</p>

<p>Let's say that the outcome of interest is lung cancer (1 = lung cancer; 0 = no lung cancer) and a researcher has 200k records (where 20k patients have lung cancer (cases) and 180k patients do NOT have lung cancer (non-cases)). Since only 10% of patients (20/200k) in the sample data have lung cancer, a researcher uses a random sample of 20k from the patients that do NOT have lung cancer. By doing so, the researcher would have a sample of 20k patients with lung cancer and 20k patients without lung cancer in their sample (the sample is reduced from 200k to 40k records).</p>

<p>Are there any benefits to performing binary logistic regression with equal number of cases and non-cases when the actual distribution of the outcome is not equal? Or does this bias model estimates/predictive power?</p>

<p>Thanks in advance!           </p>
",2013-10-09 19:25:15.053
57164,19264.0,1,57177.0,,,Gamma vs. lognormal distributions,<density-function><gamma-distribution><lognormal-distribution>,CC BY-SA 3.0,"<p>I have an experimentally observed distribution that looks very similar to a gamma or lognormal distribution. I've read that the <a href=""http://en.wikipedia.org/wiki/Log-normal_distribution"">lognormal distribution</a> is the maximum entropy probability distribution for a random variate $X$ for which the mean and variance of $\ln(X)$ are fixed. Does the gamma distribution have any similar properties?</p>
",2013-10-09 19:51:40.443
57167,21952.0,1,,,,simultaneous equations,<multiple-regression><econometrics>,CC BY-SA 3.0,"<p>I have the following relationships</p>

<p>logY ~ logX1 + logX2 + logX3 + logX4 + logX5 </p>

<p>and</p>

<p>X1 ~ Z1 + Z2 + Z3 + Z4 + Z5</p>

<p>X2 ~ Z1 + Z2 + Z3 + Z4 + Z5</p>

<p>X3 ~ Z1 + Z2 + Z3 + Z4 + Z5</p>

<p>where Y and Z1, Z2, Z3, Z4, Z5 are endogenous (Say while the Z's play a role in determining Y, the values of Z's are fixed depending upont he values of Y - Kind of like advertising expense has an impact on sales revenue but at the same time managers determine the advertisement expense on the expected sales revenue). So all the variable are changing simultaneously. Can anyone help me on how I can estimate this relationship? I also have instruments for each of the Z's (lagged variables have been treated as instruments and I have the previous year data for the problem as well. Thank you for all your help and suggestions.</p>
",2013-10-09 20:44:49.813
57175,20320.0,1,57193.0,,,On how to formulate and apply maximum likelihood,<estimation><matlab><maximum-likelihood>,CC BY-SA 3.0,"<p>I have just delved into the basics of maximum likelihood estimation and expectation maximization. The latter is really difficult to follow and I am having a tough time in figuring how I can apply the EM method for parameter estimation.</p>
",2013-10-09 22:19:42.207
57183,22541.0,1,,,,Which test should I use to assess for the statistical signficance of changes in multiple binomial dependent variables from T1 to T2?,<repeated-measures><multivariate-analysis><binomial-distribution><mcnemar-test>,CC BY-SA 3.0,"<p>Students have completed a test containing 20 questions at both T1 and T2, with an intervention in the interval. Scores for each question are either 0 (incorrect) or 1 (correct). I am interested in knowing whether the improvement in students' scores was significantly greater for some questions than for others. I am thinking that this may involve an extension of the McNemar test, but open to all suggestions. Thanks!</p>
",2013-10-09 23:52:28.193
543,182.0,2,,541.0,,,,CC BY-SA 3.0,"<p>As an economist, the analysis of variance (ANOVA) is taught and usually understood in relation to linear regression (e.g. in Arthur Goldberger's <em>A Course in Econometrics</em>). Economists/Econometricians typically view ANOVA as uninteresting and prefer to move straight to regression models. From the perspective of linear (or even generalised linear) models, ANOVA assigns coefficients into batches, with each batch corresponding to a ""source of variation"" in ANOVA terminology.</p>

<p>Generally you can replicate the inferences you would obtain from ANOVA using regression but not always OLS regression. Multilevel models are needed for analysing hierarchical data structures such as ""split-plot designs,"" where between-group effects are compared to group-level errors, and within-group effects are compared to data-level errors. <a href=""http://dx.doi.org/10.1214%2F009053604000001048"">Gelman's paper</a> &#91;1&#93; goes into great detail about this problem and effectively argues that ANOVA is an important statistical tool that should still be taught for it's own sake.</p>

<p>In particular Gelman argues that ANOVA is a way of understanding and structuring multilevel models. Therefore ANOVA is not an alternative to regression but as a tool for summarizing complex high-dimensional inferences and for exploratory data analysis. </p>

<p>Gelman is a well-respected statistician and some credence should be given to his view. However, almost all of the empirical work that I do would be equally well served by linear regression and so I firmly fall into the camp of viewing it as a little bit pointless. Some disciplines with complex study designs (e.g. psychology) may find ANOVA useful.</p>

<p>&#91;1&#93; Gelman, A. (2005). Analysis of variance: why it is more important than ever (with discussion). <em>Annals of Statistics</em> 33, 1â€“53. <a href=""http://dx.doi.org/10.1214%2F009053604000001048"">doi:10.1214/009053604000001048</a></p>
",2010-07-23 15:35:55.653
1787,668.0,2,,1760.0,,,,CC BY-SA 2.5,"<p>In effect you are thinking of a model in which the <em>true</em> chance of rain, <em>p</em>, is a function of the <em>predicted</em> chance <em>q</em>: <em>p</em> = <em>p(q</em>).  Each time a prediction is made, you observe one realization of a Bernoulli variate having probability <em>p(q)</em> of success.  This is a classic logistic regression setup if you are willing to model the true chance as a linear combination of basis functions <em>f1</em>, <em>f2</em>, ..., <em>fk</em>; that is, the model says</p>

<blockquote>
  <p>Logit(<em>p</em>) = <em>b0</em> + <em>b1 f1(q)</em> + <em>b2 f2(q)</em> + ... + <em>bk fk(q)</em> + <em>e</em></p>
</blockquote>

<p>with iid errors <em>e</em>.  If you're agnostic about the form of the relationship (although if the weatherman is any good <em>p(q) - q</em> should be reasonably small), consider using a set of splines for the basis.  The output, as usual, consists of estimates of the coefficients and an estimate of the variance of <em>e</em>.  Given any future prediction <em>q</em>, just plug the value into the model with the estimated coefficients to obtain an answer to your question (and use the variance of <em>e</em> to construct a prediction interval around that answer if you like).</p>

<p>This framework is flexible enough to include other factors, such as the possibility of changes in the quality of predictions over time.  It also lets you test hypotheses, such as whether <em>p</em> = <em>q</em> (which is what the weatherman implicitly claims).</p>
",2010-08-19 13:21:56.153
2169,674.0,2,,2156.0,,,,CC BY-SA 2.5,"<p>It may be worth looking at M.W. Berry's books: </p>

<ol>
<li><em>Survey of Text Mining I: Clustering, Classification, and Retrieval</em> (2003)</li>
<li><em>Survey of Text Mining II: Clustering, Classification, and Retrieval</em> (2008)</li>
</ol>

<p>They consist of series of applied and review papers. The latest seems to be available as PDF at the following address: <a href=""http://bit.ly/deNeiy"" rel=""nofollow noreferrer"">http://bit.ly/deNeiy</a>.</p>

<p>Here are few links related to CA as applied to text mining: </p>

<ul>
<li><a href=""http://www.cscjournals.org/csc/manuscript/Journals/IJCSS/volume4/Issue2/IJCSS-271.pdf"" rel=""nofollow noreferrer"">Document Topic Generation in Text Mining by Using Cluster Analysis with EROCK</a></li>
<li><a href=""http://www.google.fr/url?sa=t&amp;source=web&amp;cd=8&amp;ved=0CFoQFjAH&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.21.8862%26rep%3Drep1%26type%3Dpdf&amp;ei=P3h_TJXfAdT34AbIuN3TCw&amp;usg=AFQjCNHaVKyo45HtjNKiEPvFnPuvD2MvmQ&amp;sig2=vPQ1BmIz8vb16q1Oysi8_g"" rel=""nofollow noreferrer"">An Approach to Text Mining using Information Extraction</a></li>
</ul>

<p>You can also look at <em>Latent Semantic Analysis</em>, but see my response there: <a href=""https://stats.stackexchange.com/questions/369/working-through-a-clustering-problem/2196#2196"">Working through a clustering problem</a>. </p>
",2010-09-02 10:25:32.673
3188,450.0,2,,143.0,,,,CC BY-SA 4.0,"<p>#Edit:
As @Hunaphu's points out (and @whuber below in his answer) the original answer I gave to the OP (below) is wrong. It is indeed quicker to first sort the initial batch and then keep updating the median up or down (depending on whether a new data points falls to the left or to the right of the current median).</p>
<hr />
<p>It's bad form to sort an array to compute a median. Medians (and other quantiles) are typically computed using the <a href=""http://www.ics.uci.edu/%7Eeppstein/161/960125.html"" rel=""nofollow noreferrer"">quickselect</a> algorithm, with <span class=""math-container"">$O(n)$</span> complexity.</p>
<p>You may also want to look at my answer to a recent related question <a href=""https://stats.stackexchange.com/questions/3372/is-it-possible-to-accumulate-a-set-of-statistics-that-describes-a-large-number-of/3376#3376"">here</a>.</p>
",2010-10-09 19:02:09.717
3649,674.0,2,,3646.0,,,,CC BY-SA 2.5,"<p>I found that Spearman correlation is mostly used in place of usual linear correlation when working with integer valued scores on a measurement scale, when it has a moderate number of possible scores or when we don't want to make rely on assumptions about the bivariate relationships. As compared to Pearson coefficient, the interpretation of Kendall's tau seems to me less direct than that of Spearman's rho, in the sense that it quantifies the difference between the % of concordant and discordant pairs among all possible pairwise events. In my understanding, Kendall's tau more closely resembles <a href=""http://en.wikipedia.org/wiki/Gamma_test_(statistics)"" rel=""noreferrer"">Goodman-Kruskal Gamma</a>.</p>

<p>I just browsed an article from Larry Winner in the J. Statistics Educ. (2006) which discusses the use of both measures, <a href=""http://www.amstat.org/publications/jse/v14n3/datasets.winner.html"" rel=""noreferrer"">NASCAR Winston Cup Race Results for 1975-2003</a>.</p>

<p>I also found <a href=""https://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data/3744#3744"">@onestop</a> answer about <a href=""https://stats.stackexchange.com/questions/3730/pearsons-or-spearmans-correlation-with-non-normal-data"">Pearson's or Spearman's correlation with non-normal data</a> interesting in this respect.</p>

<p>Of note, Kendall's tau (the <em>a</em> version) has connection to Somers' D (and Harrell's C) used for predictive modelling (see e.g., <a href=""http://www.imperial.ac.uk/nhli/r.newson/miscdocs/intsomd1.pdf"" rel=""noreferrer"">Interpretation of Somersâ€™ D under four simple models</a> by RB Newson and reference <a href=""http://www.jstatsoft.org/v15/i01/paper"" rel=""noreferrer"">6</a> therein, and articles by Newson published in the Stata Journal 2006). An overview of rank-sum tests is provided in <a href=""http://www.jstatsoft.org/v15/i01/paper"" rel=""noreferrer"">Efficient Calculation of Jackknife Confidence Intervals for Rank Statistics</a>, that was published in the JSS (2006).</p>
",2010-10-24 14:26:35.747
4714,60.0,2,,4705.0,,,,CC BY-SA 3.0,"<p>Reverend <a href=""http://en.wikipedia.org/wiki/Thomas_Bayes"">Thomas Bayes</a> for discovering Bayes' theorem</p>
",2010-12-04 03:46:20.583
5020,1411.0,2,,5015.0,,,,CC BY-SA 2.5,"<p>I think this one is tricky; as you hint, there's 'moral hazard' here: if you hadn't looked at the interaction at all, you'd be free and clear, but now that you have there is a suspicion of data-dredging if you drop it.</p>

<p>The key is <em>probably</em> a change in the meaning of your effects when you go from the main-effects-only to the interaction model. What you get for the 'main effects' depends very much on how your treatments and contrasts are coded. In R, the default is treatment contrasts with the first factor levels (the ones with the first names in alphabetical order unless you have gone out of your way to code them differently) as the baseline levels.</p>

<p>Say (for simplicity) that you have two levels, 'control' and 'trt', for each factor. Without the interaction, the meaning of the 'v1.trt' parameter (assuming treatment contrasts as is the default in R) is ""average difference between 'v1.control' and 'v1.trt' group""; the meaning of the 'v2.trt' parameter is ""average difference between 'v2.control' and 'v2.trt'"".</p>

<p>With the interaction, 'v1.trt' is the average difference between 'v1.control' and 'v1.trt' <strong>in the 'v2.control' group</strong>, and similarly 'v2.trt' is the average difference between v2 groups in the 'v1.control' group. Thus, if you have fairly small treatment effects in each of the control groups, but a large effect in the treatment groups, you could easily see what you're seeing.</p>

<p>The only way I can see this happening <em>without</em> a significant interaction term, however, is if all the effects are fairly weak (so that what you really mean by ""the effect disappeared"" is that you went from p=0.06 to p=0.04, across the magic significance line).</p>

<p>Another possibility is that you are 'using up too many degrees of freedom' -- that is, the parameter estimates don't actually change that much, but the residual error term is sufficiently inflated by having to estimate another 4 [ = (2-1)*(5-1)] parameters that your significant terms become non-significant. Again, I would only expect this with a small data set/relatively weak effects.</p>

<p>One possible solution is to move to sum contrasts, although this is also delicate -- you have to be convinced that 'average effect' is meaningful in your case. The very best thing is to plot your data and to look at the coefficients and understand what's happening in terms of the estimated parameters.</p>

<p>Hope that helps.</p>
",2010-12-14 02:25:20.237
8699,155.0,2,,8681.0,,,,CC BY-SA 3.0,"<p>You may wish to read answers to this existing question on <a href=""https://stats.stackexchange.com/questions/7/locating-freely-available-data-samples"">freely available datasets</a>.</p>

<p>In general, I imagine that you'd want a dataset with some interesting metric variables.
In psychology research methods classes that I've taught, we've often looked at datasets with intelligence or personality test scores.</p>

<p>If you want a personality example, I have some <a href=""https://github.com/jeromyanglim/Sweave_Personality_Reports"" rel=""nofollow noreferrer"">personality data and metadata on github</a> based on the <a href=""http://ipip.ori.org/"" rel=""nofollow noreferrer"">IPIP</a>, an public domain measure of the Big 5 factors of personality.</p>

<ul>
<li><a href=""https://github.com/jeromyanglim/Sweave_Personality_Reports"" rel=""nofollow noreferrer"">github repository home</a></li>
<li><a href=""https://github.com/jeromyanglim/Sweave_Personality_Reports/blob/master/data/ipip.tsv"" rel=""nofollow noreferrer"">data</a></li>
<li><a href=""https://github.com/jeromyanglim/Sweave_Personality_Reports/blob/master/meta/ipipmeta.tsv"" rel=""nofollow noreferrer"">metadata</a></li>
<li><a href=""http://blog.revolutionanalytics.com/2010/12/how-to-create-pdf-reports-with-r.html"" rel=""nofollow noreferrer"">David Smith's summary</a></li>
</ul>
",2011-04-14 10:55:13.367
9529,192.0,2,,9524.0,,,,CC BY-SA 3.0,"<p><a href=""http://www.imdb.com/title/tt0138704/"">Pi</a></p>
",2011-05-07 14:19:42.887
10069,2666.0,2,,10008.0,,,,CC BY-SA 3.0,"<p>In my experience, not only is it necessary to have all lower order effects in the model when they are connected to higher order effects, but it is also important to properly model (e.g., allowing to be nonlinear) main effects that are seemingly unrelated to the factors in the interactions of interest.  That's because interactions between $x_1$ and $x_2$ can be stand-ins for main effects of $x_3$ and $x_4$.  Interactions sometimes <em>seem</em> to be needed because they are collinear with omitted variables or omitted nonlinear (e.g., spline) terms.</p>
",2011-05-21 12:31:20.447
13060,1805.0,2,,13058.0,,,,CC BY-SA 3.0,"<p>Check out the <a href=""http://cran.r-project.org/web/packages/digitize/index.html"" rel=""noreferrer"">digitize</a> package for <a href=""http://cran.r-project.org/"" rel=""noreferrer"">R</a>.  Its designed to solve exactly this sort of problem.</p>
",2011-08-18 05:14:07.900
14790,2081.0,2,,14729.0,,,,CC BY-SA 3.0,"<p>You seem to ask a really provoking question: how to detect, given a singular correlation (or covariance, or sum-of-squares-and-cross-product) matrix, which column is linearly dependent on which. I tentatively suppose that <strong>sweep operation</strong> could help. Here is my probe in SPSS (not R) to illustrate.</p>

<p>Let's generate some data:</p>

<pre><code>        v1        v2        v3         v4          v5
    -1.64454    .35119   -.06384    -1.05188     .25192
    -1.78520   -.21598   1.20315      .40267    1.14790
     1.36357   -.96107   -.46651      .92889   -1.38072
     -.31455   -.74937   1.17505     1.27623   -1.04640
     -.31795    .85860    .10061      .00145     .39644
     -.97010    .19129   2.43890     -.83642    -.13250
     -.66439    .29267   1.20405      .90068   -1.78066
      .87025   -.89018   -.99386    -1.80001     .42768
    -1.96219   -.27535    .58754      .34556     .12587
    -1.03638   -.24645   -.11083      .07013    -.84446
</code></pre>

<p>Let's create some linear dependancy between V2, V4 and V5:</p>

<pre><code>compute V4 = .4*V2+1.2*V5.
execute.
</code></pre>

<p>So, we modified our column V4.</p>

<pre><code>matrix.
get X. /*take the data*/
compute M = sscp(X). /*SSCP matrix, X'X; it is singular*/
print rank(M). /*with rank 5-1=4, because there's 1 group of interdependent columns*/
loop i= 1 to 5. /*Start iterative sweep operation on M from column 1 to column 5*/
-compute M = sweep(M,i).
-print M. /*That's printout we want to trace*/
end loop.
end matrix.
</code></pre>

<p>The printouts of M in 5 iterations:</p>

<pre><code>M
     .06660028    -.12645565    -.54275426    -.19692972    -.12195621
     .12645565    3.20350385    -.08946808    2.84946215    1.30671718
     .54275426    -.08946808    7.38023317   -3.51467361   -2.89907198
     .19692972    2.84946215   -3.51467361   13.88671851   10.62244471
     .12195621    1.30671718   -2.89907198   10.62244471    8.41646486

M
     .07159201     .03947417    -.54628594    -.08444957    -.07037464
     .03947417     .31215820    -.02792819     .88948298     .40790248
     .54628594     .02792819    7.37773449   -3.43509328   -2.86257773
     .08444957    -.88948298   -3.43509328   11.35217042    9.46014202
     .07037464    -.40790248   -2.86257773    9.46014202    7.88345168

M
    .112041875    .041542117    .074045215   -.338801789   -.282334825
    .041542117    .312263922    .003785470    .876479537    .397066281
    .074045215    .003785470    .135542964   -.465602725   -.388002270
    .338801789   -.876479537    .465602725   9.752781632   8.127318027
    .282334825   -.397066281    .388002270   8.127318027   6.772765022

M
   .1238115070   .0110941027   .0902197842   .0347389906   .0000000000
   .0110941027   .3910328733  -.0380581058  -.0898696977  -.3333333333
   .0902197842  -.0380581058   .1577710733   .0477405054   .0000000000
   .0347389906  -.0898696977   .0477405054   .1025348498   .8333333333
   .0000000000   .3333333333   .0000000000  -.8333333333   .0000000000

M
   .1238115070   .0110941027   .0902197842   .0347389906   .0000000000
   .0110941027   .3910328733  -.0380581058  -.0898696977   .0000000000
   .0902197842  -.0380581058   .1577710733   .0477405054   .0000000000
   .0347389906  -.0898696977   .0477405054   .1025348498   .0000000000
   .0000000000   .0000000000   .0000000000   .0000000000   .0000000000
</code></pre>

<p>Notice that eventually column 5 got full of zeros. This means (as I understand it) that V5 is linearly tied with some of <em>preceeding</em> columns. Which columns? Look at iteration where column 5 is last not full of zeroes - iteration 4. We see there that V5 is tied with V2 and V4 with coefficients -.3333 and .8333: V5 = -.3333*V2+.8333*V4, which corresponds to what we've done with the data: V4 = .4*V2+1.2*V5.</p>

<p>That's how we knew which column is linearly tied with which other. I didn't check how helpful is the above approach in more general case with many groups of interdependancies in the data. In the above example it appeared helpful, though.</p>
",2011-10-03 09:34:57.197
16212,1927.0,2,,16209.0,,,,CC BY-SA 3.0,"<p>Here is a possibility, very similar than that of @Roman Lustrik, but just a little bit more automatic.</p>

<p>Say that</p>

<blockquote>
  <p>x &lt;- c(""a"", ""b"", ""b"", ""c"")</p>
</blockquote>

<p>Then</p>

<pre><code>   &gt; x &lt;- as.factor(x)
   &gt; levels(x) &lt;- 1:length(levels(x))
   &gt; x &lt;- as.numeric(x)
</code></pre>

<p>makes the job:</p>

<pre><code>   &gt; print(x)
   [1] 1 2 2 3
</code></pre>
",2011-11-06 11:12:09.377
16337,2081.0,2,,16313.0,,,,CC BY-SA 4.0,"<p><strong>Spearman rho vs Kendall tau</strong>. These two are so much computationally different that you <em>cannot</em> directly compare their magnitudes. Spearman is usually higher by 1/4 to 1/3 and this makes one incorrectly conclude that Spearman is &quot;better&quot; for a particular dataset. The difference between rho and tau is in their ideology, <em>proportion-of-variance</em> for rho and <em>probability</em> for tau. Rho is a usual Pearson r applied for ranked data, and like r, is more sensitive to points with large moments (that is, deviations from cloud centre) than to points with small moments. Therefore <strong>rho is quite sensitive to the shape of the cloud after the ranking</strong> done: the coefficient for an oblong rhombic cloud will be higher than the coefficient for an oblong dumbbelled cloud (because sharp edges of the first are large moments). Tau is an extension of Gamma and is <strong>equally sensitive to all the data points</strong>, so it is less sensitive to peculiarities in shape of the ranked cloud. Tau is more &quot;general&quot; than rho, for rho is warranted only when you believe the underlying (model, or functional in population) relationship between the variables is strictly monotonic. While Tau allows for nonmonotonic underlying curve and measures which monotonic &quot;trend&quot;, positive or negative, prevails there overall. Rho is comparable with r in magnitude; tau is not.</p>
<p><strong>Kendall tau as Gamma</strong>. Tau is just a standardized form of Gamma. Several related measures all have numerator <span class=""math-container"">$P-Q$</span> but differ in normalizing <em>denominator</em>:</p>
<ul>
<li>Gamma: <span class=""math-container"">$P+Q$</span></li>
<li>Somers' D(&quot;x dependent&quot;): <span class=""math-container"">$P+Q+T_x$</span></li>
<li>Somers' D(&quot;y dependent&quot;): <span class=""math-container"">$P+Q+T_y$</span></li>
<li>Somers' D(&quot;symmetric&quot;): arithmetic mean of the above two</li>
<li>Kendall's Tau-b corr. (most suitable for square tables): geometric mean of those two</li>
<li>Kendall's Tau-c corr<span class=""math-container"">$^1$</span>. (most suitable for rectangular tables): <span class=""math-container"">$N^2(k-1)/(2k)$</span></li>
<li>Kendall's Tau-a corr<span class=""math-container"">$^2$</span>. (makes nÐ¾ adjustment for ties): <span class=""math-container"">$N(N-1)/2 = P+Q+T_x+T_y+T_{xy}$</span></li>
</ul>
<p>where <span class=""math-container"">$P$</span> - number of pairs of observations with &quot;concordance&quot;, <span class=""math-container"">$Q$</span> - with &quot;inversion&quot;; <span class=""math-container"">$T_x$</span> - number of ties by variable X, <span class=""math-container"">$T_y$</span> - by variable Y, <span class=""math-container"">$T_{xy}$</span> â€“ by both variables; <span class=""math-container"">$N$</span> - number of observations, <span class=""math-container"">$k$</span> - number of distinct values in that variable where this number is less.</p>
<p>Thus, tau is directly comparable in theory and magnitude with Gamma. Rho is directly comparable in theory and magnitude with Pearson <span class=""math-container"">$r$</span>. Nick Stauner's nice answer here tells how it is possible to compare rho and tau indirectly.</p>
<p><a href=""https://stats.stackexchange.com/q/3943/3277"">See also</a> about tau and rho.</p>
<hr />
<p><span class=""math-container"">$^1$</span> Tau-c of a variable with itself can be below <span class=""math-container"">$1$</span>: specifically, when the distribution of <span class=""math-container"">$k$</span> distinct values is unbalanced.</p>
<p><span class=""math-container"">$^2$</span> Tau-a of a variable with itself can be below <span class=""math-container"">$1$</span>: specifically, when there are ties.</p>
",2011-11-09 11:41:29.170
48133,594.0,2,,48103.0,,,,CC BY-SA 3.0,"<p>If you just want a good estimate of $\omega$ and don't care much about
its standard error:</p>

<pre><code>ssp &lt;- spectrum(y)  
per &lt;- 1/ssp$freq[ssp$spec==max(ssp$spec)]
reslm &lt;- lm(y ~ sin(2*pi/per*t)+cos(2*pi/per*t))
summary(reslm)

rg &lt;- diff(range(y))
plot(y~t,ylim=c(min(y)-0.1*rg,max(y)+0.1*rg))
lines(fitted(reslm)~t,col=4,lty=2)   # dashed blue line is sin fit

# including 2nd harmonic really improves the fit
reslm2 &lt;- lm(y ~ sin(2*pi/per*t)+cos(2*pi/per*t)+sin(4*pi/per*t)+cos(4*pi/per*t))
summary(reslm2)
lines(fitted(reslm2)~t,col=3)    # solid green line is periodic with second harmonic
</code></pre>

<p><img src=""https://i.stack.imgur.com/ZF1P2.png"" alt=""sine plot""></p>

<p>(A better fit still would perhaps account for the outliers in that series in some way, reducing their influence.)</p>

<p>---</p>

<p>If you want some idea of the uncertainty in $\omega$, you could use profile likelihood (<a href=""http://www.math.umt.edu/patterson/ProfileLikelihoodCI.pdf"" rel=""noreferrer"">pdf1</a>, <a href=""http://www.utstat.toronto.edu/reid/research/slacpaper.pdf"" rel=""noreferrer"">pdf2</a> - references on getting approximate CIs or SEs from profile likelihood or its variants aren't hard to locate)</p>

<p>(Alternatively, you could feed these estimates into nls ... and start it already converged.)</p>
",2013-06-06 08:03:29.833
16537,1831.0,2,,15542.0,,,,CC BY-SA 3.0,"<p><a href=""http://deeplearning.net/"">Deep Learning</a> got a lot of focus since 2006. It's basically an approach to train deep neural networks and is leading to really impressive results on very hard datasets (like document clustering or object recognition). Some people are talking about the second neural network renaissance (eg in <a href=""http://www.youtube.com/watch?v=rkCNbi26Hds"">this Google talk</a> by Schmidhuber).</p>

<p>If you want to be impressed you should look at this Science paper <a href=""http://www.utstat.toronto.edu/~rsalakhu/papers/science.pdf"">Reducing the Dimensionality of Data with Neural Networks,</a> Hinton &amp; Salakhutdinov.</p>

<p>(There is so much work going on right now in that area, that there is only two upcoming books I know about that will treat it: <a href=""http://www.cambridge.org/aus/catalogue/catalogue.asp?isbn=9780521192248"">Large scale machine learning</a>, Langford et al and <a href=""http://www.cs.ubc.ca/~murphyk/MLbook/index.html"">Machine Learning: a probabilistic perspective</a> by Kevin Murphy.)</p>

<p>If you want to know more, check out what the main deep learning groups are doing: <a href=""http://www.cs.stanford.edu/people/ang/"">Stanford</a>, <a href=""http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html"">Montreal</a> and most importantly <a href=""http://www.cs.toronto.edu/~hinton/"">Toronto #1</a> and <a href=""http://www.utstat.toronto.edu/~rsalakhu/"">Toronto #2</a>.</p>
",2011-11-14 20:26:28.373
17000,65.0,2,,16998.0,,,,CC BY-SA 3.0,"<p>Do not the Wikileaks texts suit you?</p>
",2011-11-24 21:48:24.163
18345,5448.0,2,,18335.0,,,,CC BY-SA 3.0,"<p>The LR (likelihood ratio) test actually is testing the hypothesis that a specified subset of the parameters equal some pre-specified values.  In the case of model selection, generally (but not always) that means some of the parameters equal zero.  If the models are nested, the parameters in the larger model that are not in the smaller model are the ones being tested, with values specified implicitly by their exclusion from the smaller model.  If the models aren't nested, you aren't testing this any more, because BOTH models have parameters that aren't in the other model, so the LR test statistic doesn't have the asymptotic $\chi^2$ distribution that it (usually) does in the nested case.</p>

<p>AIC, on the other hand, is not used for formal testing.  It is used for informal comparisons of models with differing numbers of parameters.   The penalty term in the expression for AIC is what allows this comparison.  But no assumptions are made about the functional form of the asymptotic distribution of the differences between the AIC of two non-nested models when doing the model comparison, and the difference between two AICs is not treated as a test statistic.</p>

<p>I'll add that there is some disagreement over the use of AIC with non-nested models, as the theory is worked out for nested models.  Hence my emphasis on ""not...formal"" and ""not...test statistic.""  I use it for non-nested models, but not in a hard-and-fast way, more as an important, but not the sole, input into the model building process.</p>
",2012-01-01 17:41:56.327
20240,1073.0,2,,20234.0,carlosdc,,,CC BY-SA 3.0,"<p>I think you're mixing multiple important concepts. Let me try to clarify a couple of things:</p>

<ul>
<li><p>There are metaheuristic methods, which are methods that iteratively try to improve a candidate solution. Examples of this are tabu search, simulated annealing, genetic algorithms, etc. Observe that while there can be many cases where these methods work nicely, there isn't any deep understanding of when these methods work and when they don't. And more importantly when they don't get to the solution, we can be arbitrarily far from it. Problems solved by metaheuristic methods tend to be discrete in nature, because there are far better tools to handle continuous problems. But every now and then you see metaheuristics for continuous problems, too.</p></li>
<li><p>There are numerical optimization methods, people in this community carefully examine the nature of the function that is to be optimized and the restrictions of the solution (into groups like convex optimization,     quadratic programming, linear programming, etc) and apply algorithms  that have been shown to work for that type of function, and those type of restrictions. When people in this area say ""shown to work"" they mean a proof. The situation is that these types of methods work in continuous problems. But when your problem falls in this category, this is definitely the tool to use.</p></li>
<li><p>There are discrete optimization methods, which tend to be things that in nature are connected to algorithms to well studied discrete problems: such as shortest paths, max flow, etc. People in this area also care that their algorithms really work (proofs). There are a subset of people in this group that study really hard problems for which no fast algorithm is expected to exist. They then study approximation algorithms, which are fast algorithms for which they are able to show that their solution is within a constant factor of the true optimum. This is called ""approximation algorithms"". These people also show their results as proofs.</p></li>
</ul>

<p>So... to answer your question, I do not think that metaheuristics are approximation algorithms. It doesn't seem to me as something connected to opinion, it is just fact.</p>
",2012-02-10 21:45:51.503
24602,3662.0,2,,24506.0,,,,CC BY-SA 3.0,"<p>If your using linear regression I would recommend using <a href=""http://biostat.mc.vanderbilt.edu/wiki/Main/Rrms"" rel=""nofollow noreferrer"">the rms package</a> in R. It is very easy to use and has lots of nice features. </p>

<p>Here's an example:</p>

<pre><code># Load package (remember to install.packages(""rms"") or this will fail the first time)
library(rms)

# Get a dataset to experiment with
data(mtcars)
mtcars$am &lt;- factor(mtcars$am, levels=0:1, labels=c(""Automatic"", ""Manual""))

# The rms package needs this work properly
dd &lt;- datadist(mtcars)
options(datadist=""dd"")

# Do the regression
f &lt;- ols(mpg~wt, data=mtcars, x=T, y=T)

# Plot regular mean confidence interval
p &lt;- Predict(f, wt=seq(2.5, 4, by=.001), conf.type=""mean"")
plot(p, ylim=c(10, 30), col=""lightblue"")

# Plot wide confidence interval
p &lt;- Predict(f, wt=seq(2.5, 4, by=.001), conf.type=""individual"")
plot(p, ylim=c(10, 30), col=""lightblue"")
</code></pre>

<p>Gives this output:</p>

<p><img src=""https://i.stack.imgur.com/UAvh7.png"" alt=""Plain line""></p>

<p>Now usually you want to test the linearity assumption:</p>

<pre><code># Try the model with a restricted cubic spline
f &lt;- ols(mpg~rcs(wt, 3), data=mtcars, x=T, y=T)
anova(f)
</code></pre>

<p>Gives this output:</p>

<pre><code>&gt; anova(f)
                Analysis of Variance          Response: mpg 

 Factor     d.f. Partial SS MS         F     P     
 wt          2   922.04230  461.021149 65.54 &lt;.0001
  Nonlinear  1    74.31705   74.317047 10.56 0.0029
 REGRESSION  2   922.04230  461.021149 65.54 &lt;.0001
 ERROR      29   204.00489    7.034651             
</code></pre>

<p>And if you plot the graphs with the same code as a bove you get this picture:</p>

<p><img src=""https://i.stack.imgur.com/jkftF.png"" alt=""Regression with a spline""></p>

<p>If you want to make your formula more complicated just add that variable:</p>

<pre><code>f &lt;- ols(mpg~rcs(wt, 3)+am, data=mtcars, x=T, y=T)
p &lt;- Predict(f, wt=seq(2.5, 4, by=.001), am=levels(mtcars$am), conf.type=""individual"")
plot(p)
</code></pre>

<p>I don't know anything about JMP, it shouldn't be too difficult but I recommend learning R because it gives you an incredible freedom. </p>

<p>Hope this helped.</p>
",2012-05-06 10:01:28.687
25087,8363.0,2,,25072.0,,,,CC BY-SA 3.0,"<p>I will just address question 2.  I have some doubts about how well the author knows his subject if he really said it the way you have presented it.  PCA is applied to the sample just like EFA and CFA.  It simply takes a list of n possibly related factors looks at how the points (samples) scatter in n-dimension space and then gets the first principal component as the linear combination that explains more of the variability in the data than any other linear combination.  Then the second looks at orthogonal directions to the first to find theone out of those that explains the most of the remaining variability and so on with the 3rd and 4th.  So sometimes one can take just 1-3 components to describe most of the variation in the data.  That is why factor analysis and principal componet analysis are described according to 1 and 2 in your statement.</p>
",2012-05-14 14:04:26.687
26657,190.0,2,,26070.0,,,,CC BY-SA 3.0,"<p>Here is a simple toy example illustrating the effect of dimension in a discrimination problem e.g. the problem you face when you want to say if something is observed or if only random effect is observed (this problem is a classic in science). </p>

<p><strong>Heuristic.</strong>  The key issue here is that the Euclidian norm gives the same importance to any direction. This constitutes a lack of prior, and as you certainly know in high dimension there is no free lunch (i.e. if you have no prior idea of what you are searching for, then there is no reason why some noise would not look like what you are searching for, this is tautology ...). </p>

<p>I would say that for any problem there is a limit of information that is necessary to find something else than noise. This limit is related somehow to the ""size"" of the area you are trying to explore with regard to the ""noise"" level (i.e. level of uninformative content). </p>

<p>In high dimension if you have the prior that your signal is sparse then you can remove (i.e. penalize) non sparse vector with a metric that fills the space with sparse vector or by using a thresholding technique. </p>

<p><strong>Framework</strong> Assume that $\xi$ is a gaussian vector with mean $\nu$ and diagonal covariance $\sigma Id$ ($\sigma$ is known) and that you want to test the simple hypothesis </p>

<p>$$H_0: \;\nu=0,\; Vs \; H_{\theta}: \; \nu=\theta $$
(for a given $\theta\in \mathbb{R}^n$) $\theta$ is not necessarily known in advance. </p>

<p><strong>Test statistic with energy</strong>. The intuition you certainly have is that it is a good idea to evaluate the norm/energy $\mathcal{E}_n=\frac{1}{n}\sum_{i=1}^n\xi_i^2$ of you observation $\xi$ to build a test statistic.  Actually you can construct a standardized centered (under $H_0$) version $T_n$ of the energy $T_n=\frac{\sum_i\xi_i^2-\sigma^2}{\sqrt{2n\sigma^4}}$. That makes a critical region at level $\alpha$ of the form $\{T_n\geq v_{1-\alpha}\}$ for a well chosen $v_{1-\alpha}$</p>

<p><strong>Power of the test and dimension.</strong> In this case it is an easy probability exercise to show the following formula for the power of your test: </p>

<blockquote>
  <p>$$P_{\theta}(T\leq v_{1-\alpha})=P\left (Z\leq \frac{v_{1-\alpha}}{\sqrt{1+2\|\theta\|_2^2/(n\sigma^2)}}-\frac{\|\theta\|^2_2}{\sqrt{2n\sigma^4+2\sigma^2\|\theta\|_2^2/(n\sigma^2)}}\right )$$
  with $Z$ a sum of $n$ iid random variables with $\mathbb{E}[Z]=0$ and $Var(Z)=1$. </p>
</blockquote>

<p>This means that the power of your test is increased by the energy of your signal $\|\theta\|^2_2$ and decreased by $n$. Practically speaking this means that when you increase the size $n$ of your problem if it does not increase the strength of the signal at the same time then you are adding uninformative information to your observation (or you are reducing the proportion of useful information in the information you have): this is like adding noise and reduces the power of the test (i.e. it is more likely that you are gonna say nothing is observed while there is actually something). </p>

<p><strong>Toward a test with a threshold statistic.</strong> If you do not have much energy in your signal but if you know a linear transformation that can help you to have this energy concentrated in a small part of your signal, then you can build a test statistic that will only evaluate the energy for the small part of your signal. If you known in advance where it is concentrated (for example you known there cannot be high frequencies in your signal) then you can obtain a power in the preceding test with $n$ replaced by a small number and $\|\theta\|^2_2$ almost the same... If you do not know it in advance you have to estimate it this leads to well known thresholding tests. </p>

<p>Note that this argument is exactly at the root many papers such as </p>

<ul>
<li>A Antoniadis, F Abramovich, T Sapatinas, and B Vidakovic. Wavelet methods for testing
in functional analysis of variance models. International Journal on Wavelets and its
applications, 93 :1007â€“1021, 2004.</li>
<li>M. V. Burnashef and Begmatov. On a problem of signal detection leading to stable distribution. Theory of probability and its applications, 35(3) :556â€“560, 1990.</li>
<li>Y. Baraud. Non asymptotic minimax rate of testing in signal detection. Bernoulli, 8 :577â€“606, 2002.</li>
<li>J Fan. Test of significance based on wavelet thresholding and neymanâ€™s truncation. JASA,
91 :674â€“688, 1996.</li>
<li>J. Fan and S-K Lin. Test of significance when data are curves. JASA, 93 :1007â€“1021, 1998.</li>
<li>V. Spokoiny. Adaptative hypothesis testing using wavelets. Annals of Statistics, 24(6) :2477â€“2498, december 1996.</li>
</ul>
",2012-06-12 09:23:27.760
69414,7189.0,2,,58876.0,,,,CC BY-SA 3.0,"<p>I think what people mean by support recovery is very simple. If the observed data is generated using a model of the form $$y=Xw^*+\epsilon$$
$y\in\mathbb{R}^n, X\in \mathbb{R}^{n\times p},\epsilon\in\mathbb{R}^n$ is noise (assumed to be subGaussian), where $w^*\in\mathbb{R}^p$ is known to be sparse, under what conditions will the solution of the lasso problem</p>

<p>$$\hat{w}=\arg\min\limits_w \frac{1}{2n}||y-Xw||_2^2+\lambda_n||w||_1$$</p>

<p>have the same indices that are non-zero as the true solution $w^*$? i.e. support($\hat{w}$)=support($w^*$)</p>
",2014-03-17 19:14:24.097
27132,668.0,2,,27120.0,,,,CC BY-SA 3.0,"<p><strong>The conceptual uses of ""square"" and ""squared"" are subtly different,</strong> although (almost) interchangeable:</p>

<ul>
<li><p>""Squared"" refers to the past <em>action</em> of taking or computing the second power.  E.g., $x^2$ is usually read as ""x-squared,"" not ""x-square."" (The latter is sometimes encountered but I suspect it results from speakers who are accustomed to clipping their phrases or who just haven't heard the terminal dental in ""x-squared."")</p></li>
<li><p>""Square"" refers to the <em>result</em> of taking the second power.  E.g., $x^2$ can be referred to as the ""square of x.""  (The illocution ""squared of x"" is never used.)</p></li>
</ul>

<p>These suggest that a person using a phrase like ""mean squared error"" is thinking in terms of a <em>computation</em>: take the errors, square them, average those.  The phrase ""mean square error"" has a more conceptual feel to it: average the square errors.  The user of this phrase may be thinking in terms of square errors rather than the errors themselves.  I believe this shows up especially in theoretical literature where the second form, ""square,"" appears more often (I believe: I haven't systematically checked).</p>

<p>Obviously both are equivalent in function and safely interchangeable in practice.  It is interesting, though, that some careful Google queries give substantially different hit counts.  Presently, </p>

<pre><code>""mean squared"" -square -root -Einstein -Relativity
</code></pre>

<p>returns about 367,000 results (notice the necessity of ruling out the phrase ""$e=m c^2$"" popularly quoted in certain contexts, which demands the use of ""squared"" instead of ""square"" when written out), while</p>

<pre><code>""mean square"" -squared -root  -Einstein -Relativity
</code></pre>

<p>(maintaining analogous exclusions for comparability) returns an order of magnitude more, at 3.47 million results.  This (weakly) suggests people favor ""mean square"" over ""mean squared,"" but don't take this too much to heart: ""mean squared"" is used in official SAS documentation, for instance.</p>
",2012-06-20 20:29:04.380
30434,9605.0,2,,16366.0,,,,CC BY-SA 3.0,"<p>This is indeed something often glossed over.</p>

<p>Some people are doing something a bit cheeky: holding out a proportion of the words in each document, and giving using predictive probabilities of these held-out words given the document-topic mixtures as well as the topic-word mixtures. This is obviously not ideal as it doesn't evaluate performance on any held-out documents.</p>

<p>To do it properly with held-out documents, as suggested, you do need to ""integrate over the Dirichlet prior for all possible topic mixtures"". <a href=""http://people.cs.umass.edu/~wallach/talks/evaluation.pdf"">http://people.cs.umass.edu/~wallach/talks/evaluation.pdf</a> reviews a few methods for tackling this slightly unpleasant integral. I'm just about to try and implement this myself in fact, so good luck!</p>
",2012-08-20 14:56:05.707
30864,4890.0,2,,30862.0,,,,CC BY-SA 3.0,"<p>A low rank approximation $\hat{X}$ of $X$ can be decomposed into a matrix square root as $G=U_{r}\lambda_{r}^\frac{1}{2}$ where the eigen decomposition of $X$ is $U\lambda U^T$, thereby reducing the number of features, which can be represented by $G$ based on the rank-r approximation as $\hat{X}=GG^T$. Note that the subscript $r$  represents the number of eigen-vectors and eigen-values used in the approximation. Hence, it does reduce the number of features to represent the data. In some examples low-rank approximations are considered as basis or latent variable (dictionary) based expansions of the original data, under special constraints like orthogonality, non-negativity (non-negative matrix factorization) etc.</p>
",2012-08-28 00:53:18.257
30960,132.0,2,,30957.0,,,,CC BY-SA 3.0,"<p>You can ""fit"" the model to different data and then simulate:</p>

<pre><code>m2 &lt;- Arima(z,model=m1)
simulate.Arima(m2,future=TRUE,bootstrap=TRUE)
</code></pre>

<p><code>m2</code> will have the same parameters as <code>m1</code> (they are not re-estimated), but the residuals, etc., are computed on the new data.</p>

<p>However, I am concerned with your model. Seasonal models are for when the seasonality is fixed and known. With animal population data, you almost certainly have aperiodic population cycling. This is a well-known phenomenon and can easily be handled with non-seasonal ARIMA models. Look at the literature on the Canadian lynx data for discussion.</p>

<p>By all means, use the square root, but then I would use a non-seasonal ARIMA model. Provided the AR order is greater than 1, it is possible to have cycles. See </p>

<p>You can do all this in one step:</p>

<pre><code>m1 &lt;- auto.arima(y, lambda=0.5)
</code></pre>

<p>Then proceed with your simulations as above.</p>
",2012-08-29 17:25:54.740
31587,7007.0,2,,31575.0,,,,CC BY-SA 4.0,"<p>Please, check the comments above. Here is a quick implementation in R.</p>

<pre><code>x &lt;- c(1,2,1,1,3,4,4,1,2,4,1,4,3,4,4,4,3,1,3,2,3,3,3,4,2,2,3)
p &lt;- matrix(nrow = 4, ncol = 4, 0)
for (t in 1:(length(x) - 1)) p[x[t], x[t + 1]] &lt;- p[x[t], x[t + 1]] + 1
for (i in 1:4) p[i, ] &lt;- p[i, ] / sum(p[i, ])
</code></pre>

<p>Results:</p>

<pre><code>&gt; p
          [,1]      [,2]      [,3]      [,4]
[1,] 0.1666667 0.3333333 0.3333333 0.1666667
[2,] 0.2000000 0.2000000 0.4000000 0.2000000
[3,] 0.1428571 0.1428571 0.2857143 0.4285714
[4,] 0.2500000 0.1250000 0.2500000 0.3750000
</code></pre>

<p>A (probably dumb) implementation in MATLAB (which I have never used, so I don't know if this is going to work. I've just googled ""declare vector matrix MATLAB"" to get the syntax):</p>

<pre><code>x = [ 1, 2, 1, 1, 3, 4, 4, 1, 2, 4, 1, 4, 3, 4, 4, 4, 3, 1, 3, 2, 3, 3, 3, 4, 2, 2, 3 ]
n = length(x) - 1
p = zeros(4,4)
for t = 1:n
  p(x(t), x(t + 1)) = p(x(t), x(t + 1)) + 1
end
for i = 1:4
  p(i, :) = p(i, :) / sum(p(i, :))
end
</code></pre>
",2012-09-11 17:05:10.090
32053,346.0,2,,32038.0,,,,CC BY-SA 3.0,"<p>Short answer: Yes, you can use ID as random effect with 6 levels.</p>

<p>Slightly longer answer: The <a href=""http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html"" rel=""noreferrer"">@BenBolker's GLMM FAQ</a> says (among other things) the following under the headline ""<em>Should I treat factor xxx as fixed or random?</em>"":</p>

<blockquote>
  <p>One point of particular relevance to 'modern' mixed model estimation
  (rather than 'classical' method-of-moments estimation) is that, for
  practical purposes, there must be a reasonable number of
  random-effects levels (e.g. blocks) â€” more than 5 or 6 at a minimum.</p>
</blockquote>

<p>So you are at the lower bound, but on the right side of it.</p>
",2012-09-20 12:35:29.537
35160,12273.0,2,,35097.0,,,,CC BY-SA 3.0,"<p>The main issue is that the first experiment (Sun gone nova) is not repeatable, which makes it highly unsuitable for frequentist methodology that interprets probability as estimate of how frequent an event is giving that we can repeat the experiment many times. In contrast, bayesian probability is interpreted as our degree of belief giving all available prior knowledge, making it suitable for common sense reasoning about one-time events. The dice throw experiment is repeatable, but I find it very unlikely that any frequentist would intentionally ignore the influence of the first experiment and be so confident in significance of the obtained results.</p>

<p>Although it seems that author mocks frequentist reliance on repeatable experiments and their distrust of priors, giving the unsuitability of the experimental setup to the frequentist methodology I would say that real theme of this comic is not frequentist methodology but blind following of unsuitable methodology in general. Whether it's funny or not is up to you (for me it is) but I think it more misleads than clarifies the differences between the two approaches.</p>
",2012-11-12 16:27:57.197
42517,594.0,2,,42513.0,,,,CC BY-SA 3.0,"<p>Of course, why not? </p>

<p><img src=""https://i.stack.imgur.com/xNNl9.png"" alt=""histogram with mean""></p>

<p>Here's an example (one of dozens I found with a simple google search):</p>

<p><img src=""https://i.stack.imgur.com/fUqwi.jpg"" alt=""hist with mean and median""></p>

<p>(Image source is is the measuring usability blog, <a href=""http://www.measuringusability.com/average-times.php"" rel=""nofollow noreferrer"">here</a>.)</p>

<p>I've seen means, means plus or minus a standard deviation, various quantiles (like median, quartiles, 10th and 90th percentiles) all displayed in various ways.</p>

<p>Instead of drawing a line right across the plot, you might mark information along the bottom of it - like so:</p>

<p><img src=""https://i.stack.imgur.com/0gvWk.png"" alt=""histogram with marginal boxplot""></p>

<p>There's an example (one of many to be found) with a boxplot across the top instead of at the bottom, <a href=""https://www.soils.org/images/publications/aj/99/5/1366fig1.jpeg"" rel=""nofollow noreferrer"">here</a>. </p>

<p>Sometimes people mark in the data:</p>

<p><img src=""https://i.stack.imgur.com/vRujH.png"" alt=""histogram rugplot with jitter""><br>
(I have jittered the data locations slightly because the values were rounded to integers and you couldn't see the relative density well.)</p>

<p>There's an example of this kind, done in Stata, on <a href=""https://www.ctspedia.org/do/view/CTSpedia/BasicHistogramExamples"" rel=""nofollow noreferrer"">this page</a> (see the third one <a href=""https://www.ctspedia.org/wiki/pub/CTSpedia/BasicHistogramExamples/pic2.png"" rel=""nofollow noreferrer"">here</a>)</p>

<p>Histograms are better with a little extra information - <a href=""https://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-histogram/51753#51753"">they can be misleading on their own</a></p>

<p>You just need to take care to explain what your plot consists of! (You'd want a better title and x-axis label than I used here, for starters. Plus an explanation in a figure caption explaining what you had marked on it.)</p>

<p>--</p>

<p>One last plot:</p>

<p><img src=""https://i.stack.imgur.com/7sHku.png"" alt=""histogram with stripchart""></p>

<p>--</p>

<p>My plots are generated in R.</p>

<p>Edit: </p>

<p>As @gung surmised, <code>abline(v=mean...</code> was used to draw the mean-line across the plot and <code>rug</code> was used to draw the data values (though I actually used <code>rug(jitter(...</code> because the data was rounded to integers).</p>

<p>Here's a way to do the boxplot in between the histogram and the axis: </p>

<pre><code>hist(Davis2[,2],n=30)
boxplot(Davis2[,2],
  add=TRUE,horizontal=TRUE,at=-0.75,border=""darkred"",boxwex=1.5,outline=FALSE)
</code></pre>

<p>I'm not going to list what everything there is for, but you can check the arguments in the help (<code>?boxplot</code>) to find out what they're for, and play with them yourself.</p>

<p>However, it's not a general solution - I don't guarantee it will always work as well as it does here (note I already changed the <code>at</code> and <code>boxwex</code> options*). If you don't write an intelligent function to take care of everything, it's necessary to pay attention to what everything does to make sure it's doing what you want.</p>

<p>Here's how to create the data I used (I was trying to show how Theil regression was really able to handle several influential outliers). It just happened to be data I was playing with when I first answered this question.</p>

<pre><code> library(""car"")
 add &lt;- data.frame(sex=c(""F"",""F""),
       weight=c(150,130),height=c(NA,NA),repwt=c(55,50),repht=c(NA,NA))
 Davis2 &lt;- rbind(Davis,add)
</code></pre>

<p>* -- an appropriate value for <code>at</code> is around -0.5 times the value of <code>boxwex</code>; that would be a good default if you write a function to do it; <code>boxwex</code> would need to  be scaled in a way that relates to the y-scale (height) of the boxplot; I'd suggest 0.04 to 0.05 times the upper y-limit might often be okay.</p>

<p>Code for the marginal stripchart:</p>

<pre><code> hist(Davis2[,2],n=30)
 stripchart(jitter(Davis2[,2],amount=.5),
       method=""jitter"",jitter=.5,pch=16,cex=.05,add=TRUE,at=-.75,col='purple3')
</code></pre>
",2013-03-19 21:39:28.337
53264,17249.0,2,,53261.0,,,,CC BY-SA 3.0,"<p>I can only think of this referring to $\eta^2$, computed as:</p>

<p>$\eta^2={SS_{effect} \over SS_{total}}$</p>

<p>This is the proportion of variance explained in the dependent variable by the grouping variable (in this case, a binary variable). This would be indeed the same value as the $R^2$ obtained if the difference between the two groups was estimated using simple linear regression:</p>

<p>$y_i=\beta_0+\beta_1group_i+\epsilon_i$</p>

<p>I can see from the paper that the second F test is actually that of an interaction term, and since it has 1 degree of freedom, I am deducing that the second factor was also a binary variable. In this case, the $\eta^2$'s are partial $\eta^2$'s, which are the proportion of variance explained by the grouping variable (or the interaction term) controlling for the other grouping variable. In this more complex case, the partial $\eta^2$'s are the same as the partial $R^2$'s obtained from the multiple linear regression:</p>

<p>$y_i=\beta_0+\beta_1group_{1i}+\beta_2group_{1i}+\beta_3 \cdot group_{1i} \cdot group_{2i} + \epsilon_i$</p>
",2013-08-14 03:24:19.637
53471,11155.0,2,,49879.0,,,,CC BY-SA 3.0,"<p>I think adaptive in this context just means the reestimation on a rolling basis. So the parameter should not change until there is a change point. Then the true parameter increases and stays constant after it decreases again because of the second change point. The estimated parameter is evaluated compared to the true parameter: How fast does it get the change point? How fast does it <em>adapt</em> to the new environment?</p>
",2013-08-17 07:53:38.020
54624,503.0,2,,54622.0,,,,CC BY-SA 3.0,"<p>Yes it is. I don't know the commands in <code>R</code> but in <code>SAS PROC MIXED</code> you can have variables at either level in the MODEL statement and you can include interactions. e.g., a split plot design </p>

<pre><code>proc mixed;
   class A B Block;
   model Y = A B A*B;
   random Block A*Block;
run;
</code></pre>

<p>where A is assigned to whole plots and B is assigned to subplots.</p>
",2013-09-03 22:08:41.237
55182,17573.0,2,,55150.0,,,,CC BY-SA 4.0,"<p>You have defined causality incorrectly, yes.  Probably, you have heard the saying &quot;correlation isn't causation.&quot;  You have essentially defined causality as correlation.  The problem is worse than that, though.  Causality is not a statistical or probabilistic concept at all, at least as those topics are normally taught.  There is no statistical or probabilistic definition of causality: nothing involving conditional expectations or conditional distributions or suchlike.  It is hard to pick up this fact from courses in statistics or econometrics, though.</p>
<p>Unfortunately, we tend to do a better job saying what causality isn't than what causality is.  Causality always and everywhere comes from theory, from a priori reasoning, from assumptions.  You mentioned econometrics.  If you have been taught instrumental variables competently, then you know that causal effects can only be measured if you have an &quot;exclusion restriction.&quot;  And you know that exclusion restrictions always come from theory.</p>
<p>You said you wanted math, though.  The guy you want to read is <a href=""https://projecteuclid.org/journals/statistics-surveys/volume-3/issue-none/Causal-inference-in-statistics-An-overview/10.1214/09-SS057.full"" rel=""nofollow noreferrer"">Judea Pearl</a>.  It's not easy math, and the math sometimes wanders off into philosophy, but that's because causality is a hard subject.  Here is <a href=""https://web.archive.org/web/20140209084900/http://vserver1.cscs.lsa.umich.edu:80/%7Ecrshalizi/notebooks/causality.html"" rel=""nofollow noreferrer"">a page</a> with more links on the subject.  Here is <a href=""https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/"" rel=""nofollow noreferrer"">a free online book</a> I just came across.  Finally, here is <a href=""https://stats.stackexchange.com/questions/59588/what-do-endogeneity-and-exogeneity-mean-substantively"">a previous question</a> where I gave an answer you might find useful.</p>
",2013-09-12 13:18:37.003
56091,2802.0,2,,55722.0,,,,CC BY-SA 3.0,"<p>If you are searching for proofs, I have been working for some time on a free stats textbook that collects lots of proofs of elementary and less elementary facts that are difficult to find in probability and statistics books (because they are scattered here and there). You can have a look at it at <a href=""http://www.statlect.com/"">http://www.statlect.com/</a></p>
",2013-09-25 11:04:55.647
56783,20473.0,2,,56780.0,,,,CC BY-SA 3.0,"<p><em>(This is an adaptation from Granger &amp; Newbold(1986) ""Forecasting Economic Time Series"").</em>  </p>

<p>By construction, your <em>error cost function</em> is $\left[Y-g(X)\right]^2$. This incorporates a critical assumption (that the error cost function is symmetric around zero) -a different error cost function would not necessarily have the conditional expected value as the $\arg \min$ of its expected value.
You cannot minimize your error cost function because it contains unknown quantities. So you decide to minimize its expected value instead. Then your objective function becomes</p>

<p>$$E\left[Y-g(X)\right]^2 = \int_{-\infty}^{\infty}\left[y-g(X)\right]^2f_{Y|X}(y|x)dy $$</p>

<p>which I believe answers also your second question. It is intuitive that the expected value will be of $Y$ conditional on $X$, since we are trying to estimate/forecast $Y$ based on $X$. Decompose the square to obtain</p>

<p>$$E\left[Y-g(X)\right]^2 = \int_{-\infty}^{\infty}y^2f_{Y|X}(y|x)dy  -2g(X)\int_{-\infty}^{\infty}yf_{Y|X}(y|x)dy \\+ \Big[g(X)\Big]^2\int_{-\infty}^{\infty}f_{Y|X}(y|x)dy$$</p>

<p>The first term does not contain $g(X)$ so it does not affect minimization, and it can be ignored. The integral in the second term equals the conditional expected value of $Y$ given $X$, and the integral in the last term equals unity. So </p>

<p>$$\arg \min_{g(x)} E\left[Y-g(X)\right]^2 = \arg \min_{g(x)} \Big\{ -2g(X)E(Y\mid X) + \Big[g(X)\Big]^2 \Big\}$$</p>

<p>The first derivative w.r.t $g(X)$ is $-2E(Y\mid X) + 2g(X)$ leading to the first order condition for minimization $g(X) = E(Y\mid X)$ while the second derivative is equal to $2&gt;0$ which is sufficient for a minimum.</p>

<p><strong>ADDENDUM:The logic of the ""add and subtract"" proof approach.</strong>  </p>

<p>The OP is puzzled by the approach stated in the question, because it seems tautological. It isn't, because while using the tactic of adding and subtracting makes a <em>specific part</em> of the objective function zero for an arbitrary choice of the term that is added and subtracted, it does NOT equalize the <em>value function</em> , namely the value of the objective function evaluated at the candidate minimizer.</p>

<p>For the choice $g(X) = E(Y \mid X)$ we have the value function $ V\left(E(Y\mid X)\right) = E\Big[ (Y-E(Y \mid X))^2\mid X\Big]$
For the arbitrary choice $g(X) = h(X)$we have the value funtion $ V\left(h(X)\right) = E\Big[ (Y-h(X))^2\mid X\Big]$.</p>

<p>I claim that </p>

<p>$$V\left(E(Y\mid X)\right) \le V\left(h(X)\right)$$
$$\Rightarrow E(Y^2\mid X) -2E\Big [(YE(Y \mid X))\mid X\Big] + E\Big [(E(Y \mid X))^2\mid X\Big] \\\le E(Y^2\mid X) -2E\Big [(Yh(X))\mid X\Big] + E\Big [(h(X))^2\mid X\Big]$$</p>

<p>The first term of the LHS and the RHS cancel out. Also note that the outer expectation is conditional on $X$. By the properties of conditional expectations we end up with</p>

<p>$$...\Rightarrow  -2E(Y \mid X)\cdot E\Big (Y\mid X\Big) + \Big [E(Y \mid X)\Big]^2 \le  -2E(Y\mid X)h(X) + \Big [h(X)\Big]^2$$</p>

<p>$$\Rightarrow  0 \le  \Big [E(Y \mid X)\Big]^2-2E(Y\mid X)h(X) + \Big [h(X)\Big]^2$$</p>

<p>$$\Rightarrow  0 \le  \Big [E(Y \mid X) - h(x)\Big]^2$$
which holds with strict inequality if $h(x) \neq E(Y \mid X)$. So $E(Y \mid X)$ is the global and unique minimizer.</p>

<p>But this also says that the ""add-and-subtract"" approach is not the most illuminating way of proof here.</p>
",2013-10-04 01:05:36.887
56860,155.0,2,,56859.0,,,,CC BY-SA 3.0,"<p>Coming from a behavioural sciences background, I associate this terminology particularly with introductory statistics textbooks. In this context the distinction is that :</p>

<ul>
<li><strong>Descriptive statistics</strong>  are functions of  the sample data that are intrinsically interesting in describing some feature of the data. Classic descriptive statistics include mean, min, max, standard deviation, median, skew, kurtosis.</li>
<li><strong>Inferential statistics</strong> are a function of the sample data that assists you to draw an inference regarding an hypothesis about a population parameter. Classic inferential statistics include z, t, $\chi^2$, F-ratio, etc.</li>
</ul>

<p>The important point is that any statistic, inferential or descriptive, is a function of the sample data. A parameter is a function of the population, where the term population is the same as saying the underlying data generating process.</p>

<p>From this perspective the status of a given function of the data as a descriptive or inferential statistic depends on the purpose for which you are using it. </p>

<p>That said, some statistics are clearly more useful in describing  relevant features of the data, and some are well suited to aiding inference. </p>

<ul>
<li><strong>Inferential statistics:</strong>  Standard test statistics like t and z, for a given data generating process, where the null hypothesis is false, the expected value is strongly influenced by sample size. Most researchers would not see such statistics as estimating a population parameter of intrinsic interest.</li>
<li><strong>Descriptive statistics</strong>: In contrast descriptive statistics do estimate population parameters that are typically of intrinsic interest. For example the sample mean and standard deviation provide estimates of the equivalent population parameters. Even descriptive statistics like the minimum and maximum provide information about equivalent or similar population parameters, although of course in this case, much more care is required. Furthermore, many descriptive statistics might be biased or otherwise less than ideal estimators. However, they still have some utility in estimating a population parameter of interest.</li>
</ul>

<p>So from this perspective, the important things to understand are:</p>

<ul>
<li><strong>statistic</strong>: function of the sample data</li>
<li><strong>parameter</strong>: function of the population (data generating process)</li>
<li><strong>estimator</strong>: function of the sample data used to provide an estimate of a parameter</li>
<li><strong>inference</strong>: process of reaching a conclusion about a parameter</li>
</ul>

<p>Thus, you could either define the distinction between descriptive and inferential based on the intention of the researcher using the statistic, or you could define a statistic based on how it is typically used.</p>
",2013-10-05 05:51:35.693
57055,22.0,2,,57053.0,,,,CC BY-SA 3.0,"<p>I think only one descriptive statistic is needed: ""47% are male"" (assuming 0 encodes female and 1 encodes male). No other statistics are really helpful to describe those data. If you thought these were a randomish sample of a larger population, you could compute the confidence interval for that proportion. </p>
",2013-10-08 13:08:34.837
57177,594.0,2,,57164.0,,,,CC BY-SA 4.0,"<p>As for qualitative differences, the lognormal and gamma are, as you say, quite similar.</p>
<p>Indeed, in practice they're often used to model the same phenomena (some people will use a gamma where others use a lognormal). They are both, for example, constant-coefficient-of-variation models (the CV for the lognormal is <span class=""math-container"">$\sqrt{e^{\sigma^2} -1}$</span>, for the gamma it's <span class=""math-container"">$1/\sqrt \alpha$</span>).</p>
<p>[How can it be constant if it depends on a parameter, you ask? It applies when you model the scale (location for the log scale); for the lognormal, the <span class=""math-container"">$\mu$</span> parameter acts as the log of a scale parameter, while for the gamma, the scale is the parameter that isn't the shape parameter (or its reciprocal if you use the shape-rate parameterization). I'll call the scale parameter for the gamma distribution <span class=""math-container"">$\beta$</span>. Gamma GLMs model the mean (<span class=""math-container"">$\mu=\alpha\beta$</span>) while holding <span class=""math-container"">$\alpha$</span> constant; in that case <span class=""math-container"">$\mu$</span> is also a scale parameter. A model with varying <span class=""math-container"">$\mu$</span> and constant <span class=""math-container"">$\alpha$</span> or <span class=""math-container"">$\sigma$</span> respectively will have constant CV.]</p>
<p>You might find it instructive to look at the density of their <em>logs</em>, which often shows a very clear difference.</p>
<p>The log of a lognormal random variable is ... normal. It's symmetric.</p>
<p>The log of a gamma random variable is left-skew. Depending on the value of the shape parameter, it may be quite skew or nearly symmetric.</p>
<p>Here's an example, with both lognormal and gamma having mean 1 and variance 1/4. The top plot shows the densities (gamma in green, lognormal in blue), and the lower one shows the densities of the logs:</p>
<p><img src=""https://i.stack.imgur.com/I9ARM.png"" alt=""gamma and lognormal, densitiy and density of log"" /></p>
<p>(Plotting the log of the density of the logs is also useful. That is, taking a log-scale on the y-axis above)</p>
<p>This difference implies that the gamma has more of a tail on the left, and less of a tail on the right; the far right tail of the lognormal is heavier and its left tail lighter. And indeed, if you look at the skewness, of the lognormal and gamma, for a given coefficient of variation, the lognormal is more right skew (<span class=""math-container"">$\text{CV}^3+3\text{CV}$</span>) than the gamma (<span class=""math-container"">$2\text{CV}$</span>).</p>
",2013-10-09 22:43:42.927
59199,21029.0,2,,59176.0,,,,CC BY-SA 3.0,"<ol>
<li>Is $(X, X+Y)$ normal? Yes! It is a linear combination of independent univariate normal distributions.</li>
<li>Means: the mean of $X$ is $\mu_1$, and the mean of $X+Y$ is the sum of the means because they are independent, so $\mu_1+\mu_2$.</li>
<li>Variance-covariance. The variance of the sum of two independent random variables is the sum of their variance. So, the variance of $(X,X+Y)$ is $(\sigma_1^2, \sigma_1^2+\sigma_2^2)$. Now calculate the covariance:</li>
</ol>

<p>$$ Cov(X,X+Y) = Cov(X,X)+Cov(X,Y) = \sigma_1^2+0$$</p>

<p>A multidimension normal distribution is defined by its mean and variance-covariance matrix. Therefore,</p>

<p>$$ (X,X+Y)\sim N \left(\left( \begin{array}{c} \mu_1 \\ \mu_1+\mu_2 \end{array}\right),
\left( \begin{array}{ccc}
\sigma_{1}^2 &amp; \sigma_{1}^2  \\
\sigma_{1}^2 &amp;\sigma_{1}^2+\sigma_{2}^2  \\
 \end{array} \right) \right)
$$ </p>
",2013-11-09 08:11:27.770
59239,1411.0,2,,59110.0,,,,CC BY-SA 3.0,"<p>I think you're exactly right.</p>

<p>Set up data like your example:</p>

<pre><code>d &lt;- expand.grid(Site=factor(1:10),rep=1:5)
d &lt;- transform(d,Clone=factor(LETTERS[(as.numeric(Site)+1) %/% 2]))
library(lme4)
## could use development version of lme4 to simulate, but will do
## it by hand
beta &lt;- c(2,1,3,-2,2)  ## clone effects (intercept + differences)
X &lt;- model.matrix(~Clone,d)
set.seed(1)
u.site &lt;- rnorm(length(levels(d$Site)),sd=1)
    d$y &lt;- rnorm(nrow(d),
       mean=X %*% beta + u.site[d$Site],
       sd=2)
</code></pre>

<p>Now analyze:</p>

<pre><code>m1 &lt;- lmer(y~Clone+(1|Site),data=d)
round(fixef(m1),3)
## (Intercept)      CloneB      CloneC      CloneD      CloneE 
##       2.624      -0.034       2.504      -2.297       2.396

VarCorr(m1)
##  Groups   Name        Std.Dev.
##  Site     (Intercept) 0.0000  
##  Residual             1.6108
</code></pre>

<p>I don't think there's actually anything wrong, but I used a pretty big residual variance, and so in this case (probably only on a subset of replicates), <code>lmer</code> estimates a zero among-site variation.</p>
",2013-11-10 00:36:44.380
59371,7155.0,2,,57549.0,,,,CC BY-SA 3.0,"<p>Replacing the euclidean distance in kNN with another distance function is equivalent to ""kernelizing it."" A valid Mercer kernel is any function taking two observations that is continuous, symmetric and has a positive definite covariance matrix $\forall x \in D$. Many interesting properties such as stationarity can be imbued in a kernel that make it an attractive option for things like, time-series and geospatial statistics. There exists kernels for structured input that otherwise could not be represented as fixed length vectors. There exists kernels in the literature that are not valid Mercer kernels and still empirically perform well.</p>

<p>If you would like to know more about kernels, I'd recommend reviewing the literature on Gaussian Processes and SVMs.</p>
",2013-11-11 18:29:39.543
59372,668.0,2,,58770.0,,,,CC BY-SA 3.0,"<p>A <em>formula</em> is requested.  Unfortunately, the situation is so complicated it appears that any formula will merely be a roundabout way of enumerating all the possibilities.  Instead, this answer offers an <em>algorithm</em> which is (a) tantamount to a formula involving sums of products of binomial coefficients and (b) can be ported to many platforms.</p>

<hr>

<p>To obtain such a formula, <strong>break down the possibilities into mutually disjoint groups</strong> in two ways: according to how many letters <em>not</em> in the word are selected in the rack (let this be $m$) and according to how many wildcards (blanks) are selected (let this be $w$).  When there are $r=7$ tiles in the rack, $N$ available tiles, $M$ available tiles with letters not in the word, and $W=2$ blanks available, the number of possible choices given by $(m,w)$ is</p>

<p>$$\binom{M}{m}\binom{W}{w}\binom{N-M-W}{r-m-w}$$</p>

<p>because the choices of non-word letters, blanks, and word letters are independent conditional on $(m,w,r).$</p>

<p><strong>This reduces the problem to finding the number of ways to spell a word when selecting only from the tiles representing the word's letters,</strong> <em>given</em> that $w$ blanks are available and $r-m-w$ tiles will be selected.  The situation is messy and no closed formula seems available.  For instance, with $w=0$ blanks and $m=3$ out-of-word letters are drawn there will be precisely four letters left to spell ""boot"" that were drawn from the ""b"", ""o"", and ""t"" tiles.  Given there are $2$ ""b""'s, $8$ ""o""'s, and $6$ ""t""'s in the Scrabble tile set, there are positive probabilities of drawing (multisets) ""bboo"", ""bbot"", ""bbtt"", ""booo"", ""boot"", ""bott"", ""bttt"", ""oooo"", ""ooot"", ""oott"", ""ottt"", and ""tttt"", but only one of these spells ""boot"".  And that was the easy case!  For example, supposing the rack contains five tiles chosen randomly from the ""o"", ""b"", and ""t"" tiles, together with both blanks, there are many more ways to spell ""boot""--and not to spell it.  For instance, ""boot"" can be spelled from ""__boott"" and ""__bbttt"", but not from ""__ttttt"".</p>

<p><strong>This counting--the heart of the problem--can be handled recursively.</strong>  I will describe it with an example.  Suppose we wish to count the ways of spelling ""boot"" with one blank and four more tiles from the collection of ""b"", ""o"", and ""t"" tiles (whence the remaining two tiles show non-blank letters not in {""b"", ""o"", ""t""}).  Consider the first letter, ""b"":</p>

<ol>
<li><p>A ""b"" can be drawn in $\binom{2}{1}$ ways from the two ""b"" tiles available.  This reduces the problem to counting the number of ways of spelling the suffix ""oot"" using both blanks and just three more tiles from the collection of ""o"" and ""t"" tiles.</p></li>
<li><p>One blank can be designated as a ""b"".  This reduces the problem to counting the number of ways of spelling ""oot"" using the remaining blank and just three more tiles from the collection of ""o"" and ""t"" tiles.</p></li>
</ol>

<p>In general, steps (1) and (2)--which are disjoint and therefore contribute additively to the probability calculations--can be implemented as a loop over the possible number of blanks that might be used for the first letter.  The reduced problem is solved recursively.  The base case occurs when there's one letter left, there is a certain number of tiles with that letter available, and there may be some blanks in the rack, too.  We only have to make sure that the number of blanks in the rack plus the number of available tiles will be enough to obtain the desired quantity of that last letter.</p>

<p>Here is <code>R</code> code for the recursive step.  <code>rack</code> usually equals $7$, <code>word</code> is an array of counts of the letters (such as <code>c(b=1, o=2, t=1)</code>), <code>alphabet</code> is a similar structure giving the numbers of available tiles with those letters, and <code>wild</code> is the number of blanks assumed to occur in the rack.</p>

<pre class=""lang-r prettyprint-override""><code>f &lt;- function(rack, word, alphabet, wild) {
  if (length(word) == 1) {
    return(ifelse(word &gt; rack+wild, 0, choose(alphabet, rack)))
  }
  n &lt;- word[1]
  if (n &lt;= 0) return(0)
  m &lt;- alphabet[1]
  x &lt;- sapply(max(0, n-wild):min(m, rack), 
              function(i) {
                choose(m, i) * f(rack-i, word[-1], alphabet[-1], wild-max(0, n-i))
              })
  return(sum(x))
}
</code></pre>

<p>An interface to this function specifies the standard Scrabble tiles, converts a given word into its multiset data structure, and performs the double sum over $m$ and $w$.  Here is where the binomial coefficients $\binom{M}{m}$ and $\binom{W}{w}$ are computed and multiplied.</p>

<pre class=""lang-r prettyprint-override""><code>scrabble &lt;- function(sword, n.wild=2, rack=7, 
              alphabet=c(a=9,b=2,c=2,d=4,e=12,f=2,g=3,h=2,i=9,j=1,k=1,l=4,m=2,
                         n=6,o=8,p=2,q=1,r=6,s=4,t=6,u=4,v=2,w=2,x=1,y=2,z=1),
              N=sum(alphabet)+n.wild) {
  word = sort(table(strsplit(sword, NULL))) # Sorting speeds things a little
  a &lt;- sapply(names(word), function(s) alphabet[s])
  names(a) &lt;- names(word)
  x &lt;- sapply(0:n.wild, function(w) {
    sapply(sum(word):rack-w, 
           function(i) {
             f(i, word, a, wild=w) *
               choose(n.wild, w) * choose(N-n.wild-sum(a), rack-w-i)
           })
  })
  return(list(numerator = sum(x), denominator = choose(N, rack),
              value=sum(x) / choose(N, rack)))
}
</code></pre>

<hr>

<p>Let's try out this solution and time it as we go.  The following test uses the same inputs employed in <a href=""https://stats.stackexchange.com/a/74680"">the simulations by @Rasmus BÃ¥Ã¥th</a>:</p>

<pre class=""lang-r prettyprint-override""><code>system.time(x &lt;- sapply(c(""boot"", ""red"", ""axe"", ""zoology""), scrabble))
</code></pre>

<p>This machine reports $0.05$ seconds total elapsed time: reasonably quick.  The results?</p>

<pre><code>&gt; x
            boot        red         axe         zoology     
numerator   114327888   1249373480  823897928   11840       
denominator 16007560800 16007560800 16007560800 16007560800 
value       0.007142118 0.07804896  0.0514693   7.396505e-07
</code></pre>

<p>The probability for ""boot"" of $114327888/16007560800$ exactly equals the value $2381831/333490850$ obtained in <a href=""https://stats.stackexchange.com/a/74690/919"">my other answer</a> (which uses a similar method but couches it in a more powerful framework requiring a symbolic algebra computing platform).  The probabilities for all four words are reasonably close to BÃ¥Ã¥th's simulations (which could not be expected to give an accurate value for ""zoology"" due to its low probability of $11840/16007560800,$ which is less than one in a million).</p>
",2013-11-11 18:34:44.430
59515,22564.0,2,,58861.0,,,,CC BY-SA 3.0,"<p>If I understood correctly you are not worried about false positives. In that case the patients that do not have the complication are not of interest to you. You want to design the study to claim a certain degree of sensitivity.</p>

<p>There may be a way to calculate this directly, but since no one has answered you yet I will show my approach. I calculated the p-values for a one sided proportion test using the R prop.test() function and null hypothesis that % correct &lt;0.5. <strong>This approach ignores all the patients who did not have complications. It gives the number of patients with complications that you will need.</strong></p>

<p>This was done for samples sizes in the sequence of 10 to 100 by 1, and for % correct from .01 to 1 by .01. Then from these results I found the minimum % correct that yielded p&lt;0.05 for each sample size.</p>

<p>The upper chart shows the relationship between ""significant"" % corrects vs sample size. While the lower shows power functions for a few example sample sizes. For the latter, the horizontal line corresponds to p=.05. </p>

<p>If n=10 then the minimum observed % correct that will allow you to ""reject"" (at p&lt;0.05) the hypothesis that your method is will detect less than half the patients with complications is 76.1%. Even for n=100 (remember this is 100 patients <strong>with complications</strong>), you will need to observe at least a 58.3% success rate to claim ""significance"".</p>

<p>If you want to calculate the total number of patients to enroll you need to multiply the sample sizes by 1/0.15 which is ~7.</p>

<p>I am fairly confident about this approach but not 100%, so hopefully someone will check it.</p>

<p><img src=""https://i.stack.imgur.com/8akbx.png"" alt=""enter image description here""></p>

<p><strong>R code:</strong></p>

<pre><code>n&lt;-seq(10,100,by=1) # sample sizes to check
perc.correct&lt;-seq(.001,1,by=.001) # observed percent correct to check

alpha&lt;-0.05 # ""Significance"" Cutoff

out=matrix(nrow=length(n)*length(perc.correct),ncol=3)
cnt=1
for(i in n){
  for(j in perc.correct){
    p&lt;-prop.test(j*i,i, p=.5, alternative=""g"", correct=F)$p.value
    out[cnt,]&lt;-cbind(i,100*j,p)
    cnt&lt;-cnt+1
  }
}


# get lowest % correct that yielded p&lt;0.05 for each sample size
out2=matrix(nrow=length(n),ncol=2)
cnt&lt;-1
for(n2 in n){
  min.perc&lt;-head(out[which(out[,1]==n2 &amp; out[,3]&lt;alpha),2],1)
  out2[cnt,]&lt;-cbind(n2,min.perc)
  cnt&lt;-cnt+1
}


# plots
layout(matrix(c(1,1,2,3,4,5), ncol=2, nrow=3, byrow=T))
plot(out2, xlab=""Sample Size"", ylab=""% Correct"",
     main=c(""% Correct to Reject Hypothesis Success &lt;50%"",paste(""alpha="",alpha))
)

for(n2 in c(10,30,60,100)){
  min.perc&lt;-head(out[which(out[,1]==n2 &amp; out[,3]&lt;alpha),2],1)
  plot(out[which(out[,1]==n2),2],out[which(out[,1]==n2),3], 
       xlab=""% Correct"", ylab=""P-Value"", 
       main=c(paste(""n="",n2),paste(""Min % Correct for p&lt;"", alpha,""="",min.perc))
  )
  abline(h=alpha)
}
</code></pre>

<p><strong>Edit:</strong>
Here is a different perspective. In this case we take the theory of % correct prediction of complications >50% as the null hypothesis. So using the significance testing logic we would choose to study the imaging technique more if it is not significant, but not use it if the result is significant. Here we are attempting to disprove the research hypothesis rather than the opposite as was done above (although the original way using reverse logic is the common approach for some strange reason).</p>

<p><img src=""https://i.stack.imgur.com/0tijE.png"" alt=""enter image description here""></p>

<p><strong>R code 2 (lines with minor changes marked with ""#*"" at the end):</strong></p>

<pre><code>n&lt;-seq(10,100,by=1) # sample sizes to check
perc.correct&lt;-seq(.001,1,by=.001) # observed percent correct to check

alpha&lt;-0.05 # ""Significance"" Cutoff

out=matrix(nrow=length(n)*length(perc.correct),ncol=3)
cnt=1
for(i in n){
  for(j in perc.correct){
    p&lt;-prop.test(j*i,i, p=.5, alternative=""l"", correct=F)$p.value #*
    out[cnt,]&lt;-cbind(i,100*j,p)
    cnt&lt;-cnt+1
  }
}


# get highest % correct that yielded p&lt;0.05 for each sample size
out2=matrix(nrow=length(n),ncol=2)
cnt&lt;-1
for(n2 in n){
  max.perc&lt;-tail(out[which(out[,1]==n2 &amp; out[,3]&lt;alpha),2],1) #*
  out2[cnt,]&lt;-cbind(n2,max.perc) #*
  cnt&lt;-cnt+1
}


# plots
layout(matrix(c(1,1,2,3,4,5), ncol=2, nrow=3, byrow=T))
plot(out2, xlab=""Sample Size"", ylab=""% Correct"",
     main=c(""% Correct to Reject Hypothesis Success &gt;50%"",paste(""alpha="",alpha))
)

for(n2 in c(10,30,60,100)){
  max.perc&lt;-tail(out[which(out[,1]==n2 &amp; out[,3]&lt;alpha),2],1) #*
  plot(out[which(out[,1]==n2),2],out[which(out[,1]==n2),3], 
       xlab=""% Correct"", ylab=""P-Value"", 
       main=c(paste(""n="",n2),paste(""Max % Correct for p&lt;"", alpha,""="",max.perc)) #*
  )
  abline(h=alpha)
}
</code></pre>
",2013-11-13 16:30:01.650
59638,23776.0,2,,58292.0,,,,CC BY-SA 3.0,"<p>Since the ""deficient"" status in known for the moment of entry into follow-up, the data can be regarded as left-censored (not truncated, where it is unknown what the status is at entry). </p>

<p>Take a look at this paper, which deals with a very similar problem in medical research. R, SAS and Stata code (for example using survfit) is provided in the web appendix.</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/21422059"" rel=""nofollow"">Cain et al., 2011 survival with left-censoring and left-truncating</a></p>
",2013-11-15 07:41:25.893
59733,23085.0,2,,58330.0,,,,CC BY-SA 3.0,"<p>So I think I figured out a valid counter example.</p>

<p>$X_i \overset{iid}\sim N(\theta, 1)$. Then $\bar{X}$ is minimax. (This can be shown by using a sequence of priors). However, $\bar{X}$ being unbiased for $\theta$, can only be Bayes for any proper prior if the Bayes Risk is 0.</p>

<p>$R(\theta,\bar{X}) =\dfrac{1}{n} \implies \int_{\Theta} R(\theta,\bar{X}) \pi(\theta) d\theta = \dfrac{1}{n} \int_{\Theta} \pi(\theta) d\theta = \dfrac{1}{n}$</p>

<p>Thus, as long $\pi$ is proper, Bayes risk of $\bar{X}$ cannot be 0. Hence $\bar{X}$ cannot be Bayes with respect to any proper. The same reasoning can be made for admissible estimators.</p>
",2013-11-16 21:11:29.867
59802,22564.0,2,,59064.0,,,,CC BY-SA 3.0,"<p>I think your request for the ""overall correlation"" may be asking the wrong question. If you already know that you have varied factor1 and factor2, the correlations you want to look for are conditional the combination of those factors. It is unlikely the independent variables have absolutely 0 effect on the dependent variables, so looking at the total correlation actually includes less information than looking at each individually.</p>

<p><img src=""https://i.stack.imgur.com/HP8Nv.png"" alt=""enter image description here""></p>

<pre><code>  factor1 factor2      r     p
1       A       1  -0.67 0.034
2       B       1 -0.043 0.907
3       A       2 -0.366 0.298
4       B       2 -0.632  0.05
5       A       3  0.066 0.856
6       B       3 -0.276  0.44
</code></pre>

<p><strong>R code:</strong></p>

<pre><code>set.seed(1154)

dat &lt;- data.frame(id=gl(10, 6),
                   factor1=gl(2, 3, labels=c(""A"", ""B"")),
                   factor2=gl(3, 1),
                   DV1=rnorm(60),
                   DV2=rnorm(60))



out=matrix(nrow=6,ncol=4)
par(mfrow=c(3,2))
cnt&lt;-1
for(j in unique(dat$factor2)){
  for(i in unique(dat$factor1)){
    sub&lt;-dat[which(dat$factor1==i &amp; dat$factor2==j),]

    cor.result&lt;-cor.test(sub$DV1,sub$DV2)

    p&lt;-round(cor.result$p.value,3)
    r&lt;-round(cor.result$estimate,3)
    out[cnt,]&lt;-cbind(i,j, r, p)

    plot(sub$DV1,sub$DV2, xlab=""DV1"", ylab=""DV2"",
         main=c(paste(""Factor1:"", i),paste(""Factor2:"", j),paste(""r="",r,""p="",p)))
    abline(lm(sub$DV2~sub$DV1))
    cnt&lt;-cnt+1
  }
}

out&lt;-as.data.frame(out)
colnames(out)&lt;-c(""factor1"",""factor2"",""r"",""p"")
</code></pre>
",2013-11-18 08:50:56.970
59809,23094.0,2,,58353.0,,,,CC BY-SA 3.0,"<p>After much searching, I have found exactly the software I am looking for:</p>

<p><a href=""http://www.datplot.com/"" rel=""nofollow"">http://www.datplot.com/</a></p>

<p>Simple, GUI-driven software that allows you to import raw data and plot graphs, with dynamic scroll and zoom.</p>
",2013-11-18 12:42:37.623
60004,15827.0,2,,57869.0,,,,CC BY-SA 3.0,"<p>I guess I would like to read or at least browse in that too, but only a polymath or a committee could write it, and the polymath isn't evident and committee books often don't work well. Also, many of the general books on statistics that tend to pop up from (e.g.) searches on Amazon just leave out most of the interesting technical details and/or are written by people not close to any cutting edge. </p>

<p>But I would recommend browsing in the <em>Encyclopedia of Statistical Sciences</em> if a library near you holds a copy: </p>

<p><a href=""http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471150444.html"" rel=""nofollow"">http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471150444.html</a></p>

<p>and also that you look through what appears in <em>Statistical Science</em>, which has a good track record of readable review and discussion papers. </p>

<p>I would venture an assertion that most specialists in econometrics, psychometrics, machine learning, etc. would have little confidence that people outside their own field really understand what is currently central and most interesting in that field. (So, what else is new?) </p>
",2013-11-20 14:43:40.717
60010,22601.0,2,,57752.0,,,,CC BY-SA 3.0,"<p>Here's my solution to the problem. I calculate all possible combinations of k of n items and calculate their mutual dependencies by transforming the problem in a graph-theoretical one: Which is the complete graph containing all k nodes with the smallest edge sum (dependencies)? Here's a python script using the networkx library and one possible output. Please apologize for any ambiguity in my question!</p>

<p><strong>Code:</strong></p>

<pre><code>import networkx as nx
import itertools
import os

#Create new graph
G=nx.Graph()

#Each node represents a dimension
G.add_nodes_from([1,2,3,4,5,6,7,8,9,10,11])

#For each dimension add edges and correlations as weights
G.add_weighted_edges_from([(3,1,0.563),(3,2,0.25)])
G.add_weighted_edges_from([(4,1,0.688),(4,3,0.438)])
G.add_weighted_edges_from([(5,1,0.25),(5,2,0.063),(5,3,0.063),(5,4,0.063)])
G.add_weighted_edges_from([(6,1,0.063),(6,2,0.25),(6,3,0.063),(6,4,0.063),(6,5,0.063)])
G.add_weighted_edges_from([(7,2,0.25),(7,3,0.063),(7,5,0.125),(7,6,0.063)])
G.add_weighted_edges_from([(8,1,0.125),(8,2,0.125),(8,3,0.5625),(8,5,0.25),(8,6,0.188),(8,7,0.125)])
G.add_weighted_edges_from([(9,1,0.063),(9,2,0.063),(9,3,0.25),(9,6,0.438),(9,7,0.063),(9,8,0.063)])
G.add_weighted_edges_from([(10,1,0.25),(10,2,0.25),(10,3,0.563),(10,4,0.125),(10,5,0.125),(10,6,0.125),(10,7,0.125),(10,8,0.375),(10,9,0.125)])
G.add_weighted_edges_from([(11,1,0.125),(11,2,0.063),(11,3,0.438),(11,5,0.063),(11,6,0.1875),(11,7,0.125),(11,8,0.563),(11,9,0.125),(11,9,0.188)])

nodes = set(G.nodes())
combs = set(itertools.combinations(nodes,6))
sumList = []
for comb in combs:
    S=G.subgraph(list(comb))
    sum=0
    for edge in S.edges(data=True):
        sum+=edge[2]['weight']
    sumList.append((sum,comb))

sorted = sorted(sumList, key=lambda tup: tup[0])    

fo = open(""dependency_ranking.txt"",""wb"")

for i in range(0,len(sorted)):
    totalWeight = sorted[i][0]
    nodes = list(sorted[i][1])
    nodes.sort()
    out = str(i)+"": ""+str(totalWeight)+"",""+str(nodes)
    fo.write(out.encode())
    fo.write(""\n"".encode())

fo.close()

S=G.subgraph([1,2,3,4,6,7])
sum = 0
for edge in S.edges(data=True):
        sum+=edge[2]['weight']
print(sum)
</code></pre>

<p><strong>Sample output:</strong></p>

<pre><code>0: 1.0659999999999998,[2, 4, 5, 7, 9, 11]
1: 1.127,[4, 5, 7, 9, 10, 11]
2: 1.128,[2, 4, 5, 9, 10, 11]
3: 1.19,[2, 4, 5, 7, 8, 9]
4: 1.2525,[4, 5, 6, 7, 10, 11]
5: 1.377,[2, 4, 5, 7, 9, 10]
6: 1.377,[2, 4, 7, 9, 10, 11]
7: 1.377,[2, 4, 5, 7, 10, 11]
</code></pre>

<p>Input graph:
<img src=""https://i.stack.imgur.com/4t9Rh.png"" alt=""enter image description here""></p>

<p>Solution graph:
<img src=""https://i.stack.imgur.com/6A0nj.png"" alt=""enter image description here""></p>

<p>For a toy example, k=4, n=6:
Input graph:
<img src=""https://i.stack.imgur.com/TZ0iZ.png"" alt=""enter image description here""></p>

<p>Solution graph:
<img src=""https://i.stack.imgur.com/IKcqR.png"" alt=""enter image description here""></p>

<p>Best,</p>

<p>Christian</p>
",2013-11-20 15:38:27.410
60256,12900.0,2,,58248.0,,,,CC BY-SA 4.0,"<p>Genetic algorithms were used to lower the prime gap to 4680 in the recent Zhang twin primes proof breakthrough and <a href=""https://terrytao.wordpress.com/tag/polymath8/"" rel=""nofollow noreferrer"">associated Polymath project</a>. The bound has been lowered by other methods but it shows some potential for machine learning approaches in this or related areas. they can be used to devise/optimize effective ""combs"" or basically sieves for analyzing/screening smallest-possible prime gaps.</p>

<p><a href=""https://www.quantamagazine.org/mathematicians-team-up-on-twin-primes-conjecture-20131119/"" rel=""nofollow noreferrer"">Together and Alone, Closing the Prime Gap </a> (Erica Klarreich, Quanta magazine, 19 November 2013):</p>

<blockquote>
  <p>The team eventually came up with the Polymath projectâ€™s record-holder â€” <a href=""https://math.mit.edu/~primegaps/sieve.html?ktuple=632"" rel=""nofollow noreferrer"">a 632-tooth comb whose width is 4,680</a> â€” using a genetic algorithm that â€œmatesâ€ admissible combs with each other to produce new, potentially better combs.</p>
</blockquote>
",2013-11-23 21:14:03.227
60467,24121.0,2,,59063.0,,,,CC BY-SA 4.0,"<p>Suppose you have data <span class=""math-container"">$\{Y_t,X_{t-h}\}_{t=h+1}^T$</span>, where <span class=""math-container"">$h \in \{1,2,\ldots\},$</span> and your goal is to build a model (say, <span class=""math-container"">$\hat f(X_{t-h})$</span>) to predict <span class=""math-container"">$Y_t$</span> given <span class=""math-container"">$X_{t-h}$</span>. For concreteness, suppose the data is daily and <span class=""math-container"">$T$</span> corresponds to today.</p>
<p>In-sample analysis means to estimate the model using all available data up to and including <span class=""math-container"">$T$</span>, and then compare the model's fitted values to the actual realizations. However, this procedure is known to draw an overly optimistic picture of the model's forecasting ability, since common fitting algorithms (e.g. using squared error or likelihood criteria) tend to take pains to avoid large prediction errors, and are thus susceptible to overfitting - mistaking noise for signal in the data.</p>
<p>A true out-of-sample analysis would be to estimate the model based on data up to and including today, construct a forecast of tomorrow's value <span class=""math-container"">$Y_{T+1}$</span>, wait until tomorrow, record the forecast error <span class=""math-container"">$e_{T+1} \equiv Y_{T+1} - \hat f(X_{T+1-h}),$</span> re-estimate the model, make a new forecast of <span class=""math-container"">$Y_{T+2}$</span>, and so forth. At the end of this exercise, one would have a sample of forecast errors <span class=""math-container"">$\{e_{T+l}\}_{l=1}^L$</span> which would be truly out-of-sample and would give a very realistic picture of the model's performance.</p>
<p>Since this procedure is very time-consuming, people often resort to &quot;pseudo&quot;, or &quot;simulated&quot;, out-of-sample analysis, which means to mimic the procedure described in the last paragraph, using some historical date <span class=""math-container"">$T_0 &lt; T$</span>, rather than today's date <span class=""math-container"">$T$</span>, as a starting point. The resulting forecasting errors <span class=""math-container"">$\{e_t\}_{t=T_0+1}^T$</span> are then used to get an estimate of the model's out-of-sample forecasting ability.</p>
<p>Note that pseudo-out-of-sample analysis is not the only way to estimate a model's out-of-sample performance. Alternatives include cross-validation and information criteria.</p>
<p>A very good discussion of all these issues is provided in Chapter 7 of</p>
<p>[old link]</p>
<p><a href=""http://www.stanford.edu/%7Ehastie/local.ftp/Springer/OLD/ESLII_print4.pdf"" rel=""nofollow noreferrer"">http://www.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf</a></p>
<p>[new link, 11/01/2021]</p>
<p><a href=""https://web.stanford.edu/%7Ehastie/Papers/ESLII.pdf"" rel=""nofollow noreferrer"">https://web.stanford.edu/~hastie/Papers/ESLII.pdf</a></p>
",2013-11-26 17:19:29.190
60584,22923.0,2,,54724.0,,,,CC BY-SA 3.0,"<p>In these 2 situations, comparative performance flexible vs. inflexible model also depends on:</p>

<ul>
<li>is true relation y=f(x) close to linear or very non-linear;</li>
<li>do you tune/constrain flexibility degree of the ""flexible"" model when fitting it.</li>
</ul>

<p>If relation is close to linear and you don't constrain flexibility, then linear model should give better test error in both cases because flexible model likely to overfit in both cases. </p>

<p>You can look at it as that:</p>

<ul>
<li>In both cases data doesn't contain enough information about true relation (in first case relation is high dimensional and you have not enough data, in second case it corrupted by noise) but 
<ul>
<li>linear model brings some external prior information about true relation (constrain class of fitted relations to linear ones) and </li>
<li>that prior info turns out to be right (true relation is close to linear). </li>
</ul></li>
<li>While flexible model doesn't contain prior information (it can fit anything), so it fits to noise.</li>
</ul>

<p>If however true relation is very non-linear, it's hard to say who will win (both will loose :)). </p>

<p>If you tune/constrain degree of flexibility and do it in a right way (say by cross-validation), then flexible model should win in all cases. </p>
",2013-11-28 02:02:48.857
69580,27332.0,2,,58069.0,,,,CC BY-SA 3.0,"<p>Actually it's mentioned in the Regression section of <a href=""http://en.wikipedia.org/wiki/Mean_squared_error"">Mean squared error</a> in Wikipedia:</p>

<blockquote>
  <p>In regression analysis, the term mean squared error is sometimes used
  to refer to the unbiased estimate of error variance: the residual sum
  of squares divided by the number of degrees of freedom.</p>
</blockquote>

<p>You can also find some informations here: <a href=""http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics#Regressions"">Errors and residuals in statistics</a>
It says the expression mean squared error may have different meanings in different cases, which is tricky sometimes.</p>
",2014-03-19 13:05:56.903
61071,3999.0,2,,58609.0,,,,CC BY-SA 3.0,"<p>A few examples from clinical research might be variables that arise <em>after</em> randomization - randomization doesn't protect you from those at all. A few off the top of my head, that have been raised as either possibilities or been noted:</p>

<ul>
<li>Changes in behavior post voluntary adult male circumcision for the prevention of HIV</li>
<li>Differential loss to follow-up between treatment and control arms of an RCT</li>
<li>A more specific example might include the recent ""Benefits of Universal Gowning and Gloving"" study looking at prevention of hospital acquired infections (<a href=""http://haicontroversies.blogspot.com/2013/10/dont-bugg-me.html"" rel=""noreferrer"">blog commentary here</a>, the paper is behind a paywall). In addition to the intervention, and potentially <em>because</em> of it, both hand hygiene rates and contact rates between patients and staff/visitors changed.</li>
</ul>

<p>Randomization protects against none of those effects, because they arise post-randomization.</p>
",2013-12-04 17:29:38.140
61392,2958.0,2,,59134.0,,,,CC BY-SA 3.0,"<p>I think the problem is that you train (or rather: optimize) using your ""test"" set. In other words, you can do this, but then you need an additional independent test set for the final validation. Or a nested validation set up from the beginning.</p>

<p>This is how I see the problem:</p>

<p>There can be combinations of training and test data (particular splits), where the model trained on that training data works well with the given test set -- regardless of how representative the test set is for the actual problem. Your strategy is actually a search strategy that tries to find such combinations. As there is no guarantee that you'll encounter a really satisfactory model before encountering one of the ""fake satisfactory"" models, there is trouble lurking. </p>

<p>Because you decide <em>depending on the test set performance</em> whether to go on for a new model or not, your testing is not independent. I think this is related to the problems with other iterative model optimization approaches, where an increase in model quality seems to occur also between equivalent models.</p>

<p>Here's a simulation:</p>

<ul>
<li>multivariate normally distributed data, sd = 1 for 25 variates, first 4 informative being 0 for one class and 1 for the other. </li>
<li>500 cases of each class in the data set, split 80:20 randomly without replacement into train and ""test"" sets.  </li>
<li>50000 cases each class independent test set.</li>
<li>repeat until ""acceptable"" accuracy of 90% is reached according to internal test set.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/DgI0s.png"" alt=""enter image description here""><br>
circles: internal test estimate, dots and whiskers: external independent test set with 95% ci (Agresti-Coull method), red line: cumulative maximum of internal estimate. </p>

<p>Your rule basically uses the cumulative maximum of the internal test set. In the example that means that within few iterations, you end up with an optimistic bias that claims 1/3 less errors than your models actually have. Note that the models here cannot be distinguished with a 200 cases test set. The order of differences between the large external test set results is the same as the confidence interval width.<br>
You can also nicely see what I mean with <em>skimming variance</em>: the internal test set estimate itself is unbiased. What causes the bias is doing (potentially large) numbers of iterations and picking the maximum. </p>

<p>Besides the optimization that is hidden in this procedure as well, the problem is of course the large variance of the accuracy. Other performance measures like Brier's score have lower variance and thus do not lead to such serious overfitting that fast. </p>

<hr>

<p>The code of the simulation:</p>

<pre><code>require (""binom"")
require (""MASS"")

set.seed (seed=1111)

randomdata &lt;- function (class, n, p = 25, inf = 1:4){
  x &lt;- matrix (rnorm (n * p), nrow = n)
  x [, inf] &lt;- x [, inf] + class 

  data.frame (class = class, X = I (x))
}


data &lt;- rbind (randomdata (class = 0, n = 500), 
               randomdata (class = 1, n = 500)) 
indeptest &lt;- rbind (randomdata (class = 0, n = 5e4), 
                    randomdata (class = 1, n = 5e4)) 

internal.acc &lt;- rep (NA, 100)
external.acc &lt;- rep (NA, 100)

for (i in 1 : 100){
  i.train &lt;- sample (nrow (data), size=nrow (data) *.8, replace=FALSE)
  train &lt;- data [ i.train, ]
  test  &lt;- data [- i.train,]

  model &lt;- lda (class ~ X, train)

  pred &lt;- predict (model, test)
  indep.pred &lt;- predict (model, indeptest)

  #table (reference = test$class, prediction = pred$class)
  internal.acc [i] &lt;- sum (diag (table (reference = test$class, prediction = pred$class))) / nrow (test)
  external.acc [i] &lt;- sum (diag (table (reference = indeptest$class, prediction = indep.pred$class))) / nrow (indeptest)

  if (internal.acc [i] &gt;= 0.9) break ;
  cat (""."")
  }

internal.acc &lt;- internal.acc [1 : i]
external.acc &lt;- external.acc [1 : i]

plot (internal.acc, ylab = ""accuracy"", xlab = ""iteration"")
points (external.acc, pch = 20)
lines (cummax (internal.acc), col = ""red"")

ci &lt;- binom.agresti.coull (external.acc*nrow (indeptest), nrow (indeptest))
segments (x0 = seq_along (external.acc), x1 = seq_along (external.acc), y0 = ci$lower, y1 = ci$upper)
</code></pre>
",2013-12-08 21:16:58.060
61894,13165.0,2,,58372.0,,,,CC BY-SA 3.0,"<p>BP like methods find the optimal parameters for maximizing posterior. This is basically the parameter estimation. Inference can also be done in a similar way. Prediction of the new label could be cast as maximizing a posterior given the optimal parameters of the new model. This is a fully Bayesian approach to estimation, in contrary to directly doing EM steps. For more details see section 4 @ <a href=""http://www.eecs.berkeley.edu/~wainwrig/Papers/Wainwright06_JMLR.pdf"" rel=""nofollow"">http://www.eecs.berkeley.edu/~wainwrig/Papers/Wainwright06_JMLR.pdf</a> </p>
",2013-12-14 21:30:35.820
62181,13037.0,2,,57487.0,,,,CC BY-SA 3.0,"<p>Consider $\mathbf{Y} = \mathbf{X}\mu + \epsilon$ where the matrices are defined as follows:</p>

<p>$\begin{pmatrix} X_1 \\ X_2 \\ \vdots\\ X_n \\ Y_1 \\ Y_2 \\ \vdots \\ Y_m \end{pmatrix} = \begin{pmatrix} 1 \\ 1\\ \vdots \\ 1 \end{pmatrix}\begin{pmatrix} \mu \end{pmatrix} + \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \\ \delta_1 \\ \delta_2 \\ \vdots \\ \delta_m\end{pmatrix}$</p>

<p>Now we consider this as a weighted least squares problem. Let $w_i = \rho$ for $i=1,\ldots,n$ and $w_i = 1$ for $i=n+1,\ldots,n+m$. Then the weighted least squares estimate of $\mu$ is </p>

<p>$\hat{\mu}_{WLS} = \left(\mathbf{X}^T\mathbf{W}^{-1}\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{W}\mathbf{Y}$ where $diag(\mathbf{W}) = (w_1,\ldots,w_{n+m})=(\rho,\ldots,\rho,1,\ldots,1)$.</p>

<p>Solving for $\hat{\mu}$ we get</p>

<p>$\begin{align*}
\hat{\mu} &amp;= \dfrac{1}{\rho n + m}\left(\rho\sum_{i=1}^n X_i + \sum_{i=1}^m Y_i\right)\\
&amp;= \dfrac{n\rho\bar{X} + m\bar{Y}}{n\rho + m}\\
&amp;= \dfrac{\rho\bar{X} + \tfrac{m}{n}\bar{Y}}{\rho +\tfrac{m}{n}}\\
&amp;= \dfrac{\rho\bar{X} + \theta\bar{Y}}{\rho +\theta}
\end{align*}$</p>
",2013-12-18 18:33:25.387
63675,13549.0,2,,57341.0,,,,CC BY-SA 3.0,"<p>In the interest of closing up this question, I have received confirmation from a colleague that the correct interpretation is that permutation destroys the structure of autocorrelation.</p>

<p>I have bolded (in my original question) the passage in the user guide that is relevant to the answer. </p>
",2014-01-10 23:03:52.990
66211,668.0,2,,58203.0,,,,CC BY-SA 3.0,"<p>You are correct: there are serious problems with the sampling procedures described in <a href=""http://ocw.jhsph.edu/courses/statmethodsforsamplesurveys/PDFs/Lecture2.pdf"" rel=""noreferrer"">that reference</a>.  Its description of how systematic sampling should be carried out is plainly wrong, but the problems can be fixed.</p>

<p>We are talking about systematic sampling of a population of $N$ sampling units.  They have been indexed with $1, 2, \ldots, N$ beforehand.  Systematic sampling selects a desired number $n$ of individuals in the population by taking them at relatively regular intervals within this set of indexes.</p>

<p>The three methods in the reference begin by choosing the nominal sampling interval $k$ either to be $N/n$ (potentially non-integral), $\lceil N/n \rceil$ (rounding up), or $\lfloor N/n \rfloor$ (rounding down).  Starting at a random integer $r$ chosen uniformly from the set $\{1, 2, \ldots, \lfloor k \rfloor\}$ they select the units at indexes $r, [r + k], [r + 2k], \ldots, [r + (n-1)k],$ where ""$[x]$"" means to round $x$ to an integer according to some definite rule if $x$ is not integral.  These indexes are taken modulo $N$ if necessary, circling back to the beginning if the sequence goes beyond $N$.</p>

<p>To see more clearly what can happen, let's consider a small population of, say, $N=7$ from which we wish to obtain a sample of size $n=3.$ </p>

<ol>
<li><p>Using $k = N/n = 7/3$, the possible starting indexes are $r=1$ and $r=2$.  In the first case the selected indexes would be $1, [1+7/3], [1+2*7/3]$ = $1, 3, 6$ and in the second case the indexes would be $2, [2+7/3], [2+2*7/3]$ = $2, 4, 7$.  We can picture these two samples--both equally likely to be chosen--by drawing marks below the selected indexes thus:</p>

<pre><code>Sample   1 2 3 4 5 6 7
   r=1   *   *     *
   r=2     *   *     *
</code></pre></li>
<li><p>Using $k = \lceil 7/3 \rceil = 3$ gives three possible samples, each with a chance of $1/3$ of being chosen:</p>

<pre><code>Sample   1 2 3 4 5 6 7 
   r=1   *     *     *
   r=2   * *     *   
   r=3     * *     *
</code></pre>

<p>Notice how in the second and third samples indexes of $8$ and $9$ were calculated and wrapped around modulo $7$ to the beginning, turning into $1$ and $2$, respectively.</p></li>
<li><p>Using $k = \lfloor 7/3 \rfloor = 2$ gives two possible samples, each with a chance of $1/2$ of being chosen:</p>

<pre><code>Sample   1 2 3 4 5 6 7 
   r=1   *   *   *
   r=2     *   *   *
</code></pre></li>
</ol>

<p><strong>The problems pointed out in the question are apparent:</strong></p>

<ul>
<li><p>In case 1, index $5$ is in neither possible sample and therefore is never selected.</p></li>
<li><p>In case 2, indexes $1$ and $2$ each have a $2/3$ chance of being in the sample (because they each appear in two out of the three equally likely samples) while the other indexes have only a $1/3$ chance of being in the sample.  Thus this sampling method is biased towards the first two subjects in the list.</p></li>
<li><p>In case 3, index $7$ is never selected.</p></li>
</ul>

<p>In general, for larger $N$ a certain fraction of the population will either never be selected (as in cases 1 and 3) or the <em>initial</em> subjects in the list will have relatively high chances of being selected (case 2).  In case 3, the <em>final</em> subjects in the list will never be selected, while in case 1 the non-selected subjects are situated fairly evenly throughout the list.</p>

<p>Normally, systematic sampling is used because there is a natural ordering of the subjects that is related to how they can be sampled: their indexes correspond to their time of enrollment in a human trial or are positions of possible samples in a physical medium or process (like soils in a streambed or products coming off an assembly line).  Therefore the problems in cases 1 and 2 are <em>serious</em> ones, because they systematically omit or over-weight one end of the population, whose properties easily could differ from those of the rest of the population.  <strong>Any study using these methods would thereby be seriously flawed.</strong></p>

<p><strong>There are two solutions.</strong>  The simplest one is straightforward and is the <em>correct</em> way to conduct such sampling: choose $r$ uniformly at random from the <em>entire</em> list of indexes, not just the first $k$ of them.  For instance, here are all the equally-likely samples that can be obtained using the first (round-as-you-go) method.  The first two already appeared in (1) above:</p>

<pre><code>Sample   1 2 3 4 5 6 7
   r=1   *   *     *
   r=2     *   *     *
   r=3   *   *   *  
   r=4     *   *   *
   r=5       *   *   *
   r=6   *     *   *
   r=7     *     *   *
</code></pre>

<p>By looking down the columns it is easy to see that each index has $3/7$ chance of appearing in the sample: there is no longer any bias.</p>

<p>It is worth reflecting on how such a subtle change--choosing the initial index randomly from $1, \ldots, N$ rather than $1, \ldots, k$--can have a profound effect on the quality of the study.</p>

<p><strong>The other solution</strong> is to compensate for the sampling bias using the <a href=""http://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator"" rel=""noreferrer"">Horvitz-Thompson estimator</a>.  This adjusts for varying probabilities of inclusion in the sample.  In method (1) it would effectively recognize that the samples are only from the subpopulation indexed by $1,2,3,4,6,7$ (skipping $5$, which is never included). This wouldn't really fix much, but it would highlight the shortcomings of the results and perhaps prevent invalid inferences.</p>

<p>In method (2) the H-T estimator would give any observations from indexes $1$ and $2$ a weight of just one-half the weights of the other indexes.  For example, if the sample happened to be $r=3$ then subjects $2, 3,$ and $6$ would be drawn.  Let the values of some quantity observed for these subjects be written $x_2, x_3,$ and $x_6,$ respectively.  Because the chance of including $2$ in the sample <em>a priori</em> was $2/3$ and the chances of including $3$ or $6$ were only $1/3,$ the Horvitz-Thompson estimator of the population mean would be </p>

<p>$$\frac{\frac{1}{2/3} x_2 + \frac{1}{1/3} x_3 + \frac{1}{1/3} x_6}{\frac{1}{2/3}  + \frac{1}{1/3}  + \frac{1}{1/3}} =\frac{\frac{1}{2} x_2 + x_3 + x_6}{\frac{1}{2} + 1 + 1} = \frac{1}{5}x_2 + \frac{2}{5} x_3 + \frac{2}{5} x_6.$$</p>

<p>There are comparable formulas for the sampling variance of this estimate (used to construct confidence intervals and test hypotheses, for instance).</p>

<p>The Horvitz-Thompson estimator comes to the fore when it is impossible to avoid over- and under-weighting some subjects in a sample.  It can also be applied to rescue a study that was otherwise ruined using the procedures in the reference: after the fact, if we can compute the actual probability of each subject's inclusion in the sample, then we can apply these probabilities to obtain valid estimates and valid hypothesis tests.</p>
",2014-02-10 17:01:10.777
67041,17740.0,2,,58790.0,,,,CC BY-SA 3.0,"<p>The hinge loss term $\sum_i\max(0,1-y_i(\mathbf{w}^\intercal \mathbf{x}_i+b))$ in soft margin SVM penalizes <em>misclassifications</em>. In hard margin SVM there are, by definition, no misclassifications.</p>

<p>This indeed means that hard margin SVM tries to minimize $\|\mathbf{w}\|^2$. Due to the formulation of the SVM problem, the margin is $2/\|\mathbf{w}\|$. As such, minimizing the norm of $\mathbf{w}$ is geometrically equivalent to maximizing the margin. Exactly what we want!</p>

<p><a href=""http://en.wikipedia.org/wiki/Regularization_%28mathematics%29"">Regularization</a> is a technique to avoid overfitting by penalizing large coefficients in the solution vector. In hard margin SVM $\|\mathbf{w}\|^2$ is both the loss function <em>and</em> an $L_2$ regularizer.</p>

<p>In soft-margin SVM, the hinge loss term <em>also</em> acts like a regularizer but on the slack variables instead of $\mathbf{w}$ and in $L_1$ rather than $L_2$. $L_1$ regularization induces sparsity, which is why standard SVM is sparse in terms of support vectors (in contrast to least-squares SVM).</p>
",2014-02-19 15:21:47.693
67660,28059.0,2,,49906.0,,,,CC BY-SA 3.0,"<p>Sorry, it's an old issue, I came across this by chance. </p>

<p>There is a mistake in your code for the mcnemar test. Try with:</p>

<pre><code>n &lt;- 100
do.one &lt;- function(n) {
  id &lt;- rep(1:n, each=2)
  case &lt;- rep(0:1, times=n)
  rs &lt;- rbinom(n*2, 1, 0.5)
  c(
    'pclogit' = coef(summary(clogit(case ~ rs + strata(id))))[5],
    'pmctest' = mcnemar.test(table(rs[case == 0], rs[case == 1]))$p.value
  )
}

out &lt;- replicate(1000, do.one(n))
</code></pre>

<p><img src=""https://i.stack.imgur.com/fvyUc.png"" alt=""enter image description here""></p>
",2014-02-26 16:50:15.223
67832,17740.0,2,,57764.0,,,,CC BY-SA 3.0,"<p>I disagree with the explanation given in the other answer. SVM, works well for large dimensional problems with relatively few instances because it is well regularized. In this case, I suspect the problem is not the tool but rather how it is being used.</p>

<blockquote>
  <p>For SVM I use a dot kernel and other parameters are all in their defaults ...</p>
</blockquote>

<p>This is why your SVM results are bad. If you do not tune the SVM parameters (probably $c$ in your case), the resulting classifier will likely be poor unless you happen to get lucky with the default value. </p>

<blockquote>
  <p>when I change the parameters and try again, I get same result; Naive Bayes still outperforms SVM.</p>
</blockquote>

<p>How do you change the parameters? What search method do you use? Do you just pick a value and hope for the best? Parameter search is important and must be done properly. Typically, optimal parameters are found through cross-validation.</p>

<p>Note that Naive Bayes may well be better for your particular application. Just because SVM is known to work well on this type of problems, does not mean that it <em>always</em> does.</p>
",2014-02-28 10:02:29.313
68114,23984.0,2,,58059.0,,,,CC BY-SA 3.0,"<p>A simple Matlab code using adaBoost+SVM, probably you can start from here...</p>

<pre><code>    N = length(X); % X training labels
    W = 1/N * ones(N,1); %Weights initialization
    M = 10; % Number of boosting iterations 

    for m=1:M
        C = 10; %The cost parameters of the linear SVM, you can...
                 perform a grid search for the optimal value as well           

        %Calculate the error and alpha in adaBoost with cross validation
        cmd = ['-c ', num2str(C), ' -w ', num2str(W)];
        model = svmtrain(X, Y, cmd);
        [Xout, acc, ~] = svmpredict(X,Y,cmd);

        err = sum(.5 * W .* acc * N)/sum(W);
        alpha = log( (1-err)/err );

        % update the weight
        W = W.*exp( - alpha.*Xout.*X );
        W = W/norm(W);

    end
</code></pre>

<p>In <a href=""http://www.csi.uottawa.ca/~nat/Papers/29-Wang.pdf"">Wang's et al's Boosting Support Vector Machines for Imbalanced Data Sets</a>, they applied a slightly different formula in the weight update, aiming at dealing with class imbalance.</p>

<p><a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances"">Weights for data instances instruction</a> is also shown in Prof. Lin's website.</p>

<p>Hope it helps.</p>
",2014-03-03 17:53:00.610
71920,19377.0,2,,57652.0,,,,CC BY-SA 3.0,"<p>The tail dependence coefficients $\lambda_U$ and $\lambda_L$ are measures of extremal dependence that quantify the dependence in the upper and lower tails of a bivariate distribution with continuous margins $F$ and $G$.</p>

<p>The coefficients $\lambda_U$ and $\lambda_L$ are defined in terms of quantile exceedences.
For the upper tail dependence coefficient $\lambda_U$ one looks at the probability that $Y$ exceeds the $u$-quantile $G^{-1}(u)$, given that $X$ exceeds the $u$-quantile $F^{-1}(u)$, and then consider the limit as $u$ goes to $1$, provided it exists.
Large values of $\lambda_U$ imply that joint extremes are more likely than for low values of $\lambda_U$. Interpretation for the lower tail dependence coefficient $\lambda_L$ is analogous.</p>

<p>If $\lambda_U = 0$, then $X$ and $Y$ are said to be <em>asymptotically independent</em>; if $\lambda_U \in (0, 1]$ they are <em>asymptotically dependent</em>.
For independent variables $\lambda_U = 0$; for perfectly dependent variables $\lambda_U = 0$. Note that $\lambda_U = 0$ does NOT imply independence; the comment/statement in the question's reference is wrong. Indeed, consider a bivariate normal distribution with correlation $\rho \notin \{0, 1\}$. Then, one can show that $\lambda_U = 0$ but the variables are dependent.</p>

<p>A distribution with, say, $\lambda_U = 0.5$ does not mean that there is a linear dependence between $X$ and $Y$; it means that $X$ and $Y$ are asymptotically dependent in the upper tail, and the strength of the dependence is 0.5. For example, the <a href=""http://en.wikipedia.org/wiki/Copula_%28probability_theory%29#Archimedean_copulas"" rel=""noreferrer"">Gumbel copula</a> with parameter $\theta = \log(2)/\log(1.5)$ has 
a coefficient of upper tail dependence equal to $0.5$.</p>

<p>The following figure shows a sample of 1000 points of the Gumbel copula with parameter $\theta  = \log(2)/\log(1.5)$ with uniform margins (left) and standard normal margins (right). The data were generated with the copula package in R (code is provided below).</p>

<p><img src=""https://i.stack.imgur.com/96UZ7.png"" alt=""enter image description here""></p>

<pre><code>## Parameters
theta &lt;- log(2)/log(1.5)
n     &lt;- 1000

## Generate a sample
library(copula)
set.seed(234)
gumbel.cop &lt;- archmCopula(""gumbel"", theta)
x &lt;- rCopula(n, gumbel.cop)

## Visualization
par(mfrow = c(1, 2))
plot(x, pch = 16, col = ""gray"", xlab = ""U"", ylab = ""V"")
plot(qnorm(x), pch = 16, col = ""gray"", xlab = ""X"", ylab = ""Y"")
par(mfrow = c(1, 1))
</code></pre>
",2014-04-14 20:00:46.893
79542,24527.0,2,,57851.0,,,,CC BY-SA 3.0,"<p>I would have to say that it would be extremely difficult for you to estimate the relative quantities of coins in circulation through any but an exhaustive (collecting a large portion of those coins simultaneously) survey.  </p>

<p>The reason is because most businesses (I believe) hold a reasonably large portion of coins in stock and will only distribute the coins which most efficiently lead to correct change.  Thus even if you go into the same store 100 times and collect change each time unless you have exhausted the stock of available coins, the coins that you receive in exchange for your sampling will only be those which correspond only with the least change required to fulfill your needs.</p>

<p>Assuming you draw change requirements uniformly between 1 cent and 499 cents this ratio is:</p>

<pre><code>       200        100         25         10          5          1 
0.13559322 0.06779661 0.25423729 0.13559322 0.06779661 0.33898305
</code></pre>

<p>If the store has no shortage of coins then your sampling procedure will automatically return the above ratios which have no correlation between the specific samples and the greater population of coins in circulation.  To see how I came up with these numbers see my <a href=""http://www.econometricsbysimulation.com/2014/07/estimating-required-change.html"" rel=""nofollow"">blog post</a> on the topic.</p>

<p>But this does not account for the oddities of prices which tend to cluster ending in .09 as in .99, .49, or .39 (in the US at least) which will definitely contribute to higher ratio of pennies required for many purchases than in the uniform draw of change.  Purchase requirements would need be specified so as to not cause further contamination of the data.  Overall, I think it is clear that this is a pretty problematic study design.</p>

<p>If you were forced to do something like this then you might be alt to 1. record change totals for each purchase, 2. calculating efficient coinage selection via the method I propose on my blog for each purchase, 3. record coins actually returned, 4. estimate the different between the optimal returned coin quantities and that actually returned to estimate to what degree coin stocks might be diverging from the optimal quantities.  From there I am not sure what to do with it in order to estimate total coins available in the currency.</p>

<p>Good luck and thanks for the interesting question!</p>
",2014-07-05 22:35:10.283
85353,37033.0,2,,57665.0,,,,CC BY-SA 4.0,"<p>I think you pretty much nailed it in your Edit. Generative model makes more restrictive assumption about the distribution of <span class=""math-container"">$x$</span>.</p>
<p>From <a href=""https://www.microsoft.com/en-us/research/wp-content/uploads/2005/01/qiszummerminka-aistats05.pdf"" rel=""nofollow noreferrer"">Minka</a></p>
<p>&quot;Unlike traditional generative random fields, CRFs only
model the conditional distribution <span class=""math-container"">$p(t|x)$</span> and do not explicitly model the marginal <span class=""math-container"">$p(x)$</span>. Note that the labels <span class=""math-container"">${ti }$</span> are
globally conditioned on the whole observation <span class=""math-container"">$x$</span> in CRFs.
Thus, we do not assume that the observed data <span class=""math-container"">$x$</span> are conditionally independent as in a generative random field.&quot;</p>
",2014-09-08 15:28:58.277
85707,36513.0,2,,55361.0,,,,CC BY-SA 3.0,"<p>I'm first outlining an approach for two companies in detail, the extension to even more companies then should be intuitive (at least for the likelihood, the prior could be more tricky).</p>

<p>Imagine there are two companies <em>A</em> and <em>B</em>, where <em>A</em> has $N_A$ locomotives and <em>B</em> has $N_B$ locomotives.  We assume $N_A \ge N_B$ (you can always switch <em>A</em> and <em>B</em> to make this hold).  The total number for that hypothesis of locomotives is $N_{tot} = N_A + N_B$.</p>

<p>Imagine you see a locomotive with the number $n$.  There are three cases for the likelihood:</p>

<ol>
<li>$N_A &lt; n$: This can't happen, so the likelihood is zero.</li>
<li>$N_B &lt; n \le N_A$: This locomotive must come from company <em>A</em>, so there is only one locomotive with this number.  Thus the likelihood is $1/N_{tot}$</li>
<li>$n \le N_B$: This locomotive can be either from <em>A</em> or from <em>B</em>, so there are two locomotives with this number.  The likelihood to see one of them is $2/N_{tot}$.</li>
</ol>

<p>As a quick sanity check: The likelihood to see any number at all is $$\sum_{i=1}^\infty L(i) = \sum_{i=1}^{N_B} \frac{2}{N_{tot}} + \sum_{i=N_B+1}^{N_A} \frac{1}{N_{tot}} \\ = \frac{2\cdot N_B}{N_{tot}} + \frac{N_A-N_B}{N_{tot}} = \frac{N_A+N_B}{N_{tot}} = 1$$.</p>

<hr>

<p>Generally, there will be (number of companies + 1) cases, one for each interval $N_i &lt; n \le N_{i+1}$.  Luckily, we can look at the problem from a different angle and see that what we need for the likelihood are actually just two numbers: $N_{tot}$, the total number of locomotives; and $N_n$, the number of locomotives that have the number $n$.  How likely are we to see one of the $N_n$ locomotive, out of $N_{tot}$ locomotives?  This will happen in $\frac{N_n}{N_{tot}}$ of all cases, so this fraction is the likelihood.
In Python, you can calculate this with two sum generators (and you don't even have to order the companies by size).  If <code>Ns</code> contains a list (or tuple) of company sizes according to your hypothesis, then this will give the likelihood for seeing a locomotive with number <code>n</code>:</p>

<pre><code>total_number_of_locomotives = sum(N for N in Ns)
number_of_locomotives_with_that_number = sum(1 for N in Ns if n&lt;=N)
likelihood = (number_of_locomotives_with_that_number / total_number_of_locomotives)
</code></pre>

<p>Note that the trivial case with one company is also handled by this code (the first sum just will be $N$, the second sum will be 0 or 1, depending on whether $n\le N$).</p>

<hr>

<p>For the priors, <a href=""http://en.wikipedia.org/wiki/Zipf%27s_law"">Zipf's law</a> could be a good starting point for a realistic distribution of company sizes.</p>
",2014-09-12 09:27:13.160
85831,37646.0,2,,57779.0,,,,CC BY-SA 3.0,"<p>This is the simplest proof I've been able to find.  </p>

<p>Just by rearranging factorials, we can rewrite the hypergeometric probability function as<br>
$$ \mathrm{Prob}(X=x) = \frac{1}{x!} \cdot \dfrac{M^{(x)} \, K^{(x)}}{N^{(x)}} \cdot \dfrac{(N-K)^{(M-x)}}{(N-x)^{(M-x)}}, $$
where $a^{(b)}$ is the falling power $a(a-1)\cdots(a-b+1)$.
Since $x$ is fixed, 
\begin{align*}
\dfrac{M^{(x)} \, K^{(x)}}{N^{(x)}} 
&amp;= \prod_{j=0}^{x-1} \dfrac{(M-j) \cdot (K-j)}{(N-j)}  \\
&amp;= \prod_{j=0}^{x-1} \left( \dfrac{MK}{n} \right) \cdot \dfrac{(1-j/M) \cdot (1-j/K)}{(1-j/N)} \\
&amp;= \left( \dfrac{MK}{N} \right) ^x \; \prod_{j=0}^{x-1} \dfrac{(1-j/M) \cdot (1-j/K)}{(1-j/N)},
\end{align*}
which $\to \lambda^x$ as $N$, $K$ and $M$ $\to \infty$ with $\frac{MK}{N} = \lambda$.</p>

<p>Lets replace $N-x$, $K-x$ and $M-x$ by new variables $n$, $k$ and $m$ for simplicity.  Since $x$ is fixed, as $N,K,M \to \infty$ with $KM/N \to \lambda$, so too $n,k,m \to \infty$ with $nk/m \to \lambda$.  Next we write 
$$ A = \dfrac{(N-K)^{(M-x)}}{(N-x)^{(M-x)}} = \dfrac{(n-k)^{(m)} }{(n)^{(m)}} = \prod_{j=0}^{m-1} \left( \dfrac{n-j-k}{n-j} \right)= \prod_{j=0}^{m-1} \left( 1 - \dfrac{k}{n-j} \right)$$ 
and take logs:
$$ \ln \, A = \sum_{j=0}^{m-1} \ln  \left( 1 - \dfrac{k}{n-j} \right).  $$ 
Since the bracketed quantity is an increasing function of $j$ we have
$$ \sum_{j=0}^{m-1} \ln  \left( 1 - \dfrac{k}{n} \right) \le \ln \, A \le \sum_{j=0}^{m-1} \ln  \left( 1 - \dfrac{k}{n-m+1} \right), $$
or
$$ m \, \ln  \left( 1 - \dfrac{k}{n} \right) \le \ln \, A \le m \, \ln  \left( 1 - \dfrac{k}{n-m+1} \right). $$
But $\ln (1-x) &lt; -x$ for $0 &lt; x &lt; 1$, so 
$$ m \, \ln  \left( 1 - \dfrac{k}{n} \right) \le \ln \, A &lt; -m \, \left( \dfrac{k}{n-m+1} \right), $$
and dividing through by $km/n$ gives
$$ \frac{n}{k} \, \ln  \left( 1 - \dfrac{k}{n} \right) \le \dfrac{\ln \, A}{km/n} &lt; - \, \left( \dfrac{n}{n-m+1} \right) = - \, \left( \dfrac{1}{1-m/n+1/n} \right). $$
Finally, we let $k$, $m$ and $n$ tend to infinity in such a way that $km/n \to \lambda$.  Since both $k/n \to 0$ and $m/n \to 0$, both the left and right bounds $\to -1$.  (The left bound follows from $\lim_{n \to \infty} (1-1/n)^n = e^{-1}$, which is a famous limit in calculus.)  So by the Squeeze Theorem we have $\ln \, A \to -\lambda$, and thus $A \to e^{-\lambda}$.</p>
",2014-09-14 08:16:03.980
92891,10986.0,2,,57939.0,,,,CC BY-SA 3.0,"<p>I also just started to look at this question. </p>

<p>As mentioned before, when we use the normal distribution to calculate p-values for each test, then these p-values do not take multiple testing into account. To correct for it and control the family-wise error rate, we need some adjustments. Bonferonni, i.e. dividing the significance level or multiplying the raw p-values by the number of tests, is only one possible correction. There are a large number of other multiple testing p-value corrections that are in many cases less conservative.</p>

<p>These p-value corrections do not take the specific structure of the hypothesis tests into account.</p>

<p>I am more familiar with the pairwise comparison of the original data instead of the rank transformed data as in Kruskal-Wallis or Friedman tests. In that case, which is the Tukey HSD test, the test statistic for the multiple comparison is distributed according to the studentized range distribution, which is the distribution for all pairwise comparisons under the assumption of independent samples. It is based on probabilities of multivariate normal distribution which could be calculated by numerical integration but are usually used from tables.</p>

<p>My guess, since I don't know the theory, is that the studentized range distribution can be applied to the case of rank tests in a similar way as in the Tukey HSD pairwise comparisons.</p>

<p>So, using (2) normal distribution plus multiple testing p-value corrections and using (1) studentized range distributions are two different ways of getting an approximate distribution of the test statistics. However, if the assumptions for the use of the studentized range distribution are satisfied, then it should provide a better approximation since it is designed for the specific problem of all pairwise comparisons.</p>
",2014-12-03 04:05:14.547
94362,5179.0,2,,57676.0,,,,CC BY-SA 3.0,"<p>The ""two uniforms"" are not absolutely necessary when generating from a mixture, but they make the simulation easy to understand. The mixture of normal distributions,
$$rf_a(x)+(1-r)f_b(x)$$
has a probability mass of $r$ associated with the first normal and $(1-r)$ with the second normal. This means that the distribution of $X\sim f$ can be decomposed as
$$\mathbb{P}(X\in\mathcal{A})=r\mathbb{P}(X_a\in\mathcal{A})+(1-r)\mathbb{P}(X_b\in\mathcal{A})$$ 
for any measurable set $\mathcal{A}$, where $X_a$ and $X_b$ are normal random variables with means $a$ and $b$ respectively. This can be reinterpreted as
$$X=\begin{cases} X_a &amp;\text{with probability $r$}\\
X_b &amp;\text{with probability $1-r$}\end{cases}$$
meaning that to generate from the mixture, one can follow the steps</p>

<ol>
<li>Pick between components $a$ and $b$ by generating a uniform $U\sim\mathcal{U}(0,1)$ and, if $U&lt;r$ take $\mu=a$ and else take $\mu=b$;</li>
<li>Generate $X$ as $X_a$ or $X_b$ depending on the first step result, by generating a uniform $V\sim\mathcal{U}(0,1)$ and take $X=\Phi^{-1}(V)+\mu$</li>
</ol>

<p>This explains for the use of two uniforms.</p>
",2014-12-16 17:53:22.597
99688,45797.0,2,,57217.0,,,,CC BY-SA 3.0,"<p>When you say ""better results"" and provide the different errors you got, is that evaluated in-sample or out-of-sample?</p>

<p>SVM tuning will <em>attempt</em> to give you the best tuning parameters for out-of-sample prediction.  But for in-sample ""performance,"" higher gamma and higher cost will always do ""better.""  I put 'performance' and 'better' in quotes here, b/c higher values of gamma and cost will not actually give you a better estimate of the underlying mapping between independent and outcome variables, but will ""overfit"" the training data.  Keep in mind, SVM is capable of arbitrary complexity, so if you let it, it will perfectly fit your data.  (That's assuming there are no instances of identical X's producing different Y's, but that almost surely won't happen with continuous data.)  When you fit so tightly, if actually harms predictive ability, b/c there is a lot of spurious complexity in the prediction function, so my guess is that gamma=0.02 will outperform out-of-sample, while gamma=0.042 will dominate in-sample. If you're interested in prediction, go with 0.02. There aren't many reasons to go with in-sample performance, but there are instances when you want to be a little liberal with the cost and gamma parameters.  (For example, if you're using SVM-regression to produce a residual which has purged variation related to all of the explanatory variables, a little over-fitting can be beneficial.)  But in most cases, go with the out-of-sample prediction accuracy. </p>

<p>However, if you evaluated this out-of-sample and found that the higher gamma did better, then I must say that I'm surprised.  It's theoretically possible for the performance manifold (the 3-d surface of out-of-sample MSE, cost and gamma) to be very spike-y, which can lead to poor tuning outcomes, but looking at the plot above, it looks pretty well-behaved.</p>

<p>Hope that helps!</p>
",2015-02-14 15:31:04.593
99851,35937.0,2,,58043.0,,,,CC BY-SA 3.0,"<p>I know it's a delayed response but this could help anyone who is trying to do this:</p>

<p><strong>Follow these steps:</strong></p>

<p>Stat -> DOE -> Factorial -> Create Factorial design</p>

<p>Select ""General Full factorial design"" and select 2 in ""Number of Factors"" </p>

<p><img src=""https://i.stack.imgur.com/NZ5yg.png"" alt=""enter image description here""></p>

<p>then Click on ""Design tab"", Enter the factor names (Environment &amp; Frequency) in the name boxes and enter ""2"" as factor as you want to have 2 levels ""H2O"" and ""Salt H2O""</p>

<p><img src=""https://i.stack.imgur.com/KyqpX.png"" alt=""enter image description here""></p>

<p>Then Click ""Factors"" tab and change the type as ""Text"" from ""Numeric"" using drop down menu. </p>

<p>Then enter the level values for two different levels as shown in the picture.</p>

<p><img src=""https://i.stack.imgur.com/shvGC.png"" alt=""enter image description here""></p>

<p>And here is the output</p>

<p><img src=""https://i.stack.imgur.com/9EFSn.png"" alt=""enter image description here""></p>

<p>You can enter the response for each combination in C7 column  and proceed with further analysis. Hope this helps !</p>
",2015-02-16 15:03:54.463
100174,46110.0,2,,57567.0,,,,CC BY-SA 3.0,"<p>Using the F-statistic formula from Hayashi:</p>

<p>$F=\frac{\left(Rb-r\right)^T\left[R\left(X^T X\right)^{-1}R^T\right]^{-1}\left(Rb-r\right)/ \rho}{s^2}$</p>

<p>(notation: we're testing the hypothesis $R\beta = r$, $R\in\mathbb{R}^{\rho\times K}$, rank($R$)=$\rho$, $s^2$ is the variance of the error term)</p>

<p>We can immediately see why this won't apply to the heteroskedastic case--in particular, we definitely don't expect $s^2$ to be a constant.</p>

<p>Intuitively, we know there's a (nontrivial) relationship between $t$ and $F$ statistics--so given that $t$ statistics are heteroskedasticity-dependent, we should expect $F$-statistics to be so as well. </p>

<p>See <a href=""http://www3.grips.ac.jp/~yamanota/Lecture_Note_9_Heteroskedasticity.pdf"" rel=""nofollow"" title=""here"">here</a> (pp 3-4) for why the Wald test you suggested is correct &amp; hence the <code>waldtest</code> function in R is appropriate.</p>
",2015-02-19 23:03:56.703
101645,20410.0,2,,2509.0,,,,CC BY-SA 4.0,"<p><em>Imagine a big family dinner where everybody starts asking you about PCA. First, you explain it to your great-grandmother; then to your grandmother; then to your mother; then to your spouse; finally, to your daughter (a mathematician). Each time the next person is less of a layman. Here is how the conversation might go.</em></p>
<p><strong>Great-grandmother: I heard you are studying &quot;Pee-See-Ay&quot;. I wonder what that is...</strong></p>
<p><strong>You:</strong> Ah, it's just a method of summarizing some data. Look, we have some wine bottles standing here on the table. We can describe each wine by its colour, how strong it is, how old it is, and so on.
<a href=""https://i.stack.imgur.com/zfB1L.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zfB1L.jpg"" alt=""Color of Wine Chart"" /></a>
<em>Visualization originally found</em> <a href=""http://winefolly.com/tutorial/wine-color-chart/"" rel=""noreferrer"">here</a>.</p>
<p>We can compose a whole list of different characteristics of each wine in our cellar. But many of them will measure related properties and so will be redundant. If so, we should be able to summarize each wine with fewer characteristics! This is what PCA does.</p>
<p><strong>Grandmother: This is interesting! So this PCA thing checks what characteristics are redundant and discards them?</strong></p>
<p><strong>You:</strong> Excellent question, granny! No, PCA is not selecting some characteristics and discarding the others. Instead, it constructs some <em>new</em> characteristics that turn out to summarize our list of wines well. Of course, these new characteristics are constructed using the old ones; for example, a new characteristic might be computed as wine age minus wine acidity level or some other combination (we call them <em>linear combinations</em>).</p>
<p>In fact, PCA finds the best possible characteristics, the ones that summarize the list of wines as well as only possible (among all conceivable linear combinations). This is why it is so useful.</p>
<p><strong>Mother: Hmmm, this certainly sounds good, but I am not sure I understand. What do you actually mean when you say that these new PCA characteristics &quot;summarize&quot; the list of wines?</strong></p>
<p><strong>You:</strong> I guess I can give two different answers to this question. The first answer is that you are looking for some wine properties (characteristics) that strongly differ across wines. Indeed, imagine that you come up with a property that is the same for most of the wines - like the stillness of wine after being poured. This would not be very useful, would it? Wines are very different, but your new property makes them all look the same! This would certainly be a bad summary. Instead, PCA looks for properties that show as much variation across wines as possible.</p>
<p>The second answer is that you look for the properties that would allow you to predict, or &quot;reconstruct&quot;, the original wine characteristics. Again, imagine that you come up with a property that has no relation to the original characteristics - like the shape of a wine bottle; if you use only this new property, there is no way you could reconstruct the original ones! This, again, would be a bad summary. So PCA looks for properties that allow reconstructing the original characteristics as well as possible.</p>
<p>Surprisingly, it turns out that these two aims are equivalent and so PCA can kill two birds with one stone.</p>
<p><strong>Spouse: But darling, these two &quot;goals&quot; of PCA sound so different! Why would they be equivalent?</strong></p>
<p><strong>You:</strong> Hmmm. Perhaps I should make a little drawing <em>(takes a napkin and starts scribbling)</em>. Let us pick two wine characteristics, perhaps wine darkness and alcohol content -- I don't know if they are correlated, but let's imagine that they are. Here is what a scatter plot of different wines could look like:</p>
<p><img src=""https://i.stack.imgur.com/jPw90.png"" alt=""PCA exemplary data"" /></p>
<p>Each dot in this &quot;wine cloud&quot; shows one particular wine. You see that the two properties (<span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span> on this figure) are correlated. A new property can be constructed by drawing a line through the centre of this wine cloud and projecting all points onto this line. This new property will be given by a linear combination <span class=""math-container"">$w_1 x + w_2 y$</span>, where each line corresponds to some particular values of <span class=""math-container"">$w_1$</span> and <span class=""math-container"">$w_2$</span>.</p>
<p>Now, look here very carefully -- here is what these projections look like for different lines (red dots are projections of the blue dots):</p>
<p><img src=""https://i.stack.imgur.com/Q7HIP.gif"" alt=""PCA animation: variance and reconstruction error"" /></p>
<p>As I said before, PCA will find the &quot;best&quot; line according to two different criteria of what is the &quot;best&quot;. First, the variation of values along this line should be maximal. Pay attention to how the &quot;spread&quot; (we call it &quot;variance&quot;) of the red dots changes while the line rotates; can you see when it reaches maximum? Second, if we reconstruct the original two characteristics (position of a blue dot) from the new one (position of a red dot), the reconstruction error will be given by the length of the connecting red line. Observe how the length of these red lines changes while the line rotates; can you see when the total length reaches minimum?</p>
<p>If you stare at this animation for some time, you will notice that &quot;the maximum variance&quot; and &quot;the minimum error&quot; are reached at the same time, namely when the line points to the magenta ticks I marked on both sides of the wine cloud. This line corresponds to the new wine property that will be constructed by PCA.</p>
<p>By the way, PCA stands for &quot;principal component analysis&quot;, and this new property is called &quot;first principal component&quot;. And instead of saying &quot;property&quot; or &quot;characteristic&quot;, we usually say &quot;feature&quot; or &quot;variable&quot;.</p>
<p><strong>Daughter: Very nice, papa! I think I can see why the two goals yield the same result: it is essentially because of the Pythagoras theorem, isn't it? Anyway, I heard that PCA is somehow related to eigenvectors and eigenvalues; where are they in this picture?</strong></p>
<p><strong>You:</strong> Brilliant observation. Mathematically, the spread of the red dots is measured as the average squared distance from the centre of the wine cloud to each red dot; as you know, it is called the <em>variance</em>. On the other hand, the total reconstruction error is measured as the average squared length of the corresponding red lines. But as the angle between red lines and the black line is always <span class=""math-container"">$90^\circ$</span>, the sum of these two quantities is equal to the average squared distance between the centre of the wine cloud and each blue dot; this is precisely Pythagoras theorem. Of course, this average distance does not depend on the orientation of the black line, so the higher the variance, the lower the error (because their sum is constant). This hand-wavy argument can be made precise (<a href=""https://stats.stackexchange.com/a/136072/28666"">see here</a>).</p>
<p>By the way, you can imagine that the black line is a solid rod, and each red line is a spring. The energy of the spring is proportional to its squared length (this is known in physics as Hooke's law), so the rod will orient itself such as to minimize the sum of these squared distances. I made a simulation of what it will look like in the presence of some viscous friction:</p>
<p><img src=""https://i.stack.imgur.com/lNHqt.gif"" alt=""PCA animation: pendulum"" /></p>
<p>Regarding eigenvectors and eigenvalues. You know what a <em>covariance matrix</em> is; in my example it is a <span class=""math-container"">$2\times 2$</span> matrix that is given by <span class=""math-container"">$$\begin{pmatrix}1.07 &amp;0.63\\0.63 &amp; 0.64\end{pmatrix}.$$</span> What this means is that the variance of the <span class=""math-container"">$x$</span> variable is <span class=""math-container"">$1.07$</span>, the variance of the <span class=""math-container"">$y$</span> variable is <span class=""math-container"">$0.64$</span>, and the covariance between them is <span class=""math-container"">$0.63$</span>. As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors (incidentally, this is called <em>spectral theorem</em>); corresponding eigenvalues will then be located on the diagonal. In this new coordinate system, the covariance matrix is diagonal and looks like that: <span class=""math-container"">$$\begin{pmatrix}1.52 &amp;0\\0 &amp; 0.19\end{pmatrix},$$</span> meaning that the correlation between points is now zero. It becomes clear that the variance of any projection will be given by a weighted average of the eigenvalues (I am only sketching the intuition here). Consequently, the maximum possible variance (<span class=""math-container"">$1.52$</span>) will be achieved if we simply take the projection on the first coordinate axis. It follows that the direction of the first principal component is given by the first eigenvector of the covariance matrix. (<a href=""https://stats.stackexchange.com/questions/217995"">More details here.</a>)</p>
<p>You can see this on the rotating figure as well: there is a gray line there orthogonal to the black one; together, they form a rotating coordinate frame. Try to notice when the blue dots become uncorrelated in this rotating frame. The answer, again, is that it happens precisely when the black line points at the magenta ticks. Now I can tell you how I found them (the magenta ticks): they mark the direction of the first eigenvector of the covariance matrix, which in this case is equal to <span class=""math-container"">$(0.81, 0.58)$</span>.</p>
<hr />
<p><em>Per popular request, I shared <a href=""https://gist.github.com/anonymous/7d888663c6ec679ea65428715b99bfdd"" rel=""noreferrer"">the Matlab code to produce the above animations</a>.</em></p>
",2015-03-06 00:30:06.837
115327,15723.0,2,,40030.0,,,,CC BY-SA 4.0,"<p>Stratification seeks to ensure that each fold is representative of all strata of the data. Generally this is done in a supervised way for classification and aims to ensure each class is (approximately) equally represented across each test fold (which are of course combined in a complementary way to form training folds).</p>
<p>The intuition behind this relates to the bias of most classification algorithms. They tend to weight each instance equally which means overrepresented classes get too much weight (e.g. optimizing F-measure, Accuracy or a complementary form of error).  Stratification is not so important for an algorithm that weights each class equally (e.g. optimizing Kappa, Informedness or ROC AUC) or according to a cost matrix (e.g. that is giving a value to each class correctly weighted and/or a cost to each way of misclassifying). See, e.g.
D. M. W. Powers (2014), What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes. <a href=""http://arxiv.org/pdf/1503.06410"" rel=""nofollow noreferrer"">http://arxiv.org/pdf/1503.06410</a></p>
<p>One specific issue that is important across even unbiased or balanced algorithms, is that they tend not to be able to learn or test a class that isn't represented at all in a fold, and furthermore even the case where only one of a class is represented in a fold doesn't allow generalization to performed resp. evaluated. However even this consideration isn't universal and for example doesn't apply so much to one-class learning, which tries to determine what is normal for an individual class, and effectively identifies outliers as being a different class, given that cross-validation is about determining statistics not generating a specific classifier.</p>
<p>On the other hand, supervised stratification compromises the technical purity of the evaluation as the labels of the test data shouldn't affect training, but in stratification are used in the selection of the training instances. Unsupervised stratification is also possible based on spreading similar data around looking only at the attributes of the data, not the true class. See, e.g.
<a href=""https://doi.org/10.1016/S0004-3702(99)00094-6"" rel=""nofollow noreferrer"">https://doi.org/10.1016/S0004-3702(99)00094-6</a>
N. A. Diamantidis, D. Karlis, E. A. Giakoumakis (1997),
Unsupervised stratification of cross-validation for accuracy estimation.</p>
<p>Stratification can also be applied to regression rather than classification, in which case like the unsupervised stratification, similarity rather than identity is used, but the supervised version uses the known true function value.</p>
<p>Further complications are rare classes and multilabel classification, where classifications are being done on multiple (independent) dimensions.  Here tuples of the true labels across all dimensions can be treated as classes for the purpose of cross-validation. However, not all combinations necessarily occur, and some combinations may be rare. Rare classes and rare combinations are a problem in that a class/combination that occurs at least once but less than K times (in K-CV) cannot be represented in all test folds. In such cases, one could instead consider a form of stratified boostrapping (sampling with replacement to generate a full size training fold with repetitions expected and 36.8% expected unselected for testing, with one instance of each class selected initially without replacement for the test fold).</p>
<p>Another approach to multilabel stratification is to try to stratify or bootstrap each class dimension separately without seeking to ensure representative selection of combinations. With L labels and N instances and Kkl instances of class k for label l, we can randomly choose (without replacement) from the corresponding set of labeled instances Dkl approximately N/LKkl instances. This does not ensure optimal balance but rather seeks balance heuristically. This can be improved by barring selection of labels at or over quota unless there is no choice (as some combinations do not occur or are rare).  Problems tend to mean either that there is too little data or that the dimensions are not independent.</p>
",2015-07-15 01:23:38.037
121372,58011.0,2,,57890.0,,,,CC BY-SA 3.0,"<p>As mentioned in a comment, the MAP estimate is the maximum likelihood estimate when you omit $g(\theta)$ or if it is a constant. If $g(\theta)$ is not a constant, then there are of course various methods for finding the MAP estimate. Omitting the survey sampling aspect (or assuming we have a completely representative sample from a population of infinite size or assuming you have included the sampling mechanism into your likelihood):</p>

<ol>
<li>Analytically (often by taking logs and finding the maximum).  </li>
<li>In some cases conjugate priors are available have known modes so that you do not need to do the analytic calculation yourself. E.g. in
the example you give we could use a Beta prior. You did not specify
how certain you were about your prior, but let's say that in a
previous survey you had 20 out of 50 for ""A"" and 30 out of 50 for ""B"" (and that there are no other options to vote for). If you are happy to use a Beta(20,30) prior, then your posterior is a Beta(20+60, 30+40) distribution.
The mode is then known to be (80-1)/(150-2)=0.53 This would not be
correct for a non-representative sample or one from a non-infinite
population and this option only exists for a few distributions. Additionally, just because a conjugate prior is available and convenient does not mean it is what you want to use (e.g. you may have wanted to express some doubt about the applicability of the previous survey to your new survey by using a mixture of a Beta(0.5,0.5) prior and a Beta(20,30) prior with weights of 0.2 and 0.8 to express this uncertainty. Then you can still do conjugate updating, but getting the updated posterior weights is a tiny bit harder.</li>
<li>Using some numeric minimization routine.</li>
</ol>

<p>In a simplistic situation where surveys really sample exactly how people will really vote (nothing else happens before the election to change the mind of people, there is no issues with voter turnout differing for parties etc.), you could then for a known total size of the number of voters predict the outcome of voting using the beta-binomial distribution (the predictive distribution of the binomial distribution with a beta prior). In reality predicting an election is of course much more difficult.</p>
",2015-09-13 09:06:45.893
217429,102539.0,2,,57626.0,,,,CC BY-SA 3.0,"<p><a href=""https://www.researchgate.net/publication/312165764_Panel_Vector_Autoregression_in_R_The_panelvar_Package"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/312165764_Panel_Vector_Autoregression_in_R_The_panelvar_Package</a></p>

<p>Here you will find the R-package and the link to the paper.</p>
",2018-01-17 12:42:00.307
227108,128628.0,2,,58826.0,,,,CC BY-SA 3.0,"<p>Some recommended standards for statistical notation are presented in <a href=""https://www.jstor.org/stable/2681417"" rel=""noreferrer"">Halperin, Hartley and Hoel (1965)</a> and <a href=""http://journals.sagepub.com/doi/abs/10.3102/0013189X001011015"" rel=""noreferrer"">Sanders and Pugh (1972)</a>.  Most of the current notation comes from conventions that were established by the biometric statisticians in the late 19th and early 20th century (most of it was done by Pearson and Fisher and their associates).  A useful list of early uses of notation is maintained by the economist John Aldrich <a href=""http://jeff560.tripod.com/stat.html"" rel=""noreferrer"">here</a>, and a historical account of the English biometric school is published in <a href=""https://www.jstor.org/stable/1403877"" rel=""noreferrer"">Aldrich (2003)</a>.  (If you have further enquiries about this topic, <a href=""http://www.economics.soton.ac.uk/staff/aldrich/aldrich.htm"" rel=""noreferrer"">Aldrich</a> is probably the world's foremost living expert in the history of notation in statistics.)</p>

<p>Aside from this explicit work, there are a lot of books that give introductions to the field, and these are careful to define notation consistent with common conventions, defining notation as they go.  There are many well-known conventions in this field that run consistently through the literature, and statisticians are well-acquainted with these through practice, even without having read the recommendations of these researchers.</p>

<p><strong>Ambiguity of the distribution-centric notation:</strong> The use of the ""distribution-centric"" notation is a standard convention that is used throughout statistical literature.  However, one interesting thing to point out about this notation is that there is a bit of wiggle-room as to what it actually means.  The standard convention is to read the object on the right-hand-side of these statements as some kind of description of a probability measure (e.g, a distribution function, density function, etc.) and then read the $\sim$ relation with meaning ""...has distribution..."" or ""...has probability measure..."", etc.  Under this interpretation the relation compares two distinct sets of things; the object on the left-hand-side is a random variable and the object on the right-hand-side is a description of a probability measure.</p>

<p>However, it is also equally valid to interpret the right-hand-side as a reference to a random variable (as opposed to a distribution) and read the $\sim$ relation as meaning ""...has the same distribution as..."".  Under this interpretation the relation is an <a href=""https://en.wikipedia.org/wiki/Equivalence_relation"" rel=""noreferrer"">equivalence relation</a> comparing random variables; the objects on the left- and right--hand-sides are both random variables and the relation is reflexive, symmetric and transitive.</p>

<p>This gives two possible (and equally valid) interpretations of a statement like:</p>

<p>$$X \sim \text{N}(\mu, \sigma^2).$$</p>

<ul>
<li><p><strong>Distributional interpretation:</strong> ""$X$ has probability distribution $\text{N}(\mu, \sigma^2)$"".  This interpretation takes the latter object to be some description of a normal probability measure (e.g., its density function, distribution function, etc.).</p></li>
<li><p><strong>Random variable interpretation:</strong> ""$X$ has the same probability distribution as $\text{N}(\mu, \sigma^2)$"".  This interpretation takes the latter object to be a normal random variable.</p></li>
</ul>

<p>Each interpretation has advantages and disadvantages.  The advantage of the random-variable interpretation is that it uses the standard symbol $\sim$ to refer to an <a href=""https://en.wikipedia.org/wiki/Equivalence_relation"" rel=""noreferrer"">equivalence relation</a>, but its disadvantage is that it requires reference to random variables with similar notation to their distribution functions.  The advantage of the distributional interpretation is that it uses similar notation for the distributions as a whole, and their functional forms with a given argument value; the disadvantage is that it uses the $\sim$ symbol in a way that is not an equivalence relation.</p>

<hr>

<p>Aldrich, J. (2003) <a href=""https://www.jstor.org/stable/1403877"" rel=""noreferrer"">The Language of the English Biometric School</a> <em>International Statistical Review</em> <strong>71(1)</strong>, pp. 109-131.</p>

<p>Halperin, M., Hartley, H.O. and Hoel, P.G. (1965) <a href=""https://www.jstor.org/stable/2681417"" rel=""noreferrer"">Recommended Standards for Statistical Symbols and Notation</a>. <em>The American Statistician</em> <strong>19(3)</strong>, pp. 12-14.</p>

<p>Sanders, J.R. and Pugh, R.C. (1972) <a href=""http://journals.sagepub.com/doi/abs/10.3102/0013189X001011015"" rel=""noreferrer"">Recommendation for a Standard Set of Statistical Symbols and Notations</a>. <em>Educational Researcher</em> <strong>1(11)</strong>, pp. 15-16.</p>
",2018-04-11 01:21:58.497
236322,116224.0,2,,58305.0,,,,CC BY-SA 4.0,"<p>Differential entropy should be considered as a measure of <em>relative</em> (privation of) information - not absolute. In particular, note that the differential entropy responds to a change in scale (i.e. you have a logarithm of a unitful quantity, which means that it will depend on the units you measure the axis <span class=""math-container"">$x$</span> in), which is not a concept that makes sense for a discrete information source. The inability to specify an absolute information in this context should be taken as based on the intuitive idea that the amount of information required to specify a specific value in an infinite continuum is itself infinite as you must distinguish one from amongst an infinitude of possibilities to achieve such a specification.</p>
<p>To understand it more precisely, consider a uniform distribution where the differential entropy is zero. A simple example is precisely that which is one unit wide:</p>
<p><span class=""math-container"">$$P(x) = \begin{cases}1,\ \mbox{if $x \in [0, 1]$}  \\ 0,\ \mbox{otherwise} \end{cases}$$</span></p>
<p>If you compute the differential entropy, <span class=""math-container"">$h$</span>, of the above distribution, you will find it is zero since <span class=""math-container"">$\ln(1) = 0$</span> and moreover in the appropriate limit <span class=""math-container"">$0 \ln(0)$</span> &quot;equals&quot; 0. This corresponds to the fact you (or the agent to which the Bayesian probability <span class=""math-container"">$P(x)$</span> is specified relative to) know the position (or whatever) of the object to within exactly one unit. If you make the distribution wider, say two units, so you have even less information, then the differential entropy will be <span class=""math-container"">$\ln(2)$</span> or about 0.693. This is the same as the discrete entropy for an infinite discrete set of bins, each standing in for &quot;a unit&quot;, or if you like, the bins between marks on a ruler and where you only report the measurement made with the ruler as a whole number of its finest ticks, now distributed uniformly among two such bins, and means we have 0.697 nats <em>less</em> information now about the position of the particle up to a resolution of one unit.</p>
<p>Negative differential entropy then just means we go the other way - since we <em>aren't</em> working with <em>discrete</em> bins, we can know it &quot;more precisely&quot; than one bin, i.e. to an accuracy less than one unit, and thus the entropy (<em>privation</em> of information) will have to be <em>less</em> now as we are more informed, thus now <em>less than zero</em>. But if I switch to a finer scale, i.e. a smaller unit, then the entropy will once again exceed zero, as now we don't have enough to know it down to that fine scale.</p>
<p>You cannot have an absolute measure because on a continuum, effectively you have an uncountable infinity of &quot;bins&quot; within any arbitrarily small interval, thus even a tiny interval of uncertainty is still effectively infinite information. Thus we have to get up &quot;high into infinity&quot; to measure the differences in entropy in realistic distributions and that is why the &quot;bottom&quot; of differential entropy is at <span class=""math-container"">$-\infty$</span>, which is like a probability zero in a continuous probability measure, and how that such does not necessarily represent impossibility, but rather negligibility with regard to the infinitude of the sets we are considering.</p>
<p>Or to go the other way intuitively, suppose you were treating the continuum as a set of bins - one for each point - like how you do an ordinary discrete random variable. Then with a probability distribution of <span class=""math-container"">$P(x) = \delta(x - a)$</span>, i.e. a delta function at some central real number <span class=""math-container"">$a$</span>, that is one bin occupied, so entropy 0, but if you now have 2 bins with probability 1/2, i.e. <span class=""math-container"">$P(x) = \frac{1}{2} \delta(x - a_1) + \frac{1}{2} \delta(x - a_2)$</span> meaning &quot;we know the particle is at either <span class=""math-container"">$a_1$</span> exactly or <span class=""math-container"">$a_2$</span> exactly but not which&quot;, then by the usual <em>discrete</em> entropy formula you have entropy <span class=""math-container"">$0.693$</span> nat (or <span class=""math-container"">$1$</span> shannon using <span class=""math-container"">$\lg$</span> instead of <span class=""math-container"">$\ln$</span>). But if you continue in this way, long before you reach a truly continuous distribution you will soon &quot;top out&quot; (after a countably infinite number of bins have been summed) using discrete entropy, saturating at positive infinity. And that is what I mean by saying that to then go up into continuous distributions, you then have to &quot;soar up high&quot; - effectively the integral jams in an uncountable infinity and rises your reference point far above the true baseline so you can distinguish the different infinite amounts that are &quot;above the infinite boundary of the discrete entropy&quot;. That rise then, by symmetry, puts the baseline infinitely far below you, or at <span class=""math-container"">$-\infty$</span>, and moreover due to the Archimedean nature of the real numbers, prevents you from distinguishing these finite-bin cases any more (all of them, if you check for yourself, have differential entropy <span class=""math-container"">$-\infty$</span>).</p>
",2018-07-02 05:22:06.023
327191,93032.0,2,,58082.0,,,,CC BY-SA 4.0,"<p>For any recent visitors, there's been new developments in this area by Hitz, Davis and Samorodnitsky (arXiv:1707.05033). Taking a peaks-over-threshold approach instead of block maxima, the Discrete Generalised Pareto Distribution is derived as the <span class=""math-container"">$\operatorname{floor}$</span> of a GPD, and discrete Maximum Domains of Attraction (DMDA) are introduced by relating them to the classical MDAs. The whole thing is linked to, but different from, <a href=""https://en.wikipedia.org/wiki/Zipf%27s_law"" rel=""nofollow noreferrer"">Zipf's Law</a>.</p>
<p>In terms of the paper's terminology, the Poisson distribution is in the DMDA of a Gumbel distribution <span class=""math-container"">$(\xi = 0)$</span>, as are the Negative Binomial and Geometric distributions.</p>
",2020-10-21 20:14:59.210
331473,9081.0,2,,58679.0,,,,CC BY-SA 4.0,"<p><strong>What is the difference between segmenting and clustering</strong>?</p>
<p>First, let us define the two terms:</p>
<ol>
<li><p><strong>Segmentation</strong> partitioning of some whole, some object, into parts vased on similarity and contiguity. See <a href=""https://en.wikipedia.org/wiki/Segment"" rel=""nofollow noreferrer"">Wikipedia</a> which gives as an example <em>Segmentation (biology), the division of body plans into a series of repetitive segments</em> and also <a href=""https://www.oxfordlearnersdictionaries.com/definition/english/segmentation"" rel=""nofollow noreferrer"">Oxford</a>.</p>
</li>
<li><p><strong>Clustering</strong> <a href=""https://en.wikipedia.org/wiki/Cluster_analysis"" rel=""nofollow noreferrer"">Wikipedia</a> says <em>the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)</em>.</p>
</li>
</ol>
<p>This is, in some sense, closely associated. If we consider some whole ABC as consisting of many <em>atoms</em>, like a market consisting of customers, or a body consisting of body parts, we can say that we <em>segment</em>  ABC but <em>cluster</em> the atoms.   But it seems that <em>segmentation</em> is more used when there is some concept of (spatial) contiguity of the atoms within the whole.</p>
<p>There seems to be confusion of this usage. On this site <a href=""https://stats.stackexchange.com/search?q=custome*+segmenta*"">customer segmentation</a> is often used, it should be <em>market segmentation</em>. The customers are not segmented (hopefully!), they are clustered. <a href=""https://en.wikipedia.org/wiki/Market_segmentation"" rel=""nofollow noreferrer"">Wikipedia got it right</a>.</p>
<p><strong>Use in connection with time series</strong>  With multiple (parallel) time series, we can <em>cluster</em> the series into groups of similar series, while <em>segmentation</em> typically refers to partitioning a single series in similar, contiguous, parts. See the tag <a href=""/questions/tagged/timeseries-segmentation"" class=""post-tag"" title=""show questions tagged &#39;timeseries-segmentation&#39;"" rel=""tag"">timeseries-segmentation</a> and <a href=""https://stats.stackexchange.com/search?q=time-series+cluster*+answers%3A1"">this list of posts about time series clustering</a>. That points to a connection to <a href=""/questions/tagged/change-point"" class=""post-tag"" title=""show questions tagged &#39;change-point&#39;"" rel=""tag"">change-point</a>-detection.  See <a href=""https://en.wikipedia.org/wiki/Time-series_segmentation"" rel=""nofollow noreferrer"">Wikipedia</a>.</p>
<p>On this site there are many posts on <a href=""/questions/tagged/image-segmentation"" class=""post-tag"" title=""show questions tagged &#39;image-segmentation&#39;"" rel=""tag"">image-segmentation</a>.</p>
",2020-12-04 16:44:02.767
