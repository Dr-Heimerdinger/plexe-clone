id,post_id,user_id,content_license,user_display_name,text,creation_date
82,143,,CC BY-SA 2.5,user88,I think this is the first candidate to be moved to Stack Overflow.,2010-07-19 21:38:27.890
101,143,,CC BY-SA 2.5,,"Possibly, but it'd need a lot more explanation on SO.",2010-07-20 01:29:42.683
127,143,,CC BY-SA 2.5,,"Most programmers know ""median"".  (sort(array))[length/2] is a big enough hint for those who forgot.  Also at its most basic for each new point you only need to do a bisection/insert on one half of the array...",2010-07-20 09:04:20.020
172,143,,CC BY-SA 2.5,,@walkytalky I don't think it would require any more explanation than any other algorithm question on SO.  Probably less as the median is a relatively basic concept.,2010-07-20 18:15:39.900
194,143,,CC BY-SA 2.5,,@Sharpie Perhaps not more than other SO algo questions. But certainly more than what it actually says here!,2010-07-21 02:58:14.737
254,414,4.0,CC BY-SA 2.5,,"@sharpie: are jokes out?  We obviously don't want the entire site to be humor, but everyone benefits from a little educational humor in small doses.",2010-07-22 05:15:40.057
273,414,,CC BY-SA 2.5,,"@Sharpie, feel free to close or reopen according to your feelings! I agree with Shane, a bit is ok, but not too much. For example, this question already included a funny cartoon. The jokes question not really a funny joke....",2010-07-22 13:58:03.300
274,414,4.0,CC BY-SA 2.5,,These cartoons are useful too; they can be included in a lecture on a particular topic where you are trying to explain a concept (e.g. correlation/causation above).  A little humor can help to keep an audience engaged.,2010-07-22 14:22:11.213
306,541,190.0,CC BY-SA 2.5,,"If I am not mistaken, 

linear regression is the estimation of coefficients that define a good linear map from X to Y.

 ANOVA is a test to know if there is significant differences in X when Y take two different values.  

Can you explain us why you think they are the same?",2010-07-23 15:29:16.043
309,543,,CC BY-SA 2.5,user28,"Thanks for the Gelman reference. I will read his paper. But, can't we analyze multilevel models using classical maximum likelihood? I agree that OLS is inefficient/inappropriate for multi-level models.",2010-07-23 15:50:47.240
311,543,182.0,CC BY-SA 2.5,,"@Srikant - there any many ways to deal with multilevel data and Gelman is ""the king"" of this field. His point is that ANOVA is a simple/clear method of capturing the key features of complex and hierarchical data structures or study designs and ANOVA is a simple/clear way of presenting the key results. In this sense it's role is complementary or exploratory.",2010-07-23 16:30:00.160
1023,1248,132.0,CC BY-SA 2.5,,I made this community wiki as there is no correct answer.,2010-08-06 02:44:34.817
1077,1248,155.0,CC BY-SA 2.5,,It probably makes sense to leave cartoons in this question: http://stats.stackexchange.com/questions/423/what-is-your-favorite-data-analysis-cartoon,2010-08-08 11:49:27.500
1121,28,4.0,CC BY-SA 2.5,,"Thanks @robin; made CW.  Although I don't entirely see this as ""argumentative""; there are two fields which have informed each other (this is a fact), and the question is how much they have evolved together over the last decade.",2010-08-09 14:17:51.890
1319,143,132.0,CC BY-SA 2.5,,Re-opened following discussion at http://meta.stats.stackexchange.com/questions/276/should-we-unclose-computing-questions,2010-08-13 00:31:38.923
2603,2509,60.0,CC BY-SA 2.5,,"Good question. I agree with the quote as well. I believe there are many people in statistics and mathematics who are highly intelligent, and can get very deep into their work, but don't deeply understand what they are working on. Or they do, but are incapable of explaining it to others.I go out of my way to provide answers here in plain English, and ask questions demanding plan English answers.",2010-09-15 21:43:29.863
2612,2509,,CC BY-SA 2.5,user1108,I had imagined a lengthy demo with a bunch of graphs and explanations when I stumbled across [this](http://www.youtube.com/watch?v=BfTMmoDFXyE).,2010-09-16 02:18:11.883
2615,2509,668.0,CC BY-SA 2.5,,"This was asked on the Mathematics site in July, but not as well and it didn't get many answers (not surprising, given the different focus there).  http://math.stackexchange.com/questions/1146/intuitive-way-to-understand-principal-component-analysis",2010-09-16 05:03:44.287
2618,2509,,CC BY-SA 2.5,,Similar to explanation by Zuur et al in Analyzing ecological data where they talk about projecting your hand on an overhead projector. You keep rotating your hand so that the projection on the wall looks pretty similar to what you think a hand should look like.,2010-09-16 09:00:49.823
3809,541,,CC BY-SA 2.5,,"ANOVA can be seen as ""syntactic sugar"" for a special subgroup of linear regression models. ANOVA is regularly used by researchers who are not statisticians by training. They are now ""institutionalized"" and its hard to convert them back to using the more general representation ;-)",2010-10-14 16:52:51.193
4419,2509,,CC BY-SA 2.5,,"Here is the link to ""Analysing ecological data"" by Alain F. Zuur, Elena N. Ieno, Graham M. Smith, where the example with the overhead-projector and the hand is given: http://books.google.de/books?id=mmPvf-l7xFEC&lpg=PA15&ots=b_5iizOr3p&dq=Zuur%20et%20al%20in%20Analyzing%20ecological%20data&hl=en&pg=PA194#v=onepage&q&f=false",2010-10-26 06:25:04.147
4636,3649,211.0,CC BY-SA 2.5,,"Thanks chl for the answer, I accepted it for the sheer scope of it.  Best, Tal",2010-10-31 13:34:32.593
5192,4187,287.0,CC BY-SA 2.5,,"I'm aware that ""sin"" is possibly inflammatory and that that some aspects of statistical analysis are not black-and-white. My intention is to solicit cases where a given commonly-taught practice is pretty clearly inappropriate.",2010-11-15 18:53:14.270
5194,4187,436.0,CC BY-SA 2.5,,You can also add biology/life sciences students to the mix if you like ;),2010-11-15 19:03:17.187
5196,4187,449.0,CC BY-SA 2.5,,maybe retitle it life science statistical sins?... or something else more specific...,2010-11-15 19:27:28.377
5972,4705,,CC BY-SA 2.5,user88,Converted to community wiki.,2010-12-04 00:14:08.750
5973,4705,1209.0,CC BY-SA 2.5,,what is community wiki?,2010-12-04 00:53:59.723
5975,4705,,CC BY-SA 2.5,,@Mariana: http://www.sharepointoverflow.com/questions/432/what-is-community-wiki,2010-12-04 04:32:47.227
5990,4705,,CC BY-SA 2.5,user88,@Mariana The idea is that pools and list-ofs are converted to a form in which they can be easily managed (due to lower rep req to edit) and voted up/down without hurting participants' reputation (votes on CW posts does not give/take reputation).,2010-12-04 16:01:50.720
6372,5015,,CC BY-SA 2.5,,"you could probably get a better answer if you provided more details about your experimental design, research question, and statistical model.",2010-12-13 23:52:15.460
6373,5015,1542.0,CC BY-SA 2.5,,"I have survey data, v1 and v2 predict the outcome, as I expected; however, the interaction between v1 (dichotomous) and v2 (5 groups) is not significant -- and (my question) it makes my v1 and v2 direct effects non-significant too. I can't find an example on reporting this in the literature.",2010-12-14 00:03:33.020
6374,5015,,CC BY-SA 2.5,,"If the v1:v2 interaction is not significant, do you need to have it included in the model?",2010-12-14 01:40:08.247
6379,5015,1506.0,CC BY-SA 2.5,,Maybe this question is relevant?  http://stats.stackexchange.com/questions/5184/removing-factors-from-a-3-way-anova-table,2010-12-14 03:00:55.870
6637,4714,,CC BY-SA 2.5,,But was he _really_ a statistician?,2010-12-19 22:27:36.527
6656,4714,60.0,CC BY-SA 2.5,,That's a tough one...when did statistics become a real field? Many of the fathers of stats were not statisticians.,2010-12-20 18:23:52.967
6702,4705,668.0,CC BY-SA 2.5,,If it weren't CW it would have to be closed as subjective and argumentative!,2010-12-21 19:37:00.287
6803,4714,1209.0,CC BY-SA 2.5,,"I know my choice is kind of arbitrary, because many are important but this is my favourite one, and his method allowed me to do lots of things.",2010-12-24 18:35:22.593
8089,1760,,CC BY-SA 2.5,,This is related to a past question: http://stats.stackexchange.com/q/2275/495,2011-01-25 20:19:06.560
8736,4187,,CC BY-SA 2.5,user88,"@whuber There was some good answers, so I've merged them both.",2011-02-06 11:02:41.973
9263,1787,723.0,CC BY-SA 2.5,,"Hmm - my question is not very well defined. The only thing I can do is pick *some* model for q() that permits setting parameters, and maximise the goodness of fit by fiddling with those parameters. That is - no matter what I do I will have to make some assumptions about what q() basically looks like.",2011-02-16 06:35:44.297
10996,543,,CC BY-SA 2.5,,"+1 for a nice clear answer.  Paragraph 3 is essentially what I was taught as a biology undergraduate, with emphasis laid on the ease of combining continuous and categorical independent variables in an ANOVA framework.",2011-03-14 15:43:11.747
11730,7965,1691.0,CC BY-SA 2.5,,"I realize that these are two slightly different questions, but I want them to be together to give some background to what I am using the results of the `scale()` function.",2011-03-24 14:52:09.670
11731,7965,674.0,CC BY-SA 2.5,,"@celenius `center=true` just means remove the mean, and `scale=TRUE` stands for divide by SD; in other words, with both options active, you're getting standardized variables (with mean 0, unit variance, and values expressed in SD units).",2011-03-24 14:58:31.323
11733,7965,1691.0,CC BY-SA 2.5,,"@chl Ah! Thank you. That is a much clearer explanation to me than the help file. Does it make a difference how a variable is standardized for clustering with kmeans though? I would assume not, but don't know for sure.",2011-03-24 15:01:49.490
11736,7965,674.0,CC BY-SA 2.5,,"@celenius I can suggest this article, [Standardizing Variables in K-means Clustering](http://www.springerlink.com/content/l438152634256311/) (Steinley, 2004) to get an idea of the effects of different kind of transformations. The utility of scaling depends on the data you have, but usually one reason to use standardized (scaled) variables is to avoid obtaining clusters that are dominated by variables having the largest amount of variation.",2011-03-24 15:18:09.887
11745,7965,668.0,CC BY-SA 2.5,,@celenius Why is collinearity a problem?  It shouldn't affect k-means at all.,2011-03-24 17:49:39.270
11749,7965,1691.0,CC BY-SA 2.5,,"@whuber I assumed that the more collinear variables I include, the more that it biases the clustering towards those collinear dimensions.",2011-03-24 18:18:28.917
11751,7965,668.0,CC BY-SA 2.5,,"@celenius It shouldn't introduce bias.  Take, for instance, the extreme of perfect collinearity in 2D: all observations are of the form $(a+t x, b+t y)$ for constants $a,b,x,y$.  K-means uses Euclidean distances and the distance between two such points given by $t$ and $s$ is just $|s-t|$: it's really a 1D problem and there's no special weighting given to any of the data.",2011-03-24 19:31:46.477
12756,8529,1895.0,CC BY-SA 3.0,,Do you have some general interests that you'd like to list? That might help guide suggestions. Applications of statistics have become pretty pervasive in a remarkably broad array of fields.,2011-04-09 01:58:29.253
12889,8529,793.0,CC BY-SA 3.0,,"@cardinal, nope, no particular interests -- the purpose was to branch out from the stuff I typically read, so I'm trying not to limit any answers. (This does maybe make the question a bit too broad, but I guess I'm looking for people's personal ""best of"" lists.)",2011-04-11 21:40:41.700
12963,2509,,CC BY-SA 3.0,,"A two pages article explaining PCA for biologists: Ringnér. [What is principal component analysis?](http://www.nature.com/nbt/journal/v26/n3/full/nbt0308-303.html). Nature Biotechnology 26, 303-304 (2008)",2011-04-13 01:55:06.840
14478,9524,221.0,CC BY-SA 3.0,,"nice idea for a collection ! @mods: Beside cw maybe renaming to ""movies every statistician should have seen"" or something like that ?",2011-05-07 12:28:31.583
14480,9524,674.0,CC BY-SA 3.0,,@steffen Thx. Better to flag the question for mods attention in the future (we aren't notified with @mods).,2011-05-07 12:37:52.090
14485,9524,2872.0,CC BY-SA 3.0,,"By the way, I found this page, which might add some movies to the collection: http://world.std.com/~reinhold/mathmovies.html",2011-05-07 14:41:51.013
14487,9524,,CC BY-SA 3.0,,Nothing that will inspire anybody to take up mathematics. Stick to books.,2011-05-07 15:21:49.440
14497,9524,221.0,CC BY-SA 3.0,,@Emre Maybe. But nevertheless it is entertaining for those who already deal with math (even more if the movie is presenting things wrong or with exaggeration).,2011-05-07 18:38:36.787
14507,9529,1693.0,CC BY-SA 3.0,,"An absorbing story and one I'd recommend, but it referred to math rather than truly involding math.  I'd say the same thing about the tv show Numb3rs.",2011-05-07 22:43:12.617
14519,4705,1209.0,CC BY-SA 3.0,,"Thank you very much for doing that, but I am sorry to tell you that I can not see anything i the first link.",2011-05-08 05:28:34.270
15357,10008,2352.0,CC BY-SA 3.0,,"My philosophy is run lots of models, check their predictions, compare, explain, run more models.",2011-05-20 03:43:22.080
15413,10008,,CC BY-SA 3.0,,"If the interactions are only significant when the main effects are NOT in the model, it may be that the main effects are significant and the interactions not. Consider one highly significant main effect with variance on the order of 100 and another insignificant main effect for which all values are approximately one with very low variance. Their interaction is not significant, but the interaction effect will appear to be significant if the main effects are removed from the model.",2011-05-20 16:26:35.877
16234,10541,674.0,CC BY-SA 3.0,,"Please, don't cross-post on [SO](http://stackoverflow.com/questions/6241181/gap-statistics-matlab-implementation) and [MatlabCentral](http://www.mathworks.fr/matlabcentral/answers/8811-gap-staticstics-implementation)!",2011-06-05 08:48:49.270
16249,10541,1895.0,CC BY-SA 3.0,,The gap statistic takes 20 lines of code or less to implement. It may be faster just to write it yourself.,2011-06-05 14:45:59.767
16253,10541,2690.0,CC BY-SA 3.0,,@chi ok will not repeat the same.,2011-06-05 15:14:57.170
16955,10911,,CC BY-SA 3.0,,"Not an answer, just an intuitive observation.  The CI for the pooled data mean (all nine obs) is $(39.7 \pm 2.13)$, CI based on the means only is $(39.7\pm 12.83)$.  Not sure what your CI is doing (typo? 17 not 27, and 51 not 61?), I get $2.98$ for std err of three means, and $4.30$ as $0.975$ quantile of T dist with 2 df.  I would think that the CI you seek would lie somewhere in between these two - as you have partial pooling.  Could also think in terms of variance formula $V(Y)=E[V(Y|Y_g)]+V[E(Y|Y_g)]$, each CI uses half of the formula",2011-06-18 04:58:50.833
16992,10911,22.0,CC BY-SA 3.0,,"@probabilityislogic: The SEM of the three experiment means is 5.168  (not 2.98 as you wrote), and the confidence interval I gave in the original post (17.4 to 61.9) is correct. The SEM is computed from the SD (8.95) by dividing by the square root of n (square root of 3). You divided by n (3) instead.",2011-06-19 13:42:39.630
17011,10911,,CC BY-SA 3.0,,"my mistake, should also replace $2.13$ by $6.40$ in the pooled interval (same mistake there)",2011-06-20 05:10:22.410
18283,412,,CC BY-SA 3.0,,"Could you be a little more precise? What type of analysis, in what context, etc.",2011-07-13 05:38:35.730
18284,412,,CC BY-SA 3.0,,"Well, I'm talking about the basics, an overview of as much as is possible.",2011-07-13 07:08:38.587
19728,4714,,CC BY-SA 3.0,,That this was selected indicates a biased prior.,2011-08-06 00:20:15.883
20768,13058,668.0,CC BY-SA 3.0,,"For one solution, see the comments to [this reply](http://stats.stackexchange.com/questions/14351/forecasting-time-series-based-on-a-behavior-of-other-one/14366#14366).  Open source solutions would include image processing or raster GIS software ([GRASS](http://grass.fbk.eu/) is a likely candidate) or, perhaps, [GNU Octave](http://www.gnu.org/software/octave/).  I'm mentioning these as a comment because I haven't used either for this specific purpose, so please take them as possibilities, not as definite solutions.",2011-08-18 04:20:43.743
20769,13058,1124.0,CC BY-SA 3.0,,"I'm hoping for code/software specifically for scraping graphs, and I remember such packages existed, at least they did 10 yrs ago, but I can't remember their names now, and don't know if they work on current operating systems.",2011-08-18 04:56:35.977
20772,13058,,CC BY-SA 3.0,,"@Alex, try googling [""Graph Digitizer Open Source""](http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=graph+digitizer+open+source)",2011-08-18 05:52:45.057
21254,13060,,CC BY-SA 3.0,,"There is a nice article / tutorial in [R Journal, June 2011](http://journal.r-project.org/archive/2011-1/RJournal_2011-1_Poisot.pdf)",2011-08-23 22:56:28.507
21943,13631,,CC BY-SA 3.0,,"see if this helps
http://www.math.bme.hu/~morvai/publications/papers/MorvaiWeissActApplMath2003ARX.pdf good day",2011-09-02 03:22:09.713
21945,13631,5898.0,CC BY-SA 3.0,,Have you considered a hidden markov model?,2011-09-02 03:25:56.700
21996,13631,,CC BY-SA 3.0,user6145,"Thanks for the answers. But there is any software package already available with some implementations? I have searched in R, but I only found the VLMC package. Thanks,
Ricardo Bessa",2011-09-02 18:17:36.373
21997,13631,,CC BY-SA 3.0,,"Ricardo, you should edit your question with this additional information instead of adding it as an answer.  Thanks, and welcome to the site!",2011-09-02 18:26:26.157
22002,13631,,CC BY-SA 3.0,,"Are there really two types of 1's in your data? That is, 1 meaning the car could be moving but is not versus 1 meaning that your car really could not be moving at this time. That would be called one-inflation (it's usually zero-inflation). If so, you need to model when the car can be moving or not versus when it might be moving but is not.",2011-09-02 19:13:09.537
22022,13631,4221.0,CC BY-SA 3.0,,"In this case there is only 1, meaning that the car is not moving. Perhaps hidden markov models can be a good option.",2011-09-03 00:13:46.983
23827,14729,2081.0,CC BY-SA 3.0,,"Your matrix is positive semi-definite, it's not positive definite though, for it is singular.",2011-10-01 17:53:04.460
23828,14729,,CC BY-SA 3.0,,What are the dimensions (no. variables; no. samples)?,2011-10-01 17:59:46.217
23830,14729,5898.0,CC BY-SA 3.0,,"Number of columns = 480. # of rows for each time series = 502. In general you find that the larger the time series the sample covariance matrix tends to be positive definite. However, there are many cases where you'd like to use a substantially smaller value of T (or exponentially weight) to reflect recent market conditions.",2011-10-01 19:24:35.453
23835,14729,2765.0,CC BY-SA 3.0,,"The question is ill-posed. If your data matrix is 480 by 502 then saying that the matrix has rank $q < 480$ (the column space of the matrix has dimension $q < 480$) is mathematically equivalent to saying that some column is a linear combination of the others, but you can't pick out one column and say that this is the column that is linearly dependent. So there is no procedure for doing this, and the suggested procedure will pick a quite arbitrary security depending on the order they are included.",2011-10-01 21:34:07.430
23846,14729,5898.0,CC BY-SA 3.0,,The covariance matrix is symmetric. It is generated by transpose(A) * A. The matrix A has dimensions 480x502. However the covariance matrix is 480x480,2011-10-01 23:26:30.863
24702,15281,668.0,CC BY-SA 3.0,,We have a specialist in this subject who is a frequent contributor.  Consult almost any of [his replies](http://stats.stackexchange.com/users/3382/irishstat) for an account of his favorite methods.  Here is a [typical one](http://stats.stackexchange.com/questions/6033/detect-changes-in-time-series/8549#8549).,2011-10-13 17:37:36.160
24706,15281,3641.0,CC BY-SA 3.0,,"@whuber, I didn't find methods on his post.",2011-10-13 19:44:42.753
24707,15281,668.0,CC BY-SA 3.0,,He provides a link to documents with references and equations.,2011-10-13 20:05:47.283
24724,14790,,CC BY-SA 3.0,,"Isn't this the reduced row echelon form? If so, aren't there packages / functions available in R?",2011-10-14 00:39:40.743
24731,14790,2081.0,CC BY-SA 3.0,,"@Arun, I'm not R user so can't know.",2011-10-14 06:42:25.397
27748,17000,5479.0,CC BY-SA 3.0,,But how could I download them in .txt,2011-11-24 22:44:25.457
28379,4714,436.0,CC BY-SA 3.0,,Does one *discover* a theorem? Shouldn't it be *postulating* or *theorizing*?,2011-12-04 13:21:30.270
28490,28,5671.0,CC BY-SA 3.0,,"Add a third culture: **data mining**. Machine learners and data miners speak quite different languages. Usually, the machine learners don't even understand what is different in data mining. To them, it's just unsupervised learning; they ignore the data management aspects and apply the *buzzword* data mining to machine learning, too, adding further to the confusion.",2011-12-06 12:05:25.373
30338,4714,5237.0,CC BY-SA 3.0,,"Sigh.  I would have hoped that one could be an enthusiast of the Bayesian approach without actually believing that Bayes the man was the greatest statistician of all time.  Shouldn't we honor someone who contributed important work to its development, like, e.g., [Jaynes](http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes)?",2012-01-07 03:26:45.850
31657,1248,668.0,CC BY-SA 3.0,,"This is a popular and much-loved thread, even though it does not (on the face of it) seem to conform to SE standards for content.  (Just what practical question is being asked here? :-)  Some rules benefit from being ... bent ... once in a while.  However, please don't use the existence of this thread to justify creating new ones that fall outside our guidelines unless you think there is a very good reason to do so!  Questions about site policy are always appropriate in [Meta](http://meta.stats.stackexchange.com/) and debate is warmly welcomed in [chat](http://chat.stackexchange.com/).",2012-01-24 15:34:51.807
33305,20240,,CC BY-SA 3.0,vzn,"re ""numerical optimization methods"", ""discrete optimization methods"", it seems many ML techniques could be proven to be within a constant factor of the true optimum if their ""initial search space"" is forced to be large, but I havent seen a ref on this.",2012-02-10 22:10:31.230
33337,20240,,CC BY-SA 3.0,,"I disagree. 

* for numerical optimization you can get into local minimum (of course you can also apply procedures that make this unprobrable).

* The same is for Neural Networks (at least it can happen during training of perceptron). 

* Genetic algorithms can also get into local minimum, moreover if you choose to big mutation rates you will get no sensible evolution! 

II also strongly suspect that there are datasets that will always make certain models have arbitralily big errors.",2012-02-11 13:21:38.393
33589,20240,,CC BY-SA 3.0,,"@vzn many people choose models for which the optimal solution can be found. This is because the use convex loss functions, as SVMs do. Finding the true optimum here means ""finding the optimal solution in your search space"", so that has nothing to do with how the search space looks like.
As jb said, for general loss functions, finding the true optimum is usually impossible / infeasible.",2012-02-14 08:44:30.200
34278,20667,221.0,CC BY-SA 3.0,,I vote for cw ;),2012-02-20 12:53:13.573
37169,28,,CC BY-SA 3.0,,There's a similar question on [data mining and statistics](http://stats.stackexchange.com/questions/1521/data-mining-and-statistical-analysis),2012-03-22 23:51:48.623
38536,23019,,CC BY-SA 3.0,,"It seems to me model b makes more sense, since you could imagine fixed developmental effects occurring at particular ages in all mice, but occasion specific conditions (captured by the random intercept, grouped by time) may be more thought of as random perturbations.",2012-04-06 01:48:46.123
38693,23087,5179.0,CC BY-SA 3.0,,"No, I think you are confusing the definition of an MA(n) model, where the regression is only in terms of the $e_{t-i}$'s, with its estimation, where the $e_{t-i}$'s are estimated from the data.",2012-04-07 17:25:28.287
38713,23087,1406.0,CC BY-SA 3.0,,"The main problem in your question is that you say that MA model is basically a linear regression. This is simply not true, since we do not observe error terms.",2012-04-07 20:18:02.153
38863,23087,5643.0,CC BY-SA 3.0,,"I think the error term *is* actually $Y_t - \hat{Y_t}$, where $\hat{Y}$ is $E(Y|Y_{t,...,t-n})$ or simply $Y_t - Y_{t-1}$. That is why an MA model parameter estimate is derived from a recurring pattern in the $Y$ partial autocorrelation function, that is the behavior of the residuals. The AR parameter estimation instead, is based on a recurring pattern of the acf(Y).",2012-04-09 15:23:42.327
41789,24506,,CC BY-SA 3.0,,It's not totally clear what kind of model you're fitting - is this a linear regression model?,2012-05-06 00:40:29.510
42014,24506,7341.0,CC BY-SA 3.0,,"Yes, it's linear regression.",2012-05-07 21:52:37.023
42176,24602,7341.0,CC BY-SA 3.0,,"Thank you very much - I haven't been able to get with my coworker who knows much more R than me, but I'm sure this is the ticket.",2012-05-08 19:46:30.480
42863,25087,7421.0,CC BY-SA 3.0,,"Thank you. Just to make sure on your thoughts this is the quote, P. 117: ""Principal factors as normally extracted are based on equations for which it is assumed that the population correlation matrix is being factored. In maximum like hood procedures it is explicitly recognized that a sample is being analyzed. Maximum like hood procedures are defined as those that best reproduce the population values (the meaning of ""best reproduce"" is further discussed in Chapter 7). Any factor solution that best reproduces the population values is a maximum like hood analysis.",2012-05-14 16:09:20.817
42870,25087,8363.0,CC BY-SA 3.0,,"The terminology is bad.  We don't say that maximum likelihood estimates best reproduce the population values.  The maximum likelihood estimates are obtained for population parameters by selecting the values that maximize the likelihood function. This means that the exact parametric model using the fitted parameter(s), in a way best describes the observed data. Maybe this is what he intended to say but phrased it poorly.",2012-05-14 16:39:30.570
42872,25087,8363.0,CC BY-SA 3.0,,"The population correlation matrix cannot be factored since it is unknown. PCA does what I described for the sample data. I don't even think that means factoring the sample correlation matrix. I am assuming he means principal components when he says ""principal factors"" but it could mean something else and really have something to do with the sample correlation matrix.I think you should look at other books on this topic that are much better written (e.g. books on multivariate analysis such as Gnandesikan or Jolliffee's book on PCA).",2012-05-14 16:46:43.573
42887,25087,7421.0,CC BY-SA 3.0,,Thank you Michael for answering this question 2. Hopefully I can get an answer for the others too!,2012-05-14 18:22:52.870
45977,16998,668.0,CC BY-SA 3.0,,This thread appears to be off topic.  See http://meta.stats.stackexchange.com/questions/1032/data-sourcing-we-need-to-make-up-our-mind/1033#comment2001_1033.,2012-06-07 21:39:07.407
47052,412,,CC BY-SA 3.0,,[Statistics explained](http://www.amazon.com/Statistics-Explained-Introductory-Guide-Scientists/dp/0521183286) covers the basics using examples from the life sciences. The answers to [this question](http://stats.stackexchange.com/questions/29380/a-statistics-book-that-explains-using-more-images-than-equations/) may also contain recommendations that you'll find useful.,2012-06-15 13:59:49.403
47139,28,,CC BY-SA 3.0,user10525,An interesting discussion in [Wasserman's blog](http://normaldeviate.wordpress.com/2012/06/12/statistics-versus-machine-learning-5-2/).,2012-06-16 10:43:07.713
47631,27120,2081.0,CC BY-SA 3.0,,"Just a side note. ""Error"" in abbreviations like MSE usually actual mean ""residuals"". Practically, residuals and errors frequently are treated as synonims, MSE=MSR. But theoretically there's important [distinction](http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics) between the two terms",2012-06-20 17:55:33.763
49899,5020,449.0,CC BY-SA 3.0,,"There's no moral hazard.  The calculation of the main effects with the interaction included is quite different from the calculation without it.  You have to do the additive model to report the main effects and then include the interaction in a separate model anyway.  You ignore the main effects in the model that includes the interaction because they're not really main effects, they're effects at specific levels of the other predictors (including the interaction).",2012-07-10 14:08:28.770
55166,30434,5249.0,CC BY-SA 3.0,,Thanks for dredging up this question! Wallach et al also have a paper on topic model evaluations: [Evaluation methods for topic models](http://doi.acm.org/10.1145/1553374.1553515),2012-08-22 08:27:14.353
55325,30434,9605.0,CC BY-SA 3.0,,"No worries. I've found there's some code for Wallach's left-to-right method in the MALLET topic modelling toolbox, if you're happy to use their LDA implementation it's an easy win although it doesn't seem super easy to run it on a set of topics learned elsewhere from a different variant of LDA, which is what I'm looking to do. I ended up implementing the Chib-style estimator from their paper using the matlab code they supply as a guide although had to fix a couple of issues in doing that, let me know if you want the code.",2012-08-23 11:28:40.423
56111,30957,8363.0,CC BY-SA 3.0,,"Just curious, why do you want to initialize the model with a different time series?",2012-08-29 15:31:12.027
56118,30957,9446.0,CC BY-SA 3.0,,@MichaelChernick I can think of few reasons why one might want to initialize the simulation with a different time-series.  1). One might have a population that is very similar to the one modeled but few time points to build a new model from (of course the big assumption would be the new population exhibits population dynamics just as the original one did which is probably not the best assumption but could still be informative about what to expect from the new population).,2012-08-29 16:16:21.897
62533,34166,,CC BY-SA 3.0,,"It would be good if you could describe the experiment a bit clearer. Without reading the original post, it is really hard to understand what the paradox is about.",2012-10-25 22:51:17.727
62693,34166,,CC BY-SA 3.0,,My comment wasn't meant to be rude btw. I realized later it might have come across a bit harsh. Hope you didn't take it the wrong way.,2012-10-28 00:42:19.703
56120,30957,9446.0,CC BY-SA 3.0,,"2). Also, one might want to simulate potential impact of a rare but extreme short-term event that causes high mortality in the population (i.e., hurricane, chemical spill, disease outbreak, etc). Of course one would have make some assumptions about how those events might change the population’s growth rate but could use the simulation results to inform wildlife managers as to potential effects by magnitude and duration of declines in population growth rates on the population’s trajectory. Those results could then be used to help inform planning and management decisions for the population.",2012-08-29 16:17:42.660
56136,30960,9446.0,CC BY-SA 3.0,,Wouldn't using a non-seasonal ARIMA model with higher AR orders (p > 1) results in the cyclic pattern being dampened as the forecasts increase a cycle or two beyond the last data point?,2012-08-29 19:02:41.150
56162,30960,132.0,CC BY-SA 3.0,,"Yes, but that's because the cycles are not strictly periodic. They are almost periodic, and as you go further out in the forecast horizon, it becomes harder to predict which part of the cycle you will be in. The point forecasts are means, and so they will naturally flatten out, indicating this uncertainty.",2012-08-30 04:24:43.993
56184,30960,8363.0,CC BY-SA 3.0,,I still think this is not a very sensible thing to do.  It is a rather strong assumption to think that a different series will have the same model form and coefficients as another series.  When you don't have sufficient data to forecast I think it is best not to.,2012-08-30 12:19:07.520
56195,30960,9446.0,CC BY-SA 3.0,,@RobHyndman Thank you for your insight.  Is the link you were refering to [Research tips - Cyclic and seasonal time series](http://robjhyndman.com/researchtips/cyclicts/)?,2012-08-30 14:05:03.150
56248,30960,132.0,CC BY-SA 3.0,,"That's relevant, but I was referring to papers such as http://www.publish.csiro.au/?paper=ZO9530163",2012-08-30 20:08:29.103
57490,31575,7007.0,CC BY-SA 3.0,,"You have $4$ states: $S=\{1:=A,2:=B,3:=C,4:=D\}$. Let $n_{ij}$ be the number of times the chain made a transition from state $i$ to state $j$, for $ij,=1,2,3,4$. Compute the $n_{ij}$'s from your sample and estimate the transition matrix $(p_{ij})$ by maximum likelihood using the estimates $\hat{p}_{ij}=n_{ij}/\sum_{j=1}^4 n_{ij}$.",2012-09-11 16:29:52.470
57491,31575,7007.0,CC BY-SA 3.0,,These notes derive the MLE estimates: http://www.stat.cmu.edu/~cshalizi/462/lectures/06/markov-mle.pdf,2012-09-11 16:30:57.337
57500,31587,6404.0,CC BY-SA 3.0,,Looks great! I'm not sure what the 3rd line does in your code though (mainly because I'm familiar with Matlab). Any chance you could write it in matlab or pseudo-code?  I'd be much obliged.,2012-09-11 17:22:15.067
57524,31587,7007.0,CC BY-SA 3.0,,"The third line does this: the chain values are $x_1,\dots,x_n$. For $t=1,\dots,n-1$, increment $p_{x_t,x_{t+1}}$.",2012-09-11 19:32:17.700
57525,31587,7007.0,CC BY-SA 3.0,,The fourth line normalizes each line of the matrix $(p_{ij})$.,2012-09-11 19:34:22.347
57535,31575,1359.0,CC BY-SA 3.0,,Similar question:http://stats.stackexchange.com/questions/26722/calculate-transition-matrix-markov-in-r,2012-09-11 20:18:13.553
57542,31587,6404.0,CC BY-SA 3.0,,Bare with my slowness here. I do appreciate the MATLAB code translation although I still can't see what it's attempting to do in your first `for` loop. The 3rd line from the original code is counting the number of times $x$ goes from state $x_i$ to state $x_j$? If you could say it in words I'd appreciate that a lot. Cheers,2012-09-11 20:35:23.493
57551,31575,6404.0,CC BY-SA 3.0,,@B_Miner could you write your code in pseudo-code form for me? Or explain it in lay terms...  However I see it works in my R console.,2012-09-11 21:41:01.917
57553,31587,6404.0,CC BY-SA 3.0,,I have implemented your R code and it works just as you explained. Which is great!  My query is now with regards to the $x$ vector... My observation sequences are of uneven lengths.  Eg the matrix I provided above. How would I change your code to handle this?,2012-09-11 21:44:50.053
57558,10911,,CC BY-SA 3.0,user14015,"does the following link answers' this?
http://www.talkstats.com/showthread.php/11554-mean-of-means",2012-09-11 21:55:13.197
57572,31587,7007.0,CC BY-SA 3.0,,"A realization of the chain is something like $1, 1, 2, 1, 2, 1, 2, 4, 3, 1$. By inspection, it jumped from $1$ to $1$ one time, from $1$ to $2$ three times, and so on. What exactly you don't understand?",2012-09-12 00:41:29.340
57583,10911,674.0,CC BY-SA 3.0,,"@TST, There appears to be nothing but a link to Wikipedia on [Pooled variance](http://en.wikipedia.org/wiki/Pooled_variance). Care to elaborate?",2012-09-12 07:45:38.580
57973,31587,6404.0,CC BY-SA 3.0,,Can you clarify for me that your vector x is the same as concatenating my matrix rows? Ie one long sequence instead of many short ones?,2012-09-15 22:41:23.583
57974,31587,7007.0,CC BY-SA 3.0,,"No, $x$ is just one row. Don't concatenate because you will introduce ""false"" transitions: last state of one line $\to$ first state of the next line. You have to change the code to loop through the lines of your matrix and count the transitions. At the end, normalize each line of the transition matrix.",2012-09-15 23:57:51.537
59038,32317,,CC BY-SA 3.0,,"I believe Apache Mahout has some clustering algorithms, but I don't know much about it (other than that is runs over Hadoop).",2012-09-25 20:59:31.763
59207,32388,,CC BY-SA 3.0,user88,"IMO the main problem is that you have so little data here that any model makes (no) sense; remember that in contrary to you R doesn't understand variable names and can't infer from that. BTW you can get ""right"" result simply by reversing the order of columns -- this even better shows that you are torturing this poor algorithm stranded in a no-information regime.",2012-09-26 23:25:18.097
59212,32388,633.0,CC BY-SA 3.0,,"In any case, the two dependence structures are identical (as long as you do not have interventions).  Also, I think you are confusing the direction of reasoning with the directions of the arrows.  Finally, all variables in a Bayesian network can be a response variable and can have arcs to other variables.",2012-09-27 00:01:10.453
59303,32317,8208.0,CC BY-SA 3.0,,Actually I will have to cluster data in the form of sets of features. In another question I have been told that I should try using some set-based distance metrics (Such as Jaccard or Tanimoto) but they're not implemented in Weka. Do you recommend me other libraries supporting these distances?,2012-09-27 14:07:10.020
62043,4187,,CC BY-SA 3.0,user16110,I just gave a talk on this subject... A link to the video follows if you are interested. http://www.youtube.com/watch?v=1SNQQvY1ESo&feature=g-upl,2012-10-21 01:13:53.057
62045,4187,,CC BY-SA 3.0,,"Hi @Amanda, could you give some indication here of what's in the talk? No-one likes the possibility of being rick-rolled.",2012-10-21 02:17:50.833
62455,10069,3733.0,CC BY-SA 3.0,,"This means that we should start deleting the terms from y ~ x1 * x2 * x3 * x4, starting deleting the highest-order terms, i.e. the normal deletion method, right?",2012-10-25 08:11:04.330
62515,10069,2666.0,CC BY-SA 3.0,,"Deletion of terms is not recommended unless you can test entire classes of terms as a ""chunk"".  For example it may be reasonable to either keep or delete all interaction terms, or to keep or delete all interactions that are 3rd or 4th order.",2012-10-25 19:13:21.093
62520,34166,668.0,CC BY-SA 3.0,,I was moved to post this as a separate question based on comments at http://stats.stackexchange.com/questions/23779.,2012-10-25 20:10:56.277
104671,57730,594.0,CC BY-SA 3.0,,How did the continuous variables became discrete (multinomial)?,2013-10-17 20:12:26.730
63093,34166,,CC BY-SA 3.0,user16414,"You might be interested in the (now large) literature in philosophy on this paradox. Here is a fairly complete bibliography (with links):
http://philpapers.org/browse/sleeping-beauty",2012-10-31 14:13:08.683
64170,35097,1506.0,CC BY-SA 3.0,,Discussion on Gelman's blog: http://andrewgelman.com/2012/11/16808/,2012-11-11 16:27:06.533
64176,35097,,CC BY-SA 3.0,,"I think a lot is wrong, both from the frequentist and Bayesian point of view. My biggest criticism each: First, P values are ultimately heuristics and are properties of a number of things including the statistical problem, data and experiment. Here, all three are grossly misrepresented for that particular question. Second, the ""Bayesian"" uses a decision theoretic approach which need not be Bayesian. It's funny, though.",2012-11-11 17:33:54.870
64262,35097,,CC BY-SA 3.0,,"To take it out of the statistics realm....the sun isn't massive enough to go nova. QED, the Bayesian is right. ([The Sun will instead become a Red Giant](http://simple.wikipedia.org/wiki/Sun#The_fate_of_the_Sun))",2012-11-12 17:15:25.793
64268,35160,,CC BY-SA 3.0,user10525,"(+1) A nice reference on this *strong and crucial* assumption of repeatability in frequentism is [Statistical Inference in Science (2000)](http://www.amazon.com/Statistical-Inference-Science-D-Sprott/dp/0387950192), chapter 1. (Although there are so many issues that it is difficult to tell which one is *the main* one)",2012-11-12 17:41:32.140
64270,35097,,CC BY-SA 3.0,,"@Glen et alii, in particular, note Randall Munroe's response to Gelman: http://andrewgelman.com/2012/11/16808/#comment-109366",2012-11-12 17:48:10.677
64271,35097,,CC BY-SA 3.0,user10525,"I believe the comic confounds **estimation** and **hypothesis testing** (Basic mistake!). 

The machine *estimates*,using a decision-theoretic approach, the probability of an event. The outcome is 0 or 1, based on the decision rule.

The frequentist statistician relates this with a p-value (why? Just for fun). He/she should have related this value with a point estimator.",2012-11-12 18:00:20.307
64356,35160,,CC BY-SA 3.0,,"Not so fast with the repeatability argument... First, the experiment that is repeatable is the querying of the machine not the sun going nova The truth of *that* is the fixed but unknown object of inference. The querying experiment can certainly be repeated, and if it were for a few more times the frequentist strategy could easily seem reasonable.",2012-11-13 09:42:33.477
64357,35160,,CC BY-SA 3.0,,"Second, one should not be too stringent on the repeatability business anyway, lest frequentists be stuck not being able to infer anything at all in non-experimental situations.  Assume for a moment that 'sun goes nova' was the candidate event.  I'm no physicist, but I'm told that the event 'sun goes nova' happens rather often (just not so much around here) so this sounds to me  like a repeat.  In any case, folk like David Cox (in 'Foundations of Statistics') cheerfully say things like: ""the repetitions contemplated are *almost always hypothetical*. This by itself seems no drawback"".",2012-11-13 09:51:26.513
64392,35160,651.0,CC BY-SA 3.0,,We could view the sun as a random sample from a population of suns in parallel universes in which we could in principle repeat the experiment if only we had a quantum mirror! ;o),2012-11-13 17:46:25.117
64393,35097,,CC BY-SA 3.0,,Discussion and response on Larry's blog: [http://normaldeviate.wordpress.com/2012/11/09/anti-xkcd](http://normaldeviate.wordpress.com/2012/11/09/anti-xkcd),2012-11-13 17:58:18.583
64445,35249,668.0,CC BY-SA 3.0,,"How, *exactly,* are the samples obtained and measured? This matters because if the samples represent averages within each layer at their locations, then the varying layer thickness will change the distributions of the values and thereby suggest one set of approaches. Otherwise, if the results are from subsampling each layer (which often happens in the lab), then another set of approaches might be favored.",2012-11-13 22:43:26.443
64447,35249,11884.0,CC BY-SA 3.0,,"Thanks @whuber for the fast reply: Layers are (re)calculated with weighted average, from sampling layers. So they do not represent the actual sampling and lab measured samples (every profile has a different layer divison for sampling). And after recalculating the layering is uniform for every sample for every property.",2012-11-13 22:49:11.590
64448,35249,668.0,CC BY-SA 3.0,,"You may have a hard time, then, interpreting the results: they could tell you more about your interpolation (averaging) method than about what's really going on. Is there a reason not to do the PCA with the original data?",2012-11-13 22:50:58.050
64451,35249,11884.0,CC BY-SA 3.0,,"Sampling is really diverse for every single point. It means sometimes the first layer is 0-1cm sometimes 0-100cms. The goal would be clustering, on a uniform layering, but I'd like to get rid of correlating properties.",2012-11-13 22:54:32.360
64453,35249,11884.0,CC BY-SA 3.0,,"@whuber I was considering also splines, not weighted average but in some cases it would have result to misleading values if NAs are present in a profile.",2012-11-13 23:05:45.187
66501,20234,12900.0,CC BY-SA 3.0,,on further thought on this question it seems the related/relevant area of research is called _global optimization_ methods/variants on top of local-type algorithms eg gradient descent...,2012-12-05 06:29:31.657
66502,20240,12900.0,CC BY-SA 3.0,,"accepting this answer as a description of current state of affairs & general categories of applications but still think there are some bridge thms that exist & remain to be proven that link up the separate areas. the proof that NNs can model or _""approximate""_ any continuous mathematical fn to arbitrary degree of accuracy is closely related... ie [kolmogorovs thm](http://neuron.eng.wayne.edu/tarek/MITbook/chap2/2_3.html)",2012-12-05 06:42:15.477
66503,20234,12900.0,CC BY-SA 3.0,,"eg [""global optimization for neural network training""](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.189) by shang & wah",2012-12-05 06:43:57.550
68498,20667,4890.0,CC BY-SA 3.0,,A similar question in lines of specific datasets has been closed here : http://stats.stackexchange.com/questions/38928/high-dimensional-regression-datasets,2012-12-22 18:09:39.020
69088,37981,,CC BY-SA 3.0,user10525,"Indeed, the measures of kurtosis are typically applied to symmetric distributions. You can calculate it for skewed ones as well but the interpretation changes since this value varies when the asymmetry is introduced. In fact, these two concepts are difficult to separate. Recently, a skewness-invariant measure of kurtosis was proposed in [this paper](http://amstat.tandfonline.com/doi/abs/10.1198/tast.2011.10194).",2013-01-03 16:22:03.980
69148,37981,594.0,CC BY-SA 3.0,,"High kurtosis is associated with peakedness and with heavy tailedness (it's also characterized as 'lack of shoulders'). One of the volumes of Kendall and Stuart discuss these issues at some length. But such interpretations, are, as you note, generally given in the situation of near-symmetry. In nonsymmetric cases, the standardized 4th moment is usually highly correlated with the square of the standardized third moment, so they're mostly measuring much the same kind of thing.",2013-01-03 23:44:05.083
70509,34166,668.0,CC BY-SA 3.0,,"Thank you, @jpust! That is a wonderful resource.  It's going to take a while to get through it. :-)",2013-01-17 01:53:36.057
100470,55576,2149.0,CC BY-SA 3.0,,@whuber the partial acf is the conditional acf. THe relationship between the pacf and the acf is the same as that between partial regression coefficients and regression coefficients.  It is a matrix of partial (auto) correlations.,2013-09-18 15:15:45.967
72633,40104,594.0,CC BY-SA 3.0,,"Some *particular* kinds of combinations have names (e.g. [the Skellam distribution](http://en.wikipedia.org/wiki/Skellam_distribution)), but the general case its just compound Poisson, I think. There will be no closed form expression that's simpler than the original sum in general, though I suppose you can just write it as a convolution. When you say 'generate' do you mean 'simulate random values from', or something else?",2013-02-08 02:56:43.813
72636,40104,668.0,CC BY-SA 3.0,,"A closely related special case--that of the average of Poisson variates, binned by integers--is treated at http://stats.stackexchange.com/questions/35042/what-is-the-distribution-of-an-average-of-poisson-random-variables.",2013-02-08 03:17:58.850
72744,40104,,CC BY-SA 3.0,,"Why are you scaling the summands with $a_1$ and $a_2$? The sum is just another Poisson distribution without this. The variables take values in the positive integers, so something like $1$ times the first plus $\sqrt{2}$ times the second is usually quite unnatural, and would let you recover the values of both variables.",2013-02-09 19:57:47.617
72751,40104,4656.0,CC BY-SA 3.0,,"The difficulty here is that unless both $a_1$ and $a_2$ are integers, one cannot be sure that $S_2$ takes on integer values only. Thus, you need to find not just $P(S_2 = k)$ for integer values of $k$ but also $P(S_2 = \alpha)$ for each $\alpha$ that can be expressed as $a_1m + a_2n$ for nonnegative integers $m$ and $n$.",2013-02-09 21:30:33.927
72753,40104,14684.0,CC BY-SA 3.0,,@DilipSarwate Is that possible? Is there an other approach to do this?,2013-02-09 21:43:26.710
72754,40104,14684.0,CC BY-SA 3.0,,@DouglasZare I have to do this... Maybe I have to turn to some kind of bootstrapping method.,2013-02-09 21:45:04.850
72755,40104,4656.0,CC BY-SA 3.0,,"I don't think you can do much better than a brute-force approach which finds the possible values that $S_2$ can take on and then for each $\alpha$, use $$P\{S_2 = \alpha\} = \sum_{a_1m + a_2n = \alpha}P\{X_1=m\}P\{X_2=n\} = \sum_{a_1m + a_2n = \alpha} \exp(-\lambda_1m)\frac{\lambda_1^m}{m!}\exp(-\lambda_2n)\frac{\lambda_2^n}{n!}.$$ For most choices of $a_1$ and $a_2$, I would expect that most sums will reduce to a single term. I expect you know that for $a_1=a_2=1$, $S_2$ is a Poisson random variable with parameter $\lambda_1+\lambda_2$.",2013-02-09 22:01:52.060
72762,40104,,CC BY-SA 3.0,,"You can't provide more context than that you ""have to do this?"" Is it homework, possibly miscopied?",2013-02-09 23:51:40.077
72772,40104,14684.0,CC BY-SA 3.0,,"@DouglasZare Yes, I don't like to end up doing monte carlo simulations (bootstrapping), so I would really like to obtain at least an approximation.",2013-02-10 03:23:31.960
72790,40121,,CC BY-SA 3.0,user88,What kind of software is that?,2013-02-10 12:17:02.557
72806,40121,14728.0,CC BY-SA 3.0,,"@mbq, This is JMP. But is this comparison circle a general concept, not specific to the software?",2013-02-10 17:13:24.430
72816,40121,,CC BY-SA 3.0,user88,"This is a first time I see such plot, so I think it is native to JMP. BTW I bet SAS would sue for even thinking to implement this in other applications (-;",2013-02-10 19:53:01.127
72823,40121,14728.0,CC BY-SA 3.0,,"@mbq, I'm sure they'll do. But what applications are these plots for? Or what other criteria I can use to compare data? Basically I want to compare several data sets and find out which set is _significantly different_ from others, so I can exclude them in my future development.",2013-02-10 20:41:46.593
72828,40121,,CC BY-SA 3.0,user88,I suspect the red ones are outliers due to various methods and radii somewhat correspond to confidence intervals; but honestly I have no idea what this mean. However the question is now properly tagged so I hope some JMP expert will give you a satisfying answer soon.,2013-02-10 21:18:19.477
74195,40859,,CC BY-SA 3.0,,"The Frank Harrel quote is out of context. Please provide the source. Otherwise, splitting your data in the way you described seems appropriate. You may consider a split into training, test, and validation set to strengthen the validity of your model.",2013-02-22 09:36:04.627
74201,40859,15044.0,CC BY-SA 3.0,,Thanks @Bernhard. Sorry if it was out of context. I thought the example discussed in that thread was very similar to mine. Why do you think that was different?,2013-02-22 11:39:19.347
74213,40859,2666.0,CC BY-SA 3.0,,"No, the context would not help.  Data splitting for your sample size is not reliable.  I.e. you get different model and different test data performance if you split again, and the mean squared error of your performance metric is high.",2013-02-22 13:18:21.483
74218,40859,13037.0,CC BY-SA 3.0,,"You could split your data and do the testing many times: split into train and test, fit model, do diagnostics, and then repeat many times (make sure your new samples are different!) I think thats called like cross-validation or something",2013-02-22 13:37:34.623
74229,40859,15044.0,CC BY-SA 3.0,,"@FrankHarrell Thanks! What alternative to ""Data splitting"" would you suggest?",2013-02-22 16:29:05.890
74231,40859,2666.0,CC BY-SA 3.0,,"The bootstrap, as in the `validate` and `calibrate` functions in the R `rms` package.",2013-02-22 17:14:43.933
74924,41244,166.0,CC BY-SA 3.0,,Is this homework? If so then please tag it as such.,2013-03-01 05:23:11.897
74925,41244,166.0,CC BY-SA 3.0,,"Also please tell us your current thinking on this question, c.f.: http://stats.stackexchange.com/faq",2013-03-01 05:24:24.830
74940,41244,15330.0,CC BY-SA 3.0,,"This is not homework.
I've found some approaches on wikipedia (http://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair).
The bayesian approach depends on an arbitrary prior distribution, which seems bizarre. He recommends using a distribution centered on 0.5 but uses an uniform. The choice of the prior distribution is not so important? How can I trust a distribution that was chosen arbitrarily?
I don't understand how the frequentist approach might give me a distribution. 
A friend said that bootstrapping might give a good answer, is it true? How would that work?",2013-03-01 11:10:20.163
74943,41244,15330.0,CC BY-SA 3.0,,"Reading about priors, the article on wikipedia (http://en.wikipedia.org/wiki/Prior_probability) seems to recommend Jeffrey's prior (http://en.wikipedia.org/wiki/Jeffreys_prior#Bernoulli_trial) which is 1/sqrt[p(1-p)], although I didnt understand the explanation of why.",2013-03-01 11:43:03.910
74956,41244,15330.0,CC BY-SA 3.0,,The distribution function cannot be estimated from the coin results alone? It will always require a prior distribution? This is counterintuitive.,2013-03-01 15:12:47.037
74958,41244,594.0,CC BY-SA 3.0,,@LeoHMArruda Can you explain the context in which such a question arises (how did you arrive at this question?),2013-03-01 15:20:51.333
74960,41244,166.0,CC BY-SA 3.0,,The question you have posed is not directly about the fairness of the coin.,2013-03-01 15:47:24.230
74990,41244,15330.0,CC BY-SA 3.0,,Well it is basically the problem of finding out if a Bernoulli trial is biased and how much by repeating it many times. I was discussing it as a simplified model of hypothesis testing in a class.,2013-03-01 20:20:27.067
76103,41914,2164.0,CC BY-SA 3.0,,There is no published or described process of how the Lewandowski method works. It is a mystery.,2013-03-12 12:53:22.320
94842,52567,594.0,CC BY-SA 3.0,,"The origin of the term is pretty old, and I believe goes back to the origins of inferential statistics in analysis of experiments; in particular, I think it referred to the way that the X-matrix related to the actual experimental design (the specific settings of the $x$-values). If I can find a specific reference I'll post an answer.",2013-08-04 22:48:37.623
77195,42513,1741.0,CC BY-SA 3.0,,"Why not. Just to add a comment. The mean is a summary value as the histogram is. You can vary the degree of information provided varying the bucket size of the histogram for example. However, usually the histogram provides more information than just the mean. You can actually approximate the mean value from an histogram. I think that is why they are not usually provided together.",2013-03-19 22:53:58.093
77393,42513,,CC BY-SA 3.0,,"One sometimes sees histograms with an overlaid distribution (e.g. most commonly in my experience, the normal distribution plotted using the sample mean and standard deviation.) Which is doing the same thing (and a bit more) as drawing a vertical line (indicating where sample mean is with the peak of the curve.)",2013-03-20 23:27:38.200
78776,43458,,CC BY-SA 3.0,,"As @EngrStudent mentions below, 5 data points isn't much. But, could you perhaps describe your data a bit better?",2013-04-02 17:30:04.907
79611,42517,5237.0,CC BY-SA 3.0,,"+1, these are nice; care to add the code? `abline(v=mean(Davis2[,2]))` & `rug(Davis2[,2])` I would guess, but how did you wedge the boxplot in there?",2013-04-10 02:45:13.830
79613,42517,594.0,CC BY-SA 3.0,,"@gung See the edit for brief details, including a reproducible example similar to the one with the boxplot. It's really doing nothing more clever than making use of several of the arguments to the `boxplot` function. Between `boxplot` and `boxp` you can do some rather nifty things with little effort.",2013-04-10 03:29:02.947
79614,42517,5237.0,CC BY-SA 3.0,,"Wisdom for the ages: ""If you don't write an intelligent function to take care of everything, it's necessary to pay attention to what everything does to make sure it's doing what you want"" ;-).",2013-04-10 03:45:13.107
79615,42517,594.0,CC BY-SA 3.0,,"Yep. I even contemplated writing something clever to set `at` and `boxwex` and so on... but at best I only do a few plots like that a year, and it takes a few seconds each time to type ?boxplot and set the right options. I figured it's easier to just pay attention to what I am doing.",2013-04-10 04:14:13.197
79654,42517,594.0,CC BY-SA 3.0,,@gung I edited to give code to create the Davis2 data I was using. Hope that helps.,2013-04-10 08:55:36.807
80250,2509,,CC BY-SA 3.0,,"This question lead me to a good paper, and even though I think that is a great quote it is not from Einstein.  This is a common misattribution, and the more likely original quote is probably this one from Ernest Rutherford who said, ""If you can't explain your physics to a barmaid it is probably not very good physics.""  All the same thanks for starting this thread.",2013-04-15 15:03:37.020
80264,44370,,CC BY-SA 3.0,,Why would you look at the entire genome when you have a control for the region that you know you are interested in?,2013-04-15 16:20:08.277
80274,44370,8063.0,CC BY-SA 3.0,,"The main reason I think is that my modification may also affect distant genomic regions (some trans effect, I imagine if I hit a transcription factor, many genes elsewhere may be messed up). So while I expect local effect, I cannot exclude distal ones... Also but less importantly, I think it would make my statement about local effect much stronger if I could show that only that region is affected in the whole genome.",2013-04-15 21:22:22.763
80315,44370,,CC BY-SA 3.0,,Your idea is good.  You might save a little effort and see what happens when you normalize your DE count by gene density; i.e. $\frac{DE_i}{GD_i}$ before attempting the permutation testing.,2013-04-16 12:20:22.480
80659,44635,668.0,CC BY-SA 3.0,,"A more robust method would be to test for symmetry around the *median*: after all, when a distribution is symmetric about its mean, its median must coincide with its median. Robustness is desirable because a single outlier would cause the empirical distribution to look highly asymmetric around the sample mean but would barely affect the symmetry around the sample median.  But precisely how do you propose to apply the Wilcoxon test here?",2013-04-18 18:44:47.707
80660,44635,728.0,CC BY-SA 3.0,,"@whuber: (1) We can test the symmetry of a distribution around 0 by Wilcoxon sign rank test, based on its sample. This can be done, by letting $x_{1i} = 0, \forall i$ in [wikipedia](http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test).  (2) How do people tell if a testing procedure is ""valid"" for a task?",2013-04-18 18:49:02.607
80916,44772,,CC BY-SA 3.0,,"For clarity, did  the same two raters see all 18 participants for all four time points?  Did all 18 participants experience the same four-conditions? Finally, what is your research question: rater difference, condition difference, or time difference?",2013-04-20 21:16:40.430
80978,44772,17076.0,CC BY-SA 3.0,,"Yes. Both raters reviewed all 18 participants at the 4 points. I am interested in condition difference (perhaps controlling for practice effect over time), but using the raters to confirm scoring.  THANKS!!!",2013-04-22 00:24:37.880
81019,44772,,CC BY-SA 3.0,,"Try to understanding your data before you do an ANOVA. First, plot your data to ensure that it is normally distributed.  Then plot total score by rater to determine if there is a difference between raters.  Estimate mean and 95% CI for the mean for each rater. Then estimate mean and 95% CI for the mean for the average total score of the raters for each of the four conditions.  Check and see conditions don't have overlapping of the confidence intervals. Try doing boxplots with notches to visualize overlap. ANOVA will tell you if one of the conditions is different, but not which.",2013-04-22 12:55:44.710
81612,45280,503.0,CC BY-SA 3.0,,"With only 6 data points, each with noise, no statistical test is going to give significant results and there will possibly be many shapes that fit.",2013-04-26 18:08:51.113
81655,45280,594.0,CC BY-SA 3.0,,"If you had 6000 data points, identifying fairly general kinds of association should be possible. If you had 600 observations and made some weak-to-moderate assumptions about the general forms of association, you might get somewhere. If you had 60 observations and made fairly strong assumptions about particular forms of association it might be doable. On six observations with large error? I doubt there's any way you're going to achieve much; even something as explicit as monotonic association is going to be nigh impossible to show if the errors are large.",2013-04-27 03:07:55.383
81900,45457,17179.0,CC BY-SA 3.0,,R for statistical computing,2013-04-29 17:46:41.690
82042,45536,17447.0,CC BY-SA 3.0,,"Could you explain the stats behind it? (im a first year so im not the best)

I understand theres a pdf, f(c) which has a CDF F(c)

and then they go onto talk about F(L)

what is the function F in F(L) here? is this a cdf?
  
and more importantly, the coursework question is, ""what does it imply when F(L) < 0.4""",2013-04-30 15:46:12.873
82044,45536,15663.0,CC BY-SA 3.0,,"I d'ont use these words but yes, i think pdf = distribution and cdf = cumulative function. F is still the cumulative function for C. As $F(infinity) = 1$, $F(L) < 1$ strictly means that $P(c>L)>0$, that there is cost superior to the fine. This does NOT strictly means that the fine is lower to the compliance cost. But as far as I don't have the question I'am not sure wether it's an hypothesis you want to make or something you wnat to show.",2013-04-30 16:01:48.870
82048,45536,15663.0,CC BY-SA 3.0,,"$F(L) < 0.4$ means that c has <40% chance to be below L and >60 to be above. Express what does it mean for the expected profit of the company, the environmental damage.",2013-04-30 16:27:55.377
103589,57200,594.0,CC BY-SA 3.0,,"You (almost) always will do a good deal better on the training data than the test data, since you optimize the fit on the training data.",2013-10-10 08:52:07.413
82051,45536,17447.0,CC BY-SA 3.0,,"hey imorin, ive attached the pdf file here
[link](http://www.scribd.com/doc/138719032/Anthony-Heyes-and-Neil-Rickman)

could you help me figure this out. there are 2 equations i do not understand how they were derived. none of my coursemates have chosen this topic so i cant get help anywhere else.


could you explain equation (3) and (4) to me?

and could you tell me how i should structure my answer with regards to what F(L) < 0.4 implies?",2013-04-30 16:39:50.690
82064,45543,668.0,CC BY-SA 3.0,,@Andre Silva http://en.wikipedia.org/wiki/Metric_%28mathematics%29.,2013-04-30 18:37:13.613
82103,45534,17447.0,CC BY-SA 3.0,,"@Glen_b alright i signed up for this website today so i wasnt aware of it, my other question which was legit just get downvoted and i was wondering why, makes sense now, thanks for the heads up",2013-05-01 01:47:35.990
82105,45534,594.0,CC BY-SA 3.0,,You can edit your questions to better follow the guidelines. That may help.,2013-05-01 01:55:23.953
82273,45536,15663.0,CC BY-SA 3.0,,I'm sorry but I can reach your pdf. Probably due to my internet proxy,2013-05-02 14:24:07.263
82294,45279,,CC BY-SA 3.0,,"If you find a solution that satisfies you, you can also post your code as an answer",2013-05-02 16:31:09.013
82303,45279,750.0,CC BY-SA 3.0,,This is turning into something like [parsets](https://code.google.com/p/parsets/). See [ggparallel](http://cran.r-project.org/web/packages/ggparallel/index.html) for an R implementation.,2013-05-02 17:23:19.523
82393,45279,674.0,CC BY-SA 3.0,,"Until I noticed @Andy's comment, I was thinking of something like a [clustergram](http://www.r-statistics.com/2010/06/clustergram-visualization-and-diagnostics-for-cluster-analysis-r-code/) (with subjects' ID vs. no. clusters) or maybe a [streamgraph](http://en.wikipedia.org/wiki/Streamgraph) (probably less appealing if you have few clusters). This, of course, assume you are willing to work at the individual level.",2013-05-03 11:39:42.770
85258,47497,18382.0,CC BY-SA 3.0,,"in the first line I don`t write ""family=""yjPower"".",2013-05-28 08:56:46.570
85261,47497,,CC BY-SA 3.0,,"Hi Daniel, welcome to the site. You don't have to code it yourself. The Yeo-Johnson transformations are implemented in the [`car` package](http://cran.r-project.org/web/packages/car/car.pdf) with the function `yjPower`. So just use `yjPower(datc$plot, lambda=lambda.max, jacobian.adjusted=FALSE)`. I think that should work.",2013-05-28 09:12:01.920
85262,47497,18382.0,CC BY-SA 3.0,,"Thanks @ COOLSerdash I tried, but the problem is in the fist line:",2013-05-28 09:24:43.480
85263,47497,18382.0,CC BY-SA 3.0,,"lambda.fm1 <- boxcox(datc$plot, ... doesn´t work because  datc$plot contains zeros",2013-05-28 09:26:30.017
85264,47497,,CC BY-SA 3.0,,"Okay, then try to use the function `boxCox` from the `car` package and use it with the option `family=""yjPower""`.",2013-05-28 09:26:34.570
85290,47497,594.0,CC BY-SA 3.0,,See the `yeo.johnson` [function](http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/VGAM/html/yeo.johnson.html) in the package `VGAM` as well. It is on CRAN.,2013-05-28 12:07:01.123
86035,34166,,CC BY-SA 3.0,,"This is nice because it's a really easy way to explain the different interpretations of probability (objective vs. personal) to someone with no experience - i.e. is the coin fair, or how many possible ways to wake are there? Obviously there's tonnes of ways to make nuanced arguments either way... Somehow this seems to me more like a rabbit hole than a paradox :D",2013-06-03 01:52:11.707
86204,47981,15827.0,CC BY-SA 3.0,,"0.04993 < 0.05, so it's just lower. Your instinct is good that no P-value can be trusted to several decimal places, but if the program says less than 0.05, people generally take it as delivered. The real issue here is making a fetish of fixed-level significance testing so that < 0.05 means ""real"", ""publishable"",  ""cause for happiness"" and the opposite means ""illusory"", ""not publishable"", ""cause for misery"". Most good introductory texts on statistics discuss this to some extent. One good one is Freedman, Pisani, Purves, _Statistics_. New York: W.W. Norton, any edition.",2013-06-04 09:35:35.103
86205,47981,,CC BY-SA 3.0,,You have to ask yourself what would be your decision if the p-value is 0.051? what if it is 0.049? Would you make different decisions? Why?,2013-06-04 09:43:12.570
86207,47981,16990.0,CC BY-SA 3.0,,"Thank you for your comments. In our case we are not pondering whether the data is publishable or not, etc... We are simply considering making a statement in the paper about the statistical significance of this result, and we want to make sure our statement is not incorrect or inaccurate.",2013-06-04 10:52:23.860
86210,47981,15827.0,CC BY-SA 3.0,,"Reporting P=0.04993 is what springs to mind. It's difficult to predict reviewers' or editors' comments. If you want to round, specifying a consistent rounding convention is always a good idea and widely acceptable. Some people would round to 3 d.p. and might also use some kind of starring convention so reporting 0.050 (3 d.p.) and starring it as <0.05 are consistent.",2013-06-04 11:27:28.397
86219,47981,,CC BY-SA 3.0,,What is the W value of the statistic? What would the corresponding critical value of your p-value be? How far are those apart?,2013-06-04 13:58:51.567
86229,47981,,CC BY-SA 3.0,,"@IslamEl-Nabarawy since the significance level is arbitrary anyway, if you have defined it at 5% then yes the p-value is significant by your definition of it. At the same time, I think AlefSin makes a good point.",2013-06-04 16:01:59.183
86391,48103,,CC BY-SA 3.0,,You're trying to fit a sine wave to the data or are you trying to fit some kind of a harmonic model with a sine and a cosine component?  There is a harmonic function in the TSA package in R that you might want to check out.  Fit your model using that and see what kind of results you get.,2013-06-05 18:31:29.100
86392,48103,16992.0,CC BY-SA 3.0,,"Have you tried different starting values? Your loss function is non-convex, so different starting values can lead to different solutions.",2013-06-05 18:34:11.083
86394,48103,15827.0,CC BY-SA 3.0,,"Tell us more about the data. Usually there is a known periodicity, so that need not be estimated from the data. Is this a time series or something else? It is much easier if you can fit separate sine and cosine terms by a linear model.",2013-06-05 18:46:34.350
86411,47981,16990.0,CC BY-SA 3.0,,"@NickCox: We reported all the results to 4 d.p., and in the text we noted that while it is lower than 0.05, it was only by a very narrow margin.",2013-06-05 21:45:33.733
86426,48103,594.0,CC BY-SA 3.0,,"Having an unknown period makes your model nonlinear (such an event is alluded to in the selected answer at the linked post). The given that, the other parameters are conditionally linear; for some nonlinear LS routines that information is important and can improve the behaviour. One option might be to use spectral methods to get the period and condition on that; another would be to update the period and the other parameters via a nonlinear and linear optimization respectively in an iterative fashion.",2013-06-05 23:13:14.360
86427,48103,594.0,CC BY-SA 3.0,,(I just edited the answer there to make the particular case of unknown period an explicit example of what can make it nonlinear.),2013-06-05 23:19:06.557
94847,52567,728.0,CC BY-SA 3.0,,"@Glen_b: Thanks! Does ""design"" have something to do with choosing a transform on the input variable, so that the output variable is also linear in the transformed input variable? For example, the design matrix in polynomial regression?",2013-08-05 00:04:27.643
94854,52567,668.0,CC BY-SA 3.0,,When you design an experiment you specify the values of $X$.,2013-08-05 00:49:16.323
86428,48103,594.0,CC BY-SA 3.0,,"Because the other parameters can be estimated linearly, you can plot $S$, the sum of squares of residual (SSE), against $\omega$ for a wide range of $\omega$ (for each $\omega$ over some set - say a grid or whatever around a sensible start value - use LS to estimate the other parameters and hence $S$); refine the detail in the more interesting areas of $\omega$ (better values of $S$). This lets you optimize $\omega$ without worrying about $-S$ being unimodal.",2013-06-05 23:24:17.103
86455,48133,,CC BY-SA 3.0,,(+1) nice answer. I tried to fit the linear model with `lm(y~sin(2*pi*t)+cos(2*pi*t)` but this didn't work (`cos` term was always 1). Just out of curiosity: what do the first two lines do (I know that `spectrum` estimates the spectral density)?,2013-06-06 08:44:28.790
86458,48133,594.0,CC BY-SA 3.0,,"@COOLSerdash Yeah, you have to have the units of $t$ being the period (as it was in the linked question) for `2*pi*t` to work. I should go back and emphasize that in the other answer. (ctd)",2013-06-06 10:06:39.880
86459,48133,594.0,CC BY-SA 3.0,,"@COOLSerdash (ctd)- The 2nd line finds the frequency associated with the biggest peak in the spectrum and inverts to identify the period. At least in this case (but I suspect more widely), the defaults on it essentially identifies the period that maximizes the likelihood so closely that I deleted the steps I had in to maximize the profile likelihood in the region around that period. The function `spec` in TSA may be better (it seems to have more options, one of which may be important sometimes), but in this case the main peak was in exactly the same place as with `spectrum` so I didn't bother.",2013-06-06 10:07:13.630
87234,48597,5237.0,CC BY-SA 3.0,,"How many variables did you measure / test?  Do you think of them as related to each other, or are they independent?",2013-06-13 05:13:56.340
87235,48597,594.0,CC BY-SA 3.0,,"With the 'probability test', do you think he might be referring to [Fisher's method](http://en.wikipedia.org/wiki/Fisher%27s_method)? If so, you'd need the various response variables to be independent, which I would usually doubt.",2013-06-13 05:39:14.840
87236,48597,594.0,CC BY-SA 3.0,,"You say ""T-test"" in your title but ""t-test"" in your body text. Did you do several univariate two-sample t-tests, or did you do a single multivariate [T-test](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.bsmsp/1200500217)? (See also [this](http://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution#Hotelling.27s_two-sample_T-squared_statistic) and [this](http://faculty.smu.edu/kyler/courses/7314/Hotellings_T.pdf) and [this](http://www.psych.yorku.ca/lab/psy6140/lectures/HotellingT2-2x2.pdf))",2013-06-13 05:40:27.923
87673,48103,11200.0,CC BY-SA 3.0,,"Ok guys, it's been a while but I solved the problem (at least to my satisfaction). What I did is to take the linearization (see linked thread) and the to just loop through all the possible periods and calculate the goodness of fit. By that value I now chose the period which fitted the data best. It's a real low level solution but it worked fine for my purpose.",2013-06-17 12:36:50.907
87845,37981,594.0,CC BY-SA 3.0,,"Indeed, given the particular way I phrased it in my earlier comment, it's true even of symmetric distributions - the square of the sample standardized third moment (squared moment skewness) is highly correlated with the sample standardized fourth moment ('kurtosis'), even at say the normal.",2013-06-19 00:27:14.723
88040,2509,15827.0,CC BY-SA 3.0,,"Alice Calaprice, _The ultimate quotable Einstein_, Princeton U.P. 2011 flags the quotation here as one of many ""Probably not by Einstein"". See p.482.",2013-06-20 10:01:41.333
89173,31587,6404.0,CC BY-SA 3.0,,I'd really appreciate it if you could comment on whether you think it is possible to create a confidence interval for the transition probabilities?,2013-06-30 08:38:16.763
90150,49879,594.0,CC BY-SA 3.0,,"That appearance might be a consequence of the intervals that were chosen, or of something else. There's really very little to go on here.  When you say ""my estimates"" you suggest the depicted estimates are yours, but your question earlier suggests the slides belong to someone else. Are the estimates yours, and if not, why call them yours?",2013-07-07 04:48:11.837
91311,49879,19492.0,CC BY-SA 3.0,,"@Glen_b no, I just were talking from my point of view, so these are not my estimates.",2013-07-13 07:54:31.493
91413,50739,503.0,CC BY-SA 3.0,,"If A, B and C are separate groups (not levels of a continuous variable) then you can't really draw lines between them, or place them equidistant on the x-axis.",2013-07-14 11:13:58.673
92286,49906,15473.0,CC BY-SA 3.0,,"It is interesting! The two methods should have similar results according to the theory. If I understand the question correctly, the null hypothesis of McNemar's test is $p_b=p_c$, while the null hypothesis of the test of conditional logistic regression is odds ratio $ad/bc=1$ within a stratum.",2013-07-18 21:29:58.033
92328,49906,5821.0,CC BY-SA 3.0,,"I seem to recall that one can parameterize the McNemar's test as a test of an odds ratio, so I wonder how one would write out the likelihood (conditional likelihood?) for that test.",2013-07-19 04:51:41.013
92517,49906,15473.0,CC BY-SA 3.0,,"I am not sure if you mean the exact version of McNemar's Test. [Breslow and Day (1980)](http://w2.iarc.fr/en/publications/pdfs-online/stat/sp32/SP32.pdf), p. 164-166 and [package](http://cran.r-project.org/web/packages/exact2x2/vignettes/exactMcNemar.pdf) `exact2x2` may be references.",2013-07-19 22:10:19.587
92675,50982,,CC BY-SA 3.0,,"I think the term *bias-variance tradeoff* does not apply here, because you are not decomposing a mean squared error into a bias and a variance, and you are not talking about the variance of an estimator but about the variance of a score.",2013-07-21 10:48:21.457
92699,50982,1790.0,CC BY-SA 3.0,,"Thanks @Lucas. I am trying to estimate the score of my classifiers $A$ and $B$ on *unseen* data. For this, I thought I could take the mean of scores on *seen* data as my estimators (i.e. $E(P_A)$ and $E(P_B)$ for $A$ and $B$ respectively). Is the variance of these estimators different from the variance of the scores $P_A$ and $P_B$ ?",2013-07-21 16:29:30.840
92756,50982,,CC BY-SA 3.0,,"@user815423426 I think the comparison depends on the loss function you have. Diebold and Mariano (2002) have a nice paper studying your question. They proposed some statistical tests comparing the ""generalization"" performance. I don't know how to set up a link in comments. The paper is: Diebold, Francis X., and Robert S. Mariano. ""Comparing Predictive Accuracy."" Journal of Business & Economic Statistics 20.1 (2002): 134-144.",2013-07-22 03:24:43.083
93226,51644,9049.0,CC BY-SA 3.0,,"I guess you want to say ""from model `m3` it is 0.0011713"" instead of `m2`.",2013-07-24 12:57:34.287
93229,51644,3733.0,CC BY-SA 3.0,,"I am sorry @user11852, yes you are correct, thanks. (BTW, for `m2` it is valid also (which is subject of [another question](http://stats.stackexchange.com/q/65386/5509)).",2013-07-24 13:17:10.070
94049,2509,2081.0,CC BY-SA 3.0,,A [link](http://stats.stackexchange.com/a/65817/3277) to a geometrical account of PCA vs regression vs canonical correlation.,2013-07-29 20:43:09.140
94400,43458,,CC BY-SA 3.0,,"@JasonMorgan i suspect that with datapoint, he refers to a single time they measured the sample. Not to the number of observations within a single measuring session.",2013-08-01 10:55:30.050
103394,57086,1805.0,CC BY-SA 3.0,,"Make a prediction equation for each tree (it will be simple split points), and then average the predictions from each equation?  You'll get one monster equation, but it will fully represent the forest.",2013-10-08 18:56:02.367
95452,52910,668.0,CC BY-SA 3.0,,"It would appear that condition numbers are used here merely as a surrogate for the sample size, so why don't you just focus on sample sizes? I am baffled by two aspects of this question. The first concerns the nature of your asymptotics: what are you doing that assures the condition numbers won't grow without bound? The second is why a ""typical"" condition number in any field would even matter, since there is no general connection between it and the inferences one would like to draw.",2013-08-09 14:29:11.350
95464,52910,3731.0,CC BY-SA 3.0,,"@whuber Thanks for the comment. (1) My goal is understanding sample size via condition numbers. (2) In a simple 2-level hierarchical model, with indep. groups and equal correlation within groups, then bounding the size of the largest group bounds the condition number (e.g. adding more ""schools"", but not more ""students per school""). (3) From my (limited) understanding, condition numbers are proxies for the expected numerical error. My naive assumption is that different fields will have orders of magnitude differences (e.g. IQ tests v. diameters of tree trunks). Do you believe that to be false?",2013-08-09 15:10:14.397
95471,52910,3922.0,CC BY-SA 3.0,,"I have seen everything from 10 to 10,000. It might be true that in well designed experiments, the condition numbers will be close to 1. It is definitely true that there is no single ""social science"" number to talk about.",2013-08-09 15:48:35.570
95475,52910,668.0,CC BY-SA 3.0,,"@StasK I have seen *infinite* values but they didn't matter: they arise when there are collinearities among variables that don't affect the parameters of interest. That's the basis of my concerns about this approach: although it's true that high condition numbers (CN) create numerical instability (a practical issue) and large standard errors for *some* coefficients (a theoretical and practical issue), what matters is whether those inflated SEs are pertinent to the investigation objective. I therefore don't see how it would be possible to establish any meaningful kind of ""typical"" CN threshold.",2013-08-09 16:15:14.267
95488,52910,3922.0,CC BY-SA 3.0,,"@whuber, I agree that there is little in the ways of providing the single best number; I assumed that the OP has weeded out perfect multicollinearities. Also, there may be condition numbers for the raw data, but then you can start adding nonlinear terms (interactions; polynomials, splines, etc.) that would affect the CN actually encountered. For non-linear models like logit, the CNs on the parameter estimates would not be the same as the CN for the regressors. Finally, there are also multilevel models in which information set differ for different parameters, producing really weird CN patterns.",2013-08-09 18:17:17.227
95510,52910,3731.0,CC BY-SA 3.0,,"@StasK So then the short answer to my question is: No. People do not normally compile these things because they are specific to both the particular model chosen and the particular data onto which the model is fit. To talk of a ""typical"" number is not well posed.",2013-08-09 20:34:34.693
96075,53264,449.0,CC BY-SA 3.0,,Given that no one else has come up with an alternative I'll mark this correct. However Patrick it's entirely possible this is an (misapplied) $\omega^2$. The $\eta^2$ better suits a typo I think (upside down $\mu$?).,2013-08-14 21:25:20.470
96227,53384,,CC BY-SA 3.0,,It will be clearer if you show the code that you have done so far.,2013-08-15 23:01:21.997
96228,53384,20838.0,CC BY-SA 3.0,,"I actually haven't coded anything so far in terms of bootstrapping. The code for my model is pretty complex, I don't thank that would help. As an example, we can assume that the model is a smoothing procedure like a moving average, with the moving window as the only model parameter. I have a series of (synthetic) measurements over time and add an error (not necessarily homoskedastic and normally distributed) to that. I then want to estimate the moving window which comes closest to the underlying ""true"" I know and want to assess uncertainty by bootstrapping my synthetic error. Does that help?",2013-08-15 23:09:27.523
96230,53384,20838.0,CC BY-SA 3.0,,"Here's some very bad MATLAB-style pseudo code, maybe it helps understand what I'd like to do: http://pastebin.com/yTRahzr5",2013-08-15 23:24:48.637
96231,53384,,CC BY-SA 3.0,,"Sorry Fred, I don't know Matlab.Please tag as Matlab to get inputs from users.",2013-08-15 23:27:55.787
96232,53384,20838.0,CC BY-SA 3.0,,"Oh my question really isn't limited to MATLAB (and that isn't really MATLAB code, it's just some pseudo-code based on MATLABs syntax for for-loops and comments that wouldn't work anyway). But I can tag it just in case.",2013-08-15 23:32:52.950
96268,53404,166.0,CC BY-SA 3.0,,Some questions to guide your thinking... What does a very negative t statistic mean?  Is a negative F statistic possible?  What does a very low F statistic mean?  What does a high F statistic mean?,2013-08-16 06:41:29.950
96271,53404,,CC BY-SA 3.0,,Why are you under the impression that a one-tailed test has to be an F-Test? To answer your question: The F-Test allows to test a hypothesis with more than one linear combination of parameters.,2013-08-16 07:17:40.957
96279,53404,19125.0,CC BY-SA 3.0,,Do you want to know why one would use a one-tailed instead of a two-tailed test?,2013-08-16 07:53:59.867
98029,31587,,CC BY-SA 3.0,,"@Zen Suppose instead of finding state transition matrix for 1 history step I wish to calculate for 2 history step.
P(State1|(State1, State2)): Probability of getting state1 given t-1 was state1 and t-2 was state2. How do I calculate these probabilities?",2013-08-30 07:09:58.817
98046,31587,7007.0,CC BY-SA 3.0,,"If it is a Markov chain, then $P(1\mid 1,2)=P(1\mid 2)$. So, there is nothing new. If it is not a Markov chain, keep a record of the $n^3$ triple of three consecutive transitions, simulate the process and compute the fraction of each triple.",2013-08-30 11:47:56.480
98500,54637,,CC BY-SA 3.0,,You might want to check out information about p-value meta-analysis. One good starting point: http://en.wikipedia.org/wiki/Fisher%27s_method,2013-09-04 07:00:37.703
98533,54624,,CC BY-SA 3.0,,"The R commands are either lme() or lmer(). Both are mixed effects models. lmer() is newer and preferred, although lme() seems to work fine for most applications. Pinheiro and Bates - Mixed Effects Models in S and S-Plus covers both the theory and applications of these models in R pretty well.",2013-09-04 12:12:46.180
98603,54724,17740.0,CC BY-SA 3.0,,Generalization error is far from trivial. Unfortunately rules of thumb don't help much in this regard.,2013-09-04 20:28:43.960
99036,54915,5237.0,CC BY-SA 3.0,,"I don't think this question needs to be migrated to SO necessarily, b/c the OP wants to know if his code matches the how the test is supposed to work. Ie, the OP is asking about the *ideas* primarily. It's a subtle distinction, & admittedly, someone will have to know both R & the Pettitt test to answer, but I think this is a stats question, not a coding question.",2013-09-08 14:08:47.190
99055,54915,668.0,CC BY-SA 3.0,,"@gung OK, I won't vote to close, but I must ask Raz_Lobo: please explain how you know your code is giving incorrect output. (Warning to readers: the two pdf links in the question appear to have very long load times.)",2013-09-08 16:22:50.893
99059,54915,668.0,CC BY-SA 3.0,,"Raz_Lobo, your second link does not load (at least not for me) and the formulas in the first one on pp 5 and 6 are clearly incorrect. For a more accurate account of the Pettitt test (published in a reputable journal), please see Equation 5 in http://www.stat.purdue.edu/~zhanghao/ASS/Reference%20Papers/Temporal%20and%20Spatial%20variability%20of%20annual%20water%20level.pdf.",2013-09-08 16:30:23.080
99145,54915,21523.0,CC BY-SA 3.0,,"@whuber I see these other equation you appoint early in other paper, but, really, I don't know how to implement it, where is a max in a oscilating function between -1 < x < 1? You can see the equations I implemented in this other paper: ""http://www.homogenisation.org/files/private/WG1/Bibliography/Applications/Applications (P-S)/sahin_etal_2010.pdf"" (I think that this is reputable, only links other because them reflects level of significance equation) In order to check code is giving incorrect output, you can see  table IX in the paper I link.",2013-09-09 09:34:46.857
99165,54915,,CC BY-SA 3.0,anon,"Are you purely interested in implementing the Pettitt test in R or do you have some applications in mind, also? If your aim is to produce an implementation of the test, could you consider getting the original article from JSTOR (http://www.jstor.org/discover/10.2307/2346729?uid=2129&uid=2&uid=70&uid=4&sid=21102620240847) which would probably describe the test in more details. If you are more interested in applying the test in some research setting, have you checked the cpm package that contains an implementation of the Mann-Whitney test (similar to Pettitt test) using a simulation approach?",2013-09-09 13:39:28.897
99286,35249,,CC BY-SA 3.0,,"Slightly off-topic, but there's a [geoscience proposal on area51](http://area51.stackexchange.com/proposals/36296/geoscience) that's currently in the commitment phase. While this is certainly a stats question, the fact that it's so field specific means you might get better help there or pointers to solutions that CV users might not be aware of. So go and sign up! :D",2013-09-10 08:23:13.240
99288,54915,21523.0,CC BY-SA 3.0,,"@JTT I'm trying to apply Sahin et al metodology to my own data, then yes, I interested in implement Pettitt test and, yes, in R. Finally, a friend I ask, can found original article: [link] ftp://oceane.obs-vlfr.fr/pub/irisson/papers/Pettitt1979-A%20non-parametric%20approach%20to%20the%20change-point%20problem00.pdf I think that, with original article, I'll could implement it. At respect of cpm package, I tryed it but I can't compile it.",2013-09-10 08:42:39.803
99315,55043,,CC BY-SA 3.0,,Your intuition seems related to “parallel analysis”.,2013-09-10 13:28:20.927
99441,35249,503.0,CC BY-SA 3.0,,"Have you looked into functional data analysis? I am very far from an expert in that, but the little I know suggests it might be useful here. See e.g. [this book](http://www.textbooks.com/BooksDescription.php?BKN=773525&SBC=ME3&kpid=9780387400808U&network=GoogleShopping&tracking_id=9780387400808U&utm_medium=cpc&utm_term=9780387400808U&utm_source=googleshopping&kenshu=2275d976-eb91-6b88-5311-00004cb31ff0&gclid=CIGl0ceWw7kCFaYDOgodFxIA6A)",2013-09-11 10:52:36.547
99559,55150,9175.0,CC BY-SA 3.0,,"You said that $E(XE(Y|X))=E(E(XY|X))=E(XY)$. I believe this is wrong. E(Y|X) is a constant. Therefore, $E(XE(Y|X))$ is equal to $E(Y|X)E(X)$. Another point, $E(Y|X)=b0+b1*X$ comes from the simple linear regression model.",2013-09-12 01:36:15.333
99561,55150,21630.0,CC BY-SA 3.0,,"Let E(Y|X) = b, where b is a constant. Then take expectations of both sides. One finds that E(E(Y|X)) = E(b) = b. By law of iterated expectations, E(E(Y|X)) = E(Y). Therefore, if E(Y|X) is constant, it must be equal to E(Y).",2013-09-12 01:38:27.553
99578,55150,,CC BY-SA 3.0,,"If E(Y/X)=b, that's implies Y does not depend on X, and E(Y)=b, you are confusing yourself.",2013-09-12 05:39:57.397
99588,55150,11489.0,CC BY-SA 3.0,,"I don't understand why ""this makes no sense"". You are starting off with a definition of causality that is I think equivalent to definition of independence in statistics. And independent variables have zero covariance, where is the story?",2013-09-12 06:17:54.650
99589,55150,21630.0,CC BY-SA 3.0,,"January, no, they are not the same thing! X and Y are independent if the joint distribution factors into the product of the marginals, and this is definitely not the same thing. I don't see what your point is? Azeem, aside from restating what I previously said, do you have anything to contribute? Instead of saying I am wrong, can you explain WHY I am wrong?",2013-09-12 06:41:03.020
99592,55150,16474.0,CC BY-SA 3.0,,"In your set up you ignore the possibility of confounding variables etc., which is an OK first step if you are trying to get your head around a complicated concept like causality. In that case, I don't see a problem with the statement that no causal relation between $X$ and $Y$ implies no correlation between $X$ and $Y$. In your world there are only two variables, and they are either related or not. If they are not related then, in your world, there is no other mechanism that could result in a correlation between the two.",2013-09-12 07:41:33.470
99603,55150,21630.0,CC BY-SA 3.0,,"Thank you Maarten Buis! That helps a lot! However, I am interested in something more mathematically rigorous. Let Z be another variable. Consider $E(Y|X,Z)$. Now, X is causally unrelated to Y iff X does not influence $E(Y|X,Z)$, i.e., $E(Y|X,Z) = E(Y|Z)$ (using the same logic). As before, $E(XY) = E(E(XY|X,Z)) = E(XE(Y|X,Z))$ which is only slightly more complicated. But now the assumption $E(Y|X,Z) = E(Y|Z)$ means that $E(XY) = E(XE(Y|Z))$, which is not, in general, equal to $E(X)E(Y)$. Therefore, in the presence of possible confounding factors, no causation does not imply no correlation.",2013-09-12 10:52:51.493
99605,55150,21630.0,CC BY-SA 3.0,,I'm still not entirely happy with this argument. What do you think?,2013-09-12 10:55:43.037
99606,55150,11489.0,CC BY-SA 3.0,,"@Christian, if X and Y are independent, then $E(Y|X)=E(Y)$, this is simple to prove (I'm not sure about the other way, though, meaning that independence is a special case of not having causal relationship as per your definition). Why do you expect that two variables for which $E(Y|X)=E(Y)$ have $cov(X,Y) \ne 0$? What is so unexpected about them having zero covariance?",2013-09-12 10:58:45.620
99608,55150,11489.0,CC BY-SA 3.0,,"OK, wait a sec. Actually, $E(Y|X)=E(Y)$ iff $cov( X, Y ) = 0$, right? So I think that you just found a fancy name for a non-zero covariance...",2013-09-12 11:36:26.467
99621,55182,21630.0,CC BY-SA 3.0,,Thank you sincerely. I will have a read of his work and get back to you when I have time.,2013-09-12 13:45:24.580
99668,55182,5045.0,CC BY-SA 3.0,,"Excellent answer. The [Morgan & Winship book](http://books.google.com/books?id=lFTP49c5wJoC&lpg=PP1&pg=PP1#v=onepage&q&f=false) is quite a bit easier than Pearl, with a focus on social science problems.",2013-09-12 16:53:16.847
99762,55150,21630.0,CC BY-SA 3.0,,"@January, I don't know about the converse. Can you please provide a mathematical proof that $Cov(X,Y) = 0$ implies $E(Y|X) = E(Y)$?",2013-09-13 11:10:20.757
99769,55150,11489.0,CC BY-SA 3.0,,"@Christian I think it is straightforward and it feels correct, but I don't have time to work it out right now; do you have a counter-example?",2013-09-13 11:28:23.717
99868,55150,21630.0,CC BY-SA 3.0,,I (confidently) suspect it is not true.,2013-09-14 09:57:43.040
100433,55576,,CC BY-SA 3.0,,This is a cross-post http://stackoverflow.com/questions/18871792/generating-random-data-based-on-partial-correlation Please decide whether it belongs here (as I'd say) or on SO and ask a moderator to migrate or perhaps close it.,2013-09-18 12:40:38.503
100451,55576,668.0,CC BY-SA 3.0,,"Could you please explain precisely what you mean by a ""partial correlation matrix""? Is this a correlation matrix, a correlation matrix with missing entries, or a matrix of partial correlations?",2013-09-18 13:54:16.090
103395,57086,22034.0,CC BY-SA 3.0,,"Good idea @Zach. But unfortunately I'm trying to avoid anything ""monster.""",2013-10-08 18:57:21.623
100478,55576,668.0,CC BY-SA 3.0,,"@Irish Thank you.  Your interpretation might be correct or it might not: it assumes this is a question about time series, even though time series have not been mentioned or tagged. (The value of 168 = 7*24 certainly is suggestive.) I want to hear from the *original poster* concerning his question rather than guesses (no matter how intelligent or well-meaning) from others.",2013-09-18 16:07:36.940
100527,55617,,CC BY-SA 3.0,,You could convert the ranks into preference scores that come from a normal distribution: http://www.ats.ucla.edu/stat/stata/faq/prank.htm,2013-09-18 20:08:59.427
100750,55722,,CC BY-SA 3.0,user25658,What level of text are you looking for?  I think that Degroot book is aimed more at undergraduate students.  A good book for graduate level studies is Statistical Infernece by Casella and Berger.,2013-09-19 22:17:19.853
100776,55722,668.0,CC BY-SA 3.0,,"This definition of ""self sufficient"" is subjective, because your ability to ""understand the book"" depends on your background.",2013-09-20 01:44:27.580
100806,55576,21833.0,CC BY-SA 3.0,,@Momo Apologies for cross posting. Will look into that.,2013-09-20 06:11:44.113
100807,55576,21833.0,CC BY-SA 3.0,,"@whuber What I meant by a partial correlation matrix is a matrix that has partial correlations in it (calculated for any two pairs of entries by partialling out all other pairs. In your words ""a matrix of partial correlations"". Yes, this is regarding time series as you have rightly pointed out. It is on the lines of back calculating a time series (168*12) if I have a pre-defined matrix having partial correlation data.",2013-09-20 06:12:14.773
100821,55722,,CC BY-SA 3.0,,I'm guessing that there is no book that you will find completely satisfactory.,2013-09-20 09:18:09.120
100831,55617,21846.0,CC BY-SA 3.0,,"This seems reasonable to me. However, I am a bit confused by their example. They say that ""The z-scores will be normally distributed with mean equal to zero and a standard deviation of one."" but the inverse normal transformation they apply actually results in scores with a standard deviation of 1.486. Am I missing something or is there an error in the example?",2013-09-20 10:37:47.500
100836,55722,21885.0,CC BY-SA 3.0,,Self sufficient given the knowledge that you have after obtaining a bachelor in mathematics. With regards to the topics Degroot is what I am looking for but I don't like books in which core results (e.g. chi square distribution of the test statistics given the null hypothesis is true for the likelihood ratio test) are not derived. I will have a look at Statistical Inference by Casella and Berger.,2013-09-20 11:25:44.337
100927,55722,10448.0,CC BY-SA 3.0,,[Here](http://bayesianthink.blogspot.com/2012/12/the-best-books-to-learn-probability.html#.Ujzq9BVx05k) is a good list of books for to learn probability and statistics. There may be German versions for these books but I'm not sure. HTH,2013-09-21 00:41:50.377
100935,55722,594.0,CC BY-SA 3.0,,"How can a book on probability and statistics ever be *complete*? Even huge multi-volume tomes (Kendall and Stuart's .. etc's *Advanced theory of Statistics* in its latest incarnations, for example, come to thousands of pages if I recall correctly) aren't remotely complete.",2013-09-21 01:36:14.747
101555,541,,CC BY-SA 3.0,,"Upvoted your comment, but experimentalists are even crazier than I thought if this is syntactic sugar for them! Which version is more intuitive.... ANOVA hypothesis test on $\beta$: is the ratio of explained variance to the unexplained variance sufficiently high? T-test on the $\beta$ term of a regression model: is the effect of $\beta$ sufficiently different from zero? And, with the latter formulation you also get the direction of change. And, if you had to transform the data, you can back-transform the parameter estimate into a physically meaningful quantity. Unlike SS.",2013-09-25 20:44:50.790
101769,56273,503.0,CC BY-SA 3.0,,"Welcome to the site. You can't *prove* this formula. It's a guideline. It can be wrong or right, and whether you need more or less classes than it suggests is (at least in part) a matter of opinion.",2013-09-27 11:57:24.887
101770,56273,22126.0,CC BY-SA 3.0,,so from where comes this formula?,2013-09-27 12:04:05.967
101773,56273,503.0,CC BY-SA 3.0,,"I am not sure where that specific formula comes from, but probably someone who had run a lot of histograms thought that it generally gave good results.",2013-09-27 12:18:28.583
102144,56273,594.0,CC BY-SA 3.0,,see here: http://www.robjhyndman.com/papers/sturges.pdf,2013-09-30 12:52:01.127
102488,56684,,CC BY-SA 3.0,Willemien,"I Don't understand the problem, what do you want to test?",2013-10-02 22:25:12.987
102575,56684,,CC BY-SA 3.0,Ben,"@Willemien I want to test difference of in proportions of the two samples. One sample will be ""true"" 1.21% of the time, and the other will be ""true"" 1.33% of the time, for instance. Is this difference significant? My main problem stems from the heavy skew.",2013-10-03 12:41:03.603
102716,56783,15280.0,CC BY-SA 3.0,,"Thanks for you answer. It helps clarifying my second question. As I tried to convey in the title of the question, my main issue (the first one in the post) was more about the proof mechanism. My main concern is about my understanding of the proof I presented in the question. As I explained, my understanding of the proof leads me to blatantly problematic statement. So I would like to understand were my mistake is as it might reveal some deeper misunderstandings about concepts of expectaction and conditional expectation. Any thoughts about this?",2013-10-04 04:00:08.607
102740,56783,20473.0,CC BY-SA 3.0,,"I added some explanation on the ""add and subtract"" approach to proof.",2013-10-04 10:27:24.233
102789,56783,15280.0,CC BY-SA 3.0,,"Took me some time to understand it, but I finally got my basic mistake : true enough $E \Big[ - 2 \big(Y - h(X) \big) \big(h(X)  - g(X)\big) +   \big(h(X)  - g(X)\big)^2\Big] = 0 $ when $g(X) = h(X)$, but by no means does it imply that $h(X)$ minimizes the expression. There is no reason which the bracketed expression could not be lower than zero. Because of the minus sign in front of $\big(Y - h(X) \big) \big(h(X)  - g(X)\big)$ one could find some $g(X)$ such that $E \Big[ - 2 \big(Y - h(X) \big) \big(h(X)  - g(X)\big) +   \big(h(X)  - g(X)\big)^2\Big] < 0$.",2013-10-04 16:24:22.190
102832,56783,20473.0,CC BY-SA 3.0,,Hmmm... the minus sign in the expression you refer to is a mistake - it should be a plus sign. You could of course then rearrange the terms to obtain again a minus sign... does this hurt the intuition you gained?,2013-10-04 21:03:11.077
102845,56783,15280.0,CC BY-SA 3.0,,"Thanks for keeping up with the question. I edited the initial post to correct for this mistake. Fortunately, I think it does not hurt the gained intuition. Actually it helps me understand yet another mistake : I was assuming that the minus sign was important to guarantee that $0$ was not necessarily the minimum of $E[−2(Y−h(X))(h(X)−g(X))+(h(X)−g(X))^2]$. But I realize this is not just about the sign before the 2. (Hopefully) What I really needed to understand is that, in general (i.e. for arbitrary $h(X)$) $E[2(Y−h(X))(h(X)−g(X))]$ needs not be minimized when $g(X)=h(X)$ (right?).",2013-10-04 23:36:23.880
102846,56783,20473.0,CC BY-SA 3.0,,"Right. Just think of it as any other minimization. Taking the first derivative w.r.t $g$ for this expression you get the necessary condition $-2E(Y-h) = 0$, so it has nothing to do with $g=h$.",2013-10-04 23:53:20.463
102881,56860,20120.0,CC BY-SA 3.0,,How is it justified to call t or F *scores* (rather than e.g. t-*tests*) inferential statistics?,2013-10-05 13:41:54.423
104153,57444,22262.0,CC BY-SA 3.0,,@zkurtz I had this in an old version -- looking for solutions other than logistic regression.,2013-10-14 16:36:57.877
102882,56859,4656.0,CC BY-SA 3.0,,"Descriptive statistics: A coin was tossed ten times and came down heads six times.  Statistical inference: The maximum likelihood estimate of the probability of Heads is $0.6$, or, This information is insufficient to reject the hypothesis that the coin is a fair coin.",2013-10-05 13:44:39.663
102883,56860,155.0,CC BY-SA 3.0,,"@jona The t-score is the ""statistic"" that is used in the t-test, therefore one could describe the t-score as an inferential statistic when used as part of such an inferential process. I guess I have started with the assumption that a statistic is a function of the data. But perhaps you are alluding to the  point  that we often think of inferential statistics as the broader set of techniques used to do inference?",2013-10-05 13:47:04.867
102884,56860,20120.0,CC BY-SA 3.0,,"Let me phrase it differently - isn't a t-statistic a description of a sample, rather than an inferential statement (such as a p-value)?",2013-10-05 13:56:31.233
102885,56860,155.0,CC BY-SA 3.0,,"Well yes, a function of the data is equivalent to a description of a sample. I guess I was thinking that such statistics are used in an inferential process (e.g., researchers relate the t-statistic to a t-distribution to get a p-value and then relate p to alpha to draw an inference). I've often seen textbooks use these examples. But I suppose the p-value and the binary inference itself could be seen as statistics (i.e., functions of the sample data). And the binary inference itself could be seen as most clearly aligned to the inference. Is that what you are getting at?",2013-10-05 14:06:14.737
102887,56860,20120.0,CC BY-SA 3.0,,"The definition of the p-value (probability of sample given some population) refers to the population (or alternatively, long-run frequencies), so I'd file it under inferential. The definition of *t* is phrased only in reference to the sample, isn't it?",2013-10-05 14:20:57.633
102891,56860,155.0,CC BY-SA 3.0,,"So for example, you use the data to get to *t* which is related to a distribution, which gives you *p*, which in turn yields a binary inference about a population parameter. So from a frequentist perspective, t, p, and the binary inference are all random variables. All were involved in the inferential process. I'm not sure what the pros and cons are of labelling all or only some such statistics as inferential.",2013-10-05 14:36:52.690
102892,56860,155.0,CC BY-SA 3.0,,"There are also many other ways of doing inference (e.g., bootstrapped confidence intervals, cut-offs on Bayesian posterior densities). So perhaps in those cases the above definitions would need to be tweaked to focus more on the final inference. That said, once I go outside the traditional frequentist test statistics, I tend to think more in terms of inferential procedures rather than needing to clearly distinguish descriptive from inferential statistics.",2013-10-05 14:37:43.160
102913,56859,21762.0,CC BY-SA 3.0,,"Inference without the concept of ""population"": Assume your data are generated by some (partially) unknown random mechanism/rule. Inferential methods allow to assess properties of this mechanism based on the data. Example: You want to verify an electro-physical formula based on outcomes that can be measured only approximately or under imperfect conditions.",2013-10-05 17:09:41.500
102914,56859,12683.0,CC BY-SA 3.0,,@Michael: Yes; or indeed *make* your data be generated by a known random mechanism - random assignment of experimental treatments.,2013-10-05 17:23:25.437
102944,56875,947.0,CC BY-SA 3.0,,"Is it appropriate to focus solely on the 2008-2009 period (learning sample of the model) to develop an econometrics model earmarked for estimating what would happen in adverse economic scenarios? I gather I am simply repeating my question. As is, I think it is clear enough.",2013-10-06 00:28:09.963
102981,56911,503.0,CC BY-SA 3.0,,"Unless ""observation"" has some order (which it usually does not) then you should not draw a line plot like this. It implies that ""observation"" isn't just a label.",2013-10-06 10:28:52.367
102991,56911,5237.0,CC BY-SA 3.0,,"How is this different from *outlier detection* (applied to residuals)? If you want this to be done non-visually, what does the *plot* of the residuals have to do with anything?",2013-10-06 14:05:51.327
102995,56928,16046.0,CC BY-SA 3.0,,"Maybe I should move the question to math exchange, any suggestions?",2013-10-06 15:42:08.587
102996,56875,947.0,CC BY-SA 3.0,,"You can answer it in a precise way.  The answer is either Yes, No, or it depends.  The key is to support one's answer most rigorously.",2013-10-06 16:05:01.650
103009,56911,1506.0,CC BY-SA 3.0,,The plot was just to help with the explanation. The observations are ordered.,2013-10-06 18:35:27.720
103010,56911,5237.0,CC BY-SA 3.0,,"The plot does help with the explanation. So, I gather the plot is (ultimately) irrelevant and does not have to do with your real issue beyond its communicative role here; this is all well & good. How does what you want differ from outlier detection?",2013-10-06 18:42:30.477
103013,56928,6162.0,CC BY-SA 3.0,,"I totally agree with your opinion about Gelman et al's book. Try *The Bayesian choice*, by C. Robert.",2013-10-06 20:01:03.637
103017,56911,2958.0,CC BY-SA 3.0,,Would the calculations behing [Process Control Charts](http://en.wikipedia.org/wiki/Control_chart) help?,2013-10-06 20:34:52.580
103018,56928,16046.0,CC BY-SA 3.0,,@StéphaneLaurent pretty happy that got the same opinion from somebody else. Will give it a try. Thanks.,2013-10-06 20:36:23.937
103058,56768,,CC BY-SA 3.0,,"In econometrics, there is a systematic approach called BLP model studying your problem. A good reference about BLP model is this paper ""A Practitioner's Guide to Estimation of Random Coefficients Logit Models of Demand"" by Nevo(200).",2013-10-07 03:26:03.317
103060,56911,1506.0,CC BY-SA 3.0,,@gung It is similar to outlier detection and can certainly be used here.  However this does not take into account the independence violation.  Plus the standard deviation may be so inflated in some situations that the points do not seem like outliers.,2013-10-07 04:15:32.383
103061,56928,3183.0,CC BY-SA 3.0,,"For what it's worth, there's a [new edition of Bayesian Data Analysis](http://www.amazon.com/Bayesian-Analysis-Edition-Chapman-Statistical/dp/1439840954) coming out soon.  I don't know if it would be better for your taste, but it looks like it will have a lot of other improvements.",2013-10-07 04:24:50.827
103066,56928,6162.0,CC BY-SA 3.0,,"@DavidJ.Harris This book will always be nubearable for me. Too much text, not enough mathematics. There's more information in one half page of *The Bayesian Choice* than in 5 pages of Gelman et al's book. This book provides some cooking recipes for Bayesian analysis: some recommendations without any theoretical justification. I've also started to read [Gelman & Hill's book](http://www.stat.columbia.edu/~gelman/arm/). Nice to learn R & WinBUGS, but the practical approach is grisly, for example the authors fit Gaussian models to data far, far , far to be Gaussian,  without worrying about that.",2013-10-07 06:18:31.290
103125,56768,4910.0,CC BY-SA 3.0,,"Do you have more information, like a model you are assuming or something? Or are you actually going to use more than one data point? Otherwise, in your case, if the shares at t-1 is A, B, C and A looses say 10 % then B would become B + B / (B+C) * 10 % and C would become C + C / (B + C) * 10 %. Right?",2013-10-07 16:02:51.150
103397,57086,7155.0,CC BY-SA 3.0,,"Would you mind restating why random forests pose implementation issues for you? They're not particularly intensive unless you have thousands of features. You could prune it down, but it's unlikely you'll have an analytic form that's digestible.",2013-10-08 19:14:17.357
103173,56768,11490.0,CC BY-SA 3.0,,"Hi Rasmus. I don't have any model at the moment. The only information that I can use is the market share of each company in each year $1, ..., T$. For example in year 1 we might have $A_1 = 0.2$, $B_1 = 0.5$ and $C_1 = 0.3$, which could become $A_2 = 0.4$, $B_2 = 0.1$ and $C_2 = 0.5$. I have this kind of data for T years. What I want to estimate is, for example, given that $A_2 = 0.4$ how much how that market share comes from $A_1$ (""loyal costumers""), $B_1$ and $C_1$ (""stolen costumers"")?",2013-10-07 21:07:15.300
103178,57015,668.0,CC BY-SA 3.0,,"I have a high degree of belief that exactly one of your hypotheses is true :-).  Your data might be able to cast doubt on one of them.  But what is ""small""? To some people that means $N\le 2$; to others, it might mean $N\le 10^6$.",2013-10-07 21:34:19.693
103181,57015,22454.0,CC BY-SA 3.0,,My sample size is $N = 10$.,2013-10-07 21:39:30.730
103202,57015,594.0,CC BY-SA 3.0,,"Hypotheses are about *populations*, not samples. You can check whether $\bar x < \bar y$ at a glance - no need for p-values or anything.",2013-10-07 23:20:35.337
103209,57015,503.0,CC BY-SA 3.0,,"Given your small sample and lack of knowledge about the distributions, I'd suggest a permutation test.",2013-10-07 23:53:50.697
103210,57015,503.0,CC BY-SA 3.0,,"@whuber ""N le 2""? :-)",2013-10-07 23:54:21.417
103221,56955,2121.0,CC BY-SA 3.0,,"You mentioned each survey was already weighted. Are they weighted using similar methods? Or, did the particulars of the surveys necessitate using different weighting methods?",2013-10-08 01:20:35.267
103225,56955,22423.0,CC BY-SA 3.0,,"hi @Jonathan, sampling method is the same as mentioned in the example, but of course the values of the calculated weights, sample size, and subject particulars are different for the 5 surveys",2013-10-08 01:51:10.383
103228,57026,5237.0,CC BY-SA 3.0,,"What does ""C"" stand for in this context?",2013-10-08 02:07:03.430
103229,57026,22458.0,CC BY-SA 3.0,,"Umm, the hinge loss coefficient? I've mostly found it called C, some people use gamma but that confuses it with the radial basis coefficient. Basically the coefficient associated with the slack variables in the objective function.",2013-10-08 02:32:26.233
103232,57015,668.0,CC BY-SA 3.0,,"@Peter See http://stats.stackexchange.com/a/1836/919 for why $N=1$ can work. $N=2$ is usually needed in order to estimate the variability. In applications where observations are sufficiently expensive, $N \gt 2$ may be considered extremely *large*. (I work in a field where (a) private parties pay for observations which are (b) required by government regulations that (c) are viewed as a burden and, in the worst situations, (d) an observation (actually a monitoring station) can cost \$100K or more. If you want to tell my clients they need a larger $N$, you had better have a *great* reason!)",2013-10-08 03:10:44.680
103287,57015,21762.0,CC BY-SA 3.0,,"@whuber: Maybe there is an additional binary grouping variable ""condition"" involved which is expected to change the order of the true means(?).",2013-10-08 10:27:23.497
103306,57053,,CC BY-SA 3.0,,47% of observations are coded as 1?,2013-10-08 13:10:25.643
103309,57053,10060.0,CC BY-SA 3.0,,"Just a side tip, don't name dichotomous variables this way. Instead of calling it ""gender,"" call it ""male"" or ""female."" That way you'd know what 1 (aka Yes) stands for. If this is ""male,"" then you can easily figure out 0.47 is the fraction of males.",2013-10-08 13:20:54.527
103337,57015,668.0,CC BY-SA 3.0,,@Michael Thank you! Your interpretation sheds new light on the question.,2013-10-08 14:46:04.770
103346,57065,9049.0,CC BY-SA 3.0,,"Good question: I can probably argue pro and against the removal of outliers. Why not use medians if you worried about outliers and what you are looking for is just a ""central tendency""? Given that money-related variables often have highly skewed distribution (eg. Pareto) that might not unreasonable in the first place.",2013-10-08 15:14:30.707
103354,57065,668.0,CC BY-SA 3.0,,"@user11852 Medians tell you little about the mean, which is what is relevant to revenue.  It would be interesting to see your argument in favor of removing the ""outliers,"" especially when these are likely the major contributors to the total revenue.",2013-10-08 15:33:57.610
103355,57065,22477.0,CC BY-SA 3.0,,"Unfortunately median would always be zero, as < 10% of users spend at all",2013-10-08 15:35:54.210
103357,57065,20286.0,CC BY-SA 3.0,,"Also, consider whether in practice you make more profit from the ""lots of small spenders"" or the ""very few big spenders."" If you make your profit from those ""outliers"" you probably don't want to remove them--maybe you want to analyze them separately.",2013-10-08 15:38:51.637
103359,57012,20286.0,CC BY-SA 3.0,,Your examples all have measurements from one or the other Labs but no samples analyzed by both labs. Is that always the case? The best way to proceed will depend on that.,2013-10-08 15:44:43.340
103362,57065,9049.0,CC BY-SA 3.0,,"@whuber: Let me stress, it was a comment, not an answer. I definitely not an expert on bootstrapping; I would generally argue that outliers are *legitimate* data, if they are not obviously corrupted observations. Nevertheless if one bootstraps a somewhat small sample that has obvious outliers I would worry that he could end up ""amplifying"" their influences or overlooking sample heterogeneity.",2013-10-08 15:46:52.757
103364,57065,668.0,CC BY-SA 3.0,,"@user11852 Your general argument that outliers are legitimate is helpful. But, concerning the possibility of amplification, it seems to me that the contrary is true: bootstrapping has a chance of working only if the full sample is used.  Otherwise it presents a fairy tale, telling us how things would be if outliers didn't exist--but obviously they do. The larger problem is that bootstrapping has little theoretical justification when applied to small samples: the theory is an *asymptotic* one.",2013-10-08 15:53:41.447
103368,57065,9049.0,CC BY-SA 3.0,,"@whuber: I agree with what you say. Regarding your median comment: I guess an issue is that I treated revenue as roughly equivalent to income. Usually a debate between using mean or median does arise there; eg. in household income cases. (In retrospect, having just <10% of the user data generating revenue definitely it is not a good assumption.) Also I didn't mean to imply that a median ""is the mean"" or something like that. I specifically mentioned it *as a ""central tendency""* value.",2013-10-08 16:17:03.443
103370,57065,668.0,CC BY-SA 3.0,,"@user11852 And that's the crux of the matter: central tendency is not terribly meaningful when tracking revenue; only the sum (or equivalently, the mean) is. As far as the less than 10% goes, that depends on the business. Plenty rely on just covering costs with routine transactions and making profits on a small number of very large or high-profit sales. *E.g.*, one model for how an airline could make a profit is from the outsized margins reaped from the very small number of first-class passengers.",2013-10-08 16:24:05.757
103371,57012,20773.0,CC BY-SA 3.0,,"@EdM yes, that is precisely the problem. I am sorry I was not more clear about this.",2013-10-08 16:24:38.037
103380,57065,9049.0,CC BY-SA 3.0,,"@whuber: Cool, thank you for the insight on the matter!",2013-10-08 16:59:15.470
103383,57065,450.0,CC BY-SA 3.0,,This is an important question (+1). Can you add a small sample of your dataset or a simulated sample resembling it to the question? I think providing an illustration will be more fruitful in this case.,2013-10-08 17:31:08.450
103588,57198,1406.0,CC BY-SA 3.0,,Does the wikipedia answer your question: http://en.wikipedia.org/wiki/Linear_regression?,2013-10-10 08:28:21.573
103402,57086,22034.0,CC BY-SA 3.0,,"@Jacob--The problem is that RF has lots of decision trees. I'd love to report a single formula (< a few lines long if possible) that predicts nearly as accurately as RF. Since I'm publishing my work to an audience of modest statistical sophistication, I think exporting pages upon pages of trees would severely limit the probability of my findings becoming implemented in clinical settings.",2013-10-08 19:38:44.853
103427,57012,594.0,CC BY-SA 3.0,,Let's assume the sensitivity/detection limit problem was solvable. What is the question of interest? Are you trying to compare means across labs or something?,2013-10-08 23:15:16.167
103446,57110,668.0,CC BY-SA 3.0,,What units are your dates measured in?  The lags in the plots are enormous--they look like seconds.  Taking first differences at lags of seven *seconds* won't fix a weekly cycle!,2013-10-09 04:14:57.447
103454,5015,,CC BY-SA 3.0,,Another possibility is paradoxical confounding: Example 1: http://epm.sagepub.com/content/56/3/430.abstract Example 2: http://optimalprediction.com/files/pdf/V1A19.pdf,2013-10-09 07:06:50.557
103461,57015,21762.0,CC BY-SA 3.0,,"Okay. To provide help, we would need to know much more about $x$ and $y$. What do they measure? Are they paired or unpaired?",2013-10-09 08:01:54.570
103480,57137,,CC BY-SA 3.0,,"since you are right with your suspicion, why not answer your question yourself? I means you basically state the answer in the question anyways.",2013-10-09 12:48:05.200
103485,57012,20773.0,CC BY-SA 3.0,,"Thanks, @Glen_b. Upon looking back, I can see how unclear my question is. The question is how to combine information from Lab A and Lab B such that they are on the same scale. I've gone back to edit my original post, so hopefully that clears things up.",2013-10-09 13:29:54.123
103488,57126,3922.0,CC BY-SA 3.0,,"Your client should just concentrate on pursuing these wealthy guys to buy/contribute, and this is a PR question, not a statistics question.",2013-10-09 13:53:41.350
103534,57110,22494.0,CC BY-SA 3.0,,"my data is daily data measured in MWs, I tried first (and second) differences at lags of 7, 14, 365,364,366, but there is still seasonality.",2013-10-09 19:02:16.607
103547,57167,13037.0,CC BY-SA 3.0,,Which variables do you actually observe/have data on?,2013-10-09 20:47:33.067
103551,57167,21952.0,CC BY-SA 3.0,,I have observations on all the variables.,2013-10-09 21:07:09.263
103554,57167,13037.0,CC BY-SA 3.0,,"All? As in you know values of $Y$, $X$s, and $Z$s? A simple approach is just to do linear regression of $Y$ with $Z_1, Z_2, Z_3,$ and $Z_4$.",2013-10-09 21:12:06.590
103558,57167,21952.0,CC BY-SA 3.0,,But the problem here is with endogeneity. the values of Z's are determined by Y in some way (as explained above). Also changing any value of Z's will have an impact of changing everything else in the equation. so everything is being determined simultaneously.,2013-10-09 21:33:13.723
103559,57164,594.0,CC BY-SA 3.0,,Why would such a property be of any value in deciding which would be an appropriate model?,2013-10-09 21:57:54.357
103560,57164,19264.0,CC BY-SA 3.0,,"@Glen_b I'm still a beginner when it comes to statistics so my knowledge is pretty basic. Looking at the plots of gamma and lognormal distributions, qualitatively they look very similar. I'm looking for quantitative differences between the two. For instance, what are some examples of physical applications where gamma or lognormal distributions occur?",2013-10-09 22:06:30.377
103563,57164,594.0,CC BY-SA 3.0,,"In reality, likely neither ever actually occurs; they're extraordinarily simple models which are sometimes useful (if rough) approximations of reality. I will post an answer that discusses some qualitative differences.",2013-10-09 22:31:39.953
103564,57167,5045.0,CC BY-SA 3.0,,"I don't think this system of equations is identified. It it was, however, one might use 3SLS (Three Stage Least Squares).",2013-10-09 22:38:40.917
103566,57156,1741.0,CC BY-SA 3.0,,Can you elaborate further? If want to predict the classes you have to use supervised classification.,2013-10-09 23:19:24.733
103569,57175,594.0,CC BY-SA 3.0,,"""ML"" is not an algorithm but a criterion. MLE doesn't find a minimum, it's the value of (a monotonic function) of the thing you try to optimize. Minimizing $J$ probably\* *is* ML (in that its argmin will correspond to ML. \*(I haven't done more than taken a quick glance at the paper, but you should have at least mentioned you were doing some form of nearest-neighbor modelling here, and better still you should write more details of your model in your question.)",2013-10-10 00:34:40.740
103571,57175,20320.0,CC BY-SA 3.0,,"I have added the nearest neighbor point, my mistake sorry for that. So, the thing is EM solution for minimizing the objective function.",2013-10-10 00:45:27.693
103572,57183,13037.0,CC BY-SA 3.0,,Mcnemars is great and all...but why not just use logistic regression?,2013-10-10 00:53:41.957
103576,57186,12544.0,CC BY-SA 3.0,,"Not sure what you mean by a difference of 20%. There are three schools, so there are two differences (because once you know the first two, you know the third). In addition, power in this analysis depends on the proportions. You have more power if the proportion is around 50% than if it is around 1%.",2013-10-10 03:52:55.920
103577,57186,12544.0,CC BY-SA 3.0,,"Can you give an example of the effect you expect, e.g. 20%, 30%, 50%. Also, how do you plan to analyze?",2013-10-10 03:53:59.877
103578,57186,22542.0,CC BY-SA 3.0,,"Hello - thank you for your response.  I will try to clarify.  I will be asking students at each school if, for example, there was another school they could have attended, and the using chi2 tests to compare proportions at each.  Within each school, I will be using logistic regression to look at association between factors examined (e.g. option for other school) and school of choice.  I do not know what effect to expect.",2013-10-10 04:06:05.980
103579,57186,22542.0,CC BY-SA 3.0,,"In my calculations above, I simply assumed that 50% of population could have chosen a different school.  I had been told,I thought, that this would give a good sample size where such proportions are unknown?",2013-10-10 04:15:12.213
103580,57189,5237.0,CC BY-SA 3.0,,"What would happen if $X$ were a *standard normal*? (Ie, $X\sim\mathcal N(0,1)$.) What would happen then?",2013-10-10 04:29:46.530
103581,57198,594.0,CC BY-SA 3.0,,"""*is it Gaussian noise or random error*"" -- yes, that random error term is usually taken to be Gaussian noise, though if you're only estimating the coefficients (rather than computing intervals or doing hypothesis tests) it doesn't have to be Gaussian. If it's Gaussian then LS is optimal in several different senses at once. If it's not Gaussian then you still have that it's best linear unbiased.",2013-10-10 06:43:58.463
103582,57198,22548.0,CC BY-SA 3.0,,"Thanks a lot for reply :). the model is going to test hypotheses that are made before, then it's Gaussian noise. would you please give some reasons why we add this term to the model?",2013-10-10 06:56:00.780
103584,57195,10450.0,CC BY-SA 3.0,,"[Describing Temporal Correlation Spatially in a Visual Analytics Environment,"" Abish Malik et al.][1]


  [1]: http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&ved=0CEkQFjAB&url=http://www.purdue.edu/discoverypark/vaccine/assets/pdfs/publications/pdf/Describing%2520Temporal%2520Correlation%2520Spatially.pdf&ei=elhWUsriM8eZiAKh7oDYCg&usg=AFQjCNHAL8nhgTwNQTfWEoy6VNdgO4mbdQ&bvm=bv.53899372,d.cGE",2013-10-10 07:35:51.723
103586,57198,594.0,CC BY-SA 3.0,,Because the observations don't actually lie on a line/plane/hyperplane.,2013-10-10 07:50:41.407
104672,57730,13037.0,CC BY-SA 3.0,,"I thought she said speaker was discrete (1,2,or 3)",2013-10-17 20:13:26.633
103591,57198,22548.0,CC BY-SA 3.0,,"this is wikipedia's definition:  ε is called the error term, disturbance term, or noise. This variable captures all other factors which influence the dependent variable yi other than the regressors xi. The relationship between the error term and the regressors, for example whether they are correlated, is a crucial step in formulating a linear regression model, as it will determine the method to use for estimation.  I'm writing a research paper, is this definition good enough to bring in research paper?",2013-10-10 08:59:00.677
103592,57200,15563.0,CC BY-SA 3.0,,"Yes, that makes sense. I guess I did not separate the Train data into two sets. Thank you.",2013-10-10 09:01:14.910
103593,57185,15827.0,CC BY-SA 3.0,,"Your question title is about measurement error; your final question is about bias. The two are not the same. You also ask simultaneously: what is the difference, and is there a difference? That said, here goes. The central idea of bias is that a procedure is wrong on average, yielding a mean that is higher or lower than the true or correct value, known somehow. That could be true of a measurement procedure, but it can apply where measurement error is not the question. That's what I understand from statistics; psychometrics may have a different notion, but I doubt it.",2013-10-10 09:02:10.390
103594,57164,633.0,CC BY-SA 3.0,,"@glen_b: the reason is that if you're measuring only those statistics then the minimum assumptive distribution is uniquely the exponential family distribution with those sufficient statistics.  Whereas any distribution might be a poor model of reality, if one is not free to choose which measurements are taken, then this is an excellent way of choosing a model.",2013-10-10 09:04:18.900
103595,57164,6162.0,CC BY-SA 3.0,,@Glen_b I guess the lognormal distribution should appear in some physical situations because of the CLT.,2013-10-10 09:06:39.173
103596,57186,12544.0,CC BY-SA 3.0,,So you're planning to do three separate logistic regressions? (Chi-square tests on contingency tables and logistic regression give the same answer). I'm still not sure what you are going to be testing (but maybe that's me). Can you give an example?,2013-10-10 09:12:21.053
103598,57186,21762.0,CC BY-SA 3.0,,How are you dealing with multiple testing?,2013-10-10 09:39:07.073
103599,57203,6162.0,CC BY-SA 3.0,,"No need of ""many"" exponential variates to be Gamma.",2013-10-10 09:40:37.467
103601,57186,503.0,CC BY-SA 3.0,,What software do you have access to?,2013-10-10 10:08:37.793
103602,57205,,CC BY-SA 3.0,,"Welcome @Fabio: Could you please give more explanation to what you mean with ""not touch the data""? Do you mean not to remove anything? It would also help to introduce the content you linked to and explain why this helps the OP.",2013-10-10 10:22:03.277
103604,57212,503.0,CC BY-SA 3.0,,Welcome to the site. What sort of model did you develop?,2013-10-10 11:00:26.270
103605,57212,22190.0,CC BY-SA 3.0,,I am developing a logistic model.,2013-10-10 11:01:25.997
103606,57210,21762.0,CC BY-SA 3.0,,Is this about PCA or FA?,2013-10-10 11:11:28.233
103608,57214,13889.0,CC BY-SA 3.0,,Thanks for the answer but I'm afraid the AUC statistics are expected to vary depending on the benchmark. Sorry I didn't make that clear.,2013-10-10 12:19:53.817
103609,57218,21398.0,CC BY-SA 3.0,,I mean running my analysis with the missing value dataset.,2013-10-10 12:48:21.707
103611,57195,750.0,CC BY-SA 3.0,,"Your approach sounds very similar to estimating the [variogram](http://en.wikipedia.org/wiki/Variogram) - which suggests a non-map based scatterplot of the semi-variogram. If you can settle on a spatial weights matrix, you could also plot a Moran Scatterplot, which is $y$ on the y-axis and $W\cdot y$ on the x-axis.",2013-10-10 13:33:26.207
103612,57216,674.0,CC BY-SA 3.0,,"Could you provide a reference for the above screenshot? It is good practice to properly cite the work of others. By the way, this should be amended to your previous answer, not posted as a new one.",2013-10-10 13:34:37.753
103613,57221,21762.0,CC BY-SA 3.0,,What is $x$? Is it an additional independent observation? Why do you think $(x-\text{true mean})/s$ would follow a $t$?,2013-10-10 14:01:30.037
103614,57221,503.0,CC BY-SA 3.0,,Do you mean to ask what distribution it would follow *if* something about $x$ was true? (i.e. some null hypothesis).,2013-10-10 14:12:00.540
103616,57221,594.0,CC BY-SA 3.0,,"If you had the true mean there, why would that be distributed as a $t$?",2013-10-10 14:19:59.793
103618,5015,5237.0,CC BY-SA 3.0,,"It is not clear how this addresses the OP's question (especially w/o any discussion), & in general, Simpson's paradox isn't the best way to think about the relationship b/t an interaction & its composite variables.",2013-10-10 14:44:43.233
103619,57226,674.0,CC BY-SA 3.0,,"Please, register your original account and merge it with this one; follow the instructions on our [Help Center](http://stats.stackexchange.com/help/merging-accounts). Next, I would advise to merge your three different replies into one single answer.",2013-10-10 15:04:10.173
103620,57160,2164.0,CC BY-SA 3.0,,"David, Post your data to dropbox.com so we can take a look.  I'm not sure why you a reaching for logs.  Transformations (like drugs) can have nasty side effects that are masking the real issue (like stress :) ). Instead of a log, you might need to consider other things like level shifts, changes in parameters, etc.",2013-10-10 15:23:25.743
103625,57230,668.0,CC BY-SA 3.0,,"It is difficult to understand this description.  Could you perhaps post a simple example of your dataset?  And please clarify what you mean by ""represent"": does that mean a logical file format, a graphical visualization, a statistical summary, or perhaps something else?  What is the intended purpose of this representation?",2013-10-10 15:33:58.047
103627,57230,22569.0,CC BY-SA 3.0,,"Hey, updated the question",2013-10-10 15:39:42.840
103628,57223,346.0,CC BY-SA 3.0,,Can you post the data via `dput()`?,2013-10-10 15:50:32.060
103629,57223,22564.0,CC BY-SA 3.0,,"@Henrik. I don't have the simulation data used to generate that figure any longer. Do you want an example of the simulation result or the actual data? Also, is there some advantage to using dput() rather than simply posting a table?",2013-10-10 15:59:03.010
103630,57233,22567.0,CC BY-SA 3.0,,"I do have a common intercept $\beta_0$, but I have a number of factors in the model. I was just showing one factor in the example. Is it possible to suppress the intercept with more than one factor?  I thought that you could do it for one, but then it broke down with multiple.  That would fix my problem, as it would allocate the overall intercept to each factor.",2013-10-10 16:02:07.313
103631,57228,668.0,CC BY-SA 3.0,,"There are several people on [gis.se] actively using `R` for their work: consider [searching the site](http://gis.stackexchange.com/questions/tagged/r?sort=votes). Certainly you will want to take a look at [""How to make beautiful maps in R""](http://gis.stackexchange.com/questions/48828).",2013-10-10 16:06:41.993
103632,52871,,CC BY-SA 3.0,,"any chance you can provide a slimmed down data set, otherwise it is difficult for one to replicate your error and help you debug?",2013-10-10 16:07:34.920
103661,57193,1717.0,CC BY-SA 3.0,,"If you can't solve the ML because the derivatives are too complicated, then EM is a good alternative. If $u$'s are observations and follow a gaussian distribution, then you need to include latent variables in that distribution. For example, what is the probability of a signal $u_{t}$ given some other variable $z_{t}$ provided you can calculate $p(z_{t}|\theta)$. If you can do that, follow the procedure I described and it will give you an estimate of $\theta$.",2013-10-10 19:12:37.063
103633,57228,5237.0,CC BY-SA 3.0,,"Welcome to the site, @mikeLdub. This question seems to be *only* about how to do this in R. As such, it may be off-topic for CV (see our [help page](http://stats.stackexchange.com/help)); but could be on-topic on [Stack Overflow](http://stackoverflow.com/). If you have a question about the substantive statistical / visualization issues, please edit to clarify; if not we will migrate it for you (*please don't cross-post, though*). If it does go to SO, it will need a [reproducible example](http://stackoverflow.com/questions/5963269/) to be on-topic there; can you add a `dput()` of your data?",2013-10-10 16:08:33.253
103634,57228,5237.0,CC BY-SA 3.0,,This question appears to be off-topic because it is only about how to use R.,2013-10-10 16:09:37.697
103635,57230,668.0,CC BY-SA 3.0,,"Thanks.  But what are the data?  The quiz answers?  Scores and subscores?  Counts of questions attempted?  I notice your edit does not address the last two questions I asked in my previous comment--please clarify those points, too.",2013-10-10 16:12:30.330
103636,57233,5237.0,CC BY-SA 3.0,,"You can use your entire regression equation, ie w/ the intercept & the other factors & continuous covariates, etc. The only difference is that you are multiplying the parameters related to the unknown factor by the marginal probabilities that the observation is each possible level of the unknown factor.",2013-10-10 16:14:00.057
103637,57236,594.0,CC BY-SA 3.0,,"so if you simply choose the right definitions ('death' in this case is 'of immobility') ... it would seem to be very sensible; any change of state works really, it doesn't have to be death. The only thing that worries me is that while people don't generally recover from death after a few months (once you have entered the state for more than a few minutes, you remain there) ... but people can potentially become non-mobile again. If that can't/won't happen in your circumstances, there should be no problem; if it can, then you might look at models where people can move between states over time.",2013-10-10 16:19:03.420
103638,57236,10060.0,CC BY-SA 3.0,,"This is more about how you define an ""event."" It's called ""survival"" but the event does not have to be lacking ability to survive. Turning the idea around, you can use 1 to represent ""revival.""",2013-10-10 16:19:53.440
103639,57236,22572.0,CC BY-SA 3.0,,"@Glen_b, thats exactly my problem. during recovery you can certainly move back and forth between states. I'm envisioning something like this: [link](http://imgur.com/1KkvScI) but with the ability to have increasing/decreasing mobility, and the ""event"" being reaching your baseline",2013-10-10 16:25:28.857
103640,57128,1693.0,CC BY-SA 3.0,,"I'm curious - how, mechanically, would you go about combining the 1000 sets of results using spss?",2013-10-10 16:29:08.773
103641,57236,594.0,CC BY-SA 3.0,,"There are already models for sickness and disability of varying levels of sophistication, often based on Markov chains (a small number of states, often continuous time), or related to them. It's quite possible to have several different levels of mobility. [e.g.](http://www.google.com/search?q=markov+chain+sickness|disability+models)",2013-10-10 16:34:31.747
103643,57237,668.0,CC BY-SA 3.0,,"An interesting question. However, the second ""obviously"" (about not respecting the marginal distribution) is not at all clear to me. Why is it obvious? The distribution of $(v,a)$, as reflected by your ""two-dimensional histogram,"" depends on how you have sampled these variables; I wonder whether this might explain possible differences. What kind of data are represented by this histogram and how exactly do you ""draw values"" from it?",2013-10-10 16:44:04.057
103646,57156,19822.0,CC BY-SA 3.0,,"Let's just say that i have two datasets with same number of variables and samples. the first data set contain the class information(A and B) while  second dataset does not have any class info. Using RF code, first dataset was classified in to the two classes with very good accuracy. The run parameters given above are for that particular classification where the class info is required in the dataset. Now my question is how to classify the second datset in to two class?",2013-10-10 16:54:34.700
103647,57230,22569.0,CC BY-SA 3.0,,Count of questions attempted.,2013-10-10 16:57:16.037
103648,57241,8414.0,CC BY-SA 3.0,,"gung, thanks for your answer.  I suppose my question might be a little ambiguous.  What I want is not a relationship between x and y in model 3 (which is what you've done), but in model 1 (Y = b11 + b12 * X + e1).  I have clarified my question to this effect.",2013-10-10 17:18:27.297
103649,57223,346.0,CC BY-SA 3.0,,@Flask I am interested in the actual data. And the `dput()` of the data makes it the easiest to read it into R.,2013-10-10 17:31:04.020
103651,57241,8414.0,CC BY-SA 3.0,,Thanks for the edit.  Is it possible to directly specify the size of the population effect for coefficient b12?,2013-10-10 17:33:19.510
103653,57223,346.0,CC BY-SA 3.0,,"Difference between which two groups actually interests you (given the original question, I expect you are only interested in two groups)?",2013-10-10 17:39:51.300
103655,57241,5237.0,CC BY-SA 3.0,,"Your question at this point is what would be: what is the population correlation between $x$ & $y$ in general. I wonder if that might be best asked as a new question, as I'm not sure off the top of my head. In the simplest case, where all 3 variables ($x$, $med$, $y$) are normally distributed, & the relationship b/t $x$ & $y$ is *fully* mediated, then $\rho_{x,y} = \rho_{x,med}*\rho_{med,y}$. However, it's more complex if the distributions aren't normal (eg, your $x$ is equal frequencies of $-.5$ & $+.5$), or w/ more complex mediational situations.",2013-10-10 17:47:37.873
103656,57223,22564.0,CC BY-SA 3.0,,"@Henrik. I am interested in all comparisons. The example of two groups was just a simplification. As noted in the question I am not interested only in this specific data. There is data in publications that was generated via the same process that I would like to judge the reliability of given they performed t-tests. Actually, that is also a simplification. What has actually been done previously varies including two-way anova,  one-way anova followed by newman-keuls, ""SAS glm"". I am most interested in the accuracy of the newman-keuls method.",2013-10-10 17:55:01.463
103657,57193,20320.0,CC BY-SA 3.0,,Thank you for a good general introduction. My question was I have observations u whose distribution is gaussian. I need to minimize/optimize using ML and EM.In my case u's are higher dimensional signal which has the parameters embedded in it.I need to formulate u's so that I can apply EM.Assuming u's follow a Gaussian distribution & u is a 2*N dimensional signal where N represents no.of samples.How do I formulate since I cannot do a derivative of u's?Is it possbile to find the parameters assuming gaussian distribution and unknown signal generating model?,2013-10-10 18:04:35.810
103658,57223,6162.0,CC BY-SA 3.0,,"Have you tried to fit a Gaussian model with a data transformation ? Such as  `gls(f(Value) ~ Group, data=dat, na.action=na.omit, 
      correlation=corSymm(form=  ~ 1 | Subject), 
      weights=varIdent(form = ~1 | Group))` (with `nlme` package)",2013-10-10 18:41:07.590
103659,57225,21398.0,CC BY-SA 3.0,,Thanks. I'm doing a multilevel logistic analysis with 5 imputed datasets and I'm going to combine them manually in a pooled version,2013-10-10 18:49:04.077
103689,57223,346.0,CC BY-SA 3.0,,"@StéphaneLaurent I see, makes sense. And the residuals are relatively similar to the ones reported below. But only if you use `lme` they are really identical.",2013-10-10 21:06:23.107
103690,57193,20320.0,CC BY-SA 3.0,,Is there any implementation for this kind of thing or do I follow the matlab link for implementation?,2013-10-10 21:07:18.417
103662,57244,674.0,CC BY-SA 3.0,,"Some relevant threads: [How to convert molecular categorical variables to dummy variables for cluster analysis?](http://stats.stackexchange.com/q/22210/930), [What is the difference between independence.test in R and Cochrane and Armitage trend test?](http://stats.stackexchange.com/q/8774/930) This [response of mine](http://stats.stackexchange.com/a/9394/930) has some references [1,2,4] that might be useful, but [Introduction to Genetic Association Studies](http://cshprotocols.cshlp.org/content/2012/3/pdb.top068163.full), by Cathryn M. Lewis and Jo Knight, is probably more recent.",2013-10-10 19:26:46.907
103663,57242,668.0,CC BY-SA 3.0,,"Whether it ""makes sense"" or not may depend on the costs and constraints on your data collection process. Conceivably, it is easy to collect lots of data along individual lines but expensive to set up each line: such a circumstance would suggest an approach like this one. What is optimal, though, depends on details of the costs and the specific constraints. Could you perhaps share this kind of information with us or, more generally, explain why you are contemplating such an approach?",2013-10-10 19:34:54.857
103664,57245,22564.0,CC BY-SA 3.0,,Thank you for your response. I have run the code and duplicated your results. I will need to examine the code of these functions and run some simulations to understand what is occurring. I added some further questions in the original post.,2013-10-10 19:51:36.890
103666,57248,,CC BY-SA 3.0,,"It should be the other way around. If you're assuming a gene-dosage effect you have only one parameter and it's a one degree of freedom test. If you dpn't assume the gene-dosage effect, you have 2 parameters and if you want to test them jointly it's a 2 dof test.",2013-10-10 19:55:23.267
103667,57193,20320.0,CC BY-SA 3.0,,Is the process similar to EM of gaussian mixture models and its implementation http://www.mathworks.com/matlabcentral/fileexchange/26184-em-algorithm-for-gaussian-mixture-model. In my case what is z_t?After I find out the distribution how do I find the probability?By this time u must have know that I am really weak in this area.From your procedure which steps should I follow to minimize u_t using EM?It will be really nice if you can edit your answer adding how to apply EM on u's such that u's can be minimized.,2013-10-10 19:58:24.427
103668,57245,6162.0,CC BY-SA 3.0,,"I'm wondering why `gls(f(Value) ~ Group, data=dat, na.action=na.omit, correlation=corSymm(form= ~ 1 | Subject))` provides quite different residuals. Isn't it the same model ?",2013-10-10 20:02:43.097
103670,57223,22564.0,CC BY-SA 3.0,,"@Stéphane. I have done this using the sqrt transformation as suggested by Henrik, but do not understand the output.",2013-10-10 20:14:14.830
103671,57253,668.0,CC BY-SA 3.0,,"Welcome to our site, Jen.  You would open your question up to many more (knowledgeable) people by explaining what these parameters 'lambda' and 'lambda2' mean: many of us don't want to go to the trouble of looking up the documentation for your package in order to find out (and that's something that surely you have already done).",2013-10-10 20:18:27.730
103672,57252,12683.0,CC BY-SA 3.0,,Welcome to Cross Validated! Please have a look at the possible duplicate & if you're still in doubt edit your question to explain how.,2013-10-10 20:19:45.630
103673,57193,1717.0,CC BY-SA 3.0,,"This process is exactly the same as in mixture models. $z_{t}$ depends on your problem. As I said, in mixture models $z_{t}$ are the mixture coefficients, so it's quite easy to define a joint distribution $p(X,Z|\theta)$. Obviously, I don't know the dependencies of your observations $u_{t}$, that is something you should know, but basically the idea is to find such joint distribution. Furthermore, in EM you don't minimize $u$. Instead, maximize the likelihood of  $p(U|\theta)$ using $Q$ as a proxy. I recommend you to read chapter 9 of Bishop's book.",2013-10-10 20:20:37.147
103674,57252,2857.0,CC BY-SA 3.0,,"""standard errors and p-values of interactions of standardized models are not reliable""? That is not true as far as I know. SE and p-values are equivalent between standardized and un-standardized models.",2013-10-10 20:25:15.777
103675,57248,10278.0,CC BY-SA 3.0,,@andrea you are right I will correct my answer.,2013-10-10 20:37:27.703
103676,57245,346.0,CC BY-SA 3.0,,"@StéphaneLaurent I think the same model can only be obtained with `lme` as you need to take the nested structure of the data into account. The following model gives the same F and p value for the effect of `Group` and the same residuals: `lme(Value ~ Group, random = ~ 1|Subject, dat)`",2013-10-10 20:37:55.473
103677,57223,346.0,CC BY-SA 3.0,,"Taken from `?aov`: ""`aov` is designed for balanced designs"". So your second edit doesn't provide a reasonable model I guess.",2013-10-10 20:39:40.670
103678,57245,6162.0,CC BY-SA 3.0,,"Sorry, the equivalent `gls` model is `gls(f(Value) ~ Group, data=dat, na.action=na.omit, correlation=corCompSymm(form= ~ 1 | Subject))` (exchangeable repeated measures). But it also provides different residuals.",2013-10-10 20:41:26.763
103679,57223,6162.0,CC BY-SA 3.0,,"Flask, see also the comments below @Henrik's answer. The advantage of `gls` is that one can specifiy different variances per group with the `weights` argument. But one inconvenient is that Kenwards-Rogers degrees of freedom provided by the `pbkrtest` package are not available for `gls` models.",2013-10-10 20:44:52.277
103680,57245,346.0,CC BY-SA 3.0,,"@StéphaneLaurent Nah, I don't think so. Just look at the dfs, the denominator dfs are 105. `gls` cannot handle the nested structure (i.e., replicates for participants).",2013-10-10 20:46:49.893
103681,57223,346.0,CC BY-SA 3.0,,"@StéphaneLaurent If you want variance weights and nested structure (i.e., replicates per participants) you can simply use `lme`instead of `gls`. Or what sepaks against `lme`?",2013-10-10 20:48:17.403
103682,57245,6162.0,CC BY-SA 3.0,,"Yes, degrees of freedom given in the output are particular, but estimates (fixed effects and variances) are the same.",2013-10-10 20:50:38.863
103684,57193,20320.0,CC BY-SA 3.0,,"So for my case it will theta=arg max E [log p(U|theta)] ?This means the probability of u's given theta, but I do not know theta as I need to estimate theta ! Also, will this formulation give me multiple parameters theta_1, theta_2 etc as my model has multiple parameters to be estimated.",2013-10-10 20:51:15.413
103685,57223,6162.0,CC BY-SA 3.0,,I have nothing against `lme`. I'm just trying with `gls`. Considering the subject as a random effect is equivalent to consider an exchangeable correlation structure.,2013-10-10 20:56:23.140
103686,57193,1717.0,CC BY-SA 3.0,,"In your case, $E[\log p(U,Z|\theta)]$. Remember that this is a recursive procedure, so you start by guessing $\theta$. After several iterations, the estimate for $\theta$ will converge to the ML estimate. As for your second question, yes. In the same way $X$ represents the set of observations, $\theta$ represents a set of parameters, although in this case I assumed there was only one parameter to be estimated.",2013-10-10 20:56:46.410
103687,57215,22548.0,CC BY-SA 3.0,,"Thanks a lot for your answer. it is really good. and another question is: in my paper, I introduced some hypotheses and a regression model to determine the effect on each independent variable on dependent variable, I tested hypotheses by running multiple linear regression in SPSS software. in this case, do I need to specify what the noise should be?",2013-10-10 21:02:52.770
103688,57251,5448.0,CC BY-SA 3.0,,"That doesn't look like a convergence theorem to me... convergence theorems generally say something like ""as $t \to \infty$, some function of $t$ (e.g., a probability distribution) approaches some other function"".",2013-10-10 21:02:54.867
103691,57254,668.0,CC BY-SA 3.0,,"You need to know (or assume) more about what's going on. In some cases, the units within a packet might tend to have very similar weights; in other cases, they might often be wildly different.  Would it be possible to destroy a small representative sample of the packets in order to assess this?",2013-10-10 21:11:16.907
103692,57215,449.0,CC BY-SA 3.0,,"Generally, no, but I don't know nearly enough details about your paper. The shape of the noise is an assumption of your regression model and usually not something you need to specify in the paper.",2013-10-10 21:12:16.147
103693,57193,20320.0,CC BY-SA 3.0,,let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/11008/discussion-between-srishti-m-and-robert-smith),2013-10-10 21:12:40.647
103694,57193,20320.0,CC BY-SA 3.0,,"Lastly, in some papers I have read that if the objective is to minimize then I take the negative of the log likelihood, apply kalman filters. Is this correct? Do I need to apply kalman filter? Or as u said maximizing the probability yields the parameters. thank you so much for continued help",2013-10-10 21:13:18.730
103695,57206,668.0,CC BY-SA 3.0,,"A lot of this material can work well for large datasets and I agree with your initial assessment that normality testing can be limited or questionable with small datasets. But given the *huge* variability of skewness and kurtosis, it would seem that any effort to identify the type of underlying distribution based on these statistics would be even more questionable and less certain.  Consequently, wouldn't this approach be (at best) misleading even as a preliminary check?",2013-10-10 21:16:55.807
103697,57251,594.0,CC BY-SA 3.0,,"Just because you have a Gibbs sampler doesn't guarantee convergence (it's quite easy to construct a situation where sampling the full conditionals won't converge). Convergence of a particular implementation of Gibbs sampling (that is, with a particular model), and for MCMC implementations in general, is shown by establishing that the sampling scheme satisfies the conditions for convergence of a Markov Chain (usually fairly easy). If you look at what conditions have to apply for a Markov Chain to converge to its stationary distribution, you can see what you need to hold.",2013-10-10 21:29:23.927
103698,57251,22578.0,CC BY-SA 3.0,,I added the quotation from the book I am using,2013-10-10 21:36:44.770
103700,57258,22564.0,CC BY-SA 3.0,,"Do you know why this output is different in R 2.14.2 vs 3.0.1? It also does not say the ""['lmerMod'] in line 4 of your first code panel.",2013-10-10 21:58:39.493
103701,57260,5237.0,CC BY-SA 3.0,,"At present, this is more of a comment than an answer. Would you mind expanding it a little bit to make it more answer-ish?",2013-10-10 21:58:50.500
103702,57258,6162.0,CC BY-SA 3.0,,@Flask Probably an update of the `lme4` package.,2013-10-10 22:01:39.113
103706,57259,22564.0,CC BY-SA 3.0,,"So in this case, if I had **not transformed** using square root I get pvalues of .9124, .0099, and .0046 respectively. Also, I still do not understand how it is possible to ignore the within-subject variance when comparing group means. The confidence intervals for the means are the same as well?",2013-10-10 22:14:58.587
103708,57223,6162.0,CC BY-SA 3.0,,"As said in my second answer, taking the subject means is a correct approach. The group means are sufficient statistics if you don't want to estimate variance components.",2013-10-10 22:19:47.927
103709,57259,6162.0,CC BY-SA 3.0,,"Yes, as long as you are interested in means only, you don't loose information by only looking at the subjects means. You don't ignore within-variance, you only ignore the decomposition of variance.",2013-10-10 22:23:44.257
103710,57259,6162.0,CC BY-SA 3.0,,"@Flask A colleague of mine, which is not mathematician but which has a very strong intuition in statistics, would say that the subject is the ""unit of observation"", and then only his mean value plays a role.",2013-10-10 22:25:21.123
103711,57259,22564.0,CC BY-SA 3.0,,mmm..This does not meet my intuition. If I am less sure about the individual means it should decrease my confidence in the estimate for group means. [see second to last post in this thread](http://www.physicsforums.com/showthread.php?t=608932&page=2),2013-10-10 22:35:27.267
103712,57259,6162.0,CC BY-SA 3.0,,It took me one year of practice to understand :) I'm going to bed now.,2013-10-10 22:41:50.287
103714,57259,22564.0,CC BY-SA 3.0,,"I am sure you are correct, but if that is true then this procedure must be answering the wrong question for my purposes. If I have no idea what is the cause of this large within-subject variance (subgroups amongst the measurements, etc) this must cause me to doubt the ""external validity"" of the conclusions due to the significance test more so than if the measurements were precise and consistent.",2013-10-10 22:52:46.453
103715,52871,18040.0,CC BY-SA 3.0,,"Just as an FYI you shouldn't use ""t"" as a variable name since it's the name of an internal R function ?t.",2013-10-10 22:58:01.883
103717,57263,503.0,CC BY-SA 3.0,,What's the treatment? (I don't see any mention of one).,2013-10-10 23:10:28.477
103718,57261,5448.0,CC BY-SA 3.0,,"If $x_i = 0$, then $\log(p_i^{x_i}) = 0$.  I'd deal with it by dropping the corresponding $i$s from your model, setting the corresponding $\hat{p_i} = 0$ (the MLE), and fitting on the rest of the data.  (Then of course adding back in the elements you removed for fitting purposes.)",2013-10-10 23:18:13.720
103719,57261,16703.0,CC BY-SA 3.0,,"@jbowman I'm not sure if I understand what you mean precisely, could you elaborate? Do you mean, in this example, dropping $x_1$ for both $sample_1$ and $sample_2$, or only for $sample_1$? In the former case, I have many samples and for almost all $i$ at least one $p_i^{x_i}$ is 0, so then I don't have many left over. If it's the latter case I don't see how that would be done, but I'll know where to look.",2013-10-10 23:26:26.627
103720,57261,5448.0,CC BY-SA 3.0,,"Hmmm, I may have misunderstood the problem.  Are you assuming the $i^{th}$ element of $p$ has the same value across all the samples?  If so, it seems to me you can just add up the $x_i$ across your samples and use that total instead of doing a sample-by-sample calculation.",2013-10-10 23:30:04.633
103721,57230,10060.0,CC BY-SA 3.0,,"Are you thinking about a graph like a report card for each student? Or are you thinking of visualizing a lot of students on a graph? If it's the latter, how many?",2013-10-10 23:35:56.483
103722,57268,3183.0,CC BY-SA 3.0,,I don't understand what you mean about telling the models apart.  They have different coefficients. They have different variances in the random effects. So they're different. I'm not sure what I'm missing.,2013-10-11 00:02:51.807
103723,57268,9049.0,CC BY-SA 3.0,,"In addition to David's comment for extra clarifications. While I *really* appreciate the fact you went into the trouble of giving R-code unless you specify the random seed (eg. `set.seed(0)`) one will not be able to replicate your results/graphs. (In general, code comments would be a plus too.)",2013-10-11 00:20:03.413
103724,412,,CC BY-SA 3.0,user31367,[Here](http://bayesianthink.blogspot.in/2012/12/the-best-books-to-learn-probability.html#.UldEJdJHJBE) is good list of books for non-statistician scientists. Most of them are probability related and some are readable for ones with a non-statistics background. HTH,2013-10-11 00:23:27.370
103765,57278,1895.0,CC BY-SA 3.0,,"(+1) Nicely written question. Welcome to the site. Just to clarify: In one instance you know the parameters and want to sample from the distribution, but in the other you have *data* and want to find the associated parameters?",2013-10-11 12:00:20.583
103725,57268,18040.0,CC BY-SA 3.0,,"@DavidJ.Harris True, but I feel like I'm not fitting the right model in the first scenario.  I don't think it's the right model in the first scenario, because intuitively, all those lines seem to have the same slope and the same intercept.  So I guess I'm wondering, what's the 3rd parameter that says ""There's variation between groups in the slope, the intercept and the x-values of the groups"".  Perhaps my question is more, is there a better model to answer the preceding question?",2013-10-11 00:39:55.483
103726,57268,3183.0,CC BY-SA 3.0,,"If you want all the differently-colored groups to have the same slopes and same intercepts, then you can enforce that by removing the random effect.",2013-10-11 00:43:20.520
103727,57268,3183.0,CC BY-SA 3.0,,"In general, linear regression techniques don't have much to say about $x$, just about $y|x$. There's nothing in either model that says orange has to be on the left and blue has to be on the right.",2013-10-11 00:44:34.993
103730,57268,9049.0,CC BY-SA 3.0,,"+1 to David on this. You are fitting the *right* model. An LME model assumes your data follow a distribution like $y \sim N(X\beta, Z \Sigma_{\gamma} Z^T + \Sigma_{\epsilon})$. To quote: Pinheiro & Bates (2000) : *""random effects (...) can be regarded as additional error terms to account for correlation among observations within the group""*, ie. it has nothing to do with your $X$, only your $Z$. Now if your feel certain values of $X$ might be less reliable or something like that, that's another question not answered directly by an LME though.",2013-10-11 01:05:50.353
103733,57274,9049.0,CC BY-SA 3.0,,I would recommend using `nlme`'s `lme` or `lme4`'s `lmer` function. They are better documented and with cleaner syntax. In the long run their flexibility will definitely prove beneficial. I think what you are doing appears sensible. Just be weary of *multiple testing* and *data dredging* issues.,2013-10-11 02:42:26.963
103735,56784,5237.0,CC BY-SA 3.0,,"Interesting question. I don't know the answer, but the idea that some degrees of freedom should be lost makes sense. If you hadn't seen it already, this answer by @whuber should be thought-provoking: [how-to-understand-degrees-of-freedom](http://stats.stackexchange.com/questions/16921//17148#17148). It seems to me that some simulation studies should enable you to get a toehold here, at least for some specific cases.",2013-10-11 04:06:00.847
103736,57206,22555.0,CC BY-SA 3.0,,"Perhaps it is best to qualify the method further: Hahn and Shapiro (as referenced above) advise that caution should be exercised, especially when the sample size is less than 200 - and recommend that this be followed by further verification, such as a frequency table that compares the fitted distribution with the actual data.  But in my view it is a useful method that *can suggest* where the data might lay within a spectrum of possibilities.  I have used it on data sets not smaller than roughly 3000 and have built it into computer simulation software where it has proved useful.",2013-10-11 04:38:29.383
103738,57269,19822.0,CC BY-SA 3.0,,"Thanks Simone for the comments... As you mentioned in your comments, my objective is to  predict the class labels of records in the unlabeled dataset using RF. However, i would like to know whether this is possible using the RF code of Brieman and Cutler..??",2013-10-11 05:48:15.413
103739,57269,1741.0,CC BY-SA 3.0,,"I am not that familiar with that code. It might also be a bit outdated. Try WEKA or R, they provide recent random forest implementations.",2013-10-11 05:54:42.780
103740,57213,22190.0,CC BY-SA 3.0,,"I read your paper, but I am not able to understand what is the meaning of getting concordance and discordance as 25.0 and how to know by this whether the model is good or not",2013-10-11 05:57:39.060
103741,56784,,CC BY-SA 3.0,,"Not sure how helpful this is, but there is a similar problem in the field of robust estimation. Specifically, a method of robust estimation (eg trimmed mean) often requires a parameterized input (eg parameter defining how much to trim). This parameter can be chosen by a data-driven method (eg see how fat the tails are before choosing the trimming parameter). But pre-selecting the trimming parameter does affect the distribution of the trimmed mean, versus, say, a fixed parameter rule. The usual way it is dealt with in that literature is via a bootstrap.",2013-10-11 06:10:11.600
103742,57259,6162.0,CC BY-SA 3.0,,"i don't understand what you say. If you want to compare group means, the procedure is correct.",2013-10-11 06:21:29.663
103743,56784,594.0,CC BY-SA 3.0,,"@ColinTBowers -- potentially somewhat helpful, thanks. Didn't think about the possibility of bootstrapping.",2013-10-11 06:59:39.877
103745,57263,21762.0,CC BY-SA 3.0,,"You cannot distinguish the ""online program"" effect from any sort of time/placebo effect",2013-10-11 07:45:50.070
103746,57281,21762.0,CC BY-SA 3.0,,Just one pint out of 20 is defective.,2013-10-11 07:53:49.630
103747,57281,9074.0,CC BY-SA 3.0,,"Oh, sorry, didn't notice that. Will have to update the answer when I get home. Thanks for noticing.",2013-10-11 08:02:27.790
103748,57265,21762.0,CC BY-SA 3.0,,30 tests will need a huge sample size to keep the experiment wise error rate low.,2013-10-11 08:09:13.900
103753,57284,12683.0,CC BY-SA 3.0,,What are you trying to do - perform a test or estimate something with an associated confidence interval?,2013-10-11 09:13:05.887
103754,57186,22542.0,CC BY-SA 3.0,,"It is not you, jeremy - I struggle to explain my thoughts in English :)  I am looking, for example, at a school in an area that has changed significantly in recent decades. I am looking at people who enrolled in that school at three different times and asking them why they chose that school: for example, they wanted to go elsewhere but could not afford, school was their first preference, etc.",2013-10-11 09:55:49.540
103755,57186,22542.0,CC BY-SA 3.0,,"All analysis is in STATA, though I think I can access SPSS",2013-10-11 09:56:14.713
103756,57220,12683.0,CC BY-SA 3.0,,"Look [here](http://stats.stackexchange.com/questions/5450/what-if-interaction-wipes-out-my-direct-effects-in-regression), & at Ray's answer in particular. **There is no sense at all in worrying about the significance or otherwise of main effects if you have an interaction term in the model.**",2013-10-11 10:23:28.123
103757,57222,12683.0,CC BY-SA 3.0,,"What do you mean by ""center *earnings* before taking the logarithm""? It can't be to subtract the sample mean earnings or you'd be trying to take logs of negative numbers.",2013-10-11 10:37:31.057
103761,57288,21762.0,CC BY-SA 3.0,,"Not completely sure about what you mean by ""reversing"": Obviously $|X - \mu|$ is the same as $|\mu - X|$, where $\mu$ denotes true mean of $X$. In this way, the answer is ""yes"".",2013-10-11 11:12:47.790
103762,57287,21762.0,CC BY-SA 3.0,,"If you already have the population at hand, why would you use bootstrap samples to make inference about this population?",2013-10-11 11:14:19.080
103763,57230,22569.0,CC BY-SA 3.0,,"Kind of. However the graph will only be there for one 'student' or 'user'. It will go up on their profile page, each user has their own set of analytics.",2013-10-11 11:16:47.063
103764,57288,,CC BY-SA 3.0,,"Very interesting, an extremely subtle question.  I think the point he is getting at @MichaelMayer is that although $|X-\mu|$ is numerically $|\mu-X|$ the distribution of $X|\mu$ is not the same as $\mu|X$.  My feeling is this either leads us into Bayesian statistics, where it will all fall out fine once you specify priors etc., or into those tortured lawyer-esque frequentist statements about exactly what a confidence intervals is.",2013-10-11 11:51:31.193
103766,57291,,CC BY-SA 3.0,,This question appears to be off-topic because it is has no statistical content other than finding a function/specific R package. You can try http:/www.rseek.org ,2013-10-11 12:08:11.700
103767,57288,21762.0,CC BY-SA 3.0,,"@Corone: You are right, I didn't realize the Bayesian flavor of the question :-)",2013-10-11 12:09:57.120
103768,57278,20144.0,CC BY-SA 3.0,,"@cardinal well, ideally the process would be find the parameters -> make a prediction given $E(x)$ with those parameters by sampling from the distribution. The idea is that through my experiments $E(x)$ can change, albeit not randomly, but $T$ can't (it is a thermodynamical property). But I don't know $T$ and I have some data. Is this approach incorrect? Just as a comment, we can consider $E(x) = \mathbf{E}\cdot\mathbf{x}$, but as I said it is not random, I want to predict the most likely state given such $\mathbf{E}$",2013-10-11 12:13:32.717
103769,57278,20144.0,CC BY-SA 3.0,,"And finally, in the data I have I know $\mathbf{E}$ and the marginal distributions of each $x_{i}$.",2013-10-11 12:14:33.283
103772,57293,21762.0,CC BY-SA 3.0,,Is it possible to include all PCs?,2013-10-11 12:33:54.143
103773,57287,21624.0,CC BY-SA 3.0,,"@MichaelMayer Yes, I could. The problem is there are many populations, and it could be better if I have an uniform method to do so. :)",2013-10-11 12:35:43.703
103775,57242,22573.0,CC BY-SA 3.0,,Thanks. I updated my question. Hope this makes it more clear.,2013-10-11 12:40:18.537
103777,57298,21762.0,CC BY-SA 3.0,,The first integral should start at 1 (maybe its just a typo),2013-10-11 12:43:41.530
103778,57288,19436.0,CC BY-SA 3.0,,"@Corone Yes, the question is can we argue about $\mu|X$ and what may be a suitable rigorous mathematical framework.  I'll not accept ""Bayesian statistics"" as an answer, that's too cheap :)",2013-10-11 12:54:28.643
103780,57300,19395.0,CC BY-SA 3.0,,Thank you so much for you answer!It's funny how Stata does not allow to choose an exact result (I am assuming the program assumes normality). I guess I have to dust off my R knowledge...,2013-10-11 13:32:37.647
103781,57288,1889.0,CC BY-SA 3.0,,"Suppose I observe the value $19.21886$ from an unknown distribution (which might be a Cauchy distribution of unknown centre and spread, or from something completely different).  I do not even know if the population distribution has a mean or standard deviation, so it is difficult to say much in general about how far the mean is from my observation.",2013-10-11 13:42:36.290
103782,57242,668.0,CC BY-SA 3.0,,"It does not sound like computing a covariance matrix would address your problem. Instead, it seems that you may have one or more measurements of the distance of something (along with its orientation), made with a known amount of radial error, and you wish to estimate the location and quantify the uncertainty in that location. Would this be an accurate interpretation?",2013-10-11 13:58:37.167
103783,57206,668.0,CC BY-SA 3.0,,"I can see your method giving useful information with datasets of 3000 or greater. However, then there is no need to perform distributional testing to assess the applicability of a t-test of the mean.",2013-10-11 14:00:52.337
103784,57300,15827.0,CC BY-SA 3.0,,"@asdir What you tried in Stata is not clear, but see the help on `permute` for an example of use with `ranksum` or `search somersd` for more general approaches.",2013-10-11 14:03:53.980
103785,57242,22573.0,CC BY-SA 3.0,,"yes, that's exactly what I'm trying to do...",2013-10-11 14:08:11.717
103786,57259,22564.0,CC BY-SA 3.0,,Well is there a procedure I can use to do this that will give me a wider estimate for group mean if the measurements are less precise? Regardless of any math theory such a procedure is bound to be closer to what I have in mind. I messed around with rjags and the credible intervals for group level means seem to scale with the within-subject variance. Is this what I want to do?,2013-10-11 14:08:56.487
103787,57285,21108.0,CC BY-SA 3.0,,"(BTW: I was using the formula as you defined it, I mixed A and B when writing the question. Good you notice!)",2013-10-11 14:09:31.303
103788,57259,6162.0,CC BY-SA 3.0,,"@Flask, as I said, the within-variance is not ignored. Try some simulations, use ""my"" method, increase the within-variance and you will see that the intervals become larger (because by increasing the within-variance you increase the total variance).",2013-10-11 14:12:57.213
103789,57297,668.0,CC BY-SA 3.0,,"Could you please clarify whether you want to test equality of *eigenvalues* (as stated in the first sentence), *eigenvectors* (as in the penultimate paragraph), or the full *""eigendecomposition""* (as in the title)?  (The latter would be equivalent to testing equality of the correlation matrices.)",2013-10-11 14:13:38.863
103790,57288,668.0,CC BY-SA 3.0,,"If you do not assume a prior distribution for (mean, SD), then it is hard even to make sense of this question. (What does it mean to be ""close""? What does it mean to talk about the ""probability"" of the mean?) So I do not think you can reject a Bayesian solution as ""too cheap.""",2013-10-11 14:28:23.990
103791,57305,22143.0,CC BY-SA 3.0,,$\int f(x)^2 dx$ is also $\mathbb{E}[f(x)]$ where the expectation is with respect to $X \sim f(x)$. Does this help?,2013-10-11 14:28:31.940
103792,57220,,CC BY-SA 3.0,,The relevant concept here is 'marginal effect'. Ask instead whether (i.e. within what range or setting) *that* is significant.,2013-10-11 14:34:47.123
103793,57305,1895.0,CC BY-SA 3.0,,**Related**: http://stats.stackexchange.com/questions/9926,2013-10-11 14:37:48.093
103794,57293,1895.0,CC BY-SA 3.0,,Have you (already) read the relevant section on this in *Elements of Statistical Learning*?,2013-10-11 14:44:19.050
103795,57306,9522.0,CC BY-SA 3.0,,"Thanks a lot for your answer. I also though that Fisher´exact test could be a good method for the analysis. I have not any statistic software to perform the results for other functional classes I would like to test too. Do you know any ""online"" tool to obtain the pvalues with all the decimals?",2013-10-11 14:55:33.103
103796,57305,1895.0,CC BY-SA 3.0,,"Tom, Do the comments and the link answer your question or are you looking for something further?",2013-10-11 15:14:07.703
103797,57306,15827.0,CC BY-SA 3.0,,You can download R for free. See http://www.r-project.org/ So having no software is soluble (and thinking that you need a way of calculating online is incorrect). But please do a little searching to find out these things for yourself. See advice at http://stats.stackexchange.com/help/how-to-ask on asking a good question.,2013-10-11 15:16:11.793
103798,57307,15827.0,CC BY-SA 3.0,,"See answer and comments at http://stats.stackexchange.com/questions/72553/which-statistic-test-to-used which already give implicit and explicit answers, namely download R.",2013-10-11 15:18:07.647
103799,57288,20473.0,CC BY-SA 3.0,,"You need to clarify whether you are looking at $\mu|X$ (@Corone style and a comment of yours) or $\mu|x$ (@Henry style and the main body of the question)... and I won't accept ""both"" as an answer, that's too expensive (to answer). :)",2013-10-11 15:32:26.183
103800,57308,22262.0,CC BY-SA 3.0,,My data is not of the form to trust a test on the Pearson correlation. In fact it would violate almost every assumption of such a test. Is this still the approach you would take?,2013-10-11 15:37:06.073
103828,57302,750.0,CC BY-SA 3.0,,"Yes you are correct about the last typo (I meant slopes not intercepts). I read the updates, and besides my answer and comment (and David Harris's comments) I don't see what else you are confused about exactly.",2013-10-11 17:19:38.033
104262,57477,594.0,CC BY-SA 3.0,,"I have gone into more detail. If that doesn't suffice, please indicate clearly where the problem lies.",2013-10-15 02:47:33.973
103801,57259,22564.0,CC BY-SA 3.0,,"This function does not include any information regarding within-subject variance: `dd <- aggregate(tvalue~Group+Subject, data=tdat, FUN=mean)`. I have run simulations where I sample each individual value from  `dnorm(Subject_mean, sd)` and with sd from `seq(.001, 10, by=.001)`. It is true that larger sd widens the distribution of pvalues, but this is only because the means differ from the actual data. If the means are the same I will get the same pvalue.",2013-10-11 15:37:29.833
103802,57308,22507.0,CC BY-SA 3.0,,How did you produce the data?  Why you cannot trust a test on the Pearson correlation?  Please elaborate.,2013-10-11 15:42:57.830
103803,57265,2490.0,CC BY-SA 3.0,,"Assuming you mean family-wise or experiment-wise error rate, I don't see how this is 30 tests. So I don't think that applies here, but I would be interested in hearing why you think it is. It's one test/experiment, with a single hypothesis and a single variable with two levels. One of the levels could be considered the control.",2013-10-11 15:44:15.237
103804,57259,22564.0,CC BY-SA 3.0,,I have added the code and result to the question,2013-10-11 15:49:04.647
103805,57308,22262.0,CC BY-SA 3.0,,"Heteroscedasticity, non-normality, autocorrelation, non stationarity. Pearson correlation test unbiasedness is known to be extremely susceptible to violations of its assumptions.",2013-10-11 15:51:28.810
103806,57206,22555.0,CC BY-SA 3.0,,"Whether one views this as a useful technique, as I do, or otherwise, as appears to be your view, it nonetheless is a quick and long-established (by Pearson) alternative to testing for normality (and Students-t application) in the context of this thread.  Please don't get me wrong, I acknowledge and do agree with your concerns.  But we would both agree, would we not, that without prior information, trying to establish whether an entire population can be modelled on a Gaussian from a very small data sample is a shot in the dark at best with any method, and at worst is dangerous.",2013-10-11 15:52:57.690
103807,57254,22583.0,CC BY-SA 3.0,,Dr Huber - I do agree with you that the best way would be to destroy a small sample of the packets to assess the intra-packet variability. I was just hoping to to use mathematical statistics to come to a conclusion of some sort without destroying packets. I gave the question some more thought I have updated my original post. Your thoughts?,2013-10-11 16:03:09.577
103808,57206,668.0,CC BY-SA 3.0,,"That's right. All I am saying is that if it is dangerous to try, from a small sample, to test whether the population is Gaussian, then it must be at least as dangerous to use the skewness and kurtosis to identify what the underlying distribution might be! In fact, it seems like such an attempt would actually be worse because it relies on unstable statistics like the kurtosis. Although Pearson's system can be a powerful guide to help people identify possible distributions, it provides less insight than even limited graphical displays like histograms.",2013-10-11 16:15:50.073
103809,57307,668.0,CC BY-SA 3.0,,"@Nick I deleted the first part of your comment in the spirit of http://stats.stackexchange.com/help/behavior. I know you were helping, but we should take care to consider the feelings of newcomers who are not used to the site and the necessarily abbreviated conversations that occur in comments.",2013-10-11 16:17:54.363
103810,57206,22555.0,CC BY-SA 3.0,,"It can be argued that the Student's-t should not be applied unless there is prior information that the population is in fact Gaussian, as it is designed for less than 30 samples anyway.  The Students-t is in essence a way to 'narrow the spread' of a predicted population as a small number of samples is increased, but the assumption is that the population must be Gaussian to start with.",2013-10-11 16:19:33.527
103812,52871,18447.0,CC BY-SA 3.0,,@gjabel it's a huge data set.I'm not sure how I can provide a slim data.Do you the problem due to data?,2013-10-11 16:25:03.543
103813,57206,668.0,CC BY-SA 3.0,,"That argument continues to take place.  Going back at least to Box 50 years ago, many have pointed out that the t-test actually is an approximation to a non-parametric (permutation) test: it does not require Normality of the population, but only approximate Normality of the *sampling distribution* of the mean. Simulations (and some theory) indicate the t-test may break down with skewed distributions, but even then it can be surprisingly robust.",2013-10-11 16:25:34.213
103814,52871,18447.0,CC BY-SA 3.0,,@DistribEcology I've tried `th` instead of `t` as variable name but it did resolve the problem,2013-10-11 16:27:09.267
103815,57206,22555.0,CC BY-SA 3.0,,"@whuber, I surrender.  You've just exceeded my depth of knowledge on this subject ;-}",2013-10-11 16:28:59.410
103816,57302,18040.0,CC BY-SA 3.0,,This has been helpful in me restating the question.  What I'm getting at is how could I tell apart the following scenarios.,2013-10-11 16:29:30.040
103817,57266,18447.0,CC BY-SA 3.0,,"How can I narrow down priors of variances while I'm using Wishart Dist.? The `cov[1:2,1:2]` is an identity matrix and I set df of Wishart Dist. as 2 because of the dimensions of `th`. I completely agree with you about trickiness of BUGS models.",2013-10-11 16:32:05.323
103818,57288,19436.0,CC BY-SA 3.0,,"@whuber I'm not rejecting a Baysian solution at all.  I'm rejecting just ""Bayesian statistics solves this"" as too unspecific for an answer.",2013-10-11 16:48:08.690
103819,57284,,CC BY-SA 3.0,user14650,p is significant if it is smaller than the significance level. significance level must be chosen to fit the research topic. it is not a fixed value.,2013-10-11 16:59:51.547
103820,57307,17249.0,CC BY-SA 3.0,,"Why do you need a p-value more exact than <.0001? I could have seen why you might have wanted an exact p-value if it had been > .001, but is there a reason why you want to be more precise than ""<0.0001"" ?",2013-10-11 17:02:53.247
103821,57286,,CC BY-SA 3.0,user14650,you cannot interpret p without knowing the conf. level alpha,2013-10-11 17:04:54.080
103822,57288,19436.0,CC BY-SA 3.0,,"@AlecosPapadopoulos in Vapnik-Chervonenkis theory it's $\mu|x$ (that is, actual values, data).  Also, it is not Chebyshev's inqequality, but a large deviation type inequality that is inverted.  If you happen to have one of their books around, any of the results that starts with: ""With probability $1-\delta$"" I would call unfounded.",2013-10-11 17:08:25.550
103823,57310,17249.0,CC BY-SA 3.0,,(+1) I think this is better than suggesting to the OP to download R. I was not aware of this online resource.,2013-10-11 17:09:40.097
103824,57302,750.0,CC BY-SA 3.0,,"@DistribEcology, are you asking about whether `x` significantly varies between `g`? That might be best answered by boxplots of `x ~ g` or an anova. If you are asking about whether `x` has a differing effect in different `g`, then you can use the example in my answer as an inferential test between the models (one with random effects and random intercepts and one with only a random intercept).",2013-10-11 17:09:51.210
103825,57259,6162.0,CC BY-SA 3.0,,@Flask This discussion is too long. Maybe you should isolate this new specific question and open a new thread.,2013-10-11 17:17:36.347
103826,57302,18040.0,CC BY-SA 3.0,,"I tried to elucidate the question by adding significant edits to the question, which maybe you can check out.  But I'm not sure I follow that adding random intercepts adds nothing.  Do you mean slopes?  Clearly there's lot's of variance between intercepts, but none between slopes.",2013-10-11 17:18:02.863
103890,57286,,CC BY-SA 3.0,user14650,why then does the function wilcox.test expect an argument conf.level and set a default for it?,2013-10-12 08:48:47.137
103829,57302,18040.0,CC BY-SA 3.0,,"Well, I suppose @DavidJ.Harris comments not withstanding, I wonder what is the appropriate model to make inferences that there are grouping is correlated with both X and Y.  Is there a cohesive model formulation I'm missing, or just to to fit the mixed effects model and throw in a secondary ANOVA with `x ~ g` That's what I'm trying to get at.",2013-10-11 17:26:40.743
103830,57298,3446.0,CC BY-SA 3.0,,"There's no such thing as a cumulative density function.  The word ""cumulative"" contradicts the word ""density"".  See this disambiguation page on Wikipedia: http://en.wikipedia.org/wiki/Cumulative_density_function",2013-10-11 17:27:57.960
103831,57302,750.0,CC BY-SA 3.0,,"Like David said in the comment, it seems your asking about the between group variation for `x` within `g`, which doesn't have anything to do with a model of `y = x|g`. `x` varying within `g` is neither a necessary nor sufficient condition for intercepts or slopes to vary when predicting `y`. So a second test seems appropriate to answer that question - although your example graphic sort of says it all in this example.",2013-10-11 17:30:41.850
103832,57263,22585.0,CC BY-SA 3.0,,"Peter, sorry, should have been clearer. This site has coaching tools and online content to help people manage these mental health conditions, so the ""treatment"" is the use of these resources.",2013-10-11 17:32:48.320
103833,57306,668.0,CC BY-SA 3.0,,"@Nick Your advice is good, but please do not couch it as a characterization of the poster: such phrasing is all too easily misunderstood as an attack, which I doubt you intended.  Therefore I removed the preliminary phrase in your comment (which added no information to it).",2013-10-11 17:48:25.727
103835,57302,18040.0,CC BY-SA 3.0,,I guess I was hoping there would be a nice elegant modeling framework that I was missing. But I take your and David's point.,2013-10-11 17:57:14.757
103836,57311,668.0,CC BY-SA 3.0,,"+1 It's a good question. Note that it is pertinent even to the case of ""continuous"" dependent variables: in many situations it is not the case that numerical differences have the same meaning or interpretation regardless of the levels of the original values.",2013-10-11 18:03:30.937
103837,57314,668.0,CC BY-SA 3.0,,"Comparisons to the ""real world"" are next to impossible except in contrived situations: after all, we compute the CIs precisely because we do *not* know the values they target. However, there are a huge number of simulation studies of coverage of CIs: it is practically unimaginable that there exists any CI in existence that has not been studied in that way.  Having said that, there are some notable exceptions, such as a study of the [CIs for the speed of light provided by physicists](http://books.google.com/books?id=ajd1V305PgQC&pg=PA58#v=onepage&f=false).",2013-10-11 18:06:00.040
103838,57259,22564.0,CC BY-SA 3.0,,Hopefully the answers to this question will help me: http://stats.stackexchange.com/questions/72573/when-making-inferences-about-group-means-are-credible-intervals-sensitive-to-wi,2013-10-11 18:06:51.590
103839,57297,18198.0,CC BY-SA 3.0,,"Sorry I do mean test the full eigen-decomposition, however not that you mention it, being able to test the eigenvectors seperately would be nice so I can try to understand where any differences are coming from.",2013-10-11 18:07:42.517
103840,57293,18198.0,CC BY-SA 3.0,,"Sorry Micheal can't use all the PC's , I'm trying to reduce the multi-collinearity by removing the smallest PC's. Will Look at Elements of statistical learning!",2013-10-11 18:11:29.377
103844,57302,3183.0,CC BY-SA 3.0,,"You can evaluate whether ""Part of that difference between districts is because of the underlying relationship between spending and test scores"" by seeing how much of the variance associated with your district grouping factor disappears when you remove (or randomize) spending.",2013-10-11 18:46:24.837
103845,57266,18040.0,CC BY-SA 3.0,,I'd try modifying the inits you draw from the rgamma in your init function as a place to start.,2013-10-11 18:56:03.643
103846,57315,16474.0,CC BY-SA 3.0,,"One thing I noticed that the variances in your original example are very small 6.5e-6 = 0.00000065. It depends a bit on how you scaled your variables, but to me this suggest that either you need to rethink the scale of your variables or your variance is _de facto_ zero",2013-10-11 19:19:44.163
103847,57314,3446.0,CC BY-SA 3.0,,"We do not know the true values at the time we form confidence intervals, but in some cases one learns them later.",2013-10-11 19:49:46.893
103848,57319,2081.0,CC BY-SA 3.0,,"It sounds in your text that you equate ""reversing the sign of the coefficient"" with ""suppressor effect"". But actually these two are different phenomena. Suppressing can exist without sign reversal, and vice versa.",2013-10-11 19:51:37.463
103849,57314,668.0,CC BY-SA 3.0,,"Yes; that is the case with the speed of light experiments, which cover 90 years and ultimately are compared to a consensus value obtained 25 years after that. But it took a long time to pin down even this fundamental physical constant. In other fields (economics, for instance), finding true values typically is impossible, yet their estimates likely are subject to much more unexpected and unmodeled error. We should be cautious in generalizing CI coverage results from one field to another or even from one kind of experiment to another within a field.",2013-10-11 20:00:14.623
103850,57316,503.0,CC BY-SA 3.0,,What are the counts? Are they all small numbers or do they vary over a wide range?,2013-10-11 20:13:02.473
103852,57321,22564.0,CC BY-SA 3.0,,"That model makes sense, yet the code does not seem to incorporate that information and I guess this is what I would like to understand. Am I doing this right for parameter estimation?: `lmer(Value~Group -1 + (1|Subject), dat)` `lmer(Value~Group -1 + (1|Subject), dat2)`, where dat is the original data and dat2 is the simulated with small within-subject variance. I get the same standard errors.",2013-10-11 20:18:35.103
103854,57321,6162.0,CC BY-SA 3.0,,"I have not tried, but that sounds strange, you remove the fixed intercept but there is a random intercept by subject. From the theoretical point of view I don't see any problem but I don't exactly know how `lmer` deals with models without interecept. Keep the intercept to be sure.",2013-10-11 20:31:10.460
103855,57321,22564.0,CC BY-SA 3.0,,I followed [this instruction](https://stat.ethz.ch/pipermail/r-help/2008-April/160074.html) as I could not otherwise figure out how to get an interval estimate. My understanding of the R formula syntax is low so maybe it makes no sense.,2013-10-11 20:34:51.587
103856,57315,9049.0,CC BY-SA 3.0,,I don't seem to be able to replicate your results. Which version of `lme4` are you using? If you are not using version 1.0-4 or newer I would recommend upgrading before anything else. Currently I get `a  failure to converge in 10000 evaluations` message.,2013-10-11 20:39:46.030
103857,57261,,CC BY-SA 3.0,,Can't you just add a very tiny number to each element and renormalize?,2013-10-11 20:56:36.737
103858,57265,21762.0,CC BY-SA 3.0,,You are mentioning multiple Likert-type items. I thought you would want to compare them between groups.,2013-10-11 21:08:51.470
103859,57321,6162.0,CC BY-SA 3.0,,"@Flask AFAIK there's currently no package in R providing a way to get ""correct"" confidence intervals for `lmer` models. For your model in  the particular case of a balanced design there exist some exact least-squares methods, but I don't know whether they are available in some package.",2013-10-11 21:37:44.797
104123,57436,1693.0,CC BY-SA 3.0,,I'll study this.  Did you intend 'x.and.z' instead of 'x.and.y' in lines 4 and 5 of your R code?,2013-10-14 14:02:09.670
103860,57328,668.0,CC BY-SA 3.0,,"Note that in removing the means you have made your sequences not quite iid: values now have a slight negative correlation.  In standardizing the covariance matrix you will exacerbate that somewhat. There is an inevitable trade-off between maintaining independence *within* each sequence and no correlation *between* sequences.  If that's ok with you, then the next question is one of computational efficiency: although there are readily available solutions (e.g., SVD), they may start to founder as $M$ and $N$ grow large.",2013-10-11 21:39:58.693
103861,57321,6162.0,CC BY-SA 3.0,,Though I wonder whether the `lsmeans` package together with the `pbkrtest` package could provide good confidence intervals.,2013-10-11 21:40:26.713
103862,57305,22607.0,CC BY-SA 3.0,,"Yes I think they're good, & enough to answer my question. Thanks cardinal (not sure how to mark this as answered).",2013-10-11 21:43:28.013
103863,57273,594.0,CC BY-SA 3.0,,"Are you after a basic but general discussion of MCMC overall, a description of a specific kind of MCMC, or a specific discussion of it in relation to your problem?",2013-10-11 21:43:52.313
103864,47981,6162.0,CC BY-SA 3.0,,Do you think that this statistical significance has a practical significance ? Be aware of the meaning of statistical significance before claiming it.,2013-10-11 22:32:41.417
103865,47981,5821.0,CC BY-SA 3.0,,"I dunno... maybe we should run a double bootstrap and calculate a confidence interval for the $p$-value! In all honesty, I would report: ""The findings were borderline significant, $0.049 < p < 0.050$."" At that point, you're splitting hairs, and everyone suddenly remembers that 1/20 odds of a false positive is a completely arbitrary way to run science.",2013-10-11 22:56:51.217
103866,57329,594.0,CC BY-SA 3.0,,"(1) I'm not sure you can necessarily compare a GLM and a time series model via BIC. (2) In any case which you used depends on what you want to do well at; even when BIC's are comparable, BIC is no guarantee of out of sample performance. *Why* do you want to optimize on one or the other?",2013-10-12 00:09:41.507
103867,57284,12683.0,CC BY-SA 3.0,,You seem to be confusing the confidence level of a confidence interval - in this case for the pseudomedian - with a p-value for a test. I'll post something later.,2013-10-12 00:11:34.483
103868,57336,594.0,CC BY-SA 3.0,,"With large enough samples, even trivial differences may be statistically significant. Is a difference in the third significant figure of any *practical* importance (how much difference could it make to you, really)? If there's no practical difference, there's no point testing for statistical significance.",2013-10-12 00:13:59.900
103869,57314,594.0,CC BY-SA 3.0,,"One major problem is the issue of biased intervals (e.g. caused by a model that misses a possibly small but important effect). Perhaps counterintuitively (until you understand [what's happening](http://stats.stackexchange.com/questions/66473/in-what-settings-would-confidence-intervals-not-get-better-as-sample-size-increa/66475#66475), at least), in real world problems coverage tends to get *worse* as sample sizes increase. For example, a 90% interval might have 88% actual coverage at $n=20$ and say 25% actual coverage at $n=10000$... and one that keeps decreasing with larger $n$.",2013-10-12 00:20:36.260
103870,57329,10135.0,CC BY-SA 3.0,,"Do you have any reference showing that we cannot compare GLM and time series using BIC? Because to me, it is possible since BIC just depends on estimated log likelihood and number of parameter and number of observations. These models can be used to price some products and you want your price to be unique. So at the end you need to pick up one.",2013-10-12 00:24:19.637
103871,57329,594.0,CC BY-SA 3.0,,"Having seen the particular assumptions under which BIC was derived, I don't see how the comparisons implied by that derivation applies to your situation; the [onus would be yours](http://en.wikipedia.org/wiki/Philosophic_burden_of_proof#Holder_of_the_burden) to show that what you're doing makes sense. [In fact I have one reference that says you can't compare *likelihoods* across models with different error distributions, which if it were correct would wipe out a lot more than just BIC. I don't know that the claim of the reference is correct, though.]",2013-10-12 01:02:07.340
103872,57317,22564.0,CC BY-SA 3.0,,Well I just found this question. No answer was accepted: http://stats.stackexchange.com/questions/12002/how-to-calculate-the-confidence-interval-of-the-mean-of-means?rq=1,2013-10-12 01:03:28.597
103873,57329,594.0,CC BY-SA 3.0,,"Some related questions: [1](http://stats.stackexchange.com/questions/65455/can-you-test-likelihood-ratio-between-different-models)  [2](http://stats.stackexchange.com/questions/43312/can-i-use-a-likelihood-ratio-test-when-the-error-distributions-differ); there are a number of others as well. As you see from [2], even if you can compare the likelihoods a problem comes up; this problem would apply to a comparison of BICs (the variance difficulty would translate to a shift-issue in difference of BIC's - if one BIC involves an unknown constant not present in the other, what does one do?)",2013-10-12 01:05:22.633
103876,57273,22596.0,CC BY-SA 3.0,,"Well, everything I find in terms of explanations seems to reference systems that already follow some distribution, like Monopoly. I was hoping for an explanation in terms of trying to fit a model to a preexisting set of data, which I cannot seem to find an example of. So, I suppose a discussion of how it relates to my specific _type_ of problem?",2013-10-12 04:51:13.130
103877,57273,594.0,CC BY-SA 3.0,,Then you'll need to explain your problem better.,2013-10-12 05:11:47.650
103878,57286,594.0,CC BY-SA 3.0,,"@what *You* choose your $\alpha$. The program doesn't need to know what you choose. I might choose a different significance level, but we both compare it to the same p-value.",2013-10-12 05:41:19.820
103880,57308,22507.0,CC BY-SA 3.0,,What do you mean by autocorrelation and non-stationarity? Your dependent variable and predictors are time series?,2013-10-12 06:04:26.677
103881,57343,21243.0,CC BY-SA 3.0,,"What exactly do you mean by ""not nearest to any point"", exactly? It would seem that by definition, if there are centroids living in a real or other metric space with points, a given point must necessarily be nearest to some centroid.",2013-10-12 06:21:32.687
103882,57343,22629.0,CC BY-SA 3.0,,For example if we pick 3 centroids and all the datapoints are nearest to either centroid 1 or 2 .In such case all the points would be assigned to centroid 1 or 2 and centroid 3 would not have any points assigned,2013-10-12 06:30:42.827
103883,57343,21243.0,CC BY-SA 3.0,,"Aha. In that case, the points can be assigned to one arbitrarily without any real issue. 

Additionally, you might also find interesting some of the methods of choosing the initial points, such as [K-Means++](http://en.wikipedia.org/wiki/K-means%2B%2B).",2013-10-12 06:39:13.097
103884,57343,436.0,CC BY-SA 3.0,,@LCialdella: I think what he means is the situation where one of the centroids has no points assigned to it.,2013-10-12 06:55:46.783
103885,57343,14799.0,CC BY-SA 3.0,,Try [K-means++](http://en.wikipedia.org/wiki/K-means%2B%2B).,2013-10-12 07:08:19.033
103886,57343,22629.0,CC BY-SA 3.0,,@LCialdella .You got me wrong.I might not have explained clearly .I meant what 'nico' has mentioned.,2013-10-12 07:13:49.123
103887,57343,22629.0,CC BY-SA 3.0,,@nico could you please let me know what needs to be done in such scenarios,2013-10-12 07:14:31.033
103888,57308,22262.0,CC BY-SA 3.0,,Yes these are time series which display autocorrelation and non-stationarity. e.g. in the post I talk about pass bands and denoising.,2013-10-12 08:14:08.087
103891,57271,11490.0,CC BY-SA 3.0,,"Thanks for your answer Alecos. I started with a similar idea, but then I was discouraged by the number of unknown parameters. One question: even if you estimate all the $\gamma$s you still don't have an estimate for the transfers $G^{A -> B}$ etc, right?",2013-10-12 09:57:35.030
103893,57317,6162.0,CC BY-SA 3.0,,"It's curious that nobody here seems to know my ""trick"". I have just answered this question.",2013-10-12 10:05:43.587
103894,57271,20473.0,CC BY-SA 3.0,,"No, you do (an estimate that is), because, say $\hat G_{t-1}^{A -> B} = \hat \gamma_{21}A_{t-1}$. Etc. This is the whole point.",2013-10-12 10:13:52.430
103897,57343,2081.0,CC BY-SA 3.0,,"I see no problem. A cluster may stay empty, after all. I checked your situation - with one initial center far away - in SPSS, which uses Hartigan (1975) algorithm. There comes out an empty cluster without any error message.",2013-10-12 10:38:55.440
103900,57350,594.0,CC BY-SA 3.0,,"No doubt someone will chime in with formal definitions, informally, all expectations are expectations over the distribution of (/expectation with respect to) some (possibly multivariate) random variable, whether it has been explicitly specified or left implied. In many cases it's obvious ($\text{E}(X)$ implies $\text{E}_X(X)$ rather than $\text{E}_W(X)$). Other times, it's necessary to distinguish; consider the law of total variance for example: $\text{Var}[Y] = \text{E}_X\left[\text{Var}[Y\mid X]\right] + \text{Var}_X\left[\text{E}[Y\mid X]\right]$.",2013-10-12 11:32:52.627
103901,57349,22630.0,CC BY-SA 3.0,,"@NickCox I've added the link. In first senetence it asked us to subtract mean of each pixel overall images but standard deviation is on over all pixels and all images, so, in SD formula which mean should I use is it mean of that pixel position or mean of all pixels of all images? More importantly, should I take means and sds differently for r,g,b domains or combine rgb as one value and calculate this.",2013-10-12 11:54:03.867
103902,57349,22630.0,CC BY-SA 3.0,,"@NickCox Thank you very much!, if possible consider adding an answer. More importantly, should I take means and sds differently for r,g,b domains or combine rgb as one value and calculate this?. In general what is preferred?",2013-10-12 12:03:35.697
103903,57349,15827.0,CC BY-SA 3.0,,"Glad that helped, but now this is a morphing into a quite different new question in image processing, and (1) you should pose that in a new thread (2) it's not clear to me that it is essentially a statistical question that belongs here (3) sorry, but I am not experienced enough in that field to advise you.",2013-10-12 12:07:32.573
103904,57349,22630.0,CC BY-SA 3.0,,@NickCox I mean if you don't mind please add answer to this thread question so that I can mark as accepted. I don't need an answer for the question in the comment. Sorry if I'm troubling you.,2013-10-12 12:11:42.293
103905,57349,15827.0,CC BY-SA 3.0,,OK; I combined my earlier comments into an answer (and deleted the corresponding comments).,2013-10-12 12:16:56.620
103906,57319,1693.0,CC BY-SA 3.0,,"Hi ttnphns.  A reversal is one way a suppressor effect can work, would you say that's right?",2013-10-12 12:41:27.780
103907,57319,2081.0,CC BY-SA 3.0,,"Yes, I think. Adding a suppressor can reverse the sign of a coefficient. As well as not. So what is your question about - a suppressing phenomenon or a changing of a sign phenomenon?",2013-10-12 13:05:23.213
103908,57286,12683.0,CC BY-SA 3.0,,@what: It doesn't expect it - it's optional - & it's for something other than the Wilcoxon signed-rank test itself: the computation of a confidence interval for the pseudomedian. It's explained in the help page.,2013-10-12 13:09:02.730
103909,57308,22507.0,CC BY-SA 3.0,,"Then I would still use the same approach, each time adding the most correlative predictor, but instead of the statistical significance, I would use the number of predictors as a metavariable.  Note however that the autocorrelation makes predictors more, not less, significant, so you never want to add insignificant predictors. To determine metaparameters, I'd use a validation by rolling.",2013-10-12 13:26:01.770
103910,57317,6162.0,CC BY-SA 3.0,,I have just taken a quick look at your JAGS model. It is different than the frequentist model because you assume a different variance for each subject (nested in group).,2013-10-12 13:26:29.833
103913,57296,21398.0,CC BY-SA 3.0,,I will also be happy when it is well done :-D,2013-10-12 13:44:55.203
103914,57318,6630.0,CC BY-SA 3.0,,What's VIF? What's MC?,2013-10-12 14:07:48.923
103915,57356,8671.0,CC BY-SA 3.0,,"thanks for your reply. But how can I find the dependency between (c=1, k=1) and (c=2, k=1) ?",2013-10-12 14:12:01.880
103916,57356,6630.0,CC BY-SA 3.0,,"what do you mean by (c=1,k=1)? is that a random quantity?",2013-10-12 14:15:00.643
103917,57356,8671.0,CC BY-SA 3.0,,For example I want to find the dependency between the cluster 1 (k=1) and a class (c=1).,2013-10-12 14:21:35.523
103918,57329,20473.0,CC BY-SA 3.0,,"@Glen_b I believe that this paper of Vuong (1989), http://www.jstor.org/discover/10.2307/1912557  provides a general framework for non-nested models.",2013-10-12 14:26:02.840
103919,57355,9446.0,CC BY-SA 3.0,,Perhaps your question is answered [here](http://stats.stackexchange.com/questions/60383/bonferroni-adjustment-in-spss-what-does-it-do).,2013-10-12 14:28:09.760
103920,57356,6630.0,CC BY-SA 3.0,,what is random in $k=1$? It's always $k=1$. You can't quantify a statistical dependence if there's no randomness.,2013-10-12 14:40:31.563
103921,57356,8671.0,CC BY-SA 3.0,,K and c are random variables here. I want to calculate the dependecy between a particular k and particular c. how to do it with MI?,2013-10-12 14:41:38.773
103922,57313,22611.0,CC BY-SA 3.0,,"Thank you for the great response. However, I am confused by this:

""You should always correlate exogenous variables ...""

Everything I've read on the subject (e.g., Byrne's book on SEM with AMOS, etc.) says to correlate variables only if doing so is supported by theory and empirical results (like modification indices). Have I misinterpreted the literature?",2013-10-12 14:51:48.507
103923,57356,6630.0,CC BY-SA 3.0,,"Once you fix k = 1, it's no longer random!!",2013-10-12 15:03:59.870
103924,57318,5448.0,CC BY-SA 3.0,,"@Memming - VIF = Variance Inflation Factor, MC is multicollinearity.  - Hugh - please don't use acronyms unless they are really, really standard and widely known, like ""BIC"" or ""GLM"".",2013-10-12 15:33:20.460
103925,57356,8671.0,CC BY-SA 3.0,,e.g. http://en.wikipedia.org/wiki/Cluster_labeling#Mutual_Information,2013-10-12 15:53:53.693
103926,57355,20927.0,CC BY-SA 3.0,,"thank you!might just opt for a one-way ANOVA because SPSS does not seem to do the Bonferroni with multiple t-tests, so would need to do that manually, best to do that i should think.Thanks!",2013-10-12 16:13:04.097
103927,56911,2149.0,CC BY-SA 3.0,,Glen  .... this is precisely what I pointed out in my response.,2013-10-12 16:20:07.000
103928,57356,6630.0,CC BY-SA 3.0,,"@user570593 I see. In that wikipedia page, the random variables are indicator functions of k=1 and c=1. That's the confusion.",2013-10-12 16:29:30.053
103929,57317,6162.0,CC BY-SA 3.0,,"... and your JAGS model also assumes a different between-variance for each group (because you run the model separately for each group, as I understand)",2013-10-12 16:34:56.677
103930,57356,6630.0,CC BY-SA 3.0,,I updated the answer. Take a look.,2013-10-12 16:35:40.480
103931,57271,11490.0,CC BY-SA 3.0,,"Yes, but $\hat{\gamma}_{21}A_{t-1}$, $\hat{\gamma}_{22}B_{t-1}$  $\hat{\gamma}_{23}C_{t-1}$ don't sum to $B_t$. I need 3 numbers that sum up to $B_t$, maybe that could be achieved with a model with time-varying coefficients?",2013-10-12 16:35:40.503
103932,57317,22564.0,CC BY-SA 3.0,,"@Stéphane what was the trick? Also the variance does appear to be different for each subject. Also I am interested in your last comment. I do run it separately for each group, you think I should compare to running it on all groups at once?",2013-10-12 16:49:00.217
103933,57356,8671.0,CC BY-SA 3.0,,Thank you very much for your answeres. Still I have some problems. I want to calculate the dependancy between a class and a cluster. How can I find it using MI?,2013-10-12 16:55:54.477
103934,57356,6630.0,CC BY-SA 3.0,,"@user570593 You want to know if ""being in this cluster or not"" tells you anything about ""having this class label or not"". That's why you use the indicator function. And that's all good. If you had more than 2 clusters, the answer would be different. I think your problem is conceptual.",2013-10-12 16:59:57.247
103935,57321,22564.0,CC BY-SA 3.0,,I accepted earlier on accident. Let me think on this for a bit.,2013-10-12 17:01:28.767
103936,57271,20473.0,CC BY-SA 3.0,,"Indeed, and this is why the error terms exist. In general $\hat A_t$ never equals $A_t$, this is a core fact of any estimation procedure. If for some reasons you _need_ to obtain this equality, then one interesting avenue is to devise a way to _allocate_ the error of each period over the estimated coefficients (that's one way to obtain ""time-varying"" coefficients).",2013-10-12 17:08:30.663
103937,57358,10135.0,CC BY-SA 3.0,,"Hint: Always in these type of problems, draw a diagram, here a unit square ( i.e. $0<x<1$ and $0<y<1$). Then fixed $Z=z$ so $X+Y=z$. Now try to draw this line i.e. $Y=-X+z$ on that square. Note that here $z$ is the intercept of your line and depending on what values it takes, it will move your line (up or down) and changing the area below your line.",2013-10-12 17:14:33.250
103938,57321,22564.0,CC BY-SA 3.0,,Can you show and prove what you say in the update with some R code and a simpler toy example?,2013-10-12 17:18:48.130
103939,57356,8671.0,CC BY-SA 3.0,,"If MI(I(c=1),I(k=1))=MI(I(c=2),I(k=1)) what I understood is the dependancy between c=1 and k=1, and the dependancy between c=2 and k=1 is same (if I calculate it). I expect to have different values for MI(c=1, k=1) and MI(c=2, k=1) so that I can select some clusters which determined a class c. for e.g. a weighting sceme based on MI is explained in  http://lastlaugh.inf.cs.cmu.edu/alex/BagOfVisWords.MIR07.pdf",2013-10-12 17:22:52.213
103940,57358,10135.0,CC BY-SA 3.0,,"I forgot to add that sometimes it is easier to calculate the area above your line and then subtract it from 1. It means that instead of finding $P(Z\leq z)$, you will find $P(Z>z)$ and then $P(Z\leq z)=1-P(Z>z)$.",2013-10-12 17:24:39.227
103941,57356,6630.0,CC BY-SA 3.0,,"@user570593 If you know it is not cluster 2, you know immediately it is cluster 1. That's why it doesn't matter. It sounds like you have more than 2 clusters in reality in which case MI would work fine. Do you see what I mean?",2013-10-12 17:27:24.413
103942,57356,8671.0,CC BY-SA 3.0,,"Lets say I have 50 clusters. The problem I have is when I calculate the MI(c=1, k=1) and MI(c=2, k=1) I am getting the same value. (e.g. from the table I have 3 clusters when I calculate MI(c=1, k=1) and MI(c=2, k=1) getting same values). If i get different values for MI(c=1, k=1) and MI(c=2, k=2) s.t. MI(c=1,k=1)>MI(c=2,k=1) I can conclude that the cluster k=1  has high dependency with the class c=1. But for all k=1,...K I am getting MI(c=1, k) = MI(c=2, k)",2013-10-12 17:30:55.207
103943,57358,22637.0,CC BY-SA 3.0,,"I have drawn a figure and while I understand what I need to do, I still cannot explain the lower limit of integration of x, that is z-1.",2013-10-12 17:39:08.240
103944,57361,22381.0,CC BY-SA 3.0,,I'm not sure how this answers my question?Can you explain?Is it possible for a volatile series to be defined as stationary?,2013-10-12 17:46:40.870
103945,57317,6162.0,CC BY-SA 3.0,,"The ""trick"" is to reduce the mixed model to a simple model by taking for observations the subjects means in your case and the groups means in the other question. I don't know what you should do but I claimed that the sampling distribution of your Bayesian model is not the same as the one of the frequentist model.",2013-10-12 17:51:30.947
103946,57321,6162.0,CC BY-SA 3.0,,"This is a ""toy example"": http://stats.stackexchange.com/a/72610/8402",2013-10-12 17:53:14.627
103947,57356,6630.0,CC BY-SA 3.0,,"Don't you mean MI(I(c=1), I(k=1)) vs MI(I(c=1), I(k=2))? Why are you changing c? There are only 2 values for c. Your notation of c and k are not consistent in the previous comment, I think.",2013-10-12 17:58:41.443
103948,57317,22564.0,CC BY-SA 3.0,,"I see what you mean that the model is different. However, the bayesian model displays the behavior I desire in that uncertainty is propagated from individual to group level. As I said if this does not occur then I consider something to be wrong regardless of what can be proven by math. There is some logical issue at play. Either the model that does not propagate the error is misspecified or I am asking the wrong question. The jags model has other issues but the best way to do that would be a different question I suppose.",2013-10-12 18:02:12.557
103949,57321,22564.0,CC BY-SA 3.0,,I have the problem that I can understand in code and images better than generalized equations.,2013-10-12 18:17:48.917
103950,57362,22381.0,CC BY-SA 3.0,,How can a series be stationary if it exhibits volatility?How do you define stationarity when applying a GARCH model?,2013-10-12 18:24:10.870
103951,57361,22381.0,CC BY-SA 3.0,,If a time series exhibit volatility clustering doesn't that mean that the series in non-stationary and GARCH cannot be applied to it(if it's non-stationary)?,2013-10-12 18:32:10.400
103952,57317,6162.0,CC BY-SA 3.0,,"Frankly I don't have the courage to try to understand your R code. It is long and hard to read. With the notations of my answer,  the length of the frequentist interval is proportional to $\hat\sigma^2$, hence it increases when $\sigma^2_w$ increases. If you don't see this behavior with your simulations, you have simulated something else.",2013-10-12 18:37:17.147
103953,57362,22381.0,CC BY-SA 3.0,,Would it be okay if I include AR and MA terms in my mean equation?If the return series exhibit some autocorrelation at short lags.,2013-10-12 18:43:26.197
103954,57307,9522.0,CC BY-SA 3.0,,I need to obtain the exact pvalue to be able to compare different functional groups of genes.,2013-10-12 18:48:11.620
103955,57366,22381.0,CC BY-SA 3.0,,"I was thinking of fitting the ARMA first, then fitting the residuals to a GARCH model. Is this wrong?How can I ""check the residuals for any linear time series properties which can then be modelled using ARMA processes.""?Can the ljung-box test be used to detect ARCH effect?",2013-10-12 18:48:59.930
103956,57310,9522.0,CC BY-SA 3.0,,Thanks a lot Jamie and Patrick. The Microsoft tool works perfectly and now I can calculate all the pvalues :),2013-10-12 18:50:53.570
103957,57366,306.0,CC BY-SA 3.0,,"simplest way is to look for the auto correlation function of the squared series. if it is significant then try out the GARCH model. if the autocorrelation of the square of the residuals gets removed, then the GARCH does help to model the dependence in the squared series.",2013-10-12 18:52:35.680
103958,57359,19681.0,CC BY-SA 3.0,,"These issues sound like stylistic concerns.  That's not to say that the questions are unimportant, but that the answers may depend more on your precise goals for the analysis.  I don't see how any of the approaches that you mention would be ""generally bad"".  It might be easier to get the answer you're looking for with a little more background on the scientific problem, and specifically what kind of interpretative statement you want to be able to draw from the model.",2013-10-12 18:54:51.893
103959,57367,674.0,CC BY-SA 3.0,,"Two-way comparisons with paired samples are not restricted to matching. Siblings, twins, pre/post measurement, or responses to a question asked to both wife and husband might all be tested using, e.g., a t-test for paired samples. Could you clarify what your situation actually look like?",2013-10-12 18:57:54.153
103960,57366,22381.0,CC BY-SA 3.0,,"If I do that my mean return will be 0 right?I want to be able to get a mean that will not be a straight line, like a mean function that will depend on AR and MA terms + the GARCH error.",2013-10-12 18:59:38.167
103961,57366,306.0,CC BY-SA 3.0,,"there are three things : one is the decision of whether there are GARCH effects present, the other is a justification of using ARMA and GARCH and the third is to actually fit the model when the above two are affirmative. the fitting is not so simple as do it in two different stages. you have to fit both the ARMA and the GARCH parts simultaneously. There are methods available for this.",2013-10-12 19:06:18.897
103962,57317,22564.0,CC BY-SA 3.0,,It's mostly just code for plots :/. I think I must be using incorrect terminology and it is leading to confusion. Hopefully someone will comment on your answer to the linked question and it will clarify for me.,2013-10-12 19:12:59.990
103963,57361,20473.0,CC BY-SA 3.0,,"I take it that by ""volatility clustering"" you mean that it appears that the time series is characterized by different variance in different intervals. First, this is just an indication of possible non-stationarity, not proof. Second, the ARCH model and its extensions attempt to explain this ""volatility clustering"" by modelling the _conditional_ variance as time-changing, while maintaining the assumption of a constant _unconditional_ variance (and hence, the assumption of 2nd-order stationarity).",2013-10-12 19:13:17.750
103964,57363,306.0,CC BY-SA 3.0,,search for recommender systems. this looks like a problem that can be adopted to that. start here http://en.wikipedia.org/wiki/Recommender_system,2013-10-12 19:21:42.567
103965,57356,8671.0,CC BY-SA 3.0,,"No..  I need  MI(I(c=1), I(k=1)) vs MI(I(c=2), I(k=1))",2013-10-12 19:24:35.467
103966,57362,1406.0,CC BY-SA 3.0,,"Stationary means constant mean, variance and correlation depending only on lag. AR and MA terms can be included in the mean equation. The key in GARCH processes is conditional volatility. Note that volatility is not variance. The mean volatility is series variance.",2013-10-12 19:28:50.913
103967,57366,22381.0,CC BY-SA 3.0,,Would the use of ARMA be justified if there are correlations in the return series?I think there are packages in R that does the fitting. I only need to know when to apply an ARMA-GARCH or simply a GARCH. Can I use ljung-box test to test for GARCH effects?,2013-10-12 19:29:34.540
103968,57361,22381.0,CC BY-SA 3.0,,Well lets assume that there is indeed volatility clustering. The series itself would be non-stationary so how can I apply a GARCH model to a non-stationary series as mpiktas did say that GARCH should be applied to stationary series.,2013-10-12 19:33:44.197
103969,57361,20473.0,CC BY-SA 3.0,,"No, volatility clustering does _not_ necessarily imply non-stationarity. So if it can be ""explained"" by GARCH modelling, then you can operate on the assumption of unconditional stationarity. Indeed, this appears a bit circular - but then again, we can almost never be sure that an actual observed stochastic process is, or is not, stationary.",2013-10-12 19:38:54.117
103970,57362,22381.0,CC BY-SA 3.0,,"As reference take for example the SP500 data in R, the return data seems to be constant in its mean but exhibit blatant conditional heteroskedasticity. So it is possible to apply a GARCH model on it despite having non constant variance?",2013-10-12 19:40:09.887
103971,57361,22381.0,CC BY-SA 3.0,,If the variance is varying through time doesn't that mean that the series is not stationary?,2013-10-12 19:42:22.333
103972,57361,20473.0,CC BY-SA 3.0,,"The variance is an _unknown_ moment of the theoretical _unconditional_ distribution that your process follows. It is either time-varying (so the process is non-stationary), or it is unconditionally constant but conditionally time-varying (GARCH-like). These two different scenarios can have the same result as regards the actual evolution of the process through time (i.e. on what you actually observe). So implement GARCH, and if it performs well, you can maintain the hypothesis of stationarity. If it doesn't, then you can entertain the possibility of unconditional non-stationarity.",2013-10-12 20:00:39.623
103973,57356,6630.0,CC BY-SA 3.0,,"@user570593 so you have only 2 clusters, if 'c' means clusters.",2013-10-12 20:10:03.237
103974,57361,22381.0,CC BY-SA 3.0,,So there is no way to know if the series is stationary?We just apply the GARCH model like that?,2013-10-12 20:12:23.997
103975,57361,20473.0,CC BY-SA 3.0,,"Yes, but we scrutinize its performance. Essentially, the concept of GARCH modeling itself has made the unconditional 2nd-order stationarity issue a bit moot (assuming we have mean-stationarity), when it comes to applied modelling.",2013-10-12 20:15:14.547
103976,57362,22381.0,CC BY-SA 3.0,,"usually can I apply the GARCH model to any log return series that exhibits volatility clustering?I am asking this because I saw in a dissertation that the ADF test was applied to test for stationarity, so I thought that stationarity was necessary before applying the GARCH model.",2013-10-12 20:17:15.447
103977,57361,22381.0,CC BY-SA 3.0,,let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/11032/discussion-between-andy-and-alecos-papadopoulos),2013-10-12 20:19:12.610
103978,57329,594.0,CC BY-SA 3.0,,"Thanks Alecos; it's an important reference, and one I had forgotten about since it came out. I'm going to take a close look now. (My recollection was I didn't follow it in 1989, but I've learned quite a few things since then.) -- that may well give a way of doing what is needed here.",2013-10-12 20:48:01.487
103979,57372,594.0,CC BY-SA 3.0,,"This may be over-relying on a particular choice of phrasing; you're assuming rather a lot from what might be simply a poor choice of words - not everyone here has English as a first language. It's definitely worth raising as a potential problem, but to simply state things so baldly (""absolutely not"") implies you know more than we can tell from what's here. (Further, the reference to a 'lab notebook' implies the OP is doing work in a lab. I doubt this is the case. Again, you imply you know more than we have here.)",2013-10-12 20:56:39.273
103981,57368,4656.0,CC BY-SA 3.0,,"This looks a lot like a homework problem, and if it is so, please add the `homework` or `self-study` tag as appropriate. With regard to parts (i) and (ii), first draw a diagram of the $x$-$y$ plane, indicate on it the region where $f_{X,Y}(x,y)$ is nonzero as well as the regions where $X/Y \leq t$ etc.  You will have different diagrams depending on the numerical value of $t$.  Integrate the joint pdf over the regions to get the answers you need.",2013-10-12 21:50:39.493
103982,57371,594.0,CC BY-SA 3.0,,"Correlation is useless for this purpose, even with single-predictors. Consider X on 1,2,3,4,5 and fits f and g being (12, 15, 17, 19, 23) and (101.12, 101.15, 101.17, 101.19 and 101.23) respectively. They're *perfectly* correlated, but they're nowhere near each other.",2013-10-12 22:01:15.430
103983,57356,8671.0,CC BY-SA 3.0,,No C means class and K means clusters. I want to find out the dependancy of a particular cluster with a class.,2013-10-12 22:10:03.037
103984,57372,436.0,CC BY-SA 3.0,,As a non-native English speaker I definitely did not read the OP's sentence in this sense...,2013-10-12 22:13:07.747
103985,57334,436.0,CC BY-SA 3.0,,"But, again, you can use very similar arguments to not reject null. There is nothing special about 0.05, if you had chosen 0.06 as your limit you would probably not be asking the question, but the situation would not be that much different... Rather in these situations I would ask: ""what is the real-life meaning of this result?"". For instance if this was a biological experiment I would look for the biological significance of the specific result, report the p-value as it is and rather comment on the biology.",2013-10-12 22:15:30.820
103986,45804,594.0,CC BY-SA 3.0,,"Would something like summing the squares of the differences between F and G (or the absolute value of their differences) over all the values for which you have outputs for both suffice as a measure? With 6+ dimensions, I'd suggest not using a lattice-type grid of values, by the way, but random draws from the space of values (though you might use quasi-random sequences instead I suppose).",2013-10-12 22:16:43.557
103987,57334,594.0,CC BY-SA 3.0,,@nico this was already the point of my item (2); it argues against over-reliance on the formal approach in (1),2013-10-12 22:18:21.067
103988,57368,4656.0,CC BY-SA 3.0,,"@Glen_b I have noticed some people (here as well as on math.SE) positively bristling at the  suggestion that the `homework` tag be added. ""It is _not_ homework,"" they insist, ""I am studying this stuff on my own."" These people are much happier tagging the problem as `self-study`. My responses and/or answer treat both tags the same....",2013-10-12 22:28:29.447
103989,57358,668.0,CC BY-SA 3.0,,"A few of the ways one might go about computing the distribution of a sum of uniform variates are described in my answer at http://stats.stackexchange.com/a/43075/919.  That question might even be a duplicate of this one, depending on what you mean by ""that specific form.""  What do you mean?",2013-10-12 22:32:58.067
103990,57356,6630.0,CC BY-SA 3.0,,let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/11033/discussion-between-memming-and-user570593),2013-10-12 22:52:49.917
103991,57195,7229.0,CC BY-SA 3.0,,What if you try to increase the plotting threshold (0.5) and to use more than 4 color steps? Or to use thinner-thicker lines instead of colors.,2013-10-12 23:07:05.000
103992,57359,594.0,CC BY-SA 3.0,,I'd suggest using orthogonal polynomials.,2013-10-12 23:11:59.743
103994,57367,22641.0,CC BY-SA 3.0,,"Example -- a sample is partitioned into two groups using a clustering algorithm. Those groups are then dependent. However, different subjects reside in each group and are not matched in any way. So, a paired t-test can't be used since no pairing is possible. However, the dependence between the two groups precludes using methods that require sample independence.",2013-10-12 23:22:13.693
103995,57359,346.0,CC BY-SA 3.0,,@Glen_b Can you give some more details?,2013-10-12 23:32:00.083
103996,57357,668.0,CC BY-SA 3.0,,+1 This is an interesting and (seemingly) powerful approach.  Thank you for sharing this idea.,2013-10-12 23:35:00.303
103997,57368,668.0,CC BY-SA 3.0,,"Notice that the event $X/Y\le t$ can be written both as $X\le tY$ and $Y\ge (\frac{1}{t})X.$ The chance of the latter is $1$ minus the chance of $Y\lt (\frac{1}{t})X.$ Because the distribution of $(X,Y)$ is identical to that of $(Y,X)$, this latter chance is the same as that of $X\lt(\frac{1}{t})Y$ which (because the distributions are continuous) is that of $X\le(\frac{1}{t})Y.$ When $t\gt 1,$ $\frac{1}{t}\lt 1,$ which reduces the calculation to the one you have already successfully performed.",2013-10-12 23:40:49.440
103998,57367,16588.0,CC BY-SA 3.0,,This probably shouldn't be tagged with 'paired-data',2013-10-12 23:50:25.220
103999,57371,22507.0,CC BY-SA 3.0,,"acbart writes: _I want to find some statistical measure of how ""similar"" the two functions are_.  The two functions are ""similar"".",2013-10-13 00:12:27.173
104000,57367,594.0,CC BY-SA 3.0,,"@ndoogan I added it, but it's possible I misunderstood the situation. I will remove it. The OP should clarify the nature of the dependence.",2013-10-13 00:13:14.443
104001,57372,16990.0,CC BY-SA 3.0,,"Mike McCoy, thank you for your answer, but I'm afraid in this case Glen_b is correct. I am not a native English speaker, and while I strive to write and speak as fluently as my skills allow, usage and connotation continue to elude me. So, in this particular case, we didn't try different things until we found something that was significant. Actually, what we were trying to prove is that there were no statistically significant increase in some error value, and in one particular case we found that the error was actually reduced, and when we ran the W test, this is where we got the 0.0499.",2013-10-13 00:25:38.173
104002,57334,16990.0,CC BY-SA 3.0,,"Thank you Glen and nico. This part of the data was secondary to our experiments, so we just ended up reporting the value as is. In any case, I am marking this as the accepted answer. Thanks again to everyone who participated with answers or comments.",2013-10-13 00:32:04.810
104005,57353,22638.0,CC BY-SA 3.0,,@Elvis If there are 0's in an array .. how are we supposed to handle that situation?,2013-10-13 03:34:06.977
104006,57373,594.0,CC BY-SA 3.0,,Is this for some subject?,2013-10-13 03:58:47.257
104007,57195,22547.0,CC BY-SA 3.0,,"@nadya - I've been thinking about increasing the plotting threshold, and i think it's a good idea. More than 6 colors and the eye will have difficulty recognizing the different levels, though. I could potentially plot just the $n$ highest correlations at each site. But, I wish there were a way to avoid having to calculate and plot $\textrm{order}((n^2)/2)$ correlations for each month's worth of data. There might be something I can use from network / graph theory to reduce the number of pairs.",2013-10-13 04:16:01.267
104008,57373,594.0,CC BY-SA 3.0,,"As it stands the question seems to be underspecified, but maybe I missed something.",2013-10-13 04:18:38.660
104009,57366,306.0,CC BY-SA 3.0,,do the ljung-box test on the square of the series. ARMA is supposed to model the autocorrelations in the series but not for the autocorrelations in the square of the series for which GARCH exists.,2013-10-13 04:36:24.410
104010,57357,1506.0,CC BY-SA 3.0,,"Thanks, great answer.  I have  been looking into signal processing methods with the robFilter package but I was not aware of this technique.",2013-10-13 05:05:49.510
104013,57374,594.0,CC BY-SA 3.0,,"Under random sampling, yes; this is simply a confidence interval for a binomial proportion, with the usual caveats about the assumptions of and interpretation of confidence intervals.",2013-10-13 06:24:21.550
104014,57374,594.0,CC BY-SA 3.0,,"As long as $np(1-p)$ is not small (bigger than 10 is usually plenty), you can use the normal interval described [here](http://stats.stackexchange.com/questions/30281/sample-size-for-binomial-confidence-interval/30306#30306). If the $n$ is small and $p$ is very near 0 or 1, you may need to consider one of the [other binomial approximate confidence intervals](http://stats.stackexchange.com/questions/28316/confidence-interval-for-a-proportion-when-sample-proportion-is-almost-1-or-0). Many other posts here cover aspects of CIs for binomial proportions; the search bar turns many up.",2013-10-13 06:36:35.060
104015,57374,594.0,CC BY-SA 3.0,,"Further examples of previous posts on this topic: [e.g. 1](http://stats.stackexchange.com/questions/4756/confidence-interval-for-bernoulli-sampling), [e.g. 2](http://stats.stackexchange.com/questions/4756/confidence-interval-for-bernoulli-sampling)",2013-10-13 06:39:19.323
104016,57374,594.0,CC BY-SA 3.0,,"Just as a side note - if you want to control your confidence level, then the size of the interval also depends on that.",2013-10-13 06:45:24.270
104017,57372,21586.0,CC BY-SA 3.0,,"Mike, I also did not see a problem in the phrasing of the question. And it seems nobody else saw signs of data snooping, mining, dredging, whatsoever here ... And it definitely lies in the eye of the beholder. There is no mathematical fact but a decision rule chosen by the statistician. Re-read what AlefSin, Glen in his point (2) and I wrote.",2013-10-13 06:56:43.053
104018,57338,,CC BY-SA 3.0,,"I suggest you to read Lecun, Efficient Backprop, 1986 (http://scholar.google.it/scholar?cluster=15983004533596008350&hl=en&as_sdt=0,5) where the author proposes and discuss some tricks and tuning about NNs. I always normalize input features between -1 and 1.",2013-10-13 08:25:31.243
104019,57185,,CC BY-SA 3.0,user10619,"@Nick The psychometry presumes that generally there is an error in Scale for measurement of say, fear. And hence, pursues the analysis. For example, error (measurement) variance is deducted from observed variance for arriving at true variance. The statistician presumes inertia of large numbers for mean and variance. There is no need for dealing with measurement error.",2013-10-13 09:44:00.070
104020,57185,15827.0,CC BY-SA 3.0,,"I am at a loss to know what kind of answer you seek. But the implication that statisticians ignore measurement error is contradicted by a substantial literature by statisticians. Go to www.amazon.com and search for ""measurement error models"" to bring up several major works.",2013-10-13 10:13:25.520
104021,57380,15827.0,CC BY-SA 3.0,,"You can ensure positive predictions by using a generalized linear model with logarithmic link function. By the way, although your $R^2$ value is quite encouraging, a better check of whether the model follows the main shape of the data is a plot of residual vs predicted. Plots of observed vs predicted may also help illuminate your problem.",2013-10-13 10:24:29.107
104022,57372,,CC BY-SA 3.0,,"@IslamEl-Nabarawy If you wanted to establish equivalence/lack of difference, you have many other problems than how to interpret a value close to the threshold or potential data snooping. Just finding a *p*-value slightly over .05 (or whatever error level you choose) is definitely not enough. Look up “testing for equivalence” here and elsewhere or ask a question specifically about that because it's an entirely different problem.",2013-10-13 10:32:39.763
104024,57379,15827.0,CC BY-SA 3.0,,Spelling out which R function you used is always good practice.,2013-10-13 11:41:54.743
104025,56784,19681.0,CC BY-SA 3.0,,"It could be interesting to break the problem down into a simplest case.  Imagine something like just 5 observations from your favorite distribution, and put a single divider in the data to form just two bins.",2013-10-13 11:53:08.290
104026,57384,5001.0,CC BY-SA 3.0,,"You wrote ""if you think that"" and ""if the confidence level that you want"".  I believe the interpretation is easier to understand if it is stated objectively; independent of what I think. I believe the interpretation will be easier to understand if it is independent of my wants.  Please see that my example is free of my wants and my subjective beliefs.",2013-10-13 12:51:21.257
104027,57384,306.0,CC BY-SA 3.0,,"a 90% confidence level means that you are fine if the estimate is right 9 out of 10 times. 99.73% confidence level means that you are fine if the estimate is right 9973 out of 10000 times. Generally used values are 90%, 95% and 99%. But it is a subjective decision that has to be decided by the researcher.",2013-10-13 13:08:16.190
104028,57382,503.0,CC BY-SA 3.0,,"Where are you getting the idea about ""gross sampling error""? What is ""MSE""?",2013-10-13 13:25:05.313
104029,57384,5001.0,CC BY-SA 3.0,,"That's sweet but you probably don't know me well enough to say ""you are fine"".  Please rephrase your answer if you care to.",2013-10-13 13:25:59.023
104030,57380,503.0,CC BY-SA 3.0,,"@NickCox gave one suggestion. I would plot the data in more ways than just residual vs. predicted. However, you can certainly rescale money variables. One common method is to take log(cost) as the dependent variable. (I think this winds up equivalent to the log link function, but might be easier to comprehend). Log(cost) can, of course, be negative. And logs of money variables are often sensible because, e.g. a difference between 0.01 and 0.02 per click is important, but difference between 1.01 and 10.2 per click is not.",2013-10-13 13:38:39.820
104031,57389,5001.0,CC BY-SA 3.0,,"Ok Thanks.  I can simply remove the mention of the 0.514 but I will not be offended if anybody offers to do a, superior in several respects, rewrite.",2013-10-13 14:17:36.463
104032,57386,436.0,CC BY-SA 3.0,,"Also, if one just wanted to see the linear regression: `lines(x, fitted(lm(y~x)))` would do the trick.",2013-10-13 14:52:36.763
104033,57373,5448.0,CC BY-SA 3.0,,"Is this homework or self-study?  If so, please add the appropriate tag...  Try deriving the PDF using $p(x_1, x_2, x_3) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)$.",2013-10-13 15:17:12.077
104035,57373,668.0,CC BY-SA 3.0,,The joint distribution for which the pdf is a constant $1/6$ where $0\le X_i$ and $X_1+X_2+X_3\le 1$ does *not* have uniform marginals. @jbowman How are these conditional probabilities to be obtained given that only the *marginal* distributions are specified?,2013-10-13 15:52:42.193
104036,55209,20222.0,CC BY-SA 3.0,,"@Moderator, thank you for migrating this question as a duplicate to  CrossValidated. There are many smart people here, hopefully one or more will offer a reply to this question as I still don't have answers to this.",2013-10-13 15:56:46.467
104037,57380,15827.0,CC BY-SA 3.0,,@Peter Flom I think meant 1.02 not 10.2.,2013-10-13 16:06:59.233
104038,57393,668.0,CC BY-SA 3.0,,"Didn't you mean to type ""two-sided"" instead of ""one-sided""?",2013-10-13 16:25:13.920
104039,57378,668.0,CC BY-SA 3.0,,"Yes, you are being specific, but your specific question is covered by all the answers to the duplicate: you are testing the hypothesis that the intercept equals zero. That is well defined and specific.",2013-10-13 17:10:06.297
104041,57381,22547.0,CC BY-SA 3.0,,How well do these functions deal with missing data? I quite often have gaps in the time series.,2013-10-13 17:11:42.890
104042,57394,668.0,CC BY-SA 3.0,,"What does ""do"" mean in your final formula?",2013-10-13 17:12:03.273
104043,57394,16046.0,CC BY-SA 3.0,,@whuber edited.,2013-10-13 17:16:11.757
104044,57394,668.0,CC BY-SA 3.0,,"Thanks, but questions need to be understandable on their own, so if you can, please describe what this means rather than just providing links.  Otherwise you may severely limit the potential audience for this question and reduce your chances of getting great answers.",2013-10-13 17:19:29.760
104045,57393,14850.0,CC BY-SA 3.0,,"Yes I did, thanks, well spotted. @whuber, you really pop up everywhere with good responses",2013-10-13 17:22:07.990
104046,57397,668.0,CC BY-SA 3.0,,"I have upvoted this because it's a legitimate solution, but it is risky in practice. After all, the solution is arbitrarily sensitive to values of $\mathbf Z$: a single high-leverage value will steer the estimates far from a decent fit merely to enforce the constraint. Thus, at a minimum, this procedure *must* be accompanied by a careful goodness-of-fit test to the data.",2013-10-13 17:24:05.110
104047,57396,668.0,CC BY-SA 3.0,,There is a substantial difference between the answer to this question for a *single* value of $c$ and an answer that is valid for more than one value.  Which application do you have in mind?,2013-10-13 17:27:25.360
104048,57396,22656.0,CC BY-SA 3.0,,a single value of c. I edited the question.,2013-10-13 17:28:01.300
104049,57397,20473.0,CC BY-SA 3.0,,"@whuber You are right. So, OP, tread carefully here.",2013-10-13 17:30:37.653
104050,57396,668.0,CC BY-SA 3.0,,"OK, that's easy.  For the record, the solution for an arbitrary number of unspecified $c$ is given at http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Setting_confidence_limits_for_the_shape_of_a_distribution_function.",2013-10-13 17:30:58.187
104051,57394,16046.0,CC BY-SA 3.0,,"Well I am gonna be honest about this. My understanding of causal framework is very primitive and naive and as stated in the paper I linked to is: ""expressions of the form $P(Y = y|\text{set}(X = x))$ or $P(Y = y|\text{do}(X = x))$ to denote the probability (or frequency) that event $(Y = y)$ would occur if treatment condition $X = x$ were enforced uniformly over the population.""

You are right, but I thought new causal framework is a well known one among statisticians. What I am implying is the ""statistician"" above, should be a dumb to treat $X_2=x_2$  as an observational variable.",2013-10-13 17:35:24.610
104052,57381,8074.0,CC BY-SA 3.0,,"There are EOF methods that are designed for the special case of ""gappy data"" that you describe. Here is a link to a paper that reviews these methods: 
http://dx.doi.org/10.6084/m9.figshare.732650 . You'll see that the RSEOF and DINEOF methods are the most accurate for deriving EOFs from gappy data sets. The DINEOF interpolation algorithm can be found here: http://menugget.blogspot.de/2012/10/dineof-data-interpolating-empirical.html",2013-10-13 17:41:05.680
104053,57396,13037.0,CC BY-SA 3.0,,Are you sure you want $P(X\leq c)$ and not a statistic of your sample such as $P(\bar{X}\leq c)$?,2013-10-13 17:48:52.873
104054,57396,22656.0,CC BY-SA 3.0,,Yes I want to find $P(X≤c)$,2013-10-13 17:54:32.420
104055,57389,306.0,CC BY-SA 3.0,,"You understand this answer and not the one I gave, Wow! Yep I think I got your question completely wrong then. My apologies.",2013-10-13 17:55:58.970
104056,57286,,CC BY-SA 3.0,user14650,"I see. Nevertheless it's not optional. It has a default value, if you don't set it. So it ""expects"" it.",2013-10-13 18:16:18.910
104057,57396,6162.0,CC BY-SA 3.0,,Count the sample values $x_i$ such that  $x_i \leq c$ and consider a confidence interval about the binomial propotion $\theta=\Pr(X \leq c)$.,2013-10-13 18:16:41.220
104058,57313,12544.0,CC BY-SA 3.0,,"I'm not familiar with that book. If you're doing something analogous to regression, you should correlate predictors or you'll get incorrect estimates.",2013-10-13 18:28:12.573
104059,57393,15827.0,CC BY-SA 3.0,,The model fitted is not logistic. It grows from 0 and approaches an upper asymptote exponentially.,2013-10-13 18:28:54.200
104060,57386,22651.0,CC BY-SA 3.0,,"Thanks Peter, You have put it right, i actually want to have a justification of using regression analysis to predict waste generation, im sorry for that ambiguity.one of the prerequisites for regression analysis is linear relationship among variables.so im i safe to proceed and assume a linear relationship in this data?",2013-10-13 18:37:04.970
104061,57396,22656.0,CC BY-SA 3.0,,"@StéphaneLaurent I am a little confused. My question is how many samples, i.e. $x_i's$, I should choose. What is the binomial propotion? Would you please explain a little more?",2013-10-13 18:43:57.220
104062,57396,22656.0,CC BY-SA 3.0,,"@whuber Thanks for your help. Knowing the value c, how can I find the required number of sample?",2013-10-13 18:45:54.120
104063,57393,14850.0,CC BY-SA 3.0,,Is it not? I labelled it logistic because it is a specific version of the [generalised logistic function](http://en.wikipedia.org/wiki/Generalised_logistic_function),2013-10-13 18:50:27.670
104064,57393,15827.0,CC BY-SA 3.0,,"Really? A glance at the algebra suggests otherwise. In any case, I'd assert that as usually understood in growth modelling  a logistic curve has an inflexion.",2013-10-13 19:07:59.810
104065,57386,503.0,CC BY-SA 3.0,,"""Linear regression"" does demand a linear relationship among variables, but that can be surprisingly nonlinear. e.g. you can include quadratic and cubic terms, for instance. In addition, there are non-linear regression models.",2013-10-13 19:34:47.123
104066,57219,21952.0,CC BY-SA 3.0,,I made some edits to the problem. Also I have data for the previous year and the lagged variables for the Z's could be used as instruments.,2013-10-13 19:47:05.313
104067,57393,14850.0,CC BY-SA 3.0,,"You are right, there is no division, I'm going blind",2013-10-13 20:01:27.850
104068,57401,21762.0,CC BY-SA 3.0,,"Two normal distributions centered around 0, one with larger variance (compare 0.1 and 0.9 quantile)?",2013-10-13 20:02:04.353
104069,57390,20473.0,CC BY-SA 3.0,,The likelihood function of the sample is the joint density. Here the variables are not independent. Do you know how to apply the chain rule to decompose the joint density into conditional densities?,2013-10-13 20:22:07.870
104070,57401,15377.0,CC BY-SA 3.0,,"I am not sure if this can happen. Quantiles for Normal distributions are basically the scaled up version of the corresponding SD. Therefore if for the 1st Normal, 0.1 quantile is higher then 0.9 quantile will also be higher. What I am looking for is opposite. I am looking for this scenario: 0.1 quantile is higher for 1st r.v. than 2nd r.v. but, 0.9 quantile is lower for 1st than 2nd.",2013-10-13 20:26:28.333
104071,57401,668.0,CC BY-SA 3.0,,@Michael Mayer is correct.  It might help to draw a picture of the two CDFs overlaid on one another.,2013-10-13 21:03:50.783
104072,57405,19681.0,CC BY-SA 3.0,,"It may lead somewhere to play with the algebra here.  Notice that the two logs in the definition of $f$ can be combined (sum of logs is the log of a product).  Subsequently, plugging this simplified version of $f$ into 13.1 might lead to an interpretable expression.  The Law of Total Probability comes to mind.",2013-10-13 21:24:11.873
104073,57353,5875.0,CC BY-SA 3.0,,"If two discrete distributions with same support are both 0 at some point of this support, you can just remove this point, as I did above.",2013-10-13 21:45:22.553
104074,57368,21840.0,CC BY-SA 3.0,,"@whuber I understood your explanation, the only part I am struggling with is why are we using the fact that $(X,Y)$ and $(Y,X)$ have same distribution.",2013-10-13 22:03:53.663
104076,57380,594.0,CC BY-SA 3.0,,A little sample data would help people illustrate potential solutions.,2013-10-13 22:21:10.770
104078,57368,668.0,CC BY-SA 3.0,,Because it's simpler than doing the same calculation twice!,2013-10-13 23:20:32.307
104079,57402,22547.0,CC BY-SA 3.0,,"This is also an interesting idea. Because some of the domains can be quite large, I'd probably group the data into $x \times x$ km cells rather than $x^\circ$ latitude-by-longitude.",2013-10-13 23:41:41.510
104080,57195,22547.0,CC BY-SA 3.0,,There are some interesting suggestions arriving. I'm in the process of speeding up the basic data processing and am starting to try some of these ideas. I will provide comments later.,2013-10-13 23:42:41.657
104081,57402,7229.0,CC BY-SA 3.0,,"Yes, to project the coordinates is a good idea. Good luck!",2013-10-13 23:58:41.930
104083,57228,22547.0,CC BY-SA 3.0,,"If you want, post a question to Stackoverflow and I'll answer there. It's only a few lines.",2013-10-14 00:28:09.860
104084,57413,594.0,CC BY-SA 3.0,,"This may simply be my own ignorance, but I am unfamiliar with the definition of ""*time persistent process*"". Googling didn't turn up anything obvious. Could you define your term please?",2013-10-14 01:28:44.937
104085,57414,594.0,CC BY-SA 3.0,,"Your phrasing here: ""* [4,0,0,0...] should be as far away as [32,0,0,0,..] but with lower significance*"" suggests that your notion of distance or significance will have to change for it to make sense. I don't see how the two can be consistent as it stands.",2013-10-14 01:40:42.627
104086,57385,594.0,CC BY-SA 3.0,,Is this for some subject?,2013-10-14 01:44:00.987
104087,57379,594.0,CC BY-SA 3.0,,More information would help. Would you understand the meaning of the column headings if it were a linear regression model rather than a nonlinear model? Would you understand some but not others?,2013-10-14 02:26:02.990
104088,57404,22507.0,CC BY-SA 3.0,,"What do you mean by apply to real life?  Do you mean science, or technical stuff, or everyday life decisions, or business & administration?",2013-10-14 02:26:27.700
104089,57388,,CC BY-SA 3.0,,"Hi, I'll give a more detailed answer later, but simply put they are not the same. You should first be measuring the treatment effect within a study and then pooling across studies. The other way is known as the 'naive method' and is dangerous as it can give invalid results. Have a look at the wikipedia page for Sampson's paradox for some examples (http://en.wikipedia.org/wiki/Simpson%27s_paradox).",2013-10-14 02:47:46.493
104090,57417,594.0,CC BY-SA 3.0,,"You can't *prove* independence from a sample. You might find that your data are consistent with independence, but they'd also be consistent with mild dependence. Showing that they're inconsistent with being iid should be easier.",2013-10-14 03:31:46.377
104091,57416,5237.0,CC BY-SA 3.0,,"This post is being automatically flagged as 'low quality' because of its length. At present, it's sort of a comment. Can you expand it a little bit to make it more of an answer?",2013-10-14 03:31:48.230
104092,57417,5237.0,CC BY-SA 3.0,,In what sense do you want a *proof*? Are you just trying to understand the ideas? Is this a class assignment? What would having such a proof help you achieve?,2013-10-14 03:34:00.057
104093,57417,594.0,CC BY-SA 3.0,,More details/context might help,2013-10-14 03:41:18.997
104094,57389,5001.0,CC BY-SA 3.0,,"My suggested interpretation, informed by the above, becomes: Under the assumption that the true value of the y-intercept is zero, random sampling of the same number of (x,y) pairs, specifically 90, produced by the same process, would result in a least squares best fit line with a y-intercept at least as extreme as +0.00087, with a probability of 0.0027, and equal to or greater than +0.00087, with a probability of 0.00135.",2013-10-14 04:16:35.180
104095,57417,22425.0,CC BY-SA 3.0,,"@gung: I'm working on a machine learning problem. When I assumed the data is independent but not identically distributed, I got better results than assuming IID. Hence I would like to prove the data is independent but not identically distributed.",2013-10-14 05:31:45.797
104096,57418,594.0,CC BY-SA 3.0,,"If they're specified in advance, specific comparisons are usually called planned contrasts.",2013-10-14 05:40:16.690
104097,57390,21985.0,CC BY-SA 3.0,,I thought about that. But I don't get it... Do you have a more concrete hint?,2013-10-14 05:51:31.597
104098,57421,594.0,CC BY-SA 3.0,,Can you be more specific about the 'trend'?,2013-10-14 06:11:29.363
104099,57237,22570.0,CC BY-SA 3.0,,"Well, to me its kind of obvious, because the $\bf{a}$ distributions are pretty much symmetric around zero. So when generate the $a_i$ there's no dependency on $v$. When the current $v$ is at the upper edge of the marginal $\bf{v}$ distribution, you'd assume that there should be a bias towards negative $a_i$. ""draw values"" refers to: take the 1-dim probability distrbution, built the cumulative distribution, throw a random number $r$ between 0 and 1, find the $x$ where the cum. distribution has the value $r$. This $x$ is my ""drawn value""",2013-10-14 06:17:50.200
104100,57237,22570.0,CC BY-SA 3.0,,"For completeness: the data originates from gps-logging. I have a set of logged trips in cars, which log speed with 1Hz. So their's a pair of $v$ and $a$ for every datapoint. These are filled into the histogram.",2013-10-14 06:28:09.850
104101,57420,22425.0,CC BY-SA 3.0,,Thank you for your reply. However I don't understand 'We group together each of the variables separately and then calculate the correlation coefficients for each pair of the groups'. Can you please explain it ?,2013-10-14 06:28:55.460
104102,57420,306.0,CC BY-SA 3.0,,"have edited the answer, please do read it again. sorry for the earlier ambiguity.",2013-10-14 07:13:47.637
104103,57413,1406.0,CC BY-SA 3.0,,@Glen_b time persistence is simply a fancy term describing non- stationary data.,2013-10-14 07:23:02.780
104104,57413,594.0,CC BY-SA 3.0,,"The number of observations doesn't change whether or not something is stationary, though the number of observations might affect your ability to detect it.",2013-10-14 07:38:08.980
104105,57425,594.0,CC BY-SA 3.0,,"Define 'strong sources', please. We could certainly measure the observation (though it could also be described other ways), but we estimate the parameter.",2013-10-14 07:46:55.707
104107,57420,22425.0,CC BY-SA 3.0,,May I know the reason behind saying that they have different probability distributions if we cannot reject the hypothesis of these correlation coefficients being zero?,2013-10-14 07:49:12.830
104108,57420,10547.0,CC BY-SA 3.0,,This would be the definition of random variables which supposed to be independend (this only holds for the normal distribution or spheric distributions) but not iid.,2013-10-14 07:55:11.487
104110,57431,11117.0,CC BY-SA 3.0,,"In practice, I work on a model corresponding the first case you mention but the corresponding community uses the second case-terminology and I am very uncomfortable with that. I guess that following the community is the best thing to do.",2013-10-14 09:53:33.637
104113,57287,20120.0,CC BY-SA 3.0,,"I think @MichaelMayer is asking why you are trying to infer a population parameter when you have data from the entire population available, so you don't need to infer, you may simply measure. And my guess is that you did not mean to imply you have the entire population sampled; your 10,000 is actually a sample.",2013-10-14 10:36:39.233
104114,57428,503.0,CC BY-SA 3.0,,"What is the ""ID3 algorithm""? If it requires you to discretize continuous variables, it may be better to use some other method. Certainly there are classification tree methods (from your tag) that do not require binning.",2013-10-14 11:09:23.413
104115,57427,21985.0,CC BY-SA 3.0,,"Thanks a lot! Did not expect such a detailed explanation, but it's great!",2013-10-14 11:51:15.533
104117,57383,8063.0,CC BY-SA 3.0,,"The only slight problem with this approach is that there are no chiptype specific annotation packages for Agilent drosophila chips. But it can be overcome, as the Agilent output chip itself maps the probes to genomic positions. Or I can do it the hard way and go through biomaRt and annotate the genes (not probes) to cytobands on my own.",2013-10-14 12:44:00.773
104118,57427,20473.0,CC BY-SA 3.0,,You're welcome Michael. I remember you are a biologist taking a stats course. How is it going so far?,2013-10-14 13:15:32.487
104119,57423,12683.0,CC BY-SA 3.0,,"(+1) For the OP's benefit, stationarity of the original series can be checked visually by examining whether the sample auto-correlation function falls off to noise quickly (exponentially).",2013-10-14 13:30:23.733
104120,57435,594.0,CC BY-SA 3.0,,One starting point - the paper mentioned in the help for that function is [here](http://dss.ucsd.edu/~hwhite/pub_files/hwcv-041.pdf). The paper defines the term on its second page (p270). Someone who knows the stuff better than me should probably write an answer though.,2013-10-14 13:43:16.020
104122,57421,12683.0,CC BY-SA 3.0,,"Perhaps you could post a graph of the time series, its  ACF, & its PACF. How long is the time series anyway? I'm sure @vinux is right about over-differencing. Did the error variance increase when you differenced?",2013-10-14 13:57:34.813
104263,57483,22659.0,CC BY-SA 3.0,,"Thank you Dougal, I will keep that in mind for the future.",2013-10-15 03:05:40.080
104124,57237,668.0,CC BY-SA 3.0,,"Your comments indicate you are assuming that $\mathbf{a}$ and $\mathbf{v}$ are independent. That cannot possibly be, because there are physical limitations to speeds: that means many accelerations will not be experienced at the most extreme speeds. However, it's not easy to provide more detailed advice because you haven't articulated what you're trying to accomplish; instead, you have described an approach to solving an unstated problem. Why don't you change this question and ask instead about the problem you need to solve rather than how to implement a solution that looks invalid?",2013-10-14 14:14:52.903
104125,57436,7949.0,CC BY-SA 3.0,,"Yes, thanks. At first I always mistakenly used x.and.y then I noticed the mistake and fixed it in just the first line; the R code continued working for me since I had not cleared my workspace. Fixed now.",2013-10-14 14:18:07.023
104127,57427,21985.0,CC BY-SA 3.0,,Hey Alecos! It's very interessting! But I am laking many mathematical tools. That is why I ask so many questions here. But the good thing is that I understand the stuff after solving the exercises ;-),2013-10-14 14:25:24.927
104128,57421,2149.0,CC BY-SA 3.0,,"The data would be useful as the ACF is merely a descriptive summary statistic whose form can arise from a number of possible ""causes"". Please post your data so an informed analysis can proceed.",2013-10-14 14:29:55.240
104129,57423,2149.0,CC BY-SA 3.0,,"@Scortchi Whereas the acf can suggest non-stationarity, non-stationarity is a symptom and can have multiple causes.",2013-10-14 14:32:14.530
104130,57436,1693.0,CC BY-SA 3.0,,"I’d like others' help in assessing whether your example constitutes a sound litmus test.  But I‘m seeing how your example supports your conclusion.  I’ve tried out your simulation about 30 times, using a variety of sd.error values from 10 to 50.  The sd.error and the Z coefficient are correlated at 0.18 with p = .3.",2013-10-14 14:57:45.360
104131,57436,1693.0,CC BY-SA 3.0,,"Still, doesn't it trouble you to think that nearly ineffectual control for covariates would have the same expected effect on a focal coefficient as very thorough control would?",2013-10-14 15:00:43.153
104132,57438,5237.0,CC BY-SA 3.0,,"This isn't really clear. Can you state your situation in simple English w/ a concrete example? What is your response variable? Is it a count of success & failures, or a utility? How is the experiment set up?",2013-10-14 15:02:18.937
104133,57436,1693.0,CC BY-SA 3.0,,I think a stumbling point is that changing your sd.error exerts an equal effect on both X's and Z's squared correlation with Y.  I've edited my question very slightly to reflect my interest in what happens when X's connection to Y (and not Z's) gets stronger.,2013-10-14 15:22:31.163
104134,57418,19043.0,CC BY-SA 3.0,,Thanks. Now I can do a better job searching this out on my own.,2013-10-14 15:26:22.343
104135,57439,2149.0,CC BY-SA 3.0,,"If the series has one or more shifts in the mean , I believe some if not most of your recommended tests will falsely conclude about the need for differencing as a remedy . Differencing is one from of remedy for non-stationarity but ny no means (play on words) is not the only form. See the following article on the flaws of differencing http://www.autobox.com/makridakis.pdf. Other possible causes for apparent non-stationarity that do not require differencing are 1:) time-varying parameters and 2) time varying error variance",2013-10-14 15:27:09.863
104136,57421,5637.0,CC BY-SA 3.0,,"@thigger, you don't need to difference a series for a deterministic trend. I guess in your case stationary model with trend ($X_t=a + bt + Z_t$, where $Z_t$ is stationary series) would fit your data.",2013-10-14 15:28:51.677
104137,57440,10147.0,CC BY-SA 3.0,,Thanks so much. Can you please give a reference to the graphical methods used to choose among the 4 models?,2013-10-14 15:29:08.860
104138,57442,5637.0,CC BY-SA 3.0,,This may help. http://stats.stackexchange.com/questions/56538/how-to-test-heteroskedasticity-of-a-time-series-in-r,2013-10-14 15:36:17.177
104139,57426,9716.0,CC BY-SA 3.0,,"You can split your data to build a model (say 0.7), then test your model on the remaining data. Just a thought, i'm not a specialist on this area.",2013-10-14 15:41:27.993
104141,57438,5237.0,CC BY-SA 3.0,,"I gather your experiment will be *adaptive*, the nature of the next problem that a student tries will be dependent on the set of problems the student has not yet done, or done correctly, & for those that were missed before: how long it has been since they tried & how easy they thought it was. What you are trying to figure out here is how to program the experiment with respect to how long / how many intervening problems the student should see before you show then the same problem again. Is that correct?",2013-10-14 15:45:47.370
104142,57440,2666.0,CC BY-SA 3.0,,"Stratify by a top predictor and within each stratum compute the inverse transformation (logit, etc.) of the cumulative proportion $Y \geq y$ for all $y$ above $min(Y)$.  Look for parallelism.",2013-10-14 15:55:54.847
104143,57438,13385.0,CC BY-SA 3.0,,"@gung: Yes, that's right.  But the dependency of the next problem on the ""current"" one is weak.  New problems are randomly drawn from a bank.  The next one seen is only dependent because it might have been seen and scheduled already.",2013-10-14 15:56:56.357
104144,57426,20470.0,CC BY-SA 3.0,,"Yes, thank you. It is more the suitability of HMMs for the task that I am unsure about.",2013-10-14 15:59:25.960
104145,57443,15827.0,CC BY-SA 3.0,,Does AUTOBOX do ARCH/GARCH as well as ARIMA?,2013-10-14 15:59:40.610
104146,57421,12683.0,CC BY-SA 3.0,,"@vinux: Good point, & I think that might, if the error variance is small compared to the trend, explain the apparent need for a big MA term after differencing - you'd be introducing negative auto-correlation.",2013-10-14 16:03:13.817
104147,57437,15827.0,CC BY-SA 3.0,,"It's almost axiomatic that statistical people don't have good data on anything like uses of statistical methods, statistical software, etc. But you could search your favourite citation database to get some indications. But relative use doesn't tell you much about desirability, at least not much more than newspaper circulation or book sales figures tell you what's worth reading.",2013-10-14 16:03:21.570
104148,57444,22143.0,CC BY-SA 3.0,,A couple of clarifications: 1) Is indexing by $i$ important to the question? 2) You want $P(Y \leq y | X = x)$ but do you know the conditional distribution $P(Y=y|X = x)$?,2013-10-14 16:09:44.317
104149,57435,1889.0,CC BY-SA 3.0,,Presumabbly it is when you use a linear model when the underlying process is not linear,2013-10-14 16:28:13.880
104150,57429,20473.0,CC BY-SA 3.0,,"It would help to clarify by defining explicitly and in mathematical terms what $Y$ is. ""Payback rate"" is a very general term. Is it measured as a ratio of monetary values? Does it count instances of payments-non payments? Is it a weighted average of the behavior of old and new customers? And if yes, what are the weights used? Etc. How one models the RHS has obviously a great deal to do with what exactly does this RHS attempt to explain (the LHS of the regression specification). Otherwise, it would be blind mechanical search for a good fit.",2013-10-14 16:34:38.473
104151,57444,19681.0,CC BY-SA 3.0,,Why not use logistic regression?  You have a binary outcome: greater than 0.1 or not.,2013-10-14 16:35:32.293
104152,57444,22262.0,CC BY-SA 3.0,,"@Theja I'm not sure I follow. If I know the latter then I automatically know the former with integration. Also I think the notation is confused -- in the former you use $P$ to mean probability, but in the latter you use it to refer to a density function.",2013-10-14 16:35:32.620
104154,57444,19681.0,CC BY-SA 3.0,,OK.  Maybe it would help clarify your problem if you explain why logistic regression is not ideal here.,2013-10-14 16:38:24.027
104155,57444,22262.0,CC BY-SA 3.0,,"@zkurtz I'm fielding alternatives; I will definitely try logistic regression. I just don't want it as the answer because I already know that approach. Just wondering; don't I want to avoid discretizing a continuous response (here, $Y$) and avoid throwing away the information if I can help it?",2013-10-14 16:43:11.390
104156,57447,22262.0,CC BY-SA 3.0,,"Could you please clarify ""it doesn't appear that $Y$ is binary"". In the question I said it was continuously normally distributed so this comment is a bit confusing to me.",2013-10-14 16:44:02.567
104157,57444,19681.0,CC BY-SA 3.0,,"@user2763361, I guess the question is whether that information is relevant.  You seem to have asked a binary question: is Y greater than 0.1 or not?  If you had asked ""what is the distribution of Y"", then reducing to binary would certainly be silly and quantile regression would be obviously advantageous.",2013-10-14 16:53:15.433
104158,57442,2149.0,CC BY-SA 3.0,,@vinux The tests you recommend all require an error process that is free of pulses/level shifts/seasonal pulses/local time trends and has time invariant parameters and no points in time where the error varince changes deterministically.,2013-10-14 17:02:33.903
104159,57444,22143.0,CC BY-SA 3.0,,"@user2763361, sorry about the abuse of $P()$. I kind of agree with what zkurtz is suggesting. If the only aim is to find P(Y \leq 0.1), then create a dependent variable Y' = 1 if $Y \leq .1$ and 0 otherwise. Then, run logistic regression. From that you can get the probability directly since logistic function's output $\in [0,1]$.",2013-10-14 17:02:50.413
104160,57421,2149.0,CC BY-SA 3.0,,This series could have three local trends plus 2 anomalous data points. Only the data can correctly speak ! Please post the actual data .,2013-10-14 17:22:39.190
104161,57407,15377.0,CC BY-SA 3.0,,"Thanks everyone for your quick pointer. However at this point, I realize the question I asked for is not properly specified. I should mention a caveat that, the quantiles should be of the same side of Origin (assuming location parameters for both r.v.s are zero). For example, let say 5th and 1st quantile of 2 random variables. My question is that, given 2 real valued r.v.s, if 5th quantile of a r.v. is smaller than 5th quantile of 2nd r.v. then 1st quantiles also will be of similar order? Is it a necessary property? Or there exist random variables for which such rule may not follow?",2013-10-14 17:29:16.163
104162,57443,15827.0,CC BY-SA 3.0,,"Thanks; I take that as ""Not directly, and we think justifiably"".",2013-10-14 17:29:46.707
104163,57444,12282.0,CC BY-SA 3.0,,"I've read this question several times, and I can't figure out what you're asking. Looking at some of the other replies I think I'm not alone. You bring up a design matrix, so I assume there's some explanatory variables which it gets multiplied by. But then you say Y is distributed $N(0, \sigma^2)$, with no mention of X - if we know the distribution of Y, why care about X at all? Is Y actually the noise? And what does ""...my model has estimated the $\tau$-th quantile to be 0.1, which is point on the x-axis in Y's pdf."" mean?",2013-10-14 17:39:34.350
104164,57444,12282.0,CC BY-SA 3.0,,"...(continued) I think you need to edit your question and add in a section where you define all the variables, explicitly state their relationships, and say exactly what you're trying to find. Give us too much information. I think the answers you get will get better then.",2013-10-14 17:40:54.013
104165,57443,2149.0,CC BY-SA 3.0,,@Nick Correct ! We just use the language here as we really don't know it .,2013-10-14 17:48:39.977
104166,57421,12683.0,CC BY-SA 3.0,,Really looks like @vinux is right & you've created a non-invertible series by differencing a series with deterministic trend. Try de-trending as suggested & then see what you've got left to deal with (@IrishStat is also right that there seem to be a couple of outliers there). Note as well you've a very small sample size for ARIMA - if the purpose is to forecast I'm not sure I wouldn't be using exponential smoothing unless I'd some background knowledge to go on.,2013-10-14 18:02:12.820
104167,57383,,CC BY-SA 3.0,anon,"You're right, of course, Bioconductor project does not distribute the annotations for the Drosophila array. Just in case you decide to try the cytoband approach, I have produced an annotation package using the refseq information from Agilent, and it is shared on my Google Drive at https://docs.google.com/file/d/0B5F_KFI2_sBKZzZXR3ZsV2pvQ1k/edit?usp=sharing. I just updated it today.",2013-10-14 18:25:12.977
104168,57442,5637.0,CC BY-SA 3.0,,"I agree with you @IrishStat. But, usually financial time series are free from mean level pulses or the volatility part is dominated than conditional expectation. Anyway I was trying to give an option for the tests in R.",2013-10-14 18:30:20.563
104169,57421,22669.0,CC BY-SA 3.0,,@Scortchi - if I'm honest the main purpose here is to teach myself more about time series analysis - the data were collected to demonstrate that there's a change (which is pretty definite on any analysis); I just saw the opportunity to play with it and try to examine the properties of the change. I have a version averaged weekly (with consequently 4x the data points) but it's much noisier. I'm itching to have a go with vinux's model suggestion now!,2013-10-14 18:35:54.037
104170,57451,668.0,CC BY-SA 3.0,,"Your integrals make no sense, because $U$ and $u$ have different meanings (one is a random variable, the other is a dummy variable of integration) and $W$ and $w$ have different meanings.  What solution did you get using double integrals and why is it important to obtain one using triple integration?  (I obtain $5/36 + \log(2)/6 \approx 0.254413$.)",2013-10-14 18:37:12.347
104171,57351,,CC BY-SA 3.0,,"I have two questions. 1) Not sure if I understand this properly, can I interpret the expectation as one of the first two equations, if either X or Y has been fixed? 2) Can you give an example for EQ 4 and EQ 5? I have a hard time interpreting them and I think concrete examples would help. Thanks!",2013-10-14 18:50:23.860
104172,57451,21840.0,CC BY-SA 3.0,,"@whuber Using double integral, I obtained 0.2545. I want to see how can we solve using triple integral.",2013-10-14 18:51:01.600
104173,57442,22677.0,CC BY-SA 3.0,,"@vinux just out of curiosity, how do you know that i was referring to financial time series on this question instead of other field of science, does ARCH/GARCH and ARIMA model only exist on financial studies?",2013-10-14 18:56:17.833
104174,57451,668.0,CC BY-SA 3.0,,"Perhaps you should show your demonstration using double integrals, because your triple integrals still make no sense.",2013-10-14 18:57:14.797
104175,57421,22669.0,CC BY-SA 3.0,,"@vinux - I feel an idiot for ignoring the simplest solution, thanks! A simple ""reg y D_date_MY"" already gives a better model than the ARIMA one where the MA term was desperately trying to eject itself. I'll have a look to see if it can be improved by adding a stationary model (though I'm suspicious not!)",2013-10-14 18:59:58.547
104221,57462,22143.0,CC BY-SA 3.0,,"@whuber, though I did put forth the idea of sampling uniformly from the surface, I am not sure if it leaves gaps as you mention. In fact, I think one way to achieve uniform sampling is to fit a grid to your planar surface and pick some of these points uniformly at random with replacement.",2013-10-14 21:33:32.957
104261,57484,22698.0,CC BY-SA 3.0,,"Yes I mean that rho=cor(x,y)=cor(x,z)=cor(y,z) and what are the limits for rho. Dilip, can you extend that to say that rho must be non-negative, ie >= 0?",2013-10-15 02:44:22.660
104176,57453,5237.0,CC BY-SA 3.0,,"Welcome to the site, @Spy_Lord. This question seems to be *only* about how to do this in R. Thus, it may be off-topic for CV (see our [help page](http://stats.stackexchange.com/help)); but could be on-topic on [Stack Overflow](http://stackoverflow.com/). If you have a statistical question about RF, please edit to clarify; if not, we could migrate it for you (*please don't cross-post*). However, it will need a [reproducible example](http://stackoverflow.com/questions/5963269/) to be on-topic there; so you'll need to show what you've tried so far & add a `dput()` of your data.",2013-10-14 19:03:10.937
104177,57453,5237.0,CC BY-SA 3.0,,This question does not appear to be about statistics within the scope defined in the help center.,2013-10-14 19:04:06.067
104178,57396,6162.0,CC BY-SA 3.0,,What is your criterion ? What I had in mind is for example to find $n$ such that the length of the confidence interval is below a prespecified maximal length.,2013-10-14 19:09:37.893
104179,57372,21947.0,CC BY-SA 3.0,,"@IslamEl-Nabarawy If I understand correctly, you found that there was no significance for several statistics except for one specific test. Assuming you are testing against the null hypothesis that ""one or more of these statistics is significant"", but that each $p$-value was determined independently, then your $p$ values are too small. (Unfortunately, I doubt that there are good tools to control for this; standard methods such as the Bonferroni correction are not applicable to this test.) Nevertheless, this *bolsters* your hypothesis that there are no significant relationships.",2013-10-14 19:10:31.990
104180,57351,20473.0,CC BY-SA 3.0,,"@ceiling cat 1) $E[h(X,\bar y)] = \int_{-\infty}^{\infty} h(x,\bar y) f_X(x)dx$ is correct because essentially you do _not_ have _two_ random variables any more. Likewise for fixing $X$ to $\bar x$.",2013-10-14 19:10:33.973
104181,57451,21840.0,CC BY-SA 3.0,,@whuber I have posted the solution using double integral.,2013-10-14 19:14:16.933
104182,57453,22682.0,CC BY-SA 3.0,,"Ah OK, my apologies. I would be alright with a solution outside R, so I suppose it overlaps slightly between CV and Stack Overflow. However user31264 looks like he's given me a workable solution anyway.",2013-10-14 19:15:22.533
104183,57351,20473.0,CC BY-SA 3.0,,"@ceiling cat 2)-EQ5 : Consider $Z = X^2(Y-(Y+2)^3) = h(X,Y)$. $Z$ is a random variable alright (for an appropriate support). Then using the specific meaning for the short hand notation, $E_X(Z)=E_X[(h(X,Y)] = \int_{-\infty}^{\infty} x^2(y-(y+2)^2) f_{X}(x)dx$ where $f_{X}(x)$ is the density of $X$ (whatever that is). Obviously $Y$ is not integrated, and it will stay intact. But the result you will obtain won't be a number (as in my previous comment), but a random variable (a function of $Y$), since $Y$ here is _not_ fixed, just not-integrated out.",2013-10-14 19:18:29.690
104184,57351,20473.0,CC BY-SA 3.0,,"@ceiling cat In both cases in my two previous comments, the ""mechanics"" of mathematical calculations will be the same. The end results though have different interpretations.",2013-10-14 19:22:14.193
104185,57351,20473.0,CC BY-SA 3.0,,"@ceiling cat 2)-EQ4: Consider the same random variable $Z$. Its expected value conditional on $X$ is (using the other meaning for the shorthand notation) $E_X[Z] = E(Z\mid X) = \int_{-\infty}^{\infty} z f_{Z|X}(z\mid x)dz$. Note that here the $x$'s and $y$'s do not appear directly in the integrand -they are ""condensed"" in the $z$ symbol.",2013-10-14 19:29:14.960
104187,57455,22143.0,CC BY-SA 3.0,,"What is the feedback? Is it updating the algorithm only if it makes error or is it updating the algorithm with the new data irrespective of how the algorithm performed? Depending on the situation, the effect may be different.",2013-10-14 20:01:10.150
104188,57448,22656.0,CC BY-SA 3.0,,Thanks a lot! Would you please introduce a reference for this inequality?,2013-10-14 20:04:57.153
104189,57452,22143.0,CC BY-SA 3.0,,Can you clarify: Do we have a 1-D distribution $D$ from which we get $N_s$ examples $\{x_i\}_{i=1}^{N_s}$? By population size $N_p$ do you mean that the values which random variable takes is finite and the size of this set of values is $N_p$?,2013-10-14 20:13:38.957
104190,57396,6162.0,CC BY-SA 3.0,,"About your update: yes, this is what I said, if you count the sample values $x_i$ such that $x_i \leq c$ then this count has a binomial distribution.",2013-10-14 20:14:29.053
104191,57412,,CC BY-SA 3.0,,I asked a similar question at http://stats.stackexchange.com/questions/72570/simulating-from-posterior-predictive-over-many-periods but it looks like yours has received more up votes so far.,2013-10-14 20:15:12.887
104192,57418,594.0,CC BY-SA 3.0,,Also sometimes '*a priori* contrasts' or 'planned comparisons'.,2013-10-14 20:19:04.030
104193,57421,,CC BY-SA 3.0,,"@thigger It's worth bearing in mind that, as a rule, one should have at least N=50 observations in order to build an ARIMA model. If I understand correctly, the series to be modeled contains N=31 obs, which is a good deal less than N=50. Furthermore, one generally only considers N/4 partial- and auto-correlation coefficients, so, in this case, only the first 8 lags in the ACF and PACF are relevant for identifying tentative models. For y; both the ACF and PACF have two significant spikes at lags 1 and 2. For D.y; both the ACF and PACF have one significant spike at lag 1.",2013-10-14 20:25:23.803
104195,57421,,CC BY-SA 3.0,,"@thigger Essentially, I'd like to emphasize the point made by Scortchi that your sample size is very small. Hopefully my other comments provide you with some useful help too.",2013-10-14 20:28:43.740
104196,57452,22627.0,CC BY-SA 3.0,,"@Theja: yes, the population distribution is 1-D, and $N_p$ is the number of values from the distribution. Here's the background: the shape of a manufactured part is deemed acceptable iff the maximal deviation $X_p$ from the desired shape, as measured over $10^9$-ish locations, is small enough. In fact, $X_p$ became the measure of the shape's fitness in the industry. It's infeasible to measure (manufactured - desired) at every of the $10^9$ location, so only a sample of $10^5$-ish points are measured. Thus the question: how to estimate $X_p$ given $X_s$ of the sample and the sample's momenta.",2013-10-14 20:42:10.310
104197,57458,5237.0,CC BY-SA 3.0,,"Your latter strategy is [winsorising](http://en.wikipedia.org/wiki/Winsorising). Statisticians are usually leery of this, & prefer to use [robust analyses](http://en.wikipedia.org/wiki/Robust_statistics). As far as what the instructor had wanted students to do, I can't say for sure; your best bet would be to see if you can find it in the materials that are online, or send them an email. Regarding how to speed up these algorithms in `R`, that's off-topic for CV (see our [help page](http://stats.stackexchange.com/help)), but should be on-topic on [Stack Overflow](http://stackoverflow.com/).",2013-10-14 20:45:19.070
104199,57461,22685.0,CC BY-SA 3.0,,"I think i may know the answer to this question.  But I'm not 100%.  If X and Y have a statistical distance that is e(k)-close then depending on the value of ""k"", X and Y could be ""statistically indistinguishable"", which is a good thing in cryptography because you don't want an adversary to be able to easily guess which distribution you sampled a variable from.",2013-10-14 20:49:55.787
104258,57477,594.0,CC BY-SA 3.0,,*Every* part of the correct formula is covered by my discussion above. Can you more clearly explain what you don't get and I can point to where it's already covered by my answer. I can try expanding my explanation further.,2013-10-15 02:17:20.650
104259,57484,22507.0,CC BY-SA 3.0,,"Do you mean that rho=cor(x,y)=cor(x,z)=cor(y,z), and what are the limits for rho?",2013-10-15 02:23:44.780
104200,57460,1693.0,CC BY-SA 3.0,,"To me, the relative strength of coeff.'s matters a lot, as I tried to say in a recent comment and edits.  And I've just learned about Leamer, E. E., A Result on the Sign of Restricted Least Squares Estimates,"" Journal of Econometrics, 3 (1975),  387-390.  See a brief summary at http://davegiles.blogspot.com/2013/05/when-can-regression-coefficients-change.html. Apparently in OLS there is a minimum predictive power required of 1 variable (relative to that of another) in order for its inclusion to cause a sign change for the other.  I'd like to know the rule for groups of covariates, in logit.",2013-10-14 20:55:01.523
104201,57396,,CC BY-SA 3.0,,"I suggest you take a look at _inverse binomial sampling_. This is a sequential method that adaptively selects sample size to _guarantee_ a certain confidence level for a prescribed _relative_ confidence interval. So, for example, this method can assure that the estimated probability does not deviate from the true probability by more than, say, 10% with 95% confidence. Take a look at an explanation here (see especially the last reference): http://stats.stackexchange.com/questions/71164/monte-carlo-estimation-of-probabilities/71228#71228",2013-10-14 20:55:45.960
104202,57455,14748.0,CC BY-SA 3.0,,"Feedback is used irrespectively, right or wrong.",2013-10-14 20:58:49.580
104203,57461,668.0,CC BY-SA 3.0,,"Have you consulted the [Wikipedia article on ""statistical distance""](http://en.wikipedia.org/wiki/Statistical_distance), which points out there are *many* different distances?  Which definition does your reference use?",2013-10-14 20:59:22.113
104204,57456,19265.0,CC BY-SA 3.0,,"I know, that they are typically nonzero when the 2-class data is non-separable.
I know, that we are trying to minimize their sum.
But I don't know, what is the loss function they are calculated with. Is it a step function or hinge loss or something else?",2013-10-14 20:59:52.853
104205,57457,668.0,CC BY-SA 3.0,,"Although you did not use integral notation, you indeed did compute a triple integral through a process of three successive integrations (in a somewhat mysterious way)--and that's usually what somebody means when they request a ""triple integral.""",2013-10-14 21:01:45.703
104206,57459,12683.0,CC BY-SA 3.0,,How do you know?,2013-10-14 21:06:34.853
104207,57319,668.0,CC BY-SA 3.0,,"You might get more serious attention if you were to explain your undefined terms and acronyms, in particular ""RSQ"" and ""B."" Although many readers will make educated guesses, the more experienced of them will know that there are multiple possible correct guesses. For example, the meaning of ""B"" depends on how both `RA` and `HHS` are encoded, so even your statements about its sign are ambiguous.",2013-10-14 21:08:57.393
104208,57459,22143.0,CC BY-SA 3.0,,"There are two options: 'thresholding' may mean remove those rows/observations where any of the values is not in $[20,16000]$ or  it could mean what you just described. I think the latter is used in statistics since it generates censored data, thus my answer.",2013-10-14 21:12:33.207
104209,57456,22143.0,CC BY-SA 3.0,,"Yes, it is the hinge loss. The hinge loss has been removed from the objective and made into a bunch of constraints (their number equalt to the number of examples, $l$ in your notation). In particular, $\max[0,1-y_iw^Tx_i]$ is the loss on example $i$.",2013-10-14 21:14:39.170
104210,57462,668.0,CC BY-SA 3.0,,"(1) I am curious why MaxEnt might apply here, because it seems to me that the more long-tailed the underlying distribution becomes, the more uncertain any sample-based estimate of its maximum will be.  That suggests this principle might not even be relevant to the question. (2) Of what value is an inequality relating the maximum of a sample to its expectation when the concern is about the maximum of the *population*? This inequality seems to ignore the potentially huge negative bias in using the maximum of a sample to estimate the population max.",2013-10-14 21:16:18.430
104211,57462,668.0,CC BY-SA 3.0,,"(3) Why make measurements uniformly at random, which is known to leave fairly large spatial gaps with high probability, when other procedures--such as gridded samples--will surely leave smaller gaps?",2013-10-14 21:17:22.107
104212,57457,6162.0,CC BY-SA 3.0,,"@whuber Why mysterious ? I only use the formula $E[f(X,Y)]=E[E[f(X,Y) \mid Y]]$ at each step.",2013-10-14 21:18:33.090
104213,57458,12683.0,CC BY-SA 3.0,,Not sure - I've not heard of 'thresholding' before. I'd guess it means 'Winsorizing' just because if he'd meant 'discard data less than 20 or more than 16k' it would have been straightforward to say just that. But people don't always like to be straightforward. On the other hand you cross thresholds rather than piling stuff up on them.,2013-10-14 21:21:06.817
104214,57463,668.0,CC BY-SA 3.0,,"As explained in comments at http://stats.stackexchange.com/questions/57847/formula-to-calculate-a-t-distribution, there are trade-offs between computation time, storage, and programming complexity.  What are your preferences concerning those?  What is the anticipated range of degrees of freedom?  How accurate do the calculations need to be?",2013-10-14 21:21:19.300
104215,57457,668.0,CC BY-SA 3.0,,"The mysteries lie primarily in the details, which are carried out wholly without explanation. A look at the question shows the O.P.'s effort fell apart in not recognizing the importance of tracking the domains of the RVs, as evidenced by the appearances of ""$\min$"" and ""$\max$"" in this answer. Thus, although you have provided a *correct* answer, it does little to reveal what went wrong or to explain the methods you have used to break up the integrals and identify the proper ranges to use in each one.",2013-10-14 21:24:24.487
104216,57458,22507.0,CC BY-SA 3.0,,"What is the meaning of your data?  For example, what is U58516_at?  Is it a gene? a specie? an animal?  What is X1, X2, etc.?  What the positive and negative numbers mean?  From the question one can only understand that there are columns starting with U and ending with _at, there are rows starting with X, there are numbers corresponding to each column/row pair, and all this is somehow connected to genetics. Also, why shouldn't you ask your professor what he or she means by thresholding?",2013-10-14 21:25:00.470
104217,57463,22687.0,CC BY-SA 3.0,,"@whuber I'm afraid I don't see any implementations of the actual formula in that question. There is a link to Wikipedia's article on the Student's T, but I can't find the formula for the density function there. There is also a link to an R builtin, but my software package is written de novo in embedded C.
Basically I just need a pointer at somewhere that clearly describes the math and I can handle over the programming from there.",2013-10-14 21:26:47.753
104218,57462,22143.0,CC BY-SA 3.0,,"I am not applying MaxEnt per se. I chose uniform distribution over an interval as my data model. GIven that, the estimator for the upper interval bound is the answer given in my answer. I alluded to maxEnt because it says (I think) when you have no information, use the distribution with the maximum entropy.",2013-10-14 21:28:40.640
104219,57462,668.0,CC BY-SA 3.0,,"I don't believe MaxEnt says anything like that at all: you must always bear in mind the *purpose* of the distributional assumption.  I can't decipher what else you said in that comment--too many ""estimators"" appear in one sentence--but nevertheless I still see nothing in this answer that directly relates properties of the sample to the maximum of the *population.*",2013-10-14 21:31:43.027
104220,57461,22685.0,CC BY-SA 3.0,,@whuber - I think Kolmogorov–Smirnov statistical distance. But I'm not 100% sure.,2013-10-14 21:32:02.537
104222,57463,668.0,CC BY-SA 3.0,,"I did not refer you to that thread for its formulas--you are correct, it unfortunately lacks any--but to point out that there are *myriad* ways to compute the t distribution. It's not really a math question, but one of scientific programming. For instance, in your situation an attractive solution might be to store a few tables and interpolate within them, because then you won't have to program any kind of numerical integration routines. If you don't disclose your engineering constraints and objectives, you will reduce the opportunity to learn about such options.",2013-10-14 21:35:20.047
104223,57462,22143.0,CC BY-SA 3.0,,"I am claiming that the $max_{i=1,..,N_s}x_i$ is the estimator of the maximum of the *population* here. This is motivated from the fact that if the unknown distribution was uniform, the estimator would make sense. In other cases, I do not know how to construct the estimator from the sample which can tell me the maximum of the population.",2013-10-14 21:37:37.823
104224,57462,668.0,CC BY-SA 3.0,,let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/11061/discussion-between-whuber-and-theja),2013-10-14 21:38:02.153
104225,57447,2666.0,CC BY-SA 3.0,,Someone suggested using binary logistic regression but that would not be the best choice if $Y$ is not binary.,2013-10-14 21:39:50.870
104227,57416,22507.0,CC BY-SA 3.0,,It *is* an answer. The topic starter asked for a name of a statistical test.,2013-10-14 21:45:05.577
104228,57416,5237.0,CC BY-SA 3.0,,"I agree that it is an answer, that's why I didn't vote to delete it when SE's software automatically flagged it. However, it would be nice if you could expand it a little.",2013-10-14 21:47:29.183
104229,57463,22687.0,CC BY-SA 3.0,,"@whuber Well, that brings me back to the question title. If I were to build a table offline that I then cubic-interpolate at runtime, how can I build that table?",2013-10-14 21:49:47.730
104230,57407,594.0,CC BY-SA 3.0,,"Ron, I have added your change to your question and responded to it. Please note that the way you worded it in your comment doesn't make sense (what's the ""fifth quantile""? What's the ""first quantile""?), but with that part left out, we can get somewhere.",2013-10-14 21:52:15.027
104231,57457,6162.0,CC BY-SA 3.0,,"@whuber Yes, I do not pretend this is the expected answer :) (though my answer could help to perform the ""true"" triple integral calculation).",2013-10-14 21:54:59.367
104232,57467,503.0,CC BY-SA 3.0,,Look into equivalence testing.,2013-10-14 22:00:46.957
104234,57467,594.0,CC BY-SA 3.0,,"You can't *show* that it's 0, since it can be arbitrarily close to zero while being unequal to it. e.g. if $b_1 = 0.0000001$ then $b_1\neq 0$ - and you'd ideally reject that point null, yet with reasonable values for and moderate sample size (and for the disposition of the $x$'s I guess), you can't. Peter's suggestion to consider equivalence testing is a good one (but it's showing something a bit different from what you're asking).",2013-10-14 22:04:06.460
104235,57468,5045.0,CC BY-SA 3.0,,"Have you tried $y^{\frac{1}{2}}$ or $y^{\frac{1}{3}}$? These can be applied to zero (and negative values for the cube root), which is probably why you are loosing data. It might help if you post some summary statistics of your raw data (min, max, mean, median, kurtosis, skewness).",2013-10-14 22:09:08.430
104236,57463,594.0,CC BY-SA 3.0,,"For small integer d.f. you can do integration by parts. For larger d.f. you might do it by numerical integration, or by identifying a suitable approximation for the cdf (or some equivalent), some of which are in published algorithms.",2013-10-14 22:09:54.417
104237,57463,22687.0,CC BY-SA 3.0,,@Glen_b Where can I find one of those published algorithms? I lack the necessary statistics background to know the right words to punch into Google Scholar.,2013-10-14 22:15:00.820
104238,57467,16469.0,CC BY-SA 3.0,,"Thanks @PeterFlom, equivalence testing is what I was looking for. If you write it as an answer I will gladly accept it.",2013-10-14 22:49:54.187
104239,57463,594.0,CC BY-SA 3.0,,"Some algorithms for the cdf of the t are based on the incomplete beta function (which is a commonly used function in various parts of mathematics or physics). Plain googling on *algorithm cdf|""distribution function"" student t* turns up plenty of references within the pages linked (e.g. [here](http://devdoc.madlib.net/v0.2beta/student_8cpp.html)), such as Abramowitz and Stegun's *Handbook of Mathematical Functions* (which gives some small-d.f.-exact and approximate calculations), and various other books and papers.",2013-10-14 23:02:04.970
104240,57463,594.0,CC BY-SA 3.0,,"If you want the noncentral t (e.g. for power calculations) a standard reference is Lenth, R. V. 1989. ""Algorithm AS 243: Cumulative distribution function of the noncentral t distribution"". *Applied Statistics*, 38, 185-189.",2013-10-14 23:02:43.397
104242,57468,594.0,CC BY-SA 3.0,,"What kind of data do you have (e.g. is it count data, measurements)? Why did you transform it?",2013-10-14 23:06:07.333
104243,57449,594.0,CC BY-SA 3.0,,"Along the lines of $\sum_{j=1}^{n}(X_{j}-\bar X)(X_{j}-\bar X)^T
=\sum_{j=1}^{n} (X_{j}X_{j}^T-X_{j}\bar X^T-\bar X X_{j}^T+\bar X \bar X^T)$
and $\sum_{j=1}^{n}\bar X X_{j}^T=\bar X\sum_{j=1}^{n} X_{j}^T=n\bar X\bar X^T = \frac{1}{n} X^T1 (X^T1)^T$ etc",2013-10-14 23:27:24.997
104245,57465,14748.0,CC BY-SA 3.0,,"Thanks for your response. Just to clarify, what do you mean when you say ""influence the data source""? Once the algorithm goes live, it only gets fed examples where it had predicted 1's, since those are the only ones being checked. Does that ""influence the data source""?",2013-10-14 23:34:33.053
104246,57473,594.0,CC BY-SA 3.0,,"If you want standard errors, your second formula shouldn't have squared terms, and it's not the standard error of the distributions but the standard error of the sample means you're talking about there.",2013-10-14 23:37:55.263
104247,57473,5643.0,CC BY-SA 3.0,,"Yes sample means, not distribution means. Fixed the question.",2013-10-15 00:21:30.703
104249,57474,10060.0,CC BY-SA 3.0,,Check **the other tables** of the outputs and look for the coding schemes. These two modules might have coded your sex variable differently; one might have used male as reference while the other one used female.,2013-10-15 01:10:33.537
104250,57473,594.0,CC BY-SA 3.0,,You still have the first error I mentioned. I will fix it for you.,2013-10-15 01:22:57.853
104251,57483,7483.0,CC BY-SA 3.0,,You might get more informative answers about the scikit-learn specifics if you ask at https://github.com/scikit-learn/scikit-learn/issues.,2013-10-15 01:31:28.460
104254,57477,5643.0,CC BY-SA 3.0,,"I'm ok with the variance algebra, what I am asking is why the square root goes to the sum of the two variances divided into the sample sizes. Notice that in the second (wrong) formula the variance is already squared, that is we use the standard deviation.",2013-10-15 01:48:16.380
104255,57484,5237.0,CC BY-SA 3.0,,"Presumably by ""pho"", you mean *rho* ($\rho$). However, your question is not clear. What do you mean by ""What’s the tightest bound you can give""?",2013-10-15 01:57:19.573
104256,57273,22596.0,CC BY-SA 3.0,,"The problem is the simple part: how do you use MCMC Hammer to generate a model, and then fit that model to a pre-existing data set, when the model has a large amount of parameters?",2013-10-15 02:01:42.340
104257,57484,22698.0,CC BY-SA 3.0,,"Well the name of the variable is just a dummy. By tightest bound, I mean something like [-1, 1] for a correlation, but this clearly isn't the tightest possible bound.",2013-10-15 02:08:41.217
104260,57442,5637.0,CC BY-SA 3.0,,@FirhatNawfanH. Yes. Usually ARCH/GARCH mainly used in financial time series.,2013-10-15 02:42:06.040
104264,57485,22659.0,CC BY-SA 3.0,,"Thanks Jacob. Theta can indeed be a vector, however the elements of the vector represent that theta parameter in D-dimensional space. For example, if I wanted a hyperparam $\sigma$ for that SE kernel to be its variance, then that works. However, say for some reason I also needed another hyperparameter $A$ that scaled the entire thing, i.e. I had A*exp(\sigma*d**2), then it doesn't work. What if I wanted this kernel with those two hyperparameters but with two features? So we have $\mathbf{\theta} = \{A,\sigma\}$, and in the 1-D case $A$ and $\sigma$ are both scalars, but in a 2-D case they",2013-10-15 03:10:32.340
104265,57485,22659.0,CC BY-SA 3.0,,"must be 2-dimensional vectors themselves. So in terms of scikits code, see the final return value for the SE kernel, it reshapes theta just to make sure it has the proper formatting for a single row vector, where each element is a feature (but that kernel restricts $|\mathbf{\theta}|$ to 1). In a 2-D case I would expect $\mathbf{\theta}$ to be 2x2 matrix, where the first row corresponds to $A$ and the second row corresponds to $\sigma$, where the columns are the dimensions/features. Anyway, it's not actually like that, and it screams at me. I don't know the correct way to do this. Any insight?",2013-10-15 03:13:35.713
104266,57485,7155.0,CC BY-SA 3.0,,"Write the function for corr with the parameters hardcoded. From a programmatic view you probably want to do it as a partial application. From a statistics view you lose the ability to specify upper and lower bounds of theta, which would require a rewrite.",2013-10-15 03:37:12.963
104267,57485,22659.0,CC BY-SA 3.0,,"OK, that will probably be really tedious but maybe do-able for proof of concept at least. I really just wanted to make sure there wasn't another way and I was just not seeing it. Thanks.",2013-10-15 04:03:38.677
104269,57473,594.0,CC BY-SA 3.0,,I have made the thing you called a sum of standard errors into a sum of standard errors.,2013-10-15 04:37:47.040
104270,57487,668.0,CC BY-SA 3.0,,A derivation is sketched at http://stats.stackexchange.com/questions/72262/how-to-average-variables-having-given-standard-deviations.,2013-10-15 04:57:18.850
104271,57492,668.0,CC BY-SA 3.0,,"For recommendations to have some objective support, they need to respond to a more specific question than this.  Please consider indicating the intended audience (and their background) and the purpose of the text.  Also, a ""handbook"" is so close to a ""textbook"" that it does not appear sufficiently to distinguish this question from the predecessor, at least not without further elaboration of your aims.",2013-10-15 05:02:34.957
104272,57484,668.0,CC BY-SA 3.0,,"**Closely related**: http://stats.stackexchange.com/questions/5747.  It's not quite a duplicate, but its answers provide some results for the general trivariate correlation matrix that are easy to specialize to this case. The two points they do not cover are sufficiency--namely, showing that any matrix whose coefficients satisfy the conditions actually is a correlation matrix for some distribution--and tightness (there are no better bounds).",2013-10-15 05:06:02.347
104273,57473,668.0,CC BY-SA 3.0,,"Perhaps the most famous--and likely the earliest--completely intuitive demonstration of this relation was obtained by Galton with his [quincunx](http://www1.kcn.ne.jp/~h-uchii/quinc.html).  (Don't visit Wikipedia or any of the top hits on this term, because they explain only its most trivial uses: the link I gave is the first I could find that reproduces one of Galton's illustrations and hints at the revelations this machine makes possible.)",2013-10-15 05:15:01.360
104274,57473,668.0,CC BY-SA 3.0,,"The intuition is one of *cancellation of independent errors.*  It took scientists rather a long time--approximately 150 years from the beginning of probability theory in the mid 17th century to the work of Gauss and Laplace around 1800 as applied to combining astronomical observations--to realize that this actually happens with measurements! Because such cancellation is at the heart of the Central Limit Theorem, [intuitions for that](http://stats.stackexchange.com/questions/3734) are relevant here, too.",2013-10-15 05:21:36.373
104275,57474,22163.0,CC BY-SA 3.0,,"No, that is not it. I have been very careful to get the same reference for every variable in both logistic and Genlin. In every respect (since I have left out the repeated subcommand) the models should be the same.",2013-10-15 06:10:28.813
104276,57469,16469.0,CC BY-SA 3.0,,I agree with that. But that is from the perspective of significance testing. I was looking for a technique specifically designed to test the hypothesis of whether two groups are equal (or a coefficient is zero).,2013-10-15 06:13:39.253
104278,57422,22668.0,CC BY-SA 3.0,,"Thanks Gilbert. That was exactly the kind of answer that I was looking for -clear and precise. Do you have, or know any reference that I can include in my document concerning these comments? I have spent a couple of weeks looking for one -I have not been able to find anything discussing these points in the (clear) way that you just did.",2013-10-15 06:31:22.893
104279,57436,7949.0,CC BY-SA 3.0,,@rolando2 This is a reply to your comment on ineffectual control: your question still needs a bit more refinement regarding your definition of reversal. Are you interested in a reversal of the expectations of the coefficients? The answer above and the one below adress that. Or do you want to consider the power to statistically *detect* reversal (e.g. reject the null hypothesis that the coefficients of both the full and reduced model have the same sign)? The later depends on the residual variance and the magnitude of the expectations of the coefficient (thus indirectly also on R squared).,2013-10-15 06:55:09.097
104280,57446,20470.0,CC BY-SA 3.0,,"@ alto, thank you.  As you say, I will be looking at $p = log(P(O|hmm))$, and values like $p_1 =-2504, p_2 = -2403, p_3= -2450$, etc. so spotting a significant increase in $p$ may be problematic. In the meantime, I think training HMM2 will be hard. The number of points I have for HMM2 (no event) will be much higher and there may be no patter but only noise. What do you think?  **P.S**: I chose 5 in as my window size arbitrarily, it is likely to be longer than that in an actual implementation.",2013-10-15 07:13:31.163
104281,57498,15827.0,CC BY-SA 3.0,,"There are many dedicated texts that give you the details. At the opposite extreme is this one paragraph. Data come in different forms (e.g. counts can only be zero or positive integers, some measurements can be positive only, some are less restricted). Also, data come in different shapes: experience with data shows that. So, statisticians and others have proposed many different models, some with mathematical derivations and underlying ideas about generating processes, and others just proposed more or less empirically as shapes that might be fair fits for at least some distributions.",2013-10-15 08:16:55.943
104282,57492,20130.0,CC BY-SA 3.0,,"@whuber Thank you for your commentary. I described the details. The difference between ""handbook"" and ""textbook,"" as I see it, is in the depth of details and scope of topics. Handbooks in this case exclude discussions and explanation and concern upper-level topics usually omitted in textbooks.",2013-10-15 08:40:24.730
104284,57503,12683.0,CC BY-SA 3.0,,"Shape is associated not just with skew, but with anything that isn't to do with central tendency or dispersion; a shape parameter might well affect higher moments. It's any parameter that isn't a location or scale parameter. And cross out 'mostly' in the second sentence unless you can think of an exception.",2013-10-15 09:20:55.697
104285,57503,15827.0,CC BY-SA 3.0,,"@Scortchi Agreed on ""shape"". Vinux: A simple example is the Poisson. Change the mean and you change variance, skewness, kurtosis, ... It is not clear whether you regard such linked changes as usual or exceptional.",2013-10-15 09:33:13.637
104286,57503,12683.0,CC BY-SA 3.0,,@Nick: But would you call the mean of the Poisson  a location parameter? I wouldn't. Same goes for the mean of the exponential distribution - it's a scale parameter.,2013-10-15 09:36:55.283
104288,57503,15827.0,CC BY-SA 3.0,,"I guess that mathematical statisticians have invariance and equivariance criteria for different kinds of parameter. I am reacting to @Vinux's line that location indicates central tendency and the idea that location and central tendency are the same ballpark. That's probably loose at best by mathematical standards, but it's a data analytic view.",2013-10-15 09:46:20.217
104289,57503,12683.0,CC BY-SA 3.0,,Would it be right to say that for a location parameter $\theta$ the density $f(x;\theta)=f(x-\theta;0)$ and changing it only changes the mean; while for a scale parameter $\phi$ the density $f(x;\phi)=\frac{f(\frac{x}{\phi};1)}{\phi}$ and changing it only changes the variance & perhaps the mean?,2013-10-15 09:54:47.213
104291,57503,5637.0,CC BY-SA 3.0,,I think the question is very broad. I am not sure about  any standard definition for shape in distribution context. I was trying to give a layman's picture.,2013-10-15 09:59:38.470
104292,57469,21762.0,CC BY-SA 3.0,,As user31264 and also glen_b said: You can't. Equivalence tests are only able to show that a parameter is *close* to a certain value but not that it has exactly this value.,2013-10-15 10:05:42.527
104293,57513,21762.0,CC BY-SA 3.0,,How large is the reference group?,2013-10-15 10:07:02.830
104294,57513,10594.0,CC BY-SA 3.0,,Each treatment group had equal number of $n=20$,2013-10-15 10:11:54.957
104295,57423,22669.0,CC BY-SA 3.0,,"@vinux - just thought I'd add the other component to your answer here - It works well as (Xt=a+bt+Zt, where Zt is stationary series) - and the stationary series appears to be pretty much white noise. Thanks!
My underlying error was a failure to appreciate the difference between trend removal by differencing and trend removal by subtraction of a linear trend - the -1 MA(1) term seems to have been trying to convert a random walk back into white noise.",2013-10-15 10:18:22.473
104296,57513,21762.0,CC BY-SA 3.0,,"Maximum-likelihood estimates cannot be calculated in case of quasi-complete separation (only 0 in control, almost only 1 in test groups). This is confirmed by the huge standard errors.",2013-10-15 10:35:06.683
104297,57503,12683.0,CC BY-SA 3.0,,"Fair enough - but 'skewness' is even less a layman's term than 'shape'. Location parameters *shift*, scale parameters *stretch*, and shape parameters change the *shape*.",2013-10-15 10:53:03.027
104298,57505,7860.0,CC BY-SA 3.0,,"No idea how I should apply such a method to my issue, sorry. Upvote for pointing me to `scikit-learn`, I hadn't heard about that package, thanks.",2013-10-15 10:57:21.287
104299,57503,5637.0,CC BY-SA 3.0,,@Scortchi You are right. I should not associate shape with skewness. I will edit my answer.,2013-10-15 10:58:22.017
104300,57513,10594.0,CC BY-SA 3.0,,thanks @Michael Mayer. Indeed my data has quasi-complete separation problem. The estimated coefficients and the SE tend to be too large. Do you have any suggestions about how to deal with it? Present the result descriptively?,2013-10-15 11:05:01.797
104301,57503,15827.0,CC BY-SA 3.0,,"@Scortchi The lay use of ""skewed"" is increasingly as meaning ""biased"", as in ""the results are skewed by including too many people of type X"". It may have been around for some time, but I've noticed it more recently. I haven't noticed ""skewness"" as a synonym for ""bias"".",2013-10-15 11:07:29.013
104305,57504,2666.0,CC BY-SA 3.0,,"There are different brands of nonparametric regression, some better called semiparametric.  Semiparametric models use only the rank of $Y$ so don't dependent on having a proper transformation of $Y$.  Such models include the proportional odds and proportional hazard models.",2013-10-15 12:07:02.523
104306,57436,1693.0,CC BY-SA 3.0,,"Thanks @Erik!  Of your 2, it's close to the former. I'm asking about ""what conditions are necessary in order to obtain the theoretically expected reversal of a coefficient.""",2013-10-15 12:09:52.630
104307,57519,21884.0,CC BY-SA 3.0,,Wow... that software seems extremely powerful. I guess that answers the question that it is indeed $O(1/n)$ too. Thank you very much.,2013-10-15 12:14:59.737
104308,57519,21884.0,CC BY-SA 3.0,,"wolfies, is your software available for users who don't have mathematica installed?",2013-10-15 12:40:34.743
104309,57523,22410.0,CC BY-SA 3.0,,"thanks. But how does that relate to ""If the (colored) clusters look separated in at least some of the plots. They won’t be very separated in all of the plots.""",2013-10-15 13:03:15.033
104310,57519,17328.0,CC BY-SA 3.0,,"mathStatica is built on top of _Mathematica_, so it requires _Mathematica_. We have thought of porting it to other computer algebra systems, but it is unfortunately a lot of work to do that.",2013-10-15 13:07:02.977
104311,57517,14874.0,CC BY-SA 3.0,,"Ok, my formulation was awkward. What I wanted to say is, that the dependence is nonlinear and we could assume something like $E(x_i|Y) = \sqrt{x_i}$. I hope its clear now?!",2013-10-15 13:18:32.270
104312,57296,21398.0,CC BY-SA 3.0,,No one knows. No survey specialists here :'-(.,2013-10-15 13:23:38.960
104313,57523,10278.0,CC BY-SA 3.0,,"Well take a look at the red cluster.  It's very well separated if you consider `Petal.Width` against `Petal.Length` but less well separated if you consider `Sepal.Width` against `Sepal.Length`.  You can tell this by looking at the univariate density plots, the red curve overlaps much more the blue and green curves when you consider `Sepal.Width` then when you consider `Petal.Width`.",2013-10-15 13:28:54.390
104314,57526,594.0,CC BY-SA 3.0,,"You *might* choose MMSE, its a fine criterion, but that doesn't mean you have to use it.",2013-10-15 13:33:14.030
104315,57526,594.0,CC BY-SA 3.0,,"For the normal it gives a divisor of $n+1$, but one problem is you don't actually know what distribution you really have. Yet the $n-1$ form is unbiased for every distribution. I often just use ML, but I'm generally as happy with $n-1$, and not averse to $n+1$, even though I rarely use it. It's only a hard choice when $n$ is small.",2013-10-15 13:39:02.147
104316,57527,,CC BY-SA 3.0,,"Hey, it was not really about the inclusion of seasonal component to explain the response variable, but how to handle the seasonality in the regressors.",2013-10-15 13:42:53.573
104317,57528,2149.0,CC BY-SA 3.0,,"Does Proc Ucm deal with multiple level shifts , multiple time trends , changes in seasonality, lead and lag effects around known events while detecting parameter transience and variance heterogeneity ?  or does it have embedded assumptions about the non-existence of these characteristics ? It might be interesting for us to share some results offline. If you are interested please contact me. Alternatively if we can get the OP to post his actual data we could have a public bakeoff/comparison",2013-10-15 13:58:40.613
104318,57528,22705.0,CC BY-SA 3.0,,"The trend, seasonality components are estimated using Kalman filter (random walk, EM approach).  So, while I've not encountered the problems you're raising, i'm guessing it should. Yes, running it on OP's data-set will help determine. To answer your question in another way, I've read that any ARIMA model can be expressed as a state space equation and thus be modeled using UCM.",2013-10-15 14:13:55.060
104319,57527,22705.0,CC BY-SA 3.0,,"Yes, JohnnyB. Maybe I wasn't clear. PROC UCM helps estimate trend & seasonality components from your response variable pattern. You don't have to explicitly estimate the trend/seasonality using indicator variables/proc timeseries etc. http://support.sas.com/documentation/cdl/en/etsug/60372/HTML/default/viewer.htm#etsug_ucm_sect001.htm  Hope I could explain better.",2013-10-15 14:20:20.397
104320,57528,15827.0,CC BY-SA 3.0,,"What language or package is this please? The mention of proc (?PROC) leads me to guess SAS, but please spell it out. See http://meta.stats.stackexchange.com/questions/1479/how-to-ask-a-good-question-on-crossvalidated for the advice ""Say what programming language you're using"". (It applies to answers as well as questions.)",2013-10-15 14:28:15.277
104322,57505,22678.0,CC BY-SA 3.0,,"Well, here is an example: http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#example-svm-plot-oneclass-py",2013-10-15 14:42:10.060
104323,57532,20304.0,CC BY-SA 3.0,,Thanks.  That makes sense.  I guess that due to the nature of subscription-based services the curve will get fuller as time goes on.  There must be some limiting time where all accounts cancel but I guess we don't know what it is yet...,2013-10-15 14:42:21.483
104324,57528,22705.0,CC BY-SA 3.0,,"Yes. it is SAS. Sorry, didn't know the conventions. But, I've bumped into presentations of stata which has implemented ucm as well.",2013-10-15 14:44:38.520
104325,57532,3999.0,CC BY-SA 3.0,,"@user1893354 The day you go out of business ;) And the curve should never get ""fuller"" (by which I assume you mean go up) because your time variable is not calendar time, but time since the account was opened.",2013-10-15 14:45:19.007
104326,57533,2081.0,CC BY-SA 3.0,,"I'm not fluent in deciphering formulae, but what I may say for sure is that discriminant analysis (see what's written in its tag) first extracts latent dimensions (discriminants), like PCA does. Only then it classifies - with those. As far as I can see, your question is about classification only. It is unclear if you are speaking of the original variables or the discriminant latent variables here.",2013-10-15 14:47:46.497
104328,57492,3922.0,CC BY-SA 3.0,,possible duplicate of [Good econometrics textbooks?](http://stats.stackexchange.com/questions/4612/good-econometrics-textbooks),2013-10-15 14:50:18.687
104329,57535,668.0,CC BY-SA 3.0,,"My reply at http://stats.stackexchange.com/a/13317 addresses the negative part of this question: namely, why $R^2$ does not of itself tell you anything about linearity, especially when the independent variables in the data sets are different.  Although you did not ask it, this question begs for a positive response, too (and perhaps you should mention this explicitly): given that your proposed methods don't do the job, what *does* work for assessing linearity?",2013-10-15 14:56:01.060
104330,57465,22143.0,CC BY-SA 3.0,,"No that does not influence the data source. By influence, I mean, if the output of the algorithm (lets say an ad) affects the data source (say user behavior such as click/no click) which is then fed back to update the algorithm.",2013-10-15 14:57:52.003
104331,57464,22143.0,CC BY-SA 3.0,,the algorithm is receiving 0s and 1s according to the OP. It is only that it is seeing these 0s and 1s when the algorithm output is 1.,2013-10-15 14:59:00.947
104332,57532,20304.0,CC BY-SA 3.0,,"Sorry, by ""fuller"" I meant that the curve should get closer to zero (as older accounts eventually start cancelling)",2013-10-15 15:00:48.500
104333,57535,22713.0,CC BY-SA 3.0,,"Thank you for your answer. I am trying to show how linear the data increases by time, and their difference. If my methods don't work, what else method can I use to address this?",2013-10-15 15:07:35.917
104334,57389,166.0,CC BY-SA 3.0,,"@broiyan:  Sounds close. I think technically the precise value obtained, if re-obtained, would be included in the probability.  Therefore, I would go to the expense of extra words to make that clear by saying ""as extreme or more extreme"".  Also, because you are phrasing it in a frequentist framework, I wouldn't talk about the probability of that event, but the proportion of times you would expect it to occur if the experiment were repeated over and over.",2013-10-15 15:08:03.567
104335,57448,22656.0,CC BY-SA 3.0,,"Thanks for the reference,  but I cannot find α in the formula.",2013-10-15 15:11:37.880
104336,57536,21804.0,CC BY-SA 3.0,,"There are several different models you could build based on this data. Using more than one predictor would rersult in a regression plane though, so the task seems to imply that you choose one. But the dataset itself does not necessitate a specific choice, so some clarification is indeed missing...",2013-10-15 15:22:01.200
104337,57535,668.0,CC BY-SA 3.0,,"Please edit your question to include the clarification you just made.  It would also help to explain more fully what you mean by ""more"" or ""less"" linear and precisely what ""difference"" you refer to.",2013-10-15 15:23:35.403
104338,57541,22714.0,CC BY-SA 3.0,,"Sorry, I don't understand your approach. What is nob value? Could you explain your approach in more details? Can I run a hierachical clustering? Thank you.",2013-10-15 15:23:56.837
104340,57528,15827.0,CC BY-SA 3.0,,Indeed; Stata has a `ucm` command too. There will be other implementations too.,2013-10-15 15:29:52.367
104341,57504,668.0,CC BY-SA 3.0,,"I think this answer may be misleading because it is not sufficiently clear about what ""function type"" means: the distinction between *linear* and *nonlinear* lies in how the *parameters* enter the functional formula, not the variables.  For instance, the second bullet is *not* an example of a nonlinear model; indeed (assuming $(x_1,x_2,x_4)$ are the variables), is has no parameters at all!",2013-10-15 15:31:47.237
104342,57533,19265.0,CC BY-SA 3.0,,There is only one dimension. What discriminants can we extract here?,2013-10-15 15:38:35.083
104343,57541,15827.0,CC BY-SA 3.0,,"As usual, @IrishStat, your inclination is to treat everything as a time series! If the data are not a time series, I think the validity of your method depends on whether the same classes (breakpoints) would be identified from a series and itself reversed, i.e. the method depends on past and future being interchangeable.",2013-10-15 15:43:29.297
104344,57541,22714.0,CC BY-SA 3.0,,I am not working with time series... What methods can I use? Can I use multiple classification methods (like hierachical ascending) by only one variable as a classifier?,2013-10-15 15:49:27.217
104346,57542,5203.0,CC BY-SA 3.0,,"Thanks, @whuber, I somehow mix up row and column all the time (and English is my native language!)",2013-10-15 16:04:47.167
104347,57541,2149.0,CC BY-SA 3.0,,"@Nick The trick when using Interevention Detection for non-time series data (whis is what we have) (successful I might add) is to disable ARIMA identification, seasonal pulse identification and trend detection. These constraints eliminate any and all unwarranted/unwanted time series structure. All that is left to identify is level/step shifts (group classification) and pulse detection (one time anomalies). Send me your email address and I will forward you the results of any set of values that you wish to send to me.",2013-10-15 16:20:31.597
104349,57541,15827.0,CC BY-SA 3.0,,"@IrishStat Thanks, but I already have code that does something similar to my satisfaction (I wrote the answer to the thread cited as duplicate to this). I was just curious about any hidden assumptions behind your suggestion.",2013-10-15 16:36:37.500
104350,57547,15827.0,CC BY-SA 3.0,,"The first half of this is missing, i.e. what your scientific problem is, how Date features in your analysis, why you are treating it as a factor, etc.",2013-10-15 16:42:56.187
104351,57541,2149.0,CC BY-SA 3.0,,@jos nob is the number of observations in the set. The procedure to do Intervention Detection can be found in a number of places including http://www.unc.edu/~jbhill/tsay.pdf,2013-10-15 16:55:23.337
104352,57464,22507.0,CC BY-SA 3.0,,Then it will be biased another way (toward 0's).,2013-10-15 16:58:34.527
104353,57547,2857.0,CC BY-SA 3.0,,Probably [related](http://stats.stackexchange.com/questions/9751/do-we-need-a-global-test-before-post-hoc-tests),2013-10-15 17:05:57.397
104355,57485,7155.0,CC BY-SA 3.0,,"The most obvious way to me, would be write the two separate corr functions that are partially applied on theta. E.g. def square_exp(d), then write a function to combine the results via multiplication or addition with the argument def function(theta, d), where theta doesn't do anything in the function.",2013-10-15 17:21:52.837
104356,57398,5821.0,CC BY-SA 3.0,,You're saying clogit isn't a two tailed test?,2013-10-15 17:25:25.877
104358,57545,668.0,CC BY-SA 3.0,,"**This question is not answerable** because the domain of the pmf has not been specified.  (I suspect there is a typographical error and that the pmf might be $(1-\pi)^{1-x}\pi^x$ for $x\in\{0,1\}$.)",2013-10-15 17:47:37.777
104359,57553,9716.0,CC BY-SA 3.0,,"Sorry for the mistake, i edited the question.",2013-10-15 17:56:21.573
104360,57545,21985.0,CC BY-SA 3.0,,"The exercise IS realy like this... I was also puzzled by this, but I can not say why it looks odd. There is no solution for sure?",2013-10-15 17:57:40.953
104361,57545,5448.0,CC BY-SA 3.0,,"A lot of exercises like this are really more designed (in my experience) to emphasize or test recognition of the pmf.  So  we'd assume that $p(x;\pi)$ is some slight specialization of a standard pmf, perhaps by a simple transform of the random variable, try to figure out which one it is, then get the sample space from the definition of the (standard) pmf.  In this case, the random variable $x$ can be easily transformed to another random variable, let us say $y$, for which $p(y;\pi)$ is a well-known distribution, and everything else follows from that.",2013-10-15 18:07:10.543
104362,57554,15827.0,CC BY-SA 3.0,,"This may be clear to some, but I can't understand the distinction you are making. Empirical CDFs are necessarily calculated from the data. What is the modified CDF? Can you give an example of what the result would look like? Or an accessible reference for the term ""modified CDF""?",2013-10-15 18:21:22.150
104363,57553,9483.0,CC BY-SA 3.0,,"The issue is that the search space of your GA, viz. the polynomials of degree 10, is too far away from your target function `f(x) = 1 /(1 + (5*x)^2)`, hence the poor fitness value.",2013-10-15 18:22:00.740
104364,57553,9716.0,CC BY-SA 3.0,,"So theoretically, if i increase this search i should get better fits?",2013-10-15 18:23:33.113
104365,57553,9483.0,CC BY-SA 3.0,,"Yes but since your target function cannot be approximated by a polynomial, increasing the degree is useless. In evolutionary computation such task is called symbolic regression and we use genetic programming to optimize, not GA (GA typically requires to have a pretty good knowledge on the structure of your target function).",2013-10-15 18:30:03.400
104366,57555,15827.0,CC BY-SA 3.0,,"This seems akin to a longstanding joke in which one person says ""Suppose that there are $s$ sheep"" and somebody asks ""But what if the number of sheep is different?"". If you denote some probability $p$, that is your notation: no more. Clearly what its magnitude is numerically and whether you can estimate it accurately are key questions; in abstraction all we can say is that some probabilities are more difficult to estimate than others. That can be said independently of any speculation about ""true randomness"".",2013-10-15 18:31:06.587
104367,57448,16644.0,CC BY-SA 3.0,,"The 3rd equation on the reference page says that with a sample of size $b$ the probability that you will be more than $\epsilon$ away from the true cdf is $\leq 2e^{-2b \epsilon^2}.$ For your application just set this probability to be $\leq \alpha$ (where your $\alpha$ will be very small, since you want to be highly likely to be close to the true cdf) and then solve for $b.$",2013-10-15 18:46:47.240
104368,57561,15827.0,CC BY-SA 3.0,,"Not so. I've never met your first usage, even as a mistake. If you were right, and I don't think you are, it would be much flagged that a minute difference in wording was associated with such a big difference in meaning: textbook writers would be obliged to explain at  length and there would be campaigns to change the terminology.  I challenge you to find even one explanation of your definition in the literature. @whuber already gave an excellent answer that remains definitive.",2013-10-15 18:55:11.040
104369,57558,12544.0,CC BY-SA 3.0,,"I'm not sure why, but breaking it into two steps works for me: 
AccLog <- as.logical(as.numeric(Accuracy)) then run m2 with AccLog.",2013-10-15 18:57:09.953
104370,57558,12544.0,CC BY-SA 3.0,,"You get the same result with glm, not just glmer.",2013-10-15 19:00:23.033
104371,57562,21958.0,CC BY-SA 3.0,,"For example, a model could look like this:
Y_t=alpha + beta_1*X_t*Y_(t-1) + beta_2*X_(t-1)*Y_(t-2) +sigma_t*epsilon_t

, where the X_t and X_(t-1) are two latent variables either 1 or 0, alpha,beta's,sigma are parameters, and epsilon_t is standard normally distributed. 
So this is an autoregressive model, with lag=2.",2013-10-15 19:04:02.800
104373,57562,21958.0,CC BY-SA 3.0,,"If I wanted to get the residuals, I could choose the mean of the posterior distributions of the alpha,beta_1,beta_2 and sigma_t and get something like:
r_t= (Y_observed - Y_predicted)/sigma_t = standard normally distributed, but I would have to choose some value for the latent variables. And this makes no sense.",2013-10-15 19:15:30.850
104374,57555,16588.0,CC BY-SA 3.0,,"It might be worth expanding on what you mean by ""true randomness seems impossible."" Or more generally, give a concrete example of what you're trying to describe/ask.",2013-10-15 19:18:13.447
104375,57533,2081.0,CC BY-SA 3.0,,"Ah, well (I didn't see it). Then term ""discriminant analysis"" _proper_ is inapplicable, although word ""discrimination"" can retained, as it is close to ""classification"" or ""distinguishing"".",2013-10-15 19:26:05.770
104376,57566,1895.0,CC BY-SA 3.0,,"Symmetry and the law of the unconscious statistician. But a word of caution: ""...like a N(0,1) random variable..."" is sloppy, imprecise wording on the part of your professor. Such a statement is not, in general, true for any old symmetric distribution. The requisite moments must exist (be well-defined) in the first place.",2013-10-15 19:39:02.050
104377,57550,12683.0,CC BY-SA 3.0,,"For logistic regression [it's fine](http://stats.stackexchange.com/questions/67903/does-down-sampling-change-logistic-regression-coefficients/68726), though you'll have to be careful that the sample of NS firms is not biased with respect to any predictors.",2013-10-15 19:41:29.893
104378,57554,15363.0,CC BY-SA 3.0,,"So, the empirical cumulative is a step function that gives the mapping of the number of data points in in the population that are ≤ X. But, in R, the same is not done by ecdf. You can see this anamoly here - http://stats.stackexchange.com/questions/51607/strange-behavior-of-r-function-ecdf",2013-10-15 20:16:04.507
104379,57286,12683.0,CC BY-SA 3.0,,Doesn't do anything when `conf.int = FALSE` which is also the default. Try `?wilcox.test`.,2013-10-15 20:28:53.687
104410,57587,15539.0,CC BY-SA 3.0,,"I have one additional question for you. Given all 3 of these models, the X1X2 model I posted above and two separate ones for X1 and X2, how can you tell which is best, given the R^2 and MSE values for each?",2013-10-16 02:19:14.097
104380,57554,15827.0,CC BY-SA 3.0,,I don't see that thread supporting your report of an anomaly. It seems a matter of convention to use $\le$ rather than $<$; the same difference can be found in literature on survival functions (or the same beasts otherwise named). Elsewhere I am the author of a program for plotting CDFs that supports different conventions (`distplot` in Stata).,2013-10-15 20:31:37.960
104381,57566,668.0,CC BY-SA 3.0,,"This assertion is false unless a particularly narrow interpretation of ""symmetric"" is adopted (and even then @Cardinal's admonition must be heeded): see http://stats.stackexchange.com/a/29010 for more about this. For *familiar* counterexamples in Cardinal's spirit you may contemplate what happens with any Student $t$ distribution with $\nu$ degrees of freedom and you use any odd power greater than or equal to $\nu$.",2013-10-15 20:55:41.677
104382,57565,668.0,CC BY-SA 3.0,,"Could you explain the reasoning that justifies your assertion that ""there shouldn't be any difference""?",2013-10-15 20:59:52.493
104383,57565,22727.0,CC BY-SA 3.0,,"Assuming his sampling is truly random and that he already has a list of every single person with heart disease in the world (as implied by his 'Or is it better to only look at the people who have heart disease.' And since he is only interested in finding ten people with heart disease, then it is just as random and more efficient to select from the group you are interested in studying rather than selecting from the billions of people on earth over and over until all 10 have heart disease.",2013-10-15 21:11:54.900
104385,57573,,CC BY-SA 3.0,user30490,"I am reading through the paper but am stuck, do you think you can elaborate on these calculations?",2013-10-15 21:57:52.180
104386,57573,,CC BY-SA 3.0,user30490,Or rather is it obvious why this result holds?,2013-10-15 22:12:42.023
104387,57521,594.0,CC BY-SA 3.0,,There's a fairly good introduction [here](http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval),2013-10-15 22:17:46.620
104388,57578,15827.0,CC BY-SA 3.0,,No problem in principle. Plot the data and the fitted curve as well as looking at the numeric results to see how well it works.,2013-10-15 22:23:32.070
104389,57578,594.0,CC BY-SA 3.0,,"Well, as you already note, you can get multicollinearity (though this can be avoided with the use of orthogonal polynomials).",2013-10-15 22:42:47.413
104390,57554,594.0,CC BY-SA 3.0,,It doesn't give the proportion $\leq X$ but the proportion $\leq x$.,2013-10-15 22:43:45.327
104392,57554,594.0,CC BY-SA 3.0,,"I'm looking at a plot of an [ECDF in R right now](http://i.imgur.com/kmWSwaY.png), it's definitely giving $\leq x$, and I'm not sure I understand the problem you have with it. It's an empirical cdf so obviously it has to be defined in terms of proportions, since the probabilities are unknown. What's the issue?",2013-10-15 23:02:13.677
104393,57538,594.0,CC BY-SA 3.0,,"Generally, the gist of these kind of problems go: combine and expand the exponents, collect like terms, complete the square, write quadratic as $(x-g(y,\mu,...))^2+S$, spot the density.",2013-10-15 23:06:19.433
104394,57506,22639.0,CC BY-SA 3.0,,this seems closest to what I was looking for.,2013-10-15 23:28:51.703
104395,57578,16588.0,CC BY-SA 3.0,,"An alternative to the suggestion of @Glen_b is to first center X, then create a squared version of the centered X variable. Include them both and multicollinearity will be substantially reduced. As an added bonus, model interpretation is often nicer with centered IVs.",2013-10-16 00:12:18.833
104396,57578,15827.0,CC BY-SA 3.0,,"In this case, what most makes the model interpretable is, I suggest, to plot data and model fit. Centering X does no harm, but the most interesting level for X is often that at which the quadratic has a turning point, whenever that occurs within the range of the data. (It may lie way outside that range.) Numerical stability given correlated predictors is much less of a difficulty with modern statistical software than it was a few decades ago when regression programs did not always use good algorithms. But an X and its square can't be interpreted as having separate effects, regardless.",2013-10-16 00:35:14.263
104397,57583,594.0,CC BY-SA 3.0,,Is this for some subject?,2013-10-16 00:36:00.143
104398,57572,19681.0,CC BY-SA 3.0,,"Not sure why you say the dependent variable is not repeatedly measured.  For the subject with ID = S_1, it looks like there are 4 measurements.",2013-10-16 00:42:01.537
104399,57572,22732.0,CC BY-SA 3.0,,"Because I'm trying to predict whether an individual will ever get cancer. So the true outcome would be all Ys if the individual ever gets cancer and all Ns otherwise. In the table above, this would translate into all Ys for S_1, all Ns for S_2, all Ys for S_3.",2013-10-16 00:52:03.660
104401,57564,1411.0,CC BY-SA 3.0,,"it might be worth submitting this as an issue at https://github.com/lme4/lme4/issues -- I think it *should* work correctly with a factor, as `glm()` does ...",2013-10-16 00:54:03.350
104402,57583,15827.0,CC BY-SA 3.0,,"The answer would probably be a publishable paper in some literatures, e.g. statistical hydrology. By the same token, an answer seems unlikely unless it already has been published.",2013-10-16 00:58:02.527
104403,57577,14888.0,CC BY-SA 3.0,,"How about in the case where the two separate measures of the dependent variable are to measure an experimental effect that takes place in between measures? This is more akin to a paired t-test, but in this example there are two factors (or more). Most discussions/introductions to repeated measures ANOVA that I have found are about cases in which a dependent variable is predicted by a few to several independent variables that are measured on the same set of individuals (so the values are correlated within individuals).",2013-10-16 01:07:14.310
104404,57584,17249.0,CC BY-SA 3.0,,I cannot imagine doing this being possible.,2013-10-16 01:27:53.150
104405,57584,15539.0,CC BY-SA 3.0,,Neither can I.. could some type of technique involving dummy/indicator variables be the key? I'm just completely throwing that out there...,2013-10-16 01:37:39.587
104406,57572,19681.0,CC BY-SA 3.0,,"Ah, OK. I'll venture a guess that mixed models are appropriate.  Your primary concern seems to be that the response is fixed, within-subject.  One way of looking at this is that the within-subject variance is zero. I suspect that this fits in with mixed models as a special case, but here is where my knowledge ends.",2013-10-16 01:39:14.637
104407,57587,15539.0,CC BY-SA 3.0,,"Ok! Great.. I actually do have the ANOVA tables for X1 and X2 separately, so I have 2 additional tables. How could I go about completing the partial F-test for X1, given X2 is already in the model if I have the ANOVA tables for X1 and X2 separately?",2013-10-16 01:45:05.427
104408,57587,5237.0,CC BY-SA 3.0,,"That's more or less in the other answers I link to, but you might have to read between the lines. But I'll edit so that it's explicit here.",2013-10-16 01:47:55.610
104409,57480,22423.0,CC BY-SA 3.0,,"Thanks @Jonathan, this is what I wished to know. Just a final question before I accept this as answer:  if in 4 surveys the weights were summed to total respondents, and 1 survey was weighted to reflect Total US population. If I multiply (Respondent size/Total US pop) to the weights of that 1 survey, and union the 5 datasets together with their weights.  
Would it be right to say that I have achieved final weights where each individual record do not have any special importance relative to another dataset?",2013-10-16 01:48:56.320
104411,57570,4656.0,CC BY-SA 3.0,,I like the way your answer skirts around the answer that most people would arrive at.,2013-10-16 02:22:56.087
104412,57587,5237.0,CC BY-SA 3.0,,"You could use theoretical knowledge that you have of the subject matter, but to a first approximation there isn't a ""best"", at least that we will be able to discern. Note, eg, that the MSE *must* go down as you add variables, & R2 *must* go up.",2013-10-16 02:26:43.897
104413,57582,16588.0,CC BY-SA 3.0,,This could be a helpful start: http://stats.stackexchange.com/q/3463/24000,2013-10-16 02:48:27.377
104414,57570,668.0,CC BY-SA 3.0,,@Dilip I have no clue what that might mean.  What answer to you believe most people would offer?,2013-10-16 02:52:38.580
104415,57582,16588.0,CC BY-SA 3.0,,Also: http://stats.stackexchange.com/a/26846/24000,2013-10-16 02:55:24.420
104416,57570,4656.0,CC BY-SA 3.0,,"I would have said that $X$ is a geometric random variable with parameter $\pi$, where $\pi$ does not have the usual meaning that it has in mathematical circles, pun intended, but is a number in $(0,1)$, and $x$ takes on all nonnegative integer values. Using the hint, the experiment would consist of tossing a biased coin with $P(\text{Heads})=\pi$ until a Head occurs, the sample space would be the set $$\Omega=\{H,TH,TTH,\ldots\}$$ with outcomes having probabilities $\pi,(1-\pi)\pi,\ldots$, and $X$ being the number of Tails in the outcome.",2013-10-16 03:07:02.060
104417,57587,15539.0,CC BY-SA 3.0,,"From the values I calculated, I have: for X1: R2 = 8%, MSE=5900; for X2: R2 = 80.1%, MSE = 1880, for X1X2: R2=80.6%, MSE=1940. Which model do you feel is best? It appears to me as if it's a clear race between X2 and X1X2(combined), but which?",2013-10-16 03:43:55.373
104418,53439,22705.0,CC BY-SA 3.0,,"This looks like a very exciting problem to solve. but, i think there are too many problems you can solve.
so, the first step is to fix on one or two key problems. and, then determine the factors which will be essential to solve that problem. i don't think we should talk about any model till then.",2013-10-16 05:10:07.357
104419,57529,15563.0,CC BY-SA 3.0,,"Thank you @JTT. So if I now use newdat to create a SVM model, I suppose my model takes input in this new rotated universe, which means I will need to also rotate my Test data before applying it to the model. Is this correct? And if yes, how do you rotate a test data.frame with the same rotation?",2013-10-16 05:24:35.300
104420,57530,15563.0,CC BY-SA 3.0,,Thank you for providing so much details. Unfortunately the example code is too cryptic for me. I see you are using predict. Where is the manual for prcomp predict? is it here: http://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html ?,2013-10-16 05:36:26.843
104421,57529,,CC BY-SA 3.0,anon,"The easiest way is to use the `predict()` method for the test data. Using the example above, `predict(pr, USArrests)` will return the same matrix as `pr$x`. For test data, replace the USarrests with the name of the test data. You can do the same thing by hand, but this is easier, since the predict methods takes automatically care of the correct scaling of the test data set.",2013-10-16 05:36:43.183
104422,57529,15563.0,CC BY-SA 3.0,,How does predict work? Does it use all Principal Compenents. In your answer you had chosen only 2 Components to cover 80% of variance. What does predict do?,2013-10-16 05:44:23.613
104423,57592,15539.0,CC BY-SA 3.0,,"Just on a purely theoretical basis, would it not be possible to be exclusionary for some of the results? As in, exclude Roku results where Roku 2 is mentioned Roku 3 is not, but include results with both Roku 2 and Roku 3 mentioned.
However, I think on a broad, automated scale this is a very difficult task. I think this explains why so many search engines have difficulty with the concept of 'relevance'. I'd love to hear your feedback on this suggestion.",2013-10-16 05:56:31.893
104424,57592,15539.0,CC BY-SA 3.0,,"I think that part of the reason that this may be difficult is that you would need sets of related data. Using automated methods, I would think it would be quite difficult for an engine to determine that that are indeed two different Roku, 2 and 3 (and perhaps the original).",2013-10-16 06:01:17.543
104425,57529,,CC BY-SA 3.0,anon,"The function `predict()` uses by default all the components. However, you can limit the number of components that are returned, e.g., `predict(pr, USArrests)[,1:2]. Would that work for you?",2013-10-16 06:33:40.360
104426,57579,2081.0,CC BY-SA 3.0,,"The really important matter is the meaning of it. Whether a missing is ""not known value"" (and thence NA*0=0) or ""unprovided entry"" (and thence NA*0=NA).",2013-10-16 06:55:26.923
104428,57530,8074.0,CC BY-SA 3.0,,"I have now added more explanation to my answer. Hopefully it is clearer to you now. Yes, you were correct in your link to the `predict.prcomp` help.",2013-10-16 07:32:36.150
104429,57594,15539.0,CC BY-SA 3.0,,"Adjusted R^2 is not to be used for this problem. From the values I calculated, I have: for X1: R2 = 8%, MSE=5900; for X2: R2 = 80.1%, MSE = 1880, for X1X2: R2=80.6%, MSE=1940. Which model do you feel is best? It appears to me as if it's a clear race between X2 and X1X2(combined), but which?",2013-10-16 07:52:09.740
104430,57594,22705.0,CC BY-SA 3.0,,"I would not take a stance between the two options, till I understand the problem and look at the interpretation offered by the two models. May be a model with x1, x2 and x1x2 will give you some insights. You can then possibly articulate interaction effects better. And, how are your residuals? Are you sure you have no information left there? Finally, why not adj r sq?",2013-10-16 08:11:02.780
104431,57554,15363.0,CC BY-SA 3.0,,"Sorry people, I think I misunderstood the statement. Thanks for stepping in !",2013-10-16 08:44:20.747
104432,57596,17328.0,CC BY-SA 3.0,,So ummm what is the question?  Find t such that ... what?  That the amount raised = amount spent? Or find t to maximise something subject to a budget constraint? Or something else?,2013-10-16 08:46:28.083
104433,57448,22656.0,CC BY-SA 3.0,,"Thanks. Sorry I am not good at statistic, I have another question; Can we say ϵ is confidence interval?",2013-10-16 08:47:18.880
104435,57588,,CC BY-SA 3.0,,"From your comment to @Learnerbeaver's reply below, I gather that this is homework/self study, which is why I added the relevant tag. So a few hints: what have you learned about the effect on R^2 when adding a variable to a model? How do R^2 and MSE relate? What does this imply about how useful R^2 and MSE are in selecting models (and why Learnerbeaver recommends adjusted R^2)? Finally, look at your specific values of R^2 and MSE for the three models in the comment below. You may be able to figure out the answer yourself by now.",2013-10-16 09:30:22.927
104436,57603,22750.0,CC BY-SA 3.0,,I generate P(L) and P(D) from two different models. I do not know which is best.,2013-10-16 10:00:48.117
104437,57577,503.0,CC BY-SA 3.0,,That case isn't really different than the one I described. RM ANOVA is a generalization of a paired t-test but it makes some unrealistic assumptions such as sphericity.,2013-10-16 10:09:13.940
104438,57588,503.0,CC BY-SA 3.0,,"If you've studied AIC and BIC and so on in class, that might be what is being looked for.",2013-10-16 10:25:35.127
104439,57588,15539.0,CC BY-SA 3.0,,"Yes, it appears option 2 with X2 is the option, as MSE goes up when X1 is added to the model in the third option (X1,X2), so X2 would have to be it. Can you perhaps offer me a theoretical comment on why this is so?",2013-10-16 10:42:43.390
104542,57662,15827.0,CC BY-SA 3.0,,"The view that Poisson regression is for counts only is widely rebutted: for one informal account with Stata flavour see http://blog.stata.com/2011/08/22/use-poisson-rather-than-regress-tell-a-friend/ Also, for Stata ""non-integer frequency weights"" just can't be frequency weights, but they can be analytic weights.",2013-10-16 22:48:59.660
104440,57601,12282.0,CC BY-SA 3.0,,"The question as posed is nice and straightforward to read, but some other info it would be useful to add in at the end: 1) What kind of variables are L and D? Continuous? Categorical? What values can they take?  2) Given know P(L|D) and P(D|L) exactly, could you tell us what they are?  3) What models have you used to estimate P(L) and P(D)? Why?  4) What data do you have access to? It sounds like you have samples from P(L) and samples from P(D), to which you have fitted a model. If so, how many samples from each?   5) What are you going to do with P(L,D) once you know it?",2013-10-16 11:41:13.200
104441,57601,12282.0,CC BY-SA 3.0,,"And why I would like to know: 1) To get an idea of how much missing information there is to infer, given you knowledge of the two conditional distributions. 2) Similar reasons to 1.  3) If you have good reason to believe one model's estimates are closer to the 'true' distribution, you should favour using it .  4) If you have more samples of L than D, for example, estimates of P(L) may be more accurate. 5) Depending on the intended application, it may be best to keep *both* estimates of P(L,D).",2013-10-16 11:49:35.207
104442,57595,1895.0,CC BY-SA 3.0,,"Some intuition: In going from $Y_k$ to $Y_k^2$, do you lose any information? If so, about what? Now, consider the example of $X_n$ being a simple random walk. Is it a martingale with respect to $\sigma(X_0,\ldots,X_{n-1})$? What about $\sigma(X_0^2,\ldots,X_{n-1}^2)$?",2013-10-16 12:07:53.623
104443,57607,22752.0,CC BY-SA 3.0,,Thank you for your help. That was exactly what I was looking for. Could you please explain to me why you could treat $\beta$ as a constant (I'm referring to the second equality of the last block of derivations)?,2013-10-16 12:22:19.957
104444,57607,22143.0,CC BY-SA 3.0,,"$V(const + W) = Var(W)$ is a property of variance operator. Basically, constant terms do not appear since $V(const + W) = E[(const + w - E[const + W])^2] = E[(W - E[W])^2] = V(W)$.",2013-10-16 12:38:52.347
104445,57601,22750.0,CC BY-SA 3.0,,"1) What kind of variables are L and D? Continuous? Categorical? What values can they take? 

They are continuous probabilities, so can take values between 0 amd 1?

2) Given know P(L|D) and P(D|L) exactly, could you tell us what they are? 

P(L|D) = 0.385, P(D|L) = 0.735

3) What models have you used to estimate P(L) and P(D)? Why?",2013-10-16 12:48:13.370
104446,57601,22750.0,CC BY-SA 3.0,,"I have two linear regression models to estimate each value. I want to estimate whether things and L and D are the same time. I could build a third model but wondered if I could use the two I have.

4) What data do you have access to? It sounds like you have samples from P(L) and samples from P(D), to which you have fitted a model. If so, how many samples from each? 

I have the same set of samples for each model. That's the historical data to calculate the regression values.",2013-10-16 12:48:51.873
104447,57601,22750.0,CC BY-SA 3.0,,"5) What are you going to do with P(L,D) once you know it? –  Pat 52 mins ago
	 
I am going to select those with the highest probablity from a current set of data where neither L or D are known. I want to select the things which are most likely to be L and D at the same time.

I am inclined to keep both estimates and average them.",2013-10-16 12:49:49.363
104448,57607,22752.0,CC BY-SA 3.0,,"@Theja Thanks but that was actually not what I meant. I know about that property, but what I tried to ask is why $\beta$ *can be seen as a constant*. Isn't it some kind of random variable?",2013-10-16 12:50:38.133
104449,57607,22752.0,CC BY-SA 3.0,,"Oh wait, I think I am confusing $\beta$ with $\hat{\beta}$ which *is* a random variable. Never mind then :).",2013-10-16 12:55:07.817
104450,57607,22143.0,CC BY-SA 3.0,,$\beta$ is the true unknown model which is *assumed* to be fixed and not random (see point 1. in the answer above).,2013-10-16 12:55:28.257
104451,57607,17573.0,CC BY-SA 3.0,,"@rbm Well, if you are a Bayesian, then you are going to see that step as a mistake (I think---but ask a Bayesian to be sure).  If you are a Frequentist, parameters are always constants.",2013-10-16 13:12:00.577
104452,57611,503.0,CC BY-SA 3.0,,Seems sensible to me,2013-10-16 13:21:41.877
104454,57448,16644.0,CC BY-SA 3.0,,"Say it this way: $\epsilon$ is the half-width of the confidence interval. Since your empirical cdf will be within $\epsilon$ with probability $1 - \alpha,$ the confidence interval has width $2 \epsilon.$ Here is another way to look at it: If $G(c)$ is your empirical cdf at $c,$ the confidence interval is $[G(c) - \epsilon, G(c) + \epsilon].$",2013-10-16 13:34:42.047
104455,57420,306.0,CC BY-SA 3.0,,i have said that different cases are to be handled separately and this is just one particular case. and different normally distributed random variables having different parameters of normal distribution are said to be of different distributions if that is the confusion here.,2013-10-16 13:39:51.890
104456,57564,1411.0,CC BY-SA 3.0,,"oops.  Reading this more carefully I see that it isn't a `glmer` issue at all.  I will say that at least the development version of `lme4` gives an error `Response is constant - cannot fit the model` , which at least gives a clue ...",2013-10-16 13:43:02.467
104457,57600,1790.0,CC BY-SA 3.0,,"Thanks Dikran. When you say `""average over the parameter values""` I think understand how to do this through an ensemble method (e.g. building the ensemble output as the average of the the classifier outputs), but I am not sure how do this with a Bayesian approach when working with a discriminative model. I understand the theory of a fully Bayesian approach (i.e. avoid point estimates, and marginalize out the parameters to build the final posterior), but, assuming that my prior on the parameters is uniform, wouldn't this be equivalent to building the averaging ensemble?",2013-10-16 13:54:55.483
104458,57458,22684.0,CC BY-SA 3.0,,"Thanks a lot. The label of the first line indicates the name of gene, but I do not know what the numbers mean either.",2013-10-16 14:03:24.647
104459,57446,4320.0,CC BY-SA 3.0,,"@Berkan I don't think either issue you mention (more no event sequences than event sequences and just noise for no event) should rule out the 2 HMM approach. If you took the prior $P(HMM1)$ into account (I've update my original answer in this regard) then you may need to adjust for the unbalanced class distribution (more no events than events), but there are lots of ways to deal with this. See [this](http://stats.stackexchange.com/questions/64163/how-to-deal-with-low-frequency-examples-in-classification/64165#64165) answer I gave for example.",2013-10-16 14:03:47.033
104460,57458,22684.0,CC BY-SA 3.0,,"This is from a online course material and I did not take this course. Anyway, I may try to email that professor to seek more information. Thanks",2013-10-16 14:05:01.767
104461,57601,12282.0,CC BY-SA 3.0,,"Ok. There's a few things I don't understand in your answers. But one thing you said leapt out at me: ""I have the same set of samples for each model. That's the historical data to calculate the regression values. "" So, does this mean you have paired data? As in, your dataset has samples $(l_1,d_1), (l_2,d_2), \ldots$ where each pair $(l_i, d_i)$ tells you the value of L and D at the same time? If so, and if you're able to, I strongly advise building a third model using these pairs to directly find out what you want to know. It's almost certainly the best way to estimate P(L,D) from your data.",2013-10-16 14:11:50.133
104462,57446,4320.0,CC BY-SA 3.0,,"@Berkan As for the window size, based on my own personal experience I expect what I've said in this matter will hold for any **fixed** window size. Obviously all of the things I've said will need to be tested empirically for your particular problem.",2013-10-16 14:14:55.540
104464,57577,14888.0,CC BY-SA 3.0,,"What I don't understand yet is this: In the paired t-test, it is an outcome variable that is ""paired"" or ""correlated."" In the case I described, it is the outcome variable that is correlated.   In RM ANOVA, the outcome variable is predicted by independent variables, which are correlated. That is a different situation, no?",2013-10-16 14:15:20.923
104465,57577,14888.0,CC BY-SA 3.0,,"I do see the parallel between the paired t-test and RM ANOVA, that error variance is minimized. It is that aspect that generalizes. However, they differ in that paired t-test focuses on the differences between an outcome variable, whereas RM ANOVA focuses on repeated measures of predictor variables.",2013-10-16 14:15:51.223
104466,57613,9716.0,CC BY-SA 3.0,,check out the *forecast* package - it's great.,2013-10-16 14:17:17.500
104467,57446,20470.0,CC BY-SA 3.0,,"thanks for updating your answer, it is a lot clearer now. Since I will be working with logarithms, I will be making the comparison: $log(P(HMM1))+log(P(O|HMM1)) >?   log(P(HMM2))+log(P(O|HMM2)) $. Now, $log(P(HMM1))$ is calculated using the forward algorithm, how do I calculate  $log(P(HMM1))$? Is ti just a prior that I appoint?",2013-10-16 14:17:52.563
104468,57577,503.0,CC BY-SA 3.0,,RM ANOVA can certainly be used when the DV is repeated. It generalizes the paired t because it is about more than 2 repeats.  In the paired t-test the difference in the two outcomes is predicted by a single two-level categorical variable.,2013-10-16 14:18:11.960
104469,57577,14888.0,CC BY-SA 3.0,,"The RM ANOVA method that I am familiar with (based on Andy Field's Statistics book for R) only ""works"" when the IVs are repeated and the outcome is a single measure. So, for my needs, I did the following: For each individual, I subtracted the two DV measures, then ran a two-way ANOVA using the DV difference as the outcome. 1) Is that valid? 2) Is that still a RM ANOVA?  I think the answers to those questions will clear things up for me.",2013-10-16 14:26:08.950
104470,57575,19359.0,CC BY-SA 3.0,,"Thanks for the advice, that's very helpful!  And I appreciate your suggestion for double-checking the null deviance estimates.  As for the degrees of freedom, that is not a typo-- although there are 7 (probabilistically) independent variables, the rank of the matrix of observations is only 5 (i.e. the observations are linearly dependent).  Thanks!",2013-10-16 14:29:10.437
104471,57599,18296.0,CC BY-SA 3.0,,It seems that it is not a simple question. I am surprised that this Topic is not treated or explicitly treated in the books.,2013-10-16 14:38:13.710
104472,57617,20062.0,CC BY-SA 3.0,,"Boxplot for each group with brackets above looks great, and you also display variability of data...(or instead of brackets you can use notches). If you have basic skills with ""R"" I can provide you working code.",2013-10-16 14:57:07.793
104473,57544,20740.0,CC BY-SA 3.0,,Interesting hadn't thought of that with the variance.  New customers have a higher variance so more news means not only worse performance but higher variance and it is definitely heteroskedastic...,2013-10-16 15:02:24.943
104474,57617,22399.0,CC BY-SA 3.0,,Two issues with a boxplot. (a) It can be a challenge to interpret the plot for a non-technical audience (especially for someone who has never seen a boxplot) (b) it does not scale well if I have lots of such mulltiple-comparisons test. Imagine doing the above for 20 such tests in which case a table with 20 rows with suitable emphasis/visualization seems compact relative to 20 boxplots.,2013-10-16 15:02:31.173
104476,57617,7700.0,CC BY-SA 3.0,,"Two questions: 1-What exactly do you want to show, basic data, the multiple comparisons, the comparison's differences, all of the above? 2-What is the visualizations purpose and audience? Data exploration for you or explanation for a non-tech audience (if both you probably need two viz's).  Also, you mention that the mean for D is different than A & B, what about C?",2013-10-16 15:12:04.253
104477,57577,503.0,CC BY-SA 3.0,,"1) Yes, even if not ideal for reasons in my first answer. 2) I don't think so, but terminology isn't key.",2013-10-16 15:16:18.820
104478,57617,22399.0,CC BY-SA 3.0,,1. Right now my goal is to show the means and just draw the attention to the ones that are different. 2. The audience is not statistically aware and showing them a table of means with an emphasis on the ones that are different seems to be the right approach to me.,2013-10-16 15:19:04.303
104479,57446,20470.0,CC BY-SA 3.0,,"thanks for updating your answer, it is a lot clearer now. Since I will be working with logarithms, I will be making the comparison: $log(P(HMM1))+log(P(O|HMM1))>?log(P(HMM2))+log(P(O|HMM2))$. Now, $log(P(HMM1))$ is calculated using the forward algorithm. Do I calculate log(P(HMM1)) using simple MLE based on frequencies? i.e. for the given case, $HMM1 = (5 * 2,000) / 108,000$ where the numerator is the number of points that fall under HMM1 and denominators is the size of the data set.",2013-10-16 15:23:39.693
104480,57615,22369.0,CC BY-SA 3.0,,"We've actually counteracted the issue of weeks not lining up by munging the weeks to line up. For example, this year there would actually be an additional week so the first week's information includes 8 weekdays instead of 5. It's not a perfect system but the holiday weeks should match up, the final week should roughly correspond every year, and so on.

What I'm interested in is, is there a good method to build a model based on the trends from previous years to apply to this year's data?",2013-10-16 15:28:35.953
104481,57617,7700.0,CC BY-SA 3.0,,"How many points do you have? With just 4, it's going to be difficult to show why D is different but C is not. For this, I'd almost do a simple dot-plot with a different symbol for D since it is statistically different.",2013-10-16 15:29:13.217
104482,57586,1693.0,CC BY-SA 3.0,,"I'm not the swiftest; are you arguing that with large samples there is never a sound basis for expecting a sign to reverse, theory or no theory?",2013-10-16 15:31:45.530
104483,57623,22262.0,CC BY-SA 3.0,,"This is very useful, but not quite what I'm after. Up voted anyway.",2013-10-16 15:38:09.437
104484,57611,21762.0,CC BY-SA 3.0,,You are assuming that the hazard ratio associated with treatment does not substantially depend on the propensity score (i.e. on the variables used for matching).,2013-10-16 15:38:11.190
104485,57617,20062.0,CC BY-SA 3.0,,"Maybe you can try to make a little explanation how to look at it. It is not necessary to be a ""rocket scientist"" to understand boxplot. Do not pamper you audience :)",2013-10-16 15:38:12.277
104486,55609,1805.0,CC BY-SA 3.0,,"In excel, your best bet is probably to assume that sales tomorrow will be the same as sales today.  This is called the ""naive"" forecast, and is usually a great place to start.",2013-10-16 16:09:11.677
104488,57586,20473.0,CC BY-SA 3.0,,"Yes, this is the result that emerges (for the particular model specification examined) - I don't know whether it is a known result, it was new to me -but it is correct. But you shouldn't be surprised: each specific model has a ""technical identity"" that creates certain rigidities that do not permit the model to accommodate all possible aspects of a real phenomenon (such as a sign reversal in your case).",2013-10-16 16:35:54.987
104489,57586,1693.0,CC BY-SA 3.0,,It sounds as if you are consigning statistical control to a  much more limited role than many people believe it can play.  And that as a consequence of your conclusion many authors (and contributors to this site) would have to completely revise what they've written about sign reversals in regression.,2013-10-16 16:41:09.857
104543,57662,15827.0,CC BY-SA 3.0,,"Stata commands relevant (here names are self-explanatory): `zip`, `zinb`, `nbreg`.",2013-10-16 22:51:41.157
104490,57570,668.0,CC BY-SA 3.0,,"@Dilip I see, thank you.  It is of note that you had to supply several assumptions that, although they may be obvious to a trained statistician, are certainly not explicit in the question.  I have instead attempted to take this question at face value--much as a neophyte would--and answer it on its merits (which are few indeed :-).",2013-10-16 16:46:41.163
104491,57586,20473.0,CC BY-SA 3.0,,"You are generalizing the result, while I don't. It is not I that I conclude - it is the mathematics of the model that lead to the conclusion, that holds for the exact specific assumptions of this particular model: a logit model. One Binary regressor of interest. No Constant Term. The case of dropping _all_ control variables at once. Perhaps if you change any one of these assumptions, the result may not hold (or perhaps it will).",2013-10-16 16:49:14.840
104492,57586,1693.0,CC BY-SA 3.0,,Thank you for your responses.  I'm puzzled by all of them :-),2013-10-16 16:55:25.273
104493,57572,22732.0,CC BY-SA 3.0,,"That's what I figured. I'm trying to piece together what the correlation structure should look like. It's not going to be autocorrelated as the within subject prediction is always the same, but what is it then... Thanks a lot for the feedback.",2013-10-16 16:59:17.430
104494,57504,668.0,CC BY-SA 3.0,,"Thanks for editing, but the same potential to be misleading still exists.  In the second bullet, despite all appearances, there really is only one effective parameter, equal to $a_1 \frac{a_2}{a_4}$ (because $a_2/a_2=1$ in the argument to the exponential makes $a_2$ disappear) and it enters *linearly* into the formula, not in a nonlinear fashion. The point is that linearity is a *mathematical* property of the function and not a mere *syntactic* property of how it is written down.",2013-10-16 17:18:27.220
104495,57626,5237.0,CC BY-SA 3.0,,"Welcome to the site, @Roman. Asking for R packages is off-topic for CV (see our [help page](http://stats.stackexchange.com/help)). Moreover, this Q would be off-topic on [Stack Overflow](http://stackoverflow.com/) as well. You might try the r-help listserv.",2013-10-16 17:34:04.223
104496,57626,5237.0,CC BY-SA 3.0,,This question appears to be off-topic because it is about asking for R packages.,2013-10-16 17:34:19.270
104497,57629,21762.0,CC BY-SA 3.0,,Do you know anything about the shape of the distribution of the $Y$-values?,2013-10-16 17:34:31.560
104498,57629,21864.0,CC BY-SA 3.0,,"actually, I don't have this luxury. The application should work for any distribution.",2013-10-16 17:36:15.240
104499,57632,16046.0,CC BY-SA 3.0,,Can I know a source that I could see how this formulas are achieved?,2013-10-16 17:38:06.123
104500,57629,21762.0,CC BY-SA 3.0,,"Then you might go with Chebychev's inequality (the finite sample version) http://en.wikipedia.org/wiki/Chebyshev%27s_inequality It gives bounds for the probability to deviate more than $k$ standard deviations from the mean. If you know mean and standard deviation, you can derive the range based on this",2013-10-16 17:39:10.647
104501,57626,22762.0,CC BY-SA 3.0,,could I ask for the algorithm for panel VAR estimation?,2013-10-16 17:40:38.280
104502,57632,3580.0,CC BY-SA 3.0,,"@Naji the CRP is a member of the exponential family and the number of tables is the sufficient statistic. If $P$ is the number of tables and $\mathcal P$ is the partition, then $f(\mathcal P) = h(\mathcal P) \exp\left\{Pa - [\log \Gamma(e^a + N) - \log \Gamma(e^a)]\right\}$ where $a = \log \alpha$. It follows from the properties of exponential families that $E[P]$ and $\mbox{Var}(P)$ are related to the derivatives of the function $\log \Gamma(e^a + N) - \log \Gamma(e^a)$ with respect to $a$. Those are just the formulas that pop out when you do the calculation.",2013-10-16 17:47:37.833
104503,57626,5237.0,CC BY-SA 3.0,,"Sure, you can ask about how to deal w/ this situation, & in the process of answering someone might be able to provide some helpful R code (or not...). It's just asking 'what package will do X' that's off-topic. If you want the question to stay here (& stay open), just edit your Q to make it on-topic. It may help you to read the [relevant section of the help page](http://stats.stackexchange.com/help/on-topic) & our [guide to asking questions](http://meta.stats.stackexchange.com/questions/1479/how-to-ask-a-good-question-on-crossvalidated) in reformulating your Q.",2013-10-16 17:51:02.503
104504,57632,3580.0,CC BY-SA 3.0,,"@Naji (continued) For a source, [this paper](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&ved=0CDsQFjAB&url=http%3A%2F%2Fwww.fas.harvard.edu%2F~junliu%2FTechRept%2F96folder%2Fnonpbayes.pdf&ei=MtJeUvepCoP49QTH24G4BA&usg=AFQjCNEosiOI_kGroCnu_PbMMiEINc6XdQ&sig2=oN7LojuVmm7FdrrYyT6uvA&bvm=bv.54176721,d.eWU) has them, but no derivations as far as I know.",2013-10-16 17:52:30.277
104505,57611,20456.0,CC BY-SA 3.0,,"Actually, the HR may substantially depend on age: older people have more comorbidities and this could influence treatment they had been given. How should I proceed? I thought to add the variable age as a covariate in the Cox regression. Correct?",2013-10-16 18:01:12.467
104506,57608,20473.0,CC BY-SA 3.0,,"You may want to look up ""choice-based modeling"".",2013-10-16 18:04:49.757
104507,57633,2081.0,CC BY-SA 3.0,,[This](http://stats.stackexchange.com/a/36158/3277) might be of some relevance.,2013-10-16 18:09:23.460
104508,57446,4320.0,CC BY-SA 3.0,,"I think you mean you can calculate $P(O|HMM1)$ using the forward algorithm. If so, then yes $P(HMM1)$ would just be the number of sequences where the event you're trying to predict occurs divided by the total number of sequences. You should have $P(HMM1) + P(HMM2) = 1$ since you would be assuming each each sequence is either generated by HMM1 or HMM2.",2013-10-16 18:10:21.637
104509,57637,,CC BY-SA 3.0,user30490,I am not sure I follow how to do this.  Do you think you could just show the last step of what the likelihood should be?,2013-10-16 18:10:48.570
104510,57626,5237.0,CC BY-SA 3.0,,"I edited this in the hopes that it might lead to more productive answers for you. Please make sure it is still asking what you want to know & see if you like it. If not, click ""rollback"" to return it to your last edit with my apologies.",2013-10-16 18:18:14.827
104511,57586,20473.0,CC BY-SA 3.0,,"""Puzzled"" is the best start to learn and really understand anything - at least from my personal experience!",2013-10-16 18:26:51.197
104512,57570,4656.0,CC BY-SA 3.0,,"Alas, I am not a trained statistician, and yes, I had to supply several assumptions based on the `Hint: for example. a Bernoulli random variable can be used to model a coin with probability of success` $p \in ]0,1[$, that is in the problem statement. To me, the natural extension was to a geometric random variable. I agree with you about the merits of the question; indeed, even the notation is quite dreadful. $\pi_0$ and $\pi_1$ as the prior probabilities of two hypotheses is common usage, but just $\pi$? with all the confusion likely to ensue when normal distributions are encountered?",2013-10-16 18:30:04.127
104513,57626,5237.0,CC BY-SA 3.0,,"I don't know the answer to your question, but I think you may be able to get helpful responses now. GL",2013-10-16 18:30:14.360
104514,57636,20062.0,CC BY-SA 3.0,,don't you want ...to determine which VARIABLE give the greatest segregation between the yes and no samples? (instead of correlation),2013-10-16 18:34:58.897
104515,57636,20062.0,CC BY-SA 3.0,,Data frame with 98 observations and 107 variables sounds like simple table (no correlation matrix). Please clarify!,2013-10-16 18:37:26.767
104516,57632,16046.0,CC BY-SA 3.0,,what is $f$ exactly? If it is the pdf I don't understand how can its domain be a vector of variable size of random variables.,2013-10-16 18:41:39.093
104517,57640,668.0,CC BY-SA 3.0,,"You seem to state that you know the vectors $(a_k)$ and $(b_k)$ up to permutation. (To maximize their inner product you put both sequences in ascending order; to minimize their inner product you put one in ascending order and the other in descending order.) But I'm not sure that's what you're saying, because if you *do* have this kind of information then you can easily compute both the means and the SDs, yet you specifically remark that you have the means and yet cannot find the SDs.  Could you please edit the question to clarify exactly what information you really have?",2013-10-16 18:57:24.100
104518,57632,3580.0,CC BY-SA 3.0,,"@Naji it is the pmf of $\mathcal P$, i.e. a mass function on the space of partitions. The domain is the set of **partitions of** $\{1, 2, ..., N\}$. In probability-speak, $\mathcal P$ is a random element (rather than a random variable or vector).",2013-10-16 19:02:31.827
104519,57633,16746.0,CC BY-SA 3.0,,Thanks! Still it is not clear to me why Euclidean distance can not be used as similarity measure instead of cosine of angle between two vectors and vice versa?,2013-10-16 19:08:03.713
104520,57641,21243.0,CC BY-SA 3.0,,Would you mind linking to the paper? It might help us better understand the context.,2013-10-16 19:12:31.880
104521,57640,22767.0,CC BY-SA 3.0,,"I think I can not put the sequences ascending or descending, because although I know for instance 80% of $a_k$ have the same vale of $a_1$, still I don't know the value of $a_1$. Do you think I can calculate SD with this condition?",2013-10-16 19:28:19.770
104522,57578,16144.0,CC BY-SA 3.0,,@Glen_b How do you orthogonalize these predictors?,2013-10-16 19:58:55.343
104523,57616,5448.0,CC BY-SA 3.0,,"Do you want to be guided on a journey to figure out the answer or would you prefer to just be given the answer, along with an explanation of why it's the answer?",2013-10-16 20:05:48.023
104524,57640,668.0,CC BY-SA 3.0,,"Given that you do not know the values of the $a_k$ nor of the $b_k,$ you will have to find extreme values that are consistent with what you do know. Your information includes the mean and the fact that the $a_k$ are non-negative, but you haven't given us any information about the possible values of the $b_k$ nor about the possible upper bounds on the $a_k$: both of these are essential for bounding the inner product.",2013-10-16 20:39:20.010
104525,57644,668.0,CC BY-SA 3.0,,"Your distribution does not look Normal: it is clearly left-skewed.  Without information about the details of the calculations giving the results, it really is not possible to answer this question.",2013-10-16 20:47:24.663
104526,57653,,CC BY-SA 3.0,,"Is TP true positive, FP false positive, Tn true negative and FN false negative? If so please edit your question, abbreviations are typically not uniquely defined.",2013-10-16 20:55:19.263
104527,57596,668.0,CC BY-SA 3.0,,"Have you noted the nice symmetry of this distribution?  Because the height at $20-y$ is the same as the height at $20 + 2y$ for $0 \le y \le 20,$ then for small $dy$ the number of families with incomes between $20-y$ and $20-(y+dy)$ for $0\lt y\le 20$ is one-half the number of families with incomes between $20 + 2y$ and $20 + 2y + 2 dy.$  Thus the contribution from each of the latter families, which is proportional to $2 y t,$ must exactly balance the transfer to the former families, who receive an amount proportional to $y/2.$  That makes it obvious what $t$ should be.",2013-10-16 20:56:47.213
104528,57655,449.0,CC BY-SA 3.0,,"In addition, given the covariates are covarying with X it's best that's it's the response variable and not a predictor.",2013-10-16 20:58:12.440
104529,57656,5448.0,CC BY-SA 3.0,,Do you know the functional form of $f$?,2013-10-16 21:00:35.193
104530,57595,22746.0,CC BY-SA 3.0,,"Intuitively yes I would say you lose information. $g (x) = x^2$ is not one-to-one (ie it has 2 values on the domain for each range element). We know that if we are given $Y_0 = k_0, \ldots, Y_n= k_n$, we can get X_n. However, give $Y_0^2 = m_0, \ldots, Y_n^2= m_n$, we cannot derive $Y_0$ as it is not one-to-one. So therefore we cannot get $X_n$. I think this is the answer to the first part.  Thanks for the tip.",2013-10-16 21:05:36.253
104531,57616,22756.0,CC BY-SA 3.0,,"A journey sounds nice. This isn't for a class and the answer is given at the end of the question. I don't care to just know the answer - I already know it!

I've taken a stats course many years ago, but I didn't appreciate it enough then. I'm trying to remedy that now and really begin to understand the underlying patterns.

I'd appreciate the help. This particular problem doesn't seem to fit with the others from this section and a proper approach isn't clearly demonstrated (to me) from the text's information on the binomial distribution nor its examples given.",2013-10-16 21:11:19.340
104532,57635,21864.0,CC BY-SA 3.0,,"In my case, I cannot assume that my distribution is normal or is not, and the outlier detection should work for all cases. In fact, I only have an idea about what data set to run the outlier detection on just after the user interaction. The variance test was the most general idea that doesn't require me to have previous knowledge of the distribution of my data. What I understand from your explanation is that I should replace comparing the distance with comparing the statistic, is that what you mean? and what is ""med"" and ""mad""",2013-10-16 21:19:47.480
104534,57662,5237.0,CC BY-SA 3.0,,"If you think there are too many zeros, you may want to run a zero-inflated Poisson (zip) model, instead of negative binomial, which is more for overdispersion.",2013-10-16 21:47:19.693
104535,57644,12140.0,CC BY-SA 3.0,,"Right, the distribution doesn't look exactly like Normal. Perhaps, the number of samples (100) is not sufficient. It takes several hours to obtain a single sample. But I certainly see the trend - the more samples, the more bell-shaped the distribution becomes.",2013-10-16 22:17:38.957
104536,57664,16043.0,CC BY-SA 3.0,,"Why Beta(1/2, 1/2)? This is a bimodal distribution with the weight concentrated around 0 and 1. OP, and future readers, will be interested in explanation about why this is the prior, rather than simply a prior among many alternatives. Some coins could be 1 with probability 0.75, so a prior of Beta(3,1) could represent that... or Beta(30,10)...",2013-10-16 22:24:38.273
104537,57664,633.0,CC BY-SA 3.0,,"@user777: The distribution is over the coin's probability $p$ — it is not binomial, but Beta.  OP already said he was familiar with the Jeffreys' prior, but I'll mention it as you suggest.  I did say that ""assuming you know nothing to start"", so the Jeffreys' prior is IMO the most reasonable choice.",2013-10-16 22:26:31.577
104538,57662,503.0,CC BY-SA 3.0,,"More precisely, `SPSS` is worrying about enough to not let the model run, while 'Stata` is telling the user it's a problem but letting the user deal with that.  Also, as @gung points out, for too many zeroes, a ZIP model seems more apropos; for overdispersion, negative binomial; for both - ZINB.",2013-10-16 22:31:57.880
104539,57659,594.0,CC BY-SA 3.0,,When you say there are many zeroes - are there more than would be expected for a Poisson model? Or is it just that the Poisson mean is quite small?,2013-10-16 22:37:08.863
104540,57653,503.0,CC BY-SA 3.0,,"Any measure that did *not* give your classifier a 0 would be highly suspect. Your classifier is not doing anything. In a sense, no estimation of your classifier is needed other than the fact that it is not predicting any positives.",2013-10-16 22:38:26.050
104541,57659,15827.0,CC BY-SA 3.0,,Just adding 0.5 strikes me as a fudge at best; do you have literature or theoretical support for that?,2013-10-16 22:45:27.210
104545,57644,668.0,CC BY-SA 3.0,,You already have enough samples to demonstrate a significant departure from normality.  It is unlikely additional samples will change the shape of the distribution appreciably.,2013-10-16 22:57:56.263
104546,57656,16039.0,CC BY-SA 3.0,,"Well.. I don't know for sure, but I was planning on fitting either a linear or quadratic function of the fitted values.",2013-10-16 23:02:17.213
104547,57665,503.0,CC BY-SA 3.0,,It is a good idea to spell out acronyms and abbreviations.,2013-10-16 23:09:59.053
104548,57668,4656.0,CC BY-SA 3.0,,What does $Y|X$ mean? There is a _conditional_ distribution of $Y$ given the value of $X$ that uses notation like $f_{Y|X}(y|x)$ but $Y|X$ is not a random variable.,2013-10-16 23:20:27.863
104549,57668,9175.0,CC BY-SA 3.0,,I am sorry that was supposed to mean $f_{Y|X}(y|x)$,2013-10-16 23:25:03.473
104550,57659,5045.0,CC BY-SA 3.0,,Some [useful references](http://stats.stackexchange.com/a/38588/7071) on the why.,2013-10-16 23:32:43.723
104551,57661,15827.0,CC BY-SA 3.0,,"Is this mainly about what makes sense statistically or do you seek Stata support? If the latter, it is arguably off-topic here (and would not be well received on SO, but you do not show a programming problem). In either case, please explain ADL and please don't assume that abbreviations used in your field make sense to all readers.",2013-10-16 23:33:10.800
104552,57664,633.0,CC BY-SA 3.0,,"Added the proof for the likelihood.  For the proof of the prior, you'll have to read Jeffreys' paper.",2013-10-17 00:01:49.237
104553,57669,594.0,CC BY-SA 3.0,,In what context does this arise?,2013-10-17 00:03:17.307
104554,57665,19325.0,CC BY-SA 3.0,,"@PeterFlom Thanks for the suggestion, I've added the full spelling of some terms.",2013-10-17 00:06:17.080
104555,57653,5203.0,CC BY-SA 3.0,,"@PeterFlom, well said! I think the OP got hung up on precision/recall definition of F measure, which gives you an undefined (0/0) answer.",2013-10-17 00:06:21.953
104556,57669,1145.0,CC BY-SA 3.0,,I'm playing around with the idea an index of 'stability' or 'solidity' of experimental evidence that might be based on how susceptible a result is to change upon the addition of an extra datum.,2013-10-17 00:11:57.130
104557,57671,1145.0,CC BY-SA 3.0,,"Thank you very much. I can now screw up the many pages of scribbles that I have made. (Some bits are similar to yours, but certainly not all of them!)",2013-10-17 00:13:20.247
104558,57644,12140.0,CC BY-SA 3.0,,I'm less concerned about getting exact Normal distribution. What's puzzling is general bell-like shapes that I'm getting each and every time. And its coming from a process that is unlike any random distribution. So the question is really if I can disprove that the process obeys CLT.,2013-10-17 00:16:20.033
104559,57671,503.0,CC BY-SA 3.0,,Wow. That is a lot of LaTeX!  :-),2013-10-17 00:31:49.720
104560,57668,10684.0,CC BY-SA 3.0,,"I think the OP probably means ""Prove that $Y/X$ and $X$ are independent"" rather than ""Prove that $Y|X$ and $X$ are independent.""",2013-10-17 00:43:33.723
104561,57480,2121.0,CC BY-SA 3.0,,I think that would be appropriate - you would be essentially adjusting your weighting scheme so you fall into situation #1.,2013-10-17 01:12:18.440
104562,57673,449.0,CC BY-SA 3.0,,"What have you done to try to address the problem? What you're asking implies you don't know what *t*, *p*, or the test are. It's hard to tell exactly where you're confused. Please expand your question.",2013-10-17 01:28:22.170
104563,57662,5045.0,CC BY-SA 3.0,,Here's a [simulation paper](http://personal.lse.ac.uk/TENREYRO/ppml-fsr.pdf) where the Poisson quasi-MLE does very well when there are lots of zeros.,2013-10-17 01:50:10.727
104564,57669,594.0,CC BY-SA 3.0,,Sounds a bit like an influence function or an empirical influence function,2013-10-17 01:50:56.050
104565,57676,594.0,CC BY-SA 3.0,,"The effort involved in your approach is far greater (and unless special care is taken, it's more likely to suffer numerical stability issues).",2013-10-17 02:50:06.297
104566,57674,594.0,CC BY-SA 3.0,,"+1 Good answer, though I'd add the minor point that in both cases, *equality* corresponds to rejection as well -- and while it's not an issue with continuously-distributed test statistics, it matters when they're discrete.",2013-10-17 02:51:44.447
104567,57679,,CC BY-SA 3.0,user30490,But $v\sim\text{Inverse-Gamma}$?  I have $$v^{-p/}=v^{-p/2+1-1}=v^{-(p/2-1)}-1$$,2013-10-17 03:39:01.900
104568,57685,22507.0,CC BY-SA 3.0,,The problem you discuss (representing 4d points as 2d points) is called **dimensionality reduction**.  Google or read wikipedia articles about the topic.,2013-10-17 04:25:04.057
104570,57359,594.0,CC BY-SA 3.0,,Details included. Sorry it took some days.,2013-10-17 06:05:43.990
104571,57672,17328.0,CC BY-SA 3.0,,"There are many competing ways of expressing Inverse distributions. Accordingly, if you fail to provide the functional forms you are using, there is nothing 'clear' about the above. The definition I use is that if $X$~$Gamma(a,b)$ with pdf $$f(x) = \frac{x^{a-1} e^{-\frac{x}{b}}}{b^a \Gamma (a)}$$ then $1/X$ ~ $InverseGamma(a,b)$.",2013-10-17 06:06:43.790
104572,57578,594.0,CC BY-SA 3.0,,Outline discussion [here](http://stats.stackexchange.com/questions/72626/how-to-include-a-linear-and-quadratic-term-when-also-including-interaction-with/73042#73042),2013-10-17 06:08:16.740
104574,57688,594.0,CC BY-SA 3.0,,"One explanation you don't seem to have ruled out is random variation. If you observed a new data set for each, might your decisions have gone the other way? You might like to consider cross-validation",2013-10-17 07:16:10.963
104575,30862,,CC BY-SA 3.0,,"They usually have sparse representations - you don't need to store $mn$ numbers for a low rank approximation.  For example, a rank 1 approximation requires $n+m$ numbers.",2013-10-17 07:26:48.173
104576,57647,22570.0,CC BY-SA 3.0,,"I think my problem is rather clear - I have the measured distribution of $\bf{v}$ and $\bf{a}$ and from this I'd like to sample a pseudo-random $\bf{v_{rand}}$, that ultimately reproduces the input. I'm well aware of your point on whether what comes out of it is realistic, but that's a different question...",2013-10-17 07:50:10.537
104577,57635,450.0,CC BY-SA 3.0,,"med is median and mad is the median absolute deviation. Even then, you will find the the quantile of the normal are tilted to the conservative side (for symmetric distributions)",2013-10-17 07:53:15.917
104578,57647,22555.0,CC BY-SA 3.0,,"At the very least, as indicated in the equation above, this would not be a stationary effect.  I would think that a first step would be to bin the readings according to time interval and then compare them.  I don't know how many readings you have but this comparison could be run through something like [Pearson's Distribution](http://stats.stackexchange.com/a/72434/31323) as a starting point - to try to classify the nature of the distribution.",2013-10-17 08:09:13.697
104579,57446,20470.0,CC BY-SA 3.0,,"Yes, thanks I will give it a try and see how it goes. *P.S:* I calculate $log(P(O|HMM1))$ since I use scaling factors and $P(O|HMM1)$ is out of the dynamic range of the machine.",2013-10-17 08:13:42.407
104580,57389,5001.0,CC BY-SA 3.0,,"Informed by the above, it becomes: Under the assumption that the true value of the y-intercept is zero, random sampling of the same number of (x,y) pairs, specifically 90, produced by the same process, would result in a least squares best fit line with a y-intercept as extreme as or more extreme than +0.00087, with an expectation of 27 times out of 10000 trials, and equal to or greater than +0.00087, with an expectation of 135 times out of 100000 trials.",2013-10-17 08:25:31.893
104581,57378,5001.0,CC BY-SA 3.0,,"I think there's an overlooked distinction between the most up voted answer in the purported duplicate, and the answer sought here.  The ""dup"" provides an understanding of the p-value, whereas this question seeks an answer that is an example of precise wording of the interpretation of the p-value.  A concise answer, such as that in the comments here might be valuable to some practitioners.",2013-10-17 08:35:34.800
104582,57693,8719.0,CC BY-SA 3.0,,Thank you for your answer! I was surprised to find out that I did a kind-of power analysis without knowing. :-) Statistics is more intuitive than I thought.,2013-10-17 08:43:36.630
104583,57693,4910.0,CC BY-SA 3.0,,"I would say it is often intuitive, but unfortunately hidden by a thick coating of arithmetics and mathematical notation... :)",2013-10-17 08:46:10.767
104584,57601,22750.0,CC BY-SA 3.0,,"Thanks. It's not quite that simple. I do have l and d pairs as you say, but they are derived from other variables which are integers (I am using a probit model). So l is 1 if g is < 3 (i.e. g=[0,1,2]) otherwise 0, and d is 1 if f=a, otherwise 0). But there are 4 possible combinations of l and g so I could probably do a probit model based on that new variable.",2013-10-17 08:48:18.917
104585,57550,12683.0,CC BY-SA 3.0,,"What I mean is that ideally you'd pick firms from country A at random until you have enough doing business in country B for the NS group in your sample. If you were to start, say, looking at larger firms first then it should be no surprise to find when you compare NS with S that larger firms appear more likely not to have a subsidiary in country B. If you can't do a random sample for some reason then the compellingness of your conclusions is negatively correlated to the plausibility of such selection biases.",2013-10-17 08:52:55.540
104586,57692,20062.0,CC BY-SA 3.0,,There are plenty of sources how to do simple correlation of two columns if you just google it...show some research effort,2013-10-17 09:00:20.837
104587,57692,22784.0,CC BY-SA 3.0,,Correlation would be best in my case ? What about T-student ?,2013-10-17 09:02:34.207
104588,57507,4910.0,CC BY-SA 3.0,,"Well, the interpretation would rather be that a 95% credibility intervals includes with 95% probability the true parameter (given the model assuptions, of course...). I would say that the 95% credibility interval is extremely useful information, and often the reported end result of a Bayesian analysis (If you do not choose to look at the complete posterior distribution, that is).",2013-10-17 09:07:58.597
104589,57692,20062.0,CC BY-SA 3.0,,If you mean the student's t.test it compares the means of two groups to look for possible difference. Not your case.,2013-10-17 09:10:29.200
104590,57651,22762.0,CC BY-SA 3.0,,thank you @fredrikhs for your comments. actually {vars} is good for time-series. how to use this package for the purpose of panels? direct applying doesn't work...,2013-10-17 09:21:39.090
104591,57698,12282.0,CC BY-SA 3.0,,"Both methods will give exactly the same answer, I'm afraid.",2013-10-17 10:07:43.710
104592,57640,22767.0,CC BY-SA 3.0,,"My experimental data are sparse, but I might be able to find the standard deviation from the error bars of the experiments. So, I will change the question to see how this expression could be calculated knowing also the SDs. The reply to that would help me too.",2013-10-17 10:08:25.633
104593,57698,8386.0,CC BY-SA 3.0,,"With this sort of question it is important to consider whether balls are replaced after drawing. Would you a) replace each ball before drawing the next one, or b) for approach 2, replace the balls after each sample of 50, or c) not replace at all?",2013-10-17 10:11:54.710
104594,57698,22788.0,CC BY-SA 3.0,,"I ran an experiment with both approaches. In approach 2, each sample of 50 is drawn without replacement. But I draw the sample of the next 50 with replacement. Like Pat mentions I found that both estimations yield similar results. My follow up question is will it make more sense to draw the sample of 50 with replacement?",2013-10-17 10:17:37.747
104595,57696,503.0,CC BY-SA 3.0,,"Given that you have 5 dichotomous variables, I am not sure what graphs you created. Can you tell us? Also, even though it's a 5 way interaction, it's only $2^5 = 32$ combinations, so you could look at the predicted value for every combination without it being too overwhelming.",2013-10-17 10:17:51.880
104596,57662,22775.0,CC BY-SA 3.0,,"Thank you all, I'm running the regressions for the two genders separately. In males the frequency of zeros is 11%, in the females 35%. Adding .5 was suggested to me because when I run the model with up to 3-way interactions in the se s of the estimates for the 2-way interaction terms are larger than the se s of the respective 2-way interaction terms when the model is run with only up to 2-way interactions in. I thought that was to be expected, but to the person who pointed it out it seemed odd because it happens when the higher-order interaction term is significant but also when it is not.",2013-10-17 10:19:30.107
104597,57698,12282.0,CC BY-SA 3.0,,"Ohh, I hadn't thought about sampling with/without replacement. I just assumed the former. Disregard my earlier comment.",2013-10-17 10:24:52.100
104598,57698,,CC BY-SA 3.0,,"@ryk I edited your question for consistent notation and terminology, please check if it is still what you wanted to ask (or roll back with my apologies).",2013-10-17 10:39:06.413
104599,57699,20473.0,CC BY-SA 3.0,,"First you need to carefully define _what the ""effectiveness"" metric will be_.",2013-10-17 10:40:10.783
104600,57699,,CC BY-SA 3.0,,What is 80' and 90'? Does this stand for minutes?,2013-10-17 10:40:32.033
104601,57698,12282.0,CC BY-SA 3.0,,"So, at the risk of getting things wrong again, my gut feeling is you should use method 1. My reasoning is if we imagined for a moment that there were only 500 balls in the jar, method 1 is **guaranteed** to get you the exact proportion, while method 2 has a non-zero chance of giving a wrong result. If we increase the number of balls then the maximum error of method 1 will increase (e.g. with 501 balls it might be wrong by $\pm$ 1/501), but unless there's some variance improvement inherent to method 2 I'm not seeing, I suspect it will still retain a slight advantage.",2013-10-17 10:49:31.730
104602,57696,22787.0,CC BY-SA 3.0,,Thank you for your response. The graphs that I've created were based on the est. marginal means. Not sure if that answers your question but the scores was ranged from 0-2. What do you mean by predicted values though?,2013-10-17 10:50:55.300
104603,57662,22775.0,CC BY-SA 3.0,,"I should add that this is preliminary data and we're exploring, plus we're not expecting a good fit as the predictors are 6 but we're only fitting up to 3-way interactions. Also, estat gof does not run after glm. I am not running Poisson regression as such (ie poisson), I am running a Poisson under a generalized linear model (ie glm).",2013-10-17 10:59:08.777
104604,57699,22790.0,CC BY-SA 3.0,,"@AlecosPapadopoulos, patients were followed for several years. I want to compare survival times.",2013-10-17 11:01:23.743
104605,57699,22790.0,CC BY-SA 3.0,,"@Momo, patients were operated in 1980's and 1990's.",2013-10-17 11:01:56.403
104606,57696,503.0,CC BY-SA 3.0,,The predicted value is the value that the ANOVA predicts for each combination of values. What software are you using?,2013-10-17 11:08:50.757
104608,57696,22787.0,CC BY-SA 3.0,,"Ah, got it. I'm using SPSS version 21",2013-10-17 11:29:47.450
104609,57696,503.0,CC BY-SA 3.0,,"Unfortunately, I don't know `SPSS`.",2013-10-17 11:43:35.723
104668,57729,14806.0,CC BY-SA 3.0,,"Sorry, I just assumed everyone would look at the other post. I should probably have posted it here anyway. Thanks for the reply.",2013-10-17 20:09:35.363
104610,57644,668.0,CC BY-SA 3.0,,"And the answer is yes, there are ways to disprove the process is exhibiting CLT-like behavior: apply a distribution test to the data you have already collected.  Getting ""general bell-like shapes"" is common and often has little to do with the CLT.  But, once again, please note that you have not supplied any of the information about your process that would be needed for readers here to give you objective, informed, or relevant advice.",2013-10-17 11:48:07.463
104611,57696,22787.0,CC BY-SA 3.0,,"No worries, thank you for your help though!",2013-10-17 11:49:51.010
104612,57616,20470.0,CC BY-SA 3.0,,I would be very interested in reading a detailed answer (with pointers to further reading where necessary) to this question.,2013-10-17 12:00:22.810
104613,57688,19681.0,CC BY-SA 3.0,,"Each of the link functions is an actual function whose value can be computed.  Try looking up the formula for each of your links (for example, the logit is clearly defined on wikipedia), and then graph each function to see its ""physical"" meaning.",2013-10-17 12:15:35.570
104614,57696,10060.0,CC BY-SA 3.0,,"To get the predicted value, in SPSS Mixed Model panel, after you have specified all the information, Click `Save` button, and check the box `Predicted values`. You should then obtain a new variable after the model is completed. Then you can proceed to see the 32 means, and compare them without treading into the mess of 4- or 5-way interaction terms.",2013-10-17 12:22:02.317
104615,57477,5643.0,CC BY-SA 3.0,,"The intuition I can't capture is when you say ""...the sum will always be too large"". I need to read through @whuber links, if you can elaborate a bit more it would help. Thank you.",2013-10-17 12:30:57.047
104616,57705,21762.0,CC BY-SA 3.0,,"If you take differences, you treat the variables as being at least interval scaled. So, if the sample size is not too small, you could as well consider a linear (mixed effects) regression.",2013-10-17 12:31:32.937
104617,57699,20473.0,CC BY-SA 3.0,,"No it is not a problem that method B was not available. This fact does not affect any inference related to survival times. You are not trying to explain ""why they chose A over B"", but _given the choice_ (for whatever reason), what was the survival time.",2013-10-17 12:37:36.787
104618,57707,668.0,CC BY-SA 3.0,,"Whenever you are implementing a well-known statistical procedure, it is a *great* idea to compare your results to those produced by working software.  When you run your data through a stats package, what does it report?  Does it have the same problem or not?",2013-10-17 12:41:07.070
104619,57696,22787.0,CC BY-SA 3.0,,Just ran the SPSS and have gotten the predicted means. But am still confused as to how I'd compare the means that way.. tq for the help,2013-10-17 12:50:45.413
104620,57707,21918.0,CC BY-SA 3.0,,"@whuber, yes, that's a good idea.",2013-10-17 12:55:54.880
104621,57710,21762.0,CC BY-SA 3.0,,Cramérs V is for two nominals. What is bad about regression? Take the numeric variable as response and regress it to the nominal (using dummies). Look at the $R^2$ and the associated global F-test.,2013-10-17 13:14:31.763
104623,57601,12358.0,CC BY-SA 3.0,,"Why can't you just count up the fraction of instances for each of the four cases, i.e. the joint distribution, and then compute the marginal/conditional probabilities when you need them?  If you do that, your two ""different"" estimates are identitical.",2013-10-17 13:27:32.103
104625,57565,668.0,CC BY-SA 3.0,,"Thanks, but that's not an explanation, it's just a reiteration of your assertion (which is nevertheless correct: but repeating it does not make it any more convincing).  Because intuitions about randomness are often incorrect, even the most ""obvious"" statements about randomness and probability ought to be justified by appeals to axiomatic principles or established theorems.",2013-10-17 13:33:50.707
104626,57651,15183.0,CC BY-SA 3.0,,"Can you give an example, what does the data look like?",2013-10-17 14:04:00.433
104627,57709,22787.0,CC BY-SA 3.0,,"Thank you for your response thaq. Really appreciate it! 

In terms of splitting the data (going with the sex example), I have gotten to the stage where I had split the data to male/female Caucasians/Asians. At this stage though, some main effects were significant, some were not, the interaction effects were mostly non-significant and at times both main and interaction effects was not significant as well. However, some the graphs clearly shows that there is a interaction effect but only at one level of the 3rdvariable. For example:

Lineup ethnicity x Participant sex at two levels of Lineup Sex",2013-10-17 14:10:38.283
104628,40104,668.0,CC BY-SA 3.0,,"Provided $\lambda_i$ are not terribly small, continuous approximations to this linear combination ought to work well for computing the CDF (such as a Cornish-Fisher expansion).  What can you tell us about the possible values of the $\lambda_i$ and $a_i$?",2013-10-17 14:11:18.140
104629,57709,22787.0,CC BY-SA 3.0,,"Male Caucasians Lineup: Lineup ethnicity x Participant Sex
    - All main effects & interactions are non sig,

Female Caucasian Lineup: Lineup ethnicity x Participant Sex
    - All main effects & interactions are non.sig

But the graph indicates an interaction effect only for Female Caucasians. Would I then mention, ""There were no reported sig. effects. However the graph indicates an interaction effect for .. (..then explain with the study theory)""?",2013-10-17 14:12:04.293
104630,57709,22787.0,CC BY-SA 3.0,,"If I'm understanding your response correctly then, at this stage, if  I do not have any significant interaction effects. I would calculate the ANOVA and report it (even when there are sig. main effects). The same applies if everything was not sig. (main effects + interactions)?",2013-10-17 14:16:24.180
104631,57699,22790.0,CC BY-SA 3.0,,"@AlecosPapadopoulos, thanks. That partly confirms my intuition. However, isn't it a problem for estimating propensity scores (PS)? Given that this is an observational study, the goal of PS is to statistically reconostruct the assignment process to methods A and B. And for the first group, all where assigned to A because B was not available. Would not that distort the propensity scores?",2013-10-17 14:22:23.730
104632,57594,22705.0,CC BY-SA 3.0,,"Let me try expressing my discomfort in a different way. If you only have 2 variables as regressors, you really don't need to do regression. You could just define business rules? Its kind of odd to use regression if you have only 1 or 2 variables in the model.",2013-10-17 14:27:12.927
104633,57710,22795.0,CC BY-SA 3.0,,"Nothing wrong with regression, but as we have already that measure we would like to check it in another way just as double check with a correlation coefficient....thanks for the answer",2013-10-17 14:29:49.223
104634,57699,20473.0,CC BY-SA 3.0,,"Indeed you are right on that, but now I am confused: I thought that you wanted to compare the two methods in terms of their effectiveness. I understand now that ""effectiveness"" will be an _explanatory_ variable in order to see how it affects the propensity to assign A or B? In that case, the whole first sample should not be considered, in my opinion, because it is irrelevant to the object of study, if the object of study is the comparative construction of PS -""faced with two options how do we decide which one?"". For the first group of patients there was no ""dilemma"".",2013-10-17 14:38:05.323
104669,57711,5237.0,CC BY-SA 3.0,,"That's true, @Tal, but it's simply due to tradition. That is, it's what people do because it's what people do. There is a legitimate question about whether it is the best way to go about things. Even if you believe the appropriate answer here is ""no"" (which is undoubtedly a defensible position), this is certainly a question worth asking.",2013-10-17 20:11:12.327
104635,57707,,CC BY-SA 3.0,,"Have you considered the modified newton's method? It was a method built specifically (as far as I know) to deal with running into singular matrix. You simply make an $n$ by $n$ identidy matrix which you subtract from your Hessian to avoid a matrix that does not invert.  As for your second question, Newton's method can run into problems that it finds local and not global maximum, so you should try different starting points and see what you get.  It can also start moving in one direction (the wrong one) and simply never converge, so starting point selection is very important.",2013-10-17 14:55:11.003
104636,57711,5237.0,CC BY-SA 3.0,,"Just out of curiosity, are you also the person behind this question: [assessing-approximate-distribution-of-data-based-on-a-histogram](http://stats.stackexchange.com/q/51718/), & this question: [what-is-the-intuition-behind-conditional-gaussian-distributions](http://stats.stackexchange.com/q/71260/)? (The usernames seem similar to me.) If so, would you mind [registering](http://stats.stackexchange.com/help/creating-accounts) your account, & then [merging](http://stats.stackexchange.com/help/merging-accounts) these into 1 account? We're happy to help you, but this makes the site run smoother.",2013-10-17 15:01:44.867
104638,57707,,CC BY-SA 3.0,,"I found this.  I believe that the method that I was suggesting here is the one that they're calling the Levenberg-Marquadt method.                         Discovery - Unconstrained Optimization 24 So that • ˆH(x) is symmetric p.d. • ˆH(x) is not too close to singular, i.e., its smallest eigenvalue is bounded below by a constant bigger than zero. Popular methods: • Greenstadt’s method: Modify eigenvalues. • Levenberg-Marquardt method: Add a scaled identity matrix • Modified Cholesky Stratigies: Perform Choleskey factorization of the Hessian and modify the diagonal elements",2013-10-17 15:04:10.283
104640,57616,5448.0,CC BY-SA 3.0,,"Let's consider a concrete, simple example; you have 5 slides from a person who has the pathogen.  What is the probability that you fail to correctly identify this person as having the pathogen?  A hidden assumption is that the presence / absence of the pathogen on a slide is independent of the presence / absence of the pathogen on other slides taken from the same specimen.",2013-10-17 15:28:07.410
104641,57656,5448.0,CC BY-SA 3.0,,"Is your target variable $y > 0$ or perhaps $\geq 0$?  If so, that opens up some options...",2013-10-17 15:30:19.310
104642,57712,12544.0,CC BY-SA 3.0,,I would add the regression line to them.,2013-10-17 15:36:46.067
104644,57694,18848.0,CC BY-SA 3.0,,"Thanks Ladislav, I ran the approach and I also confirmed it with the plots and it worked well. Thanks again",2013-10-17 16:29:54.493
104645,57713,5448.0,CC BY-SA 3.0,,"Nice use of an often-forgotten tool... and, of course, for either of $\lambda_1$ or $\lambda_2 \leq 5$ or so, the brute force convolution method won't be all that painful.",2013-10-17 16:30:04.337
104646,57616,22756.0,CC BY-SA 3.0,,That would be the probability of obtaining 5 false negatives in a row:,2013-10-17 16:35:23.437
104647,57720,5237.0,CC BY-SA 3.0,,"When working w/ novices, it's helpful to keep explanations simpler even if you sacrifice a little nuance & accuracy; however, describing what p-values are as telling ""you if there is actually a relationship between the independent and dependent variables"" may be a bit too far in that direction for comfort.",2013-10-17 16:43:32.020
104648,57720,13549.0,CC BY-SA 3.0,,I appreciate the comment gung. I haven't ever answered a question before on here before. I only ventured to help on this one as I have taught simple linear regression to about 150 biology students in the last year but perhaps I've gotten too used to toning it down. I'll see if there's a way to delete answers.,2013-10-17 16:48:12.447
104651,57720,22798.0,CC BY-SA 3.0,,What is the difference between the r^2 value and the adjusted r^2 value? Also if you look at the blue chart on the bottom this includes a p-value and adjusted r^2,2013-10-17 17:40:52.457
104652,57719,5448.0,CC BY-SA 3.0,,"Yes, sometimes the easiest problems are the hardest!",2013-10-17 17:45:20.027
104654,57641,22631.0,CC BY-SA 3.0,,@LCialdella Alright Ill add a link.,2013-10-17 17:54:39.140
104655,57641,22631.0,CC BY-SA 3.0,,http://personal.ee.surrey.ac.uk/Personal/P.Jackson/pub/avsp09/HaqJackson_AVSP09.pdf,2013-10-17 17:55:24.957
104656,57660,5203.0,CC BY-SA 3.0,,Glad I could help. Keep @PeterFlom's advice in mind though--the problem here isn't the evaluation method; it's the classifier.,2013-10-17 18:26:32.757
104657,57668,9175.0,CC BY-SA 3.0,,@Flounderer was right. The question was about proving $\frac{Y}{X}$ and $X$ are independent which was straightforward.,2013-10-17 18:35:51.660
104658,57595,17573.0,CC BY-SA 3.0,,"$F=G$ is probably not what you want, not least because it's not true.  You do want the law of iterated expectations, though, that much you have right.  Something about sub sigma algebras . . .",2013-10-17 18:36:25.960
104659,57675,18767.0,CC BY-SA 3.0,,"That makes sense now, thanks! I don't know why I was so sure that he was using Bayes' theorem...",2013-10-17 18:47:45.863
104660,57628,17573.0,CC BY-SA 3.0,,I tried to resist but could not.  Have you read Arthur Goldberger's wonderful chapter on micronumerosity?  It's quoted in full in this blog post:  http://davegiles.blogspot.com/2011/09/micronumerosity.html,2013-10-17 19:14:32.700
104661,57718,9245.0,CC BY-SA 3.0,,"If you have a good grasp of what the different foods and environmental conditions in the""universe"" are you can use techniques for ""Positive and Unlabeled Examples.""  They do tend to have more of a machine learning than a statistical motivation, though.  I believe Charles Elkan has a good paper on the subject.",2013-10-17 19:14:44.023
104662,57726,5237.0,CC BY-SA 3.0,,"Welcome to the site, @alexhli. If this question were *only* searching for a function or library to do this in Python, it would be off-topic for CV (see our [help page](http://stats.stackexchange.com/help)). However, it's not clear to me whether that's what you are asking (eg, ""*preferably* in Python""). If you have a substantive statistical question about these methods beyond looking for a function, would you edit to clarify it?",2013-10-17 19:15:03.827
104663,57711,,CC BY-SA 3.0,,"Why do you want to do that? In all of the scientific literature I know, different main and interaction effects of an ANOVA are *not* considered as members of the same family of comparisons. In other words, alpha is controlled for each effect by itself. Multiple comparisons are corrected for only when simple effects are tested.",2013-10-17 19:47:03.590
104664,57661,,CC BY-SA 3.0,user31629,It is on the latter so I apologize for posting off-topic. By ADL I meant Autoregressive Distributed Lag.,2013-10-17 19:53:03.050
104665,57729,13037.0,CC BY-SA 3.0,,"You should always include a sample of your data, the exact commands you ran, and the exact error you got. Otherwise it is hard to know what you did (or what you did wrong). NVM - I see you posted the info in the other post.",2013-10-17 20:03:30.553
104666,57717,668.0,CC BY-SA 3.0,,"Because the ""reflection"" is straightforward to interpret, this question effectively is a [duplicate of questions](http://stats.stackexchange.com/search?q=interpretation+log+add) about interpreting the ""started log"" $\log(1 + Z)$.",2013-10-17 20:03:51.617
104667,57477,594.0,CC BY-SA 3.0,,"Do you see that $\sqrt{a^2+b^2}\leq a+b$ (with $a, b\geq0$), and you only have equality if one of them *is* $0$? (This is just that the hypotenuse of a right angled triangle is smaller than the sum of the other two sides)",2013-10-17 20:04:40.207
104670,57729,594.0,CC BY-SA 3.0,,CoG? Could you please expand your abbreviation in-question?,2013-10-17 20:11:37.163
104673,57715,8414.0,CC BY-SA 3.0,,Could you add some more context to your description?  What other questions do you want to ask of your data?,2013-10-17 20:17:48.447
104674,57727,5448.0,CC BY-SA 3.0,,"Is this self-study or homework?  If so, please add the appropriate tag - we'll guide you to an answer, rather than just providing one outright.",2013-10-17 20:19:16.173
104675,57730,594.0,CC BY-SA 3.0,,"Ah, I get it. You're reading it as ""who is speaking"" is the response? I didn't read it like that.",2013-10-17 20:20:14.643
104676,57729,14806.0,CC BY-SA 3.0,,Center of Gravity. It's measure of where the greatest energy concentration occurs over a given frequency in speech.,2013-10-17 20:22:58.407
104677,57726,594.0,CC BY-SA 3.0,,"If you were to modify the question to something like ""What is a way or ways to what I want, and is there a python implementation?"" the first part should be sufficiently on topic. But then your question would require clarification (you end by asking about exploring, not testing -- those are very different exercises)",2013-10-17 20:24:39.380
104678,57730,14806.0,CC BY-SA 3.0,,"Just to clarify, the end result ('p' in this case) is going to be a list of p-values for each level, right?",2013-10-17 20:29:47.463
104679,57730,14806.0,CC BY-SA 3.0,,"Also, this leads to another question: So if we know that CoG and Kurtosis are significant in predicting differences between speakers, does this also mean that there is significant difference between speakers based on these variables?",2013-10-17 20:40:33.583
104680,57731,,CC BY-SA 3.0,,Do you need an analytical solution or a piece of code?,2013-10-17 20:53:04.240
104681,57656,16039.0,CC BY-SA 3.0,,"Hmm, no unfortunately not. Out of curiosity, what would those options be?",2013-10-17 20:59:57.487
104682,57718,13549.0,CC BY-SA 3.0,,"Alex - thanks for this, I've filed it away for future reference. The hope with this is to eventually make a predictive model and the Elkan paper seems like it might help with that, in the later stages",2013-10-17 21:00:17.047
104683,57730,13037.0,CC BY-SA 3.0,,"@Shakesbeery it is important to note that multinomial regression fits pairwise models (like speaker 3 compared to speaker 1, speaker 2 compared to speaker 1) I highly recommend going through the examples on the link I provided to gain a greater understanding of the output.",2013-10-17 21:02:02.490
104684,57725,13549.0,CC BY-SA 3.0,,Unfortunately I know little about Bayesian analysis beyond what it can be used for. I can always start doing some reading though! I'll keep this open for a few more days to see if I get other suggestions,2013-10-17 21:03:52.427
104685,57731,20473.0,CC BY-SA 3.0,,"You write ""then the distance between the _first_ and the _second_ point is found"". So even though the points are dispersed in space, still they are indexed $1,...,K$, _before they are dispersed_, and you consider distances only following the index sequentially? Also is $Dth$ one symbol, or a product that we should know something about?",2013-10-17 21:06:12.080
104686,57656,5448.0,CC BY-SA 3.0,,"The ""family"" parameter in `gam` specifies a link function, as with a generalized linear model; part of that is a specification of a mean-variance relationship.  For example, for the Poisson family, variance = mean (with the default link function); for the Gamma, the standard deviation = the mean (with the default link function.)  You may still be able to make use of this approach by transforming `y` mildly; the nonparametric nature of the right hand side (at least in your example) means you don't have to worry (much) about functional forms being changed by doing so.",2013-10-17 21:18:22.500
104687,57718,16588.0,CC BY-SA 3.0,,You're not allowed to sample non-event cases because there could be error in those samples? Is there no possibility of error in the observed events? There's always error. That's why we bother with statistical inference.,2013-10-17 21:28:11.460
104688,57685,18268.0,CC BY-SA 3.0,,"Thank you, user31264. I did my research, but I was wondering if there is a way to do this without losing information (i.e. having all 100% variance preserved in the modified dataset).",2013-10-17 21:35:59.587
104689,57718,13549.0,CC BY-SA 3.0,,"No, it was for non-statistical reasons that I was asked to look for another method. Although if nothing else appears to work as well as what I had planned then that may be persuasive enough to let me get my samples. I need to find out first though.",2013-10-17 21:40:11.953
104690,57696,14799.0,CC BY-SA 3.0,,"First, try redefining the levels of the within-subject factors, from absolute to relative: same/different as the subject. This should interchange some main effects and interactions. Second, if the only possible values of the dependent variable are 0/1/2 then some of the interactions might be due to floor or ceiling effects, in which case you might try an ordinal logistic analysis.",2013-10-17 21:44:46.567
104692,57477,594.0,CC BY-SA 3.0,,"Looked at another way, consider the implied variance. If $a+b$ is the standard error of the difference, the implied variance is $a^2+b^2+2ab$, which for $a,b>0$ is bigger than the sum of the variances. If you accept that $\text{Var}(X-Y)=\text{Var}(X)+\text{Var}(Y)$ then if you say ""add the standard errors"", clearly this results in an additional positive term in the variance that shouldn't be there. i.e. making it 'too large'",2013-10-17 21:57:25.660
104693,57731,22806.0,CC BY-SA 3.0,,"Thank you both for responding. Momo, I need an analytical solution. Alecos, yes I index the points and after that I model their locations using 2D-Poisson. The distances has no relation with the index. Dth is one symbol denoting the threshold distance.",2013-10-17 21:57:41.010
104696,57695,594.0,CC BY-SA 3.0,,What are you trying to achieve (i.e. what is the actual problem you seek to ask of your data)?,2013-10-17 22:08:46.223
104697,57656,16039.0,CC BY-SA 3.0,,"Thanks for your suggestion. That does sound awfully fiddly though, and I feel like I might be better off manually calculating regression weights and iterating.",2013-10-17 22:09:41.830
104698,57712,594.0,CC BY-SA 3.0,,"""*The negative slope indicates that the values are decreasing together negatively*"".  If they 'decrease together' (i.e. one decreases when the other one decreases) they'd have a *positive slope*. You mean that one *decreases* as the other *increases*, which is the opposite of any sense of 'together'. Adding 'negatively' to the end of that doesn't serve to make it less confusing.",2013-10-17 22:27:26.800
104699,57656,5448.0,CC BY-SA 3.0,,"Yes, that's another idea.  It's just that if you can do it easily with the ""family"" parameter, the iteration is handled for you.  But if it's going to be work, it might well be better to do it all by hand, well inside a loop at any rate, and get more flexibility.",2013-10-17 22:50:22.950
104701,57685,22507.0,CC BY-SA 3.0,,I afraid that is not possible. Suppose you have 4 points in vertices of a regular tetrahedron. Their mutual distances will be the same.  You cannot reproduce it at a plane.,2013-10-17 23:07:03.190
104702,57734,10060.0,CC BY-SA 3.0,,"The F-statistics you found there should be useful. Try to calculate the p-value of this ANOVA test. The resultant p-value should also be the p-value of the interaction term in your model 5. Now, you have the p-value and the point estimate, and you know that regression coefficients are tested with t-statistics, and you also know the df is 1... you should be able to deduct what the standard error of the interaction term is.",2013-10-17 23:17:48.530
104703,57656,16039.0,CC BY-SA 3.0,,"I will have a play around with the 'family parameter' and see if I can get it to work easily for me, but I'm comfortable programming, so it might be easier to get the flexibility like you say. Thanks for the help.",2013-10-17 23:27:44.723
104704,57734,16588.0,CC BY-SA 3.0,,"why not look at the output of `summary(lm(y ~ x1 + x2 + I(x1*x2)))` for the standard error? Also, the `I()` is unnecessary.",2013-10-17 23:48:39.800
104705,57730,594.0,CC BY-SA 3.0,,@Benjamin I see from a clarification on the identical question posted to reddit that the OP does appear to intend it the way you interpret it. My apologies.,2013-10-18 00:03:16.843
104706,57729,594.0,CC BY-SA 3.0,,"As I already asked please clarify *in the question*. That is, please edit your question to make the question clear (and even better, fully define 'center of gravity'). Hardly any of us are speech researchers, and not everyone reads all the comments in order to understand the questions, which should stand alone.",2013-10-18 00:05:29.260
104707,57730,13037.0,CC BY-SA 3.0,,@Glen_b man...this is probably first time that I have been correct and a mod has been wrong. My life has gotten so much better!,2013-10-18 00:07:03.233
104709,57731,20473.0,CC BY-SA 3.0,,"It seems there is an internal consistency problem with your formulation. Denote $d_{ij}$ the distance between points $i$ and $j$. Assume we are at point 3. Assume first that points 1 and 2 are in different groups. It may be the case that both $d_{13}$ and $d_{23}$ exceed the threshold. In which group should point 3 be placed, since it is eligible for both existing? Maybe the answer is that we measure the distance of each point only from its predecessor? (i.e. we measure only $d_{23}$? (CONTINUED)",2013-10-18 00:29:51.707
104710,57731,20473.0,CC BY-SA 3.0,,"(CONTD)...But if this were the case, then assume that points 1 and 2 are in the same group: in order to determine whether point 3 should join them or be placed in a separate group, we must measure both $d_{13}$ and $d_{23}$. So we arrive at the internal consistency problem: if for each point we must measure the distance from all its predecessors, then indeterminacies may arise (case in previous comment). If we are to measure the distance only from its predecessor, then we cannot apply the criterion stated for joining a group. What are your thoughts on that?",2013-10-18 00:33:43.480
104711,57685,18268.0,CC BY-SA 3.0,,"Okay, a little more reading have clarified this. Thank you.",2013-10-18 00:36:54.490
104712,57717,22800.0,CC BY-SA 3.0,,"I'm not sure the question referenced as 'answered' was sufficient.Suggestions were made to use other transformations or a GLM framework, but the author specifically asked about the interpretation of 'beta' in terms of percentages.I did not find a specific yes or no confirmation in the comments regarding the correctness of interpretation. My question differs in adding a reflection, and specifically I'm interested in a reverse transform of 'b1'to get a 'units' interpretation (via exponentiation)which is a different question.I apologize if it is glaring before me but I have not found an answer.",2013-10-18 00:57:24.423
104713,57666,15583.0,CC BY-SA 3.0,,Any suggestion of how to initialize the `lbfgs_malloced` array?,2013-10-18 01:02:27.167
104714,57730,594.0,CC BY-SA 3.0,,"Sorry, but that's still to come -- I'm not a mod.",2013-10-18 01:10:43.510
104715,57738,594.0,CC BY-SA 3.0,,"Analyses in R should be able to manage just fine with variables coded M/F and L/R; it has probably already made them factors (check by something like `is.factor(gender)` or even just use `str()` on them); if not, try `as.factor()`. You could analyze this data using ANOVA, yes",2013-10-18 01:20:16.507
104716,57738,5237.0,CC BY-SA 3.0,,"*Note, although this question mentions using R, I see no reason to think it is off-topic. The main question seems to be ""is an ANOVA appropriate"", which is clearly on-topic & has nothing to do with R.*",2013-10-18 01:25:28.937
104717,57738,594.0,CC BY-SA 3.0,,@gung I agree -- and have highlighted the main question to make it clear that an on-topic question is being asked.,2013-10-18 01:27:16.880
104718,57733,2081.0,CC BY-SA 3.0,,"I don't understand your notation $\rho_y$ and $\rho_y$. The formula you give should probably be meant $\beta_x = b_x(\sigma_x/\sigma_y)$. This formula of converting _b_ in _beta_ is valid also when there is no intercept; but then $\sigma_x$ and $\sigma_y$ must be standard deviations not from the _means_ but from 0, - they are _root mean squares_ then.",2013-10-18 01:27:33.247
104719,57738,5237.0,CC BY-SA 3.0,,"Because each individual contributes two data, you may want to use repeated measures ANOVA here. If you are less familiar with that, you could try using difference scores.",2013-10-18 01:31:45.077
104720,57664,633.0,CC BY-SA 3.0,,"@user777: I think that for any particular distribution $d$ of a continuous exponential family, there is a sample space transformation that makes it so that $d$ is uniform over the new sample space.  In other words, this intuition about something being uniform, multimodal or unimodal is probably irrelevant.  Specifically, $\textrm{Beta}(\frac12, \frac12)$ is unimodal if you were to transform the sample space to be the space of log-odds.",2013-10-18 01:40:39.850
104721,57741,,CC BY-SA 3.0,,"I have voted to close as this question solicits either opinions (person A's good writer need to be everyone's good writer), or list like answers; there is no true answer. Such questions are not a good fit for *any* [se] site. Hopefully this explains why people are voting to close?",2013-10-18 02:06:49.557
104722,57734,594.0,CC BY-SA 3.0,,"@ndoogan +1, but the `*` operator in R formulas includes the main effects, while `:` represents the interaction-only. That is the formula `y ~ x1*x2` is equivalent to `y ~ x1 + x2 + x1:x2`",2013-10-18 02:25:38.440
104723,57666,22143.0,CC BY-SA 3.0,,"Since the documentation notes *A user does not have to use this function for libLBFGS built without SSE/SSE2 optimization*, I assume you want to have SSE optimization? Hmm, I have not explored it.",2013-10-18 02:26:31.223
104724,57733,22340.0,CC BY-SA 3.0,,@ttnphns Please see edits for better clarification,2013-10-18 02:27:32.197
104725,57734,594.0,CC BY-SA 3.0,,"`?confint` will generate one from the model (though it's easy by hand from the output, as ndoogan suggests).",2013-10-18 02:27:49.497
104726,57733,2081.0,CC BY-SA 3.0,,It'd be nice to follow more standard notation: $b$ = regr. coef; $\beta$= standardized regr. coef; $r$ = empirical correlation etc.,2013-10-18 02:36:06.213
104727,57729,14806.0,CC BY-SA 3.0,,"@Glen_b Well you don't need to be a speech researcher to know the difference between _in question_ and _in the question_... Hehe, but jokes aside, I will clarify my post. Thanks.",2013-10-18 03:11:24.543
104728,57746,594.0,CC BY-SA 3.0,,"You might like to draw a plot of f vs t first, to see what's going on, though for the second one you'll have to look at it as a process approaching a limit.",2013-10-18 03:56:37.897
104729,57745,2081.0,CC BY-SA 3.0,,P.S. There is a good short overview about ordinal variable approaches in Jeromy Anglim's blog http://jeromyanglim.blogspot.ru/2009/10/analysing-ordinal-variables.html,2013-10-18 03:58:37.860
104730,57628,18198.0,CC BY-SA 3.0,,"Your initial comment about resisting commenting seems to suggest that you think my research into ridge and other regularization methods is misguided? I have read about micronumerosity in the course of my research. Unfortunately I can't get any more data for a given day, but I can add data in the bayesian sense by building priors based on previous days observations. Could this be used to address micronumerosity?",2013-10-18 04:03:32.283
104731,57710,2081.0,CC BY-SA 3.0,,"You haven't said anything specific about your ""numeric/ordinal"" variable. _What_ makes you to pose it ordinal? numeric?",2013-10-18 04:15:18.160
104733,57746,594.0,CC BY-SA 3.0,,Is this for some subject?,2013-10-18 04:48:01.220
104734,57737,13846.0,CC BY-SA 3.0,,My main questions (the two in bold) cannot be addressed using `anova()`. I want to see whether the log-odds in each of the conditions is significantly different from 0.,2013-10-18 04:51:29.987
104735,57683,22763.0,CC BY-SA 3.0,,"You're right, I've edited my question. The real question though, is can I use the new $\mu_\text{sum}$ and $\sigma_\text{sum}$ to compute a CDF using the sum of three specific samples. I don't think this works but I wanted to be sure.",2013-10-18 05:37:16.847
104736,57683,594.0,CC BY-SA 3.0,,@mikepk are you asking whether you can use *sample* means and standard deviations to compute the *population* distribution function? Or are you trying to find the ECDF of the summed sample values? Or something else? What are you actually trying to achieve in the end?,2013-10-18 05:40:35.200
104737,57744,12140.0,CC BY-SA 3.0,,"I certainly didn't expect to see that results have bell-shaped form. I discovered this property accidentally, and now want to explore it further, because it's very helpful with what I'm doing.",2013-10-18 05:46:55.213
104738,57331,594.0,CC BY-SA 3.0,,"Your notation is now confusing and contradictory. I suggest saying ""Let $S=X+Y+Z$,"" replacing ""$\text{sum}$"" with ""$s$"" in subscripts and $F_x(x_x + x_y + x_z)$ with $F_S(x + y + z)$. If you agree that is what you're asking, then your question would make sense, and the short answer is ""yes, that's what you do"".",2013-10-18 06:02:08.937
104739,57723,,CC BY-SA 3.0,,Great answer with added bonus of all references outside of the paywall!,2013-10-18 06:04:39.687
104740,57710,22795.0,CC BY-SA 3.0,,"ordinal beacuse I have a variable coming from a survey test so its range is  -4,4, you can also think it as interval but this kind of survey variable are considered mostly as ordinal and the others are numeric, in specific continuous as they are features extracted.",2013-10-18 07:49:03.560
104741,57738,21762.0,CC BY-SA 3.0,,"@gung: Since it is about two trials, its probably not a paired design. This should be clarified by the OP.",2013-10-18 07:51:19.850
104742,57711,21762.0,CC BY-SA 3.0,,"You are doing three tests, so its $\alpha/3$. (I'd prefer the less conservative Bonferroni-Holm correction, which is almost as simple to apply). @Tal: Setting a global error in modelling would successfully avoid people doing p-value based variable selection...",2013-10-18 07:55:53.477
104744,57727,21762.0,CC BY-SA 3.0,,The answer to the second question is yes.,2013-10-18 08:39:15.730
104745,57753,13846.0,CC BY-SA 3.0,,"This is a great trick! It actually (I think) does exactly what changing the reference level does, but without the hassle of having to change the reference levels multiple times for each condition/category. Thanks!",2013-10-18 08:40:41.387
104746,57741,,CC BY-SA 3.0,,"I like this question, and am disappointed that we won't be seeing any answers to it.",2013-10-18 08:53:26.230
104747,57745,21762.0,CC BY-SA 3.0,,"One measure of association between an ordinal and a nominal is called ""Freeman's $\theta$"". Unfortunately, I don't have any open access reference at hand.",2013-10-18 09:02:21.013
104748,57759,594.0,CC BY-SA 3.0,,"In what way(s) were the scores you computed inadequate for your purposes? Without knowing that, we'd likely suggest things with the same inadequacies.",2013-10-18 09:31:41.093
104749,57758,1927.0,CC BY-SA 3.0,,The trace is invariant under cyclic permutations.,2013-10-18 09:40:12.897
104750,57758,594.0,CC BY-SA 3.0,,[$tr(AB)=tr(BA)$](http://en.wikipedia.org/wiki/Trace_%28linear_algebra%29#Trace_of_a_product),2013-10-18 09:41:02.940
104751,57760,22824.0,CC BY-SA 3.0,,Thank you for your help :)! That's exactly what I needed to know.,2013-10-18 09:52:42.727
104752,57759,14525.0,CC BY-SA 3.0,,I have edited my post to address your questions,2013-10-18 09:53:42.613
104753,57745,2081.0,CC BY-SA 3.0,,"@Michael thanks, here I found a paper ""A further note on freeman's measure of association""  http://moreno.ss.uci.edu/22.pdf",2013-10-18 10:30:45.237
104754,57731,22806.0,CC BY-SA 3.0,,"That is why I'm indexing the points before modeling their locations. This is a sequential process, so in your example point 3 will be put in group 1 because it has be tested with point 1 before point 2 regardless to the positions of the points.",2013-10-18 10:51:55.067
104755,57731,22806.0,CC BY-SA 3.0,,"I find the distance of a certain point with all other points following the indexes. If the distance between the point of concern and another point is higher than the threshold, then I test it with all the other points in the same group of that point. If all the distances are higher than the threshold then I put that point in that group.",2013-10-18 10:53:56.593
104756,57757,2081.0,CC BY-SA 3.0,,"I don't quite get what you mean by ""maps"", but if what you realy want is to _compare visually_ several frequency (or other contingency) tables, here is two choices among a few: (1) multiple (3-way) **correspondence analysis**; (2) individual-scaling model of **multidimensional unfolding**. Both are quite advanced statistical techniques, so you might prefer easier ways, such as mosaic, paneled, etc. charts showing frequencies.",2013-10-18 11:03:40.767
104757,57764,,CC BY-SA 3.0,,Related: http://en.wikipedia.org/wiki/No_free_lunch_theorem and http://stats.stackexchange.com/questions/17066/what-is-a-good-resource-that-includes-a-comparison-of-the-pros-and-cons-of-diffe,2013-10-18 11:04:53.450
104758,57765,15827.0,CC BY-SA 3.0,,"Correlation only makes sense if values are paired. If one or more of your values in sample 1 is not paired with a value in sample 2, then those values can't be used in a correlation. Most of all, if no values are paired, correlation does not apply.",2013-10-18 11:11:08.070
104759,57765,2081.0,CC BY-SA 3.0,,"@Tania, you have _one_ sample, two variables.",2013-10-18 11:14:13.463
104760,57763,,CC BY-SA 3.0,,"+1 I have also frequently seen (and perhaps used) “correlational” as a synonym for “observational” or “non-experimental”. At this stage, I don't think there is much hope for another, more specific definition to take hold.",2013-10-18 11:21:59.370
104761,57765,15827.0,CC BY-SA 3.0,,"As @ttnphns points out, ""sample"" is not the right term here. What are paired are values of variables.",2013-10-18 11:22:55.723
104762,57765,22830.0,CC BY-SA 3.0,,Yes Nick! I am looking for correlation between 2 scales (psy tests). The n of one is a little higher than the n of another. I mean not ALL the respondents who filled up one form (scale) have filled up the other. There are some (very few though) missing...,2013-10-18 11:23:55.653
104763,57765,15827.0,CC BY-SA 3.0,,"So, you can't use any cases (observations, records) with missing values. Your software should take care of that somehow. (Depending on quite what your scale is, correlation might not be best, but that's another story.)",2013-10-18 11:29:05.127
104764,57764,22827.0,CC BY-SA 3.0,,"I am using the rapid miner tool, I'm using 10 fold cross validation with stratified sampling. for Naive bayes I check the laplacian correction and for SVM I use a dot kernel and other parameters are all in their defaults, but when I change the parameters and try again, I get same result; naive bayes outperforms SVM",2013-10-18 11:34:57.897
104765,57765,22830.0,CC BY-SA 3.0,,"That is what I exactly thought: software should have taken care of it!I have already computed the correlation by using SPSS, but I wanted to be sure that it is ok if a few observations are missing in one of the variables, and n (s) are not matching exactly.",2013-10-18 11:43:07.847
104944,57826,1412.0,CC BY-SA 3.0,,"I used to scratch my head about ""sth"" and eventually realized it is  tweeter-speak for ""something"". The other keyboardism I would like to eradicated is ""wanna"" for ""want to"".",2013-10-19 18:40:25.463
104766,57765,15827.0,CC BY-SA 3.0,,"I don't know what you mean by ""ok"". You have some missing values; whether that implies a bias is impossible for us to say. Sample size $n$ as far as the correlation is concerned is the number of pairs of values included in the correlation; it can only be unclear or misleading to others if you think of this or report it as using different sample sizes.",2013-10-18 11:59:14.103
104767,57765,15827.0,CC BY-SA 3.0,,"Why remove mathematical formatting? It is cosmetic here, but it does no harm. (We are wasting time trying to make small improvements to your presentation if you keep reversing them.)",2013-10-18 12:01:32.287
104768,57628,17573.0,CC BY-SA 3.0,,"No, the point of Goldberger's chapter is that multicollinearity is not a problem to be solved via statistical technique.  Just like micronumerosity is not a problem to be solved via statistical technique.  The standard errors are big because your data don't reveal the thing you are interested in, not because you analyzed them incorrectly.  Of course, you can always make the standard errors smaller by bringing in outside information, but then it's the outside information not the data which are identifying the parameter(s) of interest.",2013-10-18 12:02:18.087
104769,57765,22830.0,CC BY-SA 3.0,,"Nick, would you suggest I rerun the analysis by taking care of the missing values, and make the n (s) equal?",2013-10-18 12:04:04.900
104770,57600,651.0,CC BY-SA 3.0,,"In the Bayesian approach, the models would be weighted by their marginal likelihood (i.e. Bayesian evidence) and any prior placed over the hyper-parameters, so it would be a special case of averaging over an ensemble with a particular method for weighting the models.",2013-10-18 12:20:09.750
104771,57765,15827.0,CC BY-SA 3.0,,"Sorry, but I am at a loss to know what you did that needs correcting. No statistical software worthy of the name will calculate a correlation from differing numbers of values for the two variables. Perhaps you should show us what commands you used and what output you got.",2013-10-18 12:23:20.933
104772,57767,15827.0,CC BY-SA 3.0,,"For others I will add that in climatology ""anomaly"" just means deviation from a reference level; there is no implication of anything pathological or very unusual. In terms of the question, who is claiming that this is ""optimal""? What is optimal will depend at least tacitly on a model for the time series, quite apart from any other considerations?",2013-10-18 12:26:35.777
104773,57752,15563.0,CC BY-SA 3.0,,This page might help: http://stackoverflow.com/questions/6782070/display-correlation-tables-as-descending-list?rq=1,2013-10-18 12:30:48.703
104774,57763,17573.0,CC BY-SA 3.0,,"That is a very strict definition of ""true experiment.""  The populations in medical trials are generally convenience samples, for example, who are then randomly assigned.  So, they would be quasi-experiments.  Most people seem to mean just ""random assignment"" by experimental.",2013-10-18 12:31:54.270
104775,57756,17573.0,CC BY-SA 3.0,,"Wikipedia has good definitions of quasi-experiment, natural experiment, cohort study, and observational study.  Your study is an observational, longitudinal (or cohort) study.  Your study doesn't look quasi-experimental to me---it would be if students were randomly assigned to G1/G2 somehow by nature.  A picture of the ""pyramid of evidence:""  http://sourcesandmethods.blogspot.com/2011/05/evaluating-analytic-methods-what-counts.html",2013-10-18 12:33:47.810
104776,57734,16588.0,CC BY-SA 3.0,,"@Glen_b you are exactly right. However, if the main effects are already present in the formula, the x1*x2 notation will not re-add them.",2013-10-18 13:05:07.263
104777,57765,22830.0,CC BY-SA 3.0,,Many thanks Nick and @ttnphns! I will have to go back and check where did I go wrong:-) I might bother you again. TC,2013-10-18 13:05:20.420
104778,57763,503.0,CC BY-SA 3.0,,"@Bill Indeed. Both ""quasi-experiment"" and ""experiment"" are used differently by different people. But this masks problems of external validity with convenience samples.",2013-10-18 13:21:01.520
104780,57767,8629.0,CC BY-SA 3.0,,"True, my choice of the word ""optimal"" was not optimal. Replaced with ""reasonable"".",2013-10-18 13:28:21.470
104781,57769,8629.0,CC BY-SA 3.0,,"Thanks, this makes sense for purely additive processes. However, I often see people use this approach when the underlying process is clearly *not* purely additive. See my updated question.",2013-10-18 13:35:26.563
104782,57477,5643.0,CC BY-SA 3.0,,"ok, I see all the numbers, although the intuition probably lies in geometry, rather than statistics. I get that adding variances would overestimate the standard error, because of the extra $2ab$ term, but why squaring the sum of the variances would give the ""right"" estimate? Yes, this is really a geometry question.",2013-10-18 13:42:38.600
104783,57762,22601.0,CC BY-SA 3.0,,"Thank you so much for this elaborate response, but I'm afraid that I've stated my problem wrong. I am very sure that your post is of use for others and thus vote it up! Thanks!",2013-10-18 13:43:18.037
104784,57767,22507.0,CC BY-SA 3.0,,"(1) Is the multiplicative part negligibly small?  After all, the base level changed by maybe 1 degree of Kelvin, while the base temperature is about 300 K (roughly). (2) Is there any statistical evidence that the seasonal variation shows long term trends, or there is a long term change in seasonal patterns?",2013-10-18 13:47:43.397
104785,57770,22833.0,CC BY-SA 3.0,,So you just to the 0.56 ignoring the constant + 1 there?,2013-10-18 14:02:46.050
104786,57586,1693.0,CC BY-SA 3.0,,"I can't find a mistake with your math.  I have run a model, though, that fits all the conditions you've described (except that the control variables are *added* all at once, not dropped).  This arrangement, the first I've run through the origin, is actually the only one I've used that does create a sign reversal.  N is 5k.",2013-10-18 14:18:45.417
104787,57767,8629.0,CC BY-SA 3.0,,"In my case (tropospheric NO2 pollution levels), the amplitude of the seasonal variation is clearly changing with time (clearly meaning by visual inspection). Which in my opinion shows that the multiplicative part cannot be neglected. As to (2), I'm not sure how to provide this statistical evidence. Any suggestions?",2013-10-18 14:18:47.010
104788,57680,306.0,CC BY-SA 3.0,,"to my wisdom, this is based on the problem at hand and it is dangerous to use such thumbrules in practice.",2013-10-18 14:21:27.623
104789,57752,2081.0,CC BY-SA 3.0,,That now looks somewhat more a graph theory than a statistical question (because correlations are not seen as interdependent anymore). Maybe StackOverflow can yield better answers. Some sort of constrained minimal spanning tree...,2013-10-18 14:21:47.040
104790,57773,2081.0,CC BY-SA 3.0,,"Very nice snapshots, gladden the eye! And the explanation. But - I ask you - please tell @Tania about pairwise and listwise deletion of missings and under what button it is found in SPSS.",2013-10-18 14:29:59.307
104792,57772,17328.0,CC BY-SA 3.0,,"So what conclusion did you reach based on your simulations? Why not show a diagram of the resulting plot rather than this plethora of code? In fact, I would suggest that you delete everything in your question after the line:  `Is Z distributed as well as a Beta-Binomial (with parameters n1+n2, a and b?`",2013-10-18 14:34:16.557
104817,57509,22703.0,CC BY-SA 3.0,,"Sir, the answer is simpler when we talk about the binomial experiment. The above mentioned texts as we move on never provide the motivation for the distribution. I intend to ask this question on many distributions, individually in fact.",2013-10-18 18:17:28.877
104818,57586,20473.0,CC BY-SA 3.0,,That's interesting. Can you calculate the two empirical probabilities of eq [6] in the model with the control variables present?,2013-10-18 18:30:55.553
104793,57699,22790.0,CC BY-SA 3.0,,"@AlecosPapadopoulos, the goal is to compare methods A and B in terms of patient survival. I updated my question. I want to use propensity score matching to deal with the problem that somwhat different patients (in terms of age, gender and so on) are treated by the two methods. For example the newer method B is used on older patients which were not operate at all earlier. The question is then, whether balancing data with PS I can use the first group of patients, who were operated some time ago and only with method A.",2013-10-18 14:37:07.730
104794,57776,21762.0,CC BY-SA 3.0,,Sorry for deleting the former comment. But there was a mistake in it I couldn't correct anymore ;),2013-10-18 14:45:47.540
104795,57773,10060.0,CC BY-SA 3.0,,"@ttnphns, no problem. Revised.",2013-10-18 14:50:47.387
104796,57683,22763.0,CC BY-SA 3.0,,"Thanks Glen, I'm probably not asking the right questions. I have a collection of three separate scores on three separate normally distributed ranges. Usually I compare the individual scores against all scores using a percentile (for which I'm using the CDF).

I'd like to compare the collection of all three scores against all the other possible collections of those three scores. Effectively how does this collection compare to the population of all collections. Like I said in my original question, my stats memory is dim, but I don't think this works.",2013-10-18 14:53:59.933
104797,57773,2081.0,CC BY-SA 3.0,,"Excellent (yes, really). Those shades, too... I feel like dragging it all to Flickr photostream.",2013-10-18 14:56:15.110
104798,57773,10060.0,CC BY-SA 3.0,,"@ttnphns, you're too kind. I use a screen capture software called [Snagit](http://www.techsmith.com/snagit.html) to do capture and post-capture touch up (like adding circles and arrows.) It also makes screen video, too. Pretty handy. (Disclaimer: I am not affiliate with this software's maker.)",2013-10-18 15:02:07.507
104799,57772,,CC BY-SA 3.0,,"I edited your question, please check if it is still correct. If not, you can rollback.",2013-10-18 15:03:51.780
104800,57699,17740.0,CC BY-SA 3.0,,"The fact most of the A surgeries occurred decades earlier than the B surgeries could imply that you can't really compare the two. The fact the time period is so radically different can cause significant confounding. Other seemingly unrelated treatments and life in general has changed a lot over such a long period, which will also reflect on your survival results. **Don't underestimate this hurdle.**",2013-10-18 15:05:25.407
104801,57683,22763.0,CC BY-SA 3.0,,"Reading over the original question I think I've even confused myself :). So I have a single sample $x = 300$ that I want the CDF of, so I get a value back like $F_X(300) = 0.6$. So the probability of all random samples being <= 300 is less than or equal is 0.6. Now I have three samples lets say (300, 900, 100) from three *different* data sets (all normally distributed) and I want to do something similar, the probability of three samples being less than or equal to that particular collection of three samples. The more I think about this the more it doesn't quite make sense to me.",2013-10-18 15:21:43.057
104802,57628,18198.0,CC BY-SA 3.0,,Understood but its not just that the standard errors are large the coefficients themselves tend to unfeasibly large and offsetting. Although these numbers provide the BLUE fit they are pretty much non-nonsensical in terms of real life values there are supposed to represent + I would not trust and prediction made using these extreme values. I do take your point though that the problem would probably disappear if I could just get more data.,2013-10-18 15:41:46.980
104804,57780,2081.0,CC BY-SA 3.0,,Ordinal predictors? You may try for example: A) Use the predictors as polynomial sets. B) Quantify ordinal into interval via CATREG (categorical regression). (_But_: are you really sure your variables must be treated as ordinal and not interval?),2013-10-18 15:46:41.133
104805,57752,22601.0,CC BY-SA 3.0,,"@ttnphs: a minimal spanning tree is just the thing I don't want, since pairwise correlations imply a complete graph. Nevertheless, you're right that this question might fit the mathematics-site better. Thanks!",2013-10-18 16:09:29.463
104806,57773,22830.0,CC BY-SA 3.0,,A THOUSAND thanks @Penguin_Knight!!! You are just amazing...you have saved my life:-) I wish ALL teachers/ mentors are like you! And look at these beautiful & amazing snapshots. Awesome...,2013-10-18 16:14:07.997
104807,57772,21476.0,CC BY-SA 3.0,,"Sorry for not describing the results of the simulations. I could not find a combination of the free parameters for which the two variables $z$ (sum of beta-binomials) and $z1$ (beta-binomial with adjusted $a$ and $b$) have different densities (given the same mean and variance). I edited the text to include this. @wolfies: you are indeed right, the question could stop after the line you mention, I just wanted to point out that I have strong suspicions that the sum is distributed as a Beta-Binomial, but I am looking for a proof.",2013-10-18 16:22:17.270
104808,57744,1150.0,CC BY-SA 3.0,,@OutputLogic which is great!  I'm not sure how much the second part of that question helps answer your question ... it is hard to explain how deterministic and non-deterministic processes interact.  Can you talk a bit more about what is confusing for you?  (I'm actually working on a paper that tries to explain this to psychology and CS people right now!),2013-10-18 16:24:28.490
104809,57727,22341.0,CC BY-SA 3.0,,I an sorry I do not know how to add the appropriate tag,2013-10-18 16:53:01.350
104810,57628,17573.0,CC BY-SA 3.0,,"I understand now.  When you have multicollinearity, usually you can make pretty precise comparisons and good predictions as long as you make them ""with the grain"" of the multicollinearity.  For example, suppose that X3 is always pretty close to equal to X4 in the data.  Predictions at points where X3 and X4 are pretty close to equal will usually have pretty low variance and be reasonable.  Similarly, if you want to measure the effect of X3 and X4 rising together by one point, you will get precise estimates.  It's only if you try to ask a question the data can't answer that there are problems.",2013-10-18 17:08:04.230
104811,57628,17573.0,CC BY-SA 3.0,,"To use an example I always use in class, suppose you have a sample of shoe factories with variables for total production cost, number of right shoes, number of left shoes.  You can get excellent, plausible estimates of the extra cost from producing an extra pair of shoes and excellent forecasts of how much a shoe factory producing 10000 pairs of shoes will cost to run.  But, your estimates of how much it would cost to produce 1000 left shoes and 6000 right shoes will be terrible, because nothing like that ever happens in the data.",2013-10-18 17:10:58.263
104812,57779,5448.0,CC BY-SA 3.0,,"Just out of curiosity, why are you constrained not to use Stirling's formula?",2013-10-18 17:30:42.150
104813,57779,,CC BY-SA 3.0,user30602,"I am not constrained - I know how to use it, but I would like to see if there is a simpler solution.",2013-10-18 17:33:54.500
104815,57503,22703.0,CC BY-SA 3.0,,Thanks for clearing the geometric way of explaining the doubts. I wish to have clarity on the statistical idea. A standard reference with a detailed explanation would be of great help. A confusion regarding the scale and the shape exists though !,2013-10-18 18:15:19.307
104816,57704,22793.0,CC BY-SA 3.0,,That prooved to be an interesting idea but the interleaving zone is still quite problematic (in fact it gives worst results than just applying the ridge classifier directly on the dataset without any modification).,2013-10-18 18:17:22.260
105034,57895,503.0,CC BY-SA 3.0,,See the `effects` package in `R`,2013-10-20 22:09:20.420
104819,57758,594.0,CC BY-SA 3.0,,"student; ocram beat me to the punch, with an even more general result (though it's a consequence of the one I mentioned, since you can replace either $A$ or $B$ with arbitrary products and get the result that it's true for cyclic permutations)",2013-10-18 19:00:22.363
104820,57500,5237.0,CC BY-SA 3.0,,"Here are some links to related threads that should be helpful to read: for **linear** means: [What does linear stand for in linear regression?](http://stats.stackexchange.com/questions/8689/); for a simple example of a **non-linear regression**: [Linear regression best polynomial (or better approach to use)?](http://stats.stackexchange.com/questions/70153/); for what **parameters** are: [Is any quantitative property of the population a ""parameter""?](http://stats.stackexchange.com/questions/63386/)",2013-10-18 19:00:26.273
104821,57477,594.0,CC BY-SA 3.0,,Sorry to be dense for so long about what you're after; I will add some more detail to my answer.,2013-10-18 19:04:13.100
104822,57762,2081.0,CC BY-SA 3.0,,"@Ray, thank you for being attentive to spot a lapse.",2013-10-18 19:04:27.970
104823,57787,594.0,CC BY-SA 3.0,,Which three parameter gamma? I've seen more than one. Do you mean the one that's simply a shifted two parameter gamma?,2013-10-18 19:16:41.623
104824,57683,594.0,CC BY-SA 3.0,,"@mikepk How do you come to know $\mu_X$ and $\sigma_X$? If you do know them and can assume independence, then I don't see any difficulty with doing what you suggest.",2013-10-18 19:17:28.550
104825,57790,2081.0,CC BY-SA 3.0,,"This appears technical question, you must be doing something wrong via syntax or menu. I think the question should be moved to StackOverflow.",2013-10-18 19:21:10.373
104826,57792,2081.0,CC BY-SA 3.0,,Are you speaking of **median** absolute deviation?,2013-10-18 19:24:46.293
104827,57792,21884.0,CC BY-SA 3.0,,@ttnphns No. **Mean** absolute deviation error.,2013-10-18 19:28:07.663
104828,57788,15827.0,CC BY-SA 3.0,,I don't think it's a definition or characterisation of descriptive statistics that they aim for minimum loss of information. It's entirely possible to have descriptive statistics that leave out really important detail and that's often a problem.,2013-10-18 19:43:13.417
104829,57785,12683.0,CC BY-SA 3.0,,"Who considers the first order statistic to be a good initial estimate of a location parameter, & in what context? It would certainly be a very odd one for most situations.",2013-10-18 19:43:34.890
104830,57749,22817.0,CC BY-SA 3.0,,"Yeah, I could see that it is the median. But I am not being able to prove it",2013-10-18 19:44:47.230
104831,57749,594.0,CC BY-SA 3.0,,Did you do what I suggested with the derivative? What does it give you?,2013-10-18 19:59:56.197
104832,57793,1895.0,CC BY-SA 3.0,,"Use Fubini. Then, pause. Then, ask yourself why the argument doesn't work, in general, if $X$ can take both positive and negative values.",2013-10-18 20:01:12.267
104833,57793,594.0,CC BY-SA 3.0,,See discussion of the result [here](http://stats.stackexchange.com/questions/18438/does-a-univariate-random-variables-mean-always-equal-the-integral-of-its-quanti) and [here](http://math.stackexchange.com/questions/64186/intuition-behind-using-complementary-cdf-to-compute-expectation-for-nonnegative) and [here](http://math.stackexchange.com/questions/64186/intuition-behind-using-complementary-cdf-to-compute-expectation-for-nonnegative). I've only heard it expressed as 'the expectation is the integral of the survival function' ... rather than with any particular name.,2013-10-18 20:04:02.450
104834,57793,594.0,CC BY-SA 3.0,,"Incidentally, for continuous random variables, you can show it in two lines using integration by parts.",2013-10-18 20:14:56.983
104835,57791,674.0,CC BY-SA 3.0,,For readers who will read this response to the end I would suggest to add a brief take-away message (and to provide appropriate citation if it applies).,2013-10-18 20:30:04.420
104837,57790,5237.0,CC BY-SA 3.0,,"My first suspicion is that this is not a technical problem with SPSS, but a less frequent result of stepwise selection algorithms (see my answer below). I believe this question is on-topic on CV.",2013-10-18 20:34:57.023
104838,57586,1693.0,CC BY-SA 3.0,,"I wish I could follow it.  With laypeople I say I'm a statistician, but with statisticians I say I'm a...researcher.",2013-10-18 20:37:19.347
104839,57800,22843.0,CC BY-SA 3.0,,"Good point, but how do we know that the Cov(X,Y) is less than or equal to the product of the standard deviations of X and Y?",2013-10-18 20:38:47.910
104840,57744,12140.0,CC BY-SA 3.0,,"The purpose of a random seed is to initialize chip place & route algorithm. However, the algorithm itself is deterministic. That means if you run it multiple times with the same seed, you'd get the same result. Meaning of the result is not the efficiency, but the measure of how close it is to the given constraints. So the upper bound means that the algorithm meets or even exceeds given constraints. The lower bound can theoretically any negative value, but practically there is some.",2013-10-18 20:42:10.963
104841,57790,503.0,CC BY-SA 3.0,,I agree with @gung.  This question should stay open.,2013-10-18 20:47:58.973
104842,57800,19752.0,CC BY-SA 3.0,,"I've never actually proven myself, but some Googling brought up this page: http://www2.math.umd.edu/~ddarmon/teaching/stat400/correlation-proof.pdf",2013-10-18 20:51:04.827
104844,57779,668.0,CC BY-SA 3.0,,Stirling's approximation follows easily from taking the logarithms of the binomial coefficients and so is perhaps one of the simplest and most natural solutions possible.,2013-10-18 21:03:28.323
104845,57721,,CC BY-SA 3.0,user31676,"The book, Discrete Multivariate Analysis, by Bishop, Holland, and others, has some techniques for finding patterns in sequences.",2013-10-18 21:06:56.417
104846,57791,5661.0,CC BY-SA 3.0,,"With -2 votes so far, I think there's not much I can do to save it :)   I think the ending, where they all agree with each other, and admit they can use each others methods without worry about each others philosophy, is a 'take-away message'.",2013-10-18 21:45:23.860
104847,57791,5661.0,CC BY-SA 3.0,,"No citation required.  I just made it up myself.  It's probably not very well informed, it's based on my own (mis)-interpretations of arguments I've had with a small number of colleagues over the years.",2013-10-18 21:46:57.273
104848,57778,20473.0,CC BY-SA 3.0,,Look up the answer to this question : http://stats.stackexchange.com/questions/72857/derivation-of-conditional-distribution-from-other-two-distributions/72870#72870,2013-10-18 21:56:33.543
104849,57778,10547.0,CC BY-SA 3.0,,Thanks. This might be stuipid but would it not help to center $Y$ w.r.t $W$ then $Z:=Y-W$ will have expectation $0$ and variance $\sigma_y^2$,2013-10-18 22:09:51.457
104850,57802,6162.0,CC BY-SA 3.0,,"With `lm()`, you are using $\sqrt{\hat\sigma}$ instead of $\hat\sigma$.",2013-10-18 22:40:00.533
104851,57778,20473.0,CC BY-SA 3.0,,"Then you should also eliminate $W$ from the second exp which would make $Z$ appear in there too, having again your new integrating variable ($z$) present in the two exp's. You don't gain anything, really.",2013-10-18 22:54:36.663
104852,57778,10547.0,CC BY-SA 3.0,,"Ah sure, thats what I missed. I already presumed that this would not work... Why should centering solve a integral...",2013-10-18 23:01:59.887
104853,57779,,CC BY-SA 3.0,user30602,"Ok. I used Stirling's formula, but not by taking the logarithms of the binomial coefficients. Could you show me how?",2013-10-18 23:07:11.503
104854,57790,22841.0,CC BY-SA 3.0,,"Yes, I am using enter. The reference category is droped the 2nd level becomes the reference category.",2013-10-18 23:21:39.647
104855,57799,22841.0,CC BY-SA 3.0,,"That was my first guess, but is not the case… The crosstabs look good, I have missing cases but I have 17% of the cases in the level that is dropped. I also tried to change the reference category, the result is the same. I tried to change the measure (ordinal, nominal, scale), the same result...",2013-10-18 23:41:53.497
104856,57787,5448.0,CC BY-SA 3.0,,"The meaning of ""location parameter"" and ""scale parameter"" is independent of the distribution.  Also, what do you mean by ""structure of its density""?",2013-10-19 00:00:53.580
104857,57796,20991.0,CC BY-SA 3.0,,"thank's a lot for your answer, I want to understand what you did, so the cumulative distribution is the y1, or what is this line   y1 <- cumsum(y)*diff(x)[1]. So if my question is stupid  but I just start to study r 1 week ago,",2013-10-19 00:26:56.890
104859,57790,10060.0,CC BY-SA 3.0,,"I need to know more... did you use the `categorical` button to assign a variable to be categorical or did you make a series of dichotomous indicator by yourself and feed them into the model? If it's the earlier, pay attention after assigning the categorical variable, you can actually set your reference group to either the first or last of the coding scheme (if your desirable ref. group is at the middle then you'll need to recode). If it's the latter, then you should not feed a group of dichotomous indicators into stepwise... bad idea. use `block` button to test the whole categorical variable.",2013-10-19 00:39:56.783
104860,57807,10570.0,CC BY-SA 3.0,,"Replace the nodes of your graph with $Heat_t$, $PlateArea_t$, and $Friction_t$, where $t$ is the time step (0...N). The graph is now acyclic: $Heat_t$ points to $PlateArea_{t+1}$, $PlateArea_t$ points to $Friction_{t+1}$, and so on. The trick that makes this infinitely long network possibly tractable is the assumption that the parameters are the same across time, so all the lines connecting $Heat$ nodes to $PlateArea$ nodes have the same params (and so forth). [Page 430 of this](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch21.pdf) gives a good diagram for such a graph.",2013-10-19 00:43:56.230
104861,57749,22817.0,CC BY-SA 3.0,,I got the derivative something like this $\sum_{i=1}^{n}(-\frac{y_i-t}{|y_i-t|})$. I can't equate it to zero,2013-10-19 01:03:46.100
104862,57749,22817.0,CC BY-SA 3.0,,Also for the second question it is $\sum_{i=1}^{n}({|y_i-t|^{\infty}})$ not what you have written,2013-10-19 01:17:26.913
104863,57749,594.0,CC BY-SA 3.0,,"(i) *you don't equate it to zero*, as I already explained. (ii) I discussed the difference in my answer. I suggest you think carefully about what I wrote, and answer every question I have asked, in comments or in my answer that has not already been responded to.  You don't seem to be putting much thought into working with what information you've already been given.",2013-10-19 01:25:33.733
104864,57746,594.0,CC BY-SA 3.0,,"Note that CV is NOT a 'do my homework' site. Please add the self-study tag, and read its tag wiki info to what your responsibilities include.",2013-10-19 01:26:03.737
104865,57749,22817.0,CC BY-SA 3.0,,The answer is given by minimum subgradient the one with the least slope which is at the median,2013-10-19 01:29:47.470
104867,57749,22817.0,CC BY-SA 3.0,,The answer is given by minimum subgradient the one with the least slope which is at the median,2013-10-19 01:35:56.980
104868,57810,5237.0,CC BY-SA 3.0,,"This is not quite right. A *parametric* regression model need not be a ""straight"" line it simply needs to be a function of a finite number of parameters.",2013-10-19 02:05:46.487
104871,57810,22705.0,CC BY-SA 3.0,,"Got it, what I meant to explain was that - in parametric regression, we specify the functional form in terms of the # of parameters, which parameter. The simple straight line was an example. Did I get it right?",2013-10-19 02:38:31.123
104872,57744,1150.0,CC BY-SA 3.0,,"So, why are you assuming that the place & route algorithms have anything to do with the CLT?  It has to do with the random sampling of locations and how many starting positions will exceed your output constraints, whatever they happen to be.",2013-10-19 02:50:51.660
104873,57796,5875.0,CC BY-SA 3.0,,"Yes, y1 is the cdf!",2013-10-19 03:12:10.527
104874,57815,5237.0,CC BY-SA 3.0,,"Is this supposed to be an answer or a question? Moreover, here are a couple of thoughts: mixed models can be thought of as a limited version of SEM, & I'm not sure you couldn't satisfactorily deal w/ your situation w/ a hierarchical model.",2013-10-19 03:12:55.873
104875,57815,22705.0,CC BY-SA 3.0,,"I'm getting used to this place. it is an answer - highlighting what Bayesian can do which mixed cant. Yes, mixed is a limited version of SEM. (@gung, second statement-  I didn't understand)",2013-10-19 03:15:59.697
104876,57755,,CC BY-SA 3.0,,"I think ""average dip"" is good enough. It doesn't have the dimensions of acceleration, so it's certainly not anything to do with that.",2013-10-19 03:49:02.607
104877,57787,22703.0,CC BY-SA 3.0,,It refers to the probability density function,2013-10-19 04:42:47.020
104878,57785,22703.0,CC BY-SA 3.0,,"@Scortchi Whenever an iterative procedure is being used for parameter estimation, as the initial value for the location parameter is generally taken to be either the first order statistic, or a linear combination of the first few order statistic",2013-10-19 04:45:57.787
104879,57564,19559.0,CC BY-SA 3.0,,"@BenBolker: ha, yes, an error message like that would have been helpful!",2013-10-19 05:31:30.730
104880,57749,594.0,CC BY-SA 3.0,,"Think about simplifying $\sum_{i=1}^{n}(-\frac{y_i-t}{|y_i-t|})$, (hint: what is $\text{sign}(x)|x|$?) and look again at the plot and my discussion. The slope is actually undefined at the median.",2013-10-19 05:42:26.080
104882,57812,594.0,CC BY-SA 3.0,,What is the best approach depends on the conditional distribution of $y$; you might be looking at either nonlinear least squares (weighted or unweighted) or generalized nonlinear models. What are the y-values?,2013-10-19 05:53:12.507
104883,57791,674.0,CC BY-SA 3.0,,"I've seen such dialogue (shorter, though) in the past, and I find them interesting. I was also concerned by the downvotes, hence my suggestion to put a brief summary at the top so as to motivate readers to read the rest of your post.",2013-10-19 05:59:13.547
104884,57744,12140.0,CC BY-SA 3.0,,"On the contrary, I'm observing certain behavior that looks like CLT, and trying to disprove it. If I cannot disprove it, it has far reaching implication to what I'm trying to do.",2013-10-19 06:44:19.320
104885,57811,2081.0,CC BY-SA 3.0,,"It is unclear if you want to implement Discriminant analysis or Bayes classifier. DA first extracts the discriminants. Then it classifies (in a manner of a Bayes classifier) using _those_. If you need DA you ought to read more pages (including this site) about it, to stop being so `new to this field`.",2013-10-19 07:11:53.347
104887,57783,436.0,CC BY-SA 3.0,,"Thank you Enrique, TraMineR seems very interesting I will try it on Monday!",2013-10-19 07:23:37.097
104888,57798,2081.0,CC BY-SA 3.0,,"You may read [here](http://stats.stackexchange.com/a/30724/3277) that this formula reduces to the formula of the [cosine](http://stats.stackexchange.com/a/36158/3277) similarity, and _r_ [is the cosine](http://stats.stackexchange.com/a/22520/3277) for centered data.",2013-10-19 07:25:25.143
104889,57784,436.0,CC BY-SA 3.0,,"That is interesting Andy, thank you. I guess it would need some tweaking to be generalized to multiple groups, but I will see if I can come up with something. By the way, if you register a JSTOR account you can read the paper online for free.",2013-10-19 07:28:22.763
104890,57812,15827.0,CC BY-SA 3.0,,"Much overlap with http://stats.stackexchange.com/questions/59784/regression-for-a-model-of-form-y-axk Your data may be different, but in my experience additive errors are usually implausible for power functions. As in the thread cited, a power function is not best described as even a special case of of a polynomial.",2013-10-19 07:49:41.327
104891,57812,21762.0,CC BY-SA 3.0,,How do you handle the potentially strong overfitting? By external validation?,2013-10-19 08:20:41.567
104892,57816,,CC BY-SA 3.0,,The statement for |t|>2 will only be true if the degrees of freedom are large enough. Can you provide examples of papers that make a statement like that? Is it possible for you to use the `nlme` package instead of `lme4`?,2013-10-19 08:24:12.387
104893,57776,22833.0,CC BY-SA 3.0,,what if the item is not income and has small values!,2013-10-19 08:44:39.440
104894,57816,20120.0,CC BY-SA 3.0,,Is there a way to present confidence intervals (e.g. of the slopes)? Confidence intervals not encompassing 0 => rejection of nil-null.,2013-10-19 08:44:56.793
104895,57460,2081.0,CC BY-SA 3.0,,"@Ray, your picture is a viable explanation of the sign of a coefficient; it is like picture [here](http://stats.stackexchange.com/a/70910/3277), only 2D. But I don't see how it can explain _suppression_. To show suppression you must show error term because suppression is defined wrt it.",2013-10-19 08:50:26.363
104896,57319,2081.0,CC BY-SA 3.0,,"@rolando2, let me repeat it again, that coefficient's sign reversal in response to adding a new predictor does not necessarily makes that predictor a suppressor. And vice versa, adding a clear suppressor does not necessarily changes the sign. The title of your question remains ambiguous. Choose: either you ask about sign reversal or about suppressing effect. Or about when suppressing and sign reversal will coincide.",2013-10-19 09:20:52.163
104897,57791,5661.0,CC BY-SA 3.0,,"@chi, Interesting.  Maybe I'll stop worrying about the downvotes, and just work on improving it!  But not just yet.",2013-10-19 09:20:55.303
104898,57752,14799.0,CC BY-SA 3.0,,"I'm not clear on what you want. If you were to check all $\binom{n}{k}$ subsets, would you pick the subset with the smallest sum of squared correlations, where the sum is over the $k(k-1)/2$ within-subset correlations? Do the $k(n-k)$ correlations with the remaining $n-k$ items matter?",2013-10-19 09:21:51.867
104899,57802,22848.0,CC BY-SA 3.0,,Thanks Stéphane for the correction but it still doesn't seem to work,2013-10-19 09:31:51.913
104900,57802,,CC BY-SA 3.0,,try looking at the source code: `stats:::logLik.glm`,2013-10-19 09:45:54.467
104901,57790,22841.0,CC BY-SA 3.0,,"Hi, All my variables were recode to the reference category be 1 and most of my variables have three categories. I am using Enter with the following syntax. LOGISTIC REGRESSION VAR= ""DV""
/METHOD=ENTER ""IV's""
/CONTRAST (""IV"")=Indicator (1) [...]
/PRINT=GOODFIT CI(95) 
 /CRITERIA PIN(.05) POUT(.10) ITERATE(20) CUT(.5). Do you have idea what is going wrong?",2013-10-19 10:05:11.427
104902,57819,22705.0,CC BY-SA 3.0,,"Thanks for the response @Nick. From my edited question above, it might start to appear why a multiplicative log-log model wont satisfy my requirements. It cannot isolate the interaction effects between each pair of x (which can be isolated in an additive model). Further, a log-log model necessitates that y increases at a slower rate as x tends to infinity (large values).",2013-10-19 10:10:31.763
104903,57819,15827.0,CC BY-SA 3.0,,"Your original model doesn't (obviously) have interaction terms either; the answer in both cases is to add further terms to the model if justified. Your comment on ""diminishing impacts"" is cryptic, but powers necessarily being positive is not assumed or implied by either model.",2013-10-19 10:13:36.630
104904,57819,22705.0,CC BY-SA 3.0,,"I am not sure how adding an interaction term to a multiplicative model will help isolate the pair-wise interaction effects. Because, its already a multiplicative (has all pairwise multiplicative effects) model.",2013-10-19 10:17:08.653
104905,57802,22848.0,CC BY-SA 3.0,,I did this but this function just reverse the aic slot from the glm object to find back the log-likelihood. And I don't see anything about aic in the glm function...,2013-10-19 10:20:25.350
104906,57819,15827.0,CC BY-SA 3.0,,See edits above.,2013-10-19 10:22:34.977
104907,57819,15827.0,CC BY-SA 3.0,,"A power function can have powers $>1$, so your comment (if I understand your wprding) that $y$ necessarily increases more slowly with $x$ is quite incorrect.",2013-10-19 10:33:24.337
104908,57812,22705.0,CC BY-SA 3.0,,"How I'm handling over-fitting: Build a model with 75% of data. And, then fit the model with the same set of variables for each additional week of data. And, see how the coefficients for each variable vary across weeks. If the variance of coefficients is not very high, I conclude that the model is stable and responds well to new data/rejecting any fear of over-fitting.",2013-10-19 10:33:32.773
104909,57812,594.0,CC BY-SA 3.0,,"Sales may tend to be fairly skew (I mean the conditional distribution); you might want to consider a GLM (perhaps a gamma family), with a log-link. Alternatively, transformation may make the distribution less skew.",2013-10-19 10:40:08.717
104910,57819,22705.0,CC BY-SA 3.0,,"ok, I got it. The diminishing impact is not a concern in the log-log model. But, adding pairwise interaction terms (non log) as indicated in your edited equation, will not solve isolation of the pairwise interaction effects. Because, the product term (∑ J j= 1 b j lnx j) already has the interaction. Intuitively, adding more pair wise products doesn't seem to help isolate the interaction effects.",2013-10-19 10:44:00.027
104911,57776,21762.0,CC BY-SA 3.0,,Updated the response in this direction.,2013-10-19 10:53:09.070
104912,57819,15827.0,CC BY-SA 3.0,,"I can't comment in detail on how best to model interaction effects for your data. As before, your question doesn't clearly spell out how you think interaction works or how you would model interaction yourself. If you could be more explicit on that it would help any further comment.",2013-10-19 10:59:58.190
104913,57819,15827.0,CC BY-SA 3.0,,I should emphasise that I mentioned the multiplicative power function model because it is similar in spirit to what you were thinking about but much simpler to work with. In principle and in practice I have no way of knowing whether quite different models would work as well or better for your data. I support the suggestion from @Glen_b to consider a generalized linear model with log link.,2013-10-19 11:13:22.193
104914,57790,10060.0,CC BY-SA 3.0,,"The syntax looks fine. One possible reason is that the suspect variable has a very low count in level 1 to begin with. When other predictors were also added, the extra information might have caused the level 1 of the suspect variable to be perfectly predicted (aka perfect collinearity.)",2013-10-19 11:59:22.703
104915,57802,,CC BY-SA 3.0,,"I suspect this has something to do with LogLik and AIC (which are tied together at the hip) assuming that three parameters are being estimated (the slope, intercept, and dispersion/residual standard error) whereas the dispersion/residual standard error is calculated assuming two parameters are estimated (slope and intercept).",2013-10-19 12:04:37.267
104943,57834,1412.0,CC BY-SA 3.0,,"Wish I had more ability to upvote. One feature not discussed is ""blinding"" and the need to assess the success of randomization and stability of results under resampling.",2013-10-19 18:13:32.363
105035,57270,674.0,CC BY-SA 3.0,,"Julien, please visit our [Help Center](http://stats.stackexchange.com/help/merging-accounts) to merge your two unregistered accounts.",2013-10-20 22:11:08.787
104916,57709,22716.0,CC BY-SA 3.0,,"yes, of course your graph might look as if there might be some effects, but if they do not become significant, then they are not reliable (== they are 'not really there'). Often this becomes understandable if you plot the standard deviation/error around your data points. Large variance will prevent something that looks like an effect/interaction from becoming significant. In such a case, it is incorrect to say that 'there was an interaction in the plot'. Rather, you can say that what looks like an interaction was not significant (and should not be interpreted).",2013-10-19 12:04:47.207
104917,57823,22381.0,CC BY-SA 3.0,,is it possible to have loss less than 0?,2013-10-19 12:35:37.983
104918,57785,12683.0,CC BY-SA 3.0,,Certainly not true in general. Are you thinking of say a three-parameter Weibull model?,2013-10-19 12:36:49.273
104919,57823,15827.0,CC BY-SA 3.0,,"This is more a matter of convention than of logic. In practice, at least in my experience, loss functions are functions with zero or positive value. I don't think anything stops anyone calling something a loss function when that something might be negative. It's just like thinking that you want to minimise expenditure, but if you get some net income that is better yet than zero expenditure.",2013-10-19 12:45:35.977
104920,57749,22817.0,CC BY-SA 3.0,,The answer is when it is at minimum subgradient which is the median point at the point where sign(x)|x| changes sign,2013-10-19 13:26:53.450
104921,57578,16144.0,CC BY-SA 3.0,,"I used 'orthog' in Stata to orthogonalize X and X^2. I have a couple of questions about the orthogonalized variables: 1) How do you decide if you want to orthogonalize X with respect to X^2, or vice versa? 2) The centered variables have a different effect in my model than the orthogonalized variables. How do I decide which one to chose? 3) After orthogonalization, is there any interpretation possible of the beta values of these variables?",2013-10-19 13:38:13.813
104922,57749,594.0,CC BY-SA 3.0,,"Wrong both times, I'm afraid. Forget about your problem and actually *evaluate*  sign(x)|x| for various values of $x$. Then explain how to simplify your $f'$ equation from a few comments up. Then draw it for a small sample.",2013-10-19 13:57:37.003
104923,57822,594.0,CC BY-SA 3.0,,"Loss *could* be monetary, but is far more general. You could think of it as something akin to utility, but it doesn't have to be actual utility, or even very much like it. It's just some specifies measure of the 'badness' of the outcome in some sense.",2013-10-19 14:03:26.127
104924,57749,22817.0,CC BY-SA 3.0,,The gradient is given by sign(x)|x| itself which is undefined at the points where t= the given points.,2013-10-19 14:18:20.963
104925,57287,5875.0,CC BY-SA 3.0,,I simply don't understand the question...,2013-10-19 14:28:27.773
104926,57749,594.0,CC BY-SA 3.0,,"Like I said, **actually evaluate sign(x)|x| for various values of x**. What do you notice about sign(x)|x|? Hence, what is x/|x|?",2013-10-19 14:39:35.500
104927,57826,5237.0,CC BY-SA 3.0,,"Welcome to the site, @teddypicker. I have taken the liberty of formatting your question with the $\LaTeX$ that the site affords. Please ensure it still says what you want it to. Also, could you edit your last paragraph? I cannot quite parse those sentences (& what does ""sth"" mean?).",2013-10-19 14:48:29.480
104928,57826,503.0,CC BY-SA 3.0,,@gung You are a little faster than I am!,2013-10-19 14:56:49.557
104930,57688,,CC BY-SA 3.0,,Closely related: http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models and,2013-10-19 15:25:44.153
104931,57790,22841.0,CC BY-SA 3.0,,"True, ""very low count in level 1"". In this analysis I had quasi-complete separation with other variables (very large S.E.). I recoded them and add this ""suspect variable"" as a proxy of one that I excluded. Can I use the odd ratios as they are (without the same reference category that I have in other countries?) and report that in a footnote? If not, Is it too sloppy to drop the variable just for the two countries here I am having problems?Thanks for your help!",2013-10-19 15:30:40.327
104932,57821,19559.0,CC BY-SA 3.0,,"thanks! This is useful. In my study I have ~1500 observations per condition, so I don't know if I would fit Baayen's criteria. I would like to use confidence intervals if I can, but I usually get my confidence intervals from the output of lmer (calculated through MCMC also) and that is again not an option when my mixed-effects model has the random correlation paramenters between intercept and slopes",2013-10-19 15:34:23.787
104933,57757,22823.0,CC BY-SA 3.0,,"By ""maps"" I mean 2D data that have geographic interpretation. I'm not a statistician but I think that what you're proposing will help to find some patterns inside these maps and not help me to compare one with another to find their (dis)similarity. Maybe I was not clear enough in my post so I'll update it with some more info.",2013-10-19 15:37:00.157
104934,57816,19559.0,CC BY-SA 3.0,,"@mark999: sure, for example, see [Vasishth, Brüssow, Lewis, Drenhaus, (Cognitive Science, 2008] (http://www.ncbi.nlm.nih.gov/pubmed/21635350). They only present t-values in the tables and say below Table 5: ""T scores with absolute values greater than 2 are statistically significant."" (page 704).",2013-10-19 15:48:08.580
104935,57815,3922.0,CC BY-SA 3.0,,"I think this is wrong. Anything Bayesian is just a fancy version of likelihood. If likelihood fails, Bayesian fails. If correlation between errors leads to biased frequentist estimates, no amount of Bayesian computational trickery will rectify that. On the other hand, Bayesian versions of instrumental variables that can deal with correlated measurement error issues are difficult to set up, unless you want to do non-parametric modeling of the error term distribution.",2013-10-19 15:52:09.443
104936,57821,20120.0,CC BY-SA 3.0,,"@SolLago: I've update the quote, it seems you're in the clear. Do read the paper by Baayen et al. though.",2013-10-19 16:01:53.283
104937,57828,16174.0,CC BY-SA 3.0,,"@SolLago, I just edited the answer. Its author is Jeff Long (say thanks to him). :)",2013-10-19 16:06:45.403
104938,57816,20120.0,CC BY-SA 3.0,,"In Table 5 of that paper, they also give the HPD, and it's obvious that **iff** the upper and lower edge of the HPD have the same sign, |t| > 2. (In fact, |t| > 1.) .. So basically, report HPDs.",2013-10-19 16:07:44.050
104939,57816,19559.0,CC BY-SA 3.0,,"@jona: yes, you are right. The problem is that the functions that would calculate HPD intervals (mcmcsamp, pvals.fnc) are all not implemented in R for models with random correlation parameters. So I can't use those and I don't know how to get them otherwise. Maybe I should ask this as a separate question in case anyone has suggestions?",2013-10-19 16:28:11.237
104940,57615,2149.0,CC BY-SA 3.0,,Yes there is a good method to develop this structure. It is a combibation or ARIMA and regressor variables which can include all that I mentioned above. If tou wish you can contact me at my email address and I will try and give you more details. Just click on my name and you can get my email address. Alyternatively post your data and I will analyse it and show you precisely what can be done. Do not post the cumulative but do post the actual daily values.,2013-10-19 16:37:34.733
104941,57838,21762.0,CC BY-SA 3.0,,"Thanks@GregSnow for the nice answer. There is one point that needs some attention: According to Gauss-Markov, equal variance is required for BLUE.",2013-10-19 17:20:45.633
104942,57839,7155.0,CC BY-SA 3.0,,It's not clear what you're asking. If you correct the formatting you're more likely to get an answer.,2013-10-19 17:58:23.697
105072,57875,21762.0,CC BY-SA 3.0,,Hint: think about t-test's assumpion of independent observations.,2013-10-21 05:49:32.933
104945,57818,1412.0,CC BY-SA 3.0,,Please don't capitalize R package names if that is not their correct spelling. You have not addressed the key point (sample size) in whuber's response to the question you linked to.,2013-10-19 18:42:11.473
104946,57775,1412.0,CC BY-SA 3.0,,Forgot to include the beta coeff.,2013-10-19 18:48:10.110
104947,57842,6162.0,CC BY-SA 3.0,,By the way you have to similarly be careful with the REML/ML option for lme/lmer models.,2013-10-19 18:51:14.350
104949,57811,22836.0,CC BY-SA 3.0,,"I see, thanks, I was not sure how to convert sentences into numbers...",2013-10-19 19:45:50.083
104951,57807,22792.0,CC BY-SA 3.0,,"(1) Does this imply that causal systems always do evolve in time somehow? Because a Bayesian Network represents a joint probability distribution over the variables corresponding to its vertices, for example $P(Heat,PlateArea,Friction)$ if we do not take the time $t$ into account and this does not tell us anything about the chronological order. Let's assume that we start this hypothetical machine, wait for $t$ amounts of time and at time $t$ we instantaneously sample the system and get measurements for Heat, PlateArea and Friction.",2013-10-19 20:07:20.547
104952,57842,17249.0,CC BY-SA 3.0,,(+1) Is it n-1 or is it indeed n-2 in the denominator of $\hat\sigma$ ?,2013-10-19 20:07:29.477
104953,57807,22792.0,CC BY-SA 3.0,,"(2)Lets assume again that we repeat this experiment (wait $t$ amount of time and sample instantaneously) many times more and we obtain a list of (Heat,PlateArea,Friction) measurements. Assuming all this experiments have been conducted independently, how we can set the causality relationships then? For just at an instant of time, where no chronological ordering exists, I cannot think of a causality relationship to build a Bayesian Network. So, what I am trying to understand is, do we need ""time"" to build Bayesian Networks, then? @StumpyJoePete",2013-10-19 20:14:37.593
104954,57814,22792.0,CC BY-SA 3.0,,"I have some questions about the usage of time for building a Bayesian Network, I added these as extra comments under my first question. @Learnerbeaver",2013-10-19 20:21:45.207
104955,57725,13549.0,CC BY-SA 3.0,,"Looking into it more, your suggestion would have worked well Greg if I had better prior information. Thanks.",2013-10-19 20:23:37.837
104956,57842,6162.0,CC BY-SA 3.0,,@PatrickCoulombe No : intercept + slope,2013-10-19 20:28:45.437
104957,57839,594.0,CC BY-SA 3.0,,"Please check and edit your question. The answer to your title question is 'yes' (if A and B are the right shapes for the products to be square), but the body contains errors and is inconsistent with the title. Which standard references? I expect you've made a copying error in one of those terms. For $n\times 1$ vectors, a common manipulation would go $x^Tx = \text{tr}(x^Tx) = \text{tr}(xx^T)$ or something along those lines. [In my notation $\text{tr}$ is trace.]",2013-10-19 20:52:01.437
104958,57846,6162.0,CC BY-SA 3.0,,What is the deterministic example you have in mind ? I don't see what could be interpreted as a deterministic analogous of a divergence between two distributions.,2013-10-19 20:57:34.680
104959,57848,22864.0,CC BY-SA 3.0,,Thanks for the response. That makes sense. I'm looking for a way then to identify the variables that best predict a certain vulnerability score. For example if I know a fisherman fish with a partner is that a strong predictor of where they lie on a vulnerability scale? I find in my data when I go over the raw data that on average fishermen who fish with a partner have lower vulnerability score. Is there an analysis I can do to show that across all variables?,2013-10-19 20:57:58.057
104960,57848,503.0,CC BY-SA 3.0,,"But you already used that variable in creating the index, so you already know how good a predictor it is. It is as good a predictor as you made it.",2013-10-19 20:59:58.207
104961,57830,594.0,CC BY-SA 3.0,,"Using the notation of that page, $D_n$ is often regarded as 'the test statistic' (as shown [in this image on the same page](http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/KS_Example.png/300px-KS_Example.png)), but in the discussion you quote, $\sqrt n D_n$ is being used as a test statistic. Obviously the two statistics would be equivalent (lead to rejection/non-rejection of exactly the same cases).",2013-10-19 21:00:54.413
104962,57846,22863.0,CC BY-SA 3.0,,"In the deterministic case, I'm thinking of having two non-stochastic vectors of finite dimension and I use any norm to measure how far away they are from each other. Then divide this difference by the norm of either one to get a relative difference. I know I can't think of distributions in the same manner (finite dimensional vectors) but is there an analogous relative divergence.",2013-10-19 21:14:00.237
104963,57835,594.0,CC BY-SA 3.0,,"You can still estimate the coefficients that way; the parameter estimates are still consistent, but their standard errors *aren't*. Any hypothesis tests, confidence intervals and prediction intervals are not 'valid'.",2013-10-19 21:23:27.223
104964,57831,594.0,CC BY-SA 3.0,,One rule of thumb: use enough points that it looks like a smooth curve when you plot it.,2013-10-19 21:25:53.143
104965,57842,22848.0,CC BY-SA 3.0,,"Ok, perfectly clear now. Thanks a lot ! But what do you mean with REML/ML (something to do with my last post on GuR I guess) ? Please explain (there maybe). I want to learn !",2013-10-19 21:26:55.657
104966,57460,14799.0,CC BY-SA 3.0,,"@ttnphns My definition of suppression needs only the betas. However interesting it would be to discuss what definitions of suppression might be most useful for what purposes in what situations, it would also be against Stack Exchange policy, so I guess we're going to have to agree to disagree here.",2013-10-19 21:31:52.047
104967,57842,6162.0,CC BY-SA 3.0,,"The REML estimates of the variance components in a mixed models are like the ""corrected for bias"" ML estimates. I have not seen your post on GuR yet :)",2013-10-19 22:17:31.163
104968,57851,594.0,CC BY-SA 3.0,,"The population of 'coins in circulation' and 'coins that appear in my change' would need to be the same, at the right relative frequencies. Why would this be the case?",2013-10-19 22:24:03.710
104969,57854,21762.0,CC BY-SA 3.0,,"I guess its about 'first order' approximations of something, not about order statistics.",2013-10-19 22:37:45.027
104970,57785,668.0,CC BY-SA 3.0,,"I wonder, Vani, whether we might have a communication problem here.  Most people would understand a ""first order statistic"" to be the minimum (or sometimes the maximum) of a dataset.  I hope it's obvious that is a terrible estimate of a location parameter except under very special assumptions such as those described in Scortchi's answer.  Would you perhaps have something else in mind here, such as the median?  Could you clarify this point?",2013-10-19 22:38:28.783
104971,57851,1895.0,CC BY-SA 3.0,,Related: [Canadian mint circulation currency](http://www.mint.ca/store/mint/learn/circulation-currency-1100028). You can click through to get the quantity minted for each coin by year.,2013-10-19 22:40:00.603
104972,57838,2873.0,CC BY-SA 3.0,,"@MichaelMayer, you are right, I remembered wrong. will fix.",2013-10-19 22:42:21.203
104973,57854,12683.0,CC BY-SA 3.0,,"@Michael: Might be, but something in the phrasing of the comment triggers old memories of reliability analysis & shifted failure-time distributions.",2013-10-19 22:53:40.177
104974,57785,594.0,CC BY-SA 3.0,,OP: please clarify the meaning of your question. It seems like you must either be misusing some terms or that you refer to a very particular context; in either case you can't hope for a very satisfying answer until the miscommunication is dealt with.,2013-10-20 00:03:55.427
104975,57855,22865.0,CC BY-SA 3.0,,I believe the 'discrete' description refers to the fact that draws from a Dirichlet process are discrete with probability one (it follows from the stick breaking representation of the DP).,2013-10-20 00:20:01.463
104976,57855,594.0,CC BY-SA 3.0,,"You're going to have to elaborate. If I break a stick into $k$ pieces in some fashion, the distributions of the stick lengths are continuous.",2013-10-20 00:22:27.917
104977,57595,5448.0,CC BY-SA 3.0,,"For part 2, does $\text{E}f(x) = f(\text{E}x)$ in general?  Also consider the case of a simple random walk on the integers which is currently located at 0.",2013-10-20 01:54:39.177
104978,57849,5448.0,CC BY-SA 3.0,,Consider any $t < \infty$. Is $\text{Var}(y_{t+1}) = \text{Var}(y_t)$?,2013-10-20 02:04:34.323
104980,57855,3183.0,CC BY-SA 3.0,,"@Glen_b: Your intuition matches mine, but the paper ankit linked to says ""that draws from a DP are discrete (with probability one)"".  I can't follow their argument, but I respect the authors.",2013-10-20 03:02:19.153
104981,57821,9049.0,CC BY-SA 3.0,,+1 to your answer jona. As a somewhat general comment: I would question the realistic difference of a $t$-distribution with 600+ d.f. and a Gaussian... Those $t$-values should look awful lot like $z$-values.,2013-10-20 04:50:19.583
104982,57744,1150.0,CC BY-SA 3.0,,"I should have been more precise, it has to do with your *initial random seed*. Your routing algorithms, the distribution of solutions, and the limits of the problem space are primarily reflected in the overall shape of the distribution (which happens to be normal in your case).",2013-10-20 04:54:54.363
104983,57858,22372.0,CC BY-SA 3.0,,"Thanks for the answer, but, another question rises, should the residuals be closer or far to each other? Should I try some manipulation in my variable to fix something I saw, like combining variables?",2013-10-20 05:33:22.490
104984,57855,594.0,CC BY-SA 3.0,,"@DavidJ.Harris yes, reading up about it, it seems - inconsistently with the way the word 'process' is more usually associated with distributions - to be referring to what I'd have called something like a 'multinomial process' or 'multinomial mixture', since the output is the category. (This naming scheme would be kind of like referring to inter-event times as a 'Poisson process', rather than the count of the number of events as is normally the case, or perhaps referring to a Bernoulli process as a 'beta process' because there was a beta prior on the Bernoulli probability.)",2013-10-20 06:09:09.943
104985,57861,594.0,CC BY-SA 3.0,,"The use of the phrase 'relative to' there is critical to the meaning. Because of that, it doesn't refer to the population mean but to the difference between the population and sample mean. Beware, however - I see at least one error in that article.",2013-10-20 06:22:19.783
104986,57648,6204.0,CC BY-SA 3.0,,I'm a little confused about what you are looking for in an answer. A citation to a paper that uses a conditional multinomial logit in a recommender setting?,2013-10-20 06:30:25.223
104987,57814,22705.0,CC BY-SA 3.0,,"Whether to use time or not should be dictated by your business hypothesis. Just to get some additional clarity, you could view the chapter on probabilistic graphical models by daphne koller in coursera.org. Might help.",2013-10-20 07:05:44.097
104988,57815,22705.0,CC BY-SA 3.0,,"Interesting point @StasK. Have never thought of correlating prediction errors in a Bayesian hierarchical model. But, am not sure if it's right - to correlate residuals determined by the Bayesian approach and determine if error correlation exists.",2013-10-20 07:13:48.683
104989,57855,,CC BY-SA 3.0,,"It depends on whether you think a ""countably infinite"" number of real numbers is representative of the real numbers.  I would have thought that it is, thus providing an argument against the above claim.",2013-10-20 09:04:02.507
104990,57856,,CC BY-SA 3.0,,Why is it bad to estimate a density by a discrete distribution?  Does this mean quadrature is also bad and inappropriate?,2013-10-20 09:05:14.777
104991,57833,15827.0,CC BY-SA 3.0,,"This makes out ANOVA to be a testing procedure and regression to be a modelling procedure in which you can carry out tests. But ANOVA also has an underlying model, regardless of whether this is emphasised in all introductory treatments. So, this answer does not capture any difference between them. Nor is it addressed at the question, which is why they are taught as different regardless of strong similarities.",2013-10-20 09:45:39.777
104992,57818,22524.0,CC BY-SA 3.0,,I have edited the question.,2013-10-20 11:56:06.347
104993,57775,306.0,CC BY-SA 3.0,,extremely sorry for the error have updated the value.,2013-10-20 13:20:05.780
104994,57784,750.0,CC BY-SA 3.0,,"@nico - I'm not sure what you mean by generalizing to multiple groups. I gave an example for runs in three groups, but the logic applies to more. (The typical runs test in most software I've seen only allows two, but the cited papers establishes the test statistic for multiple groups. That is why I elaborated with an example with 3 groups.)",2013-10-20 13:29:22.973
104995,57784,436.0,CC BY-SA 3.0,,"Sorry, what I meant is that the test analyses runs of binary events (success/failure), while I have more than two levels in my group variable.",2013-10-20 13:40:29.013
104996,57870,17740.0,CC BY-SA 3.0,,It's not entirely clear what you are asking. Surely you had a reason to use PCA? What are you trying to learn? Did you use PCA without knowing what it does?,2013-10-20 13:40:37.077
104997,57870,22872.0,CC BY-SA 3.0,,"Honestly, I was hoping that there would be two or three components and that certain industry sectors would correlated with each other. I don't have a pressing reason to do a PCA, other than curiosity.",2013-10-20 13:46:57.597
104998,57836,20120.0,CC BY-SA 3.0,,"Note that a confidence interval/CI is not the same as a Highest Posterior Density/HPD interval, or a Bayesian Credible Interval.",2013-10-20 13:50:29.420
104999,57873,22872.0,CC BY-SA 3.0,,"Can you please tell me more about the red lines on the biplot. Some of them appear to be grouped together. Does this mean anything. For example, energy, resources and materials appear to be going in a similar direction.",2013-10-20 14:06:15.987
105000,57875,503.0,CC BY-SA 3.0,,"Welcome to the site. If this is a homework question, please add the ""self-study"" tag. See [homework questions](http://meta.stackexchange.com/questions/10811/how-do-i-ask-and-answer-homework-questions/10812#10812)",2013-10-20 14:13:43.400
105001,57856,7007.0,CC BY-SA 3.0,,"I didn't say it is ""bad"". But suppose that you have good prior information about the smoothness of the random density. You can't use this prior information if you are modelling with the plain DP. That's the kind of thing that I have in mind.",2013-10-20 14:14:13.493
105002,57874,503.0,CC BY-SA 3.0,,"This certainly looks like a homework question. Please add the ""self-study"" tag. See [homework questions](http://meta.stackexchange.com/questions/10811/how-do-i-ask-and-answer-homework-questions/10812#10812)",2013-10-20 14:14:44.530
105003,57874,22752.0,CC BY-SA 3.0,,@PeterFlom I've added the tag. It's not homework but part of my exam-preparation.,2013-10-20 14:20:10.577
105004,57872,9049.0,CC BY-SA 3.0,,"jona, your train of thought is correct but take notice two things: 1. What you described in $non$-parametric bootstrap, not ""simple"" bootstrap; there is an inherent bias-variance trade-off between the two. 2. You need to be a bit careful how you resample your sample. You may accidentally end up missing a grouping especially if you have a lot of clusters. That is not ""the end of the world"" and would asymptotically ""not happen"" but this might mess-up your calculation procedures slightly.",2013-10-20 14:34:37.663
105005,57872,20120.0,CC BY-SA 3.0,,"Regarding 2, I agree - it's just an example that would need to be adapted to the individual model. (You might get by simply sampling from the grouping variables too.) Regarding 1 - I don't understand, can you elaborate?",2013-10-20 14:41:42.403
105006,57876,503.0,CC BY-SA 3.0,,"Welcome to the site. Since this looks like a class assignment, please add the ""self-study"" tag. See [homework questions](http://meta.stackexchange.com/questions/10811/how-do-i-ask-and-answer-homework-questions/10812#10812)",2013-10-20 14:46:22.830
105007,57873,12808.0,CC BY-SA 3.0,,I am not very familiar with R and never have I used  the vector representation. The R documentation tells the following about the direction of arrows: http://cc.oulu.fi/~jarioksa/softhelp/vegan/html/biplot.rda.html,2013-10-20 14:56:51.470
105008,57872,9049.0,CC BY-SA 3.0,,"With any bootstrapping technique the simulations get processed just like the real data. With non-parametric bootstrapping (what you described) you resample your original data. With parametric bootstrapping you simulate a new sample based on the original model you fitted. Non-parametric btsp. makes less assumptions but usually has more variance. Parametric btsp. assumes that the model you fit is ""correct"", so it makes more assumptions, but it usually has less variance. Param. btsp. also eliminate issues regarding the resampling. (Cont.)",2013-10-20 15:01:44.250
105009,57548,10547.0,CC BY-SA 3.0,,"For a more general case: $Y|X=x \sim N(x,\sigma_x^2)$ and $X\sim N(\mu_x,\sigma^2_x)$ I've $\beta = \frac{2\sigma_x^2 + 2\sigma_y^2}{\sigma_y^2\sigma_x^2}$ and $\gamma = \frac{-4\sigma_x^2 y - 4\sigma^2_y\mu_x}{4\sigma_y^2\sigma_x^2}$. It seems that $\int_{-\infty}^0 \text{exp}\{-\frac{x^2}{4\beta}-\gamma x\}dx = \int_{0}^\infty \text{exp}\{-\frac{x^2}{4\beta}-\gamma x\}dx$. With this I'll get smth. like a density multiplied by the $\text{erf}$. But since $\text{erf}$ integrates over $\gamma\sqrt{\beta}$ which depends upon $y$ how am I supposed to find the expected value and variance of $y$?",2013-10-20 15:03:13.473
105010,57872,9049.0,CC BY-SA 3.0,,"Given the fact you are making parametric assumptions to start with, when you fit your original model you might as well use them and get a better estimate (ie. if you don't believe the model why bootstrap it anyway). You have the correct idea; I just want to highlight though that there is a trade-off between the non-parametric bootstrap you outlined and the parametric bootstrap that `lme4`'s native `bootMer()` function offers.",2013-10-20 15:04:28.200
105011,57548,10547.0,CC BY-SA 3.0,,"Short: $\text{erf}(x) = \frac{2}{\sqrt{\pi}}\int_0^x \text{exp}\{-\tau^2\}d\tau$. Because $\tau = \gamma\sqrt{\beta} = \text{f}(y,\mu_x,\sigma_x^2,\sigma_y^2)$ the $\text{erf}$ part seems rather complicated.",2013-10-20 15:13:05.240
105012,57876,,CC BY-SA 3.0,,"Asking if some data “are significant” is not a well-defined problem. If you are doing a test at all (it's not the only way to analyze quantitative data), you would typically test a specific hypothesis, perhaps compare several subgroups or test if there is a particular relationship between several variables. You need to tell us what you are trying to find out in this study.",2013-10-20 15:13:41.787
105013,57850,22092.0,CC BY-SA 3.0,,Thank you for your kind help.I add a figure got from the book Numerical Recipes.There are questions remained.Please take a look at my updated post and forgive my retard questions.,2013-10-20 15:32:59.477
105014,57876,22874.0,CC BY-SA 3.0,,I am trying to find out which group of teachers uses social media in their classrooms - private versus public,2013-10-20 16:06:24.913
105015,57879,22874.0,CC BY-SA 3.0,,""" Is it a typo of ""t-test or ANOVA""?"" yes - this was a typo - my apologies   - as you suspected it should read ""t-test or ANOVA""",2013-10-20 16:07:43.497
105016,57879,22874.0,CC BY-SA 3.0,,I am trying to find out if there is a difference in teachers who use social media in their classrooms when comparing public and private school settings; Also am interested to see the relationship between teachers who use social media personally and in their classroom ; In reviewing my notes for this class I believe the professor shared the T test and ANOVA as suggestions to use in determining statistical significance - I am hoping to find a tool which will help me evaluate the data I have collected with regard to accuracy and sample size,2013-10-20 16:14:23.627
105017,57876,22874.0,CC BY-SA 3.0,,what other tools are used to analyze quantitative data?,2013-10-20 16:15:35.023
105018,57872,19559.0,CC BY-SA 3.0,,"@jona: Thanks! And thanks for taking the time to explain this as well. I am comfortable with bootstrapping (much more than with mixed effects models anyway) and I usually use the **boot** package in R. And thanks to user11852, now I know that I can also use bootMer() as well. Thanks both!",2013-10-20 16:41:09.277
105019,57870,2081.0,CC BY-SA 3.0,,"@CuriousCat, Be curious and search this site for ""pca biplot"". There is also tags `pca` and `biplot`.",2013-10-20 17:03:13.397
105020,57548,20473.0,CC BY-SA 3.0,,"$\int_{-\infty}^0 \text{exp}\{-\frac{x^2}{4\beta}-\gamma x\}dx = \int_{0}^\infty \text{exp}\{-\frac{x^2}{4\beta}+\gamma x\}dx$. Use the relation of erf with the standard normal cdf, and we'll take it from there.",2013-10-20 17:19:52.123
105021,57879,22874.0,CC BY-SA 3.0,,is the 2x2 table of results the same as a chi square?,2013-10-20 18:22:36.123
105022,57879,22874.0,CC BY-SA 3.0,,"I came across information about using fishers exact test when analyzing categorical(nominal) variables - this seems to fit my data, but as the tool is new to me wanted to gain some support for this idea",2013-10-20 18:24:26.227
105023,57849,,CC BY-SA 3.0,user30490,Awesome!  That makes perfect sense :),2013-10-20 18:38:22.287
105024,57883,14799.0,CC BY-SA 3.0,,You want the Fisher-Irwin test. See Ian Campbell's [website](http://www.iancampbell.co.uk/twobytwo/twobytwo.htm) for the details and data to support the recommendation.,2013-10-20 19:41:57.500
105025,57889,22877.0,CC BY-SA 3.0,,observed OR is 2 statistically significant different than the null value(using 2 sided type 1 error rate of 0.05),2013-10-20 20:02:43.120
105026,57887,594.0,CC BY-SA 3.0,,"*Both* variables are ordered. An ordinary chi-square will potentially be throwing away a lot of power, though it depends on the exact hypotheses you're most interested in.",2013-10-20 20:24:46.710
105027,57864,22868.0,CC BY-SA 3.0,,"Thanks. What about the bit that says ""the t-distribution can be used to estimate how likely it is that the true mean lies in any given range.""  This makes it sound like we should be able to take a sample, then for any range [A,B] we choose, calculate Prob(A<mu<B).",2013-10-20 20:46:05.980
105029,57893,668.0,CC BY-SA 3.0,,"Your formulas are inconsistent: for instance, in part (b), $F$ is not the quantile function for $f$. ($F$ assigns zero probability to negative numbers whereas $f$ is symmetric around $0$.)  This leaves the question indeterminate.  Please edit your question to fix up the mathematical problems or at least provide more information so that the question becomes unambiguous.",2013-10-20 21:29:50.693
105031,57895,594.0,CC BY-SA 3.0,,One possibility: Added variable plots,2013-10-20 21:49:43.393
105032,57895,674.0,CC BY-SA 3.0,,Of possible interest as well: [Predicted by residual plot in R](http://stats.stackexchange.com/q/18234/930).,2013-10-20 22:00:29.763
105033,57882,594.0,CC BY-SA 3.0,,"This looks like standard bookwork (especially given the start '*You conduct...*' - I don't!). If this is for some subject, or even just for your personal study, please add the `self-study` tag and [read its tag wiki info](http://stats.stackexchange.com/tags/self-study/info)",2013-10-20 22:01:59.383
105037,57886,674.0,CC BY-SA 3.0,,"While you headed and tagged this question as related to propensity scores, your two questions look at odd to me: Binary classifiers are one thing, the use of [tag:propensity-scores] with binary outcome is another topic. Are you asking whether logistic regression is better than, say, neural networks in deriving PSs?",2013-10-20 22:24:41.677
105038,57883,674.0,CC BY-SA 3.0,,"Of possible interest (in line with @Ray's comment): [Chi-square test for equality of distributions: how many zeroes does it tolerate?](http://stats.stackexchange.com/q/4023/930), [Yates continuity correction for 2 x 2 contingency tables](http://stats.stackexchange.com/q/4569/930).",2013-10-20 22:27:53.003
105039,57890,6630.0,CC BY-SA 3.0,,Can you write out your $f(X|\theta)$ and $g(\theta)$ first?,2013-10-20 22:36:53.900
105040,57896,6630.0,CC BY-SA 3.0,,"please define $r$, $S_x$ and $S_y$.",2013-10-20 22:37:47.360
105041,57826,4656.0,CC BY-SA 3.0,,What is $Y_1-Y_2$ since $Y_1$ and $Y_2$ are vectors of different dimensions?,2013-10-20 22:58:10.957
105042,57538,20473.0,CC BY-SA 3.0,,"You can now find a shorter way to calculate your integral, here: http://stats.stackexchange.com/questions/73157/calculation-of-an-unconstrained-normal-distribution-starting-from-a-censored/73327#73327",2013-10-20 23:41:03.453
105043,57880,5448.0,CC BY-SA 3.0,,It seems to me you'd only be losing important information if the probability of E varied with the duration and you were going to model it as a function of duration.  Explaining why you even care about duration might help...,2013-10-21 00:10:20.763
105044,57894,2081.0,CC BY-SA 3.0,,"In the current state this is sooner a discourse with highly uncommon terminology than a question. I'm, particularly, in difficulty to make head or tail of it. Can you make the story clearer?",2013-10-21 00:24:46.813
105045,57898,594.0,CC BY-SA 3.0,,The criteria are yours to choose. Why would you declare any observation to be an outlier?,2013-10-21 01:06:55.220
105046,57904,594.0,CC BY-SA 3.0,,This looks like standard bookwork. Please add the `self-study` tag and read its [tag wiki info](http://stats.stackexchange.com/tags/self-study/info),2013-10-21 01:07:21.473
105047,57907,594.0,CC BY-SA 3.0,,http://en.wikipedia.org/wiki/Indicator_function,2013-10-21 01:22:06.690
105049,57897,1895.0,CC BY-SA 3.0,,Are those mutually exclusive events? :-),2013-10-21 01:32:23.763
105050,57898,19870.0,CC BY-SA 3.0,,Ok thanks. I just looked at some text books and found that the IQR is one of the myriad of methods to spot an outlier.,2013-10-21 02:13:34.070
105051,57905,5237.0,CC BY-SA 3.0,,"If you think @stat's answer is good, you might *upvote* it... ;-)",2013-10-21 02:14:46.470
105052,57900,19870.0,CC BY-SA 3.0,,"Thank you, what I understood is that the criterion can be changed according to the data.",2013-10-21 02:15:53.333
105053,57905,3993.0,CC BY-SA 3.0,,"@gung Haha, yes of course, thanks for the reminder",2013-10-21 02:18:16.960
105054,57688,22426.0,CC BY-SA 3.0,,"Very helpful, thanks! I hadnt seen the other thread.",2013-10-21 02:27:50.043
105055,57905,22843.0,CC BY-SA 3.0,,Could you explain how do they just drop out and give you 0? Here's the article you asked: http://ensign.editme.com/linearregression,2013-10-21 02:38:07.357
105056,57898,594.0,CC BY-SA 3.0,,It's really your model (at least in the loose sense) that makes an outlier an outlier (because what is an outlier but something that 'doesn't fit'?),2013-10-21 02:42:42.470
105057,57906,22843.0,CC BY-SA 3.0,,"Your answer is informative and introduced a lot of new ideas, but I'm still wondering whatever happened to the pdf or ""n"" in the case of discrete r.v.s when dealing with expected values expressed as vectors? I have seen $E[XY]$ expressed as $ <X,Y>$",2013-10-21 02:54:16.353
105058,57911,5237.0,CC BY-SA 3.0,,"What you are doing, in essence, is discrete time survival analysis. [Here](http://www.ats.ucla.edu/stat/mplus/seminars/DiscreteTimeSurvival/default.htm) is some information from UCLA's stats help site. I haven't seen these slideshows, but the UCLA site is of uniformly high-quality.",2013-10-21 02:54:25.530
105059,57863,9049.0,CC BY-SA 3.0,,How do you calculate your derivatives? What software package are you  using?,2013-10-21 02:55:57.573
105060,57905,22843.0,CC BY-SA 3.0,,I'm guessing you're saying that $x$ and $y$ have the same mean?,2013-10-21 02:56:38.710
105061,57905,3993.0,CC BY-SA 3.0,,"@Guest If a variable has a mean of 0 (this is what it means for a variable to be ""centered"" about its mean), then its sum must also be 0. After all, the mean of a variable is just computed as its sum divided by something. So if the mean is 0, the sum must also be 0. Make sense?",2013-10-21 02:57:07.780
105062,57893,594.0,CC BY-SA 3.0,,"It seems like you're maybe confusing together the cdf with its inverse. First, try drawing a picture of the pdf, the cdf, and the inverse of the cdf (the quantile function). Once you can get the cdf right, it's easier to get its inverse right.",2013-10-21 03:02:46.263
105063,57875,22611.0,CC BY-SA 3.0,,"Just wanted to add - my main question is the appropriateness of using N = 15 for each group. I'm aware you can conduct a t-test with only summary statistics from each group (e.g., mean, SD, and sample size), but the sample size is usually the number of *individuals* contributing to the data - not the number of *questions*",2013-10-21 03:07:11.847
105064,57895,594.0,CC BY-SA 3.0,,"I guess I should have asked for this clarification first: do you mean linear regression with multiple predictors (x's, IVs) - that is multiple regression, or do you mean linear regression with multiple responses (y's, DVs) - that is, *multivariate* regression?",2013-10-21 03:07:20.243
105065,57858,10135.0,CC BY-SA 3.0,,"Have a look at [this](http://stats.stackexchange.com/questions/29271/interpreting-residual-diagnostic-plots-for-glm-models) and the links in the answer. It is much more comprehensive than what I was going to say here. Depending on the problem, you need to revised your model, sometimes removing a variable, sometimes transforming it. I cannot provide a single remedy, it is depends to the problem.",2013-10-21 03:45:46.760
105066,57381,22547.0,CC BY-SA 3.0,,I think this is the best answer for what is a terrible question (in hindsight).,2013-10-21 03:57:08.600
105067,57195,22547.0,CC BY-SA 3.0,,"I've realized from this that I have a lot of work to do on pre-processing the data before I start the analysis I've outlined here. Reading the response from @nadya I think it's clear I need to look at some kind of spatial aggregation, but that will be challenging as it's wrong to aggregate land and ocean data. Then I need to look at gap-filling strategies. Then (and only then) can I start to look at the mapping / visualization work.",2013-10-21 03:59:37.990
105068,57914,594.0,CC BY-SA 3.0,,"This looks like pretty standard bookwork. Is this for some subject, or even just for the purposes of your own study?",2013-10-21 04:22:39.190
105069,57319,5237.0,CC BY-SA 3.0,,"I haven't really had a chance to get to this, although I've wanted to. It's worth noting @ttnphns' point, though. The sign reversal has to do with endogeneity, not suppression. For reference, I have discussed endogeneity [here](http://stats.stackexchange.com/q/58709//58712#58712), & suppression [here](http://stats.stackexchange.com/q/33888//34016#34016).",2013-10-21 04:45:55.887
105070,57651,22762.0,CC BY-SA 3.0,,The data is in ordinary format as for the {plm} package purpose. Vars: ID country   year   REER     GDP FinalConsumpExpend DimesticDemand ...(21 vars in total) over 1994Q1:2003Q1 period of time,2013-10-21 05:23:26.677
105071,57914,,CC BY-SA 3.0,,Hint: What's the probability of getting *anything but* a 2 on a single throw of the die?,2013-10-21 05:32:31.887
105073,57887,169.0,CC BY-SA 3.0,,Of course! They are both ordered. Is there such a thing as an ordered chi square? I never heard of such a thing,2013-10-21 05:55:27.560
105074,57319,2081.0,CC BY-SA 3.0,,"@gung, you link to useful answers of yours. (Though I would doubt that sign reversal is always due to endogeneity.) If you like you might post a question about suppression etc and people, including yourself, might give their answers.",2013-10-21 06:06:01.413
105075,57893,22884.0,CC BY-SA 3.0,,"The way I was trying to do this is as follows: calculate the CDF (by integrating), then find the inverse function. Now I was just confused how to ""stick them together"" since I only draw p in [0,1].
@Glen_b When I am drawing random numbers accordingly to a given pdf I always use its quantile function. I think this is correct since it is related to the inverse of the pdf but please correct me if I am wrong!
That is of course, correct, I was thinking about calculating only x >0 first, then mirroring it! Would that work?",2013-10-21 06:07:41.430
105076,57864,6204.0,CC BY-SA 3.0,,"You can, but it usually goes in the other direction. The typical process is to pick a probability (usually 90%, 95%, or 99%) and determine the (symmetrical) range about the observed sample mean which encompasses this probability of finding the true mean. That's what a confidence interval is.",2013-10-21 06:24:21.793
105077,57893,594.0,CC BY-SA 3.0,,"Take some care. One reason for suggesting you write the cdf was that I was hoping you'd see that your *density* is probably not correctly specified (I suspect you're aiming to have continuity at $\pm x_0$ - is that the case?  If so, your $f$ is wrong. As I said, somewhere earlier, draw it.). If you don't get that right you're wasting your time jumping several steps further along.",2013-10-21 06:46:25.227
105078,57913,15321.0,CC BY-SA 3.0,,"Just to avoid any confusion, I will be using the same colors with the same number of colors in same proportion in both the cases. The only difference between the dataset would be the configuration and labels of the graph vertices.",2013-10-21 06:48:01.643
105079,57887,594.0,CC BY-SA 3.0,,"There are actually several possible ways to analyze such data, some of which would correspond to some form of chi-square. However, with this data, I'd probably be looking at modelling it as something like a cumulative logit model on the quantile groups with time as a independent variable. It kind of depends on what kinds of things you want to test.",2013-10-21 07:06:04.277
105080,57914,20470.0,CC BY-SA 3.0,,A 2-second Google search would have saved you from a 15-second question typing effort: http://math.stackexchange.com/questions/337689/if-you-roll-5-standard-six-sided-dice-whats-the-probability-that-you-get-at,2013-10-21 08:03:53.973
105081,57906,20473.0,CC BY-SA 3.0,,"The size of the sample $n$ is used when we are _estimating_ the expected value from a sample. When we _define_ the expected value the ""weight"" factor is the pmf itself. Indeed, the expected value is a ""weighted"" average, while the sample mean is an ""unweighted"" average (nevertheless, the latter is a consistent estimator of the former). As for the second issue, you are confused because in other scientific fields, the symbol $< >$ is used _instead_ of the symbol $E$ -it means exactly the same thing.",2013-10-21 08:57:34.193
105082,57919,22752.0,CC BY-SA 3.0,,"Note: this is not a homework question, but I'm studying for exams.",2013-10-21 09:12:22.147
105084,57916,3993.0,CC BY-SA 3.0,,"This is not a ""problem"" and does not need to be ""solved."" As you already noted yourself, this apparent multicollinearity is a natural consequence of using dummy codes. If you use non-orthogonal codes, you get non-orthogonal parameter estimates. My advice: ignore it.",2013-10-21 10:16:01.180
105085,57901,6162.0,CC BY-SA 3.0,,"Maybe I'm missing something, but you have only derived the distribution of $Y$, whereas the OP requires the conditional distribution of $Y$ given $Y \leq W$. Moreover there's no need to do all these calculations in order to derive the distribution of $Y$.",2013-10-21 10:33:53.223
105086,57926,22677.0,CC BY-SA 3.0,,"@`mpiktas`, suppose i wanted to fit with a mean model of p,d,q 2,3,4 what function do you suggest `rugarch` or `garchFit` or is there others? or do i need to do further differencing until reaching zero `d` before fitting?",2013-10-21 11:58:12.080
105087,57926,1406.0,CC BY-SA 3.0,,Does your data really conform to d=3? Processes with $d=3$ behave very wildly and usually are not examples of financial data.,2013-10-21 12:03:59.440
105088,57901,20473.0,CC BY-SA 3.0,,"@Stephane Laurent In the OP's question the solution to the specific integral is requested _at the end_ of OP' post, where the OP clearly indicates that the solution to this integral is what the OP needs. It would be very useful to everybody if you would post an answer with the alternative and shorter way to derive the solution to this integral.",2013-10-21 12:05:39.920
105089,57916,18914.0,CC BY-SA 3.0,,"Ok, Thanks a lot! I thought that it should be ok, but I just want to be sure:-)",2013-10-21 12:10:46.940
105090,57919,4656.0,CC BY-SA 3.0,,"Most likely by `the change-of-variables technique` is meant the usual method involving Jacobians. However, the result that you are asked to prove is false unless the univariate standard normal random variables are **independent** random variables.  See [this answer](http://stats.stackexchange.com/a/30205/6633) for a great description of how two normal random variables can fail to have a bivariate normal distribution.",2013-10-21 12:16:18.943
105091,57930,503.0,CC BY-SA 3.0,,"First, a test can't show ""no significance"" - it gives a particular p value. Second I don't understand exactly what you mean by ""subsetting on varX"", nor exactly what you are bootstrapping. What is it you are trying to do?",2013-10-21 12:17:06.847
105092,57926,22677.0,CC BY-SA 3.0,,"of course not it's only hypothetical, the biggest 'd' in my data are the index of financial sector of greece 'fin.gre' with arima order (1,2,1) while the log return of it 're.fin.gre' is (1,1,3)",2013-10-21 12:17:23.597
105093,57930,4499.0,CC BY-SA 3.0,,"After testing for all cases 5K vs all controls 5K pvalue is >0.05, not significant. I am trying to find out if there is a significance for the subset of cases `dat[ dat$caco==0 | dat$varX==""A"",]`, i.e. all controls vs cases with varX==""A"", here I have pvalue <0.05.",2013-10-21 12:23:07.950
105094,57870,5671.0,CC BY-SA 3.0,,"Since you have outliers in the data set, they might be dominating your variances. Judging from your plot, the data is a single large blob only, and there is no structure to be seen in here. You may need to do more manual preprocessing and data cleaning.",2013-10-21 12:28:03.823
105095,57930,503.0,CC BY-SA 3.0,,"In that case, you don't need to do anything else, just say what you did when you report results. However, beware that with 5000 vs. 1000 even very small differences can be significant.",2013-10-21 12:29:41.680
105096,57919,22752.0,CC BY-SA 3.0,,@DilipSarwate Thank you. I assume though that that is probably what the professor meant. Could you show me how the method with the Jacobians works assuming that that is so?,2013-10-21 12:30:48.447
105097,57894,22885.0,CC BY-SA 3.0,,@ttnphns Thanks for pointing it out. I was really using too much of terminology. Hope now it's better. I tried to make the problem description clearer. Though I understand that it's still messy. I'm working on it.,2013-10-21 12:32:53.020
105098,57930,4499.0,CC BY-SA 3.0,,"*5000 vs. 1000 even very small differences can be significant.* - that is my worry, hence I came up with above ""method"" - as a way of validation. (Apologies, I'm not a statistician.)",2013-10-21 12:41:20.833
105099,57931,17670.0,CC BY-SA 3.0,,"Thank you, is there a way to achieve this while sticking to logistic regression (i.e. without touching the likelihood function)?",2013-10-21 12:41:33.093
105100,57931,2666.0,CC BY-SA 3.0,,"It depends on what ""this"" is.  What is the ultimate goal and how will the model be used?",2013-10-21 13:00:04.627
105101,57919,4656.0,CC BY-SA 3.0,,See [this document](http://courses.engr.illinois.edu/ece313/fa2000/ppt/Lecture39.pdf) for how the Jacobian method applies to the special case of linear transformations of normal random variables.,2013-10-21 13:01:55.557
105102,57880,22643.0,CC BY-SA 3.0,,"Thank you, jbowman.  I have edited the question to include a fictitious use case, and an explanation why I think duration is important.",2013-10-21 13:07:55.397
105103,57928,17740.0,CC BY-SA 3.0,,"Whether you interpret it as a vector or a matrix doesn't really change anything. I would not worry about 2500 features, that's not *big* by current standards.",2013-10-21 13:11:15.553
105104,57926,1406.0,CC BY-SA 3.0,,"Hm so you have probably monthly data? I suggest looking at other packages then, or apply the ugarch on differenced data.",2013-10-21 13:17:11.090
105105,57930,503.0,CC BY-SA 3.0,,"Your method doesn't fix that and isn't necessary. Just look at the effect sizes - here, the proportions in each cell of your table.",2013-10-21 13:18:13.787
105106,57894,2081.0,CC BY-SA 3.0,,"`I'm doing eigen-decomposition of a covariance matrix, then using eigenvectors to make an orthogonal transformation of the data that is not mean-centered` Are you saying that you compute the PC scores by multiplying raw_data*eigenvectors, not centered_data*eigenvectors? (That gives the PCs which are completely correlated with the ""true"", centered PCs.)",2013-10-21 13:29:40.660
105107,57890,4320.0,CC BY-SA 3.0,,"If you estimate the parameters and don't want to use a prior (different then using an ""uninformative prior"") then you essentially just want to do [Maximum likelihood estimation](http://en.wikipedia.org/wiki/Maximum_likelihood), i.e., $\theta^* = \arg\max_\theta f(X|\theta)$.",2013-10-21 13:34:42.073
105108,57901,6162.0,CC BY-SA 3.0,,"Ok sorry, I'm at the office and the LaTeX rendering does not work. The shorter way I had in mind is the one given by @RayKoopman, without integral calculations. If you really want to calculate an integral, you don't need to calculate the normalization constant.",2013-10-21 13:34:48.337
105109,57928,22901.0,CC BY-SA 3.0,,"thanks marc, I guess its fair enough to use the 2500 points as individual features, I just want to make sure that the machine learning captures the 'shape/relationship of matrix elements' of the matrix vs just concentrating on which features are important and weighting them. But perhaps that is the same thing",2013-10-21 13:38:35.500
105110,57898,668.0,CC BY-SA 3.0,,"John Tukey, who invented the approach from which this method appears derived, used *two* multipliers: He set one ""fence"" at 1.5 times (an analog of) the IQR away from each quartile and another fence at 3 times the IQR from each quartile. Values beyond the first fence were ""out"" and values beyond the second were considered ""far out"" (those who remember the '60s will understand this terminology). If you think you need more extreme fences, then most likely you should consider *re-expressing* your data rather than changing the fences.",2013-10-21 13:42:38.017
105111,57901,10547.0,CC BY-SA 3.0,,"If u've $f_Y(y)$ its not a big effort to derive (5). Nevertheless is there not a small error in the calculation? Since $\int_{-\infty}^\infty \text{exp}\{-\beta w^2 + \alpha w\}dw$ = $\int_{\infty}^0 \text{exp}\{-\beta w^2 - \alpha w\}dw$ + $\int_{0}^\infty \text{exp}\{-\beta w^2 + \alpha w\}dw$ = $-\int_0^{\infty} \text{exp}\{-\beta w^2 - \alpha w\}dw + \int_{0}^\infty \text{exp}\{-\beta w^2 + \alpha w\}dw$. Then I would use Gradshteyn & Ryzhik (2007), ""Table of Integrals, Series and Products"", 7th ed., p. 336, eq. 3.322(2) with $\alpha = -\gamma$ for the 1st part.",2013-10-21 13:43:11.810
105112,57914,668.0,CC BY-SA 3.0,,Useful information is available on our site through [a search](http://stats.stackexchange.com/search?q=binomial+dice).,2013-10-21 13:45:11.027
105113,57894,22885.0,CC BY-SA 3.0,,"@ttnphns Exactly. I plotted toy data in 2d and checked the rotation. It works the same as if data was mean-centered, but just a pivot point of rotation is not located in the mean (honestly, I don't know where it is). My concern now is if it can affect optimization results in some way.
After receiving optimized coefficients, i just multiply them by transposed eigenvectors.",2013-10-21 13:50:51.147
105114,57933,20498.0,CC BY-SA 3.0,,Thank you very much for the response. Rapidminer is very good...very simple and a gateway to R..which is what im trying to practice this on. I have purchased the book. It may give me some ideas,2013-10-21 13:56:53.007
105115,57901,20473.0,CC BY-SA 3.0,,"The formula I used (which makes use of the cosh function) is quicker than what I had proposed to my other answer. Mistake, there isn't, in either ways. Be careful with how signs change/change not when swapping integral limits etc.",2013-10-21 14:02:04.663
105116,57935,668.0,CC BY-SA 3.0,,"What exactly does `points` measure and what relationship does it have (if any) to a player's ""score""?  (Or is the score a separate variable altogether?)  Note, too, that your ability to assess ""influence"" in any material or causal sense depends on how you collected these data: if they are just records of the outcomes of contests, then most likely you cannot estimate influence at all, but you might be able to identify some *quantitative relationships* among the variables.",2013-10-21 14:02:06.037
105117,57935,503.0,CC BY-SA 3.0,,"It might help to post the first few lines of your data, in `R` you can do this with `head`.",2013-10-21 14:05:23.577
105118,57901,20473.0,CC BY-SA 3.0,,"@Stephan Laurent Yes, of course, but I wanted to calculate the _integral_ for which the OP said he was stuck. And the bulk of my calculations are not about the constant term, but about manipulating the terms that contain the integrating variable.",2013-10-21 14:05:42.230
105119,57935,10409.0,CC BY-SA 3.0,,"`points` is a combination of the player's assists, goals, penalties (negative), etc...  where each attribute has a multiplier.  I should reword that to be `points` and not `score`.  Good catch.",2013-10-21 14:09:18.160
105120,57935,10409.0,CC BY-SA 3.0,,"opponent facts are things like the opponent's ""average points allowed per game""",2013-10-21 14:10:28.507
105121,57886,22880.0,CC BY-SA 3.0,,"Exactly, and I asked two questions....sorry for bad formatting.",2013-10-21 14:12:14.017
105122,57782,22507.0,CC BY-SA 3.0,,"Before asking what would be the flaws with this approach, maybe you should write why this approach should work, in your opinion.  Why do you think that the steps 2-4 improve the outcome?",2013-10-21 14:40:11.457
105123,57782,22507.0,CC BY-SA 3.0,,"Also, am I right that at the end you drop the model from the step 1 and use only model from the step 4?",2013-10-21 14:47:27.660
105124,57928,4320.0,CC BY-SA 3.0,,"@user1449677 What does ""pretty much an image"" mean? There are a lot of different features you can compute from images, [SIFT](http://en.wikipedia.org/wiki/Scale-invariant_feature_transform), [HOG](http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients), or see the Wikipedia page [Feature (computer_vision)](http://en.wikipedia.org/wiki/Feature_(computer_vision)) as a jump off point.",2013-10-21 15:29:44.547
105178,57962,10594.0,CC BY-SA 3.0,,"@jbowman, Could you give me more insight about ""My suspicion is that you really mean that the expected number of cells surviving is Nexp{−βx}, which doesn't agree with λ=Nexp{−βx}."" Why?",2013-10-21 20:48:29.147
105125,57937,668.0,CC BY-SA 3.0,,This was the original (1908) *definition* of $t$: everything else known about it comes from this characterization.  The Wikipedia article on the Student t distribution provides a reference to [Fisher's 1925 paper](http://www.sothis.ro/user/content/4ef6e90670749a86-student_distribution_1925.pdf).,2013-10-21 15:31:06.600
105126,57941,668.0,CC BY-SA 3.0,,What happens to the polynomial and its roots when you plug in $1/u$ for $\lambda$?,2013-10-21 15:41:58.217
105127,57940,668.0,CC BY-SA 3.0,,"+1 You will find an extended discussion of (1) and (3)--with definite empirical answers--in Daniel Kahnemann's book *Thinking, Fast and Slow* (2011).",2013-10-21 15:44:35.700
105128,57942,668.0,CC BY-SA 3.0,,"Could you please explain how it would be possible for a percentage of *anything* in a group of size $n=6$ to be any value other than $0, 100/6, 200/6, \ldots, 500/6,$ and $100$?  In particular, how do you arrive at $1.5667\%$?",2013-10-21 15:46:25.647
105129,57850,22092.0,CC BY-SA 3.0,,"er..When you have time,could you please..?I know they are retarded questions. I tried to understand but,sigh.",2013-10-21 15:47:54.830
105130,57941,,CC BY-SA 3.0,user30490,"Could you (""hold my hand"") and elaborate.  I don't see it still even if i plug in 1/u.",2013-10-21 15:48:29.733
105131,57942,22909.0,CC BY-SA 3.0,,"@whuber The data-sets i have are time dependent. In the second group, the case x was seen 1.5667% of the total time.",2013-10-21 15:49:58.963
105132,57782,17670.0,CC BY-SA 3.0,,"Yes, I was planning on using the model fitted with the entire data set, but it doesn't make sense to do so because it's under-performing the model fitted with the training set.",2013-10-21 15:50:35.567
105133,57931,17670.0,CC BY-SA 3.0,,I edited my question to provide detail on what I'm trying to achieve.,2013-10-21 16:03:08.613
105134,57940,651.0,CC BY-SA 3.0,,"I'd need to reread the book, but (1) seems to be a rather odd use of probabilities for decision making.  You don't need to reject hypotheses to make decisions, taking the decision that maximises the expected return is perfectly valid, and in this case would tell you that any lottery ticket is as good as any other (excluding consideration of the behaviour of other customers).",2013-10-21 16:05:37.190
105135,57940,5448.0,CC BY-SA 3.0,,"I have to say, I had a hard time reading past the first ""paradox""; an author who opines on statistics and decision-making while, it would appear, having no knowledge of statistical decision-making, is not to be trusted on the applicability of statistics in general.  Also, as Russell and Whitehead showed, logic is a part of mathematics, and of course so is probability theory, so they can't be inconsistent with each other - unless mathematics itself is internally inconsistent.  As for paradox #2, ask any actuary or gambler about whether probability can be applied to real life.",2013-10-21 16:08:40.020
105136,57782,5821.0,CC BY-SA 3.0,,I don't have a source on this right now... but are you aware you can optimize a logistic regression model to maximize the Area Under The (Receiver Operating Characteristic) Curve (or AUC)? No need to reinvent the wheel.,2013-10-21 16:11:04.680
105137,57942,503.0,CC BY-SA 3.0,,"That doesn't really answer the question, it just raises more questions. If you have N = 6, @whuber is correct. If you have something else, please describe what you have. See [how to ask a statistics question](http://www.statisticalanalysisconsulting.com/how-to-ask-a-statistics-question/).",2013-10-21 16:12:40.160
105138,57942,22909.0,CC BY-SA 3.0,,@PeterFlom Please see the edit. I hope now my question is more clear,2013-10-21 16:22:26.357
105139,57919,22752.0,CC BY-SA 3.0,,"@DilipSarwate Hmmh, I still don't really understand how it would work in this case. Could you please show me?",2013-10-21 17:02:28.883
105140,57942,436.0,CC BY-SA 3.0,,"You may need to explicit a little bit better what you did because (at least to me) it is still unclear. Something on the line of: ""In each experiment I measured 5000 events and counted how many were of type x"", or whatever you did...",2013-10-21 17:03:45.440
105141,57942,232.0,CC BY-SA 3.0,,"I think you are using incorrect terminology: you have _rates_, not _percentages_. So in the first group, you have the event occurring at an (average) rate of 0.1 / minute, for example. Is that correct?",2013-10-21 17:04:06.940
105142,57768,18845.0,CC BY-SA 3.0,,"For anyone who wants to know more about the use of negative correlation in Monte Carlo simulation, try googling ""antithetic variates"".  More info in course notes [here](http://www.columbia.edu/~ks20/4703-Sigman/4703-07-Notes-ATV.pdf) or [here](http://www.math.kent.edu/~oana/math60093/10lecture5.pdf).",2013-10-21 17:39:19.887
105143,57949,668.0,CC BY-SA 3.0,,"Please tell us what $Z,$ $\varepsilon,$ $\hat{\varepsilon},$ and $\sigma$ represent.  In particular, what formulas do you know that express $\hat{\varepsilon}$  in terms of $\varepsilon$ and $Z$?  (In so doing, I suspect you will find the answer to this question yourself.)",2013-10-21 17:39:26.730
105144,57942,22909.0,CC BY-SA 3.0,,@nico you are right i should have written before. Please see the edit,2013-10-21 17:40:19.890
105145,57948,668.0,CC BY-SA 3.0,,"Could you please explain what you mean by ""register the data"" and what the nature of these ""technical difficulties"" is?  It sounds like you may have censored data: choosing an appropriate procedure for such data depends on the nature of the censoring.",2013-10-21 17:41:34.237
105146,57205,22558.0,CC BY-SA 3.0,,"Hello Momo, thanks for the comment. ""not touch the data"" means not to remove any data or time-step or replace with 0 or the mean, it would compromise the information about the specific-time-lag linear dependence. I partially recoded the matlab (link above) autocorrelation and partial autocorrelation functions to deal with NaNs: any data couples including NaNs is excluded from the computation. This is done for each lag. It worked for me. Any suggestion is well accepted.",2013-10-21 17:47:07.990
105147,57937,20473.0,CC BY-SA 3.0,,Go to an answer of mine in math.SE where the full derivation of the t-density can be found. http://math.stackexchange.com/questions/474733/derivation-of-the-density-function-of-student-t-distribution-from-this-big-integ/480327#480327,2013-10-21 18:02:31.280
105148,57948,22910.0,CC BY-SA 3.0,,"Sorry, I tried to be concise. The used machine only can count until a limit (100000) so counts over that limit are registered as ""over 100000"" and this is the problem, instead of the decreasing curve of the normal distribution I have an over 100000 group which is not very useful.",2013-10-21 18:12:28.033
105149,57944,10409.0,CC BY-SA 3.0,,"Thanks, I have never run a regression model before.  The goal is to see what opponent_facts have the biggest influence on a player's points.  And ideally, how much should I expect a player's points to increase or decrease.

I'm looking at the ""Estimate"" column of the summary.  It tells me that a certain player_id has an Estimate of -2.7648, but I see their mean points is 2.975.  Is the Estimate column the one you were talking about where I can see the ""average points expected for a player"" or the ""average points expected against a given opponent"".  That would be really cool to know.",2013-10-21 18:12:37.067
105176,57962,5448.0,CC BY-SA 3.0,,"@Glen_b  You're correct, as usual.  To the OP - how does the number of cells $N$ differ from a sample of $N$ cells?  I am assuming whether a cell survives or not is independent, to a reasonable degree of approximation, of the survival of other cells.  My suspicion is that you really mean that the expected number of cells surviving is $N \exp\{-\beta x\}$, which doesn't agree with $\lambda = N \exp\{-\beta x\}$.",2013-10-21 20:41:21.610
105150,57782,17670.0,CC BY-SA 3.0,,"@AdamO I fitted the model with type.measure=""auc"" and although the training set model still out performs the full set model, it's now by a very small amount.  I read up on AUC, and this result is reasonable to me because in order to gauge performance I am simply optimizing on a single prediction threshold which gives the maximum difference between true/false positives, which isn't what the AUC objective is doing.  In the 3 steps I outlined at the bottom of my question, is step 3 necessary, or should I just stop at step 2 with the optimized prediction threshold since its performing better?",2013-10-21 18:14:50.633
105151,57949,22752.0,CC BY-SA 3.0,,@whuber $\hat{y}=Z \hat{\beta} + \hat{\epsilon}$. The epsilons are the error terms and $\sigma^2$ is variance. I still however don't know why this holds...,2013-10-21 18:15:05.107
105152,57782,17670.0,CC BY-SA 3.0,,"@AdamO Also, if you could post your previous comment as an answer I will accept it as the answer.",2013-10-21 18:16:00.393
105153,57944,1805.0,CC BY-SA 3.0,,"@Bradford: Yes it is, but that ""estimate"" is for that player, against the AVERAGE opponent.  So in your example, the regression model thinks that that player has an estimated mean points of -2.7648, but they played against easier than average opponents, which is the reason their actually average points is 2.975.  In other words, the regression model corrects for the fact that each player has played a different set of opponents, and some of those opponents might have been easier (or harder) than average.",2013-10-21 18:21:16.690
105154,57940,2958.0,CC BY-SA 3.0,,""" when we say that there is a 95% chance of rain tomorrow, it is unclear to what entities that 95% applies"" Gigerenzer (e.g. in ""Risk Savvy"") discusses this but in an entirely practical and non-philosophical way. He suggests that at the very least you spell out 95% of what (for weather forcasts: usually days that are similar to tomorrow), or better: that 19 out of 20 such days had rain and give a definition of what ""rain"" means specifically. He also argues that school children can understand such statements, but hardly anyone can if the vital information about the denominator is omitted.",2013-10-21 18:35:20.553
105156,57942,22909.0,CC BY-SA 3.0,,"After the correction from @nico , here is the solution:

https://onlinecourses.science.psu.edu/stat414/node/268. Thank you so much to everyone",2013-10-21 18:46:32.797
105157,57836,1411.0,CC BY-SA 3.0,,I would also note that the `?pvalues` help page in the the new version of `lme4` includes a lot of information on this topic.,2013-10-21 18:54:23.307
105158,57962,5448.0,CC BY-SA 3.0,,"Do you have a particular functional form for $g(\beta,x)$ in mind?",2013-10-21 19:02:49.237
105159,57962,10594.0,CC BY-SA 3.0,,"yes, $\lambda=g(\beta,x)=Nexp^{-\beta x}$, where N refers to the original number of cells and $exp^{-\beta x}$ refers to the fraction of cells that are died.",2013-10-21 19:07:12.577
105160,57961,22906.0,CC BY-SA 3.0,,"Thanks for your answer. However, I'm talking about the density function which takes value f1 in the range 0 to t1, value f2 for the range t1 to t2, and f3 for more than t2. In this situation, can I use the 2nd approach? I know the first approach is more applicable, but I can't derive the CDF for this complicated function.",2013-10-21 19:13:24.500
105161,57949,668.0,CC BY-SA 3.0,,"That's because you expressed $\hat{y}$ rather than $\hat{\varepsilon}$ in terms of $Z$.  You might find that searching our site for [""idempotent""](http://stats.stackexchange.com/search?q=idempotent) gives particularly helpful pointers.",2013-10-21 19:14:11.140
105162,57948,668.0,CC BY-SA 3.0,,"That is called *right censoring.*  The issue you face is not one of testing normality but of appropriately handling the censored data.  It would help for you to edit your question to reflect that more explicitly, rather than relying on readers to look through these comments.  In particular, please clarify what you mean by ""comparing"" counts: do you wish to compare two distributions, two means, or something else?",2013-10-21 19:18:52.287
105163,57962,5448.0,CC BY-SA 3.0,,"I'm not quite sure you've distinguished between $p(y=0)$ and $\lambda$, so just to check:  $p(y=0) = \exp\{-\lambda\}$, $\lambda = N\exp\{-\beta x\}$, so $p(y=0) = \exp\{N\exp\{-\beta x\}\}$?",2013-10-21 19:33:21.910
105164,57962,10594.0,CC BY-SA 3.0,,Yes.That's exactly my model,2013-10-21 19:37:11.147
105165,57962,20473.0,CC BY-SA 3.0,,"Given what you write in your question, $exp(-\lambda)$ is the probability of having zero survival, and zero survival is mapped to $y=1$ (as you write it), not $y=0$. cc @jbowman",2013-10-21 19:46:00.740
105167,57960,,CC BY-SA 3.0,,"I would have thought AUC is not best here because there is small loss for false negative, but large loss for false positive.",2013-10-21 19:50:41.993
105168,57965,668.0,CC BY-SA 3.0,,"This is closely related to a generalized [birthday problem](http://stats.stackexchange.com/search?q=birthday): $N$ would be days of the year, the $D$ independent ""markings"" would be draws from a population of people, and the chance of $C$ ""clean"" balls is the chance that among those $D$ people there are $N-C$ unique birthdays.  As an answer, do you seek a closed formula, an efficient algorithm, or an asymptotic formula (in $N$ or $D$)?",2013-10-21 19:59:09.907
105169,57962,5448.0,CC BY-SA 3.0,,"Why would your sample size ($N$) affect the fraction of cells that have died?  The count, perhaps, but the probability?",2013-10-21 20:00:38.830
105170,57782,,CC BY-SA 3.0,,"What I don't quite understand here is why you haven't included anything about the predicted future price in your model, nor have you included the magnitude of profit/loss into the optimisation.  Surely a decision to ""buy"" that leads to a 99% loss is much worse than a decision to ""buy"" that leads to a 1% loss, even though both are false positives.",2013-10-21 20:04:33.103
105171,57960,5821.0,CC BY-SA 3.0,,"Well, the real problem is that OP has a continuous outcome (ROI) and is dichotomizing it as a loss/gain. But splitting hairs aside, with ROC regression *in general* ""stupid"" marker cut-off regions indeed count toward the AUC. You can use the partial AUC if you prespecify what counts as meaningful versus stupid marker values, and partial AUC regression has all the same performance capabilities (and issues).",2013-10-21 20:21:38.770
105172,57962,10594.0,CC BY-SA 3.0,,"@jbowman. I am not sure If I get your questions. $N$ refers to the original number of cells, actually it is also a variable. $N exp^{-\beta x}$ is the cell survival function. I don't think $N$ refers to the sample size.",2013-10-21 20:25:42.797
105173,57965,22914.0,CC BY-SA 3.0,,"I'm looking for a closed formula. I will look into the birthday problem, I didn't notice they are related.",2013-10-21 20:30:02.437
105174,57962,594.0,CC BY-SA 3.0,,"""The original number of cells"" is the same as what I think jbowman intended by ""sample size"". I agree with his concern, as well.",2013-10-21 20:31:44.390
105175,57967,594.0,CC BY-SA 3.0,,"It's small numbers (small expected counts) where it's critical to model as count data. Whether there's a calculation problem at some size of count will depend on the software, but I don't see that a carefully implemented calculation should have a problem with those counts. Either way, they're certainly large enough to approximate by normal distributions, via nonlinear least squares or Iterative Reweighted Least Squares, say, but that, too, would need to be carefully implemented.",2013-10-21 20:37:05.243
105177,57962,10594.0,CC BY-SA 3.0,,"Only $exp^{-\beta x}$ is the probability of cells that have survived, which is irreverent to $N$. So $exp^{-\beta x}$ is the probability. $N exp^{-\beta x}$ gives the counts of cells that that have survived.",2013-10-21 20:44:53.877
105179,57962,10594.0,CC BY-SA 3.0,,"@jbowman, Yes, your concern about in dependency of survival among cell is reasonable. However, we assume that cell survival is independent.",2013-10-21 20:54:54.260
105180,57962,5448.0,CC BY-SA 3.0,,"You have, in one case, the probability of a cell surviving is $\exp\{-\lambda\}$, and in the other case, $\exp\{-\beta x\}$, which is inconsistent with $\lambda = \exp\{-\beta x\}$.",2013-10-21 20:58:18.310
105181,57931,2666.0,CC BY-SA 3.0,,"Unless I'm missing something, nothing you added would imply the use of a cutpoint.  Note that a predicted probability provides its own error rate.",2013-10-21 21:05:48.903
105182,57969,668.0,CC BY-SA 3.0,,"There are many suitable distribution tests that apply here and are powerful (most likely they are too powerful, but that's another issue altogether), including the Chi-squared test and the Kolmogorov-Smirnov test.  So what ""resource"" do you seek: a reference to some test that looks like yours or some test that will work well in the situations you describe?",2013-10-21 21:06:55.513
105183,57961,22906.0,CC BY-SA 3.0,,Thank you so much Aniko for your such detail suggestion. Great Support!!,2013-10-21 21:08:48.727
105184,57969,8869.0,CC BY-SA 3.0,,"Oops, I forgot to say that $N$ is very small. My understanding is that Kolmogorov-Smirnov and Chi-squared are not suitable for small $N$, since you need enough samples to give you a reasonable cumulative distribution function or histogram. I don't know a test that applies to this case. Edited post to emphasize this point.",2013-10-21 21:11:44.050
105185,57942,232.0,CC BY-SA 3.0,,"You can add an official answer to your own questions, but in this case your answer might be wrong. It completely ignores the fact that there are 15 experiments in group 1, and that outcomes within an experiment might not be independent.",2013-10-21 21:16:03.347
105186,57969,8869.0,CC BY-SA 3.0,,"Also, I am not wedded to this idea -- I just thought it might be more powerful than K-S or Chi-squared at low sample size. Forming the CDF of 5 samples strikes me as a bit ham-fisted. My boss wants me to perform a test of this sort and I just want the best tool for the job, and I don't care if it's my idea or something totally unrelated.",2013-10-21 21:27:39.023
105187,57969,668.0,CC BY-SA 3.0,,"K-S has no problems with small data sets.  You are unlikely to find anything more powerful than it unless you make strong distributional assumptions.  I don't understand why you need to construct a ""reasonable CDF"": the relevant CDF comes from accumulating $p(x)$, not from observing samples.  I also don't understand what ""further assumptions"" you could possibly apply, given that $p(x)$ appears fully to specify your reference distribution.  These, and several other disconnects, make me worry that you might not have communicated your problem accurately.  Perhaps you could provide an example?",2013-10-21 21:31:34.747
105188,57970,668.0,CC BY-SA 3.0,,"Presumably this is the PDF, because if it were used as a CDF your question would be trivially easy.  However, you need to tell us its domain, because its integral diverges.",2013-10-21 21:33:53.677
105189,57962,10594.0,CC BY-SA 3.0,,"$exp^{−\lambda}$ is the probability of counting zero survival cells, given the individual cell survival probability $exp^{−\beta x}$ and the expected number of counted survived cell as $Nexp^{−\beta x}$. Are these two definitions the same? I am confused.",2013-10-21 21:38:28.943
105190,57969,8869.0,CC BY-SA 3.0,,"In K-S the test statistic is $\sup_t |F_N(t) - F(t)|$ where $F_n$ is the empirical CDF of the sample and $F$ is the CDF of $p$, so actually the sample CDF is relevant, and having few samples is a problem since the Kolmogorov distribution is only the asymptotic distribution of the test statistic for large $N$. Of course, almost all tests are justified by asymptotic results, but it was still a concern for me. When I said ""further assumptions"" I just meant I didn't want to make any distributional assumptions. Any other questions?",2013-10-21 21:38:57.280
105191,57723,22781.0,CC BY-SA 3.0,,"Dear Patrick, I thank for your very prompt answer and helpful references! Best,",2013-10-21 22:02:40.413
105192,57972,8888.0,CC BY-SA 3.0,,"I think that is not enough information. When you say, a proportion of a field is covered in water, is there any additional knowledge about the form of that proportion? Is it one connected area? Are there constraints to the form? For example, imagine an area stretched zig zag over the whole area, but only covering 10%. For a certain, not small size of the hoop, the probability of water inside it will be 1.",2013-10-21 22:06:22.110
105193,57959,594.0,CC BY-SA 3.0,,"At a fixed $\alpha$, the ratio is a constant. The critical values themselves come from the inverse of the cdf (the quantile function).",2013-10-21 22:21:08.357
105194,57948,594.0,CC BY-SA 3.0,,"Even without the censoring (which is a big issue), why would *counts* be asserted to be 'normal after a log transformation'? What's the basis for such an assertion?",2013-10-21 22:28:05.017
105195,57970,21119.0,CC BY-SA 3.0,,Added the domain. But not sure how it would diverge considering that $\exp(-\exp(-x^2))<\exp(-x^2)$,2013-10-21 22:36:54.703
105196,57970,594.0,CC BY-SA 3.0,,"The inequality you assert is false. Take $x=1$. $\exp(-x^2)\approx 0.368 < \exp(-0.368)\approx 0.692$. As $|x|$ gets larger, so does your function. Try drawing a picture.",2013-10-21 22:41:31.290
105197,57972,594.0,CC BY-SA 3.0,,Could you explain how your intuition works? I don't see that inequality being implied by the conditions without further assumptions.,2013-10-21 23:18:01.613
105198,57973,594.0,CC BY-SA 3.0,,"'Fit a distribution to data' is equivalent to 'estimate the parameters from data'. Some common methods include maximum likelihood or method of moments. In R see, for example, the `fitdistr` function in MASS, which comes with R (`?MASS::fitdistr`), which has an example of fitting a t-distribution. It's certainly possible to do this with a t-distribution and plot the fitted distribution. However, see the warning in the example I mentioned.",2013-10-21 23:25:35.300
105199,57969,668.0,CC BY-SA 3.0,,"What you are saying, then, is that the K-S test has little power when $N=5$.  But so do all other distributional tests.  A good way to overcome the problem of knowing only the asymptotic distribution is with Monte Carlo simulation.  In effect, then, it seems that you ought to be wondering whether there is a better test *statistic* than either the K-S or the $\chi^2$ to test your particular null hypothesis.",2013-10-21 23:27:50.660
105200,57973,668.0,CC BY-SA 3.0,,"For threads on this topic, please [search our site](http://stats.stackexchange.com/search?q=student+fit+distribution).",2013-10-21 23:29:51.280
105201,57973,5237.0,CC BY-SA 3.0,,"In addition, questions that are *only* about how to do something in R, when the OP does not have a substantive statistical question, are off-topic for CV (see our [help page](http://stats.stackexchange.com/help)). That is, this Q would be off-topic even if it weren't a duplicate. Note that some of such questions might be on-topic on [Stack Overflow](http://stackoverflow.com/), but they need to be legitimate programming questions, & not just 'what is the package / function for this'.",2013-10-21 23:37:06.910
105227,57990,20470.0,CC BY-SA 3.0,,"I think the confusion is this: **Bayes' theorem** is just the manipulation of the conditional probabilities as you give at the beginning of your question. The **Bayesian Estimation** makes use of the Bayes theorem to make parameter estimations. It is only in the latter, do the maximum likelihood estimation (MLE) and the parameter theta, etc. come into play.",2013-10-22 09:00:14.807
105202,57962,5448.0,CC BY-SA 3.0,,"If the individual cell survival probability is $\exp\{-\beta x\}$, then the individual cell non-survival probability is $1 - \exp\{-\beta x\}$.  The probability that no cells out of $N$ survive is just the product of the $N$ (identical) probabilities that each cell fails to survive.  This probability is $(1-\exp\{-\beta x\})^N$.  With your definition of $\lambda$ as $N\exp\{-\beta x\}$, you are getting this probability as $\exp\{-N\exp\{-\beta x\}\}$, which is obviously not the same.",2013-10-22 01:31:02.580
105203,57968,20473.0,CC BY-SA 3.0,,"Clarifications: first I presume that by $\exp()$ you mean the Exponential distribution, and not the base of the natural logarithms. If yes, then, is $Q_j$ the mean value, or the reciprocal of the mean value (because both these widespread parametrizations of the Exponential distribution are unfortunately symbolized the same way).",2013-10-22 01:36:44.193
105204,57979,5237.0,CC BY-SA 3.0,,"It may help you to read my answer here: [When to use Fisher and Neyman-Pearson framework?](http://stats.stackexchange.com/questions/23142//51823#51823) As for how the rejection region relates to p-values, the RR is defined as that region where $p<\alpha$.",2013-10-22 01:51:31.903
105205,57979,20179.0,CC BY-SA 3.0,,"It is an excellent explaniation. So under Neyman's setting, does it requre any property of T(x), so that p<α is equivalent to x belongs to the rejection region?",2013-10-22 01:58:30.540
105206,57979,5237.0,CC BY-SA 3.0,,"I'm not sure I totally follow your question, but there is no property of T(x) being in the rejection region except that p<a.",2013-10-22 02:04:29.100
105207,57972,22587.0,CC BY-SA 3.0,,"Oops, made a typo on that inequality. Also, you're right @Sentry, the form does matter, but I'm thinking about the water being randomly dispersed. So, some areas might have a small lake, and some areas might just have tiny droplets. Both issues are fixed in the post now, I think",2013-10-22 02:05:29.463
105208,57976,594.0,CC BY-SA 3.0,,"A substitution like $U = \Theta^2$ gets something that looks like a negative fractional moment of a [Gumbel type I](http://en.wikipedia.org/wiki/Type-1_Gumbel_distribution), which is probably no use. Nevertheless, that same transformation might help in identifying good bounds.",2013-10-22 02:44:00.010
105209,57972,10570.0,CC BY-SA 3.0,,"The question about the form of the water is still unanswered. ""Randomly dispersed"" is ill-defined. I can think of at least one interpretation of ""randomly dispersed"" where the probability is always 1.",2013-10-22 02:47:25.700
105210,57856,,CC BY-SA 3.0,,"I would disagree - smoothness can be controlled by the choice of the concentration parameter, and by the shape of the base distribution.",2013-10-22 03:11:16.843
105211,57856,7007.0,CC BY-SA 3.0,,"If you're modelling with the original DP, using any base measure, the posterior distribution never has a density with respect to Lebesgue meausure.",2013-10-22 03:34:26.657
105212,57979,20179.0,CC BY-SA 3.0,,"My question is that, how does p<alpha guarantee that x is rejected. Because Neyman-Pearson does not necessrily require p-value in the procedure, so I think this p-value has to be defined first.",2013-10-22 03:55:36.667
105213,57979,5237.0,CC BY-SA 3.0,,"Yes, p<a guarantees that x is rejected. The N-P approach is based on maintaining a maximum type I error rate, so alpha comes 1st, you figure out what T(x) is when p=a, & that defines the rejection region.",2013-10-22 04:15:26.763
105214,57836,19559.0,CC BY-SA 3.0,,"@BenBolker: Thanks! I hadn't updated `lme4` in a while and I didn't know about these useful features. I am trying to use `confint` now, trying to get profile CIs. Do you know if there is a simple way to have it estimate CIs only for the fixed effects? Otherwise, given my the structure of model, it takes forever to run and it also stops  if any of the parameters cannot be estimated (which is likely when you have many, I think)",2013-10-22 04:41:04.850
105215,57836,19559.0,CC BY-SA 3.0,,"I know you have `params=` to specify the parameters you want to get CIs for, but this requires you knowing which number corresponds to which parameter, and I don't know how to do that",2013-10-22 04:42:29.467
105216,57982,6204.0,CC BY-SA 3.0,,The parameter on the discrete RV is continuous and the joint density can be fully described in reference to this parameter. See my response. Alternatively but equally valid formulation of the joint density as a peicewise function that pivots on the value of V. There's nothing wrong with describing a density that way.,2013-10-22 05:53:50.887
105217,57982,6204.0,CC BY-SA 3.0,,"Rather, you describe this peicewise density as the separate ""line densities"", but there's nothing wrong with describing the joint density as a peicewise function comprised of these separate densities. It's still a perfectly valid pdf.",2013-10-22 06:00:08.953
105218,57856,,CC BY-SA 3.0,,"You are confusing having a density with being smooth - a discrete distribution doesn't have a density either, but that doesn't mean its not smooth - for example a binomial(n,p) with n large is basically as smooth as a normal pdf",2013-10-22 06:45:33.520
105219,57991,21918.0,CC BY-SA 3.0,,"So, typically $y$ is not the random variable but $x$, right?",2013-10-22 06:49:13.697
105220,57991,6204.0,CC BY-SA 3.0,,"Y is usually a parameter on the pdf of X. In a frequentist setting y is normally a fixed value. In a Bayesian setting, Y is itself a random variable (as in the example I gave). X|Y can also be a conditional probability in the sense you mean, I was trying to give you the motivation behind why that quantity is called the likelihood.",2013-10-22 06:56:10.123
105221,57991,21918.0,CC BY-SA 3.0,,"With respect to the concrete example given in your answer, do you mean $\theta$ is actually a random variable, but in $X$'s distribution, it's taken as a parameter?",2013-10-22 06:59:44.253
105222,57995,22925.0,CC BY-SA 3.0,,"Thanks for responding. The stuff about beta distributions has given me some insights. However, I think the latter thing you mentioned is what I really want: to generate a sample that has a smallest value ""min"", a largest value ""max"", and a given sample mean?",2013-10-22 08:02:29.343
105223,57966,2081.0,CC BY-SA 3.0,,"From your (unusual) notatition one may see that `A` is the multivariate data matrix and `x` are the vector of regression coefficients. `A` implies a rectangular matrix. How can a rectangular matrix ever be invertible or uninvertible (unless we speak of generalized inversion)? In (6) you invert it, uninvertible by your own words. (7) can't follow from (6) because `A^-1 * A` would have been `I`, identity matrix, and not `1`.",2013-10-22 08:14:56.030
105224,57971,21638.0,CC BY-SA 3.0,,"If you want something other than a generic, approximate solution then I think you need to provide more details on the exact form of $f(x)$, $f(y)$ and $f(x,y)$.",2013-10-22 08:17:46.117
105225,57959,21985.0,CC BY-SA 3.0,,"Glen, thanks for the hint! That the ratio of the CDFs will be a constant is clear to me. But after some googling I still do not know how to proceed. Do I first calculate the second power of the two distributions and then calculate the ratio or vice versa? How do I get more info regarding the way that works? Thanks!!!",2013-10-22 08:22:02.463
105226,57995,22925.0,CC BY-SA 3.0,,"Ah, I see you added more information to how to get the desired results out of a triangular distribution. That's extremely helpful, and enabled me to accomplish exactly what I wanted. Thanks!",2013-10-22 08:53:29.747
105284,57990,20470.0,CC BY-SA 3.0,,"I see, I would recommend you to have a look at this great set of [introductory lecture slides](https://engineering.purdue.edu/kak/Trinity.pdf) in parameter estimation.",2013-10-22 15:09:56.113
105228,57984,21638.0,CC BY-SA 3.0,,I would start by defining the number of customers arriving at the shop in a given hour as $X \sim Pois(\lambda)$ and $k^{-1}X$ as the service time they require. Can you then see how to answer the first two parts using the properties of expectations and variances? Can you write down an expression for the final part? Show us your working so far and we can see where the difficulty lies.,2013-10-22 09:13:48.957
105229,57959,594.0,CC BY-SA 3.0,,"It's not a ratio of cdfs (cdfs are between 0 and 1, for starters), but a ratio of the inverse-cdfs (quantile function). You specify $\alpha$ (and the df for the $t$) and you can compute those quantiles and then take their ratio.",2013-10-22 09:39:07.483
105230,57966,22885.0,CC BY-SA 3.0,,"@ttnphns Oh, you right, that was stupid. I wanted to say that A'A is practically non-invertible because of numerical errors. But theoretically it still has a full rank. Can't I premultiply 5 by A' to get A'Ax = A'AVx* and then premultiply A'A by its inverse to cansel them? I'm doing inverse analiticaly, not numerically, so should I consider it to be a problem?
p.s.: Sorry for notation. It's my first question here. Probably, I need some time to figure out how it works.",2013-10-22 10:14:47.567
105231,57999,674.0,CC BY-SA 3.0,,Maybe providing a small sample of your data could help in getting more focused answers.,2013-10-22 10:44:35.727
105232,57962,10594.0,CC BY-SA 3.0,,"thanks @jbowman. I checked the two formulas. It seems that these two expressions are equal if $N\rightarrow\infty$, which is true in terms of cells. This is a characteristic of the exponential function: $e^{x}=lim_{n\rightarrow\infty}(1+\frac{x}{n})^{n}$. In our case: $e^{-\lambda}=e^{-N exp(-\beta x)}=lim_{n\rightarrow\infty}(1-\frac{N exp(-\beta x)}{N})^{N}$",2013-10-22 11:21:34.720
105233,58005,10135.0,CC BY-SA 3.0,,"If that is a homework, then add a ""self-study"" tag.",2013-10-22 11:30:55.777
105235,57959,21985.0,CC BY-SA 3.0,,"Sorry, sure we have iCDF... But I am still not sure about the solution. So $t_{1-\alpha/2}^2$ gets normal for n to infinity, right? So the term with the two distributions gets cancelled to 1? And then we can use the hint with Chi square. But because chi square for infinite n is normal, we end up with a standard normal distribution for the ratio of the two confidence intervals?",2013-10-22 11:35:07.210
105236,57319,1693.0,CC BY-SA 3.0,,"I've upvoted all 3 answers to date as useful, yet none of them seems clearly, definitively to answer the question (each neglects logistic reg., or introduces the ""no intercept"" condition);  they are at odds with one another; and the one with the most upvotes (3, besides mine) has what seems right now to be an empirical refutation, as my comment about the 5k regression shows. Thus I'm not ready to award a bounty.",2013-10-22 11:42:05.147
105237,57985,1895.0,CC BY-SA 3.0,,"Perhaps I am misreading it, but I don't think this answers the question (at least, head on). This only appears to address the *marginal* density of $u$, not anything related to a joint distribution (or density).",2013-10-22 12:01:28.763
105238,58009,9095.0,CC BY-SA 3.0,,"Thanks. There is no way I'll get the covariances, but even just the means can be helpful, given I have sample sizes and can make pretty good guesses about the variances given the other data I have. At the very least you have kept me from hitting my head against the wall all day trying to work this out.",2013-10-22 12:20:00.533
105239,57968,1895.0,CC BY-SA 3.0,,"Are you looking for an answer that Dr. Sheldon Cooper might appreciate, or one that Leonard or Raj would be satisfied with?",2013-10-22 12:22:56.220
105240,58004,22899.0,CC BY-SA 3.0,,Many thanks. I am much interested in getting hold of these books.,2013-10-22 12:40:27.947
105241,58011,436.0,CC BY-SA 3.0,,"It is not really clear what you are asking. You could start by telling us what you would consider ""bad"" and ""good"" data...",2013-10-22 12:40:37.953
105242,58011,12683.0,CC BY-SA 3.0,,What's the point of categorizing it? For statistical analysis that would generally be a bad thing to do; for simple description there's no sound or unsound way from a purely statistical perspective - it depends on the meaning of the data (are higher/lower values better?). And what's it got to do with factor analysis?,2013-10-22 12:41:37.813
105243,58010,22899.0,CC BY-SA 3.0,,These will be very helpful for me. Thanks for the guidance.,2013-10-22 12:43:27.727
105244,58011,10409.0,CC BY-SA 3.0,,"@nico that's what I'm asking.  How do I determine what's good and bad, based just on the data?",2013-10-22 12:44:29.127
105245,58005,,CC BY-SA 3.0,,Is $\sigma^2$ assumed to be known?,2013-10-22 12:44:30.457
105246,58007,22899.0,CC BY-SA 3.0,,"The first book seems to be popular from the reviews, will definitely look into it. The second also sounds promising. Thanks.",2013-10-22 12:45:29.803
105247,58011,12683.0,CC BY-SA 3.0,,"Round to the nearest whole number, then even numbers are good, odd numbers are bad.",2013-10-22 12:45:38.523
105248,58011,10409.0,CC BY-SA 3.0,,"@Scortchi I'm using it to classify data for NaiveBayes.  In R, this is called a factor, so I just assumed it was related.  Since you asked the question, I assume it is now not related at all.  Edit:  These are points assigned to a player.  Higher is better.  Negative is `terrible`.",2013-10-22 12:46:52.793
105249,58011,12683.0,CC BY-SA 3.0,,Sounds like a bad idea to be using any kind of classifier when the data is in fact continuous - see [here](http://stats.stackexchange.com/questions/68834/what-is-the-benefit-of-breaking-up-a-continuous-predictor-variable/68839). But people who insist on doing that typically do it either using meaningful cut-offs or equally sized bins.,2013-10-22 12:55:14.860
105250,57850,22092.0,CC BY-SA 3.0,,I add my reponse in the post. Please take a look when you have time.,2013-10-22 13:00:25.920
105251,57836,1411.0,CC BY-SA 3.0,,"that's something we wanted to improve.  The number of random-effects parameters (which come first) is `n_ran <- length(getME(model,""theta""))`; the number of fixed-effect parameters is `n_fix <- length(fixef(model))`.  Thus you should be able to use `params=(n_ran+1):(n_ran+n_fix)` to get just the fixed effect parameters profiled.",2013-10-22 13:03:30.363
105252,58011,10409.0,CC BY-SA 3.0,,@Scortchi thanks a lot.  This is why I asked.  I have no idea what I'm doing :),2013-10-22 13:05:05.543
105253,57948,22910.0,CC BY-SA 3.0,,"@Glen_b I use a standarized transformation which is consensed and accepted for this case. But my sample size is bigger than usual. The curve is visually very similar to normal distribution and the cut point is close to the end, but I don't have an statistical evidence of normality. I don't fully trust in K-S test, but I suppose the censored data has also an influence in a normality test.",2013-10-22 13:05:23.950
105254,57948,22910.0,CC BY-SA 3.0,,"@whuber Thanks for your comment, I have changed my question.",2013-10-22 13:06:22.987
105255,57996,17573.0,CC BY-SA 3.0,,"Can you give us an idea of how many words or powerpoint slides you can use to get the idea across?  What's your time budget, in other words?  I can talk for an hour on this subject.  When I lecture on it to undergraduates, I do talk for an hour on this subject.",2013-10-22 13:08:26.343
105256,57948,668.0,CC BY-SA 3.0,,"@Glen_b Although theory suggests square roots of counts should be approximately Normal (for largish counts), in practice a log transformation frequently works as well or better.",2013-10-22 13:11:06.710
105257,58011,2081.0,CC BY-SA 3.0,,"@Scortchi there are exceptions from `even numbers are good, odd numbers are bad` rule. E.g. `666` is terrible, `7` is excellent, etc etc",2013-10-22 13:14:47.887
105258,57976,668.0,CC BY-SA 3.0,,"@Glen_b Finding a polynomial bound for the logarithm of $f(\theta)$ is simple: the best possible one that is valid for all $\theta$ is $-k_1 - \theta^2.$ That follows from three observations: (1) $f(\theta)$ is symmetric about $0,$ (2) $\log(f(0))=-k_1,$ and (3) $\log(f(\theta))$ is asymptotically $-\theta^2.$",2013-10-22 13:15:10.743
105259,58004,20473.0,CC BY-SA 3.0,,"I second H.White's book as a _reference book of results_ regarding asymptotic theory for least-squares and Instrumental variables estimators, (It has not been written with education in mind), because it is very clearly organized in covering gradually different stochastic set ups (deterministic regressors, non-determinsitic, i.i.d. non-identical, non-independent etc.).",2013-10-22 13:15:23.997
105260,58011,668.0,CC BY-SA 3.0,,"Even if you do succeed in categorizing these numbers into five groups, that cannot (of itself) tell you *anything* about whether they are ""terrible,"" ""excellent,"" or anything in between: those are *value judgments* that cannot be determined solely from a bunch of numbers.",2013-10-22 13:21:03.757
105261,57988,4656.0,CC BY-SA 3.0,,"From $f(y)=-y^T\Omega y+k\le p(y)$, doesn't it follow that $f(0) = k \leq p(0)$ and so if you want $p(0) = f(0)$, then it muse be that $k = p(0)$? Why drag in an unnecessary term $k$ to confuse the issue?",2013-10-22 13:22:03.040
105262,58010,1406.0,CC BY-SA 3.0,,"For introduction to asymptotics of unit roots and time trends I suggest looking at JD Hamilton's Time Series analysis. I totaly forgot those. I would say though, that results about unit roots in particular albeit not heard by statisticians are very well known by mathematicians working with stochastic processes. Functional central limit theorem used by unit root literature was first mentioned by Donsker in 1952. Also interestingly that is one of directions van der Vaart book is going, namely empirical processes and their convergence.",2013-10-22 13:39:08.017
105263,58010,22899.0,CC BY-SA 3.0,,"This sounds pretty interesting, did not know that unit roots have such a history",2013-10-22 13:41:54.313
105264,58010,1406.0,CC BY-SA 3.0,,"For the empirical processes nice book is Vaart's and Wellner's ""Weak Convergence and Empirical Processes"". But this is really deep probabilistic stuff, which requires very good mathematical background.",2013-10-22 13:42:16.107
105265,57998,21918.0,CC BY-SA 3.0,,"The reason why I introduce $\theta$ is this, in the machine learning book I'm reading, given a dataset $x$, and $y$ is the  corresponding target value, so to fit a model to this dataset, I can use MLE to estimate $\theta$ which is the parameter of the model, right?",2013-10-22 13:43:13.083
105266,58010,1406.0,CC BY-SA 3.0,,"@Kirsty, to be fair, not the unit roots have such a history, but weak convergence. Central limit theorem on which most of the econometrics is based is weak convergence of sequences of random vectors. Functional central limit theorem involves convergence of sequences of functions in nice spaces. Empirical processes generalise the notions of central limit theorem and law of large numbers to sequences (nets if you know what it is) of a very general random objects.",2013-10-22 13:46:38.193
105267,58011,10060.0,CC BY-SA 3.0,,Please at least tell people which direction (positive or negative) is considered good.,2013-10-22 13:48:40.697
105268,58011,9749.0,CC BY-SA 3.0,,"If that's all the data you have, then it would just be misleading to categorise it, the figures would speak for themselves in conjunction with your statement, that higher is better than lower.  You may also consider stating what your expected average is (it might or might not be zero) and what you think is acceptable.",2013-10-22 13:49:06.877
105269,16209,306.0,CC BY-SA 3.0,,"If nothing else, use a hash table from the package ""hash"". set the characters as key and the numbers as the values and the go through each one, converting the key to the value.",2013-10-22 13:55:42.697
105270,58000,306.0,CC BY-SA 3.0,,"I thought economists believed in ""In the long run, we are all dead"". Where does asymptotics come in then ?",2013-10-22 13:59:28.557
105271,58014,21918.0,CC BY-SA 3.0,,"Thanks for your answer, I update my post, please see my update.",2013-10-22 13:59:57.100
105272,58005,11262.0,CC BY-SA 3.0,,@Stijn yes ....,2013-10-22 14:03:01.973
105273,58014,16043.0,CC BY-SA 3.0,,"This update radically changed my understanding of the question. Initially, I thought you were regarding $y$ as a parameter and $x$ as your data. Now it appears that $(x,y)$ are data and you are interested in constructing a model that describes the relationship between $x$ and $y$. I'll modify my response as I have time.",2013-10-22 14:03:10.803
105274,57990,21918.0,CC BY-SA 3.0,,"@Berkan, well I actually try to figure out what likelihood is, given $x,y,\theta$.",2013-10-22 14:09:27.220
105275,58014,668.0,CC BY-SA 3.0,,+1 This is still a great answer: I hope you keep it largely intact even if you modify it to match the changes in the question.,2013-10-22 14:22:43.240
105276,58016,668.0,CC BY-SA 3.0,,"A strong counter-example to your last statement about off-diagonal elements in $\Sigma^{-1}$ is afforded by the simplest nontrivial example in two dimensions, $\Sigma^{-1}=\left(
\begin{array}{cc}
 \frac{1}{1-\rho ^2} & -\frac{\rho }{1-\rho ^2} \\
 -\frac{\rho }{1-\rho ^2} & \frac{1}{1-\rho ^2} \\
\end{array}
\right).$  The larger off-diagonal values correspond to *more* extreme values of the correlation coefficient $\rho,$ which is the opposite of what you appear to be saying.",2013-10-22 14:27:32.723
105277,16209,,CC BY-SA 3.0,,This question appears to be only about data minapulation in R and thus belongs on StackOverflow (where it's been asked numerous times already). ,2013-10-22 14:29:56.960
105278,58017,15827.0,CC BY-SA 3.0,,"You could do this and it is ""objective"". Put on the other side all the comments implying that you are degrading your data unnecessarily and arbitrarily.",2013-10-22 14:32:49.320
105279,58010,3922.0,CC BY-SA 3.0,,"The econometric unit root theory builds upon the theory of the [Wiener process](http://en.wikipedia.org/wiki/Wiener_process) -- the asymptotic distributions of test statistics are functionals of the Wiener processes, which have been studied since what, 1950s? I have seen one of the early editions of Doob's book, looked about twice as heavy as Hamilton (which, in turn, is at least twice as heavy as say ""the black Wooldridge"", which is three times heavier than Ferguson, which is the smallest of them all :) ).",2013-10-22 14:37:15.363
105280,58016,22399.0,CC BY-SA 3.0,,@whuber Right. I should get rid of the 'absolute' word in the last sentence. Thanks,2013-10-22 14:39:01.507
105281,58010,3922.0,CC BY-SA 3.0,,"Also, empirical processes (see [Kosorok's book](http://www.amazon.com/Introduction-Empirical-Processes-Semiparametric-Statistics/dp/0387749772) embraced more by econometricians than by biostatisticians) are important in studying convergence of the cdfs, which has applications in microeconometrics of income distributions, as well as some non-parametric econometrics, which I know less of.",2013-10-22 14:39:19.047
105282,58000,3922.0,CC BY-SA 3.0,,"@htrahdis, that's the essence of Kolmogorov's 0/1 law. Either you become famous, like Adam Smith -- or rather like Trygve Haavelmo or Clive Granger or James Heckman if we are talking about econometrics -- or you perish.",2013-10-22 14:43:06.440
105283,58000,3922.0,CC BY-SA 3.0,,"related: [Good econometric textbooks](http://stats.stackexchange.com/q/4612/5739), [Higher order asymptotics](http://stats.stackexchange.com/q/46164/5739)",2013-10-22 14:46:19.083
105310,58033,6630.0,CC BY-SA 3.0,,"So weakly informative prior is just a better name for slightly informative ""uninformative prior""?",2013-10-22 17:09:34.163
105285,57982,4656.0,CC BY-SA 3.0,,"@DavidMarx As pointed out to you already by cardinal, your answer gives the _marginal_ density of $\min(Y_1,Y_2)$ and this marginal density is a valid pdf as you say, but I too do not see how your answer describes the _joint_ density of $\min(Y_1,Y_2)$ and $V$.",2013-10-22 15:10:40.453
105286,57830,3922.0,CC BY-SA 3.0,,"Do *exactly what* 1000 times? The only meaningful thing to do 1000 times is to draw the sample of a given size from the reference probability distribution to get the simulated approximation to the finite sample distribution of $D_{\max}$ -- which, as @Glen_b explained, can be found exactly, without the need to simulate.",2013-10-22 15:14:43.497
105287,58021,16043.0,CC BY-SA 3.0,,Could you please define $I_0(\alpha)$?,2013-10-22 15:17:28.797
105288,57986,3922.0,CC BY-SA 3.0,,What is the  formula for Var[TE] that you are trying to reproduce? Where is it coming from?,2013-10-22 15:24:19.097
105289,57959,594.0,CC BY-SA 3.0,,"If you think of $\frac{z^2}{t^2(n-1)}$ as a constant, $g$ (it varies with $n$ and $\alpha$, which are constants), then $\Lambda \sim g\chi^2_{n-1}$. Asymptotically, $\lim_{n\to\infty} \frac{z^2}{t^2} = 1$. So $g\chi^2_{n-1}\to ...$ -- it's possible to carry the analysis further, but I don't know whether the intent if for you to do further approximation.",2013-10-22 15:29:17.367
105290,57986,5045.0,CC BY-SA 3.0,,"I derived it using the familiar sum of two weighted variables variance formula, but I must be missing a $N^{.5}$in the denominator.",2013-10-22 15:30:40.897
105291,57986,5045.0,CC BY-SA 3.0,,"Each treatment control difference is an RV, the covariances are zero, I am allowing for unequal variances in each of the groups.",2013-10-22 15:35:27.243
105292,57948,594.0,CC BY-SA 3.0,,"@whuber I think I need some convincing that it could be better (leaving aside the issue with log-0). The approximate variance-stabilizing transform is square root, while the one that approximately takes $\gamma_1$ (moment skewness) to zero (often called the 'symmetrizing transformation') is the $\frac{2}{3}$ power (see Anscombe residuals for example). That is if the mean is not so small that the discreteness is an issue (at which point we run into the log-0 problem left aside earlier), the square root is already too strong. There are other reasons one might use the log with a Poisson",2013-10-22 15:42:45.623
105293,57948,594.0,CC BY-SA 3.0,,@whuber Under some conditions I could accept 'approximately normal after a log transform' ... but that (that at best it's approximately normal) is the very point of my initial comment there. One shouldn't expect a hypothesis test to fail to reject ('support normality' was the way the OP put it) with large $n$. There are other issues but I thought that realization was essential to get through first.,2013-10-22 15:50:36.023
105294,57976,594.0,CC BY-SA 3.0,,@whuber Good point. I was so focused on trying to find a normalizing constant I wasn't even thinking about just trying to look for a bound right off.,2013-10-22 16:01:48.727
105295,58025,5237.0,CC BY-SA 3.0,,"Can you define ""MRF"" & ""CRF""?",2013-10-22 16:06:55.523
105296,57991,6204.0,CC BY-SA 3.0,,Just because something is a random variable doesn't mean it can't be a parameter. Welcome to the wonderful world of bayesian probability :),2013-10-22 16:07:05.840
105297,58019,594.0,CC BY-SA 3.0,,"""*I am wondering whether for ordinal regression with ordinal predictors there is any graph that can be used to visually display the relationship before the formal analysis.*"" -- see [this question](http://stats.stackexchange.com/questions/56322/graph-for-relationship-between-two-ordinal-variables/) for some suggestions, but of course any pairwise display has the same problem plotting $y$ vs $x_i$ does in an ordinary regression situation (the conditional relationship may be quite different from the unconditional one in such a display).",2013-10-22 16:12:30.340
105298,58019,594.0,CC BY-SA 3.0,,"As for measures of association, there are a number of ordinal-ordinal measures; one example is [tau-b](http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient#Tau-b)",2013-10-22 16:14:47.037
105299,58025,3183.0,CC BY-SA 3.0,,@gung Markov random field and Conditional random field.,2013-10-22 16:18:08.517
105300,58017,436.0,CC BY-SA 3.0,,"@NickCox: this would work well with a normal(-ish) distribution of the data, which looking at the histogram seem not really to be the case. That said, I think the trick here is just to be consistent. This **is** an arbitrary classification. There is no statistical reason to choose 2 sigma over 2.5 or 3. And to use mean and not median. However, as long as you always use this then you are fine. If you define terrible as ""< mean-2*sigma"" than that is terrible. If I define it as ""<median-3*sigma"" than that is also fine, as long as we do not mix the two definitions.",2013-10-22 16:20:29.130
105301,58029,16043.0,CC BY-SA 3.0,,"My understanding is that a weakly-informative prior expresses more about the researcher's attitude towards the prior, rather than any mathematical properties of the prior itself. The canonical example would be Gelman's recommendation of a Cauchy prior with location 0 and scale 5/2 for logistic regression.",2013-10-22 16:24:28.833
105302,58017,15827.0,CC BY-SA 3.0,,"@nico A Box-Cox transformation is being recommended as a prerequisite. Why do this at all? remains my reaction, and several of us are singing the same song.",2013-10-22 16:39:04.933
105303,58026,8958.0,CC BY-SA 3.0,,"This R package seems to be for this too, I didn't look into it thoroughly though: http://artax.karlin.mff.cuni.cz/r-help/library/CORElearn/html/rfClustering.html",2013-10-22 16:40:13.877
105304,58016,668.0,CC BY-SA 3.0,,"Thanks, but that still doesn't cure the problem: the relationship you assert between the off-diagonal elements of the inverse and the co-variation does not exist.",2013-10-22 16:43:50.050
105305,58031,668.0,CC BY-SA 3.0,,"In addition to questions of interpretability, what matters are the linear relationships among the columns of the design matrix, whereas what you are focusing on here are the *rows* of that matrix.  And don't forget that usually $(1,1,\ldots,1)'$ is a column of the design matrix!",2013-10-22 16:47:07.097
105306,57965,22914.0,CC BY-SA 3.0,,"@whuber, in what sense are the clean balls analogous to $N-C$ unique birthdays? Wouldn't they correspond to the days of the year without any birthday?",2013-10-22 16:53:45.727
105307,58030,22935.0,CC BY-SA 3.0,,"Aaah, I think after reading your interpretation of the sentence, I just misunderstood it. The thing is that my colleague argued that once additional edges are added from y variables to the x variables (as you typically have in CRFs), you would no longer have a MRF. He pointed me to this document for an explanation, but I wasn't really satisfied as his statement contradicted what I believed to know about MRFs. Thank you a lot for this answer!",2013-10-22 16:54:20.360
105308,57965,668.0,CC BY-SA 3.0,,"I understand ""clean"" to mean ""unmarked.""  By analogy, ""marked"" = ""birthday.""  You ask for the chance to come up with exactly $N-C$ marked balls.  The analogy interprets this as $N-C$ unique birthdays.  The ""clean"" balls correspond to days without birthdays in the sample.",2013-10-22 16:55:57.437
105309,58016,22399.0,CC BY-SA 3.0,,"@whuber I think it does. In your example, the off-diagonal elements are negative. Therefore, as $\rho$ increases the off-diagonal elements decrease. You can check this by noting the following: at $\rho = 0$ the off-diagonal element is $0$; as $\rho$ approaches $1$ the off-diagonal elements approach $-\infty$ and the derivative of the off-diagonal element with respect to $\rho$ is negative.",2013-10-22 17:03:27.877
105311,58033,,CC BY-SA 3.0,user31668,"I usually use ""uninformative"" as that is more common and indicates intent. Weakly informative is probably more accurate though, as all distributions carry some information (**unless** they are improper priors..but that is another discussion)",2013-10-22 17:13:26.337
105312,58037,20473.0,CC BY-SA 3.0,,Have you tried to work out the algebra involved?,2013-10-22 17:44:21.520
105313,58030,3183.0,CC BY-SA 3.0,,"If you add *directed* edges (like in CRFs), then it's not an MRF anymore, but *undirected* edges (like in Boltzmann Machines) are totally fine.",2013-10-22 17:49:57.907
105314,58039,22637.0,CC BY-SA 3.0,,I would love to see a rigorous proof as well. While I understand the principle I am puzzled by the fact that the value has to be at least 1 deviation away from the mean. Why precisely 1?,2013-10-22 18:00:40.713
105315,58039,503.0,CC BY-SA 3.0,,"I don't see what is confusing. The variance is the average. If you add something greater than the average (that is, more than 1 sd) it increases. But I am not one for formal proofs",2013-10-22 18:02:17.487
105316,58037,22637.0,CC BY-SA 3.0,,"Yes, I have.  I have subtracted the sample variance of n values from the variance of n+1 values and I have required the difference to be greater than zero. Yet I cannt quite figure it out.",2013-10-22 18:02:53.223
105317,58040,22637.0,CC BY-SA 3.0,,"A number whose absolute value is less than 1, when squared it is also going to be less than 1 in abs. value. Yet what I do not understand is that even if Z_N falls into that category, we are adding a positive value to σ, so shouldn't it increase?",2013-10-22 18:13:19.930
105318,58039,22637.0,CC BY-SA 3.0,,It could be greater than the average by 0.2 standard deviations. Why wouldn't it increase then?,2013-10-22 18:14:19.543
105319,57856,7007.0,CC BY-SA 3.0,,"No, I'm not confusing anything. Real samples are *finite*. So, if you believe that a binomial with, say, $n=25$ is ""smooth"", good luck and good bye.",2013-10-22 18:15:43.073
105320,58040,7927.0,CC BY-SA 3.0,,"Yes, you are adding a positive value, but it will be smaller than your average deviation from the mean and therefore reduce sigma. Maybe it would make more sense to consider the value as $Z_{N+1}$.",2013-10-22 18:20:20.977
