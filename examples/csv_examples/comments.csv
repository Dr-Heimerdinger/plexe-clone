id,post_id,user_id,content_license,user_display_name,text,creation_date
12757,8504,,CC BY-SA 3.0,,"Another use of KL-divergence worth mentioning is in hypothesis testing. Assume $X_1, X_2, \ldots$ are iid from measures with density either $p_0$ or $p_1$. Let $T_n = n^{-1} \sum_{i=1}^n \log( p_1(X_i) / p_0(X_i) )$. By Neyman--Pearson, an optimal test rejects when $T_n$ is large. Now, under $p_0$, $T_n \to -D(p_0 \,\vert\vert\, p_1)$ in probability and under $p_1$, $T_n \to D(p_1 \,\vert\vert\, p_0)$. Since $D(\cdot \,\vert\vert\, \cdot)$ is nonnegative, the implication is that using the rule $T_n > 0$ to reject $p_0$ is asymptotically perfect.",2011-04-09 02:06:10.990
12802,8504,115.0,CC BY-SA 3.0,,Indeed. that's an excellent example. And in fact most general versions of the Chernoff-Hoeffding tail bounds use the KL-divergence.,2011-04-10 02:07:17.000
12819,8478,,CC BY-SA 3.0,,A related question: http://stats.stackexchange.com/questions/4/assessing-the-significance-of-differences-in-distributions,2011-04-10 15:07:34.720
14478,9524,,CC BY-SA 3.0,,"nice idea for a collection ! @mods: Beside cw maybe renaming to ""movies every statistician should have seen"" or something like that ?",2011-05-07 12:28:31.583
14480,9524,,CC BY-SA 3.0,,@steffen Thx. Better to flag the question for mods attention in the future (we aren't notified with @mods).,2011-05-07 12:37:52.090
14485,9524,2872.0,CC BY-SA 3.0,,"By the way, I found this page, which might add some movies to the collection: http://world.std.com/~reinhold/mathmovies.html",2011-05-07 14:41:51.013
14487,9524,,CC BY-SA 3.0,,Nothing that will inspire anybody to take up mathematics. Stick to books.,2011-05-07 15:21:49.440
14497,9524,,CC BY-SA 3.0,,@Emre Maybe. But nevertheless it is entertaining for those who already deal with math (even more if the movie is presenting things wrong or with exaggeration).,2011-05-07 18:38:36.787
14507,9529,,CC BY-SA 3.0,,"An absorbing story and one I'd recommend, but it referred to math rather than truly involding math.  I'd say the same thing about the tv show Numb3rs.",2011-05-07 22:43:12.617
107646,9524,,CC BY-SA 3.0,,"You can't really say what will or will not inspire someone to do something, and that includes mathematics.  Plenty of stories of famous scientists and mathematicians who were inspired by some hokey thing they saw on TV.",2013-11-08 04:07:14.190
108284,9529,,CC BY-SA 3.0,,"@rolando2 I eagerly await the day when a movie can be made about some foundational mathematician or field of mathematics which is both interesting to the public and adequately involves math. I don't think a 2 hour epic soliloquy of Galois on the night before his fatal duel would stand up to Iron Man IV, but who knows? This is why the only movies about mathematicians are about insane mathematicians.",2013-11-12 18:55:29.100
139593,9529,9081.0,CC BY-SA 3.0,,"Maybe ""life of Alexander Grothendieck""  could be made into a blockbuster movie?",2014-05-25 11:13:49.153
141343,9524,,CC BY-SA 3.0,user46862,Counts as a book hoping to become a movie with lots of (stealthy) math in it: https://www.kickstarter.com/projects/1740916372/maths-thriller,2014-06-06 13:14:10.623
148165,80672,,CC BY-SA 3.0,,I feel like I see $\mathbb{P}()$ more in the context of measure theoretic probability.,2014-07-18 17:45:32.063
148169,80677,30778.0,CC BY-SA 3.0,,"Thank you, I have included $\text{Prob}()$ in an edit to my question. Also: <GASP> ""it *is* used to denote other things than 'probability'"" say it ain't so! ;) I think also that $\pi$ is sometimes used to describe the parameter corresponding to $p$ in a PMF.",2014-07-18 18:21:49.190
148179,80677,20473.0,CC BY-SA 3.0,,"Well, Alexis, GASP indeed, but this is why when reading a paper, never skip its preparatory sections -it is where the author defines the symbolic language he will use -and if he doesn't, he is sloppy.",2014-07-18 19:13:43.080
148848,80677,,CC BY-SA 3.0,,"I disagree on one point: I have mostly seen $p()$ used for a _continuous_ random variable --- the thinking being that its probability density function evaluated at a point is similar to but distinct from the probability mass function of a discrete random variable evaluated at a point, which is a probability and can be denoted by $P()$. It is also my impression that $P()$ is more common than $\text{P}()$.",2014-07-23 13:08:10.253
148888,80677,20473.0,CC BY-SA 3.0,,@Nagel That's interesting. In which field?,2014-07-23 16:05:36.683
149191,80677,,CC BY-SA 3.0,,"@AlecosPapadopoulos: I am sure I have seen it repeatedly in statistical machine learning; I thought I had seen it in pure statistics texts too, but I shan't say for certain.",2014-07-25 09:46:57.100
149196,80677,20473.0,CC BY-SA 3.0,,"@Nagel. I wouldn't know about machine learning, but in Information Theory, and Econometrics, lower case $p(x)$ denotes usually a probability mass function. $p$ alone denotes usually a specific probability. $P()$ is the general symbol for probability of any $()$. Probability density functions in mathematical statistics are predominantly denoted by $f(x)$ or better $f_{X}(x)$, and other function-related symbols like $g$ or $h$.",2014-07-25 11:11:00.010
223637,120364,,CC BY-SA 3.0,,"Double check your equation, in the book you have $\simeq$ instead of equality. Note also the $\widehat{\mathbf{W}}$ notation.",2015-09-02 11:39:53.723
249240,133686,,CC BY-SA 3.0,,"It's the stuff of sports or financial journalism to speculate why a team or  or firm is successful (or not): sometimes there's a plausible reason (often it's just speculation or empty: the team did badly because it played badly). Otherwise the statistical focus is on identifying that a particular value is higher or lower than expected from a model for the data. You have to go beyond that model to think why. There isn't a statistical sense in which you can say that particular variables already in your model are more or less important for particular observations, because the fit is collective.",2016-01-13 11:02:31.227
249279,133686,9081.0,CC BY-SA 3.0,,"From your comment answering @Nick Cox, I think you not so much want the importance of each variable on row level, in the abstract. What you want is to explain in words a decision taken based on the model. Say you reject the loan application if probability of default, from the model, is larger than a certain cutoff, like 0.01.  Then you want an explanation of that decision in legal terms?",2016-01-13 12:59:24.780
249280,133686,57794.0,CC BY-SA 3.0,,"Thanks for reply. Agreed that Variable importance is only valid at an overall model level from a statistical point of view. But in practical situations one might need to explain/justify the score. eg: Why a Loan application was declined (this is even a legal requirement) or Why should Sales team pursue a opportunity A rather than opportunity B (who's deal conversion probability score is higher), etc. So we need a good algorithm which while giving a reasonable justification about break-down of the score components, is still not contradictory to assumptions used in statistical model development.",2016-01-13 13:04:20.743
249283,133686,57794.0,CC BY-SA 3.0,,"@kjetil. Lets say we have a Neural net Model to detect Fraud applications. When i decline a application suspecting fraud, I would also need to explain #1 reason (variable) of decline, #2 & #3 reason of decline. It would be even better if i could say how much more impactful was #1 variable on the score than #2 variable on the score. I can do it with Logistic/Linear regression (as mentioned in my original question), but not using random Forrest, Neural Network etc. Which where i need help.",2016-01-13 13:16:40.647
253575,80677,,CC BY-SA 3.0,,"@AlecosPapadopoulos I'm also used to seeing $p(x)$ used to denote a density function, and $P$ to denote a probability mass.  This seems a common convention in physics (in my experience, at least).",2016-02-04 11:58:48.957
253589,80677,20473.0,CC BY-SA 3.0,,"@WillVousden Yes, notational conventions are not consistent across discipline, in many cases.",2016-02-04 13:32:11.863
317731,20582,,CC BY-SA 3.0,,I'd love to see a mathematical answer here -- something that starts from a derivation and demonstrates concretely where the two diverge and in what general case they are equivalent.,2016-11-30 18:15:14.330
320935,171590,,CC BY-SA 3.0,,"All variable importance measures tell you only how a specific *algorithm* used your data.  When you report variable importance, you should keep this in mind: ""This is a summary of how the algorithm I chose used my training data"".  There is no concept of variable importance that is algorithm independent that is being measured.  Consequently, the variable importance measures you get from different algorithms are not comparable to eachother.",2016-12-13 19:13:09.770
320936,171590,47569.0,CC BY-SA 3.0,,"@MatthewDrury then if the goal is to rank attribute relevance, could somehow averaging the weight of multiple algorithms be a good approach? Like if both SVM and RF are saying `A` is the most relevant, could we say `A` is potentially the most relevant? How do you compare this approach to a simple correlation test between `A` and `Y`?",2016-12-13 19:17:50.943
320945,171590,,CC BY-SA 3.0,,As the poster of this question: http://stats.stackexchange.com/questions/202277/what-are-variable-importance-rankings-useful-for  I'm not sure I'm the best person to help you.  I've stopped believing that it's even possible to rank the influence of predictors in a model independent way.,2016-12-13 19:52:15.420
341443,182063,,CC BY-SA 3.0,,What model do you use? Logistic regression?,2017-03-11 09:08:27.207
341573,182063,109482.0,CC BY-SA 3.0,,It's a question about a general classifier. It could be linear or nonlinear.,2017-03-11 22:51:57.133
342001,182191,109482.0,CC BY-SA 3.0,,"By ""gradient signal"", do you mean the the norm of the gradient? That is what the links you've provided talk about.",2017-03-13 20:36:04.087
342028,182191,7483.0,CC BY-SA 3.0,,"@JohnKleve Yeah, basically. I edited in a little math showing what I think they mean, which more or less just reinforces what I said before about KL versus $L_2$ distances between probability distributions.",2017-03-13 22:17:24.333
427314,228594,,CC BY-SA 3.0,,Is this unsupervised learning ? It seems that you are training the model to be more confident about predictions that it might not have been confident about. This will very likely lead to bad solutions.,2018-04-23 15:03:10.413
427740,228594,136479.0,CC BY-SA 3.0,,"No I have labelled data. So the model's predictions is based on my labelled data. after the model predicts new and unlabelled data, I would like to use them (the predictions that pass the certainty threshhold) as part of my training data for the model to improve the model.",2018-04-25 02:18:20.027
442240,217992,9081.0,CC BY-SA 4.0,,"https://hub.hku.hk/bitstream/10722/189876/1/Content.pdf   http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.828.5088&rep=rep1&type=pdf   http://personal.vu.nl/s.j.koopman/projects/ScoreWorkShop2013/Steve%20Thiele%20talk.pdf   you will find more by searching for ""autocorrelation timevarying"". Can you find an answer in any of this?",2018-07-09 09:17:01.913
445714,212670,14355.0,CC BY-SA 4.0,,"What update are you referring to? I merely edited the tags to make the question better visible for others looking to sample from the SN distribution, that's why I replaced the normal with the skew normal tag. Your answer was what helped me resolve this, that's why I accepted it back in november. What's the problem here?",2018-07-27 12:46:30.923
445765,212670,5179.0,CC BY-SA 4.0,,"OK, apologies then, I though you had moved from skew Normal to truncated skew Normal...",2018-07-27 16:33:00.710
466818,9524,,CC BY-SA 4.0,,"""Mission Impossible""",2018-11-13 14:38:19.080
496116,266712,,CC BY-SA 4.0,,"Why do you think ""ARIMA$(p,d,q)$ is actually ARMA$(p+d,q)$""?",2019-04-07 15:50:43.970
496118,266712,196216.0,CC BY-SA 4.0,,It is here in Definition section: https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average,2019-04-07 15:53:22.927
496134,266712,,CC BY-SA 4.0,,"Note also the phrase ""having the autoregressive polynomial with $d$ unit roots"", which distinguishes from a stationary ARMA process whose AR polynomial has no roots in the unit circle. A fast search gives e.g. [these slides](http://www.fsb.miamioh.edu/lij14/672_2014_s6.pdf) as an overview of the unit root topic.",2019-04-07 17:18:00.263
496142,266712,196216.0,CC BY-SA 4.0,,"So, are the following true then? 1. An ARIMA is a non-stationary ARMA; 2. A stationary ARMA is an ARIMA with d = 0.",2019-04-07 17:32:29.983
496147,266712,,CC BY-SA 4.0,,"2. True. 1. A nonstationary ARMA can be just that, a nonstationary ARMA (e.g. ARMA$(1,0)$ with the AR coefficient >1), not necessarily an ARIMA.",2019-04-07 17:44:34.603
580109,8504,199619.0,CC BY-SA 4.0,,@cardinal Is that an alternative to KS testing distribution equality?,2020-06-06 19:37:41.717
621994,332808,35995.0,CC BY-SA 4.0,,Is that typically called *dominance analysis*? (Just curious.),2020-12-18 09:35:24.337
621996,332808,254239.0,CC BY-SA 4.0,,"I am not versed in this topic, but I think this is a term that is being used in the last years.",2020-12-18 09:49:19.637
622041,332808,,CC BY-SA 4.0,,"hi: dynamic regression is covered in hendry's ""dynamic econometrics"" text. also, harvey's latest text ( I think it's called analysis of  econometric time series ) has some chapters that cover it. pankratz has a text that's full of examples but it's pretty old now.",2020-12-18 14:02:30.543
622042,332808,,CC BY-SA 4.0,user234562,"Not quite. Dominance, also known as stochastic dominance, is more appropriately used ""in decision theory and decision analysis in situations where one gamble (a probability distribution over possible outcomes, also known as prospects) can be ranked as superior to another gamble for a broad class of decision-makers"" (https://en.wikipedia.org/wiki/Stochastic_dominance). A more appropriate concept for your situation is the *relative importance* of variables in a statistical model, as is inherent in the acronym *relaimpo* or effect size.",2020-12-18 14:04:45.347
622043,332808,,CC BY-SA 4.0,,"Note that none of the references I mentioned discuss dominance analysis, effect size or relaimpo so they may not be what you're looking for. I would take a look at them before you purchase.",2020-12-18 14:06:08.597
622045,332808,254239.0,CC BY-SA 4.0,,"@mlofton, thank you for the references, I will check them out. I have some experience with these models, my problem is related to the relative importance matter.",2020-12-18 14:22:51.253
622046,332808,254239.0,CC BY-SA 4.0,,"@user332577 thank you for your input. I agree with your comment, however it seems it has been used in the context of relative importance for linear regressions as well: 1. Budescu, D. V. (1993). Dominance analysis: A new approach to the problem of relative importance of predictors in multiple regression. 1. Azen, R., & Budescu, D. V. (2006). Comparing Predictors in Multivariate Regression Models: An Extension of Dominance Analysis. 2. Luo, W., & Azen, R. (2013). Determining Predictor Importance in Hierarchical Linear Models Using Dominance Analysis.",2020-12-18 14:24:00.477
623553,333664,59440.0,CC BY-SA 4.0,,"Pattern, look at the cross correlation between the time series. Magnitude, look at its average magnitude, and use something like a squared difference as a (dis)similarity metric.",2020-12-29 02:35:56.643
623554,333658,37792.0,CC BY-SA 4.0,,"do you mind explaining a bit more about pre and post multiplying with B, I was planning to take the Cholesky decomposition: $\Sigma = RR^T$ where R is the upper triangular",2020-12-29 02:45:27.340
623159,333419,,CC BY-SA 4.0,,"This sounds like a few-shot domain adaptation meta-learning problem. There's many different ways to solve this type of problem (which is, admittedly, a very new/active area of research in ML), so I don't think there's a ""correct"" answer for your situation. Siamese networks are a *metric-based* approach; prototypical networks (https://arxiv.org/abs/1703.05175). You can find a brief overview of meta-learning techniques at https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#define-the-meta-learning-problem; it might be worth it to pick 1 and experiment.",2020-12-26 02:26:53.557
623160,333419,,CC BY-SA 4.0,,"Furthermore, I think you might have the wrong setup in your current approach -- metric-learning approaches generally share a feature extraction backbone; i.e. you'd pass each input embedding through your network to generate [learned] output embeddings, then do something in the output space (i.e. clustering for prototypical networks).",2020-12-26 02:29:18.517
623194,333419,66768.0,CC BY-SA 4.0,,"Thanks tchainzzz, I have updated my problem description.",2020-12-26 11:58:15.543
623320,333536,35995.0,CC BY-SA 4.0,,What's wrong with a constant forecast? Would you take a nonconstant one even if its accuracy were lower?,2020-12-27 15:36:00.563
623384,333536,,CC BY-SA 4.0,,"Simple models in exponential smoothing, which never change, can be the most accurate forecast.",2020-12-28 00:17:37.363
623419,333537,5179.0,CC BY-SA 4.0,,Does this answer your question? [Probability that Secret Santa arrangement for couples will result in perfect pairings](https://stats.stackexchange.com/questions/502555/probability-that-secret-santa-arrangement-for-couples-will-result-in-perfect-pai),2020-12-28 08:15:11.883
623457,333601,,CC BY-SA 4.0,,I’m voting to close this question because it needs to be in English.,2020-12-28 13:57:51.123
623464,333537,170017.0,CC BY-SA 4.0,,"@Xi'an, the rules and the desired pairings are different in the two questions. In this question, the desired pairing is to have all couples paired. In the other question, the rules forbid any couples from being paired.",2020-12-28 14:28:14.290
623491,333536,254958.0,CC BY-SA 4.0,,"@RichardHardy, nope, absolutely not. But the previous values of the variable weren't constant and in that case, getting a constant forecast for it just seems a bit odd.. hence I'm a bit sceptical of using these forecasts.",2020-12-28 18:16:58.123
623492,333536,254958.0,CC BY-SA 4.0,,"@user54285, which never change?",2020-12-28 18:18:52.913
623493,333536,35995.0,CC BY-SA 4.0,,"If the data generating process is a random walk or white noise, previous observations will not be constant, yet an optimal (under symmetric loss) point forecast is a constant.",2020-12-28 18:23:43.090
623494,333634,199619.0,CC BY-SA 4.0,,Our Frank Harrell has a nice post about SMOTE: https://twitter.com/f2harrell/status/1062424969366462473?lang=en.,2020-12-28 19:06:19.400
623518,333647,,CC BY-SA 4.0,,"Both are equally likely  (Proof: by definition, the increments of a random walk are independent, whence they are exchangeable, and these walks have the same increments up to a permutation.) What you need to do, somehow, is capture the *pyschology* of human pattern identification in order to characterize which sample paths might get your attention as being ""unusual.""  Usually, for our observations to be objectively meaningful, we specify those events beforehand and make them relevant to whatever it is we are studying.",2020-12-28 22:26:30.260
623519,333419,,CC BY-SA 4.0,,"I would also try siamese network + metric learning. 1. Embed kids, embed pets using the same network, 2. The output is a vector of size $n$, 3. Loss function $(y - u^Tv)^2$ where $u$ are kids, $v$ are pets and $y \in \{0, 1\}$ is the ground truth. 4. During prediction, try $uv_i^T$ for all $0 \leq i \leq 9$ and pick the maximum $i$. See https://arxiv.org/pdf/1908.10084.pdf for some other loss functions or page 6 https://arxiv.org/pdf/1705.08039.pdf equation (6).",2020-12-28 22:33:21.670
623533,333647,9081.0,CC BY-SA 4.0,,"Do you have some context, or is it just some exercise? If you look at the steps, that is basicalla an uo-or-down, that is, binary process. If you have some reason to doubt the independence of these steps, then the null of independence can be tested with a runs test, maybe. Search this site ... but your sample size is to small to get much, I guess.",2020-12-29 00:31:49.817
623541,333661,255099.0,CC BY-SA 4.0,,"Thanks for the thorough reply! In the sunburns example, the standard way to address endogeneity like this would be adding the temperature as a variable to test for; this would eliminate the relationship between sunburns and sardines. Here, is it possible to do something like that within the context of a Granger test?",2020-12-29 01:21:38.680
623543,333661,255099.0,CC BY-SA 4.0,,"Also, for Convergent Cross-Mapping in particular, Sugihara shows some results in this video (https://www.youtube.com/watch?v=uhONGgfx8Do&t=1153s) that seems to address this endogeneity problem. 
Here, the population of sardines and anchovies appear to be correlated (like sunburns and anchovies), but somehow CCM is able to ""see"" that they aren't causally linked. Meanwhile, temperatures and anchovies, and temperatures and sardines are each shown as causally linked. Do you have an idea of how they were able to accomplish this?",2020-12-29 01:29:29.887
623544,333662,,CC BY-SA 4.0,,Yes - for example you can then use Chebyshev's inequality,2020-12-29 01:29:39.427
623545,333663,199192.0,CC BY-SA 4.0,,"Welcome! So you observe a subset of countries over a four year period, correct? Do you observe all countries before *and* after your treatment? That is, does each country have a precise pre-treatment *and* post-treatment period?",2020-12-29 01:30:26.233
623547,333663,255099.0,CC BY-SA 4.0,,"Thanks! 

The time-series X is the ""treatment"" so the quantity of treatment varies over time. So strictly speaking, no, but I can define a threshold: once the total/historical quantity of X_c reaches some amount, consider country c ""treated"". I could also ""bucketize"" treatment values - so that each country is in one of several possible treatment ""states"" (e.g. low/medium/high) at any given time. However, not all countries will reach the threshold at all - so some countries won't see a post-treatment period at all, or would be ""low"" for the entire time period.",2020-12-29 01:34:18.190
623548,333663,199192.0,CC BY-SA 4.0,,"Why won’t they see a post-treatment period? I would assume you’re observing outcomes on all your entities in all time periods, regardless of whether they meet your predefined threshold or not, right?",2020-12-29 01:39:22.783
623549,333663,255099.0,CC BY-SA 4.0,,"Oh yes, that's correct. I have data on the whole time series, start to end, for every country. My point was just that some countries never get treated at all. They go from 2017 to 2020, with the time series for X remaining 0 or near-0 for the entire time. Sorry for the confusion!",2020-12-29 01:41:25.340
623550,333663,199192.0,CC BY-SA 4.0,,That’s fine. So control entities have zero intensity over the full series. Then I presume treated entities go from the absence of intensity to some ‘positive jump’ in intensity midway through the panel?,2020-12-29 01:48:05.940
623552,333661,154203.0,CC BY-SA 4.0,,"I think that sardines and anchovies are different example than sunburns and sardines. I intended the variable 'sunburns' to be reacting immediately on temperature. Anchovies and sardines probably both react after some time. Therefore it is possible, that they do not predict each other, but temperature predicts both of them. If you want to make more drastic example, think that instead of sunburns we put the exact measurement of the thermometer floating on the sea. If thermometer breaks, will population of sardines change?",2020-12-29 02:01:45.347
623555,333664,35053.0,CC BY-SA 4.0,,"Thanks for your comment @Oxonon. But wouldn't they provide two, conflicting measures of dissimilarity? Is there 1 method that can combine the two?",2020-12-29 02:59:48.820
623558,333663,255099.0,CC BY-SA 4.0,,"Yes, that's right. The increase in intensity is, generally speaking, continuous/""smooth"", but it gets ramped up pretty rapidly midway through the panel, for treated entities.",2020-12-29 04:12:52.900
623559,333661,255099.0,CC BY-SA 4.0,,"Makes sense! Thanks for the explanation. So when applying a technique like CCM, do you know of any ways to ""close forks"" or separate the influence of temperature on anchovies, from the influence of sunburns on anchovies? Wondering how to apply the idea from multivariate regression to this context.",2020-12-29 04:21:59.300
623561,333663,199192.0,CC BY-SA 4.0,,"Thank you for clarifying. Two more questions. First, what is your treatment in this case? Generally, if you’re using difference-in-differences your treated countries shouldn’t be “treated” in the pre-period. Second, why do you feel you can’t implement a model with country *and* year fixed effects?",2020-12-29 04:42:44.607
623563,333658,128628.0,CC BY-SA 4.0,,"The variance rules for linear transforms are $\mathbb{V}(B \epsilon) = B \mathbb{V}(\epsilon) B^\text{T}$ and $\mathbb{V}(\epsilon B) = B^\text{T} \mathbb{V}(\epsilon) B$, so the variance you will get with your formula (using post-multiplication) is $B^\text{T} B \neq B B^\text{T}$.  Unless I am mistaken, the Cholesky decomposition doesn't allow you to swap the order of the parts.",2020-12-29 04:55:38.710
623567,333647,129041.0,CC BY-SA 4.0,,"RE: *""the increments of a random walk are independent""* — could testing the serial correlation of the steps tell anything? In this example, $stepsA$ have serial correlation $0.56$ while $stepsB$ have serial correlation $1.00$. I appreciate that any permutation could occur, but it is very unlikely that a permutation with serial correlation $1.0$ occurs. If you individually check all the $10!$ permutations there are few permutations like that. By contrast, there are many more permutations with serial correlation $0.0$",2020-12-29 06:18:22.870
623571,333675,179252.0,CC BY-SA 4.0,,This is super helpful! Thanks!,2020-12-29 09:41:47.460
623573,333677,5179.0,CC BY-SA 4.0,,"Since the tosses are independent, what happened in the 26 previous tosses does not impact the probability of a tail in the next toss.",2020-12-29 09:49:47.660
623574,333668,158099.0,CC BY-SA 4.0,,The split 84:16:16 doesn't add up to 100. Not sure what you mean by that.,2020-12-29 09:50:02.633
623576,333672,5179.0,CC BY-SA 4.0,,"Kolmogorov-Smirnov or any other goodness-of-fit test only assesses one aspect of the RNG. There is a large literature on PRNG tests, starting with [George Marsaglia's DieHard battery of tests.](https://en.wikipedia.org/wiki/Diehard_tests) Note also that the outcome of the KS test should be compared to a Uniform distribution, rather than using a p-value, which is much less informative since binary.",2020-12-29 09:55:37.707
623577,333671,5179.0,CC BY-SA 4.0,,"The proof for the radius distribution is that $(\rho,\theta)$ has density$$\rho\mathbb I_{0<\rho<R}\mathbb I_{0<\theta<2\pi}/R^2\pi$$ when moving from Euclidean to polar coordinates. Hence $r=\rho^2$ has density$$\mathbb I_{0<r<R^2}/R^2$$",2020-12-29 10:00:52.650
623579,333662,5179.0,CC BY-SA 4.0,,"The variance does not ""describe"" the Normal distribution but is one of its parameter. In that sense, the variance better describes distributions with a single parameter like the Bernoulli or the Exponential or the $\chi^2$ distributions.",2020-12-29 10:16:21.523
623580,333677,,CC BY-SA 4.0,,"If anything betting on tails is the only answer that you cannot really justify. Betting on heads through an evidence of bias argument, or betting on no impact through a gamblers fallacy argument are the only real two bets you can justify.",2020-12-29 10:18:54.840
623585,333677,144189.0,CC BY-SA 4.0,,"So what is the rational argument to change my biased human intuition that tells me to bet on tails simply because ""after 26 consecutive Heads, the coin must toss Tails very soon""?",2020-12-29 10:52:46.970
623587,332131,80394.0,CC BY-SA 4.0,,Apparently we are talking about 6 columns of 48 observations each. So you could easily copy the `dput` of the original data here into the forum so people could reproduce the problem which is usually a great help in finding solutions.,2020-12-29 11:08:54.770
623591,333536,254958.0,CC BY-SA 4.0,,"@RichardHardy understood, thank you",2020-12-29 11:40:26.633
623594,332131,216426.0,CC BY-SA 4.0,,The question is updated now,2020-12-29 12:43:26.500
623597,333419,66768.0,CC BY-SA 4.0,,"Thx, @displayname. To be discussed / tested on my side...",2020-12-29 13:07:49.973
623604,333688,,CC BY-SA 4.0,,Refer to [this](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) tutorial for typesetting math here.,2020-12-29 13:54:13.147
623606,333689,20286.0,CC BY-SA 4.0,,"Does this answer your question? [How to test the statistical significance for categorical variable in linear regression?](https://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr) Although the question is about linear regression, the principles are the same for your Cox regression. The `anova()` function in the R `rms` package provides a way to evaluate the overall significance of a categorical predictor. There are special ways to deal with ordinal categorical predictors.",2020-12-29 14:08:46.843
623610,333601,232582.0,CC BY-SA 4.0,,"Interesting behavior. I think it may have something to do with the parameters you chose to fit the random forest. Mind sharing all of them (e.g. max tree size/min node size, # of trees, # regressors randomly chosen) as well as what the response and the regressors are (i.e. purpose of your prediction)? Also what package did you use (if R) and what is the code syntax?",2020-12-29 15:05:21.787
623611,333690,158099.0,CC BY-SA 4.0,,"You may need to explain what the average here is. Is it the second version's average points? If it is, what is the cut-off?",2020-12-29 15:11:19.553
623612,333691,69747.0,CC BY-SA 4.0,,"The assumptions are always wrong, so any non-rejection is automatically a Type II error.",2020-12-29 15:34:27.670
623613,333691,226509.0,CC BY-SA 4.0,,Maybe my question was unclear. Does the type II error from step 1 (checking homogeneity) inflates Type I error in step 2 (ANOVA sensu stricto) ?,2020-12-29 15:36:38.243
623614,333689,20286.0,CC BY-SA 4.0,,"""With Cox regression each level has two different means or values according to whether the outcome is 0 or 1"" isn't really correct. Each coefficient for a non-reference level of a categorical predictor in Cox regression represents the difference in log-hazard between that level and the reference level. You can think about those coefficients as playing the roles of the regression coefficients and associated estimated means in linear regression, with similar interpretations in terms of interpreting p-values.",2020-12-29 15:37:49.667
623615,333691,226509.0,CC BY-SA 4.0,,"I've found the following : ""There is a serious problem with this
approach that is universally overlooked. The
sequential nature of testing for homogeneity of
variance as a condition of conducting the
independent samples t test leads to an inflation of
experiment-wise Type I errors."" source : https://digitalcommons.wayne.edu/coe_tbf/23/",2020-12-29 15:40:09.537
623616,333700,,CC BY-SA 4.0,,Similar to this question https://stats.stackexchange.com/questions/363797/regression-as-a-way-to-determine-variable-importance,2020-12-29 15:48:55.483
623617,333702,241161.0,CC BY-SA 4.0,,Thanks! probably I can add 2 more feature selection to the L1 logistic regression: one would be a random forest ranking the features via MDI and another always a random forest but ranking via permutation importance..I would then train the model with these different sets (if different) and evaluate which is the best by checking performance on the test set. does it sound good in your opinion?,2020-12-29 15:51:55.357
623619,333691,57214.0,CC BY-SA 4.0,,After ANOVA you can look at residuals to see if they show departure from assumptions of homoscedasticity and normality. // Maybe most important is whether effect sizes are large enough to be of practical importance. // Also do _post hoc_ comparisons of levels of the factor in the ANOVA make sense? Did these tests confirm significance where you expected to find it?,2020-12-29 15:59:25.817
623620,333702,136436.0,CC BY-SA 4.0,,"Yep exactly, although idk if i'd bother with both types of feature importance, but can't hurt to see how they compare. Whatever model/feature set combo has the best performance on the test set at the end of the day is what you go with. And the more experiments you try the better. Another thing you could try is multiple levels of top features. So try all the features RF, then just use the top 80% most important features, then the 60% most important (where the feat importance list is recreated from the 80% run), then 40%, then 20%. This is kinda like tuning the L1 reg parameter.",2020-12-29 16:01:18.310
623621,333702,136436.0,CC BY-SA 4.0,,"See the ""backwards elimination"" section of wrapper methods: https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/#:~:text=Top%20reasons%20to%20use%20feature,the%20right%20subset%20is%20chosen.",2020-12-29 16:03:22.750
623622,333690,57214.0,CC BY-SA 4.0,,"In (1) I worry about 'accept the hypothesis'; _which_ hypothesis ?Exceeding critical value should lead to rejection of null hypothesis that new version is ineffective. (To believe new version effective.) // Similarly, in (2) small P-value leads to rejection of null hypothesis, to conclude new version effective.",2020-12-29 16:11:52.163
623623,333702,241161.0,CC BY-SA 4.0,,"thanks for the suggestion. I would say I have three different methods now I can try: L1-LG, RF with permutation or MDI, and wrapper methods. All quite different in nature I would say",2020-12-29 16:14:59.410
623626,333688,252107.0,CC BY-SA 4.0,,"Hint: enumerate the rationals, then for each rational that you haven't yet covered with an element of $A$, cover it with an element from $A$ less than half the measure of the previous element. You should end up with a countable set of disjoint elements of $A$ which between them cover all rationals, but have finite total measure.",2020-12-29 16:29:42.737
623627,333710,252107.0,CC BY-SA 4.0,,"The probability of getting heads given a Beta(a, b) distribution is $(a+1)/(a+b+2)$. Should be possible solve for this being at least 2/3.",2020-12-29 16:40:19.790
623628,333686,,CC BY-SA 4.0,,"A simple model in exponential smoothing will be exactly the same in all future months. Basically a straight line. But in my experience its often right. The best predictor of the future is the most recent point (a simple model is not the most recent value, I think its a weighted average of recent points although I am not sure).",2020-12-29 16:40:43.127
623629,333679,144189.0,CC BY-SA 4.0,,"Good point. This makes me think that knowing for certain the coin is fair, then at some point we need to observe at some Tails. Therefore, there must be a way to calculate the probability of T for the next tosses in order for the coin to be fair. What do you think?",2020-12-29 16:41:36.980
623630,333710,254952.0,CC BY-SA 4.0,,"I assume that you arrived at this by taking the mean value of Beta, in the manner of the Bayesian approach I described. Could you elaborate on why this is the correct approach?",2020-12-29 16:48:43.270
623632,333665,150694.0,CC BY-SA 4.0,,"Thank you for your answer Isabella! I think the key statement was: ""worrying about autocorrelation of the model residuals only makes sense if your response values were collected over time."" Since I have an experimental design (but collected at a single point in time), then I interpret this as: I do not have to check the autocorrelation assumption in this case. I also edited my question at your request for clarity.",2020-12-29 16:53:21.237
623633,333677,5179.0,CC BY-SA 4.0,,"The rational attitude is to question the fairness assumption. (Even though [coin tossing is always fair!](https://www.npr.org/transcripts/7320273)) Or the ""observation"" of 26 tails in a row.",2020-12-29 16:57:36.430
623635,333677,144189.0,CC BY-SA 4.0,,I agree. But let's assume we know for certain the coin is fair. What is the probability of the 27th toss to be Tails in order for the coin to be fair? Please let me know if I explained myself.,2020-12-29 17:11:26.273
623637,333710,252107.0,CC BY-SA 4.0,,"Oops, I meant $a/(a + b)$. For example, if you start with a Beta(1, 1) prior and then see 3 heads and 1 tails, you should assign probability 4/6 to getting heads on the next flip.",2020-12-29 17:19:28.433
623639,333677,5179.0,CC BY-SA 4.0,,"The (joint) probability for having 27 tail tosses in a row is $1/2^{27}$, just like the probability of any fixed sequence. The marginal probability of having a tail on the 27th toss remains $1/2$.",2020-12-29 17:23:36.217
623640,333716,30778.0,CC BY-SA 4.0,,"+1 And, as with @29740 's comments, the italic-serif/Roman sans-serif somewhat parallels the Latin/Greek convention for estimate/estimand.",2020-12-29 17:30:56.937
623641,333712,,CC BY-SA 4.0,,"General statistical procedures would be problematic if their results depended on the units of measurement we chose.  Since standardization is merely a change of those units, we would be in trouble if that *did* change the p-values!",2020-12-29 17:33:34.333
623644,333663,255099.0,CC BY-SA 4.0,,"The treatment is the quantity of the variable X. I could use the sum of all the previous values of X, or just the current value of X. At the start of the time series, x will be 0 for all the countries, so they won't be treated in the pre-period. Country fixed-effects, if I understand correctly, is the same as difference-in-difference. The difficulty with adding time fixed-effects is that my data has a daily sampling rate. Adding a dummy for each day, week, or even month seems excessive (maybe not?), but time affects Y ""sharply"" - its effect can change quickly from one week to the next.",2020-12-29 17:47:44.523
623648,333663,199192.0,CC BY-SA 4.0,,"The generalized difference-in-differences estimator requires country *and* time fixed effects. We usually use this estimator in settings where treatment starts at different times for different countries. Is that the case for you? I would assume so, unless this is a major shock hitting all countries simultaneously. How many countries do you have?",2020-12-29 18:04:28.677
623650,333663,255099.0,CC BY-SA 4.0,,"I have almost all the countries in the world. Treatment does start at different times, but for many (I think even most!) countries, yes, there is a ""shock"" that hits a lot of countries in the same few months. 

A related question is whether country and time fixed effects should kind of ""intersect."" My guess is that the time effects are somewhat different in each country. I'd want to see the effect of time _given_ a value for the country dummy, rather than the overall effect of time, holding the country and X constant.",2020-12-29 18:09:47.333
623651,333671,102906.0,CC BY-SA 4.0,,@Xi'an    Don't understand the double-bar notation.,2020-12-29 18:10:53.893
623653,333663,199192.0,CC BY-SA 4.0,,That’s good news. So you have daily observations of $y$ from 2017-2020?,2020-12-29 18:32:28.090
623654,333709,255139.0,CC BY-SA 4.0,,"Sorry, I will develop it further. The seeds were collected and brought from Iceland to Sweden for a common garden experiment. In each block (a-h), 20 seeds were planted in individual pots from each of the 10 populations. This was done in spring. During the winter, snow was removed from four of the blocks (a-d), and the following spring we measured survival. Furthermore, over the summer we also recorded phenology and at the end of the study we measured above ground biomass.
I hope this clearifies it.",2020-12-29 18:33:32.807
623655,333672,255103.0,CC BY-SA 4.0,,"Hi, thank you for that information -- I had never heard of this and I will look through it to see if a test suits my needs! I should note though that I already have confidence that the RNG itself is producing uniformly random numbers; I just want to test my derived function F. For my K-S test, I'm currently using a 1-sample test against the uniform distribution and comparing the result to a critical value I found in a table for α = 0.01 (1.63 / √N). Are you saying that I should perform a 2-sample test that should itself produce uniform p-values? What would the second sample be in that case?",2020-12-29 18:38:16.800
623656,333722,,CC BY-SA 4.0,Glingol,"Hey Ben, precisely. My goal in the end is to understand how to handle this kind of variable in a logistic regression and to understand the impact of the coefficients.",2020-12-29 18:56:13.750
623657,333663,255099.0,CC BY-SA 4.0,,That's right! For each of ~220 countries.,2020-12-29 19:08:18.327
623659,333667,,CC BY-SA 4.0,,"You may always take $R=1$ by choosing the disk's radius to be your unit of measurement.  Regardless, fully general answers have been posted in the duplicate threads.",2020-12-29 19:18:53.997
623660,333671,,CC BY-SA 4.0,,$\mathbb I$ is an [indicator function](https://stats.stackexchange.com/search?q=indicator+function).,2020-12-29 19:20:28.757
623661,333722,,CC BY-SA 4.0,,"Could you explain what you mean by ""using binary""?",2020-12-29 19:24:50.193
623662,333722,255186.0,CC BY-SA 4.0,,"Sure :) A binary variable would be men vs female, which could be coded to 0 or 1. So i could calculate with the estimate something like intercept + estimate x 0 if it is a female or intercept + estimate x 1 if it is a male. For instance to calculate if a male is more likely to have a heart attack than a female.
I got that from StatQuest: https://youtu.be/C4N3_XJJ-jU?t=506",2020-12-29 19:40:05.130
623663,333722,,CC BY-SA 4.0,,"It's better practice to include your summary output as cut-and-pasted text rather than an image (among other things, an image is inaccessible to anyone using a screen reader). For this case you probably only need the first line (the `glm()` call) and the coefficient table ...",2020-12-29 19:47:43.123
623665,333689,20286.0,CC BY-SA 4.0,,"""this doesn't seem very useful if I don't know what the reference level's association with outcome/time actually is""--but that is just what you get with a Cox regression. There is a baseline hazard function over time, representing a situation with all predictors at reference levels. Then the regression coefficients represent the log-hazard differences from that baseline. If you want a closed-form survival estimate, you need to use a parametric regression rather than the Cox semi-parametric regression.",2020-12-29 19:59:01.487
623666,333707,35107.0,CC BY-SA 4.0,,"I think your first step is to figure out what you really want. Since Days is numeric, your initial model fits a fixed slope for Days and a random intercept and slope for each subject. In that context, pairwise comparisons of Day are rather meaningless because you are just comparing points on a straight line, thus the t test for any such comparison is just the t test for the slope of the fitted line, which you can get from the model summary. Your second model appears to have several times as many parameters as observations, so it makes no sense at all. What is your research plan?",2020-12-29 20:15:52.693
623667,333722,255186.0,CC BY-SA 4.0,,I updated the post. I'm still trying to learn the stack rules/best practices.,2020-12-29 20:22:49.540
623668,333728,24715.0,CC BY-SA 4.0,,"It is not really about statistics but about medical research. For statistics you need data. Obviously, there is not long-term effects data for the vaccines, so statistics won't help. I guess you could try to extrapolate the results from other vaccines etc, but this would be more about using domain knowledge than statistics.",2020-12-29 20:37:21.647
623669,333727,,CC BY-SA 4.0,,I think one important thing missing from your question is how many observations you have.,2020-12-29 20:38:04.573
623671,333724,58540.0,CC BY-SA 4.0,,"See https://stats.stackexchange.com/questions/176869/what-is-assuming-sphericity-in-anova-function. I'd suggest you switch to a multilelvel model (MLM), using R's `lmer()`. With regards to normality, what you are worried about in MLM is the normality of the residuals. You can easily look at these after running your `lmer` model. See https://www.ssc.wisc.edu/sscc/pubs/MM/MM_DiagInfer.html",2020-12-29 20:58:29.847
623672,333691,69747.0,CC BY-SA 4.0,,"It is true the the conditional nature of the tests affects the operating characteristics of the later tests. There is research on this going back decades.  You can investigate it via simulation.  My point was simply that a null test result for model specification is always a Type II error. The logical next question is, why perform specification tests at all?",2020-12-29 21:02:49.900
623673,333723,,CC BY-SA 4.0,,You should not compare different coefficients' values unless you make a special rescaling (e.g. correction for chance etc.).,2020-12-29 21:07:37.387
623675,333689,20286.0,CC BY-SA 4.0,,"I think that you might be having some more fundamental problem with your analysis for which you need help, before you worry about the p-values. In particular, it's not clear how and why you are using `survSplit()`, for example when you say ""genetic-time groups given by survSplit"". Also, I have no problem in doing `anova()` on models built on a dataset re-formatted by `survSplit`, whether in the base `survival` package or in `rms` (provided that I use the `rms` function `cph()` for the analysis with `rms`). More details about your specific application would help a lot.",2020-12-29 21:18:33.840
623678,333727,236703.0,CC BY-SA 4.0,,I updated the main post with this info.,2020-12-29 21:21:49.660
623682,333732,225131.0,CC BY-SA 4.0,,Thank you so much for your response. I greatly greatly appreciate it.,2020-12-29 21:35:54.190
623684,333657,254365.0,CC BY-SA 4.0,,"But I don't understand why I have to put 3 choose 1, because I know that 3 is the total and 1 is the success",2020-12-29 21:46:26.220
623686,333739,237141.0,CC BY-SA 4.0,,"I must have overlooked the BCEWithLogitsLoss, but now it makes sense. Cheers!",2020-12-29 22:01:27.433
623690,333679,,CC BY-SA 4.0,,"You aren't guaranteed to observe a tails. The probability of observing all heads under the assumptions of the problem tends to one as the number of flips tends to infinity. But, this does not mean you *must* observe a tails.",2020-12-29 22:36:20.900
623692,333741,61417.0,CC BY-SA 4.0,,"Okay, but do you know how the pooled variance is calculated?",2020-12-29 22:51:53.850
623695,333745,100031.0,CC BY-SA 4.0,,"[Google ngram search](https://books.google.com/ngrams/graph?content=%22type+I+error%22%2C%22Type+I+error%22&year_start=1940&year_end=2019&corpus=26&smoothing=3) indicates that capitalized is more prevalent than uncapitalized. However, some of the effect may be due to cases where 'Type' is the first word of a sentence, and thus capitalized.",2020-12-29 23:36:28.777
623696,333730,,CC BY-SA 4.0,,"The answer to your question is ""It depends on the final objective"".  Here is something to think about If you select a random number of counties, how do you know those counties represent the rest of the state?  i.e. There are urban, suburban and rural counties, you may need to stratify these in order to obtain a representative sample.",2020-12-29 23:42:13.180
623697,333736,35107.0,CC BY-SA 4.0,,Looks like `emmeans::emtrends()` would be very useful here. It estimates the slope of a numeric predictor at each level(s) of factor(s). See that interactions vignette,2020-12-29 23:46:59.123
623698,333747,199619.0,CC BY-SA 4.0,,"Do you mean to train one using loss function A, train another using loss function B, and then compare out-of-sample using loss function A (or C)? That’s definitely allowed! In fact, that’s (in some sense) what you do when you do cross-validation for a regularized model.",2020-12-29 23:46:59.147
623699,333747,246708.0,CC BY-SA 4.0,,@Dave Thanks for the quick reply. That's exactly it! That's helpful that it can actually be done. Thanks again,2020-12-29 23:49:57.907
623701,333742,120720.0,CC BY-SA 4.0,,"Reference for the problem is: Example 5.12 in Sahoo, Prasanna. (2015). [Probability and Mathematical Statistics.](https://www.researchgate.net/publication/272237355_Probability_and_Mathematical_Statistics)",2020-12-29 23:58:56.840
623704,333745,,CC BY-SA 4.0,,"I avoid this term because it's so arbitrary that it's difficult for a reader to remember what it means.  If I had to use it, I suppose capitalization would be preferable.  How about ""tYpE 1""? ;).",2020-12-30 00:07:59.950
623707,333741,57214.0,CC BY-SA 4.0,,"Pooled variance assume $H_0$ is true. Pooled success estimate is $\hat p = 16/115.$ Then pooled var is $115\hat p(1-\hat p).$ // Some versions of the test use separate variances. Minitab gives user a choice. I used 'separate' in my Answer, but just ran 'pooled' with $z=-6.77$ and $P\approx 0.$ Essentially same result.",2020-12-30 00:42:58.603
623709,333720,22.0,CC BY-SA 4.0,,"If I understand what you mean by Z test, it also assumes sampling from a normal distribution (normality of the population).",2020-12-30 01:13:55.640
623710,333668,255117.0,CC BY-SA 4.0,,"Hi, sorry there has a typo error, it is 68:16:16",2020-12-30 01:14:36.630
623715,333747,16043.0,CC BY-SA 4.0,,"@JCunn It would be helpful if you edited your post to include the detail that Dave asked after, because as it's written, it's unclear if you're measuring the same loss function for model A and model B.",2020-12-30 02:07:49.053
623716,333753,,CC BY-SA 4.0,,See also: https://stackoverflow.com/questions/65012059/numpy-variance-vs-homegrown-variance-different-result ; https://stats.stackexchange.com/questions/100041/how-exactly-did-statisticians-agree-to-using-n-1-as-the-unbiased-estimator-for. NumPy's `var()` doesn't employ a Bessel bias correction by default.,2020-12-30 02:34:00.997
623717,333753,255213.0,CC BY-SA 4.0,,@PeterO. I think there are too many samples in all parts of this question where I take the var/std that the Bessel correction is mostly moot,2020-12-30 02:49:30.117
623719,333757,199619.0,CC BY-SA 4.0,,"I know what you mean about explanations about regularization tending to use polynomial examples. However, consider it like this: unconstrained, the parameter estimates can latch onto whatever coincidence they see, but the constraints prevent them from being able to latch on too hard and overfit.",2020-12-30 03:24:41.177
623720,333757,185306.0,CC BY-SA 4.0,,See also https://stats.stackexchange.com/q/500360/232706,2020-12-30 03:27:14.427
623721,333745,254711.0,CC BY-SA 4.0,,"[I believe you can subtract the cases in which 'Type' is the first word in a sentence on ngram search](https://books.google.com/ngrams/graph?content=%28Type+I+error+-+_START_+Type+I+error%29%2Ctype+I+error&year_start=1940&year_end=2019&corpus=26&smoothing=3&direct_url=t1%3B%2C%28Type%20I%20error%20-%20%20_START_%20Type%20I%20error%29%3B%2Cc0%3B.t1%3B%2Ctype%20I%20error%3B%2Cc0). Also, if you search for ""Type I error"" in quotation marks, you are literally searching for occurrences in quotation marks. You need to drop them to get the correct result.",2020-12-30 03:51:42.993
623722,333761,255216.0,CC BY-SA 4.0,,"This answer and the comments are all reasonable explanations, thanks all. I think it's starting to click, but I'm still not entirely sure how this explains a linear model's weights. For a polynomial model, I can see the effect of lowering all weights to be simplifying say, a cubic function to a quadratic or linear one because for instance the $x^2$ or $x^3$ is not as easily able to dominate. If the model is already linear, noisy data/outliers would be fit by weight vectors that are different but not necessarily smaller (in norm / length). So what is the effect of a regularization term?",2020-12-30 04:03:15.453
623723,333761,,CC BY-SA 4.0,user233429,@khajiit regularization  is not just about the form of the model but about how many dimensions to include. The simplest model would fit $y_i=\bar{y}$ or something like that. A linear model with 1000 features is more prone to overfit than one with just 10 feature since there are fewer levers to tweak to reduce the error.,2020-12-30 04:12:18.347
623725,333761,255216.0,CC BY-SA 4.0,,"Thanks for the clarification. In the case of a dataset with only two features $x_1$ and $x_2$ (I added pictures to my post above for clarity), how does this lever-tweaking analogy hold? Two features seems like a relatively small amount with or without regularization in terms of addressing that outlier. Apologies for the extra requests for clarification, this has already been helpful in any event.",2020-12-30 04:25:28.300
623726,333762,255219.0,CC BY-SA 4.0,,These variables are the estimate of coefficients not data sets. Is there any formula to do it so I can prove it rather than doing it empirically?,2020-12-30 04:42:13.220
623727,333761,,CC BY-SA 4.0,user233429,@khajiit the C here is a bit misleading as a “regularization”. This answer gives a nice overview: https://datascience.stackexchange.com/questions/4943/intuition-for-the-regularization-parameter-in-svm,2020-12-30 04:44:12.063
623728,333764,,CC BY-SA 4.0,,But many books write this sample > 30 notation to differentiate between Z-test and t-test. Is it wrong justification?,2020-12-30 04:55:06.210
623729,333759,255208.0,CC BY-SA 4.0,,"I Isabella, Thank you for your quick reply, I appreciate it a lot. I have updated my question and hope that provides more clarity. Apologies but my stats knowledge isn't great and count data has not been covered in courses I've done! Regarding ""expected/average value of your count outcome variable at time point t is less than that at time point t−1"", that sounds correct but I don't have the stats background to have written it that way. Please let me know if any more clarification is needed, Kind regards, Scott",2020-12-30 05:01:28.147
623730,333764,75350.0,CC BY-SA 4.0,,@Daman when $n>30$ the maximum relative difference between the student t and the normal distribution is less than 5%.  The reason that books make this (arbitrary) rule is so they can don't have to put in dozens of tables at the back of the book for the various degrees of freedom for the t.  When $n>30$ someone decided the error induced by using the z in place of the t was negligible.,2020-12-30 05:09:08.697
623735,333764,57214.0,CC BY-SA 4.0,,"Technically, yes, _Wrong._ However, there may be some pedagogical advantages to delaying a thorough discussion of t tests to later chapters in a textbook. A major disadvantage is that the rule of 30 can be quite misleading and is frequently misused. With increasing available computation, the excuse for creating this confusion becomes increasingly feeble.",2020-12-30 06:21:38.443
623736,333765,57214.0,CC BY-SA 4.0,,"If data are not normal the t test may give incorrect P-values, even if the sample size is large enough for $\bar X$ to be nearly normal. It is _only for normal data_ that $\bar X$ and $S$ are stochastically independent, but the definition of the t distribution requires this independence.",2020-12-30 06:28:39.980
623738,333769,15606.0,CC BY-SA 4.0,,"Try to substitute the expressions for $Y_+$ and $Y_j$  into the expression for Q and observe that $\eta$ and $\tau$ cancel out, leaving Q only a function of $\varepsilon_+$ and $\varepsilon_j$.",2020-12-30 06:37:19.853
623740,333760,15606.0,CC BY-SA 4.0,,Have a look at this post: https://stats.stackexchange.com/questions/104704/are-estimates-of-regression-coefficients-uncorrelated,2020-12-30 07:39:07.757
623741,333765,244871.0,CC BY-SA 4.0,,Central Limit Theorem inform us that $\bar{x}$ are normally distributed. T-test needs the population's normality (e.g. http://amzn.to/2L66oqQ or http://bit.ly/38GNOO2). These books and other (e.g. http://bit.ly/3o1c1oD or https://bit.ly/3hrMDpH) inform us that for a big sample (n>80) we don't have to worry about normality. Also there exist the estimation of σ from s for big samples.  The question is: what estimation is safer to assume?,2020-12-30 07:51:18.233
623742,333765,244871.0,CC BY-SA 4.0,,"@BruceET , these books (amzn.to/2L66oqQ and bit.ly/38GNOO2) and these articles (e.g. bit.ly/3o1c1oD or bit.ly/3hrMDpH) claim that population's normality is not so important for large samples (n>80). Also for large samples we can assume $s\approx\sigma$ (e.g. https://bit.ly/34TIIwR). The question is: which approximation is the safiest?",2020-12-30 08:02:39.597
623743,333766,5179.0,CC BY-SA 4.0,,"1. The link to ""the article"" is missing. 2. The connection between $\rho$ and $(\sigma_\min,\sigma_\max)$ is not explicited.",2020-12-30 08:04:59.490
623746,333728,75014.0,CC BY-SA 4.0,,"@Tim the question is about the statistical methods for extrapolating the data from other vaccines. But yes, it is about statistical procedures specific to medicine.",2020-12-30 08:22:59.407
623748,333672,255103.0,CC BY-SA 4.0,,"I picked the 2D minimum distance test from the diehard tests and it works very well for my use case! It also helped me understand what you meant by uniform distribution of outcomes as I now run this test many times. Thanks for the suggestion. I will consider adding more tests over time as well. I'll leave this question up for now because I think the question I proposed is still interesting, but I'm happy with my new approach to testing!",2020-12-30 08:39:32.433
623749,333724,234784.0,CC BY-SA 4.0,,"Thank you very much for the insights, I do have an additional question if you don't mind. Basically, when is it a good idea to use a robust method? And should I use the F-ratio of the GG correction? 

I stumbled upon [this article](https://cran.r-project.org/web/packages/WRS2/vignettes/WRS2.pdf) by Wilcox in which he writes that when normality and homoscedasticity are not met, classic inferential methods based on means (e.g., the ANOVA, F-test) are not the way to go, and you should switch to a robust method.",2020-12-30 08:48:21.317
623751,333723,254875.0,CC BY-SA 4.0,,"I don't understand your suggestion. All what I am trying to do is to look at different confusion matrix scores and understand the classification performance. In this respect, I would like to know what extra will f-score contribute to my understanding above accuracy and kappa.",2020-12-30 09:19:59.350
623752,333766,247117.0,CC BY-SA 4.0,,"@Xi'an, thank you, I just updated the missing information.  $\rho$ is the standard deviation of proposed transition probability. I don't know why choosing $\sigma_{min}$ as $\rho$ will help with accept rate",2020-12-30 09:35:18.297
623754,333672,5179.0,CC BY-SA 4.0,,"Regarding the use of the KS-test, I am suggesting NOT to use a critical value but the comparison of the empirical distribution of the p-values with a Uniform target.",2020-12-30 09:40:20.970
623755,333762,255219.0,CC BY-SA 4.0,,"Yes, I can do that but like I said, I'd like to have more theoretical answer for two random normal variables covariance, not empirical results.",2020-12-30 09:52:29.377
623757,333708,254239.0,CC BY-SA 4.0,,"Hi @jluchman. I have read your paper and I appreciate it. The PERI approach using DA statistics is what I was looking for. I will apply it to my models. I will try to implement a function in Python to deal with simple regressions and dynamic regressions, once I am done I will get back to you. Thank you so much!",2020-12-30 10:10:06.873
623758,333776,211.0,CC BY-SA 4.0,,"this is a beautiful answer, but it answers a slightly different question. You are answering ""what should be the optimal weight $\lambda$ to use"". What I'm asking is: given that you have 2 different values to choose from ($\lambda_0$ and $\lambda_1$) - how would you devise a statistical test to decide if to stop using $\lambda_0$ in favor of $\lambda_1$. I'm thinking that it should be a test for checking if the MSE_1 is statistically smaller than MSE_0 (in the way I proposed in my question). What do you think?",2020-12-30 10:17:15.647
623759,333776,211.0,CC BY-SA 4.0,,"Also, a second small request - if it's possible to have references on where to read about these types of questions/answers - that would be GREAT. Thanks!!",2020-12-30 10:17:43.463
623760,333776,211.0,CC BY-SA 4.0,,"(e.g.: references that you've used in your sampling theory course, and if there are class notes - that would be also great!)",2020-12-30 10:18:39.280
623763,333728,120720.0,CC BY-SA 4.0,,"@Vadim ""extrapolating the data from *other* vaccines"" that means there's no data from covid-19 vaccins. This should be clarified and the question is more like 'what are long-term side effects from vaccins?'. This is still not really statistics. Sure, you can have some data about this and that can be treated statistically (one could make some prediction for the probability of a long-term effect). But the core of the question is medicine and statistics hardly plays a role.",2020-12-30 10:59:41.487
623764,333779,13244.0,CC BY-SA 4.0,,This Wikpedia entry might be a good start https://en.wikipedia.org/wiki/Sample_size_determination,2020-12-30 11:00:41.413
623765,333771,232582.0,CC BY-SA 4.0,,"Try other tests as well, see if they differ. For example, what does a White test for heteroskedasticity say or are the coefficients of power > 1 significant when regressing residuals on fitted values?",2020-12-30 11:02:52.300
623766,333728,75014.0,CC BY-SA 4.0,,"@SextusEmpiricus Perhaps, the title is misleading. The question is about the clinical studies methodology, which is basically applied statistics... to the extent that one can consider biostatistics as statistics.",2020-12-30 11:12:08.350
