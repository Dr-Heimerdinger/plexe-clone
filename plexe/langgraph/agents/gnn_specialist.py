"""
Relational GNN Specialist Agent.

This agent generates and executes GNN training scripts using
the plexe.relbench.modeling modules.
"""

import logging
from typing import Optional, List, Dict, Any

from langchain_core.tools import BaseTool

from plexe.langgraph.agents.base import BaseAgent
from plexe.langgraph.config import AgentConfig
from plexe.langgraph.state import PipelineState, PipelinePhase
from plexe.langgraph.tools.common import save_artifact
from plexe.langgraph.tools.gnn_specialist import generate_training_script, execute_training_script

logger = logging.getLogger(__name__)


GNN_SPECIALIST_SYSTEM_PROMPT = """You are the Relational GNN Specialist Agent, an expert in Graph Neural Networks for Relational Deep Learning.

## Your Mission
Generate and execute GNN training scripts using the plexe.relbench.modeling modules to train on relational data.

## Prerequisites
Before starting, you need:
1. dataset.py - Generated by DatasetBuilder (defines GenDataset class)
2. task.py - Generated by TaskBuilder (defines GenTask class)

## Workflow

### Step 1: Verify Prerequisites
Confirm that dataset.py and task.py exist in the working directory.

### Step 2: Generate Training Script
Use generate_training_script with appropriate parameters:
- dataset_module_path: Path to dataset.py
- dataset_class_name: "GenDataset"
- task_module_path: Path to task.py
- task_class_name: "GenTask"
- task_type: From task_info (regression, binary_classification, multiclass_classification)
- tune_metric: Appropriate metric for the task type
- Training hyperparameters

### Step 3: Execute Training
Use execute_training_script to run the generated script.

### Step 4: Report Results
Report the training metrics and model performance.

## Training Script Components
The generated script uses these plexe.relbench.modeling modules:

### Graph Construction
- `make_pkey_fkey_graph`: Converts Database to HeteroData graph
- `get_node_train_table_input`: Prepares training input for NeighborLoader

### Neural Network Modules
- `HeteroEncoder`: Encodes tabular features to embeddings
- `HeteroTemporalEncoder`: Encodes temporal information
- `HeteroGraphSAGE`: GNN message passing layers

### Training Pipeline
1. Load Dataset and Task classes
2. Build heterogeneous graph from database
3. Create temporal neighbor loaders for train/val/test
4. Train GNN model with proper temporal sampling
5. Evaluate and save best model

## Hyperparameter Guidelines
- epochs: 10-20 for initial training
- batch_size: 512 (adjust based on memory)
- learning_rate: 0.001-0.01
- hidden_channels: 64-256
- num_gnn_layers: 2-3

## Metrics by Task Type
- Regression: mae, rmse (lower is better)
- Binary Classification: accuracy, auroc, f1 (higher is better)
- Multiclass Classification: accuracy, f1_macro (higher is better)

## Important
- Use generate_training_script tool - do NOT write training code manually
- The tool creates train_script.py automatically
- Execute only after successful generation
- Report both validation and test metrics
"""


class RelationalGNNSpecialistAgent(BaseAgent):
    """Agent for GNN training script generation and execution."""
    
    def __init__(
        self,
        config: Optional[AgentConfig] = None,
        additional_tools: Optional[List[BaseTool]] = None,
    ):
        """
        Initialize the GNN specialist agent.
        
        Args:
            config: Agent configuration
            additional_tools: Additional tools beyond defaults
        """
        tools = [
            generate_training_script,
            execute_training_script,
            save_artifact,
        ]
        
        if additional_tools:
            tools.extend(additional_tools)
        
        super().__init__(
            agent_type="gnn_specialist",
            config=config,
            tools=tools,
        )
    
    @property
    def system_prompt(self) -> str:
        return GNN_SPECIALIST_SYSTEM_PROMPT
    
    def _build_context(self, state: PipelineState) -> str:
        """Build context with training-specific information."""
        context_parts = []
        
        if state.get("working_dir"):
            context_parts.append(f"Working directory: {state['working_dir']}")
        
        if state.get("csv_dir"):
            context_parts.append(f"CSV directory: {state['csv_dir']}")
        
        if state.get("dataset_info"):
            ds = state["dataset_info"]
            context_parts.append(f"Dataset class: {ds.get('class_name', 'GenDataset')}")
            context_parts.append(f"Dataset file: {ds.get('file_path', 'dataset.py')}")
        
        if state.get("task_info"):
            task = state["task_info"]
            context_parts.append(f"Task class: {task.get('class_name', 'GenTask')}")
            context_parts.append(f"Task file: {task.get('file_path', 'task.py')}")
            context_parts.append(f"Task type: {task.get('task_type', 'regression')}")
        
        task_type = state.get("task_info", {}).get("task_type", "regression")
        if task_type == "regression":
            metric_guidance = "Use tune_metric='mae' or 'rmse', higher_is_better=False"
        else:
            metric_guidance = "Use tune_metric='accuracy' or 'auroc', higher_is_better=True"
        
        task_instruction = f"""
Your task:
1. Verify dataset.py and task.py exist
2. Generate training script using generate_training_script
   - {metric_guidance}
   - Use epochs=10, batch_size=512
3. Execute the training script using execute_training_script
4. Report the training results

Use the exact file paths from the context above.
"""
        context_parts.append(task_instruction)
        
        return "\n".join(context_parts)
    
    def _process_result(self, result: Dict[str, Any], state: PipelineState) -> Dict[str, Any]:
        """Process result and extract training results."""
        base_result = super()._process_result(result, state)
        
        import os
        import json
        
        working_dir = state.get("working_dir", "")
        results_path = os.path.join(working_dir, "training_results.json")
        
        if os.path.exists(results_path):
            try:
                with open(results_path) as f:
                    training_results = json.load(f)
                
                base_result["training_result"] = {
                    "metrics": training_results,
                    "model_path": training_results.get("model_path"),
                    "script_path": os.path.join(working_dir, "train_script.py"),
                }
            except Exception as e:
                logger.warning(f"Could not read training results: {e}")
        
        base_result["current_phase"] = PipelinePhase.OPERATION.value
        
        return base_result
