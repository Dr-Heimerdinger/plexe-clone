"""
Tools for building RelBench Task objects from Database/Dataset.

These tools support the TaskBuilder agent in constructing proper Task definitions
(EntityTask or RecommendationTask) based on the dataset schema and user requirements.
"""

import os
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

import pandas as pd
from smolagents import tool

from plexe.core.object_registry import ObjectRegistry

logger = logging.getLogger(__name__)


# =============================================================================
# Template for generated EntityTask code
# =============================================================================

ENTITY_TASK_CODE_TEMPLATE = '''"""
Auto-generated EntityTask class for {task_name}.
Generated by TaskBuilder Agent.

IMPORTANT CONCEPTS:
- EntityTask: Prediction task over a single entity (classification or regression)
- entity_col: The column identifying the entity being predicted
- entity_table: The table containing the entity definitions
- time_col: The timestamp column for temporal splitting
- target_col: The column containing the prediction target (label)
- timedelta: The prediction window - predict over (timestamp, timestamp + timedelta]
- make_table: SQL/Pandas query to compute labels for given timestamps
"""

import duckdb
import numpy as np
import pandas as pd
from typing import Optional

from plexe.relbench.base import Database, Table, EntityTask, TaskType
from plexe.relbench.metrics import {metrics_imports}


class {class_name}(EntityTask):
    r"""{task_description}"""

  
    # Task Attributes
 
    task_type = TaskType.{task_type}
    entity_col = "{entity_col}"
    entity_table = "{entity_table}"
    time_col = "{time_col}"
    target_col = "{target_col}"
    timedelta = pd.Timedelta({timedelta})
    metrics = [{metrics_list}]
    num_eval_timestamps = {num_eval_timestamps}

    def make_table(self, db: Database, timestamps: "pd.Series[pd.Timestamp]") -> Table:
        """
        Construct the task table with labels for each timestamp.
        
        Args:
            db: The Database object containing all tables
            timestamps: Series of timestamps to compute labels for
            
        Returns:
            Table object with entity IDs, timestamps, and labels
        """
       
        # 1. PREPARE DATAFRAMES
 
        timestamp_df = pd.DataFrame({{"timestamp": timestamps}})
        
{table_loading_code}

       
        # 2. COMPUTE LABELS VIA SQL QUERY
     
{sql_query_code}

      
        # 3. RETURN TASK TABLE
    
        return Table(
            df=df,
            fkey_col_to_pkey_table={{self.entity_col: self.entity_table}},
            pkey_col=None,
            time_col=self.time_col,
        )
'''


# =============================================================================
# Template for generated RecommendationTask code
# =============================================================================

RECOMMENDATION_TASK_CODE_TEMPLATE = '''"""
Auto-generated RecommendationTask class for {task_name}.
Generated by TaskBuilder Agent.

IMPORTANT CONCEPTS:
- RecommendationTask: Link prediction task between source and destination entities
- src_entity_col: The column identifying the source entity
- src_entity_table: The table containing source entity definitions
- dst_entity_col: The column identifying the destination entity (as a list)
- dst_entity_table: The table containing destination entity definitions
- time_col: The timestamp column for temporal splitting
- eval_k: k for evaluation metrics (recall@k, ndcg@k, etc.)
- make_table: Query to compute (source, list_of_destinations) for given timestamps
"""

import duckdb
import numpy as np
import pandas as pd
from typing import Optional

from plexe.relbench.base import Database, Table, RecommendationTask, TaskType
from plexe.relbench.metrics import {metrics_imports}


class {class_name}(RecommendationTask):
    r"""{task_description}"""

  
    # Task Attributes
   
    task_type = TaskType.LINK_PREDICTION
    src_entity_col = "{src_entity_col}"
    src_entity_table = "{src_entity_table}"
    dst_entity_col = "{dst_entity_col}"
    dst_entity_table = "{dst_entity_table}"
    time_col = "{time_col}"
    timedelta = pd.Timedelta({timedelta})
    eval_k = {eval_k}
    metrics = [{metrics_list}]
    num_eval_timestamps = {num_eval_timestamps}

    def make_table(self, db: Database, timestamps: "pd.Series[pd.Timestamp]") -> Table:
        """
        Construct the task table with source entities and their target destinations.
        
        Args:
            db: The Database object containing all tables
            timestamps: Series of timestamps to compute labels for
            
        Returns:
            Table object with source entity IDs, timestamps, and lists of destination IDs
        """
        
        # 1. PREPARE DATAFRAMES
    
        timestamp_df = pd.DataFrame({{"timestamp": timestamps}})
        
{table_loading_code}

       
        # 2. COMPUTE RECOMMENDATIONS VIA SQL QUERY
    
{sql_query_code}

       
        # 3. RETURN TASK TABLE

        return Table(
            df=df,
            fkey_col_to_pkey_table={{self.src_entity_col: self.src_entity_table}},
            pkey_col=None,
            time_col=self.time_col,
        )
'''


@tool
def get_database_schema_for_task() -> Dict[str, Any]:
    """
    Retrieve the database schema information needed for task definition.
    
    This tool gets schema metadata including table names, columns, primary keys,
    foreign keys, and temporal columns from the ObjectRegistry.
    
    Returns:
        Dictionary containing:
        - tables: Dict of table name -> {columns, primary_key, time_col, row_count}
        - relationships: List of FK relationships
        - temporal_columns: Dict of table -> temporal column info
        - entity_candidates: Suggested entity tables for EntityTask
    """
    try:
        object_registry = ObjectRegistry()
        
        # Try different schema sources
        schema_sources = [
            "unified_db_schema",
            "graph_schema_metadata",
            "database_schema"
        ]
        
        schema = None
        source = None
        for src in schema_sources:
            try:
                schema = object_registry.get(dict, src)
                source = src
                break
            except KeyError:
                continue
        
        if schema is None:
            return {
                "found": False,
                "error": "No schema metadata found in registry. "
                         "Make sure DatasetBuilder has built the database first."
            }
        
        tables = schema.get("tables", {})
        relationships = schema.get("relationships", [])
        temporal_summary = schema.get("temporal_summary", {})
        
        # Identify entity candidates (tables with primary keys that are referenced by FKs)
        entity_candidates = []
        referenced_tables = set()
        for rel in relationships:
            if isinstance(rel, dict):
                referenced_tables.add(rel.get("target_table", ""))
        
        for table_name, table_info in tables.items():
            if isinstance(table_info, dict):
                pkeys = table_info.get("primary_keys", [])
                if pkeys and table_name in referenced_tables:
                    entity_candidates.append({
                        "table": table_name,
                        "primary_key": pkeys[0] if pkeys else None,
                        "is_temporal": table_name in temporal_summary
                    })
        
        return {
            "found": True,
            "source": source,
            "tables": tables,
            "relationships": relationships,
            "temporal_columns": temporal_summary,
            "entity_candidates": entity_candidates
        }
        
    except Exception as e:
        logger.error(f"Error getting database schema: {e}")
        return {"error": str(e)}


@tool
def get_dataset_info() -> Dict[str, Any]:
    """
    Get information about the registered Dataset including timestamps.
    
    Returns:
        Dictionary containing:
        - dataset_name: Name of the dataset
        - val_timestamp: Validation split timestamp
        - test_timestamp: Test split timestamp
        - code_available: Whether dataset code is registered
    """
    try:
        object_registry = ObjectRegistry()
        
        # Look for registered database metadata
        dataset_info = {}
        
        # Try to find all registered database metadata
        for key_pattern in ["database_metadata_"]:
            # Check known datasets
            possible_names = ["stack_overflow", "f1", "amazon", "house", "custom"]
            for name in possible_names:
                try:
                    metadata = object_registry.get(dict, f"database_metadata_{name}")
                    dataset_info[name] = {
                        "dataset_name": metadata.get("dataset_name"),
                        "val_timestamp": metadata.get("val_timestamp"),
                        "test_timestamp": metadata.get("test_timestamp"),
                        "description": metadata.get("description", ""),
                        "code_available": True
                    }
                except KeyError:
                    continue
        
        if not dataset_info:
            return {
                "found": False,
                "error": "No dataset metadata found. Build a dataset first using DatasetBuilder."
            }
        
        return {
            "found": True,
            "datasets": dataset_info
        }
        
    except Exception as e:
        logger.error(f"Error getting dataset info: {e}")
        return {"error": str(e)}


@tool
def get_available_metrics() -> Dict[str, Any]:
    """
    Get list of available metrics for different task types.
    
    Returns:
        Dictionary with metrics organized by task type:
        - binary_classification: List of binary classification metrics
        - multiclass_classification: List of multiclass metrics
        - regression: List of regression metrics
        - recommendation: List of recommendation metrics
    """
    return {
        "binary_classification": {
            "metrics": ["accuracy", "f1", "roc_auc", "average_precision", "auprc"],
            "primary_metric": "roc_auc",
            "description": "Metrics for binary (0/1) classification tasks"
        },
        "multiclass_classification": {
            "metrics": ["accuracy", "macro_f1", "micro_f1", "mrr", "log_loss"],
            "primary_metric": "macro_f1",
            "description": "Metrics for multi-class classification tasks"
        },
        "regression": {
            "metrics": ["mae", "mse", "rmse", "r2"],
            "primary_metric": "mae",
            "description": "Metrics for continuous value prediction tasks"
        },
        "multilabel_classification": {
            "metrics": ["multilabel_auprc_micro"],
            "primary_metric": "multilabel_auprc_micro",
            "description": "Metrics for multi-label classification tasks"
        },
        "recommendation": {
            "metrics": ["recall_at_k", "ndcg_at_k", "mrr_at_k", "map_at_k"],
            "primary_metric": "recall_at_k",
            "description": "Metrics for recommendation/link prediction tasks",
            "note": "Import from plexe.relbench.metrics"
        },
        "task_types": {
            "BINARY_CLASSIFICATION": "Binary 0/1 classification",
            "MULTICLASS_CLASSIFICATION": "Multi-class classification",
            "REGRESSION": "Continuous value regression",
            "MULTILABEL_CLASSIFICATION": "Multi-label classification",
            "LINK_PREDICTION": "Link/recommendation prediction"
        }
    }


@tool
def get_table_sample_data(table_name: str, num_rows: int = 5) -> Dict[str, Any]:
    """
    Get sample data from a table to help understand column values for task definition.
    
    Args:
        table_name: Name of the table to sample
        num_rows: Number of sample rows to return (default: 5)
        
    Returns:
        Dictionary containing:
        - columns: List of column names with dtypes
        - sample_data: Sample rows as list of dicts
        - statistics: Basic statistics for numeric columns
    """
    try:
        object_registry = ObjectRegistry()
        
        # Try to get the CSV path
        try:
            csv_path = object_registry.get(str, "path_raw_csv_files")
        except KeyError:
            return {
                "error": "CSV path not found in registry. Make sure data has been loaded."
            }
        
        # Look for the CSV file
        csv_file = os.path.join(csv_path, f"{table_name}.csv")
        if not os.path.exists(csv_file):
            return {
                "error": f"Table '{table_name}' not found at '{csv_file}'"
            }
        
        # Load and sample the data
        df = pd.read_csv(csv_file)
        
        # Get column info
        columns = [
            {"name": col, "dtype": str(df[col].dtype), "null_count": int(df[col].isna().sum())}
            for col in df.columns
        ]
        
        # Get sample data
        sample_data = df.head(num_rows).to_dict(orient='records')
        
        # Get statistics for numeric columns
        numeric_cols = df.select_dtypes(include=['number']).columns
        statistics = {}
        for col in numeric_cols:
            statistics[col] = {
                "min": float(df[col].min()) if not pd.isna(df[col].min()) else None,
                "max": float(df[col].max()) if not pd.isna(df[col].max()) else None,
                "mean": float(df[col].mean()) if not pd.isna(df[col].mean()) else None,
                "unique_count": int(df[col].nunique())
            }
        
        # Check for potential target columns (binary or categorical with few values)
        potential_targets = []
        for col in df.columns:
            unique_count = df[col].nunique()
            if unique_count == 2:
                potential_targets.append({
                    "column": col,
                    "type": "binary",
                    "values": df[col].unique().tolist()[:10]
                })
            elif 2 < unique_count <= 20:
                potential_targets.append({
                    "column": col,
                    "type": "categorical",
                    "unique_count": unique_count,
                    "values": df[col].unique().tolist()[:10]
                })
        
        return {
            "table_name": table_name,
            "row_count": len(df),
            "columns": columns,
            "sample_data": sample_data,
            "numeric_statistics": statistics,
            "potential_target_columns": potential_targets
        }
        
    except Exception as e:
        logger.error(f"Error getting sample data: {e}")
        return {"error": str(e)}


@tool
def test_sql_query(sql_query: str, timedelta_days: int = 30) -> Dict[str, Any]:
    """
    Test a SQL query on sample data before committing to the Task class.
    
    This tool helps validate SQL queries by running them on actual data with
    sample timestamps. Use this to debug and refine your queries.
    
    Args:
        sql_query: The SQL query to test (use 'timestamp_df' as the timestamp source)
        timedelta_days: The timedelta in days for the test (default: 30)
        
    Returns:
        Dictionary containing:
        - success: Whether the query executed successfully
        - row_count: Number of rows returned
        - columns: Columns in the result
        - sample_output: First few rows of output
        - error: Error message if failed
        - suggestions: Suggestions for fixing common issues
    """
    try:
        import duckdb
        object_registry = ObjectRegistry()
        
        # Get CSV path
        try:
            csv_path = object_registry.get(str, "path_raw_csv_files")
        except KeyError:
            return {
                "success": False,
                "error": "CSV path not found. Make sure DatasetBuilder has run first."
            }
        
        # Load all CSV files as DataFrames
        csv_files = [f for f in os.listdir(csv_path) if f.endswith('.csv')]
        tables_loaded = {}
        
        # Load all CSV files as Views in DuckDB (Zero-Copy)
        csv_files = [f for f in os.listdir(csv_path) if f.endswith('.csv')]
        tables_loaded = {}
        
        # Create an in-memory DuckDB connection
        con = duckdb.connect(database=':memory:')
        
        for csv_file in csv_files:
            table_name = csv_file.replace('.csv', '')
            file_path = os.path.join(csv_path, csv_file)
            
            # Create view directly from CSV without loading into Pandas (OOM safe)
            con.execute(f"CREATE OR REPLACE VIEW {table_name} AS SELECT * FROM read_csv_auto('{file_path}')")
            
            # Get row count efficiently
            count = con.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
            tables_loaded[table_name] = count
        
        # Get dataset info for sample timestamps
        dataset_info = None
        for key in ["dataset_info", "dataset_metadata"]:
            try:
                dataset_info = object_registry.get(dict, key)
                break
            except KeyError:
                continue
        
        # Create sample timestamp_df
        if dataset_info and "val_timestamp" in dataset_info:
            val_ts = pd.Timestamp(dataset_info["val_timestamp"])
            # Create a few timestamps around val_timestamp
            timestamps = pd.Series([
                val_ts - pd.Timedelta(days=60),
                val_ts - pd.Timedelta(days=30),
                val_ts
            ])
        else:
            # Use current date as fallback
            now = pd.Timestamp.now()
            timestamps = pd.Series([
                now - pd.Timedelta(days=90),
                now - pd.Timedelta(days=60),
                now - pd.Timedelta(days=30)
            ])
        
        timestamp_df = pd.DataFrame({"timestamp": timestamps})
        
        # Register timestamp_df in DuckDB
        con.register('timestamp_df', timestamp_df)
        
        # Replace {self.timedelta} placeholder if present
        test_query = sql_query.replace("{self.timedelta}", f"{timedelta_days} days")
        test_query = test_query.replace("'{self.timedelta}'", f"'{timedelta_days} days'")
        
        # Execute the query
        try:
            result_df = con.execute(test_query).df()
            
            return {
                "success": True,
                "row_count": len(result_df),
                "columns": list(result_df.columns),
                "dtypes": {col: str(result_df[col].dtype) for col in result_df.columns},
                "sample_output": result_df.head(10).to_dict(orient='records'),
                "null_counts": {col: int(result_df[col].isna().sum()) for col in result_df.columns},
                "tables_loaded": tables_loaded,
                "timestamps_used": timestamps.tolist()
            }
            
        except Exception as sql_error:
            error_msg = str(sql_error)
            suggestions = []
            
            # Provide helpful suggestions based on error
            if "not found" in error_msg.lower() or "does not exist" in error_msg.lower():
                suggestions.append("Table or column not found. Check spelling and use get_database_schema_for_task() to verify names.")
                suggestions.append(f"Available tables: {list(tables_loaded.keys())}")
            
            if "syntax" in error_msg.lower():
                suggestions.append("SQL syntax error. Check for missing commas, parentheses, or keywords.")
            
            if "ambiguous" in error_msg.lower():
                suggestions.append("Ambiguous column reference. Use table aliases (e.g., t.timestamp, a.driverId).")
            
            if "group by" in error_msg.lower():
                suggestions.append("GROUP BY error. Ensure all non-aggregated columns are in GROUP BY clause.")
            
            if "interval" in error_msg.lower():
                suggestions.append("INTERVAL syntax issue. Use: INTERVAL '30 days' or INTERVAL '1 year'")
            
            return {
                "success": False,
                "error": error_msg,
                "suggestions": suggestions,
                "tables_loaded": tables_loaded
            }
        
    except Exception as e:
        logger.error(f"Error testing SQL query: {e}")
        return {
            "success": False,
            "error": str(e),
            "suggestions": ["Unexpected error. Check that all tables exist and DatasetBuilder has run."]
        }


@tool
def analyze_potential_tasks(entity_table: str, entity_col: str) -> Dict[str, Any]:
    """
    Analyze the database to suggest potential prediction tasks for an entity.
    
    Args:
        entity_table: The entity table name (e.g., "users", "drivers")
        entity_col: The entity column (primary key) name (e.g., "userId", "driverId")
        
    Returns:
        Dictionary with suggested tasks:
        - aggregation_tasks: Tasks based on aggregating related records
        - classification_tasks: Potential binary/multiclass classification tasks
        - regression_tasks: Potential regression tasks
        - recommendation_tasks: Potential recommendation tasks
    """
    try:
        object_registry = ObjectRegistry()
        
        # Get schema info
        schema = None
        for src in ["unified_db_schema", "graph_schema_metadata"]:
            try:
                schema = object_registry.get(dict, src)
                break
            except KeyError:
                continue
        
        if schema is None:
            return {"error": "No schema found. Build database first."}
        
        relationships = schema.get("relationships", [])
        tables = schema.get("tables", {})
        
        # Find tables that reference this entity
        referencing_tables = []
        for rel in relationships:
            if isinstance(rel, dict):
                if rel.get("target_table") == entity_table:
                    referencing_tables.append({
                        "table": rel.get("source_table"),
                        "fk_col": rel.get("source_col"),
                        "relation_type": rel.get("relation_type", "many-to-one")
                    })
        
        suggestions = {
            "entity_table": entity_table,
            "entity_col": entity_col,
            "referencing_tables": referencing_tables,
            "suggested_tasks": []
        }
        
        # Suggest aggregation-based tasks
        for ref in referencing_tables:
            ref_table = ref["table"]
            if ref_table in tables:
                table_info = tables[ref_table]
                time_col = None
                if isinstance(table_info, dict):
                    temporal_cols = table_info.get("temporal_columns", [])
                    if temporal_cols:
                        time_col = temporal_cols[0]
                
                # Common aggregation patterns
                suggestions["suggested_tasks"].append({
                    "name": f"{entity_table}_{ref_table}_count",
                    "description": f"Predict number of {ref_table} records for each {entity_table}",
                    "task_type": "REGRESSION",
                    "aggregation": f"COUNT {ref_table} WHERE {ref['fk_col']} = {entity_col}",
                    "time_col": time_col
                })
                
                suggestions["suggested_tasks"].append({
                    "name": f"{entity_table}_has_{ref_table}",
                    "description": f"Predict if {entity_table} will have any {ref_table} in next period",
                    "task_type": "BINARY_CLASSIFICATION",
                    "aggregation": f"EXISTS {ref_table} WHERE {ref['fk_col']} = {entity_col}",
                    "time_col": time_col
                })
        
        return suggestions
        
    except Exception as e:
        logger.error(f"Error analyzing potential tasks: {e}")
        return {"error": str(e)}


@tool
def validate_task_code(code: str) -> Dict[str, Any]:
    """
    Validate the generated Task code by checking syntax and required components.
    
    Args:
        code: The Python code string to validate
        
    Returns:
        Dictionary with validation results:
        - valid: Boolean indicating if code is valid
        - errors: List of any errors found
        - warnings: List of any warnings
    """
    import ast
    
    errors = []
    warnings = []
    
    # Check syntax
    try:
        ast.parse(code)
    except SyntaxError as e:
        errors.append(f"Syntax error at line {e.lineno}: {e.msg}")
        return {
            "valid": False,
            "errors": errors,
            "warnings": warnings
        }
    
    # Check for required imports
    required_elements = [
        ("duckdb", "DuckDB is typically needed for efficient SQL queries"),
        ("pandas", "Pandas is required for DataFrame operations"),
        ("Table", "Must import Table from plexe.relbench.base"),
    ]
    
    for element, reason in required_elements:
        if element not in code:
            warnings.append(f"Missing '{element}': {reason}")
    
    # Check for task class inheritance
    if "EntityTask" not in code and "RecommendationTask" not in code:
        errors.append("Task class must inherit from EntityTask or RecommendationTask")
    
    # Check for required attributes
    required_attrs = ["task_type", "timedelta", "metrics"]
    for attr in required_attrs:
        if f"{attr} =" not in code and f"{attr}=" not in code:
            errors.append(f"Missing required attribute: {attr}")
    
    # Check EntityTask specific attributes
    if "EntityTask" in code:
        entity_attrs = ["entity_col", "entity_table", "target_col", "time_col"]
        for attr in entity_attrs:
            if f"{attr} =" not in code and f"{attr}=" not in code:
                errors.append(f"EntityTask missing required attribute: {attr}")
    
    # Check RecommendationTask specific attributes
    if "RecommendationTask" in code:
        rec_attrs = ["src_entity_col", "src_entity_table", "dst_entity_col", "dst_entity_table", "eval_k"]
        for attr in rec_attrs:
            if f"{attr} =" not in code and f"{attr}=" not in code:
                errors.append(f"RecommendationTask missing required attribute: {attr}")
    
    # Check for make_table method
    if "def make_table" not in code:
        errors.append("Task class must implement 'make_table' method")
    
    # Check make_table returns Table
    if "return Table(" not in code:
        warnings.append("make_table should return a Table object")
    
    return {
        "valid": len(errors) == 0,
        "errors": errors,
        "warnings": warnings
    }


@tool
def register_task_code(
    dataset_name: str,
    task_name: str,
    code: str,
    task_type: str,
    entity_table: str,
    description: str = ""
) -> Dict[str, Any]:
    """
    Register the generated Task code in ObjectRegistry for later use.
    
    Args:
        dataset_name: Name of the dataset this task is for
        task_name: Name of the task (e.g., "user_churn", "driver_dnf")
        code: The complete Python code for the Task class
        task_type: Type of task ("entity" or "recommendation")
        entity_table: The main entity table for this task
        description: Optional description of the task
        
    Returns:
        Confirmation message with registered key
    """
    try:
        object_registry = ObjectRegistry()
        
        # Store the code
        code_key = f"task_code_{dataset_name}_{task_name}"
        object_registry.register(str, code_key, code, overwrite=True)
        
        # Store metadata
        metadata_key = f"task_metadata_{dataset_name}_{task_name}"
        metadata = {
            "dataset_name": dataset_name,
            "task_name": task_name,
            "task_type": task_type,
            "entity_table": entity_table,
            "description": description,
            "code_key": code_key
        }
        object_registry.register(dict, metadata_key, metadata, overwrite=True)
        
        logger.info(f"✅ Registered task code for '{dataset_name}/{task_name}'")
        
        return {
            "success": True,
            "code_key": code_key,
            "metadata_key": metadata_key,
            "message": f"Successfully registered task code for '{dataset_name}/{task_name}'"
        }
        
    except Exception as e:
        logger.error(f"Failed to register task code: {e}")
        return {"success": False, "error": str(e)}


@tool
def export_task_code(
    dataset_name: str,
    task_name: str,
    output_dir: str = None
) -> Dict[str, Any]:
    """
    Export the generated Task code to a Python file in the chat session directory.
    
    Args:
        dataset_name: Name of the dataset
        task_name: Name of the task to export
        output_dir: Directory to save the Python file. If None, auto-creates a 
                   timestamped directory under workdir/chat-session-YYYYMMDD-HHMMSS/tasks/
        
    Returns:
        Dictionary with:
        - success: Boolean
        - file_path: Path to the exported file
    """
    from datetime import datetime
    
    try:
        object_registry = ObjectRegistry()
        
        # Get the code
        code_key = f"task_code_{dataset_name}_{task_name}"
        try:
            code = object_registry.get(str, code_key)
        except KeyError:
            return {
                "success": False,
                "error": f"No code found for task '{dataset_name}/{task_name}'. "
                         "Register the code first using register_task_code."
            }
        
        # Auto-create timestamped directory if not provided
        if output_dir is None:
            # Try to get existing session directory from registry
            try:
                output_dir = object_registry.get(str, "working_dir")
            except KeyError:
                try:
                    output_dir = object_registry.get(str, "current_chat_session_dir")
                except KeyError:
                    # Create new session directory
                    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                    output_dir = f".workdir/chat-session-{timestamp}"
                    object_registry.register(str, "current_chat_session_dir", output_dir, overwrite=True)
        
        # Create output directory
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Write the file
        file_name = "task.py"
        file_path = os.path.join(output_dir, file_name)
        
        with open(file_path, 'w') as f:
            f.write(code)
            
        # Register export info
        export_info = {
            "class_name": task_name,
            "export_path": file_path,
            "module_name": "task"
        }
        
        # Try to get metadata to populate additional fields
        metadata_key = f"task_metadata_{dataset_name}_{task_name}"
        try:
            metadata = object_registry.get(dict, metadata_key)
            export_info.update({
                "task_type": metadata.get("task_type"),
                "entity_table": metadata.get("entity_table"),
                "target_col": metadata.get("target_col"),
            })
        except KeyError:
            pass
            
        object_registry.register(dict, "task_code", export_info, overwrite=True)
        
        logger.info(f"✅ Exported task code to '{file_path}'")
        
        return {
            "success": True,
            "file_path": file_path,
            "output_dir": output_dir,
            "message": f"Successfully exported task code to '{file_path}'"
        }
        
    except Exception as e:
        logger.error(f"Failed to export task code: {e}")
        return {"success": False, "error": str(e)}


@tool
def get_entity_task_template() -> str:
    """
    Get the template for generating EntityTask code.
    
    Returns:
        The code template as a string with documentation.
    """
    return ENTITY_TASK_CODE_TEMPLATE


@tool
def get_recommendation_task_template() -> str:
    """
    Get the template for generating RecommendationTask code.
    
    Returns:
        The code template as a string with documentation.
    """
    return RECOMMENDATION_TASK_CODE_TEMPLATE


@tool
def get_sql_query_examples() -> Dict[str, Any]:
    """
    Get diverse SQL query examples for different task types.
    
    These examples show how to compute labels using DuckDB SQL queries
    over the database tables. Each pattern serves a different use case
    and can be adapted to various domains.
    
    IMPORTANT: These are PATTERN TEMPLATES - replace table names, column names,
    and conditions with actual values from your database schema.
    
    Returns:
        Dictionary with example queries for different scenarios organized by category.
    """
    return {
        # =====================================================================
        # BINARY CLASSIFICATION PATTERNS
        # =====================================================================
        "binary_churn_not_exists": {
            "category": "binary_classification",
            "description": "Churn = 1 if NO activity in next period (NOT EXISTS pattern)",
            "use_case": "User churn, product abandonment, subscription cancellation",
            "sql_pattern": """
        df = duckdb.sql(
            f\"\"\"
            SELECT
                timestamp,
                {entity_col},
                CAST(
                    NOT EXISTS (
                        SELECT 1
                        FROM {activity_table}
                        WHERE
                            {activity_table}.{entity_col} = {entity_table}.{entity_col} AND
                            {time_col} > timestamp AND
                            {time_col} <= timestamp + INTERVAL '{self.timedelta}'
                    ) AS INTEGER
                ) AS {target_col}
            FROM
                timestamp_df,
                {entity_table}
            WHERE
                EXISTS (
                    SELECT 1
                    FROM {activity_table}
                    WHERE
                        {activity_table}.{entity_col} = {entity_table}.{entity_col} AND
                        {time_col} > timestamp - INTERVAL '{self.timedelta}' AND
                        {time_col} <= timestamp
                )
            \"\"\"
        ).df()
"""
        },
        "binary_churn_with_active_filter": {
            "category": "binary_classification",
            "description": "Churn prediction with active entity filter using cross-join + double EXISTS",
            "use_case": "Customer churn prediction (H&M pattern) - filter to only recently active entities",
            "sql_pattern": """
        # Pattern: Cross-join timestamp_df with entity_table, 
        # Filter by EXISTS (recently active), then check NOT EXISTS (future activity)
        df = duckdb.sql(
            f\"\"\"
            SELECT
                timestamp,
                {entity_col},
                CAST(
                    NOT EXISTS (
                        SELECT 1
                        FROM {activity_table}
                        WHERE
                            {activity_table}.{entity_col} = {entity_table}.{entity_col} AND
                            {time_col} > timestamp AND
                            {time_col} <= timestamp + INTERVAL '{self.timedelta}'
                    ) AS INTEGER
                ) AS {target_col}
            FROM
                timestamp_df,
                {entity_table},
            WHERE
                EXISTS (
                    SELECT 1
                    FROM {activity_table}
                    WHERE
                        {activity_table}.{entity_col} = {entity_table}.{entity_col} AND
                        {time_col} > timestamp - INTERVAL '{self.timedelta}' AND
                        {time_col} <= timestamp
                )
            \"\"\"
        ).df()
"""
        },
        "binary_cte_count_case": {
            "category": "binary_classification",
            "description": "Binary classification using CTE + COUNT + CASE WHEN pattern",
            "use_case": "Paper citation, item purchase, event occurrence, badge earning",
            "sql_pattern": """
        # Use LEFT JOIN + GROUP BY + CASE WHEN for binary classification
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp,
                e.{entity_pk} as {entity_col},
                CASE WHEN COUNT({activity_table}.{activity_pk}) >= 1 THEN 1 ELSE 0 END AS {target_col}
            FROM timestamp_df t
            LEFT JOIN {entity_table} e
                ON e.{entity_time_col} <= t.timestamp
            LEFT JOIN {activity_table}
                ON e.{entity_pk} = {activity_table}.{entity_fk}
                AND {activity_table}.{activity_time_col} > t.timestamp
                AND {activity_table}.{activity_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
            GROUP BY t.timestamp, e.{entity_pk}
            \"\"\"
        ).df()
        
        # Post-processing if needed
        df = df.dropna(subset=["{entity_col}"])
        df[self.entity_col] = df[self.entity_col].astype(int)
"""
        },
        "binary_union_multi_activity": {
            "category": "binary_classification",
            "description": "UNION multiple activity tables into single CTE, then check engagement",
            "use_case": "User engagement across multiple activity types (posts, votes, comments)",
            "sql_pattern": """
        # Pattern: UNION multiple tables, create active users CTE, then check future activity
        df = duckdb.sql(
            f\"\"\"
            WITH
            ALL_ACTIVITY AS (
                SELECT id, {entity_col}, {time_col} FROM {activity_table_1}
                UNION
                SELECT id, {entity_col}, {time_col} FROM {activity_table_2}
                UNION
                SELECT id, {entity_col}, {time_col} FROM {activity_table_3}
            ),
            
            ACTIVE_ENTITIES AS (
                SELECT
                    t.timestamp,
                    e.{entity_pk},
                    COUNT(DISTINCT a.id) as n_activity
                FROM timestamp_df t
                CROSS JOIN {entity_table} e
                LEFT JOIN ALL_ACTIVITY a
                    ON e.{entity_pk} = a.{entity_col}
                    AND a.{time_col} <= t.timestamp
                WHERE e.{entity_pk} != {invalid_id}  -- e.g., != -1 for anonymous
                GROUP BY t.timestamp, e.{entity_pk}
            )
            
            SELECT
                ae.timestamp,
                ae.{entity_pk} as {entity_col},
                IF(COUNT(DISTINCT a.id) >= 1, 1, 0) as {target_col}
            FROM ACTIVE_ENTITIES ae
            LEFT JOIN ALL_ACTIVITY a
                ON ae.{entity_pk} = a.{entity_col}
                AND a.{time_col} > ae.timestamp
                AND a.{time_col} <= ae.timestamp + INTERVAL '{self.timedelta}'
            WHERE ae.n_activity >= 1  -- Only consider previously active entities
            GROUP BY ae.timestamp, ae.{entity_pk}
            \"\"\"
        ).df()
"""
        },
        "binary_status_based": {
            "category": "binary_classification",
            "description": "Classification based on status codes (MAX + CASE pattern)",
            "use_case": "Race DNF, order failure, task completion",
            "sql_pattern": """
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp AS {time_col},
                e.{entity_col},
                CASE
                    WHEN MAX(CASE WHEN r.{status_col} != {success_value} THEN 1 ELSE 0 END) = 1 THEN 1
                    ELSE 0
                END AS {target_col}
            FROM timestamp_df t
            LEFT JOIN {result_table} r
                ON r.{result_time_col} > t.timestamp
                AND r.{result_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
            LEFT JOIN {entity_table} e
                ON r.{fk_col} = e.{entity_col}
            WHERE e.{entity_col} IN (
                SELECT DISTINCT {entity_col}
                FROM {result_table}
                WHERE {result_time_col} > t.timestamp - INTERVAL '1 year'
            )
            GROUP BY t.timestamp, e.{entity_col}
            \"\"\"
        ).df()
"""
        },
        "binary_min_threshold": {
            "category": "binary_classification",
            "description": "Binary classification using MIN + threshold check (e.g., position <= 3)",
            "use_case": "Top-N finish, threshold achievement, qualifying position (F1 pattern)",
            "sql_pattern": """
        # Pattern: MIN + CASE WHEN for threshold-based binary classification
        # Active entity filter using IN subquery
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp as {time_col},
                a.{entity_col} as {entity_col},
                CASE
                    WHEN MIN(a.{position_col}) <= {threshold_value} THEN 1
                    ELSE 0
                END AS {target_col}
            FROM timestamp_df t
            LEFT JOIN {activity_table} a
                ON a.{activity_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
                AND a.{activity_time_col} > t.timestamp
            WHERE a.{entity_col} IN (
                SELECT DISTINCT {entity_col}
                FROM {activity_table}
                WHERE {activity_time_col} > t.timestamp - INTERVAL '1 year'
            )
            GROUP BY t.timestamp, a.{entity_col}
            \"\"\"
        ).df()
        
        # Post-processing: ensure integer type
        df["{target_col}"] = df["{target_col}"].astype("int64")
"""
        },
        
        # =====================================================================
        # MULTICLASS CLASSIFICATION PATTERNS
        # =====================================================================
        "multiclass_primary_category": {
            "category": "multiclass_classification",
            "description": "Predict primary/most-frequent category using ROW_NUMBER",
            "use_case": "Author's primary research area, user's main interest, product category",
            "sql_pattern": """
        # Use multiple CTEs: count categories, rank them, select top
        df = duckdb.sql(
            f\"\"\"
            WITH entity_activities AS (
                SELECT
                    t.timestamp AS {time_col},
                    ea.{entity_col},
                    i.{category_col}
                FROM timestamp_df t
                JOIN {entity_activity_table} ea
                    ON ea.{activity_time_col} > t.timestamp
                    AND ea.{activity_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
                JOIN {item_table} i
                    ON ea.{item_fk_col} = i.{item_pk_col}
            ),
            category_counts AS (
                SELECT
                    {time_col},
                    {entity_col},
                    {category_col},
                    COUNT(*) AS cnt
                FROM entity_activities
                GROUP BY {time_col}, {entity_col}, {category_col}
            ),
            ranked AS (
                SELECT
                    {time_col},
                    {entity_col},
                    {category_col},
                    ROW_NUMBER() OVER (
                        PARTITION BY {time_col}, {entity_col} 
                        ORDER BY cnt DESC
                    ) AS rn
                FROM category_counts
            )
            SELECT
                r.{time_col},
                r.{entity_col},
                c.{category_name_col} AS {target_col}
            FROM ranked r
            LEFT JOIN {category_table} c 
                ON r.{category_col} = c.{category_pk_col}
            WHERE r.rn = 1
            \"\"\"
        ).df()
"""
        },
        
        # =====================================================================
        # REGRESSION PATTERNS
        # =====================================================================
        "regression_count_cte": {
            "category": "regression",
            "description": "Count events per entity using CTE pattern",
            "use_case": "Publication count, order count, activity frequency",
            "sql_pattern": """
        df = duckdb.sql(
            f\"\"\"
            WITH entity_counts AS (
                SELECT
                    t.timestamp AS {time_col},
                    ea.{entity_col},
                    COUNT(ea.{item_col}) AS {target_col}
                FROM timestamp_df t
                JOIN {entity_activity_table} ea
                    ON ea.{activity_time_col} > t.timestamp
                    AND ea.{activity_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
                GROUP BY t.timestamp, ea.{entity_col}
            )
            SELECT {time_col}, {entity_col}, {target_col} 
            FROM entity_counts
            \"\"\"
        ).df()
"""
        },
        "regression_mean_active_filter": {
            "category": "regression",
            "description": "MEAN/AVG aggregation with active entity filter using IN subquery",
            "use_case": "Average position, average score, average rating (F1 pattern)",
            "sql_pattern": """
        # Pattern: MEAN() with active entity filter (IN subquery for recent activity)
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp as {time_col},
                a.{entity_col} as {entity_col},
                MEAN(a.{value_col}) as {target_col}
            FROM timestamp_df t
            LEFT JOIN {activity_table} a
                ON a.{activity_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
                AND a.{activity_time_col} > t.timestamp
            WHERE a.{entity_col} IN (
                SELECT DISTINCT {entity_col}
                FROM {activity_table}
                WHERE {activity_time_col} > t.timestamp - INTERVAL '1 year'
            )
            GROUP BY t.timestamp, a.{entity_col}
            \"\"\"
        ).df()
"""
        },
        "regression_sum_with_join": {
            "category": "regression",
            "description": "Sum values from related table (LTV pattern)",
            "use_case": "Customer lifetime value, revenue, total spend",
            "sql_pattern": """
        df = duckdb.sql(
            f\"\"\"
            SELECT
                timestamp,
                {entity_col},
                ltv
            FROM
                timestamp_df,
                {entity_table},
                (
                    SELECT COALESCE(SUM({value_col}), 0) AS ltv
                    FROM {transaction_table}, {item_table}
                    WHERE
                        {transaction_table}.{entity_col} = {entity_table}.{entity_col} AND
                        {transaction_table}.{item_fk_col} = {item_table}.{item_pk_col} AND
                        {transaction_time_col} > timestamp AND
                        {transaction_time_col} <= timestamp + INTERVAL '{self.timedelta}'
                )
            WHERE EXISTS (
                SELECT 1 FROM {transaction_table}
                WHERE {transaction_table}.{entity_col} = {entity_table}.{entity_col}
                    AND {transaction_time_col} <= timestamp
            )
            \"\"\"
        ).df()
"""
        },
        "regression_group_by_simple": {
            "category": "regression",
            "description": "Simple GROUP BY aggregation for entity-level metrics",
            "use_case": "Product total sales, item popularity",
            "sql_pattern": """
        df = duckdb.sql(
            f\"\"\"
            SELECT
                timestamp,
                {entity_table}.{entity_col},
                COALESCE(SUM({value_col}), 0) AS {target_col}
            FROM
                timestamp_df,
                {entity_table},
                {transaction_table}
            WHERE
                {transaction_table}.{fk_col} = {entity_table}.{entity_col} AND
                {transaction_time_col} > timestamp AND
                {transaction_time_col} <= timestamp + INTERVAL '{self.timedelta}'
            GROUP BY timestamp, {entity_table}.{entity_col}
            \"\"\"
        ).df()
"""
        },
        "regression_count_filtered": {
            "category": "regression",
            "description": "COUNT with multiple filter conditions (type, status, null checks)",
            "use_case": "Post upvotes count, filtered activity count (StackOverflow pattern)",
            "sql_pattern": """
        # Pattern: LEFT JOIN with multiple filter conditions
        # Filter entity by type/status, filter activity by type/status
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp,
                e.{entity_pk} AS {entity_col},
                COUNT(DISTINCT a.{activity_pk}) AS {target_col}
            FROM timestamp_df t
            LEFT JOIN {entity_table} e
                ON e.{entity_time_col} <= t.timestamp
                AND e.{entity_owner_col} != {invalid_id}  -- e.g., != -1
                AND e.{entity_owner_col} IS NOT NULL
                AND e.{entity_type_col} = {target_type_value}  -- e.g., PostTypeId = 1 (questions)
            LEFT JOIN {activity_table} a
                ON e.{entity_pk} = a.{activity_fk}
                AND a.{activity_time_col} > t.timestamp
                AND a.{activity_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
                AND a.{activity_type_col} = {activity_type_value}  -- e.g., VoteTypeId = 2 (upvotes)
            GROUP BY t.timestamp, e.{entity_pk}
            \"\"\"
        ).df()
"""
        },
        "regression_subquery_coalesce": {
            "category": "regression",
            "description": "Cross-join with correlated subquery + COALESCE for aggregation",
            "use_case": "Item sales prediction, entity total value (H&M pattern)",
            "sql_pattern": """
        # Pattern: Cross-join timestamp_df, entity_table with inline correlated subquery
        # The subquery computes aggregate and COALESCE handles NULL for no-activity
        df = duckdb.sql(
            f\"\"\"
            SELECT
                timestamp,
                {entity_col},
                {target_col}
            FROM
                timestamp_df,
                {entity_table},
                (
                    SELECT
                        COALESCE(SUM({value_col}), 0) as {target_col}
                    FROM
                        {transaction_table},
                    WHERE
                        {transaction_table}.{entity_col} = {entity_table}.{entity_col} AND
                        {transaction_time_col} > timestamp AND
                        {transaction_time_col} <= timestamp + INTERVAL '{self.timedelta}'
                )
            \"\"\"
        ).df()
"""
        },
        
        # =====================================================================
        # RECOMMENDATION / LINK PREDICTION PATTERNS
        # =====================================================================
        "recommendation_basic_list": {
            "category": "recommendation",
            "description": "Basic list of items entity will interact with",
            "use_case": "Product recommendation, content recommendation",
            "sql_pattern": """
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp,
                {interaction_table}.{src_entity_col},
                LIST(DISTINCT {interaction_table}.{dst_entity_col}) AS {dst_entity_col}
            FROM timestamp_df t
            LEFT JOIN {interaction_table}
                ON {interaction_table}.{interaction_time_col} > t.timestamp
                AND {interaction_table}.{interaction_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
            WHERE
                {interaction_table}.{src_entity_col} IS NOT NULL AND 
                {interaction_table}.{dst_entity_col} IS NOT NULL
            GROUP BY t.timestamp, {interaction_table}.{src_entity_col}
            \"\"\"
        ).df()
"""
        },
        "recommendation_left_join_group": {
            "category": "recommendation",
            "description": "LEFT JOIN with temporal window + GROUP BY for item lists",
            "use_case": "Customer purchase prediction, article recommendation (H&M pattern)",
            "sql_pattern": """
        # Pattern: LEFT JOIN transactions to timestamp_df with temporal filter, GROUP BY
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp,
                {transaction_table}.{src_entity_col},
                LIST(DISTINCT {transaction_table}.{dst_entity_col}) AS {dst_entity_col}
            FROM
                timestamp_df t
            LEFT JOIN
                {transaction_table}
            ON
                {transaction_table}.{transaction_time_col} > t.timestamp AND
                {transaction_table}.{transaction_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
            GROUP BY
                t.timestamp,
                {transaction_table}.{src_entity_col}
            \"\"\"
        ).df()
"""
        },
        "recommendation_filtered": {
            "category": "recommendation",
            "description": "Filtered recommendation (by rating, quality, etc.)",
            "use_case": "High-rating recommendations, quality interactions",
            "sql_pattern": """
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp,
                {interaction_table}.{src_entity_col},
                LIST(DISTINCT {interaction_table}.{dst_entity_col}) AS {dst_entity_col}
            FROM timestamp_df t
            LEFT JOIN {interaction_table}
                ON {interaction_table}.{interaction_time_col} > t.timestamp
                AND {interaction_table}.{interaction_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
            WHERE
                {interaction_table}.{src_entity_col} IS NOT NULL
                AND {interaction_table}.{dst_entity_col} IS NOT NULL
                AND {filter_condition}  -- e.g., rating = 5.0 or LENGTH(text) > 300
            GROUP BY t.timestamp, {interaction_table}.{src_entity_col}
            \"\"\"
        ).df()
"""
        },
        "recommendation_co_occurrence": {
            "category": "recommendation",
            "description": "Co-occurrence/co-citation pattern using self-join",
            "use_case": "Co-cited papers, frequently bought together, related items",
            "sql_pattern": """
        # Self-join pattern for co-occurrence relationships
        df = duckdb.sql(
            f\"\"\"
            WITH co_occurrences AS (
                SELECT
                    t.timestamp AS {time_col},
                    src.{src_entity_col},
                    other.{dst_ref_col} AS co_item
                FROM timestamp_df t
                JOIN {src_entity_table} src
                    ON src.{src_time_col} <= t.timestamp
                JOIN {link_table} link1
                    ON link1.{link_dst_col} = src.{src_entity_col}
                    AND link1.{link_time_col} > t.timestamp
                    AND link1.{link_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
                JOIN {link_table} link2
                    ON link1.{link_src_col} = link2.{link_src_col}
                    AND link2.{link_dst_col} <> src.{src_entity_col}
                    AND link2.{link_time_col} > t.timestamp
                    AND link2.{link_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
                JOIN {dst_entity_table} other
                    ON link2.{link_dst_col} = other.{dst_entity_col}
            )
            SELECT
                {time_col},
                {src_entity_col},
                array_agg(DISTINCT co_item) AS {dst_entity_col}
            FROM co_occurrences
            GROUP BY {time_col}, {src_entity_col}
            \"\"\"
        ).df()
"""
        },
        "recommendation_multi_table_chain": {
            "category": "recommendation",
            "description": "Chain multiple LEFT JOINs: timestamp -> entity -> interaction",
            "use_case": "User comments on posts, user votes on items (StackOverflow pattern)",
            "sql_pattern": """
        # Pattern: timestamp -> posts (existing) -> comments (future)
        # Predict which posts a user will comment on
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp,
                {interaction_table}.{src_entity_col} as {src_entity_col},
                LIST(DISTINCT {dst_entity_table}.{dst_pk}) AS {dst_entity_col}
            FROM timestamp_df t
            LEFT JOIN {dst_entity_table}
                ON {dst_entity_table}.{dst_time_col} <= t.timestamp  -- dst must exist before timestamp
            LEFT JOIN {interaction_table}
                ON {dst_entity_table}.{dst_pk} = {interaction_table}.{interaction_dst_fk}
                AND {interaction_table}.{interaction_time_col} > t.timestamp
                AND {interaction_table}.{interaction_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
            WHERE
                {interaction_table}.{src_entity_col} IS NOT NULL
                AND {dst_entity_table}.{dst_owner_col} != {invalid_id}  -- e.g., != -1
                AND {dst_entity_table}.{dst_owner_col} IS NOT NULL
            GROUP BY t.timestamp, {interaction_table}.{src_entity_col}
            \"\"\"
        ).df()
"""
        },
        "recommendation_self_reference": {
            "category": "recommendation",
            "description": "Self-referencing recommendation (src_table = dst_table via link table)",
            "use_case": "Related posts, linked items, post-to-post links (StackOverflow pattern)",
            "sql_pattern": """
        # Pattern: Predict which items will be linked to a source item
        # Both src and dst come from the same table via a link table
        df = duckdb.sql(
            f\"\"\"
            SELECT
                t.timestamp,
                {link_table}.{src_col} as {src_entity_col},
                LIST(DISTINCT {link_table}.{dst_col}) AS {dst_entity_col}
            FROM timestamp_df t
            LEFT JOIN {link_table}
                ON {link_table}.{link_time_col} > t.timestamp
                AND {link_table}.{link_time_col} <= t.timestamp + INTERVAL '{self.timedelta}'
            LEFT JOIN {entity_table} e1
                ON {link_table}.{src_col} = e1.{entity_pk}
            LEFT JOIN {entity_table} e2
                ON {link_table}.{dst_col} = e2.{entity_pk}
            WHERE
                {link_table}.{src_col} IS NOT NULL
                AND {link_table}.{dst_col} IS NOT NULL
                AND e1.{entity_time_col} <= t.timestamp  -- src must exist before
                AND e2.{entity_time_col} <= t.timestamp  -- dst must exist before
            GROUP BY t.timestamp, {link_table}.{src_col}
            \"\"\"
        ).df()
"""
        },
        
        # =====================================================================
        # SQL TIPS AND PATTERNS
        # =====================================================================
        "sql_building_blocks": {
            "category": "tips",
            "cte_pattern": "WITH cte_name AS (SELECT ...) SELECT ... FROM cte_name",
            "temporal_filter": "{time_col} > t.timestamp AND {time_col} <= t.timestamp + INTERVAL '{self.timedelta}'",
            "active_entity_filter": "WHERE e.{entity_col} IN (SELECT DISTINCT {entity_col} FROM {table} WHERE {time_col} <= t.timestamp)",
            "exists_filter": "WHERE EXISTS (SELECT 1 FROM {table} WHERE {condition})",
            "binary_from_count": "CASE WHEN COUNT(*) > 0 THEN 1 ELSE 0 END",
            "binary_from_boolean": "CAST(boolean_expr AS INTEGER)",
            "list_aggregation_duckdb": "LIST(DISTINCT col) or array_agg(DISTINCT col)",
            "null_safe_sum": "COALESCE(SUM(value), 0)",
            "ranking_pattern": "ROW_NUMBER() OVER (PARTITION BY group_cols ORDER BY sort_col DESC)",
        },
        "join_patterns": {
            "category": "tips",
            "cross_join": "timestamp_df, entity_table -- Cartesian product for all timestamp-entity pairs",
            "inner_join": "JOIN table ON condition -- Only matching rows",
            "left_join": "LEFT JOIN table ON condition -- Keep all left rows, NULL for non-matches",
            "self_join": "JOIN table t1 JOIN table t2 ON t1.col = t2.col -- For co-occurrence patterns",
        },
        "common_mistakes": {
            "category": "tips",
            "mistakes": [
                "Forgetting NULL filters in recommendation (WHERE col IS NOT NULL)",
                "Wrong temporal direction (should be: time > timestamp, not >=)",
                "Missing GROUP BY when using aggregations",
                "Temporal leakage: filtering entities using future data",
                "Not handling zero-count case (use COALESCE or LEFT JOIN)",
                "Hardcoding table/column names instead of using schema",
            ]
        }
    }
