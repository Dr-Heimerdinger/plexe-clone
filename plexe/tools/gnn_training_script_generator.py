"""
Tools for generating GNN training scripts for the Relational GNN Specialist Agent.

This module provides tools to generate complete, executable training scripts
that use the relbench.modeling modules for training GNN models on relational data.
"""

import os
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

from smolagents import tool

from plexe.core.object_registry import ObjectRegistry

logger = logging.getLogger(__name__)


# =============================================================================
# Training Script Template
# =============================================================================

TRAINING_SCRIPT_TEMPLATE = '''"""
Auto-generated GNN Training Script.
Generated by RelationalGNNSpecialist Agent.

This script trains a Graph Neural Network model using the RelBench framework.
It uses temporal sampling to ensure no data leakage from future to past.
"""

import os
import sys
import math
import copy
import numpy as np
from tqdm import tqdm
from typing import Any, Dict, List, Optional

import torch
import torch.nn as nn
from torch import Tensor
from torch.nn import Embedding, ModuleDict
from torch_geometric.data import HeteroData
from torch_geometric.nn import MLP
from torch_geometric.typing import NodeType
from torch_geometric.loader import NeighborLoader
from torch_geometric.seed import seed_everything
from torch_frame.data.stats import StatType

# Import custom Dataset and Task
{dataset_import}
{task_import}

# Import RelBench modeling utilities
from plexe.relbench.modeling.graph import make_pkey_fkey_graph, get_node_train_table_input
from plexe.relbench.modeling.utils import get_stype_proposal
from plexe.relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder

# Set random seed for reproducibility
seed_everything({random_seed})

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {{device}}")

# Configuration
WORKING_DIR = "{working_dir}"
CACHE_DIR = os.path.join(WORKING_DIR, "cache")
MODEL_SAVE_PATH = os.path.join(WORKING_DIR, "best_model.pt")

# Training hyperparameters
EPOCHS = {epochs}
BATCH_SIZE = {batch_size}
LEARNING_RATE = {learning_rate}
HIDDEN_CHANNELS = {hidden_channels}
NUM_GNN_LAYERS = {num_gnn_layers}
NUM_NEIGHBORS = {num_neighbors}  # Neighbors per layer
DROPOUT = {dropout}

# Task configuration
TUNE_METRIC = "{tune_metric}"
HIGHER_IS_BETTER = {higher_is_better}


# =============================================================================
# Text Embedder (for text columns)
# =============================================================================

{text_embedder_code}


# =============================================================================
# GNN Model Definition
# =============================================================================

class GNNModel(torch.nn.Module):
    """
    Heterogeneous GNN model for relational data.
    
    Components:
    - HeteroEncoder: Encodes tabular features from each table into vectors
    - HeteroTemporalEncoder: Encodes relative time information
    - HeteroGraphSAGE: Message passing between tables via foreign key relations
    - MLP: Final prediction head
    """

    def __init__(
        self,
        data: HeteroData,
        col_stats_dict: Dict[str, Dict[str, Dict[StatType, Any]]],
        num_layers: int,
        channels: int,
        out_channels: int,
        aggr: str = "mean",
        norm: str = "batch_norm",
        shallow_list: List[NodeType] = [],
        id_awareness: bool = False,
    ):
        super().__init__()

        # Tabular feature encoder
        self.encoder = HeteroEncoder(
            channels=channels,
            node_to_col_names_dict={{
                node_type: data[node_type].tf.col_names_dict
                for node_type in data.node_types
            }},
            node_to_col_stats=col_stats_dict,
        )
        
        # Temporal encoder for time-aware representations
        self.temporal_encoder = HeteroTemporalEncoder(
            node_types=[
                node_type for node_type in data.node_types if "time" in data[node_type]
            ],
            channels=channels,
        )
        
        # Graph Neural Network (message passing)
        self.gnn = HeteroGraphSAGE(
            node_types=data.node_types,
            edge_types=data.edge_types,
            channels=channels,
            aggr=aggr,
            num_layers=num_layers,
        )
        
        # Prediction head
        self.head = MLP(
            channels,
            out_channels=out_channels,
            norm=norm,
            num_layers=1,
        )
        
        # Optional: Shallow embeddings for specific node types
        self.embedding_dict = ModuleDict({{
            node: Embedding(data.num_nodes_dict[node], channels)
            for node in shallow_list
        }})

        # Optional: ID awareness embedding
        self.id_awareness_emb = None
        if id_awareness:
            self.id_awareness_emb = torch.nn.Embedding(1, channels)
            
        self.reset_parameters()

    def reset_parameters(self):
        self.encoder.reset_parameters()
        self.temporal_encoder.reset_parameters()
        self.gnn.reset_parameters()
        self.head.reset_parameters()
        for embedding in self.embedding_dict.values():
            torch.nn.init.normal_(embedding.weight, std=0.1)
        if self.id_awareness_emb is not None:
            self.id_awareness_emb.reset_parameters()

    def forward(
        self,
        batch: HeteroData,
        entity_table: NodeType,
    ) -> Tensor:
        seed_time = batch[entity_table].seed_time
        
        # Encode tabular features
        x_dict = self.encoder(batch.tf_dict)

        # Add temporal encoding
        rel_time_dict = self.temporal_encoder(
            seed_time, batch.time_dict, batch.batch_dict
        )
        for node_type, rel_time in rel_time_dict.items():
            x_dict[node_type] = x_dict[node_type] + rel_time

        # Add shallow embeddings if configured
        for node_type, embedding in self.embedding_dict.items():
            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)

        # Message passing through GNN
        x_dict = self.gnn(
            x_dict,
            batch.edge_index_dict,
            batch.num_sampled_nodes_dict,
            batch.num_sampled_edges_dict,
        )

        # Return predictions for seed nodes only
        return self.head(x_dict[entity_table][:seed_time.size(0)])


# =============================================================================
# Main Training Logic
# =============================================================================

def main():
    print("=" * 60)
    print("GNN Training Script for Relational Data")
    print("=" * 60)
    
    # -------------------------------------------------------------------------
    # Step 1: Load Dataset and Task
    # -------------------------------------------------------------------------
    print("\\n[Step 1] Loading Dataset and Task...")
    
    dataset = {dataset_class}()
    task = {task_class}(dataset)
    db = dataset.get_db()
    
    print(f"  Dataset: {{dataset.__class__.__name__}}")
    print(f"  Task: {{task.__class__.__name__}}")
    print(f"  Entity table: {{task.entity_table}}")
    print(f"  Task type: {{task.task_type}}")
    
    # -------------------------------------------------------------------------
    # Step 2: Create Train/Val/Test Tables
    # -------------------------------------------------------------------------
    print("\\n[Step 2] Creating temporal train/val/test tables...")
    
    train_table = task.get_table("train")
    val_table = task.get_table("val")
    test_table = task.get_table("test")
    
    print(f"  Train samples: {{len(train_table.df)}}")
    print(f"  Val samples: {{len(val_table.df)}}")
    print(f"  Test samples: {{len(test_table.df)}}")
    
    # -------------------------------------------------------------------------
    # Step 3: Build Heterogeneous Graph
    # -------------------------------------------------------------------------
    print("\\n[Step 3] Building heterogeneous graph from database...")
    
    # Infer column types
    col_to_stype_dict = get_stype_proposal(db)
    
    # Configure text embedder
    {text_embedder_config}
    
    # Build graph
    os.makedirs(CACHE_DIR, exist_ok=True)
    data, col_stats_dict = make_pkey_fkey_graph(
        db,
        col_to_stype_dict=col_to_stype_dict,
        text_embedder_cfg=text_embedder_cfg,
        cache_dir=CACHE_DIR,
    )
    
    print(f"  Node types: {{data.node_types}}")
    print(f"  Edge types: {{len(data.edge_types)}}")
    for node_type in data.node_types:
        print(f"    - {{node_type}}: {{data[node_type].num_nodes}} nodes")
    
    # -------------------------------------------------------------------------
    # Step 4: Create Data Loaders with Temporal Sampling
    # -------------------------------------------------------------------------
    print("\\n[Step 4] Creating temporal neighbor loaders...")
    
    loader_dict = {{}}
    
    for split, table in [
        ("train", train_table),
        ("val", val_table),
        ("test", test_table),
    ]:
        table_input = get_node_train_table_input(
            table=table,
            task=task,
        )
        entity_table = table_input.nodes[0]
        
        loader_dict[split] = NeighborLoader(
            data,
            num_neighbors=NUM_NEIGHBORS,
            time_attr="time",
            input_nodes=table_input.nodes,
            input_time=table_input.time,
            transform=table_input.transform,
            batch_size=BATCH_SIZE,
            temporal_strategy="uniform",
            shuffle=(split == "train"),
            num_workers=0,
            persistent_workers=False,
        )
        print(f"  {{split}}: {{len(loader_dict[split])}} batches")
    
    # -------------------------------------------------------------------------
    # Step 5: Initialize Model
    # -------------------------------------------------------------------------
    print("\\n[Step 5] Initializing GNN model...")
    
    model = GNNModel(
        data=data,
        col_stats_dict=col_stats_dict,
        num_layers=NUM_GNN_LAYERS,
        channels=HIDDEN_CHANNELS,
        out_channels={out_channels},
        aggr="sum",
        norm="batch_norm",
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  Total parameters: {{total_params:,}}")
    
    # -------------------------------------------------------------------------
    # Step 6: Setup Loss and Optimizer
    # -------------------------------------------------------------------------
    print("\\n[Step 6] Setting up loss function and optimizer...")
    
    {loss_function_code}
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    print(f"  Loss function: {{loss_fn.__class__.__name__}}")
    print(f"  Optimizer: Adam (lr={{LEARNING_RATE}})")
    
    # -------------------------------------------------------------------------
    # Step 7: Training Loop
    # -------------------------------------------------------------------------
    print("\\n[Step 7] Starting training...")
    print("-" * 60)
    
    def train_epoch() -> float:
        model.train()
        loss_accum = 0
        count_accum = 0
        
        for batch in tqdm(loader_dict["train"], desc="Training"):
            batch = batch.to(device)
            optimizer.zero_grad()
            
            pred = model(batch, task.entity_table)
            pred = pred.view(-1) if pred.size(1) == 1 else pred
            
            loss = loss_fn(pred.float(), batch[entity_table].y.float())
            loss.backward()
            optimizer.step()
            
            loss_accum += loss.detach().item() * pred.size(0)
            count_accum += pred.size(0)
        
        return loss_accum / count_accum

    @torch.no_grad()
    def evaluate(loader: NeighborLoader) -> np.ndarray:
        model.eval()
        pred_list = []
        
        for batch in loader:
            batch = batch.to(device)
            pred = model(batch, task.entity_table)
            pred = pred.view(-1) if pred.size(1) == 1 else pred
            pred_list.append(pred.detach().cpu())
        
        return torch.cat(pred_list, dim=0).numpy()

    # Training loop
    best_state_dict = None
    best_val_metric = -math.inf if HIGHER_IS_BETTER else math.inf
    
    for epoch in range(1, EPOCHS + 1):
        train_loss = train_epoch()
        val_pred = evaluate(loader_dict["val"])
        val_metrics = task.evaluate(val_pred, val_table)
        
        print(f"Epoch {{epoch:02d}} | Train Loss: {{train_loss:.4f}} | Val Metrics: {{val_metrics}}")
        
        # Check for improvement
        current_metric = val_metrics[TUNE_METRIC]
        improved = (
            (HIGHER_IS_BETTER and current_metric > best_val_metric) or
            (not HIGHER_IS_BETTER and current_metric < best_val_metric)
        )
        
        if improved:
            best_val_metric = current_metric
            best_state_dict = copy.deepcopy(model.state_dict())
            print(f"  -> New best {{TUNE_METRIC}}: {{best_val_metric:.4f}}")
    
    # -------------------------------------------------------------------------
    # Step 8: Final Evaluation
    # -------------------------------------------------------------------------
    print("\\n[Step 8] Final evaluation with best model...")
    print("-" * 60)
    
    # Load best model
    model.load_state_dict(best_state_dict)
    
    # Evaluate on validation
    val_pred = evaluate(loader_dict["val"])
    val_metrics = task.evaluate(val_pred, val_table)
    print(f"Best Validation Metrics: {{val_metrics}}")
    
    # Evaluate on test
    test_pred = evaluate(loader_dict["test"])
    test_metrics = task.evaluate(test_pred)
    print(f"Test Metrics: {{test_metrics}}")
    
    # -------------------------------------------------------------------------
    # Step 9: Save Model
    # -------------------------------------------------------------------------
    print(f"\\n[Step 9] Saving model to {{MODEL_SAVE_PATH}}...")
    
    torch.save({{
        "model_state_dict": best_state_dict,
        "model_config": {{
            "num_layers": NUM_GNN_LAYERS,
            "channels": HIDDEN_CHANNELS,
            "out_channels": {out_channels},
        }},
        "val_metrics": val_metrics,
        "test_metrics": test_metrics,
    }}, MODEL_SAVE_PATH)
    
    print("\\n" + "=" * 60)
    print("Training Complete!")
    print(f"Best {{TUNE_METRIC}}: {{best_val_metric:.4f}}")
    print("=" * 60)
    
    return test_metrics


if __name__ == "__main__":
    main()
'''


# =============================================================================
# Text Embedder Code Templates
# =============================================================================

GLOVE_TEXT_EMBEDDER = '''
from typing import List, Optional
from sentence_transformers import SentenceTransformer
from torch import Tensor


class GloveTextEmbedding:
    """GloVe-based text embedder using sentence-transformers."""
    
    def __init__(self, device: Optional[torch.device] = None):
        self.model = SentenceTransformer(
            "sentence-transformers/average_word_embeddings_glove.6B.300d",
            device=device,
        )

    def __call__(self, sentences: List[str]) -> Tensor:
        return torch.from_numpy(self.model.encode(sentences))
'''

GLOVE_TEXT_EMBEDDER_CONFIG = '''
    from torch_frame.config.text_embedder import TextEmbedderConfig
    text_embedder_cfg = TextEmbedderConfig(
        text_embedder=GloveTextEmbedding(device=device), 
        batch_size=256
    )
'''

NO_TEXT_EMBEDDER = '''
# No text embedder needed (no text columns detected)
'''

NO_TEXT_EMBEDDER_CONFIG = '''
    text_embedder_cfg = None  # No text columns
'''


# =============================================================================
# Loss Function Templates
# =============================================================================

LOSS_FUNCTION_TEMPLATES = {
    "regression": "loss_fn = torch.nn.L1Loss()  # MAE for regression",
    "mse_regression": "loss_fn = torch.nn.MSELoss()  # MSE for regression",
    "binary_classification": "loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary classification",
    "multiclass_classification": "loss_fn = torch.nn.CrossEntropyLoss()  # Multiclass classification",
}


# =============================================================================
# Tool: Get Training Script Template
# =============================================================================

@tool
def get_training_script_template() -> str:
    """
    Returns the GNN training script template for reference.
    
    Use this to understand the structure of the training script before
    generating a customized version with generate_training_script.
    
    Returns:
        The training script template string
    """
    return TRAINING_SCRIPT_TEMPLATE


# =============================================================================
# Tool: Generate Training Script
# =============================================================================

@tool
def generate_training_script(
    dataset_module_path: str,
    dataset_class_name: str,
    task_module_path: str,
    task_class_name: str,
    working_dir: str,
    task_type: str = "regression",
    tune_metric: str = "mae",
    higher_is_better: bool = False,
    out_channels: int = 1,
    epochs: int = 10,
    batch_size: int = 512,
    learning_rate: float = 0.005,
    hidden_channels: int = 128,
    num_gnn_layers: int = 2,
    num_neighbors: Optional[List[int]] = None,
    dropout: float = 0.0,
    use_text_embedder: bool = True,
    random_seed: int = 42,
) -> Dict[str, Any]:
    """
    Generates a complete GNN training script using plexe.relbench.modeling modules.
    
    This tool creates a Python script that:
    1. Loads the custom Dataset and Task classes
    2. Builds a heterogeneous graph using make_pkey_fkey_graph
    3. Creates temporal neighbor loaders for train/val/test
    4. Defines and trains a HeteroGNN model
    5. Evaluates and saves the best model
    
    Args:
        dataset_module_path: Path to the dataset module (e.g., "workdir/dataset.py")
        dataset_class_name: Name of the Dataset class (e.g., "MyCustomDataset")
        task_module_path: Path to the task module (e.g., "workdir/task.py")
        task_class_name: Name of the Task class (e.g., "UserLTVTask")
        working_dir: Directory to save outputs (model checkpoints, cache, etc.)
        task_type: Type of task - "regression", "binary_classification", "multiclass_classification"
        tune_metric: Metric to optimize (e.g., "mae", "accuracy", "roc_auc")
        higher_is_better: Whether higher metric values are better
        out_channels: Number of output channels (1 for regression/binary, num_classes for multiclass)
        epochs: Number of training epochs
        batch_size: Batch size for training
        learning_rate: Learning rate for Adam optimizer
        hidden_channels: Hidden dimension for GNN layers
        num_gnn_layers: Number of GNN layers (message passing depth)
        num_neighbors: List of neighbors per layer (default: [128, 128])
        dropout: Dropout rate
        use_text_embedder: Whether to use GloVe text embedder for text columns
        random_seed: Random seed for reproducibility
        
    Returns:
        Dictionary containing:
        - 'script_content': The generated Python script
        - 'script_path': Path where the script was saved
        - 'config': Configuration used for generation
    """
    try:
        # Default num_neighbors
        if num_neighbors is None:
            num_neighbors = [128] * num_gnn_layers
            
        # Ensure working_dir exists
        os.makedirs(working_dir, exist_ok=True)
        
        # Generate import statements
        dataset_module = os.path.splitext(os.path.basename(dataset_module_path))[0]
        task_module = os.path.splitext(os.path.basename(task_module_path))[0]
        
        # Add the working directory to the import path in the script
        dataset_import = f"""
# Add working directory to path for imports
sys.path.insert(0, "{os.path.dirname(os.path.abspath(dataset_module_path))}")
from {dataset_module} import {dataset_class_name}
"""
        
        task_import = f"""from {task_module} import {task_class_name}
"""
        
        # Select loss function based on task type
        if task_type == "regression":
            loss_function_code = LOSS_FUNCTION_TEMPLATES["regression"]
        elif task_type == "mse_regression":
            loss_function_code = LOSS_FUNCTION_TEMPLATES["mse_regression"]
        elif task_type == "binary_classification":
            loss_function_code = LOSS_FUNCTION_TEMPLATES["binary_classification"]
        elif task_type == "multiclass_classification":
            loss_function_code = LOSS_FUNCTION_TEMPLATES["multiclass_classification"]
        else:
            loss_function_code = LOSS_FUNCTION_TEMPLATES["regression"]
        
        # Configure text embedder
        if use_text_embedder:
            text_embedder_code = GLOVE_TEXT_EMBEDDER
            text_embedder_config = GLOVE_TEXT_EMBEDDER_CONFIG
        else:
            text_embedder_code = NO_TEXT_EMBEDDER
            text_embedder_config = NO_TEXT_EMBEDDER_CONFIG
        
        # Format num_neighbors for the script
        num_neighbors_str = str(num_neighbors)
        
        # Generate the script
        script_content = TRAINING_SCRIPT_TEMPLATE.format(
            dataset_import=dataset_import,
            task_import=task_import,
            dataset_class=dataset_class_name,
            task_class=task_class_name,
            working_dir=working_dir,
            random_seed=random_seed,
            epochs=epochs,
            batch_size=batch_size,
            learning_rate=learning_rate,
            hidden_channels=hidden_channels,
            num_gnn_layers=num_gnn_layers,
            num_neighbors=num_neighbors_str,
            dropout=dropout,
            tune_metric=tune_metric,
            higher_is_better=higher_is_better,
            out_channels=out_channels,
            text_embedder_code=text_embedder_code,
            text_embedder_config=text_embedder_config,
            loss_function_code=loss_function_code,
        )
        
        # Save the script
        script_path = os.path.join(working_dir, "train_script.py")
        with open(script_path, "w") as f:
            f.write(script_content)
        
        logger.info(f"Generated training script at: {script_path}")
        
        config = {
            "dataset_module": dataset_module_path,
            "dataset_class": dataset_class_name,
            "task_module": task_module_path,
            "task_class": task_class_name,
            "working_dir": working_dir,
            "task_type": task_type,
            "tune_metric": tune_metric,
            "higher_is_better": higher_is_better,
            "out_channels": out_channels,
            "epochs": epochs,
            "batch_size": batch_size,
            "learning_rate": learning_rate,
            "hidden_channels": hidden_channels,
            "num_gnn_layers": num_gnn_layers,
            "num_neighbors": num_neighbors,
            "dropout": dropout,
            "use_text_embedder": use_text_embedder,
            "random_seed": random_seed,
        }
        
        return {
            "status": "success",
            "script_content": script_content,
            "script_path": script_path,
            "config": config,
            "message": f"Training script generated successfully at {script_path}",
        }
        
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc(),
        }


# =============================================================================
# Tool: Execute Training Script
# =============================================================================

@tool
def execute_training_script(
    script_path: str,
    timeout: int = 3600,
) -> Dict[str, Any]:
    """
    Executes the generated training script.
    
    Args:
        script_path: Path to the training script (train_script.py)
        timeout: Maximum execution time in seconds (default: 1 hour)
        
    Returns:
        Dictionary containing:
        - 'status': 'success' or 'error'
        - 'output': Script stdout
        - 'error_output': Script stderr (if any)
        - 'return_code': Process return code
    """
    import subprocess
    
    try:
        # Run the script
        result = subprocess.run(
            ["python", script_path],
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd=os.path.dirname(script_path),
        )
        
        return {
            "status": "success" if result.returncode == 0 else "error",
            "output": result.stdout,
            "error_output": result.stderr,
            "return_code": result.returncode,
        }
        
    except subprocess.TimeoutExpired:
        return {
            "status": "error",
            "error": f"Script execution timed out after {timeout} seconds",
        }
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc(),
        }


# =============================================================================
# Tool: Get Dataset and Task Info from Registry
# =============================================================================

@tool
def get_dataset_task_info_from_registry() -> Dict[str, Any]:
    """
    Retrieves information about the Dataset and Task classes from ObjectRegistry.
    
    This tool helps find the paths and class names for the Dataset and Task
    that were generated by DatasetBuilder and TaskBuilder agents.
    
    Returns:
        Dictionary containing:
        - 'dataset_info': Information about the registered Dataset
        - 'task_info': Information about the registered Task
        - 'working_dir': The working directory being used
    """
    try:
        object_registry = ObjectRegistry()
        
        result = {
            "dataset_info": None,
            "task_info": None,
            "working_dir": None,
        }
        
        # Try to get dataset code info
        try:
            dataset_code_info = object_registry.get(dict, "database_code")
            result["dataset_info"] = {
                "class_name": dataset_code_info.get("class_name"),
                "file_path": dataset_code_info.get("export_path"),
                "module_name": dataset_code_info.get("module_name"),
            }
            if dataset_code_info.get("export_path"):
                result["working_dir"] = os.path.dirname(dataset_code_info["export_path"])
        except KeyError:
            pass
        
        # Try to get task code info
        try:
            task_code_info = object_registry.get(dict, "task_code")
            result["task_info"] = {
                "class_name": task_code_info.get("class_name"),
                "file_path": task_code_info.get("export_path"),
                "module_name": task_code_info.get("module_name"),
                "task_type": task_code_info.get("task_type"),
                "entity_table": task_code_info.get("entity_table"),
                "target_col": task_code_info.get("target_col"),
            }
        except KeyError:
            pass
        
        # Try to get working directory if not found
        if result["working_dir"] is None:
            try:
                working_dir = object_registry.get(str, "working_dir")
                result["working_dir"] = working_dir
            except KeyError:
                pass
        
        return result
        
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc(),
        }


# =============================================================================
# Tool: Validate Training Prerequisites
# =============================================================================

@tool
def validate_training_prerequisites(
    dataset_module_path: str,
    task_module_path: str,
) -> Dict[str, Any]:
    """
    Validates that all prerequisites for training are in place.
    
    Checks:
    1. Dataset module exists and is importable
    2. Task module exists and is importable
    3. Required dependencies are installed
    
    Args:
        dataset_module_path: Path to the dataset module
        task_module_path: Path to the task module
        
    Returns:
        Dictionary with validation results
    """
    issues = []
    warnings = []
    
    # Check dataset module
    if not os.path.exists(dataset_module_path):
        issues.append(f"Dataset module not found: {dataset_module_path}")
    
    # Check task module
    if not os.path.exists(task_module_path):
        issues.append(f"Task module not found: {task_module_path}")
    
    # Check required packages
    required_packages = [
        "torch",
        "torch_geometric",
        "torch_frame",
        "sentence_transformers",
        "pandas",
        "numpy",
        "tqdm",
    ]
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            issues.append(f"Required package not installed: {package}")
    
    # Check CUDA availability
    try:
        import torch
        if torch.cuda.is_available():
            warnings.append("CUDA is available - training will use GPU")
        else:
            warnings.append("CUDA not available - training will use CPU (slower)")
    except ImportError:
        pass
    
    return {
        "status": "valid" if len(issues) == 0 else "invalid",
        "issues": issues,
        "warnings": warnings,
        "dataset_path": dataset_module_path,
        "task_path": task_module_path,
    }
