"""
Tools for building RelBench Database objects from raw CSV data.

These tools support the DatasetBuilder agent in constructing proper Database objects
with schema definitions, temporal processing, and data cleaning.
"""

import os
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path

import pandas as pd
from smolagents import tool

from plexe.core.object_registry import ObjectRegistry

logger = logging.getLogger(__name__)


# =============================================================================
# Template for generated Dataset code
# =============================================================================

DATASET_CODE_TEMPLATE = '''"""
Auto-generated Dataset class for {dataset_name}.
Generated by DatasetBuilder Agent.

IMPORTANT CONCEPTS:
- val_timestamp/test_timestamp: Define temporal splitting to prevent data leakage
- time_col: The CREATION TIME of a row (not just any timestamp column)
- Tables without time_col are treated as existing since -infinity (static data)
- pkey_col can be None for junction/event tables (e.g., votes, reviews)
- get_db() automatically reindexes pkeys/fkeys to consecutive integers
"""

import os
import numpy as np
import pandas as pd
from typing import Optional

from plexe.relbench.base import Database, Dataset, Table


class {class_name}(Dataset):
    """
    Dataset for {dataset_name}.
    
    {description}
    
    Temporal Split:
    - val_timestamp: {val_timestamp}
    - test_timestamp: {test_timestamp}
    """
    
    # Temporal split timestamps
    # Only rows UP TO val_timestamp can be used for validation predictions
    # Only rows UP TO test_timestamp can be used for test predictions
    val_timestamp = pd.Timestamp("{val_timestamp}")
    test_timestamp = pd.Timestamp("{test_timestamp}")

    def __init__(
        self, 
        csv_dir: str, 
        cache_dir: Optional[str] = None,
    ):
        """
        Initialize the dataset.
        
        Args:
            csv_dir: Directory containing the CSV files.
            cache_dir: Optional directory for caching the database.
        """
        self.csv_dir = csv_dir
        super().__init__(cache_dir=cache_dir)

    def make_db(self) -> Database:
        """
        Build the Database from CSV files.
        
        Note: This method is called by get_db() which also handles:
        - Caching the database
        - Reindexing pkeys/fkeys to consecutive integers
        - Filtering rows up to test_timestamp
        
        Returns:
            Database object with all tables properly configured.
        """
        path = self.csv_dir
        
        ########################################
        # 1. LOAD CSV FILES
        ########################################
{data_loading_code}

        ########################################
        # 2. DATA CLEANING & PROCESSING
        ########################################
{data_processing_code}

        ########################################
        # 3. BUILD DATABASE
        ########################################
{table_definitions_code}

        return db
'''


@tool
def get_csv_files_from_path(path_key: str = "path_raw_csv_files") -> Dict[str, Any]:
    """
    Get list of CSV files from a registered path in ObjectRegistry.
    
    This tool retrieves the path registered by RelationalGraphArchitect
    and lists all CSV files available for database construction.
    
    Args:
        path_key: The key used to register the path (default: "path_raw_csv_files")
        
    Returns:
        Dictionary containing:
        - path: The directory path
        - csv_files: List of CSV file names
        - file_info: Dict with file name -> row count and columns
    """
    try:
        object_registry = ObjectRegistry()
        
        # Get the registered path
        try:
            csv_path = object_registry.get(str, path_key)
        except KeyError:
            return {
                "error": f"Path key '{path_key}' not found in registry. "
                         "Make sure RelationalGraphArchitect has exported CSV files first."
            }
        
        if not os.path.exists(csv_path):
            return {
                "error": f"Path '{csv_path}' does not exist.",
                "path": csv_path
            }
        
        # List CSV files
        csv_files = [f for f in os.listdir(csv_path) if f.endswith('.csv')]
        
        if not csv_files:
            return {
                "error": "No CSV files found in the directory.",
                "path": csv_path
            }
        
        # Get basic info about each file
        file_info = {}
        for csv_file in csv_files:
            file_path = os.path.join(csv_path, csv_file)
            try:
                # Read just the header and count rows
                df = pd.read_csv(file_path, nrows=5)
                full_df = pd.read_csv(file_path)
                file_info[csv_file] = {
                    "columns": list(df.columns),
                    "row_count": len(full_df),
                    "sample_data": df.head(3).to_dict(orient='records')
                }
            except Exception as e:
                file_info[csv_file] = {"error": str(e)}
        
        return {
            "path": csv_path,
            "csv_files": csv_files,
            "file_info": file_info,
            "table_count": len(csv_files)
        }
        
    except Exception as e:
        logger.error(f"Error getting CSV files: {e}")
        return {"error": str(e)}


@tool
def get_schema_metadata_from_registry() -> Dict[str, Any]:
    """
    Retrieve schema metadata (PK/FK relationships, temporal columns) from ObjectRegistry.
    
    This tool gets the schema information that was extracted by RelationalGraphArchitect,
    which is essential for defining Table objects correctly.
    
    Returns:
        Dictionary containing:
        - tables: Dict of table name -> table info (columns, PKs, temporal info)
        - relationships: List of FK relationships
        - temporal_summary: Summary of temporal columns across tables
    """
    try:
        object_registry = ObjectRegistry()
        
        # Try to get the unified schema
        try:
            schema = object_registry.get(dict, "unified_db_schema")
            return {
                "found": True,
                "source": "unified_db_schema",
                "tables": schema.get("tables", {}),
                "relationships": schema.get("relationships", []),
                "temporal_summary": schema.get("temporal_summary", {})
            }
        except KeyError:
            pass
        
        # Try graph_schema_metadata
        try:
            schema = object_registry.get(dict, "graph_schema_metadata")
            return {
                "found": True,
                "source": "graph_schema_metadata",
                "tables": schema.get("tables", {}),
                "relationships": schema.get("relationships", []),
                "temporal_summary": schema.get("temporal_summary", {})
            }
        except KeyError:
            pass
        
        return {
            "found": False,
            "error": "No schema metadata found in registry. "
                     "Make sure RelationalGraphArchitect has extracted schema first."
        }
        
    except Exception as e:
        logger.error(f"Error getting schema metadata: {e}")
        return {"error": str(e)}


@tool
def validate_database_code(code: str) -> Dict[str, Any]:
    """
    Validate the generated Database code by checking syntax and imports.
    
    Args:
        code: The Python code string to validate
        
    Returns:
        Dictionary with validation results:
        - valid: Boolean indicating if code is valid
        - errors: List of any errors found
        - warnings: List of any warnings
    """
    import ast
    
    errors = []
    warnings = []
    
    # Check syntax
    try:
        ast.parse(code)
    except SyntaxError as e:
        errors.append(f"Syntax error at line {e.lineno}: {e.msg}")
        return {
            "valid": False,
            "errors": errors,
            "warnings": warnings
        }
    
    # Check for required imports
    required_imports = [
        "pandas",
        "Database",
        "Dataset", 
        "Table"
    ]
    
    for imp in required_imports:
        if imp not in code:
            warnings.append(f"Missing import or usage of '{imp}'")
    
    # Check for required class structure
    if "class " not in code or "(Dataset)" not in code:
        errors.append("Code must define a class that inherits from Dataset")
    
    if "def make_db" not in code:
        errors.append("Dataset class must implement 'make_db' method")
    
    if "val_timestamp" not in code:
        warnings.append("Missing val_timestamp class attribute")
    
    if "test_timestamp" not in code:
        warnings.append("Missing test_timestamp class attribute")
    
    # Check that make_db returns something (db variable or Database())
    if "return db" not in code and "return Database(" not in code:
        errors.append("make_db method must return a Database object (typically 'return db')")
    
    return {
        "valid": len(errors) == 0,
        "status": "valid" if len(errors) == 0 else "invalid",
        "errors": errors,
        "warnings": warnings
    }


@tool
def register_database_code(
    dataset_name: str,
    code: str,
    val_timestamp: str,
    test_timestamp: str,
    description: str = ""
) -> Dict[str, Any]:
    """
    Register the generated Database code in ObjectRegistry for later use.
    
    Args:
        dataset_name: Name of the dataset (e.g., "stack_overflow", "f1")
        code: The complete Python code for the Dataset class
        val_timestamp: Validation timestamp string
        test_timestamp: Test timestamp string
        description: Optional description of the dataset
        
    Returns:
        Confirmation message with registered key
    """
    try:
        object_registry = ObjectRegistry()
        
        # Extract class name from code
        import re
        class_match = re.search(r"class\s+(\w+)\s*\(", code)
        if class_match:
            class_name = class_match.group(1)
        else:
            class_name = dataset_name # Fallback
            
        # Store the code
        code_key = f"database_code_{dataset_name}"
        object_registry.register(str, code_key, code, overwrite=True)
        
        # Store metadata
        metadata_key = f"database_metadata_{dataset_name}"
        metadata = {
            "dataset_name": dataset_name,
            "class_name": class_name,
            "val_timestamp": val_timestamp,
            "test_timestamp": test_timestamp,
            "description": description,
            "code_key": code_key
        }
        object_registry.register(dict, metadata_key, metadata, overwrite=True)
        
        logger.info(f"âœ… Registered database code for '{dataset_name}' (Class: {class_name})")
        
        return {
            "success": True,
            "code_key": code_key,
            "metadata_key": metadata_key,
            "class_name": class_name,
            "message": f"Successfully registered database code for '{dataset_name}'"
        }
        
    except Exception as e:
        logger.error(f"Failed to register database code: {e}")
        return {"success": False, "error": str(e)}


@tool
def export_database_code(
    dataset_name: str,
    output_dir: str = "workdir"
) -> Dict[str, Any]:
    """
    Export the generated Database code to a Python file.
    
    Args:
        dataset_name: Name of the dataset to export
        output_dir: Directory to save the Python file (default: "workdir")
        
    Returns:
        Dictionary with:
        - success: Boolean
        - file_path: Path to the exported file
    """
    try:
        object_registry = ObjectRegistry()
        
        # If output_dir is default, try to get from registry
        if output_dir == "workdir":
            try:
                output_dir = object_registry.get(str, "working_dir")
            except KeyError:
                try:
                    output_dir = object_registry.get(str, "current_chat_session_dir")
                except KeyError:
                    # Create new session directory
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                    output_dir = f".workdir/chat-session-{timestamp}"
                    object_registry.register(str, "current_chat_session_dir", output_dir, overwrite=True)
        
        # Get the code
        code_key = f"database_code_{dataset_name}"
        try:
            code = object_registry.get(str, code_key)
        except KeyError:
            return {
                "success": False,
                "error": f"No code found for dataset '{dataset_name}'. "
                         "Register the code first using register_database_code."
            }
        
        # Create output directory
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Write the file
        file_name = "dataset.py"
        file_path = os.path.join(output_dir, file_name)
        
        with open(file_path, 'w') as f:
            f.write(code)
            
        # Register export info
        # Try to get class name from metadata
        metadata_key = f"database_metadata_{dataset_name}"
        try:
            metadata = object_registry.get(dict, metadata_key)
            class_name = metadata.get("class_name", dataset_name)
        except KeyError:
            class_name = dataset_name

        export_info = {
            "class_name": class_name,
            "export_path": file_path,
            "module_name": "dataset"
        }
        
        # Save metadata to JSON file for persistence across sessions
        import json
        metadata_file = os.path.join(output_dir, "dataset_metadata.json")
        try:
            with open(metadata_file, 'w') as mf:
                json.dump(export_info, mf, indent=2)
            logger.info(f"Saved dataset metadata to '{metadata_file}'")
        except Exception as e:
            logger.warning(f"Failed to save dataset metadata file: {e}")
        
        object_registry.register(dict, "database_code", export_info, overwrite=True)
        
        logger.info(f"Exported database code to '{file_path}'")
        
        return {
            "success": True,
            "file_path": file_path,
            "message": f"Successfully exported database code to '{file_path}'"
        }
        
    except Exception as e:
        logger.error(f"Failed to export database code: {e}")
        return {"success": False, "error": str(e)}


@tool
def get_database_code_template() -> str:
    """
    Get the template for generating Dataset code.
    
    This template follows the RelBench Dataset pattern and should be used
    as a starting point for generating dataset-specific code.
    
    Returns:
        The code template as a string with placeholders.
    """
    return DATASET_CODE_TEMPLATE


@tool
def get_temporal_statistics(csv_path: Optional[str] = None, time_columns: List[str] = None) -> Dict[str, Any]:
    """
    Analyze temporal columns to help determine val_timestamp and test_timestamp.
    
    This tool reads CSV files and analyzes timestamp columns to provide
    statistics that help choose appropriate temporal split points.
    
    Args:
        csv_path: Path to the CSV directory. If None, will auto-fetch from registry
                  using key 'path_raw_csv_files'.
        time_columns: Optional list of specific time column names to analyze.
                      If None, will auto-detect timestamp columns.
    
    Returns:
        Dictionary with:
        - columns_analyzed: List of timestamp columns found
        - statistics: Per-column stats (min, max, percentiles)
        - suggested_splits: Recommended val_timestamp and test_timestamp
    """
    try:
        import glob
        from datetime import datetime
        
        # Auto-fetch path from registry if not provided
        if csv_path is None:
            object_registry = ObjectRegistry()
            try:
                csv_path = object_registry.get(str, "path_raw_csv_files")
            except KeyError:
                return {
                    "error": "No csv_path provided and 'path_raw_csv_files' not found in registry. "
                             "Either provide csv_path or run RelationalGraphArchitect first."
                }
        
        if not os.path.exists(csv_path):
            return {"error": f"Path '{csv_path}' does not exist."}
        
        all_timestamps = []
        column_stats = {}
        
        # Find all CSV files
        csv_files = glob.glob(os.path.join(csv_path, "*.csv"))
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                table_name = os.path.basename(csv_file).replace('.csv', '')
                
                # Auto-detect timestamp columns if not specified
                if time_columns:
                    cols_to_check = [c for c in time_columns if c in df.columns]
                else:
                    # Look for common timestamp column names and datetime types
                    date_keywords = ['date', 'time', 'timestamp', 'created', 'updated', '_at']
                    cols_to_check = [
                        c for c in df.columns 
                        if any(kw in c.lower() for kw in date_keywords)
                    ]
                
                for col in cols_to_check:
                    try:
                        # Try to parse as datetime
                        ts_series = pd.to_datetime(df[col], errors='coerce')
                        valid_ts = ts_series.dropna()
                        
                        if len(valid_ts) > 0:
                            key = f"{table_name}.{col}"
                            column_stats[key] = {
                                "min": str(valid_ts.min()),
                                "max": str(valid_ts.max()),
                                "count": len(valid_ts),
                                "null_count": len(ts_series) - len(valid_ts),
                                "percentile_70": str(valid_ts.quantile(0.70)),
                                "percentile_85": str(valid_ts.quantile(0.85)),
                            }
                            all_timestamps.extend(valid_ts.tolist())
                    except Exception:
                        pass
                        
            except Exception as e:
                logger.debug(f"Error processing {csv_file}: {e}")
        
        if not all_timestamps:
            return {
                "error": "No valid timestamp columns found in CSV files.",
                "columns_analyzed": [],
                "statistics": {},
                "suggested_splits": None
            }
        
        # Calculate overall statistics
        all_ts_series = pd.Series(all_timestamps)
        overall_min = all_ts_series.min()
        overall_max = all_ts_series.max()
        
        # Suggest splits at 70% and 85% of the data
        val_ts = all_ts_series.quantile(0.70)
        test_ts = all_ts_series.quantile(0.85)
        
        # Check if test_ts is too close to max_timestamp
        # If the gap is small, it might cause issues for tasks with large timedelta
        time_range = overall_max - overall_min
        test_gap = overall_max - test_ts
        
        warning = ""
        if test_gap < (time_range * 0.05): # Less than 5% of range left
             warning = (
                 f"WARNING: test_timestamp is very close to max_timestamp (gap: {test_gap}). "
                 f"Tasks with timedelta > {test_gap} will fail. "
                 f"Consider choosing an earlier test_timestamp."
             )
        
        return {
            "columns_analyzed": list(column_stats.keys()),
            "statistics": column_stats,
            "overall": {
                "min": str(overall_min),
                "max": str(overall_max),
                "total_timestamps": len(all_timestamps),
            },
            "suggested_splits": {
                "val_timestamp": str(val_ts.date()),
                "test_timestamp": str(test_ts.date()),
                "max_timestamp": str(overall_max.date()),
                "explanation": (
                    f"val_timestamp at 70th percentile: ~70% of data before this date. "
                    f"test_timestamp at 85th percentile: ~85% of data before this date. "
                    f"Max timestamp is {overall_max.date()}. "
                    f"{warning}"
                )
            }
        }
        
    except Exception as e:
        logger.error(f"Error analyzing temporal statistics: {e}")
        return {"error": str(e)}
